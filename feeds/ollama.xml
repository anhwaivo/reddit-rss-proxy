<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-05-27T19:21:52+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1kuku1g</id>
    <title>What are the most capable LLM models to run with NVIDIA GeForce RTX 4060 8GB Laptop GPU and AMD Ryzen 9 8945HS CPU and 32 RAM</title>
    <updated>2025-05-24T20:19:35+00:00</updated>
    <author>
      <name>/u/Happysedits</name>
      <uri>https://old.reddit.com/user/Happysedits</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Happysedits"&gt; /u/Happysedits &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kuku1g/what_are_the_most_capable_llm_models_to_run_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kuku1g/what_are_the_most_capable_llm_models_to_run_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kuku1g/what_are_the_most_capable_llm_models_to_run_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-24T20:19:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kufitg</id>
    <title>Cua : Docker Container for Computer Use Agents</title>
    <updated>2025-05-24T16:23:57+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kufitg/cua_docker_container_for_computer_use_agents/"&gt; &lt;img alt="Cua : Docker Container for Computer Use Agents" src="https://external-preview.redd.it/NWdkb3VuNDZhcjJmMcsvHa0C_XuOSkhUSfxPH2wNUS_IzERNrp7qS2qcV3Nx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d4005034643f2814d24f0385eb4e46b274bf994" title="Cua : Docker Container for Computer Use Agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cua is the Docker for Computer-Use Agent, an open-source framework that enables AI agents to control full operating systems within high-performance, lightweight virtual containers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8kzntcf6ar2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kufitg/cua_docker_container_for_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kufitg/cua_docker_container_for_computer_use_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-24T16:23:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kut4v8</id>
    <title>2x 3090 cards - ollama installed with multiple models</title>
    <updated>2025-05-25T03:30:58+00:00</updated>
    <author>
      <name>/u/onemorequickchange</name>
      <uri>https://old.reddit.com/user/onemorequickchange</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My mb has 64GB RAM and an i9-12900k CPU. I've gotten deepseek-r1:70b and llama3.3:latest to use both cards.&lt;br /&gt; qwen2.5-coder:32b is my goto for coding. So the real question is, what is the next best coding model that I can still run with these specs? And what would be a model to justify a upgraded hardware?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onemorequickchange"&gt; /u/onemorequickchange &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kut4v8/2x_3090_cards_ollama_installed_with_multiple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kut4v8/2x_3090_cards_ollama_installed_with_multiple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kut4v8/2x_3090_cards_ollama_installed_with_multiple/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-25T03:30:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuq0mt</id>
    <title>Is there any easy way to get up and running with chatgpt-like capabilities at home?</title>
    <updated>2025-05-25T00:33:29+00:00</updated>
    <author>
      <name>/u/MeYaj1111</name>
      <uri>https://old.reddit.com/user/MeYaj1111</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a noob, running Windows 10 on a 32GB i5-9600K w/ 8GB GTX 3070&lt;/p&gt; &lt;p&gt;I do not care about performance, I only care about capability. &lt;/p&gt; &lt;p&gt;Is there any way to get up and running with a chatgpt-like interface that I can use for general purpose things like doing research with real-time data from internet searches, &amp;quot;deep research&amp;quot; where it will take the time to think about its answer before finalizing it, basic image generation, etc? As close to the chatgpt experience as possible, aside from the performance since I know my system is crap.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MeYaj1111"&gt; /u/MeYaj1111 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kuq0mt/is_there_any_easy_way_to_get_up_and_running_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kuq0mt/is_there_any_easy_way_to_get_up_and_running_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kuq0mt/is_there_any_easy_way_to_get_up_and_running_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-25T00:33:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvb4eo</id>
    <title>Graphical card</title>
    <updated>2025-05-25T19:40:14+00:00</updated>
    <author>
      <name>/u/Zealousideal-One5210</name>
      <uri>https://old.reddit.com/user/Zealousideal-One5210</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Because I'm a complete noob for graphical cards... Couple of months ago I bought a beelink Intel Arc with this docking station &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.bee-link.com/products/beelink-ex-docking-station?variant=46659193241842"&gt;https://www.bee-link.com/products/beelink-ex-docking-station?variant=46659193241842&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now I'm looking for a graphical card that can run perfectly with ollama. Not looking for those massive big models. I'm happy with the smaller ones, because I also see the smaller ones getting better and better. And not want to spend to much (max 350 euro). So I found this card for example &lt;a href="https://amzn.eu/d/6D5vaQ8"&gt;https://amzn.eu/d/6D5vaQ8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would this work? Is this one any good for Running gemma3:8b for example?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-One5210"&gt; /u/Zealousideal-One5210 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kvb4eo/graphical_card/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kvb4eo/graphical_card/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kvb4eo/graphical_card/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-25T19:40:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuy23x</id>
    <title>Updated jarvis project .</title>
    <updated>2025-05-25T08:55:07+00:00</updated>
    <author>
      <name>/u/cython_boy</name>
      <uri>https://old.reddit.com/user/cython_boy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After weeks of upgrades and modular refinements, I'm thrilled to unveil the latest version of &lt;strong&gt;Jarvis&lt;/strong&gt;, my personal AI assistant built with Streamlit, LangChain, Gemini, Ollama, and custom ML/LLM agents. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ganeshnikhil/J.A.R.V.I.S.2.0"&gt;JARVIS&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Normal&lt;/strong&gt;: Understands natural queries and executes dynamic function calls.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Personal Chat&lt;/strong&gt;: Keeps track of important conversations and responds contextually using Ollama + memory logic.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAG Chat&lt;/strong&gt;: Ask deep questions across topics like Finance, AI, Disaster, Space Tech using embedded knowledge via LangChain + FAISS.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data Analysis&lt;/strong&gt;: Upload a CSV, ask in plain English, and Jarvis will auto-generate insightful Python code (with fallback logic if API fails!).&lt;/li&gt; &lt;li&gt;Toggle voice replies on/off.&lt;/li&gt; &lt;li&gt;Use voice input via audio capture.&lt;/li&gt; &lt;li&gt;Speech output uses real-time TTS with Streamlit rendering.&lt;/li&gt; &lt;li&gt; Enable Developer Mode, turn on USB Debugging, connect via USB, and run &lt;code&gt;adb devices&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cython_boy"&gt; /u/cython_boy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kuy23x/updated_jarvis_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kuy23x/updated_jarvis_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kuy23x/updated_jarvis_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-25T08:55:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvixua</id>
    <title>Title: Seeking Help: A "Deep Research" Project for a Retired Mathematician (Recoll, Langchain, Ollama)</title>
    <updated>2025-05-26T02:00:10+00:00</updated>
    <author>
      <name>/u/DigiDadaist</name>
      <uri>https://old.reddit.com/user/DigiDadaist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Reddit!&lt;/p&gt; &lt;p&gt;I'm a 70-year-old retired mathematician from Poland. I have a large collection of digital books and articles, indexed using Recoll. I want to build a tool that can help me explore and understand this information in more depth.&lt;/p&gt; &lt;p&gt;My idea is to create a &amp;quot;deep research&amp;quot; application that works like this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;**Find Documents:** Use Recoll (through its web interface's API) to find documents related to a topic.&lt;/li&gt; &lt;li&gt;**Ask Questions:** Use a computer program (Langchain and Ollama) to automatically generate questions about these documents. The program should be able to ask many different questions to really understand the topic.&lt;/li&gt; &lt;li&gt;**Answer Questions:** Use the same program (Langchain and Ollama) to answer the questions, using the documents as a source of information.&lt;/li&gt; &lt;li&gt;**Learn and Repeat:** The program should learn from the answers and use that knowledge to ask even better questions. It should repeat this process several times.&lt;/li&gt; &lt;li&gt;**Create Summary:** Finally, the program should create a summary of everything it has learned.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I am inspired by this project: &lt;a href="https://github.com/u14app/deep-research"&gt;https://github.com/u14app/deep-research&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I want to use:&lt;/p&gt; &lt;p&gt;* **Recoll:** Because I already use it to index my documents.&lt;/p&gt; &lt;p&gt;* **Langchain:** A framework to help build the program.&lt;/p&gt; &lt;p&gt;* **Ollama:** To run a &amp;quot;Large Language Model&amp;quot; locally on my computer (no internet needed). This model will help generate and answer questions.&lt;/p&gt; &lt;p&gt;The problems I have are:&lt;/p&gt; &lt;p&gt;* **My English is not very good.**&lt;/p&gt; &lt;p&gt;* **I am not a strong programmer.** I know some basic programming, but not enough to build this myself.&lt;/p&gt; &lt;p&gt;* **Connecting Recoll with Langchain:** I don't know how to get the information from Recoll into Langchain.&lt;/p&gt; &lt;p&gt;* **Making the program ask good questions:** I need help making the program generate questions that are interesting and useful.&lt;/p&gt; &lt;p&gt;I am looking for help from the community. I would like:&lt;/p&gt; &lt;p&gt;* **Advice and ideas:** Any suggestions are welcome!&lt;/p&gt; &lt;p&gt;* **Example code:** Especially for connecting Recoll with Langchain.&lt;/p&gt; &lt;p&gt;* **Someone to collaborate with:** If you are interested in helping me build this project, please contact me! I am willing to learn and contribute as much as I can.&lt;/p&gt; &lt;p&gt;I plan to make this project open source so that others can use it.&lt;/p&gt; &lt;p&gt;Thank you for your time and help!&lt;/p&gt; &lt;p&gt;TL;DR: Retired mathematician needs help building a &amp;quot;deep research&amp;quot; tool using Recoll, Langchain, and Ollama. Low programming skills, needs help with Recoll integration and question generation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DigiDadaist"&gt; /u/DigiDadaist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kvixua/title_seeking_help_a_deep_research_project_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kvixua/title_seeking_help_a_deep_research_project_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kvixua/title_seeking_help_a_deep_research_project_for_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-26T02:00:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kv61te</id>
    <title>Local-first AI + SearXNG in one place - reclaim your autonomy (Cognito AI Search v1.1.0)</title>
    <updated>2025-05-25T16:03:18+00:00</updated>
    <author>
      <name>/u/kekePower</name>
      <uri>https://old.reddit.com/user/kekePower</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;After many late nights and a lot of caffeine, I’m proud to share something I’ve been quietly building for a while: &lt;a href="https://github.com/kekePower/cognito-ai-search"&gt;&lt;strong&gt;Cognito AI Search&lt;/strong&gt;&lt;/a&gt;, a self-hosted, local-first tool that combines &lt;strong&gt;private AI chat&lt;/strong&gt; (via Ollama) with &lt;strong&gt;anonymous web search&lt;/strong&gt; (via SearXNG) in one clean interface.&lt;/p&gt; &lt;p&gt;I wanted something that would let me:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ask questions to a fast, local LLM without my data ever leaving my machine&lt;/li&gt; &lt;li&gt;Search the web anonymously without all the bloat, tracking, or noise&lt;/li&gt; &lt;li&gt;Use a single, simple UI, not two disconnected tabs or systems&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I built it.&lt;br /&gt; No ads, no logging, no cloud dependencies, just pure function. The blog post dives a little deeper into the thinking behind it and shows a screenshot:&lt;br /&gt; 👉 &lt;a href="https://blog.kekepower.com/blog/2025/may/25/cognito_ai_search_110_-_where_precision_meets_polish.html"&gt;Cognito AI Search 1.1.0 - Where Precision Meets Polish&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I built this for people like me, people who want control, speed, and clarity in how they interact with both AI and the web. It’s open source, minimal, and actively being improved.&lt;/p&gt; &lt;p&gt;Would love to hear your feedback, ideas, or criticism. If it’s useful to even a handful of people here, I’ll consider that a win. 🙌&lt;/p&gt; &lt;p&gt;Thanks for checking it out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kekePower"&gt; /u/kekePower &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kv61te/localfirst_ai_searxng_in_one_place_reclaim_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kv61te/localfirst_ai_searxng_in_one_place_reclaim_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kv61te/localfirst_ai_searxng_in_one_place_reclaim_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-25T16:03:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvjt36</id>
    <title>Looking to learn about hosting my first local LLM</title>
    <updated>2025-05-26T02:48:55+00:00</updated>
    <author>
      <name>/u/anmolmanchanda</name>
      <uri>https://old.reddit.com/user/anmolmanchanda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I have been a huge ChatGPT user since day 1. I am confident that I have been the top 1% user, using it several hours daily for personal and work; solving every problem in life with it. I ended up sharing more and more personal and sensitive information to give context and the more i gave, the better it was able to help me until I realised the privacy implications.&lt;/p&gt; &lt;p&gt;I am now looking to replace my experience with ChatGPT 4o as long as I can get close to accuracy. I am okay with being twice or three times as slow which would be understandable.&lt;/p&gt; &lt;p&gt;I also understand that it runs on millions of dollars of infrastructure, my goal is not get exactly there, just as close as I can.&lt;/p&gt; &lt;p&gt;I experimented with LLama 3 8B Q4 on my MacBook Pro, speed was acceptable but the responses left a bit to be desired. Then I moved to Deepseek r1 distilled 14B Q5 which was streching the limit of my laptop, but I was able to run it and responses were better.&lt;/p&gt; &lt;p&gt;I am currently thinking of buying a new or very likely used PC (or used parts for a PC separately) to run LLama 3.3 70B Q4. Q5 would be slightly better but I don't want to spend crazy from the start.&lt;/p&gt; &lt;p&gt;And I am hoping to upgrade in 1-2 months so the PC can run FP16 for the same model.&lt;/p&gt; &lt;p&gt;I am also considering Llama 4 and I need to read more about it to understand it's benefits and costs.&lt;/p&gt; &lt;p&gt;My budget initially preferably would be $3500 CAD, but would be willing to go to $4000 CAD for a solid foundation that I can build upon.&lt;/p&gt; &lt;p&gt;I use ChatGPT for work a lot, I would like accuracy and reliabiltiy to be as high as 4o; so part of me wants to build for FP16 from the get go.&lt;/p&gt; &lt;p&gt;For coding, I pay seperately for Cursor and that I am willing to keep paying until I have FP16 at least or even after as Claude Sonnet 4 is unbeatable. I am curious what open source model is as good in coding to that?&lt;/p&gt; &lt;p&gt;For the update in 1-2 months, budget I am thinking is $2000-2500 CAD&lt;/p&gt; &lt;p&gt;I am looking to hear which of my assumptions are wrong? What resources I should read more? What hardware specifications I should buy for my first AI PC? Which model is best suited for my needs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anmolmanchanda"&gt; /u/anmolmanchanda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kvjt36/looking_to_learn_about_hosting_my_first_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kvjt36/looking_to_learn_about_hosting_my_first_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kvjt36/looking_to_learn_about_hosting_my_first_local_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-26T02:48:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvj89n</id>
    <title>What's the best I can get from Ollama with my setup? Looking for model &amp; workflow suggestions</title>
    <updated>2025-05-26T02:15:34+00:00</updated>
    <author>
      <name>/u/Calebe94</name>
      <uri>https://old.reddit.com/user/Calebe94</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I'm diving deeper into local LLM workflows with Ollama and wanted to tap into the community's collective brainpower for some guidance and inspiration.&lt;/p&gt; &lt;p&gt;Here’s what I’m working with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🧠 &lt;strong&gt;CPU&lt;/strong&gt;: Ryzen 5 5600X&lt;/li&gt; &lt;li&gt;🧠 &lt;strong&gt;RAM&lt;/strong&gt;: 64GB DDR4 @ 3600MHz&lt;/li&gt; &lt;li&gt;🎮 &lt;strong&gt;GPU&lt;/strong&gt;: Radeon RX6600 (so yeah, ROCm is &lt;em&gt;meh&lt;/em&gt;, I’m mostly CPU-bound)&lt;/li&gt; &lt;li&gt;🐧 &lt;strong&gt;OS&lt;/strong&gt;: Debian Sid&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I work as a senior cloud developer and also do embedded/hardware stuff (KiCAD, electronics prototyping, custom mechanical keyboards, etc). I’m also neurodivergent (ADHD, autism), and I’ve been trying to integrate LLMs into my workflow not just for productivity, but also for cognitive scaffolding — like breaking down complex tasks, context retention, journaling, decision trees, automations, and reminders.&lt;/p&gt; &lt;p&gt;So I’m wondering:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Given my setup, what’s the best I can realistically run smoothly with Ollama?&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What models do you recommend for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Coding (Python, Terraform, Bash, KiCAD-related tasks)&lt;/li&gt; &lt;li&gt;Thought organization (task breakdown, long-context support)&lt;/li&gt; &lt;li&gt;Automation planning (like agents / planners that actually work offline-ish)&lt;/li&gt; &lt;li&gt;General chat and productivity assistance&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Any tools you’d recommend pairing with Ollama for local workflows?&lt;/li&gt; &lt;li&gt;Anyone doing automations with shell scripts or hooking LLMs into daily tools like &lt;code&gt;todo.txt&lt;/code&gt;, &lt;code&gt;obsidian&lt;/code&gt;, &lt;code&gt;cron&lt;/code&gt;, or even custom scripts?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I know my GPU limits me with current ROCm support, but with 64GB RAM, I figure there’s still a lot I can do. I’m also fine running things in CPU-only mode, if it means more flexibility or compatibility.&lt;/p&gt; &lt;p&gt;Would love to hear what kind of setups you folks are running, and what models/tools/flows are actually &lt;em&gt;worth it&lt;/em&gt; right now in the local LLM scene.&lt;/p&gt; &lt;p&gt;Appreciate any tips or setups you’re willing to share. 🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Calebe94"&gt; /u/Calebe94 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kvj89n/whats_the_best_i_can_get_from_ollama_with_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kvj89n/whats_the_best_i_can_get_from_ollama_with_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kvj89n/whats_the_best_i_can_get_from_ollama_with_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-26T02:15:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvsgeg</id>
    <title>AI vision on windows with Ollama</title>
    <updated>2025-05-26T11:53:32+00:00</updated>
    <author>
      <name>/u/LivingSignificant452</name>
      <uri>https://old.reddit.com/user/LivingSignificant452</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;br /&gt; in case you prefer the speed of a native application for windows, Obviousidea just announced they support Ollama with Light Image Editor :&lt;br /&gt; &lt;a href="https://www.obviousidea.com/light-image-resizer-ollama-support-ai-vision/"&gt;https://www.obviousidea.com/light-image-resizer-ollama-support-ai-vision/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;it speed up the upload part and directly save in the metadata the description. there is an automode to speed up the description on a set of photos.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LivingSignificant452"&gt; /u/LivingSignificant452 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kvsgeg/ai_vision_on_windows_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kvsgeg/ai_vision_on_windows_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kvsgeg/ai_vision_on_windows_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-26T11:53:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwelnh</id>
    <title>LlamaFirewall: framework open source per rilevare e mitigare i rischi per la sicurezza incentrati sull'intelligenza artificiale - Help Net Security</title>
    <updated>2025-05-27T04:36:45+00:00</updated>
    <author>
      <name>/u/Aiochedolor</name>
      <uri>https://old.reddit.com/user/Aiochedolor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kwelnh/llamafirewall_framework_open_source_per_rilevare/"&gt; &lt;img alt="LlamaFirewall: framework open source per rilevare e mitigare i rischi per la sicurezza incentrati sull'intelligenza artificiale - Help Net Security" src="https://external-preview.redd.it/Pu8f913f0BZRcZq8aaUGAPiafLL1kwOvTbOn67rnK18.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2c2f8c87594f99a011fd4879e331bec04f33f4b" title="LlamaFirewall: framework open source per rilevare e mitigare i rischi per la sicurezza incentrati sull'intelligenza artificiale - Help Net Security" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aiochedolor"&gt; /u/Aiochedolor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.helpnetsecurity.com/2025/05/26/llamafirewall-open-source-framework-detect-mitigate-ai-centric-security-risks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwelnh/llamafirewall_framework_open_source_per_rilevare/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kwelnh/llamafirewall_framework_open_source_per_rilevare/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-27T04:36:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwi20g</id>
    <title>Extract Website Information</title>
    <updated>2025-05-27T08:25:35+00:00</updated>
    <author>
      <name>/u/OriginalDiddi</name>
      <uri>https://old.reddit.com/user/OriginalDiddi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I would like to extract the informations from a local hosted website.&lt;/p&gt; &lt;p&gt;I thought it would be a simple Python script but somehow it does not work for me yet.&lt;/p&gt; &lt;p&gt;It would be nice if someone can help me create a Script, or whatever that I can use to extract webpage information and upload it to the AI. Maby even with an Open WebUI connection if thats possible,&lt;/p&gt; &lt;p&gt;(Iam noob in AI)&lt;/p&gt; &lt;p&gt;Edit&lt;/p&gt; &lt;p&gt;GPT told me I could do it A) with Python Script and BeautifulSoup to create a .txt file and upload it to open web UI or B) to use llamaindex in a Python Script to do the same. Neither worked out so far.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OriginalDiddi"&gt; /u/OriginalDiddi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwi20g/extract_website_information/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwi20g/extract_website_information/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kwi20g/extract_website_information/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-27T08:25:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwjsq1</id>
    <title>/api/generate report 404 error</title>
    <updated>2025-05-27T10:24:38+00:00</updated>
    <author>
      <name>/u/LlamaZookeeper</name>
      <uri>https://old.reddit.com/user/LlamaZookeeper</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I m trying to invoke my ollama using /api/generate, but it returned 404 error. Completion and chat looks ok. What might be the issue? If I want to do troubleshooting, where to find the debug log in ollama server? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LlamaZookeeper"&gt; /u/LlamaZookeeper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwjsq1/apigenerate_report_404_error/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwjsq1/apigenerate_report_404_error/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kwjsq1/apigenerate_report_404_error/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-27T10:24:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwiff1</id>
    <title>How Ollama manage to run LLM that require more VRAM that my card actually have</title>
    <updated>2025-05-27T08:52:25+00:00</updated>
    <author>
      <name>/u/Repulsive_Shock8318</name>
      <uri>https://old.reddit.com/user/Repulsive_Shock8318</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi ! &lt;/p&gt; &lt;p&gt;This question is (I think) low level but I'm really interested about how a larger model can fit and run on my small GPU. &lt;/p&gt; &lt;p&gt;I'm currently using Qwen3:4b on a A2000 laptop with 4GB of VRAM, and when the model is loaded in my GPU by ollama, I see theses logs &lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama | time=2025-05-27T08:11:29.448Z level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=27 layers.split=&amp;quot;&amp;quot; memory.available=&amp;quot;[3.2 GiB]&amp;quot; memory.gpu_overhead=&amp;quot;0 B&amp;quot; memory.required.full=&amp;quot;4.1 GiB&amp;quot; memory.required.partial=&amp;quot;3.2 GiB&amp;quot; memory.required.kv=&amp;quot;576.0 MiB&amp;quot; memory.required.allocations=&amp;quot;[3.2 GiB]&amp;quot; memory.weights.total=&amp;quot;2.4 GiB&amp;quot; memory.weights.repeating=&amp;quot;2.1 GiB&amp;quot; memory.weights.nonrepeating=&amp;quot;304.3 MiB&amp;quot; memory.graph.full=&amp;quot;384.0 MiB&amp;quot; memory.graph.partial=&amp;quot;384.0 MiB&amp;quot; ollama | llama_model_loader: loaded meta data with 27 key-value pairs and 398 tensors from /root/.ollama/models/blobs/sha256-163553aea1b1de62de7c5eb2ef5afb756b4b3133308d9ae7e42e951d8d696ef5 (version GGUF V3 (latest)) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the first line, the &lt;code&gt;memory.required.full&lt;/code&gt; (that is think is the model size) is bigger than &lt;code&gt;memory.available&lt;/code&gt; (that is the available VRAM in my GPU). I saw the &lt;code&gt;memory.required.partial&lt;/code&gt;that corresponding to to available VRAM. &lt;/p&gt; &lt;p&gt;So did Ollama shrink the model or load only a part of it ? I'm new to onprem IA usage, my apologize if I said something stupid &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Repulsive_Shock8318"&gt; /u/Repulsive_Shock8318 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwiff1/how_ollama_manage_to_run_llm_that_require_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwiff1/how_ollama_manage_to_run_llm_that_require_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kwiff1/how_ollama_manage_to_run_llm_that_require_more/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-27T08:52:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwmx4n</id>
    <title>Python script analyzes Git history with a local Ollama &amp; chosen AI model. Takes repo path, model, &amp; commit limit (CLI). For selected commits, it extracts diffs, then the AI generates Conventional Commit messages based on changes. Prints suggestions; doesn't alter repository history.</title>
    <updated>2025-05-27T13:12:43+00:00</updated>
    <author>
      <name>/u/sasizza</name>
      <uri>https://old.reddit.com/user/sasizza</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kwmx4n/python_script_analyzes_git_history_with_a_local/"&gt; &lt;img alt="Python script analyzes Git history with a local Ollama &amp;amp; chosen AI model. Takes repo path, model, &amp;amp; commit limit (CLI). For selected commits, it extracts diffs, then the AI generates Conventional Commit messages based on changes. Prints suggestions; doesn't alter repository history." src="https://external-preview.redd.it/DaucjXMGsNHM-CtmdilC9-Be6MC8V2z4ykjVCgOkTFc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62ca4cb88917f17e7200a6f1c665b5d959713745" title="Python script analyzes Git history with a local Ollama &amp;amp; chosen AI model. Takes repo path, model, &amp;amp; commit limit (CLI). For selected commits, it extracts diffs, then the AI generates Conventional Commit messages based on changes. Prints suggestions; doesn't alter repository history." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sasizza"&gt; /u/sasizza &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://gist.github.com/tailot/76a8f78cb58fd0c1902000c3f0c22368"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwmx4n/python_script_analyzes_git_history_with_a_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kwmx4n/python_script_analyzes_git_history_with_a_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-27T13:12:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwoedy</id>
    <title>Pdf translation and extraction to pdf.</title>
    <updated>2025-05-27T14:16:53+00:00</updated>
    <author>
      <name>/u/Constantinos_bou</name>
      <uri>https://old.reddit.com/user/Constantinos_bou</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello community! I'm trying to make an app that can read pdf files and translate them into other languages. Do you have any script or tip in mind? Thank you very much in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Constantinos_bou"&gt; /u/Constantinos_bou &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwoedy/pdf_translation_and_extraction_to_pdf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwoedy/pdf_translation_and_extraction_to_pdf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kwoedy/pdf_translation_and_extraction_to_pdf/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-27T14:16:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1kw8woc</id>
    <title>gemma3:12b-it-qat vs gemma3:12b memory usage using Ollama</title>
    <updated>2025-05-26T23:35:26+00:00</updated>
    <author>
      <name>/u/LithuanianAmerican</name>
      <uri>https://old.reddit.com/user/LithuanianAmerican</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;gemma3:12b-it-qat is advertised to use 3x less memory than gemma3:12b yet in my testing on my Mac I'm seeing that Ollama is actually using 11.55gb of memory for the quantized model and 9.74gb for the regular variant. Why is the quantized model actually using more memory? How can I &amp;quot;find&amp;quot; those memory savings?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LithuanianAmerican"&gt; /u/LithuanianAmerican &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kw8woc/gemma312bitqat_vs_gemma312b_memory_usage_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kw8woc/gemma312bitqat_vs_gemma312b_memory_usage_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kw8woc/gemma312bitqat_vs_gemma312b_memory_usage_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-26T23:35:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwqla4</id>
    <title>How to set system properties in windows for Ollama</title>
    <updated>2025-05-27T15:44:36+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When running Ollama in windows 11 in the command prompt,&lt;/p&gt; &lt;p&gt;how to set for example OLLAMA_HOST=0.0.0.0&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwqla4/how_to_set_system_properties_in_windows_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwqla4/how_to_set_system_properties_in_windows_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kwqla4/how_to_set_system_properties_in_windows_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-27T15:44:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwhxn0</id>
    <title>Open Source iOS OLLAMA Client</title>
    <updated>2025-05-27T08:16:54+00:00</updated>
    <author>
      <name>/u/billythepark</name>
      <uri>https://old.reddit.com/user/billythepark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kwhxn0/open_source_ios_ollama_client/"&gt; &lt;img alt="Open Source iOS OLLAMA Client" src="https://b.thumbs.redditmedia.com/RF21JHyswOvpdougfLK37KYr40CaEwHNJF5eJuvDolU.jpg" title="Open Source iOS OLLAMA Client" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As you all know, ollama is a program that allows you to install and use various latest LLMs on your computer. Once you install it on your computer, you don't have to pay a usage fee, and you can install and use various types of LLMs according to your performance.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/spkaaos8aa3f1.png?width=1984&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ec869871902c69c5a334c785b9600b470b52098"&gt;https://preview.redd.it/spkaaos8aa3f1.png?width=1984&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ec869871902c69c5a334c785b9600b470b52098&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However, the company that makes ollama does not make the UI. So there are several ollama-specific programs on the market. Last year, I made an ollama iOS client with Flutter and opened the code, but I didn't like the performance and UI, so I made it again. I will release the source code with the link. You can download the entire Swift source.&lt;/p&gt; &lt;p&gt;You can build it from the source, or you can download the app by going to the link.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/bipark/swift_ios_ollama_client_v3"&gt;https://github.com/bipark/swift_ios_ollama_client_v3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/billythepark"&gt; /u/billythepark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwhxn0/open_source_ios_ollama_client/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwhxn0/open_source_ios_ollama_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kwhxn0/open_source_ios_ollama_client/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-27T08:16:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwsf0v</id>
    <title>AI Presentation</title>
    <updated>2025-05-27T16:55:55+00:00</updated>
    <author>
      <name>/u/raghav-ai</name>
      <uri>https://old.reddit.com/user/raghav-ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any AI tool that can create ppt slides using ollama model, fully offline ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/raghav-ai"&gt; /u/raghav-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwsf0v/ai_presentation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwsf0v/ai_presentation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kwsf0v/ai_presentation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-27T16:55:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwhqzg</id>
    <title>Cognito: Your AI Sidekick for Chrome. A MIT licensed very lightweight Web UI with multitools.</title>
    <updated>2025-05-27T08:03:49+00:00</updated>
    <author>
      <name>/u/Asleep-Ratio7535</name>
      <uri>https://old.reddit.com/user/Asleep-Ratio7535</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kwhqzg/cognito_your_ai_sidekick_for_chrome_a_mit/"&gt; &lt;img alt="Cognito: Your AI Sidekick for Chrome. A MIT licensed very lightweight Web UI with multitools." src="https://external-preview.redd.it/LK9RDOmWWg-1fAGxwAutIIXGyDq1BJqkepaHRh_fCYA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=477c4bd91b13661b3c26cf1b6fda3685e1133731" title="Cognito: Your AI Sidekick for Chrome. A MIT licensed very lightweight Web UI with multitools." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;&lt;strong&gt;Easiest Setup: No python, no docker, no endless dev packages.&lt;/strong&gt; Just download it from &lt;a href="https://chromewebstore.google.com/detail/pphjdjdoclkedgiaahmiahladgcpohca?utm_source=item-share-cb"&gt;Chrome&lt;/a&gt; or my &lt;a href="https://github.com/3-ark/Cognito-AI_Sidekick"&gt;Github&lt;/a&gt; (Same with the store, just the latest release). You don't need an exe.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No privacy issue: you can check the code yourself.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Seamless AI Integration:&lt;/strong&gt; Connect to a wide array of powerful AI models: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local Models:&lt;/strong&gt; Ollama, LM Studio, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cloud Services: several&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom Connections:&lt;/strong&gt; all OpenAI compatible endpoints.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intelligent Content Interaction:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Instant Summaries:&lt;/strong&gt; Get the gist of any webpage in seconds.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Contextual Q&amp;amp;A:&lt;/strong&gt; Ask questions about the current page, PDFs, selected text in the notes or you can simply send the urls directly to the bot, the scrapper will give the bot context to use.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Web Search with scrapper:&lt;/strong&gt; Conduct context-aware searches using Google, DuckDuckGo, and Wikipedia, with the ability to fetch and analyze content from search results.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Customizable Personas (system prompts):&lt;/strong&gt; Choose from 7 pre-built AI personalities (Researcher, Strategist, etc.) or create your own.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text-to-Speech (TTS):&lt;/strong&gt; Hear AI responses read aloud (supports browser TTS and integration with external services like Piper).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat History:&lt;/strong&gt; You can search it (also planed to be used in RAG).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;![img](&lt;a href="https://github.com/3-ark/Cognito-AI_Sidekick/blob/main/docs/web.gif"&gt;https://github.com/3-ark/Cognito-AI_Sidekick/blob/main/docs/web.gif&lt;/a&gt;) ![img](&lt;a href="https://github.com/3-ark/Cognito-AI_Sidekick/blob/main/docs/local.gif"&gt;https://github.com/3-ark/Cognito-AI_Sidekick/blob/main/docs/local.gif&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;I don't know how to post image here, tried links, markdown links or directly upload, all failed to display. Screenshots gifs links below: &lt;a href="https://github.com/3-ark/Cognito-AI_Sidekick/blob/main/docs/web.gif"&gt;https://github.com/3-ark/Cognito-AI_Sidekick/blob/main/docs/web.gif&lt;/a&gt; &lt;a href="https://github.com/3-ark/Cognito-AI_Sidekick/blob/main/docs/local.gif"&gt;https://github.com/3-ark/Cognito-AI_Sidekick/blob/main/docs/local.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Asleep-Ratio7535"&gt; /u/Asleep-Ratio7535 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwhqzg/cognito_your_ai_sidekick_for_chrome_a_mit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwhqzg/cognito_your_ai_sidekick_for_chrome_a_mit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kwhqzg/cognito_your_ai_sidekick_for_chrome_a_mit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-27T08:03:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwnl0a</id>
    <title>Is there a way to export Ollama or OpenWebUI output as a formatted PDF similar to what Perplexity offers?</title>
    <updated>2025-05-27T13:42:20+00:00</updated>
    <author>
      <name>/u/wasnt_me_rly</name>
      <uri>https://old.reddit.com/user/wasnt_me_rly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've searched but have come up empty. Would love a plug-in which would allow me to save a conversation (in part or in full) in the format I see on the screen versus the plain text copy option available by default. Any guidance would be appreciated. TIA.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wasnt_me_rly"&gt; /u/wasnt_me_rly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwnl0a/is_there_a_way_to_export_ollama_or_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwnl0a/is_there_a_way_to_export_ollama_or_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kwnl0a/is_there_a_way_to_export_ollama_or_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-27T13:42:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwmoeo</id>
    <title>AI Runner v4.10.0 Release Notes</title>
    <updated>2025-05-27T13:01:26+00:00</updated>
    <author>
      <name>/u/w00fl35</name>
      <uri>https://old.reddit.com/user/w00fl35</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Last week we introduced multi-lingual support and ollama integration.&lt;/p&gt; &lt;p&gt;Today we've released AI Runner version 4.10.0. This update focuses on improving the stability and maintainability of the application through significant refactoring efforts and expanded test coverage.&lt;/p&gt; &lt;p&gt;Here’s a condensed look at what’s new:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Core Refactoring and Robustness:&lt;/strong&gt; The main agent base class has been restructured for better clarity and future development. Workflow saving processes are now more resilient, with better error handling and management of workflow IDs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Improved PySide6/Qt6 Compatibility:&lt;/strong&gt; We've made adjustments for better compatibility with PySide6 and Qt6, which includes fixes related to keyboard shortcuts and OpenGL.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Increased Test Coverage:&lt;/strong&gt; Test coverage has been considerably expanded across various parts of the application, including LLM widgets, the GUI, utility functions, and vendor modules. This helps ensure more reliable operation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Patched OS restriction logic and associated tests to ensure file operations are handled safely and whitelisting functions correctly.&lt;/li&gt; &lt;li&gt;Resolved a &lt;code&gt;DetachedInstanceError&lt;/code&gt; that could occur when saving workflows.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Developer Tooling:&lt;/strong&gt; A commit message template has been added to the repository to aid contributors.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The primary goal of this release was to enhance the underlying structure and reliability of AI Runner.&lt;/p&gt; &lt;p&gt;You can find the complete list of changes in the full release notes on GitHub: &lt;a href="https://github.com/Capsize-Games/airunner/releases/tag/v4.10.0"&gt;https://github.com/Capsize-Games/airunner/releases/tag/v4.10.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to share any thoughts or feedback.&lt;/p&gt; &lt;p&gt;Next Up:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I'll be working on more test coverage, nodegraph and LLM updates. &lt;/li&gt; &lt;li&gt;We have a new regular contributor (who also happens to be one of our admins) [&lt;a href="https://github.com/lucaerion%5D(lucarerion)"&gt;https://github.com/lucaerion](lucarerion)&lt;/a&gt; - thanks for your contributions to OpenVoice and Nodegraph tests and bug fixes&lt;/li&gt; &lt;li&gt;We have some developers looking into OSX and also Flux S support, so we may see some progress in these areas made&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w00fl35"&gt; /u/w00fl35 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwmoeo/ai_runner_v4100_release_notes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwmoeo/ai_runner_v4100_release_notes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kwmoeo/ai_runner_v4100_release_notes/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-27T13:01:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwm4cv</id>
    <title>D&amp;D Server</title>
    <updated>2025-05-27T12:34:55+00:00</updated>
    <author>
      <name>/u/barrulus</name>
      <uri>https://old.reddit.com/user/barrulus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So my son and I love to play D&amp;amp;D but have no one nearby who plays. Online play through D&amp;amp;d Beyond is possible but intimidating for him, so we practically never play. &lt;/p&gt; &lt;p&gt;Enter LLM’s!&lt;/p&gt; &lt;p&gt;This morning I opened up a chat with Gemma3 and gave it a simple prompt: “You are a Dungeon Master in a game of D&amp;amp;D. I am rogue halfling and [son] is chaotic wizard. We have just arrived at a harbour and walked into town, please treat this as a Session 0 style game”&lt;/p&gt; &lt;p&gt;We have been playing for hours now and having a great time! I am going to make this much more structured but what fun this is!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/barrulus"&gt; /u/barrulus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwm4cv/dd_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwm4cv/dd_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kwm4cv/dd_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-27T12:34:55+00:00</published>
  </entry>
</feed>
