<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-07-07T21:35:08+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1lqqocr</id>
    <title>Ollama Local AI Journaling App.</title>
    <updated>2025-07-03T14:14:51+00:00</updated>
    <author>
      <name>/u/Frosty-Cap-4282</name>
      <uri>https://old.reddit.com/user/Frosty-Cap-4282</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This was born out of a personal need — I journal daily , and I didn’t want to upload my thoughts to some cloud server and also wanted to use AI. So I built Vinaya to be:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Private&lt;/strong&gt;: Everything stays on your device. No servers, no cloud, no trackers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simple&lt;/strong&gt;: Clean UI built with Electron + React. No bloat, just journaling.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Insightful&lt;/strong&gt;: Semantic search, mood tracking, and AI-assisted reflections (all offline).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Link to the app: &lt;a href="https://vinaya-journal.vercel.app/"&gt;https://vinaya-journal.vercel.app/&lt;/a&gt;&lt;br /&gt; Github: &lt;a href="https://github.com/BarsatKhadka/Vinaya-Journal"&gt;https://github.com/BarsatKhadka/Vinaya-Journal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m not trying to build a SaaS or chase growth metrics. I just wanted something I could trust and use daily. If this resonates with anyone else, I’d love feedback or thoughts.&lt;/p&gt; &lt;p&gt;If you like the idea or find it useful and want to encourage me to consistently refine it but don’t know me personally and feel shy to say it — just drop a ⭐ on GitHub. That’ll mean a lot :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Frosty-Cap-4282"&gt; /u/Frosty-Cap-4282 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqqocr/ollama_local_ai_journaling_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqqocr/ollama_local_ai_journaling_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqqocr/ollama_local_ai_journaling_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T14:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrd7lq</id>
    <title>Two local LLM 4 newbie</title>
    <updated>2025-07-04T08:01:57+00:00</updated>
    <author>
      <name>/u/Powerful-Shine8690</name>
      <uri>https://old.reddit.com/user/Powerful-Shine8690</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wish to initialize my notebook to support two local LLMs (&lt;strong&gt;running NOT at the same time&lt;/strong&gt;).&lt;/p&gt; &lt;p&gt;First'll do:&lt;/p&gt; &lt;p&gt;- Work &lt;strong&gt;only in local&lt;/strong&gt;, w/out Internet access, throught my .md files (write for &lt;a href="http://Obsidian.MD"&gt;Obsidian.MD&lt;/a&gt; platform), about 1K files, in Italian language, then suggest me internal link and indexing datas;&lt;/p&gt; &lt;p&gt;- Trasform scanned text (Jpg, Pic, Jpeg, Png, Pdf and ePub) into text MD files. Scanned texts are writen in Italian, Latin and Ancient Greek;&lt;/p&gt; &lt;p&gt;Second'll do:&lt;/p&gt; &lt;p&gt;- Work locally (but also &lt;strong&gt;online&lt;/strong&gt; if necessary) to help me in JavaScript, CSS, Powershell and Python programming with Microsoft Visual Studio Code.&lt;/p&gt; &lt;p&gt;Here is my configuration:&lt;/p&gt; &lt;p&gt;PC: - Acer Predator PH317-56&lt;/p&gt; &lt;p&gt;CPU: - 12th Gen Intel i7-12700H&lt;/p&gt; &lt;p&gt;RAM: - 2x16Gb Samsung DDR5 x4800 (@2400MHz) + 2 slot free&lt;/p&gt; &lt;p&gt;Graph: - NVIDIA GeForce RTX 3070 Ti Laptop GPU 8Gb GDDR6&lt;/p&gt; &lt;p&gt;2x SSD: - Crucial P3 4TB M.2 2280 PCIe 4.0 NVMe (Os + Progr)&lt;/p&gt; &lt;pre&gt;&lt;code&gt; \- WD Black WDS800T2XHE 8 TB M.2 2280 PCIe 4.0 NVMe (Doc) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Os: - Win 11 Pro updated&lt;/p&gt; &lt;p&gt;What you expert can suggest me? Tnx in advance&lt;/p&gt; &lt;p&gt;Emanuele&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Shine8690"&gt; /u/Powerful-Shine8690 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrd7lq/two_local_llm_4_newbie/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrd7lq/two_local_llm_4_newbie/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lrd7lq/two_local_llm_4_newbie/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T08:01:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lri3xg</id>
    <title>nous-hermes2-mixtral asking for ssh access</title>
    <updated>2025-07-04T12:57:26+00:00</updated>
    <author>
      <name>/u/YetToBeTold</name>
      <uri>https://old.reddit.com/user/YetToBeTold</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I am new to this local AI self hosting, and i installed nous-hermes2-mixtral because chatgpt said its good with engineering, anyways i wanted to try a few models till i find the one that suits me, but what happened was I asked the model if it can access a pdf file in a certain directory, and it replied that it needs authority to do so, and asked me to generate an ssh key with ssh-keygen and shared its public key with me so i add it in authorized_keys under ~/.ssh.&lt;/p&gt; &lt;p&gt;Is this normal or dangerous? &lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YetToBeTold"&gt; /u/YetToBeTold &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lri3xg/noushermes2mixtral_asking_for_ssh_access/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lri3xg/noushermes2mixtral_asking_for_ssh_access/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lri3xg/noushermes2mixtral_asking_for_ssh_access/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T12:57:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqpsjh</id>
    <title>Ollama based AI presentation generator and API - Gamma Alternative</title>
    <updated>2025-07-03T13:36:35+00:00</updated>
    <author>
      <name>/u/goodboydhrn</name>
      <uri>https://old.reddit.com/user/goodboydhrn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lqpsjh/ollama_based_ai_presentation_generator_and_api/"&gt; &lt;img alt="Ollama based AI presentation generator and API - Gamma Alternative" src="https://preview.redd.it/awcrxuqjwnaf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=aa54d3b167b836a81007137b327da2d5800fd272" title="Ollama based AI presentation generator and API - Gamma Alternative" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Me and my roommates are building Presenton, which is an AI presentation generator that can run entirely on your own device. It has Ollama built in so, all you need is add Pexels (free image provider) API Key and start generating high quality presentations which can be exported to PPTX and PDF. It even works on CPU(can generate professional presentation with as small as 3b models)!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Presentation Generation UI&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It has beautiful user-interface which can be used to create presentations.&lt;/li&gt; &lt;li&gt;7+ beautiful themes to choose from.&lt;/li&gt; &lt;li&gt;Can choose number of slides, languages and themes.&lt;/li&gt; &lt;li&gt;Can create presentation from PDF, PPTX, DOCX, etc files directly.&lt;/li&gt; &lt;li&gt;Export to PPTX, PDF.&lt;/li&gt; &lt;li&gt;Share presentation link.(if you host on public IP)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Presentation Generation over API&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can even host the instance to generation presentation over API. (1 endpoint for all above features)&lt;/li&gt; &lt;li&gt;All above features supported over API&lt;/li&gt; &lt;li&gt;You'll get two links; first the static presentation file (pptx/pdf) which you requested and editable link through which you can edit the presentation and export the file.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love for you to try it out! Very easy docker based setup and deployment.&lt;/p&gt; &lt;p&gt;Here's the github link: &lt;a href="https://github.com/presenton/presenton"&gt;https://github.com/presenton/presenton&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Also check out the docs here: &lt;a href="https://docs.presenton.ai/"&gt;https://docs.presenton.ai&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Feedbacks are very appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goodboydhrn"&gt; /u/goodboydhrn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/awcrxuqjwnaf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqpsjh/ollama_based_ai_presentation_generator_and_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqpsjh/ollama_based_ai_presentation_generator_and_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T13:36:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrquyz</id>
    <title>Built an offline AI chat app for macOS that works with local LLMs via Ollama</title>
    <updated>2025-07-04T19:11:35+00:00</updated>
    <author>
      <name>/u/Disastrous-Parsnip93</name>
      <uri>https://old.reddit.com/user/Disastrous-Parsnip93</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a lightweight macOS desktop chat application that runs entirely offline and communicates with local LLMs through Ollama. No internet required once set up!&lt;/p&gt; &lt;p&gt;Key features:&lt;/p&gt; &lt;p&gt;- 🧠 Local LLM integration via Ollama&lt;/p&gt; &lt;p&gt;- 💬 Clean, modern chat interface with real-time streaming&lt;/p&gt; &lt;p&gt;- 📝 Full markdown support with syntax highlighting&lt;/p&gt; &lt;p&gt;- 🕘 Persistent chat history&lt;/p&gt; &lt;p&gt;- 🔄 Easy model switching&lt;/p&gt; &lt;p&gt;- 🎨 Auto dark/light theme&lt;/p&gt; &lt;p&gt;- 📦 Under 20MB final app size&lt;/p&gt; &lt;p&gt;Built with Tauri, React, and Rust for optimal performance. The app automatically detects available Ollama models and provides a native macOS experience.&lt;/p&gt; &lt;p&gt;Perfect for anyone who wants to chat with AI models privately without sending data to external servers. Works great with llama3, codellama, and other Ollama models.&lt;/p&gt; &lt;p&gt;Available on GitHub with releases for macOS. Would love feedback from the community!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/abhijeetlokhande1996/local-chat-releases/releases/download/v0.1.0/Local.Chat_0.1.0_aarch64.dmg"&gt;https://github.com/abhijeetlokhande1996/local-chat-releases/releases/download/v0.1.0/Local.Chat_0.1.0_aarch64.dmg&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous-Parsnip93"&gt; /u/Disastrous-Parsnip93 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrquyz/built_an_offline_ai_chat_app_for_macos_that_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrquyz/built_an_offline_ai_chat_app_for_macos_that_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lrquyz/built_an_offline_ai_chat_app_for_macos_that_works/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T19:11:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrpjci</id>
    <title>Please... how can I set the reasoning effort😭😭</title>
    <updated>2025-07-04T18:14:56+00:00</updated>
    <author>
      <name>/u/Open-Flounder-7194</name>
      <uri>https://old.reddit.com/user/Open-Flounder-7194</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lrpjci/please_how_can_i_set_the_reasoning_effort/"&gt; &lt;img alt="Please... how can I set the reasoning effort😭😭" src="https://preview.redd.it/ckugsto8dwaf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9407005119d0bbe95bb2c6de4e1a2b491749d3a1" title="Please... how can I set the reasoning effort😭😭" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried setting it to &amp;quot;none&amp;quot; but it did not seem to work, does Deepseek R1 not support the reasoning effort API or is &amp;quot;none&amp;quot; not an accepted value and it defaulted to medium or something like high? If possible how could I include something like Thinkless to still get reasoning if I need it or at least a button at the prompt window to enable or disable rasoning?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Open-Flounder-7194"&gt; /u/Open-Flounder-7194 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ckugsto8dwaf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrpjci/please_how_can_i_set_the_reasoning_effort/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lrpjci/please_how_can_i_set_the_reasoning_effort/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T18:14:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrtj0m</id>
    <title>Use all your favorite MCP servers in your meetings</title>
    <updated>2025-07-04T21:12:19+00:00</updated>
    <author>
      <name>/u/Square-Test-515</name>
      <uri>https://old.reddit.com/user/Square-Test-515</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lrtj0m/use_all_your_favorite_mcp_servers_in_your_meetings/"&gt; &lt;img alt="Use all your favorite MCP servers in your meetings" src="https://external-preview.redd.it/ZHpxdml1OXJheGFmMf7so8CSE-8PmjQuJPM-OgOW72CEju6_3gCE3GVMC0Pl.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2489d6fca27e489a49dd4b43863047cc47c4ac67" title="Use all your favorite MCP servers in your meetings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;We've been working on an open-source project called joinly for the last two months. The idea is that you can connect your favourite MCP servers (e.g. Asana, Notion and Linear) to an AI agent and send that agent to any browser-based video conference. This essentially allows you to create your own custom meeting assistant that can perform tasks in real time during the meeting.&lt;/p&gt; &lt;p&gt;So, how does it work? Ultimately, joinly is also just a MCP server that you can host yourself, providing your agent with essential meeting tools (such as speak_text and send_chat_message) alongside automatic real-time transcription. By the way, we've designed it so that you can select your own LLM (e.g., Ollama), TTS and STT providers. &lt;/p&gt; &lt;p&gt;We made a quick video to show how it works connecting it to the Tavily and GitHub MCP servers and let joinly explain how joinly works. Because we think joinly best speaks for itself.&lt;/p&gt; &lt;p&gt;We'd love to hear your feedback or ideas on which other MCP servers you'd like to use in your meetings. Or just try it out yourself 👉 &lt;a href="https://github.com/joinly-ai/joinly"&gt;https://github.com/joinly-ai/joinly&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Square-Test-515"&gt; /u/Square-Test-515 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/p9inht9raxaf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrtj0m/use_all_your_favorite_mcp_servers_in_your_meetings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lrtj0m/use_all_your_favorite_mcp_servers_in_your_meetings/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T21:12:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsz44l</id>
    <title>Blessings to all, which of the Olma models are good for vibe coding locally?</title>
    <updated>2025-07-06T11:36:07+00:00</updated>
    <author>
      <name>/u/CalmAndLift</name>
      <uri>https://old.reddit.com/user/CalmAndLift</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm just starting to test the local text generation model in Ollam. I've also tried some that were created for Ollam a year ago, and they also had a version for Ollam for code generation. I'm still searching and I hope for your help to learn about the local code generation model. Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CalmAndLift"&gt; /u/CalmAndLift &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsz44l/blessings_to_all_which_of_the_olma_models_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsz44l/blessings_to_all_which_of_the_olma_models_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lsz44l/blessings_to_all_which_of_the_olma_models_are/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-06T11:36:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsp9py</id>
    <title>Is it possible to play real tabletop, board, and card games using local free ai's?</title>
    <updated>2025-07-06T01:19:32+00:00</updated>
    <author>
      <name>/u/Game-Lover44</name>
      <uri>https://old.reddit.com/user/Game-Lover44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have no real friends to play with. Is it possible to use ai to act as a teammate or opponent. I want to play games on a real table instead of digital would something like this be possible to do locally or is it too complex? how would i set something like this up? &lt;/p&gt; &lt;p&gt;&lt;em&gt;are there better things to do?&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Game-Lover44"&gt; /u/Game-Lover44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsp9py/is_it_possible_to_play_real_tabletop_board_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsp9py/is_it_possible_to_play_real_tabletop_board_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lsp9py/is_it_possible_to_play_real_tabletop_board_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-06T01:19:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls1bc8</id>
    <title>New feature "Expose Ollama to the network"</title>
    <updated>2025-07-05T04:18:56+00:00</updated>
    <author>
      <name>/u/yAmIDoingThisAtHome</name>
      <uri>https://old.reddit.com/user/yAmIDoingThisAtHome</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How to utilize this? How is it different from http://&amp;lt;ollama\_host&amp;gt;:11434 ?&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.9.5"&gt;https://github.com/ollama/ollama/releases/tag/v0.9.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yAmIDoingThisAtHome"&gt; /u/yAmIDoingThisAtHome &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ls1bc8/new_feature_expose_ollama_to_the_network/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ls1bc8/new_feature_expose_ollama_to_the_network/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ls1bc8/new_feature_expose_ollama_to_the_network/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-05T04:18:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsjf75</id>
    <title>Preferred frameworks when working with Ollama models?</title>
    <updated>2025-07-05T20:34:51+00:00</updated>
    <author>
      <name>/u/SeaworthinessLeft160</name>
      <uri>https://old.reddit.com/user/SeaworthinessLeft160</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'd like to know what you're using for your projects (personally or professionally) when working with models via Ollama (and if possible, how you handle prompt management or logging).&lt;/p&gt; &lt;p&gt;Personally, I’ve mostly just been using Ollama with Pydantic. I started exploring Instructor, but from what I can tell, I’m already doing pretty much the same thing just with Ollama and Pydantic, so I’m not sure I actually need Instructor. I’ve been thinking about trying out Langchain next, but honestly, I get a bit confused. I keep seeing OpenAI wrappers everywhere, and the standard setup I keep coming across is an OpenAI wrapper using the Ollama API underneath, usually combined with Langchain.&lt;/p&gt; &lt;p&gt;Thanks for any help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SeaworthinessLeft160"&gt; /u/SeaworthinessLeft160 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsjf75/preferred_frameworks_when_working_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsjf75/preferred_frameworks_when_working_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lsjf75/preferred_frameworks_when_working_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-05T20:34:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lso1ys</id>
    <title>Web search doesn’t return current results, using OpenWebUI with Ollama</title>
    <updated>2025-07-06T00:15:13+00:00</updated>
    <author>
      <name>/u/AliasJackBauer</name>
      <uri>https://old.reddit.com/user/AliasJackBauer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve just setup a Z440 workstation with a 3090 for LLM learning. I’ve got OpenWebUI with Ollama configured. I’ve been experimenting with gemma3 27b. I’m trying to get web search configured. I have it enabled in the configuration. I’ve tried both google pse and Searxng and it never returns current results when I do a query like “ what’s the weather for ‘some city’” even though it says it’s checking the web. Looking for what I can do to debug this a bit and figure out whey it’s not working.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AliasJackBauer"&gt; /u/AliasJackBauer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lso1ys/web_search_doesnt_return_current_results_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lso1ys/web_search_doesnt_return_current_results_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lso1ys/web_search_doesnt_return_current_results_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-06T00:15:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsbeyy</id>
    <title>How safe is to download models that are not official release</title>
    <updated>2025-07-05T14:40:56+00:00</updated>
    <author>
      <name>/u/ResponsibleTruck4717</name>
      <uri>https://old.reddit.com/user/ResponsibleTruck4717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know anyone can upload models how safe is to download it? are we expose to any risks like pickles file have?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleTruck4717"&gt; /u/ResponsibleTruck4717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsbeyy/how_safe_is_to_download_models_that_are_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsbeyy/how_safe_is_to_download_models_that_are_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lsbeyy/how_safe_is_to_download_models_that_are_not/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-05T14:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsql6h</id>
    <title>Ollama use A LOT of memory even after offloading model to GPU</title>
    <updated>2025-07-06T02:32:36+00:00</updated>
    <author>
      <name>/u/Only_Comfortable_224</name>
      <uri>https://old.reddit.com/user/Only_Comfortable_224</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My PC has Windows11 + 16GB RAM +16GB VRAM (AMD rx9070). When I run smaller models (e.g. qwen3 14B q4 quantization) on Ollama, even though I offload all the layers to GPU, it still uses almost all the memory (~15 out of 16GB) as shown in task manager. I can confirm the GPU is being used because the VRAM usage is almost all used. I don't have such issue when using LM studio, which only uses VRAM and leaves the system RAM free so I can comfortably run other applications. Any idea how to solve the problem for Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Only_Comfortable_224"&gt; /u/Only_Comfortable_224 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsql6h/ollama_use_a_lot_of_memory_even_after_offloading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsql6h/ollama_use_a_lot_of_memory_even_after_offloading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lsql6h/ollama_use_a_lot_of_memory_even_after_offloading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-06T02:32:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltfuqi</id>
    <title>Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler</title>
    <updated>2025-07-06T23:56:31+00:00</updated>
    <author>
      <name>/u/Fluid-Engineering769</name>
      <uri>https://old.reddit.com/user/Fluid-Engineering769</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ltfuqi/websitecrawler_extract_data_from_websites_in_llm/"&gt; &lt;img alt="Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler" src="https://external-preview.redd.it/8pG6rnRIoCry-dvcBOF-au9YmfpNVda4S3Exgl6tAS8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d809c1ecb009b988b05c33802e7cbb69bf85e8a8" title="Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluid-Engineering769"&gt; /u/Fluid-Engineering769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/pc8544/Website-Crawler"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltfuqi/websitecrawler_extract_data_from_websites_in_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ltfuqi/websitecrawler_extract_data_from_websites_in_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-06T23:56:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsyodw</id>
    <title>JSON response formatting</title>
    <updated>2025-07-06T11:08:51+00:00</updated>
    <author>
      <name>/u/barrulus</name>
      <uri>https://old.reddit.com/user/barrulus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all How do you get Ollama models to respond with structured JSON reliably?&lt;/p&gt; &lt;p&gt;It seems to me that I write my app to read the json response and then the. est response comes with malformat or a change in array location or whatever. &lt;/p&gt; &lt;p&gt;edit: I already provide the schema with every prompt. That was the first thing I tried. Very limited success. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/barrulus"&gt; /u/barrulus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsyodw/json_response_formatting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsyodw/json_response_formatting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lsyodw/json_response_formatting/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-06T11:08:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltq3cy</id>
    <title>HELP ME : Ollama is utilizing my CPU more than my GPU.</title>
    <updated>2025-07-07T09:52:36+00:00</updated>
    <author>
      <name>/u/HighlightPrudent554</name>
      <uri>https://old.reddit.com/user/HighlightPrudent554</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ltq3cy/help_me_ollama_is_utilizing_my_cpu_more_than_my/"&gt; &lt;img alt="HELP ME : Ollama is utilizing my CPU more than my GPU." src="https://b.thumbs.redditmedia.com/MZQc-XntQLCWzMAi8Idw1hsWHKajJBiXh8mqNjhmPeQ.jpg" title="HELP ME : Ollama is utilizing my CPU more than my GPU." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/g5096yniafbf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f6e5c74fdd262227623dede0b89b62a1e82eb7cc"&gt;https://preview.redd.it/g5096yniafbf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f6e5c74fdd262227623dede0b89b62a1e82eb7cc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My GPU is not being utilized as much as my CPU on the KDE Neon distribution I'm currently using. On my previous Ubuntu distribution, my GPU usage was around 90%, compared to my CPU. I'm not sure what went wrong. I added the following options to /etc/modprobe.d/nvidia-power-management.conf to address wake-up issues with the GPU not functioning after sleep:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Code options nvidia NVreg_PreserveVideoMemoryAllocations=1 options nvidia NVreg_TemporaryFilePath=/tmp &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Since then, Ollama has been using my GPU less than my CPU. I've been searching for answers for a week.&lt;/p&gt; &lt;p&gt;i am running llama3.1 8b model. i used same models on both distros.&lt;/p&gt; &lt;p&gt;help me guys.............&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HighlightPrudent554"&gt; /u/HighlightPrudent554 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltq3cy/help_me_ollama_is_utilizing_my_cpu_more_than_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltq3cy/help_me_ollama_is_utilizing_my_cpu_more_than_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ltq3cy/help_me_ollama_is_utilizing_my_cpu_more_than_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T09:52:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltoggx</id>
    <title>Open Web UI APIEndpoint with One Time Use FIle</title>
    <updated>2025-07-07T08:04:30+00:00</updated>
    <author>
      <name>/u/aeldexis</name>
      <uri>https://old.reddit.com/user/aeldexis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ltoggx/open_web_ui_apiendpoint_with_one_time_use_file/"&gt; &lt;img alt="Open Web UI APIEndpoint with One Time Use FIle" src="https://b.thumbs.redditmedia.com/P2OONlK2hcNqv00iarVpdg97XaKwYWTMQVGA4oqGtco.jpg" title="Open Web UI APIEndpoint with One Time Use FIle" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was reading the docs for open web ui's api endpoint to implement into my personal app and i dont quite understand it.&lt;/p&gt; &lt;p&gt;My goal is to upload a file (docx or pdf) and get a response in a json format.&lt;/p&gt; &lt;p&gt;But I have no idea how to handle the file.&lt;/p&gt; &lt;p&gt;Im able to get the completions api to work on postman but im not sure how to get the file upload to work.&lt;/p&gt; &lt;p&gt;Any examples I could follow?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qsbjbv6qlebf1.png?width=1252&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43e0296f92b0b4705b2d6b016d455c02483bea23"&gt;https://preview.redd.it/qsbjbv6qlebf1.png?width=1252&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43e0296f92b0b4705b2d6b016d455c02483bea23&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aeldexis"&gt; /u/aeldexis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltoggx/open_web_ui_apiendpoint_with_one_time_use_file/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltoggx/open_web_ui_apiendpoint_with_one_time_use_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ltoggx/open_web_ui_apiendpoint_with_one_time_use_file/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T08:04:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltp2ef</id>
    <title>Ollama force IGPu use</title>
    <updated>2025-07-07T08:45:47+00:00</updated>
    <author>
      <name>/u/MashiatILias</name>
      <uri>https://old.reddit.com/user/MashiatILias</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I'm new here in the Ollama and AI world. I can run AIs on my laptop well enough like the small ones from 2-less billion. But they all run on the CPU. I want it to run my on IGPU which is the Irisi XE-G4. But, how to do that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MashiatILias"&gt; /u/MashiatILias &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltp2ef/ollama_force_igpu_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltp2ef/ollama_force_igpu_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ltp2ef/ollama_force_igpu_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T08:45:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltxlsf</id>
    <title>LangChain/Crew/AutoGen made it easy to build agents, but operating them is a joke</title>
    <updated>2025-07-07T15:46:12+00:00</updated>
    <author>
      <name>/u/ImmuneCoder</name>
      <uri>https://old.reddit.com/user/ImmuneCoder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built an internal support agent using LangChain + OpenAI + some simple tool calls.&lt;/p&gt; &lt;p&gt;Getting to a working prototype took 3 days with Cursor and just messing around. Great.&lt;/p&gt; &lt;p&gt;But actually trying to operate that agent across multiple teams was absolute chaos.&lt;/p&gt; &lt;p&gt;– No structured logs of intermediate reasoning&lt;/p&gt; &lt;p&gt;– No persistent memory or traceability&lt;/p&gt; &lt;p&gt;– No access control (anyone could run/modify it)&lt;/p&gt; &lt;p&gt;– No ability to validate outputs at scale&lt;/p&gt; &lt;p&gt;It’s like deploying a microservice with no logs, no auth, and no monitoring. The frameworks are designed for demos, not real workflows. And everyone I know is duct-taping together JSON dumps + Slack logs to stay afloat.&lt;/p&gt; &lt;p&gt;So, what does agent infra actually look like after the first prototype for you guys?&lt;/p&gt; &lt;p&gt;Would love to hear real setups. Especially if you’ve gone past the LangChain happy path.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ImmuneCoder"&gt; /u/ImmuneCoder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltxlsf/langchaincrewautogen_made_it_easy_to_build_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltxlsf/langchaincrewautogen_made_it_easy_to_build_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ltxlsf/langchaincrewautogen_made_it_easy_to_build_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T15:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltzcks</id>
    <title>Looking for advice.</title>
    <updated>2025-07-07T16:51:54+00:00</updated>
    <author>
      <name>/u/Devve2kcccc</name>
      <uri>https://old.reddit.com/user/Devve2kcccc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm building a SaaS ERP for textile manufacturing and want to add an AI agent to &lt;strong&gt;analyze and compare transport/invoice documents&lt;/strong&gt;. In our process, clients send raw materials (e.g., T-shirts), we manufacture, and then send the finished goods back. Right now, someone manually compares multiple documents (transport guides, invoices, etc.) to verify if &lt;strong&gt;quantities, sizes, and products&lt;/strong&gt; match — and flag any inconsistencies.&lt;/p&gt; &lt;p&gt;I want to automate this with a service that can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ingest 1 or more related documents (PDFs, scans, etc.)&lt;/li&gt; &lt;li&gt;Parse and normalize the data (structured or unstructured)&lt;/li&gt; &lt;li&gt;Detect mismatches (quantities, prices, product references)&lt;/li&gt; &lt;li&gt;Generate a validation report or alert the company&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key challenge:&lt;/h1&gt; &lt;p&gt;The &lt;strong&gt;biggest problem&lt;/strong&gt; is that &lt;strong&gt;every company uses different software&lt;/strong&gt; and formats — so transport documents and invoices come in &lt;strong&gt;very different layouts and structures&lt;/strong&gt;. We need a dynamic and flexible system that can understand and extract key information &lt;strong&gt;regardless of the template&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;What I’m looking for:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Best practices&lt;/strong&gt; for parsing (OCR vs. structured PDF/XML, etc.)&lt;/li&gt; &lt;li&gt;Whether to use &lt;strong&gt;AI (LLMs?) or rule-based logic&lt;/strong&gt;, or both&lt;/li&gt; &lt;li&gt;Tools/libraries for &lt;strong&gt;document comparison &amp;amp; anomaly detection&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open-source / budget-friendly&lt;/strong&gt; options (we're a startup)&lt;/li&gt; &lt;li&gt;LLM models or services that work well for &lt;strong&gt;document understanding&lt;/strong&gt;, ideally something we can run locally or affordably scale&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you’ve built something similar — especially in &lt;strong&gt;logistics, finance, or manufacturing&lt;/strong&gt; — I’d love to hear what tools and strategies worked for you (and what to avoid).&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Devve2kcccc"&gt; /u/Devve2kcccc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltzcks/looking_for_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltzcks/looking_for_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ltzcks/looking_for_advice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T16:51:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltlgag</id>
    <title>OrangePi Zero 3 runs Ollama</title>
    <updated>2025-07-07T04:53:36+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those that are curious about running LLM on &lt;a href="https://en.wikipedia.org/wiki/Single-board_computer"&gt;SBC&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Here is &lt;a href="http://www.orangepi.org/html/hardWare/computerAndMicrocontrollers/details/Orange-Pi-Zero-3.html"&gt;Orange Pi Zero 3&lt;/a&gt; (aarch64) packed with 4gb DDR4 running Debian 12 'Bookworm'/ &lt;a href="https://dietpi.com/#home"&gt;DietPi&lt;/a&gt; using &lt;code&gt;ollama -v&lt;/code&gt; 0.9.5&lt;/p&gt; &lt;p&gt;I even used &lt;a href="https://ollama.com/library/llama3.2:1b"&gt;llama3.2:1b&lt;/a&gt; to create this markdown table:&lt;/p&gt; &lt;p&gt;*Eval Rate Tokens per Second is average of 3 runs.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;MODEL&lt;/th&gt; &lt;th align="left"&gt;SIZE GB&lt;/th&gt; &lt;th align="left"&gt;EVAL RATE TS/S&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3:1b&lt;/td&gt; &lt;td align="left"&gt;1.4&lt;/td&gt; &lt;td align="left"&gt;3.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama3.2:1b&lt;/td&gt; &lt;td align="left"&gt;2.2&lt;/td&gt; &lt;td align="left"&gt;3.16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2.5:1.5b-instruct-q5_K_M&lt;/td&gt; &lt;td align="left"&gt;1.7&lt;/td&gt; &lt;td align="left"&gt;2.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tinydolphin:1.1b-v2.8-q6_K&lt;/td&gt; &lt;td align="left"&gt;1.6&lt;/td&gt; &lt;td align="left"&gt;2.61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tinyllama:1.1b-chat-v1-q6_K&lt;/td&gt; &lt;td align="left"&gt;1.3&lt;/td&gt; &lt;td align="left"&gt;2.52&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Here is the &lt;code&gt;ollama run --verbose llama3.2:1b&lt;/code&gt; numbers from creating markdown table&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Value&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Total Duration&lt;/td&gt; &lt;td align="left"&gt;2m54.721763625s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Load Duration&lt;/td&gt; &lt;td align="left"&gt;41.594289562s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Prompt Eval Count&lt;/td&gt; &lt;td align="left"&gt;389 token(s)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Prompt Eval Duration&lt;/td&gt; &lt;td align="left"&gt;1m17.397468287s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Prompt Eval Rate&lt;/td&gt; &lt;td align="left"&gt;5.03 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Eval Count&lt;/td&gt; &lt;td align="left"&gt;163 token(s)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Eval Duration&lt;/td&gt; &lt;td align="left"&gt;55.571782235s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Eval Rate&lt;/td&gt; &lt;td align="left"&gt;2.93 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I was able to run &lt;a href="https://ollama.com/library/llama3.2:3b-instruct-q5_K_M"&gt;llama3.2:3b-instruct-q5_K_M&lt;/a&gt; and &lt;code&gt;ollama ps&lt;/code&gt; reported 4.0GB usage. Eval Rate dropped to 1.21 Tokens/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltlgag/orangepi_zero_3_runs_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltlgag/orangepi_zero_3_runs_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ltlgag/orangepi_zero_3_runs_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T04:53:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltvl0w</id>
    <title>(Kramer UI for Ollama) I was tired of dealing with Docker, so I built a simple, portable Windows UI for Ollama.</title>
    <updated>2025-07-07T14:27:15+00:00</updated>
    <author>
      <name>/u/DanielKramer_</name>
      <uri>https://old.reddit.com/user/DanielKramer_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ltvl0w/kramer_ui_for_ollama_i_was_tired_of_dealing_with/"&gt; &lt;img alt="(Kramer UI for Ollama) I was tired of dealing with Docker, so I built a simple, portable Windows UI for Ollama." src="https://external-preview.redd.it/9sqw7guDNkr_lh4DzQzAQ3_oGbJPe0qHLVbjofkhPuc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d5b553bbacb3aa769ebe7746d6025ee8190093ba" title="(Kramer UI for Ollama) I was tired of dealing with Docker, so I built a simple, portable Windows UI for Ollama." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share a small project I built for my own purposes: Kramer UI for Ollama.&lt;/p&gt; &lt;p&gt;I love Ollama for its simplicity and its model management, but setting up a UI for it has always been a pain point. I used to use OpenWebUI and it was great, but I'd rather not have to set up docker. And using Ollama through the CLI makes me feel like a simpleton because I can't even edit my messages.&lt;/p&gt; &lt;p&gt;I wanted a UI as simple as Ollama to accompany it. So I built it. Kramer UI is a single, portable executable file for Windows. There's no installer. You just run the .exe and you're ready to start chatting.&lt;/p&gt; &lt;p&gt;My goal was to make interacting with your local models as frictionless as possible.&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Uses 45mb of ram&lt;/li&gt; &lt;li&gt;Edit your messages&lt;/li&gt; &lt;li&gt;Models' thoughts are hidden behind dropdown&lt;/li&gt; &lt;li&gt;Model selector&lt;/li&gt; &lt;li&gt;Currently no support for conversation history&lt;/li&gt; &lt;li&gt;You can probably compile it for Linux and Mac too&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can download the executable directly from the GitHub releases page [here.] (&lt;a href="https://github.com/dvkramer/kramer-ui/releases/"&gt;https://github.com/dvkramer/kramer-ui/releases/&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ltf7soogrgbf1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dfccfc5d106dd35e413bc7f65b35b758d6d05703"&gt;https://preview.redd.it/ltf7soogrgbf1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dfccfc5d106dd35e413bc7f65b35b758d6d05703&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All feedback, suggestions, and ideas are welcome! Let me know what you think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanielKramer_"&gt; /u/DanielKramer_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltvl0w/kramer_ui_for_ollama_i_was_tired_of_dealing_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltvl0w/kramer_ui_for_ollama_i_was_tired_of_dealing_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ltvl0w/kramer_ui_for_ollama_i_was_tired_of_dealing_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T14:27:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltqs3f</id>
    <title>should i replace gemma 3?</title>
    <updated>2025-07-07T10:34:52+00:00</updated>
    <author>
      <name>/u/Otherwise-Brick4923</name>
      <uri>https://old.reddit.com/user/Otherwise-Brick4923</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hi everyone,&lt;/strong&gt;&lt;br /&gt; I'm trying to create a workflow that can check a client's order against the supplier's order confirmation for any discrepancies. Everything is working quite well so far, but when I started testing the system by intentionally introducing errors, Gemma simply ignored them.&lt;/p&gt; &lt;p&gt;For example:&lt;br /&gt; The client's name is &lt;strong&gt;Lius&lt;/strong&gt;, but I entered &lt;strong&gt;Dius&lt;/strong&gt;, and Gemma marked it as correct.&lt;/p&gt; &lt;p&gt;Now I'm considering switching to the new &lt;strong&gt;Gemma 3n&lt;/strong&gt;, hoping it might perform better.&lt;/p&gt; &lt;p&gt;Has anyone experienced something similar or have an idea why Gemma isn't recognizing these errors?&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Otherwise-Brick4923"&gt; /u/Otherwise-Brick4923 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltqs3f/should_i_replace_gemma_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltqs3f/should_i_replace_gemma_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ltqs3f/should_i_replace_gemma_3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T10:34:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lttm5g</id>
    <title>Want to create a private LLM for ingesting engineering handbooks &amp; IP.</title>
    <updated>2025-07-07T13:03:07+00:00</updated>
    <author>
      <name>/u/The_ZMD</name>
      <uri>https://old.reddit.com/user/The_ZMD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to create a ollama-private gpt on my pc. This will be primarily used to ingest couple of engineering handbook so that it can understand some technical stuff. Some of my research papers, subjects/books I read for education, so it knows what I know and what I don't know.&lt;/p&gt; &lt;p&gt;Additonally I need it to compare multiple vendor data, give me best option do some basic analysis, generate report, etc. Do I need to start from scratch or something similar exists? Like a pre trained neural netrowk (like Physics inspired neural network)?&lt;/p&gt; &lt;p&gt;PC specs: 10850k, 32 gb ram, 6900xt, multiple gen 4 ssd and hdd.&lt;/p&gt; &lt;p&gt;Any help is appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The_ZMD"&gt; /u/The_ZMD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lttm5g/want_to_create_a_private_llm_for_ingesting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lttm5g/want_to_create_a_private_llm_for_ingesting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lttm5g/want_to_create_a_private_llm_for_ingesting/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T13:03:07+00:00</published>
  </entry>
</feed>
