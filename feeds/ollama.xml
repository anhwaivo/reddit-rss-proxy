<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-04-06T13:08:48+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jqajhl</id>
    <title>DocuMind (RAG app using Ollama)</title>
    <updated>2025-04-03T05:42:50+00:00</updated>
    <author>
      <name>/u/harry0027</name>
      <uri>https://old.reddit.com/user/harry0027</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jqajhl/documind_rag_app_using_ollama/"&gt; &lt;img alt="DocuMind (RAG app using Ollama)" src="https://external-preview.redd.it/dh-_Wu3nlQIF6oJL5CtgqrVT_37A3bEVuNY-wVXwauM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75ddf988c05b0a6a647e7b1644b7a61cebb09aaf" title="DocuMind (RAG app using Ollama)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m excited to share &lt;a href="https://github.com/Harry-027/DocuMind"&gt;DocuMind&lt;/a&gt;, a RAG (Retrieval-Augmented Generation) desktop app I built to make document management smarter and more efficient. It uses Ollama at backend to connect with LLMs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Harry-027/DocuMind"&gt;Github: DocuMind&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With DocuMind, you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🔎 Quickly search and retrieve relevant information from large pdf files.&lt;/li&gt; &lt;li&gt;🔄 Generate insightful answers using AI based on the context.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Building this app was an incredible experience, and it deepened my understanding of retrieval-augmented generation and AI-powered solutions.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1jqajhl/video/iqv2xswxhkse1/player"&gt;Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;#AI #RAG #Ollama #Rust #Tauri #Axum #QdrantDB&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/harry0027"&gt; /u/harry0027 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqajhl/documind_rag_app_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqajhl/documind_rag_app_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jqajhl/documind_rag_app_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T05:42:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr1kjb</id>
    <title>Docker with Ollama Tool Calling</title>
    <updated>2025-04-04T02:45:54+00:00</updated>
    <author>
      <name>/u/vvbalboa98</name>
      <uri>https://old.reddit.com/user/vvbalboa98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For context, I am trying to build an application with its own UI, and other facilities, with the chatbot being just a small part of it.&lt;/p&gt; &lt;p&gt;I have been successfully locally running Llama3.2 with tool-calling using my own functions to query my own data for my specific use case. This has been good, if not quite slow. But I'm sure once i get a better computer/GPU it will much quicker. I have written the chatbot using python and i am exposing it as a FastAPI endpoint that my UI can call. It works well locally and I love the tool calling functionality&lt;/p&gt; &lt;p&gt;However, i need to dockerize this whole setup, with the UI, chatbot and other features of the app as different services and using a named volume to share data between the different part of the app and any data/models/things that need to be persisted to prevent downloading during every start. But I am unsure of how to go about the setup. All the tutorials I have seen online for docker with ollama seem to use the official ollama image and are using the models directly. If I do this, my tool calling functionality is gone, which will be my main purpose of doing this whole thing.&lt;/p&gt; &lt;p&gt;These are the things I need for my chatbot service container:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Ollama (the equivalent of the setup.exe)&lt;/li&gt; &lt;li&gt;the Llama3.2 model&lt;/li&gt; &lt;li&gt;the python script with the tool calling functionality.&lt;/li&gt; &lt;li&gt;exposing this whole thing as an endpoint with FastAPI.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;part 3 and 4 I have done, but when i call the endpoint, the part of the script where it is actually calling the LLM (response = ollama.chat(..)) is failing because it is not finding the model.&lt;/p&gt; &lt;p&gt;Has anyone faced this issue before? Any suggestions will help because I am out of my wits rn&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vvbalboa98"&gt; /u/vvbalboa98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jr1kjb/docker_with_ollama_tool_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jr1kjb/docker_with_ollama_tool_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jr1kjb/docker_with_ollama_tool_calling/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-04T02:45:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrlse7</id>
    <title>I vibe-coded a fun open source totally local social media app, where you interact with AI personas.</title>
    <updated>2025-04-04T20:26:49+00:00</updated>
    <author>
      <name>/u/mintybadgerme</name>
      <uri>https://old.reddit.com/user/mintybadgerme</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jrlse7/i_vibecoded_a_fun_open_source_totally_local/"&gt; &lt;img alt="I vibe-coded a fun open source totally local social media app, where you interact with AI personas." src="https://external-preview.redd.it/yRMmy9rJdU6oEgkUE9UOoanY2i_uGmcxsgYwNsy1G4Y.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2926c58ead9947a5bbc987b1496791620d0f4f05" title="I vibe-coded a fun open source totally local social media app, where you interact with AI personas." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mintybadgerme"&gt; /u/mintybadgerme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/nigelp/mycrowd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jrlse7/i_vibecoded_a_fun_open_source_totally_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jrlse7/i_vibecoded_a_fun_open_source_totally_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-04T20:26:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr0fzl</id>
    <title>4x AMD Instinct Mi210 QwQ-32B-FP16 - Effortless</title>
    <updated>2025-04-04T01:47:53+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gzc1k18v3qse1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jr0fzl/4x_amd_instinct_mi210_qwq32bfp16_effortless/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jr0fzl/4x_amd_instinct_mi210_qwq32bfp16_effortless/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-04T01:47:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr0hzj</id>
    <title>Question on OLLAMA_KV_CACHE_TYPE</title>
    <updated>2025-04-04T01:50:36+00:00</updated>
    <author>
      <name>/u/dookie168</name>
      <uri>https://old.reddit.com/user/dookie168</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I run a quantized model e.g. hf.co/bartowski/google_gemma-3-27b-it-GGUF:Q4_K_M&lt;/p&gt; &lt;p&gt;And I also have OLLAMA_KV_CACHE_TYPE set to q4_0. Does that mean the model is being quantized twice? How does that affect inference accuracy?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dookie168"&gt; /u/dookie168 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jr0hzj/question_on_ollama_kv_cache_type/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jr0hzj/question_on_ollama_kv_cache_type/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jr0hzj/question_on_ollama_kv_cache_type/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-04T01:50:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqps8y</id>
    <title>Build local AI Agents and RAGs over your docs/sites in minutes now.</title>
    <updated>2025-04-03T18:14:46+00:00</updated>
    <author>
      <name>/u/DonTizi</name>
      <uri>https://old.reddit.com/user/DonTizi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Following up on Rlama – many of you were interested in how quickly you can get a local RAG system running. The key now is the new &lt;strong&gt;Rlama Playground&lt;/strong&gt;, our web UI designed to take the guesswork out of configuration.&lt;/p&gt; &lt;p&gt;Building RAG systems often involves juggling models, data sources, chunking parameters, reranking settings, and more. It can get complex fast! The Playground simplifies this dramatically.&lt;/p&gt; &lt;p&gt;The Playground acts as a user-friendly interface to visually configure your entire Rlama RAG setup before you even touch the terminal.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here's how you build an AI solution in minutes using it:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Select Your Model:&lt;/strong&gt; Choose any model available via &lt;strong&gt;Ollama&lt;/strong&gt; (like llama3, gemma3, mistral) or &lt;strong&gt;Hugging Face&lt;/strong&gt; directly in the UI.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Choose Your Data Source:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local Folder:&lt;/strong&gt; Just provide the path to your documents (./my_project_docs).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Website:&lt;/strong&gt; Enter the URL (&lt;a href="https://rlama.dev"&gt;https://rlama.dev&lt;/a&gt;), set crawl depth, concurrency, and even specify paths to exclude (/blog, /archive). You can also leverage sitemaps.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;(Optional) Fine-Tune Settings:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Chunking:&lt;/strong&gt; While we offer sensible defaults (Hybrid or Auto), you can easily select different strategies (Semantic, Fixed, Hierarchical), adjust chunk size, and overlap if needed. Tooltips guide you.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reranking:&lt;/strong&gt; Enable/disable reranking (improves relevance), set a score threshold, or even specify a different reranker model – all visually.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generate Command:&lt;/strong&gt; This is the magic button! Based on all your visual selections, the Playground instantly generates the precise rlama CLI command needed to build this exact RAG system.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Copy &amp;amp; Run:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Click &amp;quot;Copy&amp;quot;.&lt;/li&gt; &lt;li&gt;Paste the generated command into your terminal.&lt;/li&gt; &lt;li&gt;Hit Enter. Rlama processes your data and builds the vector index.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Query Your Data:&lt;/strong&gt; Once complete (usually seconds to a couple of minutes depending on data size), run rlama run my_website_rag and start asking questions!&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;That's it!&lt;/strong&gt; The Playground turns potentially complex configuration into a simple point-and-click process, generating the exact command so you can launch your tailored, local AI solution in minutes. No need to memorize flags or manually craft long commands.&lt;/p&gt; &lt;p&gt;It abstracts the complexity while still giving you granular control if you want it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try the Playground yourself:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Playground/Website:&lt;/strong&gt; &lt;a href="https://rlama.dev/"&gt;https://rlama.dev/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/dontizi/rlama"&gt;https://github.com/dontizi/rlama&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me know if you have any questions about using the Playground!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DonTizi"&gt; /u/DonTizi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqps8y/build_local_ai_agents_and_rags_over_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqps8y/build_local_ai_agents_and_rags_over_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jqps8y/build_local_ai_agents_and_rags_over_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T18:14:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr3xgm</id>
    <title>Arch-Function-Chat (1B/3B/7B) - Device friendly, family of fast LLMs for function calling scenarios now trained to chat.</title>
    <updated>2025-04-04T04:57:26+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on feedback from users and the developer community that used Arch-Function (our previous gen) model, I am excited to share our latest work: &lt;a href="https://huggingface.co/katanemo/Arch-Function-Chat-3B"&gt;Arch-Function-Chat&lt;/a&gt; A collection of fast, device friendly LLMs that achieve performance on-par with GPT-4 on function calling, now trained to chat.&lt;/p&gt; &lt;p&gt;These LLMs have three additional training objectives.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Be able to refine and clarify the user request. This means to ask for required function parameters, clarify ambiguous input (e.g., &amp;quot;Transfer $500&amp;quot; without specifying accounts, can be “Transfer from” and “Transfer to”)&lt;/li&gt; &lt;li&gt;Accurately maintain context in two specific scenarios: &lt;ol&gt; &lt;li&gt;Progressive information disclosure such as in multi-turn conversations where information is revealed gradually (i.e., the model asks info of multiple parameters and the user only answers one or two instead of all the info)&lt;/li&gt; &lt;li&gt;Context switch where the model must infer missing parameters from context (e.g., &amp;quot;Check the weather&amp;quot; should prompt for location if not provided) and maintains context between turns (e.g., &amp;quot;What about tomorrow?&amp;quot; after a weather query but still in the middle of clarification)&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Respond to the user based on executed tools results. For common function calling scenarios where the response of the execution is all that's needed to complete the user request, Arch-Function-Chat can interpret and respond to the user via chat. Note, parallel and multiple function calling was already supported so if the model needs to respond based on multiple tools call it still can.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Of course the 3B model will now be the primary LLM used in &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;. Hope you all like the work 🙏. Happy building!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jr3xgm/archfunctionchat_1b3b7b_device_friendly_family_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jr3xgm/archfunctionchat_1b3b7b_device_friendly_family_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jr3xgm/archfunctionchat_1b3b7b_device_friendly_family_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-04T04:57:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr4a4p</id>
    <title>When will we get Qwen 2.5 Omni - the most multi modal available in ollama ?</title>
    <updated>2025-04-04T05:18:57+00:00</updated>
    <author>
      <name>/u/AxelBlaze20850</name>
      <uri>https://old.reddit.com/user/AxelBlaze20850</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AxelBlaze20850"&gt; /u/AxelBlaze20850 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jr4a4p/when_will_we_get_qwen_25_omni_the_most_multi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jr4a4p/when_will_we_get_qwen_25_omni_the_most_multi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jr4a4p/when_will_we_get_qwen_25_omni_the_most_multi/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-04T05:18:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqzbvf</id>
    <title>I Created A Lightweight Voice Assistant for Ollama with Real-Time Interaction</title>
    <updated>2025-04-04T00:50:48+00:00</updated>
    <author>
      <name>/u/typhoon90</name>
      <uri>https://old.reddit.com/user/typhoon90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I just built OllamaGTTS, a lightweight voice assistant that brings AI-powered voice interactions to your local Ollama setup using Google TTS for natural speech synthesis. It’s fast, interruptible, and optimized for real-time conversations. I am aware that some people prefer to keep everything local so I am working on an update that will likely use Kokoro for local speech synthesis. I would love to hear your thoughts on it and how it can be improved.&lt;/p&gt; &lt;p&gt;Key Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time voice interaction (Silero VAD + Whisper transcription)&lt;/li&gt; &lt;li&gt;Interruptible speech playback (no more waiting for the AI to finish talking)&lt;/li&gt; &lt;li&gt;FFmpeg-accelerated audio processing (optional speed-up for faster * replies)&lt;/li&gt; &lt;li&gt;Persistent conversation history with configurable memory&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/ExoFi-Labs/OllamaGTTS"&gt;GitHub Repo: https://github.com/ExoFi-Labs/OllamaGTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Instructions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Clone Repo&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Install requirements&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Run ollama_gtts.py&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/typhoon90"&gt; /u/typhoon90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqzbvf/i_created_a_lightweight_voice_assistant_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqzbvf/i_created_a_lightweight_voice_assistant_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jqzbvf/i_created_a_lightweight_voice_assistant_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-04T00:50:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrndbt</id>
    <title>2 questions: Time to process tokens and OpenAI</title>
    <updated>2025-04-04T21:35:07+00:00</updated>
    <author>
      <name>/u/ChikyScaresYou</name>
      <uri>https://old.reddit.com/user/ChikyScaresYou</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First&lt;/p&gt; &lt;p&gt;I'm using Chronos_Hermes through ollama to analyze text, and yesterday i tested it with a chunk (arouns 1400 tokens) and took me almost 20 minutes to complete. For comparison, Mistral:7b took like 3 mins to do the same. Anyone has an idea of why could it be so slow?&lt;/p&gt; &lt;p&gt;Second&lt;/p&gt; &lt;p&gt;I heard that OpenAI released a free version of the lastest model to general use when it also released the thing that plagarizes Studio Ghibli's art. Is that true? Is the model accessible through ollama?&lt;/p&gt; &lt;p&gt;thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChikyScaresYou"&gt; /u/ChikyScaresYou &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jrndbt/2_questions_time_to_process_tokens_and_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jrndbt/2_questions_time_to_process_tokens_and_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jrndbt/2_questions_time_to_process_tokens_and_openai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-04T21:35:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1js02qr</id>
    <title>Welcome to Infinite Oracle, a mystical Ollama client that channels boundless wisdom through an ethereal voice!</title>
    <updated>2025-04-05T10:01:15+00:00</updated>
    <author>
      <name>/u/WappyFlanker</name>
      <uri>https://old.reddit.com/user/WappyFlanker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1js02qr/welcome_to_infinite_oracle_a_mystical_ollama/"&gt; &lt;img alt="Welcome to Infinite Oracle, a mystical Ollama client that channels boundless wisdom through an ethereal voice!" src="https://external-preview.redd.it/K0kmjllRfd70adzKLvdhVOoCrgtDrXoi8JgMx0gxevc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc7b9ada9bdeb73795176a66b24f04c59448a616" title="Welcome to Infinite Oracle, a mystical Ollama client that channels boundless wisdom through an ethereal voice!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Greetings, Welcome to Infinite Oracle, a mystical application that channels boundless wisdom through an ethereal voice. This executable brings you cryptic, uplifting insights powered by Ollama, Coqui TTS and whisper-asr-webservice servers running locally!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WappyFlanker"&gt; /u/WappyFlanker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/aat440hz/Infinite_Oracle"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js02qr/welcome_to_infinite_oracle_a_mystical_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1js02qr/welcome_to_infinite_oracle_a_mystical_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T10:01:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1js1b0v</id>
    <title>Somehow Ollama has stopped using my GPU and I don't know why</title>
    <updated>2025-04-05T11:25:41+00:00</updated>
    <author>
      <name>/u/PFGSnoopy</name>
      <uri>https://old.reddit.com/user/PFGSnoopy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title said, Ollama doesn't utilize the GPU anymore and I have no idea why. I haven't changed anything. &lt;/p&gt; &lt;p&gt;My Ollama is running in a VM (Ubuntu 24.10) on a Proxmox Ve 8.3.5 with GPU pass-through (not as a vGPU). &lt;/p&gt; &lt;p&gt;I want to understand how this could happen and what I can do to prevent this from happening again (provided I can fix it in the first place).&lt;/p&gt; &lt;p&gt;Edit: to provide some more context. lspci inside the VM shows that the GPU (NVIDIA RTX2000 Ada Generation) is being recognised. So I would guess, it's not a case of broken GPU pass-through. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PFGSnoopy"&gt; /u/PFGSnoopy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js1b0v/somehow_ollama_has_stopped_using_my_gpu_and_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js1b0v/somehow_ollama_has_stopped_using_my_gpu_and_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1js1b0v/somehow_ollama_has_stopped_using_my_gpu_and_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T11:25:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1js4ac5</id>
    <title>Can't set value for n_seq_max in ModelFile</title>
    <updated>2025-04-05T14:09:33+00:00</updated>
    <author>
      <name>/u/rsk_039</name>
      <uri>https://old.reddit.com/user/rsk_039</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys. I am new to editing ModelFiles in Ollama. I tried setting the value for n_seq_max to 1 (so that I can use full context window i believe), but ollama is giving me error &amp;quot;Couldn't set parameter: unknown parameter n_seq_max&amp;quot;. I tried with num_seq_max also, but same error returned. Any help with is greatly appreciated. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rsk_039"&gt; /u/rsk_039 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js4ac5/cant_set_value_for_n_seq_max_in_modelfile/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js4ac5/cant_set_value_for_n_seq_max_in_modelfile/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1js4ac5/cant_set_value_for_n_seq_max_in_modelfile/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T14:09:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jryl86</id>
    <title>MCP Servers using any LLM API and Local LLMs</title>
    <updated>2025-04-05T08:08:22+00:00</updated>
    <author>
      <name>/u/mehul_gupta1997</name>
      <uri>https://old.reddit.com/user/mehul_gupta1997</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jryl86/mcp_servers_using_any_llm_api_and_local_llms/"&gt; &lt;img alt="MCP Servers using any LLM API and Local LLMs" src="https://external-preview.redd.it/iiuYjTt5hi5en0DFmJ4Zb8-DLc00qDJtpewc3Kd7HY0.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=242eb6be14416a950f6bfe79e9deb75df1dcd369" title="MCP Servers using any LLM API and Local LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehul_gupta1997"&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/9Mml4ULLoF0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jryl86/mcp_servers_using_any_llm_api_and_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jryl86/mcp_servers_using_any_llm_api_and_local_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T08:08:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1js4hds</id>
    <title>mcp_use lets you use MCPs with ollama LLMs</title>
    <updated>2025-04-05T14:19:10+00:00</updated>
    <author>
      <name>/u/Guilty-Effect-3771</name>
      <uri>https://old.reddit.com/user/Guilty-Effect-3771</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1js4hds/mcp_use_lets_you_use_mcps_with_ollama_llms/"&gt; &lt;img alt="mcp_use lets you use MCPs with ollama LLMs" src="https://external-preview.redd.it/0vfnV86EuiE-hK1Fi2yCNTpJyrIMbgnk_6FwE-6cfX0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f734f0f1a719b322fb7d76f47c2c6230dc892b44" title="mcp_use lets you use MCPs with ollama LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey lamas! &lt;/p&gt; &lt;p&gt;I do not have a lot of experience with Ollama but many people seemed interested in using MCPs from ollama models. I am not sure what your current flow is, but I think &lt;strong&gt;mcp-use&lt;/strong&gt; can be of help and some ollama users already are reaching out because it was useful to them!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;mcp-use&lt;/strong&gt; is a Python package that simplifies working with MCP. Born out of frustration with the desktop-app-only limitations of existing MCP tools, it provides a clean abstraction over the mcp connection management and server communication. It works with any langchain supported models that also support tool calling. &lt;/p&gt; &lt;p&gt;It is super easy to get started, you need:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;one of those MCPconfig JSONs - find many here: &lt;a href="https://github.com/punkpeye/awesome-mcp-servers"&gt;https://github.com/punkpeye/awesome-mcp-servers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;6 lines of code and you can have an agent use the MCP tools from python.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Like this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zhbxz4t3z0te1.png?width=1956&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fd7e284ec9f87945d43d336727ce3cf26df5136d"&gt;https://preview.redd.it/zhbxz4t3z0te1.png?width=1956&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fd7e284ec9f87945d43d336727ce3cf26df5136d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The structure is simple: an MCP client creates and manages the connection and instantiation (if needed) of the server and extracts the available tools. The MCPAgent reads the tools from the client, converts them into callable objects, gives access to them to an LLM, manages tool calls and responses.&lt;/p&gt; &lt;p&gt;It's very early-stage, and I'm sharing it here for feedback and contributions. If you're playing with MCP or building agents around it, I hope this makes your life easier.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/pietrozullo/mcp-use"&gt;https://github.com/pietrozullo/mcp-use&lt;/a&gt; Pipy: &lt;a href="https://pypi.org/project/mcp-use/"&gt;https://pypi.org/project/mcp-use/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs: &lt;a href="https://docs.mcp-use.io/introduction"&gt;https://docs.mcp-use.io/introduction&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install mcp-use &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Happy to answer questions or walk through examples!&lt;/p&gt; &lt;p&gt;Props: Name is clearly inspired by &lt;a href="https://browser-use.com/"&gt;browser_use&lt;/a&gt; an insane project by a friend of mine, following him closely I think I got brainwashed into naming everything mcp related _use.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Guilty-Effect-3771"&gt; /u/Guilty-Effect-3771 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js4hds/mcp_use_lets_you_use_mcps_with_ollama_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js4hds/mcp_use_lets_you_use_mcps_with_ollama_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1js4hds/mcp_use_lets_you_use_mcps_with_ollama_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T14:19:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1js78cr</id>
    <title>Model for Game Tips and Guide Bot</title>
    <updated>2025-04-05T16:24:23+00:00</updated>
    <author>
      <name>/u/PassionLuck</name>
      <uri>https://old.reddit.com/user/PassionLuck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to train an AI on the game Dark and Darker. The end goal is to be able to ask the AI tips on what gear to wear, skills, and perks with damage calculation. I have all of the math formulas for damage calculations.&lt;/p&gt; &lt;p&gt;Which model should I use for this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PassionLuck"&gt; /u/PassionLuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js78cr/model_for_game_tips_and_guide_bot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js78cr/model_for_game_tips_and_guide_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1js78cr/model_for_game_tips_and_guide_bot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T16:24:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsg26x</id>
    <title>What local LLM to choose</title>
    <updated>2025-04-05T22:58:47+00:00</updated>
    <author>
      <name>/u/rhawon</name>
      <uri>https://old.reddit.com/user/rhawon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I know this sounds like a noob question, but I'm a developer who wants to get familiarized with local LLMs. As a side project, I've been developing a mobile app and a backend for it, and this app needs a relatively smart LLM running together. Currently I use phi 3.5 (via ollama that runs on docker) but that's only for testing. phi is also on docker.&lt;/p&gt; &lt;p&gt;The PC spec:&lt;/p&gt; &lt;p&gt;- GPU: 2070 Super&lt;/p&gt; &lt;p&gt;- CPU: i5 8600k&lt;/p&gt; &lt;p&gt;- RAM: corsair 16gig ddr4 3000mhz cl15&lt;/p&gt; &lt;p&gt;What would be the smartest for this poor PC to run, and for me to get better results? Cannot say I'm very happy with phi thus far.&lt;/p&gt; &lt;p&gt;PS:&lt;br /&gt; Sorry, first time posting here, if I messed up some rules, happy to fix.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rhawon"&gt; /u/rhawon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsg26x/what_local_llm_to_choose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsg26x/what_local_llm_to_choose/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsg26x/what_local_llm_to_choose/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T22:58:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1js750i</id>
    <title>I built an AI Orchestrator that routes between local and cloud models based on real-time signals like battery, latency, and data sensitivity — and it's fully pluggable.</title>
    <updated>2025-04-05T16:20:15+00:00</updated>
    <author>
      <name>/u/Emotional-Evening-62</name>
      <uri>https://old.reddit.com/user/Emotional-Evening-62</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Been tinkering on this for a while — it’s a runtime orchestration layer that lets you:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Run AI models either on-device or in the cloud&lt;/li&gt; &lt;li&gt;Dynamically choose the best execution path (based on network, compute)&lt;/li&gt; &lt;li&gt;Plug in your own models (LLMs, vision, audio, whatever)&lt;/li&gt; &lt;li&gt;Built-in logging and fallback routing&lt;/li&gt; &lt;li&gt;Works with ONNX, TorchScript, and HTTP APIs (more coming)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Goal was to stop hardcoding execution logic and instead treat model routing like a smart decision system. Think traffic controller for AI workloads.&lt;/p&gt; &lt;p&gt;pip install oblix (mac only)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emotional-Evening-62"&gt; /u/Emotional-Evening-62 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js750i/i_built_an_ai_orchestrator_that_routes_between/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js750i/i_built_an_ai_orchestrator_that_routes_between/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1js750i/i_built_an_ai_orchestrator_that_routes_between/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T16:20:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsqmmo</id>
    <title>Is it possible to make Ollama pretend to be ChatGPT?</title>
    <updated>2025-04-06T09:52:25+00:00</updated>
    <author>
      <name>/u/nahakubuilder</name>
      <uri>https://old.reddit.com/user/nahakubuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was thinking if there is possibility to reroute ChatGPT connections to Ollama.&lt;br /&gt; I have docker Ollama container, I have added Nginx to respond on `api.openai.com` + change my local DNS to point to it.&lt;br /&gt; I am coming to 2 issues.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;even with self signed certificate and added to linux the client is reporting it has invalid certificate. I think it is because of HTST, is it possible to make it to accept my self signed certificate for this public domain when is pointed locally?&lt;/li&gt; &lt;li&gt;I believe the API urls have different paths then ollama for openai. would be possible to change the paths, queries so it acts as openai? - with this one also I think is needed to mask the chatgpt models to some model what ollama supports too.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I am not sure if there is anything similar in work anywhere, as I Could not find it.&lt;/p&gt; &lt;p&gt;It would be nice if applications what force you to use public AI, would be possible to point to selfhosted ollama.&lt;/p&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;p&gt;For everyone responding. I am not looking for another GUI for ollama, I use Tabby.&lt;br /&gt; All I am looking for is to make Ollama ( Self hosted AI) to respond to queries what are meant for OpenAI.&lt;br /&gt; Reason for this is that many applications support only OpenAI, for example Bootstrap Studio.&lt;br /&gt; but if i can obfuscate ollama to act as open AI, all I need to make sure the &lt;a href="http://api.openai.com"&gt;api.openai.com&lt;/a&gt; is translated to Ollama instead of the real paid API.&lt;br /&gt; About cert, I already added the certificate to my PC and it still does not work.&lt;br /&gt; The calls are not in web browser but in apps, so certificated stored in local PC should be accepted.&lt;br /&gt; But as I Stated, the app complains about HSTS or something like that, or just says certificate invalid.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nahakubuilder"&gt; /u/nahakubuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsqmmo/is_it_possible_to_make_ollama_pretend_to_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsqmmo/is_it_possible_to_make_ollama_pretend_to_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsqmmo/is_it_possible_to_make_ollama_pretend_to_be/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T09:52:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsbfp0</id>
    <title>What do you do with ollama?</title>
    <updated>2025-04-05T19:26:50+00:00</updated>
    <author>
      <name>/u/BlueTypes_</name>
      <uri>https://old.reddit.com/user/BlueTypes_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wondering what y'all do with machines and ollama.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlueTypes_"&gt; /u/BlueTypes_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsbfp0/what_do_you_do_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsbfp0/what_do_you_do_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsbfp0/what_do_you_do_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T19:26:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1js2vzw</id>
    <title>Best uncensored ollama local model for erotic/porn prompt writing for image generation (sdxl, pony,...)</title>
    <updated>2025-04-05T12:59:34+00:00</updated>
    <author>
      <name>/u/Infinite-Stable-10</name>
      <uri>https://old.reddit.com/user/Infinite-Stable-10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I a trying to find a good local model, less than 15B, that can generate uncensored prompts (sdxl, pony) to feed my Comfy UI workflow for generation of erotic / porn images. Any recommendations? I used in past solar-pro 13B that work quite well fed with examples, but it is a little outdated. &lt;/p&gt; &lt;p&gt;If there is a good model that i 5B or 10B or around that is best so I can load it at the same time of my image generation model. &lt;/p&gt; &lt;p&gt;HELP &lt;/p&gt; &lt;p&gt;🙉&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Infinite-Stable-10"&gt; /u/Infinite-Stable-10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js2vzw/best_uncensored_ollama_local_model_for_eroticporn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js2vzw/best_uncensored_ollama_local_model_for_eroticporn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1js2vzw/best_uncensored_ollama_local_model_for_eroticporn/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T12:59:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsat5n</id>
    <title>llama 4</title>
    <updated>2025-04-05T18:59:58+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama4_omni/"&gt;https://www.llama.com/docs/model-cards-and-prompt-formats/llama4_omni/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When I can download it from Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsat5n/llama_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsat5n/llama_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsat5n/llama_4/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T18:59:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1js9nkc</id>
    <title>I built an open source Computer-use framework that uses Local LLMs with Ollama</title>
    <updated>2025-04-05T18:09:51+00:00</updated>
    <author>
      <name>/u/sandropuppo</name>
      <uri>https://old.reddit.com/user/sandropuppo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1js9nkc/i_built_an_open_source_computeruse_framework_that/"&gt; &lt;img alt="I built an open source Computer-use framework that uses Local LLMs with Ollama" src="https://external-preview.redd.it/2AUUbeOoZ7agjDBXWHt091L224zZyg21bhTPn_iKBqY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=624a29b9725e7bb2871c8940c2c220a294d0d3e4" title="I built an open source Computer-use framework that uses Local LLMs with Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandropuppo"&gt; /u/sandropuppo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/trycua/cua"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js9nkc/i_built_an_open_source_computeruse_framework_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1js9nkc/i_built_an_open_source_computeruse_framework_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T18:09:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsjhr5</id>
    <title>mistral-small:24b-3.1 finally on ollama!</title>
    <updated>2025-04-06T01:59:48+00:00</updated>
    <author>
      <name>/u/DominusVenturae</name>
      <uri>https://old.reddit.com/user/DominusVenturae</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jsjhr5/mistralsmall24b31_finally_on_ollama/"&gt; &lt;img alt="mistral-small:24b-3.1 finally on ollama!" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="mistral-small:24b-3.1 finally on ollama!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw the benchmark comparing it to Llama4 scout and remembered that when 3.0 24b came out it remained far down the list of &amp;quot;Newest Model&amp;quot; filter. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DominusVenturae"&gt; /u/DominusVenturae &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/library/mistral-small:24b-3.1-instruct-2503-q4_K_M"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsjhr5/mistralsmall24b31_finally_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsjhr5/mistralsmall24b31_finally_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T01:59:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsljwk</id>
    <title>Github Copilot now supports Ollama and OpenRouter Models 🎉</title>
    <updated>2025-04-06T04:01:00+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jsljwk/github_copilot_now_supports_ollama_and_openrouter/"&gt; &lt;img alt="Github Copilot now supports Ollama and OpenRouter Models 🎉" src="https://b.thumbs.redditmedia.com/w6tm8jDvXneFlfBCf3lkx82SjxvqcNt_DTfDjlvvfmU.jpg" title="Github Copilot now supports Ollama and OpenRouter Models 🎉" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Huge W for programmers (and vibe coders) in the Local LLM community. Github Copilot now supports a much wider range of models from Ollama, OpenRouter, Gemini, and others.&lt;/p&gt; &lt;p&gt;To add your own models, click on &amp;quot;Manage Models&amp;quot; in the prompt field.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jsljwk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsljwk/github_copilot_now_supports_ollama_and_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsljwk/github_copilot_now_supports_ollama_and_openrouter/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T04:01:00+00:00</published>
  </entry>
</feed>
