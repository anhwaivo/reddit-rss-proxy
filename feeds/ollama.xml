<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-06-17T05:07:04+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1lbkwh9</id>
    <title>System-First Prompt Engineering: 18-Model LLM Benchmark Shows Hard-Constraint Compliance Gap</title>
    <updated>2025-06-14T22:28:19+00:00</updated>
    <author>
      <name>/u/kekePower</name>
      <uri>https://old.reddit.com/user/kekePower</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;System-First Prompt Engineering&lt;/strong&gt;&lt;br /&gt; 18-Model LLM Benchmark on Hard Constraints (Full Article + Chart)&lt;/p&gt; &lt;p&gt;I tested &lt;strong&gt;18 popular LLMs&lt;/strong&gt; ‚Äî GPT-4.5/o3, Claude-Opus/Sonnet, Gemini-2.5-Pro/Flash, Qwen3-30B, DeepSeek-R1-0528, Mistral-Medium, xAI Grok 3, Gemma3-27B, etc. ‚Äî with a &lt;em&gt;fixed&lt;/em&gt;, 2 k-word &lt;strong&gt;System Prompt&lt;/strong&gt; that enforces 10 hard rules (length, scene structure, vocab bans, self-check, etc.).&lt;br /&gt; The user prompt stayed intentionally weak (one line), so we could isolate how well each model obeys the ‚Äúspec sheet.‚Äù&lt;/p&gt; &lt;h1&gt;Key takeaways&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;System prompt &amp;gt; user prompt tweaking&lt;/strong&gt; ‚Äì tightening the spec raised average scores by &lt;strong&gt;+1.4 pts&lt;/strong&gt; without touching the request.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vendor hierarchy (avg / 10-pt compliance):&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Google Gemini ‚âà 6.0&lt;/li&gt; &lt;li&gt;OpenAI (4.x/o3) ‚âà 5.8&lt;/li&gt; &lt;li&gt;Anthropic ‚âà 5.5&lt;/li&gt; &lt;li&gt;DeepSeek ‚âà 5.0&lt;/li&gt; &lt;li&gt;Qwen ‚âà 3.8&lt;/li&gt; &lt;li&gt;Mistral ‚âà 4.0&lt;/li&gt; &lt;li&gt;xAI Grok ‚âà 2.0&lt;/li&gt; &lt;li&gt;Gemma ‚âà 3.0&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Editing pain&lt;/strong&gt; ‚Äì lower-tier outputs took &lt;strong&gt;25‚Äì30 min&lt;/strong&gt; of rewriting per 2.3 k-word story, often longer than writing from scratch.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Human-in-the-loop&lt;/strong&gt; QA still crucial: even top models missed subtle phrasing &amp;amp; rhythmic-flow checks ~25 % of the time.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Figure 1 ‚Äì Average 10-Pt Compliance by Vendor Family&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wthi3m15sy6f1.jpg?width=1979&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=cb16dd08857656e40c9c596208616a899904b234"&gt;https://preview.redd.it/wthi3m15sy6f1.jpg?width=1979&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=cb16dd08857656e40c9c596208616a899904b234&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full write-up (tables, prompt-evolution timeline, raw scores):&lt;/strong&gt;&lt;br /&gt; üîó &lt;a href="https://aimuse.blog/article/2025/06/14/system-prompts-versus-user-prompts-empirical-lessons-from-an-18-model-llm-benchmark-on-hard-constraints"&gt;https://aimuse.blog/article/2025/06/14/system-prompts-versus-user-prompts-empirical-lessons-from-an-18-model-llm-benchmark-on-hard-constraints&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Happy to share methodology details, scoring rubric, or raw texts in the comments!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kekePower"&gt; /u/kekePower &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lbkwh9/systemfirst_prompt_engineering_18model_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lbkwh9/systemfirst_prompt_engineering_18model_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lbkwh9/systemfirst_prompt_engineering_18model_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-14T22:28:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbgh6x</id>
    <title>üö™ Dungeo AI WebUI ‚Äì A Local Roleplay Frontend for LLM-based Dungeon Masters üßô‚Äç‚ôÇÔ∏è‚ú®</title>
    <updated>2025-06-14T19:09:11+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I‚Äôm the creator of &lt;a href="https://github.com/Laszlobeer/Dungeo_ai"&gt;Dungeo AI&lt;/a&gt;, and I‚Äôm excited to share the next evolution of the project: &lt;a href="https://github.com/Laszlobeer/Dungeo_ai_webui"&gt;&lt;strong&gt;Dungeo AI WebUI&lt;/strong&gt;&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;This is a major upgrade from the original terminal-based version ‚Äî now with a full web interfac. It's built for immersive, AI-powered solo roleplay in fantasy settings, kind of like having your own personal Dungeon Master on demand.&lt;/p&gt; &lt;p&gt;üîπ &lt;strong&gt;What‚Äôs New:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clean and responsive WebUi&lt;/li&gt; &lt;li&gt;Easy customise character : name, character &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üé≤ It‚Äôs built with simplicity and flexibility in mind. If you're into AI dungeon adventures or narrative roleplay, give it a try! Contributions, feedback, and forks are always welcome.&lt;/p&gt; &lt;p&gt;üì¶ GitHub: &lt;a href="https://github.com/Laszlobeer/Dungeo_ai_webui"&gt;https://github.com/Laszlobeer/Dungeo_ai_webui&lt;/a&gt;&lt;br /&gt; üß† Original Project: &lt;a href="https://github.com/Laszlobeer/Dungeo_ai"&gt;https://github.com/Laszlobeer/Dungeo_ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear what you think or see your own setups!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lbgh6x/dungeo_ai_webui_a_local_roleplay_frontend_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lbgh6x/dungeo_ai_webui_a_local_roleplay_frontend_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lbgh6x/dungeo_ai_webui_a_local_roleplay_frontend_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-14T19:09:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1lb2vj9</id>
    <title>I made a free iOS app for people who run LLMs locally. It‚Äôs a chatbot that you can use away from home to interact with an LLM that runs locally on your desktop Mac.</title>
    <updated>2025-06-14T07:27:59+00:00</updated>
    <author>
      <name>/u/Valuable-Run2129</name>
      <uri>https://old.reddit.com/user/Valuable-Run2129</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is easy enough that anyone can use it. No tunnel or port forwarding needed.&lt;/p&gt; &lt;p&gt;The app is called LLM Pigeon and has a companion app called LLM Pigeon Server for Mac.&lt;br /&gt; It works like a carrier pigeon :). It uses iCloud to append each prompt and response to a file on iCloud.&lt;br /&gt; It‚Äôs not totally local because iCloud is involved, but I trust iCloud with all my files anyway (most people do) and I don‚Äôt trust AI companies. &lt;/p&gt; &lt;p&gt;The iOS app is a simple Chatbot app. The MacOS app is a simple bridge to LMStudio or Ollama. Just insert the model name you are running on LMStudio or Ollama and it‚Äôs ready to go.&lt;br /&gt; For Apple approval purposes I needed to provide it with an in-built model, but don‚Äôt use it, it‚Äôs a small Qwen3-0.6B model.&lt;/p&gt; &lt;p&gt;I find it super cool that I can chat anywhere with Qwen3-30B running on my Mac at home. &lt;/p&gt; &lt;p&gt;For now it‚Äôs just text based. It‚Äôs the very first version, so, be kind. I've tested it extensively with LMStudio and it works great. I haven't tested it with Ollama, but it should work. Let me know.&lt;/p&gt; &lt;p&gt;The apps are open source and these are the repos:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/permaevidence/LLM-Pigeon"&gt;https://github.com/permaevidence/LLM-Pigeon&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/permaevidence/LLM-Pigeon-Server"&gt;https://github.com/permaevidence/LLM-Pigeon-Server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;they have just been approved by Apple and are both on the App Store. Here are the links:&lt;/p&gt; &lt;p&gt;&lt;a href="https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB"&gt;https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12"&gt;https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PS. I hope this isn't viewed as self promotion because the app is free, collects no data and is open source.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Valuable-Run2129"&gt; /u/Valuable-Run2129 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lb2vj9/i_made_a_free_ios_app_for_people_who_run_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lb2vj9/i_made_a_free_ios_app_for_people_who_run_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lb2vj9/i_made_a_free_ios_app_for_people_who_run_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-14T07:27:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbfn7k</id>
    <title>an offline voice assistant</title>
    <updated>2025-06-14T18:33:21+00:00</updated>
    <author>
      <name>/u/ppzms</name>
      <uri>https://old.reddit.com/user/ppzms</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;&lt;em&gt;Jarvis&lt;/em&gt; is a voice assistant I made in C++ that operates entirely on your local computer with no internet required! This is the first time to push a project in Github, and I would really appreciate it if some of you could take a look at it.&lt;/p&gt; &lt;p&gt;I'm not a professional developer this is just a hobby project I‚Äôve been working on in my spare time ‚Äî so I‚Äôd really appreciate your feedback.&lt;/p&gt; &lt;p&gt;Jarvis is meant to be very light on resources and completely offline-capable (after downloading the models). It harnesses some wonderful open-source initiatives to do the heavy lifting.&lt;/p&gt; &lt;p&gt;To make the installation process as easy as possible, especially for the Linux community, I have created a setup.sh and run.sh scripts that can be used for a quick and easy installation.&lt;/p&gt; &lt;p&gt;The things that I would like to know:&lt;/p&gt; &lt;p&gt;Any unexpected faults such as crashes, error messages, or wrong behavior that should be reported.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance&lt;/strong&gt;: What is the speed on different hardware configurations (especially CPU vs. GPU for LLM)? &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Experience of Setting Up&lt;/strong&gt;: Did the README.md provide a clear message? &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code Feedback:&lt;/strong&gt; If you‚Äôre into C++, feel free to peek at the code and roast it nicely ‚Äî tips on cleaner structure, better practices, or just ‚Äúwhat were you thinking here?‚Äù moments are totally welcome!&lt;/p&gt; &lt;p&gt;Have a look at my &lt;a href="https://github.com/almimony75/jarvis"&gt;repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Remember to open the llama.cpp server in another terminal before you run Jarvis!&lt;/p&gt; &lt;p&gt;Thanks a lot for your contribution!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ppzms"&gt; /u/ppzms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lbfn7k/an_offline_voice_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lbfn7k/an_offline_voice_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lbfn7k/an_offline_voice_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-14T18:33:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lb70kk</id>
    <title>LLM with OCR capabilities</title>
    <updated>2025-06-14T11:59:58+00:00</updated>
    <author>
      <name>/u/depava</name>
      <uri>https://old.reddit.com/user/depava</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to create an app to OCR PDF documents. I need LLM model to understand context on how to map text to particular fields. Plain OCR things cannot do it. &lt;/p&gt; &lt;p&gt;It is for production, not a higload but 300 docs per day can be. &lt;/p&gt; &lt;p&gt;I use AWS, and thinking about using Bedrock and Claude. But I think, maybe it's cheaper to use some self-hosted models for this purpose? Or running in EC2 instance the model will cost more than just using API of paid models? Thank you very much in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/depava"&gt; /u/depava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lb70kk/llm_with_ocr_capabilities/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lb70kk/llm_with_ocr_capabilities/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lb70kk/llm_with_ocr_capabilities/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-14T11:59:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbpqln</id>
    <title>MiniPC Ryzen 7 6800H CPU and iGPU 680M</title>
    <updated>2025-06-15T02:36:30+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lbpqln/minipc_ryzen_7_6800h_cpu_and_igpu_680m/"&gt; &lt;img alt="MiniPC Ryzen 7 6800H CPU and iGPU 680M" src="https://a.thumbs.redditmedia.com/tpedg4ze2xDoPSRQnppyauv7nM0-4tkJkk74SJgppd8.jpg" title="MiniPC Ryzen 7 6800H CPU and iGPU 680M" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I somehow got lucky and was able to get the iGPU working with Pop_OS 24.04 but not Kubuntu 25.10 or Mint 22.1. Until I tried Warp AI Terminal Emulator. It was great watching AI fix AI.&lt;/p&gt; &lt;p&gt;Anywho, I purchased the ACEMAGIC S3A Mini PC barebones, add 64GB DDR5 memory and a 2TB Gen4 NVMe drive. Very happy, it benchmarks a little faster than my Ryzen 5 5600X and that CPU is a beast. You have to be in 'Performance Mode' when entering BIOS and then use CTRL+F1 to view all advanced settings.&lt;/p&gt; &lt;p&gt;Change BIOS to 16GB for iGPU&lt;/p&gt; &lt;p&gt;UEFI/BIOS -&amp;gt; Advanced -&amp;gt; AMD CBS -&amp;gt; NBIO -&amp;gt; GFX -&amp;gt; iGPU -&amp;gt; UMA_SPECIFIED&lt;/p&gt; &lt;p&gt;Here is what you can expect from the iGPU over just CPU using Ollama version 0.9.0&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yrdlntp707f1.png?width=719&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2dfde8984ce75b486c440a155b4a8d46ae02da18"&gt;CPU only 64GB DDR5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1jfxyhws707f1.png?width=853&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9edece60f48370d94f0838ab7f19c38d39d69fa1"&gt;iGPU working&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jtb0qhty707f1.png?width=721&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8377f4befa2721ed7aba7d159080abeb23955b04"&gt;Benefit of having iGPU working &lt;/a&gt;&lt;/p&gt; &lt;p&gt;Notice that the 70b size model is actually slower than just using CPU only. Biggest benefit is DDR5 speed.&lt;/p&gt; &lt;p&gt;Basically I just had to get the &lt;em&gt;Environment override&lt;/em&gt; to work correctly. I'm not sure how Warp AI figured it out, but it did. Plan to do a clean install and figure it out.&lt;/p&gt; &lt;p&gt;Here is what I ran to add &lt;em&gt;Environment override&lt;/em&gt;:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo systemctl edit ollama.service &amp;amp;&amp;amp; systemctl daemon-reload &amp;amp;&amp;amp; systemctl restart ollama&lt;/code&gt;&lt;br /&gt; I added this&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[Service] Environment=&amp;quot;HSA_OVERRIDE_GFX_VERSION=10.3.0&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally I was able to use iGPU. Again, Warp AI figured out why this wasn't working correctly. Here is the summary Warp AI provided.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Key changes made:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;1. Installed ROCm components: Added rocm-smi and related libraries for GPU detection&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;2. Fixed systemd override configuration: Added the proper [Service] section header to /etc/systemd/system/ollama.service.d/override.conf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;3. Environment variables are now working:&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;‚Ä¢ HSA_OVERRIDE_GFX_VERSION=10.3.0 - Overrides the GPU detection to treat your gfx1035 as gfx1030 (compatible)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;‚Ä¢ OLLAMA_LLM_LIBRARY=rocm_v60000u_avx2 - Forces Ollama to use the ROCm library&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Results:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;‚Ä¢ Your AMD Radeon 680M (gfx1035) is now properly detected with 16.0 GiB total and 15.7 GiB available memory&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;‚Ä¢ The model is running on 100% GPU instead of CPU&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;‚Ä¢ Performance has improved significantly (from 5.56 tokens/s to 6.34 tokens/s, and much faster prompt evaluation: 83.41 tokens/s vs 19.49 tokens/s)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;[Service]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Environment=&amp;quot;HSA_OVERRIDE_GFX_VERSION=10.3.0&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Environment=&amp;quot;OLLAMA_LLM_LIBRARY=rocm_v60000u_avx2&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The AVX2 wasn't needed, it's already implemented in Ollama.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lbpqln/minipc_ryzen_7_6800h_cpu_and_igpu_680m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lbpqln/minipc_ryzen_7_6800h_cpu_and_igpu_680m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lbpqln/minipc_ryzen_7_6800h_cpu_and_igpu_680m/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-15T02:36:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbxsvo</id>
    <title>Which llm model choose to sum up interviews ?</title>
    <updated>2025-06-15T11:07:54+00:00</updated>
    <author>
      <name>/u/toothmariecharcot</name>
      <uri>https://old.reddit.com/user/toothmariecharcot</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/toothmariecharcot"&gt; /u/toothmariecharcot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1lbco2q/which_llm_model_choose_to_sum_up_interviews/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lbxsvo/which_llm_model_choose_to_sum_up_interviews/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lbxsvo/which_llm_model_choose_to_sum_up_interviews/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-15T11:07:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lc6b2n</id>
    <title>Macbook Air Heating up while running Ollama</title>
    <updated>2025-06-15T17:46:10+00:00</updated>
    <author>
      <name>/u/moneymagnet98</name>
      <uri>https://old.reddit.com/user/moneymagnet98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I was trying to run deepseek-r1:14b on my Macbook Air and I noticed that it is running super hot. While I expected some heating but this felt highly unusual. I am wondering if there are any ways to mitigate this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moneymagnet98"&gt; /u/moneymagnet98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lc6b2n/macbook_air_heating_up_while_running_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lc6b2n/macbook_air_heating_up_while_running_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lc6b2n/macbook_air_heating_up_while_running_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-15T17:46:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lc2lgs</id>
    <title>#LocalLLMs FTW: Asynchronous Pre-Generation Workflow {‚ÄúStep‚Äú: 1}</title>
    <updated>2025-06-15T15:10:38+00:00</updated>
    <author>
      <name>/u/anttiOne</name>
      <uri>https://old.reddit.com/user/anttiOne</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;‚Ä¶ or ‚ÄûHow to Serve the right Recommendation BEFORE the Users even ask for it‚Äú.&lt;/p&gt; &lt;p&gt;This is the story about a production-ready #LocalLLM setup for generating custom user recommendation, implemented for a real business. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anttiOne"&gt; /u/anttiOne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@vs3kulic/building-ai-for-privacy-custom-recommendations-with-local-llms-3201bb0a3f5a"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lc2lgs/localllms_ftw_asynchronous_pregeneration_workflow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lc2lgs/localllms_ftw_asynchronous_pregeneration_workflow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-15T15:10:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbvd02</id>
    <title>ollama's 8b is only 5gb while hugging face is near 16gb, is it quantized?, if yes how to use the full unquantized llama 8b?</title>
    <updated>2025-06-15T08:23:56+00:00</updated>
    <author>
      <name>/u/Beyond_Birthday_13</name>
      <uri>https://old.reddit.com/user/Beyond_Birthday_13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lbvd02/ollamas_8b_is_only_5gb_while_hugging_face_is_near/"&gt; &lt;img alt="ollama's 8b is only 5gb while hugging face is near 16gb, is it quantized?, if yes how to use the full unquantized llama 8b?" src="https://external-preview.redd.it/wZte_idxzrfltl70eEbLpGXYidrhFHY38JLhA3vpbNc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abc79b312a92dc4f6f679bb710a084d0512a3c12" title="ollama's 8b is only 5gb while hugging face is near 16gb, is it quantized?, if yes how to use the full unquantized llama 8b?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beyond_Birthday_13"&gt; /u/Beyond_Birthday_13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lbvd02"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lbvd02/ollamas_8b_is_only_5gb_while_hugging_face_is_near/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lbvd02/ollamas_8b_is_only_5gb_while_hugging_face_is_near/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-15T08:23:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbpz7w</id>
    <title>Introducing Paiperwork - A Privacy-First AI Paperwork Assistant Built on Ollama</title>
    <updated>2025-06-15T02:49:34+00:00</updated>
    <author>
      <name>/u/Infinitai-cn</name>
      <uri>https://old.reddit.com/user/Infinitai-cn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lbpz7w/introducing_paiperwork_a_privacyfirst_ai/"&gt; &lt;img alt="Introducing Paiperwork - A Privacy-First AI Paperwork Assistant Built on Ollama" src="https://preview.redd.it/gftz2fiv807f1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=567b1c2c1d4c5b641a32ba2edf9bdebb55bc1782" title="Introducing Paiperwork - A Privacy-First AI Paperwork Assistant Built on Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey r/ollama family! üëã&lt;/p&gt; &lt;p&gt;First off, we want to express our gratitude to this incredible community. Over the past years, we learned so much from all of you - from model recommendations to optimization tips, and especially the philosophy of keeping AI local and private. This community has been instrumental in shaping what we've built, and now we want to give back.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why We Built Paiperwork&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After seeing so many talented folks here struggle with the gap between powerful local AI (thanks to Ollama!) and practical office work, we realized there was a missing piece. Most AI tools are either cloud-based (privacy concerns) or focused on coding/chat. We wanted something specifically designed for the daily grind of paperwork, document processing, and office productivity - but with the privacy-first approach that makes Ollama so special.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is Paiperwork?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Paiperwork is a completely free, open-source AI office suite that runs entirely on your machine using Ollama. Think of it as your AI-powered productivity companion that never sends your data anywhere. It's specifically designed for:&lt;/p&gt; &lt;p&gt;‚Ä¢ Document processing and analysis&lt;/p&gt; &lt;p&gt;‚Ä¢ Data visualization and reports&lt;/p&gt; &lt;p&gt;‚Ä¢ Research and knowledge management&lt;/p&gt; &lt;p&gt;‚Ä¢ Professional document creation&lt;/p&gt; &lt;p&gt;‚Ä¢ Intelligent conversations with your files&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Smart Chat Interface&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ Advanced conversation controls (regenerate, delete, copy)&lt;/p&gt; &lt;p&gt;‚Ä¢ Custom system prompts for specialized tasks&lt;/p&gt; &lt;p&gt;‚Ä¢ Image upload and analysis&lt;/p&gt; &lt;p&gt;‚Ä¢ Native support for reasoning models (Models now supported: Deepkseek R1 and Qwen distillations, etc.)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Document Intelligence&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ PDF and text processing with local RAG&lt;/p&gt; &lt;p&gt;‚Ä¢ Document Q&amp;amp;A with semantic search&lt;/p&gt; &lt;p&gt;‚Ä¢ Cross-document analysis and summarization&lt;/p&gt; &lt;p&gt;‚Ä¢ No cloud processing - everything stays local&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Professional Document Creation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ Visual template designer for multi-page documents&lt;/p&gt; &lt;p&gt;‚Ä¢ AI-enhanced content generation&lt;/p&gt; &lt;p&gt;‚Ä¢ Export options for business communications&lt;/p&gt; &lt;p&gt;‚Ä¢ Template library for common office documents&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Research Assistant&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ AI-powered web research with source citations&lt;/p&gt; &lt;p&gt;‚Ä¢ Personal knowledge repository (encrypted locally)&lt;/p&gt; &lt;p&gt;‚Ä¢ Comparative analysis tools&lt;/p&gt; &lt;p&gt;‚Ä¢ Offline knowledge search&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Data Visualization&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ Natural language chart generation&lt;/p&gt; &lt;p&gt;‚Ä¢ Interactive charts and graphs&lt;/p&gt; &lt;p&gt;‚Ä¢ Custom styling through conversation&lt;/p&gt; &lt;p&gt;‚Ä¢ Export capabilities for presentations&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Visual Design Tools (First version, we will add more features to it later)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ HTML/CSS code generation from image designs&lt;/p&gt; &lt;p&gt;‚Ä¢ Text overlay creation&lt;/p&gt; &lt;p&gt;‚Ä¢ Design principle analysis&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Privacy &amp;amp; Technical Highlights&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Zero Data Collection - No telemetry, no tracking, no cloud dependencies&lt;/p&gt; &lt;p&gt;AES-256 Encryption - All local data encrypted with your master key&lt;/p&gt; &lt;p&gt;Ollama Integration - Seamless local model management&lt;/p&gt; &lt;p&gt;Cross-Platform - Windows, macOS, Linux&lt;/p&gt; &lt;p&gt;Lightweight - Runs well on Core i3 with 16GB RAM&lt;/p&gt; &lt;p&gt;Portable - No installation required, just download and run&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Paiperwork?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In one sentence: To handle typical &amp;quot;paperwork&amp;quot; tasks and commercial communications, that's it.&lt;/p&gt; &lt;p&gt;It's not trying to replace any other chat inferface - it's purpose-built for getting work done.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical Architecture&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ Backend: Lightweight Go server handling Ollama integration&lt;/p&gt; &lt;p&gt;‚Ä¢ Frontend: JavaScript (no build process needed)&lt;/p&gt; &lt;p&gt;‚Ä¢ Database: Local SQL.js with full encryption&lt;/p&gt; &lt;p&gt;‚Ä¢ AI Engine: Your local Ollama models&lt;/p&gt; &lt;p&gt;‚Ä¢ Philosophy: Privacy-first, offline-capable, truly portable, Low-end hardware friendly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Getting Started:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Make sure you have Ollama running locally&lt;/li&gt; &lt;li&gt;Download Paiperwork here: &lt;a href="https://infinitai-cn.github.io/paiperwork/"&gt;https://infinitai-cn.github.io/paiperwork/&lt;/a&gt; We suggest you read the documentation first to find out if this software is for you)&lt;/li&gt; &lt;li&gt;Run the portable app - no installation needed&lt;/li&gt; &lt;li&gt;Start with any model you have installed in Ollama&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Web search and Research require internet connection (Only Search queries are sent to internet for the actual search)&lt;/p&gt; &lt;p&gt;A Heartfelt Thank You.&lt;/p&gt; &lt;p&gt;Infinitai-cn&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Infinitai-cn"&gt; /u/Infinitai-cn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gftz2fiv807f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lbpz7w/introducing_paiperwork_a_privacyfirst_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lbpz7w/introducing_paiperwork_a_privacyfirst_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-15T02:49:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lc5fmy</id>
    <title>Not Allowed</title>
    <updated>2025-06-15T17:09:46+00:00</updated>
    <author>
      <name>/u/marketlurker</name>
      <uri>https://old.reddit.com/user/marketlurker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When my application tries to access the API endpoint &amp;quot;&lt;a href="http://localhost:11434/api/generate"&gt;localhost:11434/api/generate&lt;/a&gt;&amp;quot; I get an error, &amp;quot;405 method not allowed&amp;quot; error. Obviously, something is not quite right. Anyone have an idea what I am missing? I am running ollama in a docker container with the port exposed.&lt;/p&gt; &lt;p&gt;For those familiar with it, I am trying to run the python app marker-pdf. I am passing&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--ollama_base_url &amp;quot;http://localhost:11434&amp;quot; --ollama_model=&amp;quot;llama3.2&amp;quot; --llm_service=marker.services.ollama.OllamaService &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;per the instructions &lt;a href="https://pypi.org/project/marker-pdf/"&gt;here&lt;/a&gt;. I am running ollama 0.9.0.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marketlurker"&gt; /u/marketlurker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lc5fmy/not_allowed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lc5fmy/not_allowed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lc5fmy/not_allowed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-15T17:09:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lc7ej5</id>
    <title>changeish - manage your code's changelog using ollama</title>
    <updated>2025-06-15T18:31:18+00:00</updated>
    <author>
      <name>/u/Kitchen_Fix1464</name>
      <uri>https://old.reddit.com/user/Kitchen_Fix1464</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was working on a large application and struggling to keep up with the change log updates. So, I created this script that will update the change log file by generating a git history and prompt to feed to ollama. It appends the output to the top of the changelog file. The script is written in bash to reduce dependency/package management. It only requires git and Ollama. You can skip the generation step if Ollama is not available and it will return a prompt.md file that can be used with other LLM interfaces.&lt;/p&gt; &lt;p&gt;This is still VERY rough and makes some assumptions that need to he customizable. With that said, I wanted to post what I have so far and see if there is any interest in a tool like this. If so, I will spend some time making it more flexible and documenting the default workflow assumptions.&lt;/p&gt; &lt;p&gt;Any feedback is welcomed. Also happy to have PRs for missing features, fixes, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kitchen_Fix1464"&gt; /u/Kitchen_Fix1464 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/itlackey/changeish/tree/main"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lc7ej5/changeish_manage_your_codes_changelog_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lc7ej5/changeish_manage_your_codes_changelog_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-15T18:31:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcs53a</id>
    <title>what is the biggest LLM i can use with a Arda 4000 20gb vram</title>
    <updated>2025-06-16T12:59:06+00:00</updated>
    <author>
      <name>/u/Better-Barnacle-1990</name>
      <uri>https://old.reddit.com/user/Better-Barnacle-1990</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, which LL-Model is the biggest i can stil use on my Arda 4000 20gb VRAM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Better-Barnacle-1990"&gt; /u/Better-Barnacle-1990 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcs53a/what_is_the_biggest_llm_i_can_use_with_a_arda/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcs53a/what_is_the_biggest_llm_i_can_use_with_a_arda/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lcs53a/what_is_the_biggest_llm_i_can_use_with_a_arda/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T12:59:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbt4zg</id>
    <title>iDoNotHaveThatMuchRam</title>
    <updated>2025-06-15T05:55:27+00:00</updated>
    <author>
      <name>/u/Virtual4P</name>
      <uri>https://old.reddit.com/user/Virtual4P</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lbt4zg/idonothavethatmuchram/"&gt; &lt;img alt="iDoNotHaveThatMuchRam" src="https://preview.redd.it/f5q7ded8ew6f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=395856e68d91cf6ad7f9706067350bde131156f3" title="iDoNotHaveThatMuchRam" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Virtual4P"&gt; /u/Virtual4P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f5q7ded8ew6f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lbt4zg/idonothavethatmuchram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lbt4zg/idonothavethatmuchram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-15T05:55:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcf4tf</id>
    <title>System specs for ollama on proxmox</title>
    <updated>2025-06-16T00:16:57+00:00</updated>
    <author>
      <name>/u/CombatRaccoons</name>
      <uri>https://old.reddit.com/user/CombatRaccoons</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So i have a fresh pc build.&lt;/p&gt; &lt;p&gt;Intrel i7 20 core 14700k. 192 gb ddr5 ram 2x rtx 5060ti 16gb vram (total 32gb) 4 tb HDD Asus z790 motherboard 1x 10gb nic&lt;/p&gt; &lt;p&gt;Looking to build an ollama (or alternative) LLM server for application API and function calling. I would like to run a VMs within proxmox to include a ubuntu server vm with ollama (or alternative).&lt;/p&gt; &lt;p&gt;Is this sufficient? What are the recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombatRaccoons"&gt; /u/CombatRaccoons &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcf4tf/system_specs_for_ollama_on_proxmox/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcf4tf/system_specs_for_ollama_on_proxmox/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lcf4tf/system_specs_for_ollama_on_proxmox/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T00:16:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcu3q8</id>
    <title>How to install Docker on Windows?</title>
    <updated>2025-06-16T14:21:59+00:00</updated>
    <author>
      <name>/u/Ok_Most9659</name>
      <uri>https://old.reddit.com/user/Ok_Most9659</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Struggling to find a clear and concise guide to installing Docker on Windows. Also, some say you must register a Docker account to use even for personal use on Windows, is this correct?&lt;br /&gt; Can any one link a clear concise installation guide for Docker on Windows?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Most9659"&gt; /u/Ok_Most9659 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcu3q8/how_to_install_docker_on_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcu3q8/how_to_install_docker_on_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lcu3q8/how_to_install_docker_on_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T14:21:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcwjea</id>
    <title>What TUI interfaces for Ollama do you know?</title>
    <updated>2025-06-16T15:55:55+00:00</updated>
    <author>
      <name>/u/q-admin007</name>
      <uri>https://old.reddit.com/user/q-admin007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for something i can install on all my Linux servers that then connects to the ollama server.&lt;/p&gt; &lt;p&gt;I want to be able to pick a model and maybe have a history of previous chats. Maybe rerun a prompt with another model would be nice, but optional.&lt;/p&gt; &lt;p&gt;Anything that comes to mind?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/q-admin007"&gt; /u/q-admin007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcwjea/what_tui_interfaces_for_ollama_do_you_know/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcwjea/what_tui_interfaces_for_ollama_do_you_know/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lcwjea/what_tui_interfaces_for_ollama_do_you_know/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T15:55:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lczeww</id>
    <title>Local Open Source VScode Copilot model with MCP</title>
    <updated>2025-06-16T17:43:24+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1lcud8j/local_open_source_vscode_copilot_model_with_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lczeww/local_open_source_vscode_copilot_model_with_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lczeww/local_open_source_vscode_copilot_model_with_mcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T17:43:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lctg3s</id>
    <title>Looking for recommendations for a GPU</title>
    <updated>2025-06-16T13:55:35+00:00</updated>
    <author>
      <name>/u/Limitless83</name>
      <uri>https://old.reddit.com/user/Limitless83</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Right now I'm running some smaller LLMs on my CPU (intel i5 11500, 64GB DDR4) on my server. But i would like to run/experiment with some larger ones.&lt;br /&gt; EDIT: i'm running Ollama and Open WebUI in a docker on Debian 12 &lt;/p&gt; &lt;p&gt;I'm looking to buy a new GPU for either my server or my gaming PC.&lt;br /&gt; My gaming PC has a NVIDIA 4070 (non TI, 12GB VRAM).&lt;br /&gt; Budget wise I'm looking at either AMD RX 7600 XT, AMD RX 9060 XT or a NVIDIA RTX 5060 TI. (between 360‚Ç¨ - 480‚Ç¨)&lt;br /&gt; So the question is: which one of these 3 cards is the best for AI or to upgrade my PC so the 4070 goes into the server. Or is there a card that I'm overlooking in the same price range?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Limitless83"&gt; /u/Limitless83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lctg3s/looking_for_recommendations_for_a_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lctg3s/looking_for_recommendations_for_a_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lctg3s/looking_for_recommendations_for_a_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T13:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcxkrw</id>
    <title>Ollama: Powering Privacy-Focused AI for My WhatsApp Chat Mimicry!</title>
    <updated>2025-06-16T16:35:16+00:00</updated>
    <author>
      <name>/u/Basic_Regular_3100</name>
      <uri>https://old.reddit.com/user/Basic_Regular_3100</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I'm excited to share how &lt;strong&gt;Ollama&lt;/strong&gt; has been instrumental in my &lt;strong&gt;Chat Mimicry AI&lt;/strong&gt; project. This tool allows users to use WhatsApp chats history file, and then an AI mimics the personalities within. It's a powerful example of what local LLMs can achieve!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ollama was indispensable during development.&lt;/strong&gt; Its simplicity for running models locally allowed for rapid iteration and testing.&lt;/p&gt; &lt;p&gt;A key advantage of &lt;strong&gt;Ollama&lt;/strong&gt; is its role in &lt;strong&gt;data privacy for the local version of my project&lt;/strong&gt;. When users run the AI locally with Ollama, their chat data never leaves their device, which builds immense &lt;strong&gt;user trust&lt;/strong&gt;. While I currently have a hosted version online, my strong preference is to eventually &lt;strong&gt;self-host the AI for potential unlimited usage&lt;/strong&gt;, and to explore how Ollama can best support that while maintaining as much privacy as possible.&lt;/p&gt; &lt;p&gt;btw: You can explore hosted version and the &lt;strong&gt;Ollama-powered local version&lt;/strong&gt; &lt;a href="https://bitwattr.pages.dev/projects/chat-mimicry-ai"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ollama is truly democratizing AI and enabling new possibilities for user control and data handling. What are your thoughts on building private AI experiences with Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Basic_Regular_3100"&gt; /u/Basic_Regular_3100 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcxkrw/ollama_powering_privacyfocused_ai_for_my_whatsapp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcxkrw/ollama_powering_privacyfocused_ai_for_my_whatsapp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lcxkrw/ollama_powering_privacyfocused_ai_for_my_whatsapp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T16:35:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcy7qr</id>
    <title>MCP llm tool calls are sky-rocketing my token usage - travel agency example</title>
    <updated>2025-06-16T16:59:32+00:00</updated>
    <author>
      <name>/u/benxben13</name>
      <uri>https://old.reddit.com/user/benxben13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wish to know if im doing something wrong or maybe missing the obvious when building pipelines with mcp llm tool calls. &lt;/p&gt; &lt;p&gt;so I've built a basic pipeline (&lt;a href="https://github.com/benx13/basic-travel-agency"&gt;GitHub repo&lt;/a&gt;) for an llm travel agency to compare: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;classical tool calling&lt;/strong&gt;: fixed pipeline where we are asking the llm to generate the parameters of some function and manually call it&lt;/li&gt; &lt;li&gt;&lt;strong&gt;mcp llm tool calling&lt;/strong&gt;: dynamic loop where the llm decides sequentially which function to call &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I found out a couple interesting things about mcp tool calls: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;at some point the llm will decide to generate a &lt;strong&gt;tool_usage_token&lt;/strong&gt; for example search_hotels_token when it decides to look up hotels&lt;/li&gt; &lt;li&gt;the engine will cancel the request execute the tool and append its output to the prompt and makes a new llm call and keeps doing that for every tool call&lt;/li&gt; &lt;li&gt;&lt;strong&gt;by calling multiple tools it means that we are going to make multiple request in which the input prompt will probably be cached but the amount of tokens will pile-up, even at a 50% discount the input tokens are only increasing exponentially because basically you will be calling the same request multiple times. especially if a tool returns a big output eg: top-20 hotels so you will call those same 20 hotels for each request you make (number of tools used).&lt;/strong&gt; &lt;/li&gt; &lt;li&gt;you can't run multiple tools in async mode for example search tools because the llm can't generate multiple tool usage stop tokens at the same time (im not sure about this) but you will probably end up doing a routing tool and run your tools manually &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;as a result of the points above I checked my openrouter usage and found a significant difference for this basic travel agency example (using 4 sonnet):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;mcp approach&lt;/strong&gt; used: &lt;ul&gt; &lt;li&gt;total input tokens: &lt;strong&gt;3415&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;total output tokens: &lt;strong&gt;1491&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Total cost: &lt;strong&gt;0.02848$&lt;/strong&gt; (and it failed at the end)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Manuel approach&lt;/strong&gt; used: &lt;ul&gt; &lt;li&gt;total input tokens: &lt;strong&gt;381&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;total output tokens: &lt;strong&gt;175&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Total cost: &lt;strong&gt;0.00201$&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I understand the benefits of having a dynamic conversation using mcp tool calls methodology but &lt;strong&gt;is it worth the extra tokens?&lt;/strong&gt; as it would be cool if you actually can pause the request instead of canceling and launching a new one but that's impossible due to infrastructure purposes. &lt;/p&gt; &lt;p&gt;below is link to the comparison GitHub repo let me know guys if I'm missing something obvious.&lt;br /&gt; &lt;a href="https://github.com/benx13/basic-travel-agency"&gt;https://github.com/benx13/basic-travel-agency&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/benxben13"&gt; /u/benxben13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcy7qr/mcp_llm_tool_calls_are_skyrocketing_my_token/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcy7qr/mcp_llm_tool_calls_are_skyrocketing_my_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lcy7qr/mcp_llm_tool_calls_are_skyrocketing_my_token/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T16:59:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcyhxk</id>
    <title>Alternatives to Apple Studio, preferably mini-pcs</title>
    <updated>2025-06-16T17:09:37+00:00</updated>
    <author>
      <name>/u/TwitchTv_SosaJacobb</name>
      <uri>https://old.reddit.com/user/TwitchTv_SosaJacobb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've been wanting to run LLM locally by using external hardware with linux os. and I often saw that people here recommend Apple Studio. &lt;/p&gt; &lt;p&gt;However are there other alternatives? I've been thinking about BeeLink or Dell Thin mini-pcs.&lt;/p&gt; &lt;p&gt;My goal was to run 7b, 14b or maybe even 32b deepseek or other models efficiently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TwitchTv_SosaJacobb"&gt; /u/TwitchTv_SosaJacobb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcyhxk/alternatives_to_apple_studio_preferably_minipcs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lcyhxk/alternatives_to_apple_studio_preferably_minipcs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lcyhxk/alternatives_to_apple_studio_preferably_minipcs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T17:09:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld9tvd</id>
    <title>[Update] Serene Pub v0.2.0-alpha - Added group chats, LM Studio, OpenAI support and more</title>
    <updated>2025-06-17T00:49:31+00:00</updated>
    <author>
      <name>/u/doolijb</name>
      <uri>https://old.reddit.com/user/doolijb</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doolijb"&gt; /u/doolijb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1ld8phi/update_serene_pub_v020alpha_added_group_chats_lm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ld9tvd/update_serene_pub_v020alpha_added_group_chats_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ld9tvd/update_serene_pub_v020alpha_added_group_chats_lm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T00:49:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld1bj1</id>
    <title>I made a macos MCP client</title>
    <updated>2025-06-16T18:53:55+00:00</updated>
    <author>
      <name>/u/SandwichConscious336</name>
      <uri>https://old.reddit.com/user/SandwichConscious336</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ld1bj1/i_made_a_macos_mcp_client/"&gt; &lt;img alt="I made a macos MCP client" src="https://preview.redd.it/q0m3hohr5c7f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=94ec86313f925cd3386804c2445f1b9f706a53c3" title="I made a macos MCP client" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am working on adding MCP support for my native macos Ollama client app. I am looking for people currently using Ollama locally (with a client or not) who are curious about MCP and would like a way to easy use MCP servers (local and remote).&lt;/p&gt; &lt;p&gt;Reply and DM me if you're interested in testing my MCP integration.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SandwichConscious336"&gt; /u/SandwichConscious336 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q0m3hohr5c7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ld1bj1/i_made_a_macos_mcp_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ld1bj1/i_made_a_macos_mcp_client/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T18:53:55+00:00</published>
  </entry>
</feed>
