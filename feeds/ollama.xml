<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-08T03:34:11+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1j4pecf</id>
    <title>Using "tools" support (or function calling) with LangchainJS and Ollama</title>
    <updated>2025-03-06T06:35:07+00:00</updated>
    <author>
      <name>/u/Inevitable-Judge2642</name>
      <uri>https://old.reddit.com/user/Inevitable-Judge2642</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j4pecf/using_tools_support_or_function_calling_with/"&gt; &lt;img alt="Using &amp;quot;tools&amp;quot; support (or function calling) with LangchainJS and Ollama" src="https://external-preview.redd.it/uOAwdw5hhwqw_jo_ftA1OGWP6olZQCGU2qrjZd0cVGc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=414eaea7fd8aeb7667d7a1b1288a4833cc3917ed" title="Using &amp;quot;tools&amp;quot; support (or function calling) with LangchainJS and Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Judge2642"&gt; /u/Inevitable-Judge2642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://k33g.hashnode.dev/using-tools-support-or-function-calling-with-langchainjs-and-ollama"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4pecf/using_tools_support_or_function_calling_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4pecf/using_tools_support_or_function_calling_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-06T06:35:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4z0ei</id>
    <title>Why does my process keep running in the background?</title>
    <updated>2025-03-06T16:11:36+00:00</updated>
    <author>
      <name>/u/drred97</name>
      <uri>https://old.reddit.com/user/drred97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, this week I tried setting up an LLM using Ollama for work. I was testing stuff without any bad intentions, terminating each process properly, but now our admins sent me a list of the gpu usage of the machine I tested on (Linux), it was full of running ollama processes from me... Is this known? Why does this happen?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/drred97"&gt; /u/drred97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4z0ei/why_does_my_process_keep_running_in_the_background/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4z0ei/why_does_my_process_keep_running_in_the_background/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4z0ei/why_does_my_process_keep_running_in_the_background/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-06T16:11:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4nl15</id>
    <title>LLM Inference Hardware Calculator</title>
    <updated>2025-03-06T04:41:10+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just wanted to share Youtuber Alex Ziskind's cool LLM Inference Hardware Calculator tool. You can gauge what model sizes, quant levels, and context sizes certain hardware can handle before you buy.&lt;/p&gt; &lt;p&gt;I find it very useful in aiding in the decision of buying the newly released Mac Studio M3 Ultra or NVIDIA digits that is coming out soon.&lt;/p&gt; &lt;p&gt;Here it is:&lt;br /&gt; &lt;a href="https://llm-inference-calculator-rki02.kinsta.page/"&gt;https://llm-inference-calculator-rki02.kinsta.page/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4nl15/llm_inference_hardware_calculator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4nl15/llm_inference_hardware_calculator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4nl15/llm_inference_hardware_calculator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-06T04:41:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4l7of</id>
    <title>Made a simple playground for easy experiment with 8+ open-source PDF-to-markdown for document ingestion (+ visualization)</title>
    <updated>2025-03-06T02:33:06+00:00</updated>
    <author>
      <name>/u/taprosoft</name>
      <uri>https://old.reddit.com/user/taprosoft</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j4l7of/made_a_simple_playground_for_easy_experiment_with/"&gt; &lt;img alt="Made a simple playground for easy experiment with 8+ open-source PDF-to-markdown for document ingestion (+ visualization)" src="https://external-preview.redd.it/Ik_UOXUVGTKj5B9oOW8_FISqZe0LfJ9NkHqhzs4tgyU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c48cee8cb3ea64e35dbb0891bce79f17bc38eb0" title="Made a simple playground for easy experiment with 8+ open-source PDF-to-markdown for document ingestion (+ visualization)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taprosoft"&gt; /u/taprosoft &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/chunking-ai/pdf-playground"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4l7of/made_a_simple_playground_for_easy_experiment_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4l7of/made_a_simple_playground_for_easy_experiment_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-06T02:33:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4rvox</id>
    <title>Running QwQ 32B on my Hardware possible ?</title>
    <updated>2025-03-06T09:40:17+00:00</updated>
    <author>
      <name>/u/Fox-Lopsided</name>
      <uri>https://old.reddit.com/user/Fox-Lopsided</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 32GB RAM with a GeForce RTX 3060 12GB, Ryzen 5 3600 CPU. Would i be able to Run the Q8_0 Variant or even the FP16 on my Hardware ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fox-Lopsided"&gt; /u/Fox-Lopsided &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4rvox/running_qwq_32b_on_my_hardware_possible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4rvox/running_qwq_32b_on_my_hardware_possible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4rvox/running_qwq_32b_on_my_hardware_possible/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-06T09:40:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4pyzt</id>
    <title>Mac Studio M3 Ultra: Is it worth the hype?</title>
    <updated>2025-03-06T07:15:15+00:00</updated>
    <author>
      <name>/u/_ggsa</name>
      <uri>https://old.reddit.com/user/_ggsa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see many people excited about the new Mac Studio with 512GB RAM (and M3 Ultra), but not everyone understands that LLM inference speed is directly tied to bandwidth, which has remained roughly the same. Also, there's a direct correlation between token/s and model size - so even if a 671B model fits in your VRAM, the benefits of 1-2 token/s (even with less than q4 quantization) are negligible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_ggsa"&gt; /u/_ggsa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4pyzt/mac_studio_m3_ultra_is_it_worth_the_hype/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4pyzt/mac_studio_m3_ultra_is_it_worth_the_hype/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4pyzt/mac_studio_m3_ultra_is_it_worth_the_hype/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-06T07:15:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j561r1</id>
    <title>Radeon VII Workstation + LM-Studio v0.3.11 + phi-4</title>
    <updated>2025-03-06T21:03:05+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/l1hb7qemv4ne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j561r1/radeon_vii_workstation_lmstudio_v0311_phi4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j561r1/radeon_vii_workstation_lmstudio_v0311_phi4/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-06T21:03:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4fj53</id>
    <title>Tool for finding max context for your GPU</title>
    <updated>2025-03-05T22:08:43+00:00</updated>
    <author>
      <name>/u/Daemonero</name>
      <uri>https://old.reddit.com/user/Daemonero</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I put this together over the past few days and thought it might be useful for others. I am still working on adding features and fixing some stalling issues, but it works well as is.&lt;/p&gt; &lt;p&gt;The MaxContextFinder is a tool that tests and determines the maximum usable context size for Ollama models by incrementally testing larger context windows while monitoring key performance metrics like token processing speed, VRAM usage, and response times. It helps users find the optimal balance between context size and performance for their specific hardware setup, stopping tests when it detects performance degradation or resource limits being reached, and provides recommendations for the largest reliable context window size.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Scionero/MaxContextFinder"&gt;Github Repo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daemonero"&gt; /u/Daemonero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4fj53/tool_for_finding_max_context_for_your_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4fj53/tool_for_finding_max_context_for_your_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4fj53/tool_for_finding_max_context_for_your_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T22:08:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5r3x0</id>
    <title>[PROMO] Perplexity AI PRO - 1 YEAR PLAN OFFER - 85% OFF</title>
    <updated>2025-03-07T15:35:20+00:00</updated>
    <author>
      <name>/u/uniquetees18</name>
      <uri>https://old.reddit.com/user/uniquetees18</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j5r3x0/promo_perplexity_ai_pro_1_year_plan_offer_85_off/"&gt; &lt;img alt="[PROMO] Perplexity AI PRO - 1 YEAR PLAN OFFER - 85% OFF" src="https://preview.redd.it/jdxlf4ofeane1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=29861542e0b0246134dcf4a884c96b4816a62120" title="[PROMO] Perplexity AI PRO - 1 YEAR PLAN OFFER - 85% OFF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title: We offer Perplexity AI PRO voucher codes for one year plan. &lt;/p&gt; &lt;p&gt;To Order: &lt;a href="https://cheapgpts.store/Perplexity"&gt;CHEAPGPT.STORE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Payments accepted: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;PayPal.&lt;/li&gt; &lt;li&gt;Revolut.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Duration: 12 Months&lt;/p&gt; &lt;p&gt;Feedback: &lt;a href="https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu"&gt;FEEDBACK POST&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uniquetees18"&gt; /u/uniquetees18 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jdxlf4ofeane1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j5r3x0/promo_perplexity_ai_pro_1_year_plan_offer_85_off/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j5r3x0/promo_perplexity_ai_pro_1_year_plan_offer_85_off/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-07T15:35:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1j57cnu</id>
    <title>What is the model doing right after submitting a prompt and before Thinking begins?</title>
    <updated>2025-03-06T21:58:29+00:00</updated>
    <author>
      <name>/u/cunasmoker69420</name>
      <uri>https://old.reddit.com/user/cunasmoker69420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been playing with some of the reasoning models, like the new QWQ-32b. I have enough VRAM to fit the whole model in memory. When I ask it a question (through Open-WebUI), I see that sort of &lt;em&gt;pending&lt;/em&gt; animation where something is going on in the background. This part lasts for several minutes sometimes. Only after a few minutes does the &lt;strong&gt;Thinking&lt;/strong&gt; section appear, where everything moves fast, followed by the equally fast answer (21 tokens/s). &lt;/p&gt; &lt;p&gt;I am trying to sort out what the bottleneck is in the process right after prompt submission and before Thinking begins. As far as I can tell, my GPUs are fully utilized during this early phase. I do see on the CPU that one core is maxed out at 100% during this process. &lt;/p&gt; &lt;p&gt;Is there some Ollama settings that should be changed?&lt;/p&gt; &lt;p&gt;EDIT: This also happens on subsequent prompts after the model is loaded into VRAM (the model first loads into VRAM within seconds from my observations anyway)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; right after prompt submission and before Thinking begins, I am stuck waiting for several minutes for something to happen and I'm wondering how to fix that&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cunasmoker69420"&gt; /u/cunasmoker69420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j57cnu/what_is_the_model_doing_right_after_submitting_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j57cnu/what_is_the_model_doing_right_after_submitting_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j57cnu/what_is_the_model_doing_right_after_submitting_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-06T21:58:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5bh6d</id>
    <title>Model / GPU Splitting Question</title>
    <updated>2025-03-07T01:07:33+00:00</updated>
    <author>
      <name>/u/Then_Conversation_19</name>
      <uri>https://old.reddit.com/user/Then_Conversation_19</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I noticed today when running different models on a dual 4090 rig that some modes balance GPU load evenly and others are either off balance or no balance (ie. single GPU) Has anyone else experienced this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Then_Conversation_19"&gt; /u/Then_Conversation_19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j5bh6d/model_gpu_splitting_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j5bh6d/model_gpu_splitting_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j5bh6d/model_gpu_splitting_question/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-07T01:07:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1j57u64</id>
    <title>Question about Ollama multi-GPU performance</title>
    <updated>2025-03-06T22:19:01+00:00</updated>
    <author>
      <name>/u/Pleasant-Sea-1380</name>
      <uri>https://old.reddit.com/user/Pleasant-Sea-1380</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I know that you can run ollama on a server with more than one GPU.&lt;br /&gt; This allows you to load models into both GPUs that are larger than one GPU's memory size.&lt;br /&gt; For example, a 30GB VRAM model can fit into two 16GB GPUs.&lt;/p&gt; &lt;p&gt;My question is regarding speed.&lt;br /&gt; Let's say that I have an ollama server with 16 connections/slots used at the same time, using one GPU which the complete model fits in. (eg imagine it's a 16GB GPU and the model is 10GB in size)&lt;br /&gt; Imagine my performance is not high enough, can I add a 2nd GPU, and keep using the smaller model 10GB and have the model in both GPUs at the same time and have double the inference processing speed ?&lt;/p&gt; &lt;p&gt;2nd question is, If I were to use a larger model that requires 2 GPUs, say the model is 30GB and I have 2x 16GB GPUs. Will the inference processing speed also be doubled by the 2 GPUs, or in this case the speed will be the same as if I had one GPU with 32GB VRAM and the same GPU performance ? &lt;/p&gt; &lt;p&gt;I hope I explained everything in a clear way...&lt;/p&gt; &lt;p&gt;Cheers and thanks for your time!,&lt;br /&gt; Terrence&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pleasant-Sea-1380"&gt; /u/Pleasant-Sea-1380 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j57u64/question_about_ollama_multigpu_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j57u64/question_about_ollama_multigpu_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j57u64/question_about_ollama_multigpu_performance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-06T22:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j51qoo</id>
    <title>Installing Ollama on Windows for old AMD GPUs</title>
    <updated>2025-03-06T18:04:05+00:00</updated>
    <author>
      <name>/u/Otherwise-Glove-8967</name>
      <uri>https://old.reddit.com/user/Otherwise-Glove-8967</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j51qoo/installing_ollama_on_windows_for_old_amd_gpus/"&gt; &lt;img alt="Installing Ollama on Windows for old AMD GPUs" src="https://external-preview.redd.it/T0QspHGEObZ_d8kLJKoJnovSydDRUPQNFDcq7YxbZ0M.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb93eb242c239030eb36580d878d0c9b485282de" title="Installing Ollama on Windows for old AMD GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Otherwise-Glove-8967"&gt; /u/Otherwise-Glove-8967 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=211ygEwb9eI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j51qoo/installing_ollama_on_windows_for_old_amd_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j51qoo/installing_ollama_on_windows_for_old_amd_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-06T18:04:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5692v</id>
    <title>Has anyone use multiple AMD GPUs on one machine? How did that work for you?</title>
    <updated>2025-03-06T21:11:39+00:00</updated>
    <author>
      <name>/u/halfam</name>
      <uri>https://old.reddit.com/user/halfam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 7900xt and have an option to get a 6800xt for free.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/halfam"&gt; /u/halfam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j5692v/has_anyone_use_multiple_amd_gpus_on_one_machine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j5692v/has_anyone_use_multiple_amd_gpus_on_one_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j5692v/has_anyone_use_multiple_amd_gpus_on_one_machine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-06T21:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j55qjm</id>
    <title>4x3090 Alibaba QwQ:32b Benchmark</title>
    <updated>2025-03-06T20:50:07+00:00</updated>
    <author>
      <name>/u/einthecorgi2</name>
      <uri>https://old.reddit.com/user/einthecorgi2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Another day another benchmark. &lt;/p&gt; &lt;p&gt;&lt;code&gt;➜ ~ ollama run qwq:32b-fp16 --verbose&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; Hello?&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Hello! How are you today?&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;total duration: 1.03327936s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;load duration: 32.759148ms&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval count: 10 token(s)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval duration: 91ms&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval rate: 109.89 tokens/s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval count: 12 token(s)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval duration: 908ms&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval rate: 13.22 tokens/s&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/einthecorgi2"&gt; /u/einthecorgi2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j55qjm/4x3090_alibaba_qwq32b_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j55qjm/4x3090_alibaba_qwq32b_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j55qjm/4x3090_alibaba_qwq32b_benchmark/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-06T20:50:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j58kf7</id>
    <title>What are some good small scale general models? (7b or less)</title>
    <updated>2025-03-06T22:51:17+00:00</updated>
    <author>
      <name>/u/Game-Lover44</name>
      <uri>https://old.reddit.com/user/Game-Lover44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im just wondering what are some good small models if any. I cant run massive models and bigger models take up more space. so is there a good choice for a small model? i mostly just want to use it for hard coding problems without gibberish being shot out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Game-Lover44"&gt; /u/Game-Lover44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j58kf7/what_are_some_good_small_scale_general_models_7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j58kf7/what_are_some_good_small_scale_general_models_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j58kf7/what_are_some_good_small_scale_general_models_7b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-06T22:51:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5ivy4</id>
    <title>How to solve this math prompt effectively with local llms?</title>
    <updated>2025-03-07T08:31:06+00:00</updated>
    <author>
      <name>/u/galdreth73</name>
      <uri>https://old.reddit.com/user/galdreth73</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j5ivy4/how_to_solve_this_math_prompt_effectively_with/"&gt; &lt;img alt="How to solve this math prompt effectively with local llms?" src="https://b.thumbs.redditmedia.com/-5qaB1z6HDr4p7HmdhxVzmIhKXMXBYq-zGwr05GXmck.jpg" title="How to solve this math prompt effectively with local llms?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/8js5evlva8ne1.png?width=753&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=15c7ac21131244ac5bad229af890b90955bf5f54"&gt;https://preview.redd.it/8js5evlva8ne1.png?width=753&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=15c7ac21131244ac5bad229af890b90955bf5f54&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;so I am experimenting a bit around with ollama locally and testing various models up to 32b, such as deepseek-r1, qwq, qwen2.5-coder or openthink. But they generally fail in solving the following task:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;can you use an numeric approach to calculate a twodimensional ellipse from five points. the output shall be the axis parameters a,b, center h,k, and the angle of the major axis to the x-axis of the coordinate system. I think an svd decomposition will help. I found out that you need at least 5 points to define an ellipse analytically, but these points have to be on a convex hull. Very important: please use python and make an example with a plot. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Either they fail by ending up in a broken approach or getting lost in endless loops. However, deepseek-r1 online was able to nail this in the first attempt. I wonder if you can give me some guidance, how I can manage to get a robust solution in local models. Do you think this is possible with 32b parameter constraints, or only feasible with much more parameters in a model?&lt;/p&gt; &lt;p&gt;Edit: Format and Image&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/galdreth73"&gt; /u/galdreth73 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j5ivy4/how_to_solve_this_math_prompt_effectively_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j5ivy4/how_to_solve_this_math_prompt_effectively_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j5ivy4/how_to_solve_this_math_prompt_effectively_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-07T08:31:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5ag6o</id>
    <title>QWQ 32B Q8_0 - 8x AMD Instinct Mi60 Server - Reaches 40 t/s - 2x Faster than 3090's ?!?</title>
    <updated>2025-03-07T00:17:32+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jjtm8u9fu5ne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j5ag6o/qwq_32b_q8_0_8x_amd_instinct_mi60_server_reaches/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j5ag6o/qwq_32b_q8_0_8x_amd_instinct_mi60_server_reaches/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-07T00:17:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5cpv4</id>
    <title>LLaDA Running on 8x AMD Instinct Mi60 Server</title>
    <updated>2025-03-07T02:10:53+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ju63pcwte6ne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j5cpv4/llada_running_on_8x_amd_instinct_mi60_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j5cpv4/llada_running_on_8x_amd_instinct_mi60_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-07T02:10:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5q9je</id>
    <title>Downloading model manifest and binaries in dockerfile with base ollama image?</title>
    <updated>2025-03-07T15:05:13+00:00</updated>
    <author>
      <name>/u/darklightning_2</name>
      <uri>https://old.reddit.com/user/darklightning_2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to run deepseek-r1 with ollama in docker but it downloads the model everytime it make a container. &lt;/p&gt; &lt;p&gt;Can I bake the model files (binaries and manifest) in the docker image to make a &amp;quot;deepseek-ollama&amp;quot; image&lt;/p&gt; &lt;p&gt;It will speed up everytime I have to deploy it to another system. It also helps in debugging many models &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darklightning_2"&gt; /u/darklightning_2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j5q9je/downloading_model_manifest_and_binaries_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j5q9je/downloading_model_manifest_and_binaries_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j5q9je/downloading_model_manifest_and_binaries_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-07T15:05:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5qbtc</id>
    <title>How to pass text file in as prompt with Powershell on Windows?</title>
    <updated>2025-03-07T15:07:32+00:00</updated>
    <author>
      <name>/u/TriodeTopologist</name>
      <uri>https://old.reddit.com/user/TriodeTopologist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I use Ollama with Powershell in windows. I can't figure out how to send in a prompt from a text file on the command line. I have tried several methods that Powershell uses to read a file and pass the output to another command but when the prompt has formatting such as ', : &amp;quot; that seems to break it at some point. &lt;/p&gt; &lt;p&gt;Can anyone give me advice on how to send in a prompt which includes text formatting, beyond copying and pasting? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TriodeTopologist"&gt; /u/TriodeTopologist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j5qbtc/how_to_pass_text_file_in_as_prompt_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j5qbtc/how_to_pass_text_file_in_as_prompt_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j5qbtc/how_to_pass_text_file_in_as_prompt_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-07T15:07:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5v1u8</id>
    <title>Built my first VScode extension Ollama Dev Companion</title>
    <updated>2025-03-07T17:51:53+00:00</updated>
    <author>
      <name>/u/StayHigh24-7</name>
      <uri>https://old.reddit.com/user/StayHigh24-7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Guys!, I have build a VScode extension to provide inline suggestions using current context and variables in scope using any model running in Ollama. I have also added a support to update the Ollama host if someone has private server running with bigger AI models on Ollama.&lt;br /&gt; Additionally I have added a chat window for asking questions using the files or whole codebase.&lt;br /&gt; I would like to get some feedback. If you have any suggestions to make the extension better I would really appreciate it.&lt;/p&gt; &lt;p&gt;Here is my extension link:&lt;br /&gt; &lt;a href="https://marketplace.visualstudio.com/items?itemName=Gnana997.ollama-dev-companion"&gt;https://marketplace.visualstudio.com/items?itemName=Gnana997.ollama-dev-companion&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StayHigh24-7"&gt; /u/StayHigh24-7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j5v1u8/built_my_first_vscode_extension_ollama_dev/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j5v1u8/built_my_first_vscode_extension_ollama_dev/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j5v1u8/built_my_first_vscode_extension_ollama_dev/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-07T17:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j654yt</id>
    <title>QwQ-32B - Question about Taiwan</title>
    <updated>2025-03-08T00:48:52+00:00</updated>
    <author>
      <name>/u/Proud_Fox_684</name>
      <uri>https://old.reddit.com/user/Proud_Fox_684</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j654yt/qwq32b_question_about_taiwan/"&gt; &lt;img alt="QwQ-32B - Question about Taiwan" src="https://b.thumbs.redditmedia.com/Gqy8n0BrjVRZ8vVniW2lbbD_VRuXh2VhJ75cLxCx72Q.jpg" title="QwQ-32B - Question about Taiwan" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/momibn125dne1.png?width=3188&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a086842ac3b5164278aeeb40f4c9c19bb5f2236f"&gt;https://preview.redd.it/momibn125dne1.png?width=3188&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a086842ac3b5164278aeeb40f4c9c19bb5f2236f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proud_Fox_684"&gt; /u/Proud_Fox_684 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j654yt/qwq32b_question_about_taiwan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j654yt/qwq32b_question_about_taiwan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j654yt/qwq32b_question_about_taiwan/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-08T00:48:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j66pg3</id>
    <title>RLAMA -- A document AI question-answering tool that connects to your local Ollama models.</title>
    <updated>2025-03-08T02:09:46+00:00</updated>
    <author>
      <name>/u/DonTizi</name>
      <uri>https://old.reddit.com/user/DonTizi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey!&lt;/p&gt; &lt;p&gt;I developed RLAMA to solve a straightforward but frustrating problem: how to easily query my own documents with a local LLM without using cloud services.&lt;/p&gt; &lt;h1&gt;What it actually is&lt;/h1&gt; &lt;p&gt;RLAMA is a command-line tool that bridges your local documents and Ollama models. It implements RAG (Retrieval-Augmented Generation) in a minimalist way:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Index a folder of documents rlama rag llama3 project-docs ./documentation # Start an interactive session rlama run project-docs &amp;gt; How does the authentication module work? &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;You point the tool to a folder containing your files (.txt, .md, .pdf, source code, etc.)&lt;/li&gt; &lt;li&gt;RLAMA extracts text from the documents and generates embeddings via Ollama&lt;/li&gt; &lt;li&gt;When you ask a question, it retrieves relevant passages and sends them to the model&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The tool handles many formats automatically. For PDFs, it first tries pdftotext, then tesseract if necessary. For binary files, it has several fallback methods to extract what it can.&lt;/p&gt; &lt;h1&gt;Problems it solves&lt;/h1&gt; &lt;p&gt;I use it daily for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Finding information in old technical documents without having to reread everything&lt;/li&gt; &lt;li&gt;Exploring code I'm not familiar with (e.g., &amp;quot;explain how part X works&amp;quot;)&lt;/li&gt; &lt;li&gt;Creating summaries of long documents&lt;/li&gt; &lt;li&gt;Querying my research or meeting notes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The real time-saver comes from being able to ask questions instead of searching for keywords. For example, I can ask &amp;quot;What are the possible errors in the authentication API?&amp;quot; and get consolidated answers from multiple files.&lt;/p&gt; &lt;h1&gt;Why use it?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;It's simple&lt;/strong&gt;: four commands are enough (rag, run, list, delete)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It's local&lt;/strong&gt;: no data is sent over the internet&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It's lightweight&lt;/strong&gt;: no need for Docker or a complete stack&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It's flexible&lt;/strong&gt;: compatible with all Ollama models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I created it because other solutions were either too complex to configure or required sending my documents to external services.&lt;/p&gt; &lt;p&gt;If you already have Ollama installed and are looking for a simple way to query your documents, this might be useful for you.&lt;/p&gt; &lt;h1&gt;In conclusion&lt;/h1&gt; &lt;p&gt;I've found that in discussions on &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; point to several pressing needs for local RAG without cloud dependencies: we need to simplify the ingestion of data (PDFs, web pages, videos...) via tools that can automatically transform them into usable text, reduce hardware requirements or better leverage common hardware (model quantization, multi-GPU support) to improve performance, and integrate advanced retrieval methods (hybrid search, rerankers, etc.) to increase answer reliability.&lt;/p&gt; &lt;p&gt;The emergence of integrated solutions (OpenWebUI, LangChain/Langroid, RAGStack, etc.) moves in this direction: the ultimate goal is a tool where users only need to provide their local files to benefit from an AI assistant trained on their own knowledge, while remaining 100% private and local so I wanted to develop something easy to use!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/dontizi/rlama"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DonTizi"&gt; /u/DonTizi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j66pg3/rlama_a_document_ai_questionanswering_tool_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j66pg3/rlama_a_document_ai_questionanswering_tool_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j66pg3/rlama_a_document_ai_questionanswering_tool_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-08T02:09:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5wd4b</id>
    <title>Feature found in llama3.1:70b-q2_k</title>
    <updated>2025-03-07T18:41:12+00:00</updated>
    <author>
      <name>/u/Onkululu</name>
      <uri>https://old.reddit.com/user/Onkululu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j5wd4b/feature_found_in_llama3170bq2_k/"&gt; &lt;img alt="Feature found in llama3.1:70b-q2_k" src="https://preview.redd.it/ren7bihlbbne1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73775fb9cb64b6043c8798f2243934f8c092a44e" title="Feature found in llama3.1:70b-q2_k" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to test llama3.1 in polish. I’ve asked it „what model are you?” and got this response, sure to say i was quite suprised XD&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Onkululu"&gt; /u/Onkululu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ren7bihlbbne1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j5wd4b/feature_found_in_llama3170bq2_k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j5wd4b/feature_found_in_llama3170bq2_k/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-07T18:41:12+00:00</published>
  </entry>
</feed>
