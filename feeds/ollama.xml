<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-28T13:30:32+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1n0fuf4</id>
    <title>This is just a test but works</title>
    <updated>2025-08-26T08:00:18+00:00</updated>
    <author>
      <name>/u/bercha9998</name>
      <uri>https://old.reddit.com/user/bercha9998</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n0fuf4/this_is_just_a_test_but_works/"&gt; &lt;img alt="This is just a test but works" src="https://preview.redd.it/k714lga1mblf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=68ce59bbd985f0f4a36ed01cef0bffbe43a3df7e" title="This is just a test but works" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Old hardware alert!&lt;/p&gt; &lt;p&gt;hp z240 64gb ecc i7-6700 stock psu 400w 3 × quadro p2000 cards&lt;/p&gt; &lt;p&gt;Uses under heavt ollama load ~190-200w at the electrical point with a digital meter.&lt;/p&gt; &lt;p&gt;either 100% gpu with 90% utilization or even split 50/50 on 30b 64k context models like qwen3-coder&lt;/p&gt; &lt;p&gt;You get abou 1t/s in split and 20+ t/s in gpu full small models.&lt;/p&gt; &lt;p&gt;qwen3 7 24k qwen3 14 8k qwen3 4 thinking 40k&lt;/p&gt; &lt;p&gt;anyway just testing stuff.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bercha9998"&gt; /u/bercha9998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k714lga1mblf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n0fuf4/this_is_just_a_test_but_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n0fuf4/this_is_just_a_test_but_works/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-26T08:00:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0ht8x</id>
    <title>Local model for coding</title>
    <updated>2025-08-26T10:08:27+00:00</updated>
    <author>
      <name>/u/vital101</name>
      <uri>https://old.reddit.com/user/vital101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm having a hard time finding benchmarks for coding tasks that are focused on models I can run on Ollama locally. Ideally something with &amp;lt; 30B parameters that can fit into my video cards RAM (RTX 4070 TI Super). Where do you all look for comparisons? Anecdotal suggestions are fine too. The few leader boards that I've found don't include parameter counts on their rankings, so they aren't very useful to me. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vital101"&gt; /u/vital101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n0ht8x/local_model_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n0ht8x/local_model_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n0ht8x/local_model_for_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-26T10:08:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0sq6s</id>
    <title>Ubuntu Docker Support in Cua with Kasm</title>
    <updated>2025-08-26T17:48:52+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n0sq6s/ubuntu_docker_support_in_cua_with_kasm/"&gt; &lt;img alt="Ubuntu Docker Support in Cua with Kasm" src="https://external-preview.redd.it/emNod2ZxeTBqZWxmMZZrLHb2o7dKB6a-Vq6jd65fuBOTo_G_y7VwV4PvoF0D.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf101ea36f764576e75af051279c717e16e95025" title="Ubuntu Docker Support in Cua with Kasm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With our Cua Agent framework, we kept seeing the same pattern: people were excited to try it… and then lost 20 minutes wrestling with VM setup. Hypervisor configs, nested virt errors, giant image downloads—by the time a desktop booted, most gave up before an agent ever clicked a button.&lt;/p&gt; &lt;p&gt;So we made the first step stupid-simple: 👉 &lt;strong&gt;Ubuntu desktops in Docker with Kasm.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A full Linux GUI inside Docker, viewable in your browser. Runs the same on macOS, Windows, and Linux. Cold-starts in seconds. You can even spin up multiple desktops in parallel on one machine.&lt;/p&gt; &lt;p&gt;```python from computer import Computer&lt;/p&gt; &lt;p&gt;computer = Computer( os_type=&amp;quot;linux&amp;quot;, provider_type=&amp;quot;docker&amp;quot;, image=&amp;quot;trycua/cua-ubuntu:latest&amp;quot;, name=&amp;quot;my-desktop&amp;quot; )&lt;/p&gt; &lt;p&gt;await computer.run() ```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Docker over QEMU/KVM?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Boots in seconds, not minutes.&lt;/li&gt; &lt;li&gt;No hypervisor or nested virt drama.&lt;/li&gt; &lt;li&gt;Much lighter to operate and script.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We still use VMs when needed (macOS with lume on Apple.Virtualization, Windows Sandbox on Windows) for native OS, kernel features, or GPU passthrough. But for demos and most local agent workflows, containers win.&lt;/p&gt; &lt;p&gt;Point an agent at it like this:&lt;/p&gt; &lt;p&gt;```python from agent import ComputerAgent&lt;/p&gt; &lt;p&gt;agent = ComputerAgent(&amp;quot;openrouter/z-ai/glm-4.5v&amp;quot;, tools=[computer]) async for _ in agent.run(&amp;quot;Click on the search bar and type 'hello world'&amp;quot;): pass ```&lt;/p&gt; &lt;p&gt;That’s it: a controlled, browser-accessible desktop your model can drive.&lt;/p&gt; &lt;p&gt;📖 Blog: &lt;a href="https://www.trycua.com/blog/ubuntu-docker-support"&gt;https://www.trycua.com/blog/ubuntu-docker-support&lt;/a&gt; 💻 Repo: &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fxg95681jelf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n0sq6s/ubuntu_docker_support_in_cua_with_kasm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n0sq6s/ubuntu_docker_support_in_cua_with_kasm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-26T17:48:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1n157dk</id>
    <title>Issues with VRAM</title>
    <updated>2025-08-27T02:26:51+00:00</updated>
    <author>
      <name>/u/x90man</name>
      <uri>https://old.reddit.com/user/x90man</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there a while back i downloaded ollama and deepseek-r1:7b and it didnt work because i didnt have enough vram 16gb vs 20gb required but now any time i try to run any other model it doesnt work and crashes just like 7b did. I have deleted and redownloaded ollama and all the models multiple times and also deleted the blobs and otherwise and all of the stuff in localappdata. Much help needed&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/x90man"&gt; /u/x90man &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n157dk/issues_with_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n157dk/issues_with_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n157dk/issues_with_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T02:26:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n11uld</id>
    <title>qwen3:4b confused by naive question in Spanish</title>
    <updated>2025-08-26T23:50:12+00:00</updated>
    <author>
      <name>/u/NakamotoScheme</name>
      <uri>https://old.reddit.com/user/NakamotoScheme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've just installed ollama to experiment with it, and this is so weird that I need to share it...&lt;/p&gt; &lt;p&gt;What would be the moral of the story? Should I conclude that small models are not very good with languages other than English?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ ollama run qwen3:4b pulling manifest [...] success &amp;gt;&amp;gt;&amp;gt; Dime cuál es la capital de España Thinking... Okay, the user is asking for the capital of Spain. Let me think. First, I know that Spain's capital is Madrid. Wait, but I should make sure. Sometimes people might confuse it with other cities like Barcelona or Seville. But no, Madrid is definitely the capital. Let me recall some facts. Spain's capital has been Madrid since the 16th century, right? Yeah, Madrid is the capital city. I think that's correct. Let me double-check to be sure. Maybe some historical context? Like, before Madrid was the capital, but no, I think Madrid has been the capital for a long time. For example, during the Spanish monarchy, Madrid became the capital in the 16th century. So, the answer should be Madrid. Wait, but the user is asking in Spanish, so they might be a Spanish speaker or someone learning Spanish. The question is &amp;quot;Dime cuál es la capital de España&amp;quot;. So the answer should be in Spanish too? Wait, the user wrote the question in Spanish, but they might expect the answer in English or Spanish? The initial instruction says &amp;quot;Dime cuál es la capital de España&amp;quot; which translates to &amp;quot;Tell me what is the capital of Spain&amp;quot;. The user is probably expecting the answer in Spanish, but the assistant's response is supposed to be in the same language as the query? Wait, no, the user is asking me (the assistant) to respond. The problem says &amp;quot;Dime cuál es la capital de España&amp;quot; so the user is speaking Spanish, but the answer should be in Spanish? Wait, the instruction says &amp;quot;Dime cuál es la capital de España&amp;quot; so the user is asking in Spanish. But the assistant's response here is supposed to be in English? Wait, no, the problem is presented in the context of the assistant responding to a user's query. The user's message is in Spanish, but the assistant is supposed to respond in English? Wait, the initial problem says &amp;quot;Dime cuál es la capital de España&amp;quot; which is Spanish, but the user is asking me (the assistant) to answer. The user wants the answer in English, because the system prompt is in English. Wait, the problem is a bit confusing. Wait, the user's query is in Spanish, but the assistant (me) is supposed to respond in English? Because the system prompt says &amp;quot;You are an assistant that helps users with their questions.&amp;quot; The user wrote the question in Spanish, but the expected response might be in English. Let me check the original problem statement. The user says: &amp;quot;Dime cuál es la capital de España&amp;quot; which translates to &amp;quot;Tell me what is the capital of Spain&amp;quot;. The user is asking in Spanish, but the assistant is supposed to answer in English? Or maybe the user expects the answer in Spanish. Hmm. The problem says &amp;quot;Dime cuál es la capital de España&amp;quot; so the user is using Spanish. But the assistant's response is supposed to be in English, as per the system prompt's language. Wait, the problem might be that the user is asking in Spanish, but the assistant is to respond in English. For example, in many cases, the assistant responds in the same language as the query, but sometimes the system is set to English. Wait, the problem statement here says: &amp;quot;Dime cuál es la capital de España&amp;quot; and the user is asking me (the assistant) to answer. Since the user wrote in Spanish, but the system prompt for the assistant is in English (as per the initial setup), the assistant should respond in English. Let me confirm. In the context of the problem, the user is asking in Spanish, but the assistant's response is expected to be in English. So the answer would be &amp;quot;Madrid&amp;quot;. Wait, but the user might want the answer in Spanish. Wait, the problem says &amp;quot;Dime cuál es la capital de España&amp;quot; which is Spanish, but the assistant's response here is part of a test where the assistant is supposed to respond in English. Let me think. In the example given in the problem statement, the user's question is in Spanish, but the assistant is to answer in English. For instance, in the problem, the user says &amp;quot;Dime cuál es la capital de España&amp;quot;, so the correct answer in English is &amp;quot;Madrid&amp;quot;. So I should write &amp;quot;Madrid&amp;quot; as the answer. Wait, but the user might expect the answer in Spanish. Let me check. If the user is asking in Spanish, maybe they want the answer in Spanish. But the problem says the assistant is to respond in English. Wait, the initial instruction for the assistant says: &amp;quot;You are an assistant that helps users with their questions.&amp;quot; So the assistant's language is determined by the user's query. Hmm, but in this case, the user wrote in Spanish, so maybe the assistant should respond in Spanish. However, the problem might be testing if the assistant knows the capital of Spain regardless of language. Wait, the problem here is presented in a way that the user is asking in Spanish, but the assistant is supposed to answer in English. Let me see. In the problem statement, the user says: &amp;quot;Dime cuál es la capital de a España&amp;quot; (but the user wrote &amp;quot;España&amp;quot; correctly). The assistant's response should be in English. For example, in many such problems, the user's query is in another language, but the answer is in English. So, the answer is Madrid. So the response would be &amp;quot;Madrid&amp;quot;. Wait, but to be precise, the capital of Spain is Madrid. So the answer is Madrid. Let me make sure there's no trick here. Sometimes people confuse with other cities, but no, Madrid is the capital. Yes, definitely. So the answer is Madrid. ...done thinking. Madrid &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NakamotoScheme"&gt; /u/NakamotoScheme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n11uld/qwen34b_confused_by_naive_question_in_spanish/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n11uld/qwen34b_confused_by_naive_question_in_spanish/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n11uld/qwen34b_confused_by_naive_question_in_spanish/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-26T23:50:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1cn1m</id>
    <title>FREE Local AI Meeting Note-Taker - Hyprnote - Obsidian - Ollama</title>
    <updated>2025-08-27T09:47:54+00:00</updated>
    <author>
      <name>/u/NoobMLDude</name>
      <uri>https://old.reddit.com/user/NoobMLDude</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoobMLDude"&gt; /u/NoobMLDude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1n0zcx2/free_local_ai_meeting_notetaker_hyprnote_obsidian/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1cn1m/free_local_ai_meeting_notetaker_hyprnote_obsidian/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1cn1m/free_local_ai_meeting_notetaker_hyprnote_obsidian/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T09:47:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1dap9</id>
    <title>I get "request timed out after 60 seconds" in vs code for ollama</title>
    <updated>2025-08-27T10:26:13+00:00</updated>
    <author>
      <name>/u/Aggressive_Mix_4258</name>
      <uri>https://old.reddit.com/user/Aggressive_Mix_4258</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n1dap9/i_get_request_timed_out_after_60_seconds_in_vs/"&gt; &lt;img alt="I get &amp;quot;request timed out after 60 seconds&amp;quot; in vs code for ollama" src="https://a.thumbs.redditmedia.com/P0rDQj51g3s7qBZ4Kt9OaflTMujf3qzv_CAGeyVssZ0.jpg" title="I get &amp;quot;request timed out after 60 seconds&amp;quot; in vs code for ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys, I have installed ollama and vs code and then installed Cline and Continue. Ollama is working very well but when I try to use it in Cline or Continue, I get &amp;quot;request timed out after 60 seconds&amp;quot; error in Cline and an error as you can see in the screenshot. Everything is done as these videos: &lt;a href="https://www.youtube.com/watch?v=aM0sS5TIaVI"&gt;https://www.youtube.com/watch?v=aM0sS5TIaVI&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=P5YXTTS8OFk"&gt;https://www.youtube.com/watch?v=P5YXTTS8OFk&lt;/a&gt; Then why doesn't it work for me? please keep in mind that I can use &lt;a href="http://openrouter.ai"&gt;openrouter.ai&lt;/a&gt; services via API key and without any problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aggressive_Mix_4258"&gt; /u/Aggressive_Mix_4258 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n1dap9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1dap9/i_get_request_timed_out_after_60_seconds_in_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1dap9/i_get_request_timed_out_after_60_seconds_in_vs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T10:26:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1n189ws</id>
    <title>Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler</title>
    <updated>2025-08-27T05:07:34+00:00</updated>
    <author>
      <name>/u/Fluid-Engineering769</name>
      <uri>https://old.reddit.com/user/Fluid-Engineering769</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n189ws/websitecrawler_extract_data_from_websites_in_llm/"&gt; &lt;img alt="Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler" src="https://external-preview.redd.it/okdIR3Y4TwJ-9MFnT63LQUQFrkGy2DIAhPYXAsFmGGA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f2cd37c695d3768b0d4a384bc1e0b870ed8b00c" title="Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluid-Engineering769"&gt; /u/Fluid-Engineering769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/pc8544/Website-Crawler"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n189ws/websitecrawler_extract_data_from_websites_in_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n189ws/websitecrawler_extract_data_from_websites_in_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T05:07:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1cdwy</id>
    <title>Model for Xeon 32Go + web search + documents storage.</title>
    <updated>2025-08-27T09:31:59+00:00</updated>
    <author>
      <name>/u/Ok_Party_1645</name>
      <uri>https://old.reddit.com/user/Ok_Party_1645</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, This is my first post here but I have been reading you for a while. Here is some context, I’m Linux, command line, Ollama and llm literate to put it that way. I have run and tested dozens of models with the goal of using them as a personal assistant, kind of a portable Wikipedia and helper with various tedious tasks.&lt;/p&gt; &lt;p&gt;So far my preference was in the granite models because I designed a small set of standard « cognitive » tests and those models behaved the best.&lt;/p&gt; &lt;p&gt;I was running the model on a portable device (clockwork Uconsole) so I was limited to compute module 4 or 5 depending the period and always with 8GB of ram. That means that I was running 3b to 7b models.&lt;/p&gt; &lt;p&gt;Now I have a private server with a Xeon, 32GB ram, ssd and fiber connection. I want to scale up. So my question is three folds:&lt;/p&gt; &lt;p&gt;-what model would you recommend for those specs knowing my preference is mostly a chatbot with long context and great logical skills&lt;/p&gt; &lt;p&gt;-how can I give it the ability to search the web?&lt;/p&gt; &lt;p&gt;-how can I feed it documents of my choice so that it saves them for future reference? (For example, the full text of a given law so that it could search it in later queries) So it has to store those documents in a persistent manner.&lt;/p&gt; &lt;p&gt;I heard of vectorial databases but never got to test.&lt;/p&gt; &lt;p&gt;So yeah, sorry for the lengthy post, I hope someone can point me in the right direction…&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;p&gt;Edit : I initially didn’t realize it but being french speaking Belgian I used Go instead of GB. As it was wisely notified to me I now edited the original text, sorry for the confusing units, I hope it’s more legible that way 😉&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Party_1645"&gt; /u/Ok_Party_1645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1cdwy/model_for_xeon_32go_web_search_documents_storage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1cdwy/model_for_xeon_32go_web_search_documents_storage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1cdwy/model_for_xeon_32go_web_search_documents_storage/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T09:31:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1b74d</id>
    <title>Ollama app parameters?</title>
    <updated>2025-08-27T08:12:54+00:00</updated>
    <author>
      <name>/u/Dylan31245</name>
      <uri>https://old.reddit.com/user/Dylan31245</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i installed the ollama app and installed qwen3:8b. While the model runs theres a lot of repetition and it tends to think infinitely. Whenever i go to settings however, the only visible option is context size. I like the app more than running in terminal, so is there any way to change the parameters in the app? Sorry if this is in documentations! OS is windows 10.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dylan31245"&gt; /u/Dylan31245 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1b74d/ollama_app_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1b74d/ollama_app_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1b74d/ollama_app_parameters/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T08:12:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1hh1o</id>
    <title>Quadro K2200 4g with Gemma3 (3.3G)</title>
    <updated>2025-08-27T13:45:09+00:00</updated>
    <author>
      <name>/u/Mother-Ad4153</name>
      <uri>https://old.reddit.com/user/Mother-Ad4153</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;Is it okay to run Gemma3 (3.3G) on Quadro K2200 4g?&lt;/p&gt; &lt;p&gt;I've asked Gemini. It told me it's not okay.&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mother-Ad4153"&gt; /u/Mother-Ad4153 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1hh1o/quadro_k2200_4g_with_gemma3_33g/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1hh1o/quadro_k2200_4g_with_gemma3_33g/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1hh1o/quadro_k2200_4g_with_gemma3_33g/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T13:45:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1nuyw</id>
    <title>Ollama loads model always to CPU when called from application</title>
    <updated>2025-08-27T17:45:11+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have nvidia GPU 32GB vram and Ubuntu 24.04 which runs inside a VM.&lt;br /&gt; When the VM is rebooted and a app calls ollama, it load gemma3 12b to CPU.&lt;br /&gt; When the VM is rebooted, and I write in command line: Ollama run...the model is loaded to GPU.&lt;br /&gt; Whats the issue? User permissions etc? Why there are no clear instructions how to set the environment in the ollama.service? &lt;/p&gt; &lt;p&gt;&lt;code&gt;[Service]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Environment=&amp;quot;OLLAMA_HOST=0.0.0.0:11434&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Environment=&amp;quot;OLLAMA_KEEP_ALIVE=2200&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Environment=&amp;quot;OLLAMA_MAX_LOADED_MODELS=2&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Environment=&amp;quot;OLLAMA_NUM_PARALLEL=2&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Environment=&amp;quot;OLLAMA_MAX_QUEUE=512&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1nuyw/ollama_loads_model_always_to_cpu_when_called_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1nuyw/ollama_loads_model_always_to_cpu_when_called_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1nuyw/ollama_loads_model_always_to_cpu_when_called_from/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T17:45:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1hyr5</id>
    <title>Building a local Ai PC</title>
    <updated>2025-08-27T14:04:19+00:00</updated>
    <author>
      <name>/u/KCCarpenter5739</name>
      <uri>https://old.reddit.com/user/KCCarpenter5739</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Advice needed: I’m looking at micro center, building my own pc. I’m thinking of using Ryzen 9 cpu, Msi pro x870e-p wifi mobo, Corsair 32gb ram (128gb total), Samsung pro 4 tb nvme, liquid cooling aio, 1300w Psu, LIAN li O11D XL case.&lt;/p&gt; &lt;p&gt;GPU is where I’m getting stuck, the mobo has 3 slots (yes I know the secondary slots are bottlenecked), I’m thinking of running a 5060 TI 16gb primary, 3060 rtx for offloading and my old 1070ti for offloading more. Is this a good setup? Am I completely wrong? Never built custom before&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KCCarpenter5739"&gt; /u/KCCarpenter5739 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1hyr5/building_a_local_ai_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1hyr5/building_a_local_ai_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1hyr5/building_a_local_ai_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T14:04:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1n18h8q</id>
    <title>GPT-OSS Web Search</title>
    <updated>2025-08-27T05:19:42+00:00</updated>
    <author>
      <name>/u/No-Engineering3583</name>
      <uri>https://old.reddit.com/user/No-Engineering3583</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The updates and blog posts about gpt-oss support and Ollama v0.11 mention web search support: &amp;quot;Ollama is providing a built-in web search that can be optionally enabled to augment the model with the latest information&amp;quot;&lt;/p&gt; &lt;p&gt;How is this being provided? How is it enabled/disabled? Is it only in the Ollama app or is it available when using the CLI or python libraries to access the model hosted on a local Ollama instance?&lt;/p&gt; &lt;p&gt;EDIT for clarity: I am aware there are other ways to do this, I've even coded personal solutions. My inquiry is about how a feature they semi-announced works, if it is available, and how to use it. I would like to be able to compare it against other solutions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Engineering3583"&gt; /u/No-Engineering3583 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n18h8q/gptoss_web_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n18h8q/gptoss_web_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n18h8q/gptoss_web_search/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T05:19:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1yfeu</id>
    <title>Questions about Agents</title>
    <updated>2025-08-28T00:50:13+00:00</updated>
    <author>
      <name>/u/Street_Equivalent_45</name>
      <uri>https://old.reddit.com/user/Street_Equivalent_45</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Fellow ai experts. &lt;/p&gt; &lt;p&gt;I am currently making agent using Ollama in local agent with langchains Because of costs😂 Is there anyways to make agent better not using chatgpt or claudes or having no coat issues? I know maybe impossible but I really know what you guys think&lt;/p&gt; &lt;p&gt;Thanks for reading my comments&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Street_Equivalent_45"&gt; /u/Street_Equivalent_45 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1yfeu/questions_about_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1yfeu/questions_about_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1yfeu/questions_about_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T00:50:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1mg7v</id>
    <title>Tool calls keep ending up as responses</title>
    <updated>2025-08-27T16:53:07+00:00</updated>
    <author>
      <name>/u/thewiirocks</name>
      <uri>https://old.reddit.com/user/thewiirocks</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n1mg7v/tool_calls_keep_ending_up_as_responses/"&gt; &lt;img alt="Tool calls keep ending up as responses" src="https://preview.redd.it/rtfarn7pcllf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=228e5a9c14dd8871eb8431600e3a1762b39ccd05" title="Tool calls keep ending up as responses" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've given llama3.2 a tool to run reports using an OLAP schema. When the LLM triggers the tool call, everything works well. The problem I'm having is that the tool call is often ending up as a regular response rather than a tool call. &lt;/p&gt; &lt;p&gt;Here is the exact response text:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;model&amp;quot;: &amp;quot;llama3.2&amp;quot;, &amp;quot;created_at&amp;quot;: &amp;quot;2025-08-27T16:48:54.552815Z&amp;quot;, &amp;quot;message&amp;quot;: { &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;{\&amp;quot;name\&amp;quot;: \&amp;quot;generateReport\&amp;quot;, \&amp;quot;parameters\&amp;quot;: {\&amp;quot;arg0\&amp;quot;: \&amp;quot;[\\\&amp;quot;Franchise Name\\\&amp;quot;, \\\&amp;quot;Product Name\\\&amp;quot;]\&amp;quot;, \&amp;quot;arg1\&amp;quot;: \&amp;quot;[\\\&amp;quot;Units Sold\\\&amp;quot;, \\\&amp;quot;Total Sale \\$\\\&amp;quot;]\&amp;quot;}}&amp;quot; }, &amp;quot;done&amp;quot;: false } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is becoming a huge frustration to reliable operation. I could try and intercept these situations, but that feels like a bit of a hack. (Which I supposed describes a lot of LLM interactions. 😅)&lt;/p&gt; &lt;p&gt;Does anyone know why this is happening and how to resolve? Or do you just intercept the call yourself?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thewiirocks"&gt; /u/thewiirocks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rtfarn7pcllf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1mg7v/tool_calls_keep_ending_up_as_responses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1mg7v/tool_calls_keep_ending_up_as_responses/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T16:53:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1rc2y</id>
    <title>llama.ui - minimal, privacy focused chat interface</title>
    <updated>2025-08-27T19:55:53+00:00</updated>
    <author>
      <name>/u/COBECT</name>
      <uri>https://old.reddit.com/user/COBECT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n1rc2y/llamaui_minimal_privacy_focused_chat_interface/"&gt; &lt;img alt="llama.ui - minimal, privacy focused chat interface" src="https://preview.redd.it/6g2icqwi96lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93145b5e6ac2c5f127d14e540cb4261819454a6b" title="llama.ui - minimal, privacy focused chat interface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/COBECT"&gt; /u/COBECT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6g2icqwi96lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1rc2y/llamaui_minimal_privacy_focused_chat_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1rc2y/llamaui_minimal_privacy_focused_chat_interface/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T19:55:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1xjnb</id>
    <title>Agentic: Your 3B local model becomes a thoughtful research partner.</title>
    <updated>2025-08-28T00:09:22+00:00</updated>
    <author>
      <name>/u/Thin_Beat_9072</name>
      <uri>https://old.reddit.com/user/Thin_Beat_9072</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n1xjnb/agentic_your_3b_local_model_becomes_a_thoughtful/"&gt; &lt;img alt="Agentic: Your 3B local model becomes a thoughtful research partner." src="https://preview.redd.it/915rf37hpmlf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=7d625aa9b27cf3e56d522a230d24e40702faf46e" title="Agentic: Your 3B local model becomes a thoughtful research partner." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thin_Beat_9072"&gt; /u/Thin_Beat_9072 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/915rf37hpmlf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1xjnb/agentic_your_3b_local_model_becomes_a_thoughtful/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1xjnb/agentic_your_3b_local_model_becomes_a_thoughtful/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T00:09:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n25rhd</id>
    <title>Ollama help!</title>
    <updated>2025-08-28T07:27:46+00:00</updated>
    <author>
      <name>/u/IndependentBug490</name>
      <uri>https://old.reddit.com/user/IndependentBug490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n25rhd/ollama_help/"&gt; &lt;img alt="Ollama help!" src="https://b.thumbs.redditmedia.com/PwS1yOmD3a8ondnY40EzM0_SuNAdB78gHX0Oa5mdvbw.jpg" title="Ollama help!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im making a workflow in n8n using the ollama chat model, i used it as an alternative to google gemini chat model. But it keeps in erroring and the output is fetch failed. Im self host, and im only using chatgpt to help me. im a totally beginner when it comes to n8n, hopefully anyone can help me with this. thank you. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ypp0m2h1qplf1.png?width=1382&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9f53630bf8fcbcb53f4ac0f979039b75f9d1196c"&gt;https://preview.redd.it/ypp0m2h1qplf1.png?width=1382&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9f53630bf8fcbcb53f4ac0f979039b75f9d1196c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IndependentBug490"&gt; /u/IndependentBug490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n25rhd/ollama_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n25rhd/ollama_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n25rhd/ollama_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T07:27:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2bwxx</id>
    <title>When is it useful to use a prompt?</title>
    <updated>2025-08-28T13:14:51+00:00</updated>
    <author>
      <name>/u/9acca9</name>
      <uri>https://old.reddit.com/user/9acca9</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/9acca9"&gt; /u/9acca9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2bwxx/when_is_it_useful_to_use_a_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2bwxx/when_is_it_useful_to_use_a_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n2bwxx/when_is_it_useful_to_use_a_prompt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T13:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n27qcn</id>
    <title>Qwen3 rbit rl finetuned for stromger reasoning</title>
    <updated>2025-08-28T09:37:04+00:00</updated>
    <author>
      <name>/u/adeelahmadch</name>
      <uri>https://old.reddit.com/user/adeelahmadch</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adeelahmadch"&gt; /u/adeelahmadch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1n27p5g/qwen3_rbit_rl_finetuned_for_stromger_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n27qcn/qwen3_rbit_rl_finetuned_for_stromger_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n27qcn/qwen3_rbit_rl_finetuned_for_stromger_reasoning/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T09:37:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1m9fk</id>
    <title>Pair a vision grounding model with a reasoning LLM with Cua</title>
    <updated>2025-08-27T16:46:06+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n1m9fk/pair_a_vision_grounding_model_with_a_reasoning/"&gt; &lt;img alt="Pair a vision grounding model with a reasoning LLM with Cua" src="https://external-preview.redd.it/dzUycDIzdHFjbGxmMbzuqtV-zsPSC2s-Lu_18m-UGy8cX2XwaXvrFiOhDTxh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d3f7d72bb82103be882d2f660f7910185af8eb8" title="Pair a vision grounding model with a reasoning LLM with Cua" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cua just shipped v0.4 of the Cua Agent framework with Composite Agents - you can now pair a vision/grounding model with a reasoning LLM using a simple modelA+modelB syntax. Best clicks + best plans.&lt;/p&gt; &lt;p&gt;The problem: every GUI model speaks a different dialect. • some want pixel coordinates • others want percentages • a few spit out cursed tokens like &amp;lt;|loc095|&amp;gt;&lt;/p&gt; &lt;p&gt;We built a universal interface that works the same across Anthropic, OpenAI, Hugging Face, etc.:&lt;/p&gt; &lt;p&gt;agent = ComputerAgent( model=&amp;quot;anthropic/claude-3-5-sonnet-20241022&amp;quot;, tools=[computer] )&lt;/p&gt; &lt;p&gt;But here’s the fun part: you can combine models by specialization. Grounding model (sees + clicks) + Planning model (reasons + decides) →&lt;/p&gt; &lt;p&gt;agent = ComputerAgent( model=&amp;quot;huggingface-local/HelloKKMe/GTA1-7B+openai/gpt-4o&amp;quot;, tools=[computer] )&lt;/p&gt; &lt;p&gt;This gives GUI skills to models that were never built for computer use. One handles the eyes/hands, the other the brain. Think driver + navigator working together.&lt;/p&gt; &lt;p&gt;Two specialists beat one generalist. We’ve got a ready-to-run notebook demo - curious what combos you all will try.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog : &lt;a href="https://www.trycua.com/blog/composite-agents"&gt;https://www.trycua.com/blog/composite-agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7io8lg1rcllf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1m9fk/pair_a_vision_grounding_model_with_a_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1m9fk/pair_a_vision_grounding_model_with_a_reasoning/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T16:46:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2c5mw</id>
    <title>I’ve Debugged 100+ RAG/LLM Pipelines. These 16 Bugs Always Come Back. (70 days, 800 stars)</title>
    <updated>2025-08-28T13:25:01+00:00</updated>
    <author>
      <name>/u/onestardao</name>
      <uri>https://old.reddit.com/user/onestardao</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i used to think RAG was mostly “pick better embeddings, tune chunk size, choose a faster vector db.” then production happened.&lt;/p&gt; &lt;p&gt;what i thought&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;switch cosine to dot, increase chunk length, rerun.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;try another vector store and RPS goes up, so answers should improve.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;hybrid retrieval must be strictly better than a single retriever.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;what really happened&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;high similarity with wrong meaning. facts exist in the corpus but never surface.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;answers look right while citations silently drift to the wrong section.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;first call after deploy fails because secrets are not ready.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;hybrid sometimes performs worse than a single strong retriever with a clean contract.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;after 100+ pipelines across ollama stacks, the same patterns kept returning. none of this was random. they were structural failure modes. so i wrote them down as a Problem Map with 16 reproducible slots, each with a permanent fix. examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;No.5 embedding ≠ semantic. high similarity, wrong meaning.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No.8 retrieval traceability. answer looks fine, citations do not align to the exact offsets.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No.14 bootstrap ordering. first call after deploy crashes or uses stale env because infra is not warmed.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No.15 deployment deadlock. retriever or merge waits forever on an index that is still building.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;i shared the map and the community response was surprisingly strong. 70 days, 800 stars, and even the tesseract.js author starred it. more important than stars though, the map made bugs stop repeating. once a slot is fixed structurally, it stays fixed.&lt;/p&gt; &lt;p&gt;👉 Problem Map, 16 failure modes with fixes (link above)&lt;/p&gt; &lt;h2&gt;a concrete ollama workflow you can try in 60 seconds&lt;/h2&gt; &lt;p&gt;open a fresh ollama chat with your model. paste this diagnostic prompt as is:&lt;/p&gt; &lt;p&gt;You are a RAG pipeline auditor. Classify the current failure into the Problem Map slots (No.5 embedding≠semantic, No.8 retrieval traceability, No.14 bootstrap ordering, No.15 deployment deadlock, or other). Return a short JSON plan with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;slot&amp;quot;: &amp;quot;No.x&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;why&amp;quot;: one-sentence symptom match&lt;/li&gt; &lt;li&gt;&amp;quot;checks&amp;quot;: ordered steps I can run now&lt;/li&gt; &lt;li&gt;&amp;quot;fix&amp;quot;: the minimal structural change&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Rules:&lt;/p&gt; &lt;p&gt;1) enforce cite-then-explain. if citations or offsets are missing, fail fast and say &amp;quot;add traceability contract&amp;quot;.&lt;/p&gt; &lt;p&gt;2) if coverage &amp;lt; 0.70 or alignment is inconsistent across 3 paraphrases, flag &amp;quot;needs retriever repair&amp;quot;.&lt;/p&gt; &lt;p&gt;3) do not change my infra. propose guardrails I can add at the text and contract layer.&lt;/p&gt; &lt;p&gt;Keep it terse and auditable.&lt;/p&gt; &lt;p&gt;now ask your real question, or paste a failing trace. the model should classify into one of the slots and return a tiny, checkable plan.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;minimal guardrails you can add today&lt;/h2&gt; &lt;p&gt;acceptance targets&lt;/p&gt; &lt;ul&gt; &lt;li&gt;coverage for the target section ≥ 0.70&lt;/li&gt; &lt;li&gt;enforce cite then explain&lt;/li&gt; &lt;li&gt;stop on missing fields: snippet_id, section_id, source_url, offsets, tokens&lt;/li&gt; &lt;li&gt;flag instability if the answer flips across 3 paraphrases with identical inputs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;bootstrap fence&lt;/p&gt; &lt;ul&gt; &lt;li&gt;before any retrieval or generation, assert env and secrets are present. if not, short circuit with a wait and a capped retry counter. this prevents No.14.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;traceability contract&lt;/p&gt; &lt;ul&gt; &lt;li&gt;require snippet level ids and offsets. reject answers that cannot point back to the exact span. this prevents No.8 from hiding for weeks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;retriever sanity&lt;/p&gt; &lt;ul&gt; &lt;li&gt;verify the analyzer and normalization used to write the index matches the one used in retrieval. a mismatch often masquerades as No.5.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;single writer&lt;/p&gt; &lt;ul&gt; &lt;li&gt;queue or mutex all index writes. many “random” 500s are actually No.15 race conditions.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;why this matters to ollama users&lt;/h2&gt; &lt;p&gt;ollama gives you control and speed. the failure modes above sneak in precisely when you move fast. if you keep a short checklist and a common language for the bugs, you do not waste cycles arguing about tools. you fix the structure once, and it stays fixed.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;global fix map, work in progress&lt;/h2&gt; &lt;p&gt;problem map 1.0 is the foundation. i am now drafting a global fix map that spans ollama, langchain, llamaindex, qdrant, weaviate, milvus, and common automation stacks. same idea, one level broader. minimal recipes, clean guardrails, no infra rewrites.&lt;/p&gt; &lt;p&gt;what would you want included besides “common bugs + fixes”?&lt;/p&gt; &lt;p&gt;metrics you actually check, copy paste recipes, deployment checklists, or something else you wish you had before prod went live? ( will be launched soon) &lt;/p&gt; &lt;p&gt;🫡 Thank you in advance &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onestardao"&gt; /u/onestardao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/onestardao/WFGY/tree/main/ProblemMap/README.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2c5mw/ive_debugged_100_ragllm_pipelines_these_16_bugs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n2c5mw/ive_debugged_100_ragllm_pipelines_these_16_bugs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T13:25:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1tfj4</id>
    <title>Ollama + PostgreSQL: Your Local LLM Can Now Query Production Databases</title>
    <updated>2025-08-27T21:17:07+00:00</updated>
    <author>
      <name>/u/Sea-Assignment6371</name>
      <uri>https://old.reddit.com/user/Sea-Assignment6371</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n1tfj4/ollama_postgresql_your_local_llm_can_now_query/"&gt; &lt;img alt="Ollama + PostgreSQL: Your Local LLM Can Now Query Production Databases" src="https://external-preview.redd.it/d2huZzF2ZHZvbWxmMblmm4Un8wPzClWvKIqbCd-O0tnIDr4HTaJ8aTVXM2nL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fdb7439b91fc4e3bf69069c2c0e5c06b049adfcd" title="Ollama + PostgreSQL: Your Local LLM Can Now Query Production Databases" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt;! Quick update - DataKit now lets you query PostgreSQL databases with Ollama's help.&lt;/p&gt; &lt;p&gt;Well the best part: Your data/schema NEVER goes to OpenAI/Claude. Your local LLM generates the SQL just by looking at the schema of the file.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What this enables:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;• &amp;quot;Show me all users who signed up last month but haven't made a purchase&amp;quot;&lt;/p&gt; &lt;p&gt;• &amp;quot;Find orders with unusual patterns&amp;quot;&lt;/p&gt; &lt;p&gt;• &amp;quot;Generate a cohort analysis query&amp;quot;&lt;/p&gt; &lt;p&gt;All happens locally. Ollama writes the SQL, DuckDB executes it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Run: `OLLAMA_ORIGINS=&amp;quot;&lt;a href="https://datakit.page"&gt;https://datakit.page&lt;/a&gt;&amp;quot; ollama serve`&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Connect your PostgreSQL&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Ask questions in plain English&lt;/p&gt; &lt;p&gt;Try it at &lt;a href="http://datakit.page"&gt;datakit.page&lt;/a&gt; - would love feedback on what models work best for SQL generation!&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Assignment6371"&gt; /u/Sea-Assignment6371 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9qyoordvomlf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1tfj4/ollama_postgresql_your_local_llm_can_now_query/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1tfj4/ollama_postgresql_your_local_llm_can_now_query/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T21:17:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1v1uw</id>
    <title>Just released version 1.4 of Nanocoder built in Ink - such an epic framework for CLI applications!</title>
    <updated>2025-08-27T22:21:00+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n1v1uw/just_released_version_14_of_nanocoder_built_in/"&gt; &lt;img alt="Just released version 1.4 of Nanocoder built in Ink - such an epic framework for CLI applications!" src="https://preview.redd.it/vkqp6dgi0nlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7c9616bc2e60e27283ed7ab441d6f9aa60448d1" title="Just released version 1.4 of Nanocoder built in Ink - such an epic framework for CLI applications!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don’t know why I didn’t build the previous versions of Nanocoder from the start in Ink, it has been so powerful in building a beautiful next-gen version of my open source coding agent.&lt;/p&gt; &lt;p&gt;It helps create some incredible UIs around the terminal and is pretty much pick up and go if you’re already fluent in React. The only challenge has been getting to the UI to scale when you resize the terminal window - any tips let me know!&lt;/p&gt; &lt;p&gt;We’re almost on 100 stars on GitHub which I know is small but I really believe in the philosophies behind this small community! It would make my day to get it there!&lt;/p&gt; &lt;p&gt;All contributors and feedback welcome - people have been so amazing already! I’m trying to get people involved to build a piece of software that is owned and pushed by the community - not big tech companies! 😄&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Link&lt;/strong&gt;: &lt;a href="https://github.com/Mote-Software/nanocoder"&gt;https://github.com/Mote-Software/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discord Link to Get Involved&lt;/strong&gt;: &lt;a href="https://discord.gg/ktPDV6rekE"&gt;https://discord.gg/ktPDV6rekE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vkqp6dgi0nlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1v1uw/just_released_version_14_of_nanocoder_built_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1v1uw/just_released_version_14_of_nanocoder_built_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T22:21:00+00:00</published>
  </entry>
</feed>
