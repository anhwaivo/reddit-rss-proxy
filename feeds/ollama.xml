<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-18T04:24:37+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1irkej7</id>
    <title>adjusting RoPE scaling and NTK-aware scaling</title>
    <updated>2025-02-17T13:57:34+00:00</updated>
    <author>
      <name>/u/Useful-Skill6241</name>
      <uri>https://old.reddit.com/user/Useful-Skill6241</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I guys im running deepseek-r1:70b -its a distilled Q4_K_M 0c1615a8ca32 (42GB)&lt;/p&gt; &lt;p&gt;I have changed the paramters in ollma for a 15,000 token context window:&lt;/p&gt; &lt;p&gt;&amp;quot; Model&lt;/p&gt; &lt;p&gt;architecture llama&lt;/p&gt; &lt;p&gt;parameters 70.6B&lt;/p&gt; &lt;p&gt;context length 131072&lt;/p&gt; &lt;p&gt;embedding length 8192&lt;/p&gt; &lt;p&gt;quantization Q4_K_M&lt;/p&gt; &lt;p&gt;Parameters&lt;/p&gt; &lt;p&gt;num_ctx 15000&amp;quot;&lt;/p&gt; &lt;p&gt;is there anywhere you guys know of where I can adjusting RoPE scaling and NTK-aware scaling. I have the ram for it, it just spits out gibberish or doesnt take into account the text after around i would say 1200 tokens of output&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Useful-Skill6241"&gt; /u/Useful-Skill6241 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irkej7/adjusting_rope_scaling_and_ntkaware_scaling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irkej7/adjusting_rope_scaling_and_ntkaware_scaling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irkej7/adjusting_rope_scaling_and_ntkaware_scaling/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T13:57:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1irek2w</id>
    <title>How do you find the right AI model for your use case?</title>
    <updated>2025-02-17T07:38:22+00:00</updated>
    <author>
      <name>/u/uber_men</name>
      <uri>https://old.reddit.com/user/uber_men</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Where can I search for AI models that fit my use case? For example, if I want to restore old blurred photos but don’t know which AI model to use, I should be able to find one like &lt;a href="https://github.com/TencentARC/GFPGAN"&gt;GFPGAN&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uber_men"&gt; /u/uber_men &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irek2w/how_do_you_find_the_right_ai_model_for_your_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irek2w/how_do_you_find_the_right_ai_model_for_your_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irek2w/how_do_you_find_the_right_ai_model_for_your_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T07:38:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1irn0f9</id>
    <title>Command-r7b</title>
    <updated>2025-02-17T15:54:46+00:00</updated>
    <author>
      <name>/u/J0Mo_o</name>
      <uri>https://old.reddit.com/user/J0Mo_o</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can command-r7b be run on ollama 0.5.7 It says it needs ollama 0.5.5 but i don't know if that means it needs THIS specific version or it needs at least this version&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/J0Mo_o"&gt; /u/J0Mo_o &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irn0f9/commandr7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irn0f9/commandr7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irn0f9/commandr7b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T15:54:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1iro9kd</id>
    <title>I Built My Own AI Code Assistant with DeepSeek &amp; LangChain!</title>
    <updated>2025-02-17T16:46:08+00:00</updated>
    <author>
      <name>/u/Special_Community179</name>
      <uri>https://old.reddit.com/user/Special_Community179</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iro9kd/i_built_my_own_ai_code_assistant_with_deepseek/"&gt; &lt;img alt="I Built My Own AI Code Assistant with DeepSeek &amp;amp; LangChain!" src="https://external-preview.redd.it/hnt7poXU-xnfeXFZfxsMOiSfirurfRIotwJUPYEsx90.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=655a950bdbfba394f2efb20d1be0901f5db6f196" title="I Built My Own AI Code Assistant with DeepSeek &amp;amp; LangChain!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special_Community179"&gt; /u/Special_Community179 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=7GAKB21lxyg&amp;amp;list=PLp01ObP3udmq2quR-RfrX4zNut_t_kNot"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iro9kd/i_built_my_own_ai_code_assistant_with_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iro9kd/i_built_my_own_ai_code_assistant_with_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T16:46:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1irn6o9</id>
    <title>Any fix to this?</title>
    <updated>2025-02-17T16:01:55+00:00</updated>
    <author>
      <name>/u/Beneficial-Cup2969</name>
      <uri>https://old.reddit.com/user/Beneficial-Cup2969</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1irn6o9/any_fix_to_this/"&gt; &lt;img alt="Any fix to this?" src="https://preview.redd.it/iaqslgar2qje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f8c6aa9074fc89619a7bbd47b7412491bf027552" title="Any fix to this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry am really new to all these private ai stuff but I can't get ollama to analyse this image I used a tutorial to use llama 3 using docker and local host for convenience am i missing something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beneficial-Cup2969"&gt; /u/Beneficial-Cup2969 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iaqslgar2qje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irn6o9/any_fix_to_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irn6o9/any_fix_to_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T16:01:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1irtewe</id>
    <title>Jarvis at Home</title>
    <updated>2025-02-17T20:08:17+00:00</updated>
    <author>
      <name>/u/AutomaticRadish5</name>
      <uri>https://old.reddit.com/user/AutomaticRadish5</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1irtewe/jarvis_at_home/"&gt; &lt;img alt="Jarvis at Home" src="https://external-preview.redd.it/dTExNzZsM3BhcmplMTt6sXG7ygBufWbZGyKocKVKKjpDeQAd8T0jjbX8B6f0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acdd6591a4ab8dc0b4906d2af1c4be8021a23cea" title="Jarvis at Home" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Neat little work in progress &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AutomaticRadish5"&gt; /u/AutomaticRadish5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6e2njv6parje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irtewe/jarvis_at_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irtewe/jarvis_at_home/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T20:08:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1irtpy2</id>
    <title>Can I store the inputs and outputs of my conversations?</title>
    <updated>2025-02-17T20:20:23+00:00</updated>
    <author>
      <name>/u/BaggiPonte</name>
      <uri>https://old.reddit.com/user/BaggiPonte</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run ollama in my CLI and I would like to store my completions for personal analysis/fine tuning. How can I do that? Or perhaps ollama can store them by default locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BaggiPonte"&gt; /u/BaggiPonte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irtpy2/can_i_store_the_inputs_and_outputs_of_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irtpy2/can_i_store_the_inputs_and_outputs_of_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irtpy2/can_i_store_the_inputs_and_outputs_of_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T20:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqvypa</id>
    <title>Ollama Vs. LM Studio</title>
    <updated>2025-02-16T16:29:24+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iqvypa/ollama_vs_lm_studio/"&gt; &lt;img alt="Ollama Vs. LM Studio" src="https://external-preview.redd.it/9twFf-ndeg92DJQ4VUhbGT49y8GQ4M4VnDD5QQ3aM0g.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d603d6ac771df4cac0cb2b45b29e921c3ab5dc19" title="Ollama Vs. LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/QGtkaDWJZlA"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqvypa/ollama_vs_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqvypa/ollama_vs_lm_studio/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T16:29:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir1y64</id>
    <title>Web Search for Ollama</title>
    <updated>2025-02-16T20:38:37+00:00</updated>
    <author>
      <name>/u/Pure-Caramel1216</name>
      <uri>https://old.reddit.com/user/Pure-Caramel1216</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ir1y64/web_search_for_ollama/"&gt; &lt;img alt="Web Search for Ollama" src="https://external-preview.redd.it/cTZzejBhcDdia2plMX2nSzWAqWPwL7GaLWhWgmcM0HgC18EKBhh979hmLHax.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02f06424f67deec37bb163c15ea8dec1329da736" title="Web Search for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Although I love Ollama, I've found it nearly impossible to get up-to-date information. Existing solutions are either too complex or simply don't work well. That’s why I built an anonymous web search tool for Ollama (check out the video above).&lt;/p&gt; &lt;p&gt;Since I’m not the only one facing this issue, I’m considering launching it as a full-fledged tool. I’d really appreciate your suggestions on how to improve it—please leave your ideas in the comments. And if you like what you see, consider upvoting so this project reaches a wider audience.&lt;/p&gt; &lt;p&gt;About the Video:&lt;/p&gt; &lt;p&gt;Model: Small 3B model&lt;/p&gt; &lt;p&gt;Hardware: Running on an NVIDIA A10 GPU&lt;/p&gt; &lt;p&gt;Demo: I ran the same prompt twice—first without web search, and then with web search.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pure-Caramel1216"&gt; /u/Pure-Caramel1216 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9wsgtkx7bkje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir1y64/web_search_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ir1y64/web_search_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T20:38:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1irnuqw</id>
    <title>Ollama shouldn't spam update fetch requests every second or two when it already has the update downloaded...</title>
    <updated>2025-02-17T16:29:29+00:00</updated>
    <author>
      <name>/u/ZdrytchX</name>
      <uri>https://old.reddit.com/user/ZdrytchX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;time=2025-02-16T03:53:53.276+08:00 level=INFO source=updater.go:103 msg=&amp;quot;New update available at https://github.com/ollama/ollama/releases/download/v0.5.11/OllamaSetup.exe&amp;quot; time=2025-02-16T03:53:54.595+08:00 level=INFO source=updater.go:138 msg=&amp;quot;update already downloaded&amp;quot; time=2025-02-16T04:53:55.056+08:00 level=INFO source=updater.go:103 msg=&amp;quot;New update available at https://github.com/ollama/ollama/releases/download/v0.5.11/OllamaSetup.exe&amp;quot; time=2025-02-16T04:53:56.349+08:00 level=INFO source=updater.go:138 msg=&amp;quot;update already downloaded&amp;quot; time=2025-02-16T05:53:56.811+08:00 level=INFO source=updater.go:103 msg=&amp;quot;New update available at https://github.com/ollama/ollama/releases/download/v0.5.11/OllamaSetup.exe&amp;quot; time=2025-02-16T05:53:58.120+08:00 level=INFO source=updater.go:138 msg=&amp;quot;update already downloaded&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZdrytchX"&gt; /u/ZdrytchX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irnuqw/ollama_shouldnt_spam_update_fetch_requests_every/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irnuqw/ollama_shouldnt_spam_update_fetch_requests_every/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irnuqw/ollama_shouldnt_spam_update_fetch_requests_every/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T16:29:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1irsdl2</id>
    <title>Can not use ollama in VS Code on remote server</title>
    <updated>2025-02-17T19:27:25+00:00</updated>
    <author>
      <name>/u/wbiggs205</name>
      <uri>https://old.reddit.com/user/wbiggs205</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a remote GPU server run Windows 11 pro with a NVIDIA t1000 8 g I installed ollama. And the web GUI. Enable. I did change the default port. I can use ollama with the GUI just fine. And I have AnythingLLM installed on my desktop I can get it to use the server but when I set up continue to use it. It will not connect. I did add this to the config file then I port the address and the new port where the endpoint is but nothing. Any ides ?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;models&amp;quot;: [ { &amp;quot;title&amp;quot;: &amp;quot;Llama3.1 8B&amp;quot;, &amp;quot;provider&amp;quot;: &amp;quot;ollama&amp;quot;, &amp;quot;model&amp;quot;: &amp;quot;llama3.1:8b&amp;quot;, &amp;quot;apiBase&amp;quot;: &amp;quot;http://&amp;lt;my endpoint&amp;gt;:11434&amp;quot; } ] } &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wbiggs205"&gt; /u/wbiggs205 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irsdl2/can_not_use_ollama_in_vs_code_on_remote_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irsdl2/can_not_use_ollama_in_vs_code_on_remote_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irsdl2/can_not_use_ollama_in_vs_code_on_remote_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T19:27:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1irt83f</id>
    <title>Is it possible to access the API from other machines on network?</title>
    <updated>2025-02-17T20:01:02+00:00</updated>
    <author>
      <name>/u/mshriver2</name>
      <uri>https://old.reddit.com/user/mshriver2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I am running Ollama on windows 10. I can use the API just fine locally but it is inaccessible from other machines on the network. I have confirmed that the port is allowed through the windows firewall. I did some research and it seems Ollama API is only set to bind to localhost by default. It seems you can change that in windows with a -- command flag. However there doesn't seem to be an equivalent in Windows. Is there any workaround to being able to access the API from other machines?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mshriver2"&gt; /u/mshriver2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irt83f/is_it_possible_to_access_the_api_from_other/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irt83f/is_it_possible_to_access_the_api_from_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irt83f/is_it_possible_to_access_the_api_from_other/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T20:01:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1irjave</id>
    <title>OpenThinker-32B-FP16 + 8x AMD Instinct Mi60 Server + vLLM + Tensor Parallelism</title>
    <updated>2025-02-17T13:01:13+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/klcjcd1d6pje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irjave/openthinker32bfp16_8x_amd_instinct_mi60_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irjave/openthinker32bfp16_8x_amd_instinct_mi60_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T13:01:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1irr90t</id>
    <title>Does GPU´s run faster when in a system with Intel CPUs?</title>
    <updated>2025-02-17T18:43:25+00:00</updated>
    <author>
      <name>/u/FrederikSchack</name>
      <uri>https://old.reddit.com/user/FrederikSchack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1irr90t/does_gpus_run_faster_when_in_a_system_with_intel/"&gt; &lt;img alt="Does GPU´s run faster when in a system with Intel CPUs?" src="https://a.thumbs.redditmedia.com/qQRsUlityZ7EE_yRjELKx2nf4t1ry7XoJflTF1W_tb0.jpg" title="Does GPU´s run faster when in a system with Intel CPUs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a small survey here:&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ip7zaz"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ip7zaz&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Can this really be true?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zmnr1p9vvqje1.png?width=603&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6d823df8cc747e9d11b1b2b6a899ac421f342be"&gt;https://preview.redd.it/zmnr1p9vvqje1.png?width=603&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6d823df8cc747e9d11b1b2b6a899ac421f342be&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrederikSchack"&gt; /u/FrederikSchack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irr90t/does_gpus_run_faster_when_in_a_system_with_intel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irr90t/does_gpus_run_faster_when_in_a_system_with_intel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irr90t/does_gpus_run_faster_when_in_a_system_with_intel/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T18:43:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1irzuyw</id>
    <title>Exposing ollama to internet</title>
    <updated>2025-02-18T00:42:01+00:00</updated>
    <author>
      <name>/u/Right_Positive5886</name>
      <uri>https://old.reddit.com/user/Right_Positive5886</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have managed to run ollama with open-webgui on a Linode VM. 32gb ram 1 tb memory and 8 core cpu. I have fronted it with nginx proxy with let’s encrypt certs. The application is up unfortunately it works for only small prompts bigger prompts the app is erroring out . It doesn’t matter whether I’m running a large model or a small one ( atm Deepseek 1.5 B ) would anyone know what is missing ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Right_Positive5886"&gt; /u/Right_Positive5886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irzuyw/exposing_ollama_to_internet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irzuyw/exposing_ollama_to_internet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irzuyw/exposing_ollama_to_internet/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T00:42:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1iref1e</id>
    <title>AMD Instinct MI50 detailed benchmarks in ollama</title>
    <updated>2025-02-17T07:28:00+00:00</updated>
    <author>
      <name>/u/Psychological_Ear393</name>
      <uri>https://old.reddit.com/user/Psychological_Ear393</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 2xMI50s and ran a series of benchmarks in ollama on a variety of models with a few quants thrown in, only running models which fit into the total 32gb VRAM&lt;/p&gt; &lt;p&gt;It's difficult to tell exactly how other benchmarks were run, so I can't really say how they perform relative to others but they at least compete with low end modern cards like the 4060 Ti and the A4000, but at substantially lower cost.&lt;/p&gt; &lt;p&gt;Full details here of the software versions, hardware, prompt and models, variations in the output lengths, TPS, results at 250 and 125 watts, size reported by ollama ps, and USD/TPS: &lt;a href="https://docs.google.com/spreadsheets/d/1TjxpN0NYh-xb0ZwCpYr4FT-hG773_p1DEgxJaJtyRmY/edit?usp=sharing"&gt;https://docs.google.com/spreadsheets/d/1TjxpN0NYh-xb0ZwCpYr4FT-hG773_p1DEgxJaJtyRmY/edit?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am very keen to hear how other card perform on the identical benchmark runs. I know they are on the bottom of the pack when it comes to performance for current builds, but I bought mine for $110USD each and last I checked were going for about $120USD, which to me makes them a steal.&lt;/p&gt; &lt;p&gt;For the models I tested, the fastest model was unsurprisingly &lt;code&gt;llama3.2:1b-instruct-q8_0&lt;/code&gt; maxing 150 tps, and the slowest was &lt;code&gt;FuseO1-DeekSeekR1-QwQ-SkyT1-32B-Preview-GGUF:Q6_K&lt;/code&gt; at 14tps.&lt;/p&gt; &lt;p&gt;I did get one refused on the prompt I used &lt;code&gt;Who discovered heliocentrism and how is that possible without being in space? Be verbose I want to know all about it.&lt;/code&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I can't provide information on who discovered heliocentrism or other topics that may be considered sensitive or controversial, such as the Copernican Revolution. Is there anything else I can help you with?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Which was really weird, and it happened more than once in llama, but no others, and I saw another different refusal on another model then never saw the refusal again&lt;/p&gt; &lt;p&gt;Some anticipated Q&amp;amp;A&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How did I deal with the ROCm problem?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The sarcastic answer is &amp;quot;&lt;em&gt;What ROCm problem?&amp;quot;&lt;/em&gt;. It seems to me like there's a lot of people who don't have an AMD card, people with an unsupported card, people on an unsupported distro, or people who ran it a long time ago who are spouting this.&lt;/p&gt; &lt;p&gt;The more serious answer is the ROCm install docs have the distro and hardware requirements. If you meet those it should just work. I initially tried in my distro of choice, which was not listed, and it was too hard so I gave up and installed Ubuntu and everything just worked. By &amp;quot;just worked&amp;quot; I mean I installed Ubuntu, followed the ROCm install guide, downloaded ollama, ran it, and ollama used the GPU without any hassle.&lt;/p&gt; &lt;p&gt;ComfyUI was similarly easy, except I had the additional steps of pulling the AMD repo, building, then running.&lt;/p&gt; &lt;p&gt;I have not tried any other apps.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How did I cool them?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I bought some 3D printed shrouds off Ebay that take an 80mm fan. I had to keep them power capped at 90 watts or they would overheat, and after some kind advice from here it was shown that the shrouds had an inefficient path for the air to travel and a custom solution would work better. I didn't do that because of time/money and instead bought silverstone 80mm industrial fans (10K RPM max) and they work a treat and keep them cool at 250 watts.&lt;/p&gt; &lt;p&gt;They are very loud so I bought a PWM controller which I keep on the case and adjust the fan speed for how hard I want to run the cards. It's outright too hard to control the fan speed through IPMI tool which is an app made by the devil to torment Linux users.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Would I buy them again?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Being old and relatively slow (I am guessing just slower than a 4070) I expected them to be temporary while I got started with AI, but they have been performing above my expectations. I would absolutely buy them again if I could live that build over again, and if I can mount the cards so there's more room, such as with PCIe extender cables, I would buy more two more MI50s for 64Gb VRAM.&lt;/p&gt; &lt;p&gt;For space and power reasons I would prefer MI60s or MI100s but this experience has me cemented as an Instinct fan and I have no interest in buying any nvidia card at their current new and used prices.&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;If there's any models you would like tested, let me know&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological_Ear393"&gt; /u/Psychological_Ear393 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iref1e/amd_instinct_mi50_detailed_benchmarks_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iref1e/amd_instinct_mi50_detailed_benchmarks_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iref1e/amd_instinct_mi50_detailed_benchmarks_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T07:28:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1irphxz</id>
    <title>Initial hardware Inspection for the 8x AMD Instinct Mi50 Servers</title>
    <updated>2025-02-17T17:35:26+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1irpgi6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irphxz/initial_hardware_inspection_for_the_8x_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irphxz/initial_hardware_inspection_for_the_8x_amd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T17:35:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1is3wcp</id>
    <title>Ollama split layers into gpu and cpu?</title>
    <updated>2025-02-18T04:04:08+00:00</updated>
    <author>
      <name>/u/wahnsinnwanscene</name>
      <uri>https://old.reddit.com/user/wahnsinnwanscene</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a way to bind different layers to either cpu or gpu? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wahnsinnwanscene"&gt; /u/wahnsinnwanscene &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is3wcp/ollama_split_layers_into_gpu_and_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is3wcp/ollama_split_layers_into_gpu_and_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is3wcp/ollama_split_layers_into_gpu_and_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T04:04:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1is11ld</id>
    <title>Testing cards (AMD Instinct Mi50s) 14 out of 14 tested good! 12 more to go..</title>
    <updated>2025-02-18T01:37:49+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1is0poz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is11ld/testing_cards_amd_instinct_mi50s_14_out_of_14/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is11ld/testing_cards_amd_instinct_mi50s_14_out_of_14/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T01:37:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1is22ug</id>
    <title>Most cost effective way of hosting 70B/32B param model</title>
    <updated>2025-02-18T02:29:31+00:00</updated>
    <author>
      <name>/u/topsy_here</name>
      <uri>https://old.reddit.com/user/topsy_here</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious &lt;/p&gt; &lt;p&gt;It comes down to efficiency. I see it like crypto mining. It’s about getting the best token count for least cost.&lt;/p&gt; &lt;p&gt;There’s Mac minis I’ve seen hosting the 72B param one. You gonna need about 8x of them which is about 3.5K usd each? &lt;/p&gt; &lt;p&gt;What about hosting on a VPS in Linus? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/topsy_here"&gt; /u/topsy_here &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is22ug/most_cost_effective_way_of_hosting_70b32b_param/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is22ug/most_cost_effective_way_of_hosting_70b32b_param/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is22ug/most_cost_effective_way_of_hosting_70b32b_param/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T02:29:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1irwerk</id>
    <title>Found 10 T4 GPU's</title>
    <updated>2025-02-17T22:09:08+00:00</updated>
    <author>
      <name>/u/ShortSpinach5484</name>
      <uri>https://old.reddit.com/user/ShortSpinach5484</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello community. I was decommissioning 10 old vmware host at work and found out that there was a 70w fanless T4 Gpu in each host. And I got ok to build a gpu farm to run local llms on them. But how should i build a gpu farm? Shure i can install debian/ubuntu on everything but is there a easy way to build a gpu farm?&lt;/p&gt; &lt;p&gt;Is there a easy way to do something like google colabs or kaggle?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShortSpinach5484"&gt; /u/ShortSpinach5484 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irwerk/found_10_t4_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irwerk/found_10_t4_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irwerk/found_10_t4_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T22:09:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1irhbrl</id>
    <title>PrivateLLMLens - A Zero-Server Web Interface for Local Ollama</title>
    <updated>2025-02-17T10:58:39+00:00</updated>
    <author>
      <name>/u/Specialist_Laugh_231</name>
      <uri>https://old.reddit.com/user/Specialist_Laugh_231</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1irhbrl/privatellmlens_a_zeroserver_web_interface_for/"&gt; &lt;img alt="PrivateLLMLens - A Zero-Server Web Interface for Local Ollama" src="https://preview.redd.it/5bfizklmkoje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4051a019d044303e3381fd284e4eee1a8ace21b9" title="PrivateLLMLens - A Zero-Server Web Interface for Local Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist_Laugh_231"&gt; /u/Specialist_Laugh_231 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5bfizklmkoje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irhbrl/privatellmlens_a_zeroserver_web_interface_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irhbrl/privatellmlens_a_zeroserver_web_interface_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T10:58:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1irvwt0</id>
    <title>CheerBot. Ollama based Cheer or Nudge through OCR of your screen.</title>
    <updated>2025-02-17T21:48:43+00:00</updated>
    <author>
      <name>/u/maranone5</name>
      <uri>https://old.reddit.com/user/maranone5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Just a quick and dirty proof of concept!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here's a little productivity monitor I threw together:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Take two screenshots&lt;/strong&gt; of your screen at a predefined time difference.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OCR with Tesseract&lt;/strong&gt; to extract text (sorry graphic designers, no design elements here!).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Analyze the text&lt;/strong&gt; through Ollama (basically, your personal productivity assistant).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Nudge or Cheer&lt;/strong&gt; accordingly using a pet personality for a bit of fun motivation.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;No APIs, no cloud – all execution is &lt;strong&gt;local&lt;/strong&gt; and &lt;strong&gt;safe&lt;/strong&gt; on your machine. Think of it like a modern-day Clippy.&lt;/p&gt; &lt;p&gt;The current version is a bit of a &lt;strong&gt;CLI mess&lt;/strong&gt; – if anyone’s up for a challenge, feel free to create a &lt;strong&gt;GUI or popup version&lt;/strong&gt; to make it more user-friendly and visually appealing!&lt;/p&gt; &lt;p&gt;Feel free to check it out and let me know what you think! 🙌&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/maranone/CheerBot"&gt;https://github.com/maranone/CheerBot&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maranone5"&gt; /u/maranone5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irvwt0/cheerbot_ollama_based_cheer_or_nudge_through_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irvwt0/cheerbot_ollama_based_cheer_or_nudge_through_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irvwt0/cheerbot_ollama_based_cheer_or_nudge_through_ocr/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T21:48:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1irrxqs</id>
    <title>DataBridge Feature Dump: We'll implement anything you want!!</title>
    <updated>2025-02-17T19:09:59+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! &lt;/p&gt; &lt;p&gt;Thanks to the power of the &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; community, &lt;a href="https://github.com/databridge-org/databridge-core"&gt;DataBridge&lt;/a&gt; just hit 350 stars! As a token of our gratitude, we're committing to implementing the &lt;strong&gt;top 3 feature requests&lt;/strong&gt; from &lt;em&gt;you&lt;/em&gt; :) &lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to participate:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Leave your dream feature or improvement - RAG or otherwise - as a reply to this post! Upvote existing ideas you’d love to see. We’ll tally the votes and build the &lt;strong&gt;top 3 most-requested features&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Let’s shape DataBridge’s future together—drop your requests below! 🚀&lt;/p&gt; &lt;p&gt;&lt;em&gt;(We'll start tallying at 2:00 pm ET on the 18th of Feb - happy to start working on stuff before that tho!)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Huge thanks again for being part of this journey! 🙌 ❤️&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irrxqs/databridge_feature_dump_well_implement_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irrxqs/databridge_feature_dump_well_implement_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irrxqs/databridge_feature_dump_well_implement_anything/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T19:09:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iryvnt</id>
    <title>Deepscaler km q4 is insanly fast abd small even on CPU only</title>
    <updated>2025-02-17T23:56:07+00:00</updated>
    <author>
      <name>/u/tshawkins</name>
      <uri>https://old.reddit.com/user/tshawkins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running on NUC11PRO 11th gen i7 with 32GB - 4200 RAM, no GPU, consumes a little over 1.1GB of RAM, Ideal for running in the background on a dev machine. &lt;/p&gt; &lt;p&gt;The Model Is a 1.5B parameter distilation of deepseek-r1. &lt;/p&gt; &lt;p&gt;Im going to be exploring its code generation abilities, to see if it is usefull as a enclosed code generator, ie no requests to the internet. &lt;/p&gt; &lt;p&gt;It would interesting to start a leader board of size vs capabilities. To understand which models are best for edge AI use. &lt;/p&gt; &lt;p&gt;thawkins@TimServFed01:~$ ollama run deepscaler:1.5b-preview-q4_K_M --verbose&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;tell me a story about 10 ants who save thier anthill from drowning during a flood, how they earned the respect of thier queen and all the other ants in the anthill. &amp;lt;think&amp;gt; Okay, so I need to write a story about 10 ants who saved their anthill from drowning during a flood, and explain how they got respect from their queen and everyone else. Hmm, let's break this down.&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;First, setting the scene: a big flood hits an ancient, powerful anthill. The ants are probably scared because of the high water level. They need to escape or find shelter quickly. But maybe they don't know much about each other or how to help.&lt;/p&gt; &lt;p&gt;They decide to go together. Maybe they leave their usual positions and start searching for spots where the water is less. That could save some lives. But why are there only 10 ants? Maybe because it's a small group, so they can work together.&lt;/p&gt; &lt;p&gt;When the flood comes in, the water level rises, and the ants find an area with low water. They go in, maybe even get drenched. But then someone notices them, like maybe a bird or a duck that spots their tiny collection. The birds might offer help by giving information or clues on where to go.&lt;/p&gt; &lt;p&gt;The queen is really important because she's known for being the leader and enforcing rules. Maybe she gets called out if they don't behave well, but when they manage to escape or protect the anthill, her respect grows. Perhaps she tells everyone about their bravery.&lt;/p&gt; &lt;p&gt;Other ants might get curious and offer help too. They could even start a little community effort, maybe building shelters or finding other ways to help. Their teamwork is key here.&lt;/p&gt; &lt;p&gt;In the end, the group realizes that working together was crucial. Maybe they find a way to ensure their safety despite the flood. They probably learn from this experience how important it is to be part of something bigger than themselves.&lt;/p&gt; &lt;p&gt;I should make sure the story shows their bravery, how they rely on each other, and the respect they receive from their queen and others. Maybe include some dialogue or actions that show their cooperation and determination. &amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Title: The Evacuation Sequence&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Once upon a time, in a lush, ancient anthill with its powerful guardian queen, there lived ten tiny ants who were deeply rooted in their tiny world but had always been curious about the world around them. These ants knew they had to act quickly because the flood was coming, and the water level was rising.&lt;/p&gt; &lt;p&gt;The ants realized that the most effective way to escape was by working together. They decided to go to a hidden spot where the water would be lower. However, their knowledge was limited, so they started searching in different areas. Despite their small size, their collective effort ensured they found an unobstructed path through the flood's waters.&lt;/p&gt; &lt;p&gt;As the flood approached, many ants were caught by some birds that spotted them. These birds offered information, guiding the ants on their escape route. The queen noticed the group's bravery and was immediately informed about their efforts.&lt;/p&gt; &lt;p&gt;The queen, ever the leader of her colony, recognized their bravery and began to notice any discrepancies in their behavior. She taught everyone about their role as part of a larger community. As the group worked together, they realized that their collective strength was essential for survival.&lt;/p&gt; &lt;p&gt;In the end, the ants successfully escaped the flood, protecting their anthill from drowning. They learned that working together could yield greater results than individual efforts. Their bravery earned them respect not only among their queen but also by becoming a model community effort.&lt;/p&gt; &lt;p&gt;Thus, their journey became a symbol of unity and resilience, teaching all who heard about it to work together for shared survival.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;total duration: 34.749197247s. load duration: 10.291253ms. prompt eval count: 44 token(s). prompt eval duration: 40ms. prompt eval rate: 1100.00 tokens/s. eval count: 708 token(s). eval duration: 34.697s. eval rate: 20.41 tokens/s. &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;Send a message (/? for help).&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tshawkins"&gt; /u/tshawkins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iryvnt/deepscaler_km_q4_is_insanly_fast_abd_small_even/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iryvnt/deepscaler_km_q4_is_insanly_fast_abd_small_even/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iryvnt/deepscaler_km_q4_is_insanly_fast_abd_small_even/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T23:56:07+00:00</published>
  </entry>
</feed>
