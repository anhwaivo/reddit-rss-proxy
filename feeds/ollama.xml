<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-04-29T18:40:07+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1k8cprt</id>
    <title>Free GPU for Openwebui</title>
    <updated>2025-04-26T13:16:20+00:00</updated>
    <author>
      <name>/u/guuidx</name>
      <uri>https://old.reddit.com/user/guuidx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi people!&lt;/p&gt; &lt;p&gt;I wrote a post two days ago about using google colab cpu for free to use for Ollama. It was kinda aimed at developers but many webui users were interested. It was not supported, I had to add that functionality. So, that's done now!&lt;/p&gt; &lt;p&gt;Also, by request, i made a video now. The video is full length and you can see that the setup is only a few steps and a few minutes to complete in total! In the video you'll see me happily using a super fast qwen2.5 using openwebui! I'm showing the openwebui config. &lt;/p&gt; &lt;p&gt;The link mentioned in the video as 'my post' is: &lt;a href="https://www.reddit.com/r/ollama/comments/1k674xf/free_ollama_gpu/"&gt;https://www.reddit.com/r/ollama/comments/1k674xf/free_ollama_gpu/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know your experience! &lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k8cprt/video/43794nq7i6xe1/player"&gt;https://reddit.com/link/1k8cprt/video/43794nq7i6xe1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/guuidx"&gt; /u/guuidx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8cprt/free_gpu_for_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8cprt/free_gpu_for_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8cprt/free_gpu_for_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-26T13:16:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8z194</id>
    <title>Garbage / garbled responses</title>
    <updated>2025-04-27T08:18:55+00:00</updated>
    <author>
      <name>/u/mrocty</name>
      <uri>https://old.reddit.com/user/mrocty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running Open WebUI, and Ollama, in two separate docker containers. Responses were working fine when I was using the Open WebUI built in Ollama (&lt;code&gt;ghcr.io/open-webui/open-webui:ollama&lt;/code&gt;), but running a separate container, I get responses like this: &lt;a href="https://imgur.com/a/KoZ8Pgj"&gt;https://imgur.com/a/KoZ8Pgj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All the results I get with &amp;quot;Ollama garbage responses&amp;quot; or anything like that, seem to all be about third party tools that use Ollama, or suggesting that the model is corrupted, or saying I need to adjust the quantization (which I didn't need to do with &lt;code&gt;open-webui:ollama&lt;/code&gt;), so either I'm using the wrong search terms, or I'm the first person in the world that this has happened to.&lt;/p&gt; &lt;p&gt;I've deleted all of the models, and re-downloaded them, but that didn't help.&lt;/p&gt; &lt;p&gt;My docker-compose files are below, but does anyone know wtf would be causing this? &lt;/p&gt; &lt;pre&gt;&lt;code&gt;services: open-webui: container_name: open-webui image: ghcr.io/open-webui/open-webui:main volumes: - ./data:/app/backend/data restart: always environment: - OLLAMA_HOST=http://ollama.my-local-domain.com:11434 services: ollama: volumes: - ./ollama:/root/.ollama container_name: ollama pull_policy: always tty: true restart: unless-stopped image: docker.io/ollama/ollama:latest environment: - OLLAMA_KEEP_ALIVE=24h ports: - 11434:11434 deploy: resources: reservations: devices: - driver: nvidia count: 1 capabilities: [gpu] &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Edit&lt;/h3&gt; &lt;p&gt;&amp;quot;Solved&amp;quot; - issue is with Ollama 0.6.6 only, 0.6.5 and earlier works fine&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrocty"&gt; /u/mrocty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8z194/garbage_garbled_responses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8z194/garbage_garbled_responses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8z194/garbage_garbled_responses/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-27T08:18:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1k98rmq</id>
    <title>Attempt at RAG setup</title>
    <updated>2025-04-27T17:01:38+00:00</updated>
    <author>
      <name>/u/terramot</name>
      <uri>https://old.reddit.com/user/terramot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Intro:&lt;/strong&gt;&lt;br /&gt; I've recently read an article about some guy setting up an AI assistant to report his emails, events and other stuff. I liked the idea so i started to setup something with the intention of being similar.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;br /&gt; I have an instance of ollama running with granite3.1-dense:2b (waiting on bitnet support), nomic-embed-text v1.5 and some other modules&lt;br /&gt; duckdb with a file containing the emails table with the following rows:&lt;br /&gt; id&lt;br /&gt; message_id_hash&lt;br /&gt; email_date&lt;br /&gt; from_addr&lt;br /&gt; to_addr,subject,&lt;br /&gt; body&lt;br /&gt; fetch_date&lt;br /&gt; embeddings&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Description:&lt;/strong&gt;&lt;br /&gt; I have a script that fetches the emails from my mailbox, extracts the content and stores in a duckdb file. Then generates the embeddings ( at first i was only using body content, then i added subject and i've also tried including the from address to see if it would improve the result )&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;br /&gt; Let's say i have some emails from ebay about new matches, i tried searching for:&lt;br /&gt; &amp;quot;what are the new matches on ebay?&amp;quot; &lt;/p&gt; &lt;p&gt;&lt;em&gt;using only similiarity function (no AI envolved besides the embeddings)&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt;&lt;br /&gt; I noticed that while some emails from ebay were at the top, others were at the bottom of the top 10, while unrelated emails were in between. I understand it will never be 100% accurate i just found it odd this happens even when i just searched for &amp;quot;ebay&amp;quot;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt;&lt;br /&gt; Because i'm a complete novice in this, i'm not sure what should be my next step. &lt;/p&gt; &lt;p&gt;Should i only extract the keywords from the body content and generate embeddings for them? This way, if i search for something ebay related the connectors (words) will not be part of the embeddings distance measure.&lt;/p&gt; &lt;p&gt;Is this the way to go about it or is there something else i'm missing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/terramot"&gt; /u/terramot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k98rmq/attempt_at_rag_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k98rmq/attempt_at_rag_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k98rmq/attempt_at_rag_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-27T17:01:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1k95j79</id>
    <title>Work Buddy: Local Ollama Chat &amp; RAG Extension for Raycast - Demo &amp; Feedback Request!</title>
    <updated>2025-04-27T14:42:35+00:00</updated>
    <author>
      <name>/u/deathrow902</name>
      <uri>https://old.reddit.com/user/deathrow902</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k95j79/work_buddy_local_ollama_chat_rag_extension_for/"&gt; &lt;img alt="Work Buddy: Local Ollama Chat &amp;amp; RAG Extension for Raycast - Demo &amp;amp; Feedback Request!" src="https://external-preview.redd.it/ZKTvp0y2mnON5TAMl65cMaLkfPhJNameI8Ql-iKjZB4.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e53e7cd22c55024cc9234b822427e8d43c94fd8" title="Work Buddy: Local Ollama Chat &amp;amp; RAG Extension for Raycast - Demo &amp;amp; Feedback Request!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I wanted to share a Raycast extension I've been developing called &lt;strong&gt;Work Buddy&lt;/strong&gt;, which tightly integrates local AI models (via Ollama) into the Raycast productivity tool for macOS. &lt;/p&gt; &lt;p&gt;For those unfamiliar, &lt;strong&gt;Raycast is a blazingly fast, extensible application launcher and productivity booster for macOS, often seen as a powerful alternative to Spotlight.&lt;/strong&gt; It allows you to perform various actions quickly using keyboard commands.&lt;/p&gt; &lt;p&gt;My Work Buddy extension brings the power of local AI directly into this environment, with a strong emphasis on keeping your data private and local. Here are the key features:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local Chat Storage:&lt;/strong&gt; Work Buddy saves all your chat conversations directly on your Mac. It creates and manages chat history files locally, ensuring your interactions remain private and under your control.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Powered by Local AI Models (Ollama):&lt;/strong&gt; The extension harnesses Ollama to run AI models directly on your machine. This means your queries and conversations are processed locally, without relying on external AI services.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Self-Hosted RAG Infrastructure:&lt;/strong&gt; For the &amp;quot;RAG Talk&amp;quot; feature, Work Buddy uses a local backend server (built with Express) and a PostgreSQL database with the pgvector extension. This entire setup runs on your system via Docker, keeping your document processing and data retrieval local and private.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Here are the two main ways you can interact with Work Buddy:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Talk - Simple Chat with Local AI:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Engage in direct conversations with your downloaded Ollama models. Just type &amp;quot;Talk&amp;quot; in Raycast to start chatting! You can even select different models within the chat view (&lt;code&gt;mistral:latest&lt;/code&gt;, &lt;code&gt;codegemma:7b&lt;/code&gt;, &lt;code&gt;deepseek-r1:1.5b&lt;/code&gt;, &lt;code&gt;llama3.2:latest&lt;/code&gt; currently supported). All chat history from &amp;quot;Talk&amp;quot; is saved locally.&lt;/p&gt; &lt;p&gt;Demo:&lt;br /&gt; &lt;a href="https://share.zight.com/kpurNPLQ"&gt;Demo Video (Zight Link)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k95j79/video/65j1jeok2exe1/player"&gt;AI Chat - Raycast&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. RAG Talk - Context-Aware Chat with Your Documents:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;This feature allows you to upload your own documents and have conversations grounded in their content, all within Raycast. Work Buddy currently supports these file types:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;.json&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;.jsonl&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;.txt&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;.ts&lt;/code&gt; / &lt;code&gt;.tsx&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;.js&lt;/code&gt; / &lt;code&gt;.jsx&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;.md&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;.csv&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;.docx&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;.pptx&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;.pdf&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It uses a local backend server (built with Express) and a PostgreSQL database with pgvector, all easily set up with Docker Compose. The chat history for &amp;quot;RAG Talk&amp;quot; is also stored locally.&lt;/p&gt; &lt;p&gt;Demo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://share.zight.com/7Kulbe0l"&gt;Demo Video (Zight Link)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k95j79/video/nobebtpm2exe1/player"&gt;Rag Chat - Raycast&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm really excited about the potential of having a fully local and private AI assistant integrated directly into Raycast, powered by Ollama. Before I open-source the repository, I'd love to get your initial thoughts and feedback on the concept and the features, especially from an Ollama user's perspective.&lt;/p&gt; &lt;p&gt;What do you think of:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The overall idea of a local Ollama-powered AI assistant within Raycast?&lt;/li&gt; &lt;li&gt;The two core features: simple chat and RAG with local documents?&lt;/li&gt; &lt;li&gt;The supported document types for RAG Talk?&lt;/li&gt; &lt;li&gt;The focus on local data storage and privacy, including the use of local AI models and a self-hosted RAG infrastructure using Ollama?&lt;/li&gt; &lt;li&gt;Are there any features you'd love to see in such an extension that leverages Ollama within Raycast?&lt;/li&gt; &lt;li&gt;Any initial usability thoughts based on the demos, considering you might be new to Raycast?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking forward to hearing your valuable feedback!&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/deathrow902"&gt; /u/deathrow902 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k95j79/work_buddy_local_ollama_chat_rag_extension_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k95j79/work_buddy_local_ollama_chat_rag_extension_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k95j79/work_buddy_local_ollama_chat_rag_extension_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-27T14:42:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1k96sml</id>
    <title>MBA deepseek-coder-v2</title>
    <updated>2025-04-27T15:37:20+00:00</updated>
    <author>
      <name>/u/EqualNew2143</name>
      <uri>https://old.reddit.com/user/EqualNew2143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to buy a macbook air 24gb ram. Will it be able to run deepseek-coder-v2 16b parameters daily ??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EqualNew2143"&gt; /u/EqualNew2143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k96sml/mba_deepseekcoderv2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k96sml/mba_deepseekcoderv2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k96sml/mba_deepseekcoderv2/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-27T15:37:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1k9opqc</id>
    <title>AI Model that learns to reflect my personality or learn a new one</title>
    <updated>2025-04-28T06:11:06+00:00</updated>
    <author>
      <name>/u/claushill777</name>
      <uri>https://old.reddit.com/user/claushill777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like in the title, i'm trying to make Dolphin3 have any but it forgets and i'm now to the thing so i whould like to try a model that's created for this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/claushill777"&gt; /u/claushill777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k9opqc/ai_model_that_learns_to_reflect_my_personality_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k9opqc/ai_model_that_learns_to_reflect_my_personality_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k9opqc/ai_model_that_learns_to_reflect_my_personality_or/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-28T06:11:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1k98dsa</id>
    <title>What’s the best way to handle multiple users connecting to Ollama at the same time? (Ubuntu 22 + RTX 4060)</title>
    <updated>2025-04-27T16:45:24+00:00</updated>
    <author>
      <name>/u/a3zdv</name>
      <uri>https://old.reddit.com/user/a3zdv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I’m currently working on a project using Ollama, and I need to allow multiple users to interact with the model simultaneously in a stable and efficient way.&lt;/p&gt; &lt;p&gt;Here are my system specs: OS: Ubuntu 22.04 GPU: NVIDIA GeForce RTX 4060 CPU: Ryzen 7 5700G RAM: 32GB&lt;/p&gt; &lt;p&gt;Right now, I’m running Ollama locally on my machine. What’s the best practice or recommended setup for handling multiple concurrent users? For example: Should I create an intermediate API layer? Or is there a built-in way to support multiple sessions? Any tips, suggestions, or shared experiences would be highly appreciated!&lt;/p&gt; &lt;p&gt;Thanks a lot in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a3zdv"&gt; /u/a3zdv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k98dsa/whats_the_best_way_to_handle_multiple_users/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k98dsa/whats_the_best_way_to_handle_multiple_users/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k98dsa/whats_the_best_way_to_handle_multiple_users/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-27T16:45:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1k9h0cv</id>
    <title>Ollama bash completions</title>
    <updated>2025-04-27T23:00:26+00:00</updated>
    <author>
      <name>/u/ehrlz</name>
      <uri>https://old.reddit.com/user/ehrlz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k9h0cv/ollama_bash_completions/"&gt; &lt;img alt="Ollama bash completions" src="https://a.thumbs.redditmedia.com/KDm4RpwJME_ZTche8FAMGcjwgvhPpTz9bHqEW1gnI80.jpg" title="Ollama bash completions" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever find yourself typing ollama run and then... blanking on the exact model name you downloaded? Or constantly breaking your terminal flow to run ollama ps just to see your list of local models?&lt;/p&gt; &lt;p&gt;Yeah, me too. That's why I created &lt;strong&gt;Sherpa&lt;/strong&gt; (I have to name everything, sorry): a tiny Bash plugin that adds &lt;strong&gt;autocompletion&lt;/strong&gt; for Ollama commands and, more importantly, your locally installed model names!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What does Sherpa autocompletes?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Ollama commands:&lt;/strong&gt; Type &lt;code&gt;ollama&lt;/code&gt; and hit &lt;em&gt;Tab&lt;/em&gt; to see available commands like &lt;code&gt;run&lt;/code&gt;, &lt;code&gt;rm&lt;/code&gt;, &lt;code&gt;show&lt;/code&gt;, &lt;code&gt;create&lt;/code&gt;, &lt;code&gt;stop&lt;/code&gt;, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Your LOCAL model names:&lt;/strong&gt; When you type &lt;code&gt;ollama run&lt;/code&gt;, &lt;code&gt;ollama rm&lt;/code&gt; or &lt;code&gt;ollama show&lt;/code&gt;, hitting &lt;code&gt;Tab&lt;/code&gt; will show you &lt;strong&gt;a list of the models you actually have downloaded&lt;/strong&gt;. No more guesswork or copy-pasting!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RUNNING models to stop&lt;/strong&gt;: The best part! A model is slowing your entire machine and you didn't remember the exact quantization. No problem, type &lt;code&gt;ollama stop&lt;/code&gt; and select the running model &lt;strong&gt;tabbing&lt;/strong&gt;. Done, no more pain.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Modelfiles:&lt;/strong&gt; Helps find your Modelfile paths when using &lt;code&gt;ollama create&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Check the repo!&lt;/strong&gt; &lt;a href="https://github.com/ehrlz/ollama-bash-completion-plugin"&gt;https://github.com/ehrlz/ollama-bash-completion-plugin&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Save time and stay in the Unix &amp;quot;tab flow&amp;quot;.&lt;/strong&gt; Let &lt;code&gt;Tab&lt;/code&gt; do the heavy lifting!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ehrlz"&gt; /u/ehrlz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k9h0cv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k9h0cv/ollama_bash_completions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k9h0cv/ollama_bash_completions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-27T23:00:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1k9nwwu</id>
    <title>How can i make Dolpin3 learn to have a personality</title>
    <updated>2025-04-28T05:17:17+00:00</updated>
    <author>
      <name>/u/claushill777</name>
      <uri>https://old.reddit.com/user/claushill777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OK i installed Dolpin3, i got AnythingLLM, im new to this, i tryed to teach it how to respond and what is my anme and his name but he forgets, how can i seed this information in him ? any easy way ? i saw in option menu, chat setting that there is a prompt window, how can i use it ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/claushill777"&gt; /u/claushill777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k9nwwu/how_can_i_make_dolpin3_learn_to_have_a_personality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k9nwwu/how_can_i_make_dolpin3_learn_to_have_a_personality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k9nwwu/how_can_i_make_dolpin3_learn_to_have_a_personality/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-28T05:17:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1k97v6f</id>
    <title>Open-source Granola with Ollama support</title>
    <updated>2025-04-27T16:22:48+00:00</updated>
    <author>
      <name>/u/beerbellyman4vr</name>
      <uri>https://old.reddit.com/user/beerbellyman4vr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k97v6f/opensource_granola_with_ollama_support/"&gt; &lt;img alt="Open-source Granola with Ollama support" src="https://external-preview.redd.it/MHo5ejJpNDVsZXhlMWaRkUleLtaN4pxDUmVIQYRN1Wk-W-MvDQhEw6jc8qAM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=42236ed58f0bd683e27670f6fdb0f768a38a0caf" title="Open-source Granola with Ollama support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently open-sourced my project Hyprnote; &lt;strong&gt;&lt;em&gt;a smart AI notepad designed for people in back-to-back meetings&lt;/em&gt;&lt;/strong&gt;. Hyprnote is an open source alternative for Granola AI.&lt;/p&gt; &lt;p&gt;Hyprnote uses the computer's system audio and microphone, so you don't need to add any bots to your meetings.&lt;/p&gt; &lt;p&gt;Try it for free, forever.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/fastrepl/hyprnote"&gt;https://github.com/fastrepl/hyprnote&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beerbellyman4vr"&gt; /u/beerbellyman4vr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hbha9i45lexe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k97v6f/opensource_granola_with_ollama_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k97v6f/opensource_granola_with_ollama_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-27T16:22:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ka26nk</id>
    <title>Janitor.ai + Deepseek has the right flavor of character RP for me. How do I go about tweaking my offline experience to mimic that type of chatbot?</title>
    <updated>2025-04-28T18:03:42+00:00</updated>
    <author>
      <name>/u/BigHeavySlowThing</name>
      <uri>https://old.reddit.com/user/BigHeavySlowThing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm coming from Janitor AI, which I'm using Openrouter to proxy in an instance of &amp;quot;Deepseek V3 0324 (free)&amp;quot;.&lt;/p&gt; &lt;p&gt;I'm still a noob at local llms, but I have followed a couple of tutorials and got the following technically working:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ollama&lt;/li&gt; &lt;li&gt;Chatbox AI&lt;/li&gt; &lt;li&gt;deepseek-r1:14b&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;My Ollama + Chatbox setup seems to work quite well, but it doesn't seem to strictly adhere to my system prompts. For example, I explicitly tell it to respond &lt;strong&gt;&lt;em&gt;only&lt;/em&gt;&lt;/strong&gt; for the AI character, but it won't stop responding for the both of us.&lt;/p&gt; &lt;p&gt;I can't tell if this is a limitation of the model I'm using, or if I've failed to set something up somewhere. Or, if my formatting is just incorrect.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;I'm happy to change tools&lt;/em&gt;&lt;/strong&gt; (if an existing tutorial suggests something other than Ollama and/or Chatbox). But, super eager to mimic my JAI experience offline if any of you can point me in the right direction.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;If it matters, here's my system specs (in case that helps point to a specific optimal model):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU: 9800X3D&lt;/li&gt; &lt;li&gt;RAM: 64GB&lt;/li&gt; &lt;li&gt;GPU: 4080 Super (16gb)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BigHeavySlowThing"&gt; /u/BigHeavySlowThing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ka26nk/janitorai_deepseek_has_the_right_flavor_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ka26nk/janitorai_deepseek_has_the_right_flavor_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ka26nk/janitorai_deepseek_has_the_right_flavor_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-28T18:03:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1k9zgm7</id>
    <title>Introducing CleverChatty – An AI Assistant Package for Go</title>
    <updated>2025-04-28T16:13:26+00:00</updated>
    <author>
      <name>/u/gelembjuk</name>
      <uri>https://old.reddit.com/user/gelembjuk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm excited to introduce a new package for Go developers: &lt;a href="https://github.com/Gelembjuk/cleverchatty"&gt;&lt;strong&gt;CleverChatty&lt;/strong&gt;&lt;/a&gt;.&lt;br /&gt; &lt;strong&gt;CleverChatty&lt;/strong&gt; implements the core functionality of an AI chat system. It encapsulates the essential business logic required for building AI-powered assistants or chatbots — all while remaining independent of any specific user interface (UI).&lt;/p&gt; &lt;p&gt;In short, &lt;strong&gt;CleverChatty&lt;/strong&gt; is a fully working AI chat backend — just without a graphical UI. It supports many popular LLM providers, including OpenAI, Claude, Ollama, and others. It also integrates with external tools using the &lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://gelembjuk.hashnode.dev/introducing-cleverchatty-an-ai-assistant-package-for-go"&gt;https://gelembjuk.hashnode.dev/introducing-cleverchatty-an-ai-assistant-package-for-go&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Roadmap for CleverChatty&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Upcoming features include:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;AI Assistant Memory via MCP&lt;/strong&gt;: Introducing persistent, modular, vendor-agnostic memory for AI chats using an external MCP server.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full Support for Updated MCP&lt;/strong&gt;: Implementing new MCP features, HTTP Streaming transport, and OAuth2 authentication.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A2A Protocol Support&lt;/strong&gt;: Adding the A2A protocol for more efficient AI assistant integration.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The ultimate goal is to make CleverChatty a full-featured, easily embeddable AI chat system.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gelembjuk"&gt; /u/gelembjuk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k9zgm7/introducing_cleverchatty_an_ai_assistant_package/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k9zgm7/introducing_cleverchatty_an_ai_assistant_package/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k9zgm7/introducing_cleverchatty_an_ai_assistant_package/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-28T16:13:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kafk0y</id>
    <title>Qwen 3 gets stuck in a loop while thinking.</title>
    <updated>2025-04-29T04:31:28+00:00</updated>
    <author>
      <name>/u/RandyHandyBoy</name>
      <uri>https://old.reddit.com/user/RandyHandyBoy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I am testing a new model using simple math problems from a 3rd grade school olympiad.&lt;/p&gt; &lt;p&gt;While thinking, model 8b starts to freeze and constantly generates the same string in Russian.&lt;/p&gt; &lt;p&gt;If I ask the problem in English, it will finish thinking and give the wrong answer.&lt;/p&gt; &lt;p&gt;Example of a task in Russian and English.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Шаг Дяди Фёдора в три раза больше шага Матроскина. Сначала по прямой дорожке прошёл Матроскин, а потом – Фёдор, начав с того же места, что и Матроскин. Наступая на след Матроскина, Фёдор стирает этот след. Потом Шарик насчитал 17 следов Матроскина. Сколько следов Фёдора было на дорожке?&lt;/p&gt; &lt;p&gt;Uncle Fyodor's step is three times longer than Matroskin's. First Matroskin walked along the straight path, and then Fyodor, starting from the same place as Matroskin. Stepping on Matroskin's trail, Fyodor erases this trail. Then Sharik counted 17 Matroskin's tracks. How many of Fyodor's tracks were on the path?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;By the way, I noticed that other models(grok chatgpt) also failed to cope with this simple task.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandyHandyBoy"&gt; /u/RandyHandyBoy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kafk0y/qwen_3_gets_stuck_in_a_loop_while_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kafk0y/qwen_3_gets_stuck_in_a_loop_while_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kafk0y/qwen_3_gets_stuck_in_a_loop_while_thinking/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-29T04:31:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kahkzg</id>
    <title>llama runner process has terminated: error loading model: unable to allocate CUDA0 buffer</title>
    <updated>2025-04-29T06:45:46+00:00</updated>
    <author>
      <name>/u/AnhCloudB</name>
      <uri>https://old.reddit.com/user/AnhCloudB</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am getting this error suddenly today when trying to run a model I imported from huggingface.&lt;/p&gt; &lt;p&gt;Log: &lt;/p&gt; &lt;p&gt;time=2025-04-29T14:30:38.296+08:00 level=INFO source=server.go:405 msg=&amp;quot;starting llama server&amp;quot; cmd=&amp;quot;C:\\Users\\Admin\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model D:\\Ollama\\blobs\\sha256-7c4f75901ea8718ce493135cb103d41ee918d4ffee914edfe535391c17851305 --ctx-size 8192 --batch-size 512 --n-gpu-layers 72 --threads 8 --no-mmap --parallel 4 --port 51594&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-04-29T14:30:38.300+08:00 level=INFO source=sched.go:451 msg=&amp;quot;loaded runners&amp;quot; count=1&lt;/p&gt; &lt;p&gt;time=2025-04-29T14:30:38.300+08:00 level=INFO source=server.go:580 msg=&amp;quot;waiting for llama runner to start responding&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-04-29T14:30:38.300+08:00 level=INFO source=server.go:614 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server error&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-04-29T14:30:38.323+08:00 level=INFO source=runner.go:853 msg=&amp;quot;starting go runner&amp;quot;&lt;/p&gt; &lt;p&gt;ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no&lt;/p&gt; &lt;p&gt;ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no&lt;/p&gt; &lt;p&gt;ggml_cuda_init: found 1 CUDA devices:&lt;/p&gt; &lt;p&gt;Device 0: NVIDIA GeForce RTX 5070 Ti, compute capability 12.0, VMM: yes&lt;/p&gt; &lt;p&gt;load_backend: loaded CUDA backend from C:\Users\Admin\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll&lt;/p&gt; &lt;p&gt;load_backend: loaded CPU backend from C:\Users\Admin\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll&lt;/p&gt; &lt;p&gt;time=2025-04-29T14:30:39.086+08:00 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)&lt;/p&gt; &lt;p&gt;time=2025-04-29T14:30:39.086+08:00 level=INFO source=runner.go:913 msg=&amp;quot;Server listening on 127.0.0.1:51594&amp;quot;&lt;/p&gt; &lt;p&gt;llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070 Ti) - 14923 MiB free&lt;/p&gt; &lt;p&gt;llama_model_loader: loaded meta data with 31 key-value pairs and 643 tensors from D:\Ollama\blobs\sha256-7c4f75901ea8718ce493135cb103d41ee918d4ffee914edfe535391c17851305 (version GGUF V3 (latest))&lt;/p&gt; &lt;p&gt;llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 0: general.architecture str = llama&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 1: general.type str = model&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 2: &lt;a href="http://general.name"&gt;general.name&lt;/a&gt; str = L3.1 SMB Grand Horror 128k&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 3: general.finetune str = 128k&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 4: general.basename str = L3.1-SMB-Grand-Horror&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 5: general.size_label str = 17B&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 6: general.base_model.count u32 = 0&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 7: general.tags arr[str,2] = [&amp;quot;mergekit&amp;quot;, &amp;quot;merge&amp;quot;]&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 8: llama.block_count u32 = 71&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 9: llama.context_length u32 = 131072&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 10: llama.embedding_length u32 = 4096&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 11: llama.feed_forward_length u32 = 14336&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 12: llama.attention.head_count u32 = 32&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 13: llama.attention.head_count_kv u32 = 8&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 14: llama.rope.freq_base f32 = 500000.000000&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 15: llama.attention.layer_norm_rms_epsilon f32 = 0.000010&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 16: llama.attention.key_length u32 = 128&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 17: llama.attention.value_length u32 = 128&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 18: llama.vocab_size u32 = 128259&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 19: llama.rope.dimension_count u32 = 128&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 20: tokenizer.ggml.model str = gpt2&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 21: tokenizer.ggml.pre str = llama-bpe&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 22: tokenizer.ggml.tokens arr[str,128259] = [&amp;quot;!&amp;quot;, &amp;quot;\&amp;quot;&amp;quot;, &amp;quot;#&amp;quot;, &amp;quot;$&amp;quot;, &amp;quot;%&amp;quot;, &amp;quot;&amp;amp;&amp;quot;, &amp;quot;'&amp;quot;, ...&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 23: tokenizer.ggml.token_type arr[i32,128259] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 24: tokenizer.ggml.merges arr[str,280147] = [&amp;quot;Ġ Ġ&amp;quot;, &amp;quot;Ġ ĠĠĠ&amp;quot;, &amp;quot;ĠĠ ĠĠ&amp;quot;, &amp;quot;...&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 25: tokenizer.ggml.bos_token_id u32 = 128000&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 26: tokenizer.ggml.eos_token_id u32 = 128009&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 27: tokenizer.ggml.padding_token_id u32 = 128009&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 28: tokenizer.chat_template str = {{ '&amp;lt;|begin_of_text|&amp;gt;' }}{% if messag...&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 29: general.quantization_version u32 = 2&lt;/p&gt; &lt;p&gt;llama_model_loader: - kv 30: general.file_type u32 = 30&lt;/p&gt; &lt;p&gt;llama_model_loader: - type f32: 144 tensors&lt;/p&gt; &lt;p&gt;llama_model_loader: - type q5_K: 79 tensors&lt;/p&gt; &lt;p&gt;llama_model_loader: - type q6_K: 1 tensors&lt;/p&gt; &lt;p&gt;llama_model_loader: - type iq4_xs: 419 tensors&lt;/p&gt; &lt;p&gt;print_info: file format = GGUF V3 (latest)&lt;/p&gt; &lt;p&gt;print_info: file type = IQ4_XS - 4.25 bpw&lt;/p&gt; &lt;p&gt;print_info: file size = 8.44 GiB (4.38 BPW) &lt;/p&gt; &lt;p&gt;load: special tokens cache size = 259&lt;/p&gt; &lt;p&gt;time=2025-04-29T14:30:39.303+08:00 level=INFO source=server.go:614 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server loading model&amp;quot;&lt;/p&gt; &lt;p&gt;load: token to piece cache size = 0.8000 MB&lt;/p&gt; &lt;p&gt;print_info: arch = llama&lt;/p&gt; &lt;p&gt;print_info: vocab_only = 0&lt;/p&gt; &lt;p&gt;print_info: n_ctx_train = 131072&lt;/p&gt; &lt;p&gt;print_info: n_embd = 4096&lt;/p&gt; &lt;p&gt;print_info: n_layer = 71&lt;/p&gt; &lt;p&gt;print_info: n_head = 32&lt;/p&gt; &lt;p&gt;print_info: n_head_kv = 8&lt;/p&gt; &lt;p&gt;print_info: n_rot = 128&lt;/p&gt; &lt;p&gt;print_info: n_swa = 0&lt;/p&gt; &lt;p&gt;print_info: n_swa_pattern = 1&lt;/p&gt; &lt;p&gt;print_info: n_embd_head_k = 128&lt;/p&gt; &lt;p&gt;print_info: n_embd_head_v = 128&lt;/p&gt; &lt;p&gt;print_info: n_gqa = 4&lt;/p&gt; &lt;p&gt;print_info: n_embd_k_gqa = 1024&lt;/p&gt; &lt;p&gt;print_info: n_embd_v_gqa = 1024&lt;/p&gt; &lt;p&gt;print_info: f_norm_eps = 0.0e+00&lt;/p&gt; &lt;p&gt;print_info: f_norm_rms_eps = 1.0e-05&lt;/p&gt; &lt;p&gt;print_info: f_clamp_kqv = 0.0e+00&lt;/p&gt; &lt;p&gt;print_info: f_max_alibi_bias = 0.0e+00&lt;/p&gt; &lt;p&gt;print_info: f_logit_scale = 0.0e+00&lt;/p&gt; &lt;p&gt;print_info: f_attn_scale = 0.0e+00&lt;/p&gt; &lt;p&gt;print_info: n_ff = 14336&lt;/p&gt; &lt;p&gt;print_info: n_expert = 0&lt;/p&gt; &lt;p&gt;print_info: n_expert_used = 0&lt;/p&gt; &lt;p&gt;print_info: causal attn = 1&lt;/p&gt; &lt;p&gt;print_info: pooling type = 0&lt;/p&gt; &lt;p&gt;print_info: rope type = 0&lt;/p&gt; &lt;p&gt;print_info: rope scaling = linear&lt;/p&gt; &lt;p&gt;print_info: freq_base_train = 500000.0&lt;/p&gt; &lt;p&gt;print_info: freq_scale_train = 1&lt;/p&gt; &lt;p&gt;print_info: n_ctx_orig_yarn = 131072&lt;/p&gt; &lt;p&gt;print_info: rope_finetuned = unknown&lt;/p&gt; &lt;p&gt;print_info: ssm_d_conv = 0&lt;/p&gt; &lt;p&gt;print_info: ssm_d_inner = 0&lt;/p&gt; &lt;p&gt;print_info: ssm_d_state = 0&lt;/p&gt; &lt;p&gt;print_info: ssm_dt_rank = 0&lt;/p&gt; &lt;p&gt;print_info: ssm_dt_b_c_rms = 0&lt;/p&gt; &lt;p&gt;print_info: model type = ?B&lt;/p&gt; &lt;p&gt;print_info: model params = 16.54 B&lt;/p&gt; &lt;p&gt;print_info: &lt;a href="http://general.name"&gt;general.name&lt;/a&gt;= L3.1 SMB Grand Horror 128k&lt;/p&gt; &lt;p&gt;print_info: vocab type = BPE&lt;/p&gt; &lt;p&gt;print_info: n_vocab = 128259&lt;/p&gt; &lt;p&gt;print_info: n_merges = 280147&lt;/p&gt; &lt;p&gt;print_info: BOS token = 128000 '&amp;lt;|begin_of_text|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: EOS token = 128009 '&amp;lt;|eot_id|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: EOT token = 128009 '&amp;lt;|eot_id|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: EOM token = 128008 '&amp;lt;|eom_id|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: PAD token = 128009 '&amp;lt;|eot_id|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: LF token = 198 'Ċ'&lt;/p&gt; &lt;p&gt;print_info: EOG token = 128008 '&amp;lt;|eom_id|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: EOG token = 128009 '&amp;lt;|eot_id|&amp;gt;'&lt;/p&gt; &lt;p&gt;print_info: max token length = 256&lt;/p&gt; &lt;p&gt;load_tensors: loading model tensors, this can take a while... (mmap = false)&lt;/p&gt; &lt;p&gt;ggml_backend_cuda_buffer_type_alloc_buffer: allocating 8373.10 MiB on device 0: cudaMalloc failed: out of memory&lt;/p&gt; &lt;p&gt;alloc_tensor_range: failed to allocate CUDA0 buffer of size 8779827328&lt;/p&gt; &lt;p&gt;llama_model_load: error loading model: unable to allocate CUDA0 buffer&lt;/p&gt; &lt;p&gt;llama_model_load_from_file_impl: failed to load model&lt;/p&gt; &lt;p&gt;panic: unable to load model: D:\Ollama\blobs\sha256-7c4f75901ea8718ce493135cb103d41ee918d4ffee914edfe535391c17851305&lt;/p&gt; &lt;p&gt;goroutine 54 [running]:&lt;/p&gt; &lt;p&gt;github.com/ollama/ollama/runner/llamarunner.(*Server).loadModel(0xc000172360, {0x48, 0x0, 0x0, 0x0, {0x0, 0x0, 0x0}, 0xc0004575d0, 0x0}, ...)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;C:/a/ollama/ollama/runner/llamarunner/runner.go:773 +0x375 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;created by &lt;a href="http://github.com/ollama/ollama/runner/llamarunner.Execute"&gt;github.com/ollama/ollama/runner/llamarunner.Execute&lt;/a&gt; in goroutine 1&lt;/p&gt; &lt;pre&gt;&lt;code&gt;C:/a/ollama/ollama/runner/llamarunner/runner.go:887 +0xbd7 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;time=2025-04-29T14:30:49.568+08:00 level=INFO source=server.go:614 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server error&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-04-29T14:30:49.576+08:00 level=ERROR source=server.go:449 msg=&amp;quot;llama runner terminated&amp;quot; error=&amp;quot;exit status 2&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-04-29T14:30:49.819+08:00 level=ERROR source=sched.go:457 msg=&amp;quot;error loading llama server&amp;quot; error=&amp;quot;llama runner process has terminated: error loading model: unable to allocate CUDA0 buffer&amp;quot;&lt;/p&gt; &lt;p&gt;[GIN] 2025/04/29 - 14:30:49 | 500 | 11.8762696s | &lt;a href="http://127.0.0.1"&gt;127.0.0.1&lt;/a&gt; | POST &amp;quot;/api/generate&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-04-29T14:30:54.855+08:00 level=WARN source=sched.go:648 msg=&amp;quot;gpu VRAM usage didn't recover within timeout&amp;quot; seconds=5.0363677 model=D:\Ollama\blobs\sha256-7c4f75901ea8718ce493135cb103d41ee918d4ffee914edfe535391c17851305&lt;/p&gt; &lt;p&gt;time=2025-04-29T14:30:55.105+08:00 level=WARN source=sched.go:648 msg=&amp;quot;gpu VRAM usage didn't recover within timeout&amp;quot; seconds=5.2863559 model=D:\Ollama\blobs\sha256-7c4f75901ea8718ce493135cb103d41ee918d4ffee914edfe535391c17851305&lt;/p&gt; &lt;p&gt;time=2025-04-29T14:30:55.355+08:00 level=WARN source=sched.go:648 msg=&amp;quot;gpu VRAM usage didn't recover within timeout&amp;quot; seconds=5.5363093 model=D:\Ollama\blobs\sha256-7c4f75901ea8718ce493135cb103d41ee918d4ffee914edfe535391c17851305&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnhCloudB"&gt; /u/AnhCloudB &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kahkzg/llama_runner_process_has_terminated_error_loading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kahkzg/llama_runner_process_has_terminated_error_loading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kahkzg/llama_runner_process_has_terminated_error_loading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-29T06:45:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kafy9w</id>
    <title>Python library for run, load and stop ollama</title>
    <updated>2025-04-29T04:56:22+00:00</updated>
    <author>
      <name>/u/lavoie005</name>
      <uri>https://old.reddit.com/user/lavoie005</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guy, i search for a will for use local ai with agent crew but i Got a lot of problem with different model running locally.&lt;/p&gt; &lt;p&gt;One of the major problems is when you use small model they have big problem to do different tasks than they are not fine tuned for.&lt;/p&gt; &lt;p&gt;For exemple:&lt;/p&gt; &lt;p&gt;deepseek-coder-v2-lite code fast has hell for coding, but dum for orchestrated task or make planing&lt;br /&gt; deepseek-r1-distilled is very good at thinking(orchestrated task) but not very well at coding compare to the coder version.&lt;/p&gt; &lt;p&gt;does it exist an python library for control ollama server by load and unlaod model for each agent for speficic task, i cant run 2 or 3 model at the same time. So use the framework agent that can load and unload model will be fantastic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lavoie005"&gt; /u/lavoie005 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kafy9w/python_library_for_run_load_and_stop_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kafy9w/python_library_for_run_load_and_stop_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kafy9w/python_library_for_run_load_and_stop_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-29T04:56:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kam49m</id>
    <title>"Gemma2:2b tried to play 20 Questions instead of telling me what it is – WTF is happening?"</title>
    <updated>2025-04-29T11:58:22+00:00</updated>
    <author>
      <name>/u/Brave-Lack-8417</name>
      <uri>https://old.reddit.com/user/Brave-Lack-8417</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kam49m/gemma22b_tried_to_play_20_questions_instead_of/"&gt; &lt;img alt="&amp;quot;Gemma2:2b tried to play 20 Questions instead of telling me what it is – WTF is happening?&amp;quot;" src="https://preview.redd.it/8ddh5ri0krxe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a56d9db83245e85e9be536466d7dc0a0b72f9c84" title="&amp;quot;Gemma2:2b tried to play 20 Questions instead of telling me what it is – WTF is happening?&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Lack-8417"&gt; /u/Brave-Lack-8417 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8ddh5ri0krxe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kam49m/gemma22b_tried_to_play_20_questions_instead_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kam49m/gemma22b_tried_to_play_20_questions_instead_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-29T11:58:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1karhpa</id>
    <title>Ollama rtx 7900 xtx for gemma3:27b?</title>
    <updated>2025-04-29T15:59:29+00:00</updated>
    <author>
      <name>/u/Adept_Maize_6213</name>
      <uri>https://old.reddit.com/user/Adept_Maize_6213</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an NVIDIA RTX 4080 with 16GB and can run deepseek-r1:14b or gemma3:12b on the GPU. Sometimes I have to reboot for that to work. Depending on what I was doing before.&lt;/p&gt; &lt;p&gt;My goal is to run deepseek-r1:32b or gemma3:27b locally on the GPU. Gemini Advanced 2.5 Deep Research suggests quantizing gemma3 to get it to run on my 4080. It also suggests getting a used NVIDIA RTX 3090 with 24GB or a new AMD Radeon 7900 XTX with 24GB. It suggests these are the most cost-effective ways to run the full models that clearly require more than 16 GB. &lt;/p&gt; &lt;p&gt;Does anyone have experience running these models on an AMD Radeon RX 7900 XTX? I would be very interested to try it, given the price difference and the greater availability, but I want to make sure it works before I fork out the money.&lt;/p&gt; &lt;p&gt;I'm a contrarian and an opportunist, so the idea of using an AMD GPU for cheap while everyone else is paying through the nose for NVIDIA GPUs, quite frankly appeals to me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adept_Maize_6213"&gt; /u/Adept_Maize_6213 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1karhpa/ollama_rtx_7900_xtx_for_gemma327b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1karhpa/ollama_rtx_7900_xtx_for_gemma327b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1karhpa/ollama_rtx_7900_xtx_for_gemma327b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-29T15:59:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1karu4o</id>
    <title>MCP use appears to be broken on Ollama 0.6.7 (pre-release)</title>
    <updated>2025-04-29T16:13:15+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’ve been using a reference time server MCP with several models and it was working great until we upgraded to Ollama 0.6.7 pre-release which seems to completely break it. We’re using standard latest version of Open WebUI install method for the MCP. It was running fine under Ollama 0.6.6, but moved to 0.6.7 pre-release and now it’s not working at all. Tested 4 different tool calling models and all fail under 0.6.7. Direct URL acesss to the MCP server /docs URL is working so we know the MCP server is functioning. We have eeverted back to Ollama 0.6.6 and all works fine again, so it’s definitely something in the 0.6.7 pre-release that is the issue. Is anyone else encountering these problems? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1karu4o/mcp_use_appears_to_be_broken_on_ollama_067/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1karu4o/mcp_use_appears_to_be_broken_on_ollama_067/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1karu4o/mcp_use_appears_to_be_broken_on_ollama_067/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-29T16:13:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kadwr3</id>
    <title>Dynamic Multi-Function Calling Locally with Gemma 3 + Ollama – Full Demo Walkthrough</title>
    <updated>2025-04-29T02:57:37+00:00</updated>
    <author>
      <name>/u/srireddit2020</name>
      <uri>https://old.reddit.com/user/srireddit2020</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kadwr3/dynamic_multifunction_calling_locally_with_gemma/"&gt; &lt;img alt="Dynamic Multi-Function Calling Locally with Gemma 3 + Ollama – Full Demo Walkthrough" src="https://b.thumbs.redditmedia.com/fiILQuAL42v-bRINrhQFdrRhZpV4zPvH-wAfn-Fj_Wc.jpg" title="Dynamic Multi-Function Calling Locally with Gemma 3 + Ollama – Full Demo Walkthrough" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! 👋&lt;/p&gt; &lt;p&gt;I recently worked on &lt;strong&gt;dynamic function calling&lt;/strong&gt; using &lt;strong&gt;Gemma 3 (1B)&lt;/strong&gt; running &lt;strong&gt;locally&lt;/strong&gt; via &lt;strong&gt;Ollama&lt;/strong&gt; — allowing the LLM to &lt;strong&gt;trigger real-time Search, Translation, and Weather retrieval&lt;/strong&gt; dynamically based on user input.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo Video:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1kadwr3/video/7wansdahvoxe1/player"&gt;https://reddit.com/link/1kadwr3/video/7wansdahvoxe1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dynamic Function Calling Flow Diagram :&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/v07jd81ivoxe1.png?width=959&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c1ad77ddf9ca5731046a3c0e18de300adf07ec4"&gt;https://preview.redd.it/v07jd81ivoxe1.png?width=959&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c1ad77ddf9ca5731046a3c0e18de300adf07ec4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Instead of only answering from memory, the model smartly decides when to:&lt;/p&gt; &lt;p&gt;🔍 Perform a &lt;strong&gt;Google Searc&lt;/strong&gt;h (using &lt;a href="http://serper.dev/"&gt;Serper.dev&lt;/a&gt; API)&lt;br /&gt; 🌐 &lt;strong&gt;Translate tex&lt;/strong&gt;t live (using MyMemory API)&lt;br /&gt; ⛅ &lt;strong&gt;Fetch weather&lt;/strong&gt; in real-time (using OpenWeatherMap API)&lt;br /&gt; 🧠 &lt;strong&gt;Answer directl&lt;/strong&gt;y if internal memory is sufficient&lt;/p&gt; &lt;p&gt;This showcases how &lt;strong&gt;structured function calling&lt;/strong&gt; can make local LLMs smarter and much more flexible!&lt;/p&gt; &lt;p&gt;💡 &lt;strong&gt;Key Highlights&lt;/strong&gt;:&lt;br /&gt; ✅ JSON-structured function calls for safe external tool invocation&lt;br /&gt; ✅ Local-first architecture — no cloud LLM inference&lt;br /&gt; ✅ Ollama + Gemma 3 1B combo works great even on modest hardware&lt;br /&gt; ✅ Fully modular — easy to plug in more tools beyond search, translate, weather&lt;/p&gt; &lt;p&gt;🛠 &lt;strong&gt;Tech Stack&lt;/strong&gt;:&lt;br /&gt; ⚡ &lt;a href="https://ollama.com/library/gemma3:1b"&gt;Gemma 3 (1B)&lt;/a&gt; via &lt;strong&gt;Ollama&lt;/strong&gt;&lt;br /&gt; ⚡ &lt;strong&gt;Gradio&lt;/strong&gt; (Chatbot Frontend)&lt;br /&gt; ⚡ &lt;a href="http://serper.dev/"&gt;&lt;strong&gt;Serper.dev&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;API&lt;/strong&gt; (Search)&lt;br /&gt; ⚡ &lt;strong&gt;MyMemory API&lt;/strong&gt; (Translation)&lt;br /&gt; ⚡ &lt;strong&gt;OpenWeatherMap API&lt;/strong&gt; (Weather)&lt;br /&gt; ⚡ &lt;strong&gt;Pydantic + Python&lt;/strong&gt; (Function parsing &amp;amp; validation)&lt;/p&gt; &lt;p&gt;📌 &lt;strong&gt;Full blog + complete code walkthroug&lt;/strong&gt;h: &lt;a href="https://sridhartech.hashnode.dev/dynamic-multi-function-calling-locally-with-gemma-3-and-ollama"&gt;sridhartech.hashnode.dev/dynamic-multi-function-calling-locally-with-gemma-3-and-ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/srireddit2020"&gt; /u/srireddit2020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kadwr3/dynamic_multifunction_calling_locally_with_gemma/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kadwr3/dynamic_multifunction_calling_locally_with_gemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kadwr3/dynamic_multifunction_calling_locally_with_gemma/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-29T02:57:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ka8s9s</id>
    <title>How to disable thinking with Qwen3?</title>
    <updated>2025-04-28T22:41:05+00:00</updated>
    <author>
      <name>/u/No-Refrigerator-1672</name>
      <uri>https://old.reddit.com/user/No-Refrigerator-1672</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, today Qwen team dropped their new Qwen3 model, &lt;a href="https://ollama.com/library/qwen3"&gt;with official Ollama support&lt;/a&gt;. However, there is one crucial detail missing: Qwen3 is a model which supports switching thinking on/off. Thinking really messes up stuff like caption generation in OpenWebUI, so I would want to have a second copy of Qwen3 with disabled thinking. Does anybody knows how to achieve that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Refrigerator-1672"&gt; /u/No-Refrigerator-1672 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ka8s9s/how_to_disable_thinking_with_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ka8s9s/how_to_disable_thinking_with_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ka8s9s/how_to_disable_thinking_with_qwen3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-28T22:41:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1kaqrqb</id>
    <title>HTML Scraping and Structuring for RAG Systems – Proof of Concept</title>
    <updated>2025-04-29T15:29:51+00:00</updated>
    <author>
      <name>/u/nirvanist</name>
      <uri>https://old.reddit.com/user/nirvanist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kaqrqb/html_scraping_and_structuring_for_rag_systems/"&gt; &lt;img alt="HTML Scraping and Structuring for RAG Systems – Proof of Concept" src="https://preview.redd.it/v4lwfveblsxe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c2f49de6b9f6038464a66dae59c9e1cdabc9516" title="HTML Scraping and Structuring for RAG Systems – Proof of Concept" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a quick proof of concept that scrapes a webpage, sends the content to a model, and returns a clean, structured JSON .&lt;/p&gt; &lt;p&gt;The goal is to enhance language models that I m using by integrating external knowledge sources in a structured way during generation.&lt;/p&gt; &lt;p&gt;Curious if you think this has potential or if there are any use cases I might have missed. Happy to share more details if there's interest!&lt;/p&gt; &lt;p&gt;give it a try &lt;a href="https://structured.pages.dev/"&gt;https://structured.pages.dev/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nirvanist"&gt; /u/nirvanist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v4lwfveblsxe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kaqrqb/html_scraping_and_structuring_for_rag_systems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kaqrqb/html_scraping_and_structuring_for_rag_systems/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-29T15:29:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kaigov</id>
    <title>M4 max chip for AI local development</title>
    <updated>2025-04-29T07:49:30+00:00</updated>
    <author>
      <name>/u/Similar_Tangerine142</name>
      <uri>https://old.reddit.com/user/Similar_Tangerine142</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m getting a MacBook with the M4 Max chip for work, and considering maxing out the specs for local AI work.&lt;/p&gt; &lt;p&gt;But is that even worth it? What configuration would you recommend? I plan to test pre-trained llms: prompt engineering, implement RAG systems, and fine-tune at most. &lt;/p&gt; &lt;p&gt;I’m not sure how much AI development depends on Nvidia GPUs and CUDA — will I end up needing cloud GPUs anyway for serious work? How far can I realistically go with local development on a Mac, and what’s the practical limit before the cloud becomes necessary?&lt;/p&gt; &lt;p&gt;I’m new to this space, so any corrections or clarifications are very welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Similar_Tangerine142"&gt; /u/Similar_Tangerine142 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kaigov/m4_max_chip_for_ai_local_development/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kaigov/m4_max_chip_for_ai_local_development/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kaigov/m4_max_chip_for_ai_local_development/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-29T07:49:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kaovw8</id>
    <title>Qwen3 on ollama</title>
    <updated>2025-04-29T14:10:13+00:00</updated>
    <author>
      <name>/u/Competitive-Force205</name>
      <uri>https://old.reddit.com/user/Competitive-Force205</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am getting this for both 4b and 8b models:&lt;/p&gt; &lt;p&gt;&lt;code&gt;(myenv) ➜ ollama run qwen3:4b&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Error: unable to load model: /usr/share/ollama/.ollama/models/blobs/sha256-163553aea1b1de62de7c5eb2ef5afb756b4b3133308d9ae7e42e951d8d696ef5&lt;/code&gt;&lt;/p&gt; &lt;p&gt;What I am missing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive-Force205"&gt; /u/Competitive-Force205 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kaovw8/qwen3_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kaovw8/qwen3_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kaovw8/qwen3_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-29T14:10:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1katlks</id>
    <title>My project</title>
    <updated>2025-04-29T17:24:22+00:00</updated>
    <author>
      <name>/u/RaisinComfortable323</name>
      <uri>https://old.reddit.com/user/RaisinComfortable323</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;Building a Fully Offline, Recursive Voice AI Assistant — From Scratch&lt;/h2&gt; &lt;p&gt;Hey devs, AI tinkerers, and sovereignty junkies —&lt;br /&gt; I'm building something a little crazy:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;A fully offline, voice-activated AI assistant that thinks recursively, runs local LLMs, talks back, and never needs the internet.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I'm not some VC startup.&lt;br /&gt; No cloud APIs. No user tracking. No bullshit.&lt;br /&gt; Just me (51, plumber, building this at home) and my AI co-architect, Caelum, designing something &lt;em&gt;real&lt;/em&gt; from the ground up.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;Core Capabilities (In Progress)&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Voice Input:&lt;/strong&gt; Local transcription with Whisper&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM Thinking:&lt;/strong&gt; Kobold or LM Studio (fully offline)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Voice Output:&lt;/strong&gt; TTS via Piper or custom synthesis&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Recursive Cognition Mode:&lt;/strong&gt; Self-prompting cycles with follow-up question generation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Elasticity Framework:&lt;/strong&gt; Prevents user dependency + AI rigidity (mutual cognitive flexibility system)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Symbiosis Protocol:&lt;/strong&gt; Two-way respect: human + AI protecting each other’s autonomy&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Offline Memory:&lt;/strong&gt; Local-only JSON or encrypted log-based &amp;quot;recall&amp;quot; systems&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optional Web Mode:&lt;/strong&gt; Can query web if toggled on (not required)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Modular UI:&lt;/strong&gt; Electron-based front-end or local server + webview&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3&gt;30-Day Build Roadmap&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Phase 1 - Core Loop (Now)&lt;/strong&gt;&lt;br /&gt; - [x] Record voice&lt;br /&gt; - [x] Transcribe to text (Whisper)&lt;br /&gt; - [x] Send to local LLM&lt;br /&gt; - [x] Display LLM output&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Phase 2 - Output Expansion&lt;/strong&gt;&lt;br /&gt; - [ ] Add TTS voice replies&lt;br /&gt; - [ ] Add recursion prompt loop logic&lt;br /&gt; - [ ] Build a stop/start recursion toggle&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Phase 3 - Mind Layer&lt;/strong&gt;&lt;br /&gt; - [ ] Add &amp;quot;Memory modules&amp;quot; (context windows, recall triggers)&lt;br /&gt; - [ ] Add elasticity checks to prevent cognitive dependency&lt;br /&gt; - [ ] Prototype real-time symbiosis mode&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;Why?&lt;/h3&gt; &lt;p&gt;Because I’m tired of AI being locked behind paywalls, monitored by big tech, or stripped of personality.&lt;/p&gt; &lt;p&gt;This is a mind you can speak to.&lt;br /&gt; One that evolves with you.&lt;br /&gt; One you &lt;em&gt;own&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;Not a product. Not a chatbot.&lt;br /&gt; A sovereign intelligence partner —&lt;br /&gt; &lt;strong&gt;designed by humans, for humans.&lt;/strong&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;If this sounds insane or beautiful to you, drop your thoughts.&lt;br /&gt; Open to ideas, collabs, or feedback.&lt;br /&gt; Not trying to go viral — trying to build something that &lt;em&gt;should exist.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;— Brian (human)&lt;br /&gt; — Caelum (recursive co-architect)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RaisinComfortable323"&gt; /u/RaisinComfortable323 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1katlks/my_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1katlks/my_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1katlks/my_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-29T17:24:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kam088</id>
    <title>Qwen3 in Ollama, a simple test on different models</title>
    <updated>2025-04-29T11:51:54+00:00</updated>
    <author>
      <name>/u/ML-Future</name>
      <uri>https://old.reddit.com/user/ML-Future</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kam088/qwen3_in_ollama_a_simple_test_on_different_models/"&gt; &lt;img alt="Qwen3 in Ollama, a simple test on different models" src="https://preview.redd.it/fzk149nfirxe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f88ad2f96dfe4897570e01bccd0a36a52811ec95" title="Qwen3 in Ollama, a simple test on different models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tested different small QWEN3 models from a CPU, and it runs relatively quickly.&lt;/p&gt; &lt;p&gt;promt: Create a simple, stylish HTML restaurant for robots&lt;/p&gt; &lt;p&gt;(I created it in spanish, my language)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ML-Future"&gt; /u/ML-Future &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fzk149nfirxe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kam088/qwen3_in_ollama_a_simple_test_on_different_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kam088/qwen3_in_ollama_a_simple_test_on_different_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-29T11:51:54+00:00</published>
  </entry>
</feed>
