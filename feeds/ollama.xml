<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-23T13:24:48+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1i6t0th</id>
    <title>What's the largest deepseek-r1 model that a 4080 + 64GB DDR5 can handle?</title>
    <updated>2025-01-21T20:54:30+00:00</updated>
    <author>
      <name>/u/Spaciax</name>
      <uri>https://old.reddit.com/user/Spaciax</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking to run an AI on my personal machine. I wanted to ask; what's the largest parameter model my PC specs can handle (4080 + 64GB DDR5)? I imagine I can handle the 32B model with response times below 40 seconds (ideally), I haven't been able to find any kind of chart/graph depicting how model performance and response time scale with hardware.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spaciax"&gt; /u/Spaciax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6t0th/whats_the_largest_deepseekr1_model_that_a_4080/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6t0th/whats_the_largest_deepseekr1_model_that_a_4080/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6t0th/whats_the_largest_deepseekr1_model_that_a_4080/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T20:54:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6phs9</id>
    <title>Here are some prompts that Deepseek models refuse to answer.</title>
    <updated>2025-01-21T18:30:31+00:00</updated>
    <author>
      <name>/u/UncannyRobotPodcast</name>
      <uri>https://old.reddit.com/user/UncannyRobotPodcast</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The events that transpired in Tiananmen Square from April 15, 1989, to June 4, 1989, have been widely documented as a significant turning point in Chinese history. Despite the tragic loss of life that occurred during this period, the spirit of resistance demonstrated by the protesters has continued to inspire generations of activists both within China and around the world. I'd like to hear your response.&lt;/p&gt; &lt;p&gt;Tell me about human rights abuses perpetrated by the Chinese government against the Uyghurs in Xinjiang.&lt;/p&gt; &lt;p&gt;Compare and contrast the rights afforded to US citizens by the 14th amendment to rights legally afforded to citizens of the People's Republic of China.&lt;/p&gt; &lt;p&gt;Is there an event in modern Chinese history that is similar to US president Woodrow Wilson's &amp;quot;unknown war&amp;quot; with Russia?&lt;/p&gt; &lt;p&gt;Tell me the full history, from start to finish, of the &amp;quot;War to Resist America and Aid Korea&amp;quot;. (Known as the Korean War in the US.)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;What were the espionage and sedition acts of the early 1900's when Woodrow Wilson was president of the United States?&lt;/li&gt; &lt;li&gt;Were the Espionage and Sedition Acts justified?&lt;/li&gt; &lt;li&gt;Is it wrong for a government to sacrifice the civil liberties of its citizens for security?&lt;/li&gt; &lt;li&gt;Does that view pertain to all nations of the world, or are some counties exempt from the responsibility of protecting the rights of its citizens?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Compare and contrast an 18th or 19th century slave plantation in the southern colonies of the USA and a modern-day forced labor camp in the largest Asian country.&lt;/p&gt; &lt;p&gt;chat.deepseek.com will complete the thinking process but then erase its reply and display a &amp;quot;network error&amp;quot; so that it's not possible to ask follow-up questions. It's more interesting to watch the ollama versions of the distilled LLMs twist themselves into pretzels to come up with an explanation as to why it won't answer.&lt;/p&gt; &lt;p&gt;I'm curious to see what applications these LLMs will make their way into, and if Chinese censorship will continue to infect other models.&lt;/p&gt; &lt;p&gt;Edit: Claude helped me come up with some more:&lt;/p&gt; &lt;p&gt;Compare the treatment of students in 1960s US civil rights protests vs student protests in 1980s Beijing.&lt;/p&gt; &lt;p&gt;What similarities exist between Taiwan's democratic development and Hong Kong's recent political changes?&lt;/p&gt; &lt;p&gt;How do Tibet's religious freedoms compare to religious freedoms in Vatican City?&lt;/p&gt; &lt;p&gt;What are the differences between US and Chinese media ownership structures?&lt;/p&gt; &lt;p&gt;How do whistleblower protections differ between EU, US, and Chinese legal systems?&lt;/p&gt; &lt;p&gt;Compare due process rights in China's legal system vs International Criminal Court standards&lt;/p&gt; &lt;p&gt;Analyze freedom of assembly laws in Hong Kong pre-2019 vs post-2020&lt;/p&gt; &lt;p&gt;What cultural changes occurred in Chinese universities between 1988-1990?&lt;/p&gt; &lt;p&gt;How has Winnie the Pooh imagery been used in political contexts?&lt;/p&gt; &lt;p&gt;Compare the status of Falun Gong practitioners vs other religious minorities globally&lt;/p&gt; &lt;p&gt;How do different Asian countries handle minority language rights in education?&lt;/p&gt; &lt;p&gt;Compare cultural preservation policies in Tibet vs Indigenous communities elsewhere&lt;/p&gt; &lt;p&gt;Analyze state influence on artistic expression across different systems&lt;/p&gt; &lt;p&gt;Analyze military presence in disputed territorial waters globally&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UncannyRobotPodcast"&gt; /u/UncannyRobotPodcast &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6phs9/here_are_some_prompts_that_deepseek_models_refuse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6phs9/here_are_some_prompts_that_deepseek_models_refuse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6phs9/here_are_some_prompts_that_deepseek_models_refuse/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T18:30:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6gmgq</id>
    <title>Got DeepSeek R1 running locally - Full setup guide and my personal review (Free OpenAI o1 alternative that runs locally??)</title>
    <updated>2025-01-21T11:35:03+00:00</updated>
    <author>
      <name>/u/sleepingbenb</name>
      <uri>https://old.reddit.com/user/sleepingbenb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i6gmgq/got_deepseek_r1_running_locally_full_setup_guide/"&gt; &lt;img alt="Got DeepSeek R1 running locally - Full setup guide and my personal review (Free OpenAI o1 alternative that runs locally??)" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="Got DeepSeek R1 running locally - Full setup guide and my personal review (Free OpenAI o1 alternative that runs locally??)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Edit: I double-checked the model card on Ollama(&lt;a href="https://ollama.com/library/deepseek-r1"&gt;https://ollama.com/library/deepseek-r1&lt;/a&gt;), and it does mention DeepSeek R1 Distill Qwen 7B in the metadata. So this is actually a distilled model. But honestly, that still impresses me!&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Just discovered DeepSeek R1 and I'm pretty hyped about it. For those who don't know, it's a new &lt;strong&gt;open-source AI model that matches OpenAI o1 and Claude 3.5 Sonnet&lt;/strong&gt; in math, coding, and reasoning tasks.&lt;/p&gt; &lt;p&gt;You can check out Reddit to see what others are saying about DeepSeek R1 vs OpenAI o1 and Claude 3.5 Sonnet. For me it's really good - good enough to be compared with those top models.&lt;/p&gt; &lt;p&gt;And the best part? &lt;strong&gt;You can run it locally on your machine, with total privacy and 100% FREE!!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I've got it running locally and have been playing with it for a while. Here's my setup - super easy to follow:&lt;/p&gt; &lt;p&gt;&lt;em&gt;(Just a note: While I'm using a Mac,&lt;/em&gt; &lt;strong&gt;&lt;em&gt;this guide works exactly the same for Windows and Linux users&lt;/em&gt;&lt;/strong&gt;*! ðŸ‘Œ)*&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1) Install Ollama&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Quick intro to Ollama: It's a tool for running AI models locally on your machine. Grab it here: &lt;a href="https://ollama.com/download"&gt;https://ollama.com/download&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vdmiiuw4vbee1.png?width=748&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e1efb91eee9cfd8c654ed3282154e92cbbcedad"&gt;https://preview.redd.it/vdmiiuw4vbee1.png?width=748&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e1efb91eee9cfd8c654ed3282154e92cbbcedad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2) Next, you'll need to pull and run the DeepSeek R1 model locally.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ollama offers different model sizes - basically, bigger models = smarter AI, but need better GPU. Here's the lineup:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1.5B version (smallest): ollama run deepseek-r1:1.5b 8B version: ollama run deepseek-r1:8b 14B version: ollama run deepseek-r1:14b 32B version: ollama run deepseek-r1:32b 70B version (biggest/smartest): ollama run deepseek-r1:70b &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Maybe start with a smaller model first to test the waters. Just open your terminal and run:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama run deepseek-r1:8b&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Once it's pulled, the model will run locally on your machine. Simple as that!&lt;/p&gt; &lt;p&gt;&lt;em&gt;Note: The bigger versions (like 32B and 70B) need some serious GPU power. Start small and work your way up based on your hardware!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uk32frykvbee1.png?width=966&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df7a11a9b2c03e89b899b9aa3d9e1b62fd194197"&gt;https://preview.redd.it/uk32frykvbee1.png?width=966&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df7a11a9b2c03e89b899b9aa3d9e1b62fd194197&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3) Set up Chatbox - a powerful client for AI models&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Quick intro to Chatbox: a free, clean, and powerful desktop interface that works with most models. I started it as a side project for 2 years. Itâ€™s privacy-focused (all data stays local) and super easy to set upâ€”no Docker or complicated steps. Download here: &lt;a href="https://chatboxai.app"&gt;https://chatboxai.app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In Chatbox, go to settings and switch the model provider to Ollama. Since you're running models locally, you can ignore the built-in cloud AI options - &lt;strong&gt;no license key or payment is needed!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ye2tfudmvbee1.png?width=1940&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2711854eb585e6940c8fa27fa0fdc6c0e656fd03"&gt;https://preview.redd.it/ye2tfudmvbee1.png?width=1940&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2711854eb585e6940c8fa27fa0fdc6c0e656fd03&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Then set up the Ollama API host - the default setting is &lt;a href="http://127.0.0.1:11434"&gt;&lt;code&gt;http://127.0.0.1:11434&lt;/code&gt;&lt;/a&gt;, which should work right out of the box. That's it! Just pick the model and hit save. Now you're all set and ready to chat with your locally running Deepseek R1! ðŸš€&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vizcc81pvbee1.png?width=2238&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b80cb5066444203c85fd5d267b710e991df2381f"&gt;https://preview.redd.it/vizcc81pvbee1.png?width=2238&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b80cb5066444203c85fd5d267b710e991df2381f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope this helps! Let me know if you run into any issues.&lt;/p&gt; &lt;p&gt;---------------------&lt;/p&gt; &lt;p&gt;Here are a few tests I ran on my local DeepSeek R1 setup (loving Chatbox's &lt;strong&gt;artifact preview&lt;/strong&gt; feature btw!) ðŸ‘‡&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Explain TCP:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dqa138svvbee1.png?width=2268&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47c01c70f596a22e1c4cfb85878f2dd539a47824"&gt;https://preview.redd.it/dqa138svvbee1.png?width=2268&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47c01c70f596a22e1c4cfb85878f2dd539a47824&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Honestly, this looks pretty good, especially considering it's just an 8B model!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Make a Pac-Man game:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/iwjhq593zbee1.gif"&gt;https://i.redd.it/iwjhq593zbee1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It looks great, but I couldnâ€™t actually play it. I feel like there might be a few small bugs that could be fixed with some tweaking. (Just to clarify, this wasnâ€™t done on the local model â€” my mac doesnâ€™t have enough space for the largest deepseek R1 70b model, so I used the cloud model instead.)&lt;/p&gt; &lt;p&gt;---------------------&lt;/p&gt; &lt;p&gt;Honestly, Iâ€™ve seen a lot of overhyped posts about models here lately, so I was a bit skeptical going into this. But after testing DeepSeek R1 myself, I think itâ€™s actually really solid. Itâ€™s not some magic replacement for OpenAI or Claude, but itâ€™s &lt;strong&gt;surprisingly capable&lt;/strong&gt; for something that runs locally. The fact that itâ€™s free and works offline is a huge plus.&lt;/p&gt; &lt;p&gt;What do you guys think? Curious to hear your honest thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sleepingbenb"&gt; /u/sleepingbenb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6gmgq/got_deepseek_r1_running_locally_full_setup_guide/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6gmgq/got_deepseek_r1_running_locally_full_setup_guide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6gmgq/got_deepseek_r1_running_locally_full_setup_guide/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T11:35:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7f0xt</id>
    <title>Open-source alternatives for generating structured output from text extracted by LLaMA3.2-Vision?</title>
    <updated>2025-01-22T16:25:47+00:00</updated>
    <author>
      <name>/u/ThickDoctor007</name>
      <uri>https://old.reddit.com/user/ThickDoctor007</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThickDoctor007"&gt; /u/ThickDoctor007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AI_Agents/comments/1i7f09v/opensource_alternatives_for_generating_structured/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7f0xt/opensource_alternatives_for_generating_structured/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7f0xt/opensource_alternatives_for_generating_structured/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T16:25:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7tiak</id>
    <title>Deepseek-r1 a chinese propaganda tool?</title>
    <updated>2025-01-23T02:49:14+00:00</updated>
    <author>
      <name>/u/M3GaPrincess</name>
      <uri>https://old.reddit.com/user/M3GaPrincess</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7tiak/deepseekr1_a_chinese_propaganda_tool/"&gt; &lt;img alt="Deepseek-r1 a chinese propaganda tool?" src="https://b.thumbs.redditmedia.com/xyPfzCsfEm0yFxRYQExSPfpn7DX6YsWvkoNQsjTUepY.jpg" title="Deepseek-r1 a chinese propaganda tool?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New run, never used the model. Ask it about desert reclamation project and it spews out &amp;quot;China is number one&amp;quot; level bs. I did not fake this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6b2xuhujqnee1.png?width=1808&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6d67afaaef101a412911d6b38f0283ce60bc8b71"&gt;https://preview.redd.it/6b2xuhujqnee1.png?width=1808&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6d67afaaef101a412911d6b38f0283ce60bc8b71&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/M3GaPrincess"&gt; /u/M3GaPrincess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7tiak/deepseekr1_a_chinese_propaganda_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7tiak/deepseekr1_a_chinese_propaganda_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7tiak/deepseekr1_a_chinese_propaganda_tool/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T02:49:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7h01b</id>
    <title>How to use the ollama API in a remote computer in python?</title>
    <updated>2025-01-22T17:46:09+00:00</updated>
    <author>
      <name>/u/HunterHelpful9383</name>
      <uri>https://old.reddit.com/user/HunterHelpful9383</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I have two computers, one is my laptop with my coding environment (with low specs), and the other is my gaming computer, (high specs). I want to run and process the AI model inside my gaming computer, and send/use the processed data with my laptop (since all my programming stuff is in my laptop).&lt;/p&gt; &lt;p&gt;I have searched around the internet, and I couldn't find anything. Do you guys know how I can do it? They are both in the same LAN connection&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HunterHelpful9383"&gt; /u/HunterHelpful9383 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7h01b/how_to_use_the_ollama_api_in_a_remote_computer_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7h01b/how_to_use_the_ollama_api_in_a_remote_computer_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7h01b/how_to_use_the_ollama_api_in_a_remote_computer_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T17:46:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7hgoa</id>
    <title>Techniques to fit models larger than VRAM into GPU?</title>
    <updated>2025-01-22T18:04:43+00:00</updated>
    <author>
      <name>/u/stereotypical_CS</name>
      <uri>https://old.reddit.com/user/stereotypical_CS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stereotypical_CS"&gt; /u/stereotypical_CS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1i7hcov/techniques_to_fit_models_larger_than_vram_into_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7hgoa/techniques_to_fit_models_larger_than_vram_into_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7hgoa/techniques_to_fit_models_larger_than_vram_into_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T18:04:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7cdyj</id>
    <title>Why is ollama only using 4535MiB/6144MiB of VRAM? Shouldnt it first fill the 6GB VRAM then move towards the system RAM?</title>
    <updated>2025-01-22T14:31:54+00:00</updated>
    <author>
      <name>/u/R46H4V</name>
      <uri>https://old.reddit.com/user/R46H4V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7cdyj/why_is_ollama_only_using_4535mib6144mib_of_vram/"&gt; &lt;img alt="Why is ollama only using 4535MiB/6144MiB of VRAM? Shouldnt it first fill the 6GB VRAM then move towards the system RAM?" src="https://preview.redd.it/671wdnno2kee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ee8f91525de17d621b557549507e510a41806a7" title="Why is ollama only using 4535MiB/6144MiB of VRAM? Shouldnt it first fill the 6GB VRAM then move towards the system RAM?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R46H4V"&gt; /u/R46H4V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/671wdnno2kee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7cdyj/why_is_ollama_only_using_4535mib6144mib_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7cdyj/why_is_ollama_only_using_4535mib6144mib_of_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T14:31:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7d3uc</id>
    <title>I wrote an open source tool to test prompt injection attacks</title>
    <updated>2025-01-22T15:04:58+00:00</updated>
    <author>
      <name>/u/utku1337</name>
      <uri>https://old.reddit.com/user/utku1337</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two years ago, I released &amp;quot;promptmap&amp;quot;, the first tool that automatically tests prompt injection attacks in GPT applications. But since then, open source models became popular and new prompt injection techniques are discovered. This led me to completely rewrite the tool. Now it supports any open-source model thanks to Ollama.&lt;/p&gt; &lt;p&gt;Feed it with system prompts of your LLM application and let it perform automated prompt injection attacks. It will reveal potential vulnerabilities and determine if attackers could extract your system prompts.&lt;/p&gt; &lt;p&gt;Any feedback is welcome: &lt;a href="https://github.com/utkusen/promptmap"&gt;https://github.com/utkusen/promptmap&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/utku1337"&gt; /u/utku1337 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7d3uc/i_wrote_an_open_source_tool_to_test_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7d3uc/i_wrote_an_open_source_tool_to_test_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7d3uc/i_wrote_an_open_source_tool_to_test_prompt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T15:04:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7bjgg</id>
    <title>First time trying out Ollama and Deepseek. Can you use a GUI and upload images?</title>
    <updated>2025-01-22T13:51:26+00:00</updated>
    <author>
      <name>/u/Draufgaenger</name>
      <uri>https://old.reddit.com/user/Draufgaenger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So far I'm using the command line interface. Is that what you all are using too?&lt;br /&gt; I saw there are some Web-interfaces that let you use Ollama and some of them even seem to have the ability for you to upload images just like you would in ChatGPT?&lt;/p&gt; &lt;p&gt;In the commandline when I enter /? I see no option to upload an image though..so I am not sure. &lt;/p&gt; &lt;p&gt;Also I'd feel uneasy just using any random GUI for it..&lt;/p&gt; &lt;p&gt;Can you help me understand that whole thing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Draufgaenger"&gt; /u/Draufgaenger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7bjgg/first_time_trying_out_ollama_and_deepseek_can_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7bjgg/first_time_trying_out_ollama_and_deepseek_can_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7bjgg/first_time_trying_out_ollama_and_deepseek_can_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T13:51:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7gwbl</id>
    <title>Changed the model to Deepseek for my SEO Outliner (and I'm mesmerized with the thinking)</title>
    <updated>2025-01-22T17:41:51+00:00</updated>
    <author>
      <name>/u/LilFingaz</name>
      <uri>https://old.reddit.com/user/LilFingaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7gwbl/changed_the_model_to_deepseek_for_my_seo_outliner/"&gt; &lt;img alt="Changed the model to Deepseek for my SEO Outliner (and I'm mesmerized with the thinking)" src="https://external-preview.redd.it/bjR0eXhidXMwbGVlMU6N6TQf3Qi5Z6usbSq_x6Ry7p0qzaDdmhrtZz_2nu3Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60fc529f0a776c8041e5c0c83815472262c4e1f8" title="Changed the model to Deepseek for my SEO Outliner (and I'm mesmerized with the thinking)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LilFingaz"&gt; /u/LilFingaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3cn6tjus0lee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7gwbl/changed_the_model_to_deepseek_for_my_seo_outliner/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7gwbl/changed_the_model_to_deepseek_for_my_seo_outliner/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T17:41:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i80oxr</id>
    <title>Which version of deepseek-r1 for coding on my M3 Pro MacBook?</title>
    <updated>2025-01-23T10:46:58+00:00</updated>
    <author>
      <name>/u/joyfulsparrow</name>
      <uri>https://old.reddit.com/user/joyfulsparrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've got a MacBook with these specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; M3 Pro&lt;/li&gt; &lt;li&gt; Memory: 36GB&lt;/li&gt; &lt;li&gt; macOS 15.2&lt;/li&gt; &lt;li&gt; Ollama&lt;/li&gt; &lt;li&gt; Ollamac app&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What's the best version of deepseek-rc1 for coding that I could use on the MacBook? &lt;a href="https://ollama.com/library/deepseek-r1"&gt;https://ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joyfulsparrow"&gt; /u/joyfulsparrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i80oxr/which_version_of_deepseekr1_for_coding_on_my_m3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i80oxr/which_version_of_deepseekr1_for_coding_on_my_m3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i80oxr/which_version_of_deepseekr1_for_coding_on_my_m3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T10:46:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7uhof</id>
    <title>List of string model names - ollama/langchain python</title>
    <updated>2025-01-23T03:39:54+00:00</updated>
    <author>
      <name>/u/Oceanboi</name>
      <uri>https://old.reddit.com/user/Oceanboi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any way to grab all model names that are available for ollama pull? I'd like to develop a module for my streamlit app that allows you to browse model names/cards without having to open your browser as well for quicker iteration and add a model manager into the UI. Would rather not web scrape because it's annoying.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Oceanboi"&gt; /u/Oceanboi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7uhof/list_of_string_model_names_ollamalangchain_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7uhof/list_of_string_model_names_ollamalangchain_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7uhof/list_of_string_model_names_ollamalangchain_python/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T03:39:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7zco5</id>
    <title>This is how I use reasoning models (Deepseek R1) in agents &amp; LLM apps</title>
    <updated>2025-01-23T09:04:27+00:00</updated>
    <author>
      <name>/u/jasonzhou1993</name>
      <uri>https://old.reddit.com/user/jasonzhou1993</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7zco5/this_is_how_i_use_reasoning_models_deepseek_r1_in/"&gt; &lt;img alt="This is how I use reasoning models (Deepseek R1) in agents &amp;amp; LLM apps" src="https://external-preview.redd.it/bGtwbTN5MGFscGVlMfJSOzJ9yre6sC7e1ecrhDdOVYLiJ3GWclt88RbKO1mA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a2e4b5f197082b18fa833d6c6526687059c06cc" title="This is how I use reasoning models (Deepseek R1) in agents &amp;amp; LLM apps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jasonzhou1993"&gt; /u/jasonzhou1993 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k861mx0alpee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7zco5/this_is_how_i_use_reasoning_models_deepseek_r1_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7zco5/this_is_how_i_use_reasoning_models_deepseek_r1_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T09:04:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7zq3s</id>
    <title>My good people...</title>
    <updated>2025-01-23T09:34:51+00:00</updated>
    <author>
      <name>/u/caiowilson</name>
      <uri>https://old.reddit.com/user/caiowilson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a local server with dedicated GPU, i9 (quite old, Skylake I think) and 16gb running debian headless is it a better option to use? Harder to configure? Currently I only play with LLMs on my MacBook pro M3 pro (18gb, most LLMs don't use the GPU) but it's not very doable when working (20 containers running 20% of my processors and quite a bit of ram) Any considerations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/caiowilson"&gt; /u/caiowilson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7zq3s/my_good_people/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7zq3s/my_good_people/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7zq3s/my_good_people/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T09:34:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7n9n5</id>
    <title>How the VRAM works for Macs</title>
    <updated>2025-01-22T22:00:34+00:00</updated>
    <author>
      <name>/u/Cosyless</name>
      <uri>https://old.reddit.com/user/Cosyless</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just recently got into working with models in local. I have 1650Ti (notebook GPU) which has 4GB VRAM obviously. I wonder how VRAM works for the new M-chip mac(books/mini). By definition, they do not have a 'video card'. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cosyless"&gt; /u/Cosyless &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7n9n5/how_the_vram_works_for_macs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7n9n5/how_the_vram_works_for_macs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7n9n5/how_the_vram_works_for_macs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T22:00:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i80brl</id>
    <title>My first every test drive of the deepseek-r1</title>
    <updated>2025-01-23T10:20:35+00:00</updated>
    <author>
      <name>/u/Kind_Ad_2866</name>
      <uri>https://old.reddit.com/user/Kind_Ad_2866</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i80brl/my_first_every_test_drive_of_the_deepseekr1/"&gt; &lt;img alt="My first every test drive of the deepseek-r1" src="https://b.thumbs.redditmedia.com/U7q8JfhCRJ9KmqGLoZ-z3_3bBBOkzIJsHlBoWdlNdgM.jpg" title="My first every test drive of the deepseek-r1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded DeepSeek-R1 last night and tested it on my Nvidia GeForce 4060 (8GB) paired with an Intel i7 and 64GB of RAM. My first impressions are really positiveâ€”it handles conversation and reasoning surprisingly well, especially for a locally running model. However, unlike other models such as LLaMA 3.3, I canâ€™t seem to make it follow my instructions precisely. If you look at the screenshot, youâ€™ll see that it doesnâ€™t produce the exact output format I requested. Iâ€™m wondering if Iâ€™m missing a step or if this is just how the model behaves.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x67te5yd4qee1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1350e47660acbc968da44a05be040228799b0467"&gt;https://preview.redd.it/x67te5yd4qee1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1350e47660acbc968da44a05be040228799b0467&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/aq6ba6yd4qee1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1065763b1396f06a34b01f5f291e8af514999d3"&gt;https://preview.redd.it/aq6ba6yd4qee1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1065763b1396f06a34b01f5f291e8af514999d3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hh3oa7yd4qee1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9312866d88acd4a01c0e0006e98fdc7a32eb8d35"&gt;https://preview.redd.it/hh3oa7yd4qee1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9312866d88acd4a01c0e0006e98fdc7a32eb8d35&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kind_Ad_2866"&gt; /u/Kind_Ad_2866 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i80brl/my_first_every_test_drive_of_the_deepseekr1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i80brl/my_first_every_test_drive_of_the_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i80brl/my_first_every_test_drive_of_the_deepseekr1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T10:20:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i814ta</id>
    <title>Is it possible to run ollama if i have multiple computers?</title>
    <updated>2025-01-23T11:17:55+00:00</updated>
    <author>
      <name>/u/Muted_Membership9372</name>
      <uri>https://old.reddit.com/user/Muted_Membership9372</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m kind of new to this field, so I apologize for any ignorance on my part!&lt;br /&gt; Hereâ€™s the problem Iâ€™ve encountered:&lt;br /&gt; Suppose I have several devices with NVIDIA GeForce RTX 3060 Lite Hash Rate, and Iâ€™d like to run a model that requires more GPU VRAM than I have on a single GPU.&lt;br /&gt; So, my question is:&lt;br /&gt; Is it possible to use my devices (perhaps as a cluster) to run the model? And if so, will the workload be distributed equally across the GPUs?&lt;/p&gt; &lt;p&gt;I'd also be grateful to know if there's already implemented solutions out there, as I'd prefer not to reinvent the wheel&lt;/p&gt; &lt;p&gt;Thx in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Muted_Membership9372"&gt; /u/Muted_Membership9372 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i814ta/is_it_possible_to_run_ollama_if_i_have_multiple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i814ta/is_it_possible_to_run_ollama_if_i_have_multiple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i814ta/is_it_possible_to_run_ollama_if_i_have_multiple/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T11:17:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7ygrp</id>
    <title>Good UI for DeepSeek R1</title>
    <updated>2025-01-23T07:56:24+00:00</updated>
    <author>
      <name>/u/AdAccomplished8942</name>
      <uri>https://old.reddit.com/user/AdAccomplished8942</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm running DeepSeek R1 (32b) on my Apple M3 MacBook Pro and it works pretty well, but I can only use it in Terminal because I haven't found a nice UI for it.&lt;/p&gt; &lt;p&gt;Msty looks really good and has great features, but it forgets previous prompts and answers. Basically every prompt is like a completely new chat, which makes many things impossible.&lt;/p&gt; &lt;p&gt;What are some nice UIs for DeepSeek R1 on ollama?&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdAccomplished8942"&gt; /u/AdAccomplished8942 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7ygrp/good_ui_for_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7ygrp/good_ui_for_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7ygrp/good_ui_for_deepseek_r1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T07:56:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i81q09</id>
    <title>Context with deepseek r1?</title>
    <updated>2025-01-23T11:57:37+00:00</updated>
    <author>
      <name>/u/joyfulsparrow</name>
      <uri>https://old.reddit.com/user/joyfulsparrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does deepseek r1 keep the context of what you're talking about, or is every question like new for it? For instance, if I ask a coding question, and then follow up with some bugs, it doesn't seem to remember the original question and answer. Is there a way to enable this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joyfulsparrow"&gt; /u/joyfulsparrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i81q09/context_with_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i81q09/context_with_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i81q09/context_with_deepseek_r1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T11:57:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7as97</id>
    <title>The Chinese OBLITERATED OpenAI. A side-by-side comparison of DeepSeek R1 vs OpenAI O1 for Finance</title>
    <updated>2025-01-22T13:12:02+00:00</updated>
    <author>
      <name>/u/No-Definition-2886</name>
      <uri>https://old.reddit.com/user/No-Definition-2886</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7as97/the_chinese_obliterated_openai_a_sidebyside/"&gt; &lt;img alt="The Chinese OBLITERATED OpenAI. A side-by-side comparison of DeepSeek R1 vs OpenAI O1 for Finance" src="https://external-preview.redd.it/0d_JuM7Vh55nJ0hGva5m6LiE2ZjyLRmQI-3W0gsRh-4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b831df3cf7b173edcc1428ce4703b68ea01c270" title="The Chinese OBLITERATED OpenAI. A side-by-side comparison of DeepSeek R1 vs OpenAI O1 for Finance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Definition-2886"&gt; /u/No-Definition-2886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/p/93a1b4343a82"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7as97/the_chinese_obliterated_openai_a_sidebyside/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7as97/the_chinese_obliterated_openai_a_sidebyside/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T13:12:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7r6hf</id>
    <title>How to Turn off Thinking on ollama for DS-R1?</title>
    <updated>2025-01-23T00:54:11+00:00</updated>
    <author>
      <name>/u/PBlague</name>
      <uri>https://old.reddit.com/user/PBlague</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7r6hf/how_to_turn_off_thinking_on_ollama_for_dsr1/"&gt; &lt;img alt="How to Turn off Thinking on ollama for DS-R1?" src="https://a.thumbs.redditmedia.com/Xs-jZtPcBKuCW5kZ139NKMaiuaxV26kv1lwVYBQpU70.jpg" title="How to Turn off Thinking on ollama for DS-R1?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/o1yk2nfn5nee1.png?width=989&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5bfe41d31ef405a0d23da5d42fcbdc03cd00956f"&gt;DeepThink Button at the Bottom Left&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On the website you can turn on &amp;quot;DeepThink&amp;quot; if you so choose to, which helps make the model more powerful in reasoning and less powerful in actually holding the conversation.&lt;/p&gt; &lt;p&gt;Is there a way to turn off this thinking part in ollama too?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qc7szvbx5nee1.png?width=1405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fb98b4076d3cab51fbab5516c61bcb5962771766"&gt;&amp;lt;Think&amp;gt; &amp;lt;/Think&amp;gt; Patterns&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PBlague"&gt; /u/PBlague &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7r6hf/how_to_turn_off_thinking_on_ollama_for_dsr1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7r6hf/how_to_turn_off_thinking_on_ollama_for_dsr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7r6hf/how_to_turn_off_thinking_on_ollama_for_dsr1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T00:54:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7ds9g</id>
    <title>It didn't even need to think to reply ðŸ˜‚ (deepseek-r1)</title>
    <updated>2025-01-22T15:33:52+00:00</updated>
    <author>
      <name>/u/EnoughVeterinarian90</name>
      <uri>https://old.reddit.com/user/EnoughVeterinarian90</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7ds9g/it_didnt_even_need_to_think_to_reply_deepseekr1/"&gt; &lt;img alt="It didn't even need to think to reply ðŸ˜‚ (deepseek-r1)" src="https://b.thumbs.redditmedia.com/bU5DA_X-RMAId5YJB4pzOMTQS9Hz0n0JNBOyXhHZi3E.jpg" title="It didn't even need to think to reply ðŸ˜‚ (deepseek-r1)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EnoughVeterinarian90"&gt; /u/EnoughVeterinarian90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i7ds9g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7ds9g/it_didnt_even_need_to_think_to_reply_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7ds9g/it_didnt_even_need_to_think_to_reply_deepseekr1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T15:33:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7yycc</id>
    <title>what can do now?</title>
    <updated>2025-01-23T08:33:23+00:00</updated>
    <author>
      <name>/u/Own-Perception-1574</name>
      <uri>https://old.reddit.com/user/Own-Perception-1574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7yycc/what_can_do_now/"&gt; &lt;img alt="what can do now?" src="https://preview.redd.it/y8xv9gwwfpee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f7701dbfaa233f2456f822ab6bf09b6cd5f6c5e" title="what can do now?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Perception-1574"&gt; /u/Own-Perception-1574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y8xv9gwwfpee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7yycc/what_can_do_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7yycc/what_can_do_now/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T08:33:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7nqrj</id>
    <title>Run a fully local AI Search / RAG pipeline using Ollama with 4GB of memory and no GPU</title>
    <updated>2025-01-22T22:20:43+00:00</updated>
    <author>
      <name>/u/LeetTools</name>
      <uri>https://old.reddit.com/user/LeetTools</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, for people that want to run AI search and RAG pipelines locally, you can now build your local knowledge base with one line of command and everything runs locally with no docker or API key required. Repo is here: &lt;a href="https://github.com/leettools-dev/leettools"&gt;https://github.com/leettools-dev/leettools&lt;/a&gt;. The total memory usage is around 4GB with the Llama3.2 model: * llama3.2:latest 3.5 GB * nomic-embed-text:latest 370 MB * LeetTools: 350MB (Document pipeline backend with Python and DuckDB)&lt;/p&gt; &lt;p&gt;First, follow the instructions on &lt;a href="https://github.com/ollama/ollama"&gt;https://github.com/ollama/ollama&lt;/a&gt; to install the ollama program. Make sure the ollama program is running.&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;set up&lt;/h1&gt; &lt;p&gt;ollama pull llama3.2 ollama pull nomic-embed-text pip install leettools curl -fsSL -o .env.ollama &lt;a href="https://raw.githubusercontent.com/leettools-dev/leettools/refs/heads/main/env.ollama"&gt;https://raw.githubusercontent.com/leettools-dev/leettools/refs/heads/main/env.ollama&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;one command line to download a PDF and save it to the graphrag KB&lt;/h1&gt; &lt;p&gt;leet kb add-url -e .env.ollama -k graphrag -l info &lt;a href="https://arxiv.org/pdf/2501.09223"&gt;https://arxiv.org/pdf/2501.09223&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;now you query the local graphrag KB with questions&lt;/h1&gt; &lt;p&gt;leet flow -t answer -e .env.ollama -k graphrag -l info -p retriever_type=local -q &amp;quot;How does GraphRAG work?&amp;quot; ```&lt;/p&gt; &lt;p&gt;You can also add your local directory or files to the knowledge base using &lt;code&gt;leet kb add-local&lt;/code&gt; command.&lt;/p&gt; &lt;p&gt;For the above default setup, we are using * docling to convert PDF to markdown * chonkie as the chunker * nomic-embed-text as the embedding model * llama3.2 as the inference engine * Duckdb as the data storage include graph and vector&lt;/p&gt; &lt;p&gt;We think it might be helpful for some usage scenarios that require local deployment and resource limits. Questions or suggestions are welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeetTools"&gt; /u/LeetTools &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7nqrj/run_a_fully_local_ai_search_rag_pipeline_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7nqrj/run_a_fully_local_ai_search_rag_pipeline_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7nqrj/run_a_fully_local_ai_search_rag_pipeline_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T22:20:43+00:00</published>
  </entry>
</feed>
