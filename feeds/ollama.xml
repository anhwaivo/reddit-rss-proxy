<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-06-08T06:08:16+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1l3fcrw</id>
    <title>I made an LLM tool to let you search offline Wikipedia/StackExchange/DevDocs ZIM files (llm-tools-kiwix, works with Python &amp; LLM cli)</title>
    <updated>2025-06-04T19:56:05+00:00</updated>
    <author>
      <name>/u/mozanunal</name>
      <uri>https://old.reddit.com/user/mozanunal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I just released &lt;a href="https://github.com/mozanunal/llm-tools-kiwix"&gt;&lt;code&gt;llm-tools-kiwix&lt;/code&gt;&lt;/a&gt;, a plugin for the &lt;a href="https://llm.datasette.io/"&gt;&lt;code&gt;llm&lt;/code&gt; CLI&lt;/a&gt; and Python that lets LLMs read and search offline ZIM archives (i.e., Wikipedia, DevDocs, StackExchange, and more) &lt;strong&gt;totally offline&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt;&lt;br /&gt; A lot of local LLM use cases could benefit from RAG using big knowledge bases, but most solutions require network calls. Kiwix makes it possible to have huge websites (Wikipedia, StackExchange, etc.) stored as &lt;code&gt;.zim&lt;/code&gt; files on your disk. Now you can let your LLM access those—no Internet needed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What does it do?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Discovers your ZIM files&lt;/strong&gt; (in the cwd or a folder via &lt;code&gt;KIWIX_HOME&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Exposes tools so the LLM can search articles or read full content&lt;/li&gt; &lt;li&gt;Works on the command line or from Python (supports GPT-4o, ollama, Llama.cpp, etc via the &lt;code&gt;llm&lt;/code&gt; tool)&lt;/li&gt; &lt;li&gt;No cloud or browser needed, just pure local retrieval&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Example use-case:&lt;/strong&gt;&lt;br /&gt; Say you have &lt;code&gt;wikipedia_en_all_nopic_2023-10.zim&lt;/code&gt; downloaded and want your LLM to answer questions using it:&lt;/p&gt; &lt;p&gt;&lt;code&gt; llm install llm-tools-kiwix # (one-time setup) llm -m ollama:llama3 --tool kiwix_search_and_collect \ &amp;quot;Summarize notable attempts at human-powered flight from Wikipedia.&amp;quot; \ --tools-debug &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Or use the Docker/DevDocs ZIMs for local developer documentation search.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to try:&lt;/strong&gt; 1. Download some ZIM files from &lt;a href="https://download.kiwix.org/zim/"&gt;https://download.kiwix.org/zim/&lt;/a&gt; 2. Put them in your project dir, or set &lt;code&gt;KIWIX_HOME&lt;/code&gt; 3. &lt;code&gt;llm install llm-tools-kiwix&lt;/code&gt; 4. Use tool mode as above!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Open source, Apache 2.0.&lt;/strong&gt;&lt;br /&gt; Repo + docs: &lt;a href="https://github.com/mozanunal/llm-tools-kiwix"&gt;https://github.com/mozanunal/llm-tools-kiwix&lt;/a&gt;&lt;br /&gt; PyPI: &lt;a href="https://pypi.org/project/llm-tools-kiwix/"&gt;https://pypi.org/project/llm-tools-kiwix/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think! Would love feedback, bug reports, or ideas for more offline tools.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mozanunal"&gt; /u/mozanunal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3fcrw/i_made_an_llm_tool_to_let_you_search_offline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3fcrw/i_made_an_llm_tool_to_let_you_search_offline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l3fcrw/i_made_an_llm_tool_to_let_you_search_offline/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-04T19:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1l36kvb</id>
    <title>smollm is crazy</title>
    <updated>2025-06-04T14:11:39+00:00</updated>
    <author>
      <name>/u/3d_printing_kid</name>
      <uri>https://old.reddit.com/user/3d_printing_kid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l36kvb/smollm_is_crazy/"&gt; &lt;img alt="smollm is crazy" src="https://external-preview.redd.it/NG9mN3N2ZGw0eDRmMYC1oXT879drMGhz7A_iST_bdDJ62X2-qbCshqC67I28.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=faaff70ac5a371991a4c0aa1609505f081603776" title="smollm is crazy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i was bored one day so i dicided to run smollm 135 m parameters. here is a video of the result:&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3d_printing_kid"&gt; /u/3d_printing_kid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fesotwdl4x4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l36kvb/smollm_is_crazy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l36kvb/smollm_is_crazy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-04T14:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3z6tu</id>
    <title>Reccomandations on budget GPU</title>
    <updated>2025-06-05T13:27:53+00:00</updated>
    <author>
      <name>/u/Oridium_</name>
      <uri>https://old.reddit.com/user/Oridium_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am looking to create a local LLM on my machine but I am unsure on which GPU should I use since I am not that affiliated with the requirements. Currently I am using an NVIDIA RTX 3060 Ti with 8 GB of VRAM but I am looking to upgrade to an RX 6800 xt with 16GB of vram. I've heard that the CUDA cores on the nvidia gpus outperform any radeon counterparts in the same price range. Also, regarding general storage, what would be the general amount of storage i should allocate for it. Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Oridium_"&gt; /u/Oridium_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3z6tu/reccomandations_on_budget_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3z6tu/reccomandations_on_budget_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l3z6tu/reccomandations_on_budget_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-05T13:27:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3rbac</id>
    <title>local models need a lot of hand holding when prompting ?</title>
    <updated>2025-06-05T05:30:50+00:00</updated>
    <author>
      <name>/u/AntelopeEntire9191</name>
      <uri>https://old.reddit.com/user/AntelopeEntire9191</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is it just me or does local models that are around the size of 14b just need a lot of hand holding when prompting them? like it requires you to be meticulous in the prompt otherwise the outputs ends up being lackluster. ik ollama released &lt;a href="https://ollama.com/blog/structured-outputs"&gt;https://ollama.com/blog/structured-outputs&lt;/a&gt; structured outputs that significantly helped from having to force the llm to have attention to detail to every sort of items such as spacing, missing commas, unnecessary syntax, but still this is annoying to have to hand hold. at times i think the extra cost of frontier models is just so much more worth that sort of already handle these edge cases for you? its just annoying and im just wonder im using these models wrong? my bullet point of instructions feels like its starting to become a never ending list and as a result only making the invoke time even longer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AntelopeEntire9191"&gt; /u/AntelopeEntire9191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3rbac/local_models_need_a_lot_of_hand_holding_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3rbac/local_models_need_a_lot_of_hand_holding_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l3rbac/local_models_need_a_lot_of_hand_holding_when/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-05T05:30:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3zpxd</id>
    <title>smollm is crazier (older version is worse)</title>
    <updated>2025-06-05T13:51:25+00:00</updated>
    <author>
      <name>/u/3d_printing_kid</name>
      <uri>https://old.reddit.com/user/3d_printing_kid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l3zpxd/smollm_is_crazier_older_version_is_worse/"&gt; &lt;img alt="smollm is crazier (older version is worse)" src="https://external-preview.redd.it/NHUycjJwenc1NDVmMZsBk0Wxe9S7tv9zfFAxZfkxM-dQVhjCUDDDFpUA1CGh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16c890621f1461a0c81e1346c489d3ee6889ad8d" title="smollm is crazier (older version is worse)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3d_printing_kid"&gt; /u/3d_printing_kid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eao3tqzw545f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3zpxd/smollm_is_crazier_older_version_is_worse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l3zpxd/smollm_is_crazier_older_version_is_worse/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-05T13:51:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1l4bcqd</id>
    <title>Any way to translate text from images with local AIs?</title>
    <updated>2025-06-05T21:35:33+00:00</updated>
    <author>
      <name>/u/iTrejoMX</name>
      <uri>https://old.reddit.com/user/iTrejoMX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to locally have something similar to sider.ai . I haven't been able to find anything that i can use for this use case or something similar. Anyone have any experience in extracting text from images and translating it? (optionally: putting translated text into the image to replace original text)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iTrejoMX"&gt; /u/iTrejoMX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l4bcqd/any_way_to_translate_text_from_images_with_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l4bcqd/any_way_to_translate_text_from_images_with_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l4bcqd/any_way_to_translate_text_from_images_with_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-05T21:35:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3tcon</id>
    <title>Open Source Alternative to Perplexity</title>
    <updated>2025-06-05T07:43:10+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with &lt;strong&gt;SurfSense&lt;/strong&gt;, it aims to be the open-source alternative to &lt;strong&gt;NotebookLM&lt;/strong&gt;, &lt;strong&gt;Perplexity&lt;/strong&gt;, or &lt;strong&gt;Glean&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent but connected to your personal external sources search engines (Tavily, LinkUp), Slack, Linear, Notion, YouTube, GitHub, Discord and more coming soon.&lt;/p&gt; &lt;p&gt;I'll keep this short—here are a few highlights of SurfSense:&lt;/p&gt; &lt;p&gt;📊 Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports &lt;strong&gt;150+ LLM's&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Supports local &lt;strong&gt;Ollama LLM's&lt;/strong&gt; or &lt;strong&gt;vLLM&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Supports &lt;strong&gt;6000+ Embedding Models&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt; &lt;li&gt;Uses &lt;strong&gt;Hierarchical Indices&lt;/strong&gt; (2-tiered RAG setup)&lt;/li&gt; &lt;li&gt;Combines &lt;strong&gt;Semantic + Full-Text Search&lt;/strong&gt; with &lt;strong&gt;Reciprocal Rank Fusion&lt;/strong&gt; (Hybrid Search)&lt;/li&gt; &lt;li&gt;Offers a &lt;strong&gt;RAG-as-a-Service API Backend&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Supports 50+ File extensions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🎙️ Podcasts&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)&lt;/li&gt; &lt;li&gt;Convert your chat conversations into engaging audio content&lt;/li&gt; &lt;li&gt;Support for multiple TTS providers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;ℹ️ &lt;strong&gt;External Sources&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Search engines (Tavily, LinkUp)&lt;/li&gt; &lt;li&gt;Slack&lt;/li&gt; &lt;li&gt;Linear&lt;/li&gt; &lt;li&gt;Notion&lt;/li&gt; &lt;li&gt;YouTube videos&lt;/li&gt; &lt;li&gt;GitHub&lt;/li&gt; &lt;li&gt;Discord&lt;/li&gt; &lt;li&gt;...and more on the way&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🔖 &lt;strong&gt;Cross-Browse&lt;/strong&gt;r Extension&lt;br /&gt; The SurfSense extension lets you save any dynamic webpage you like. Its main use case is capturing pages that are protected behind authentication.&lt;/p&gt; &lt;p&gt;Check out SurfSense on GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3tcon/open_source_alternative_to_perplexity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3tcon/open_source_alternative_to_perplexity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l3tcon/open_source_alternative_to_perplexity/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-05T07:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1l4nmrc</id>
    <title>Qwen3 Embeddings Model Support</title>
    <updated>2025-06-06T08:51:59+00:00</updated>
    <author>
      <name>/u/Informal-Victory8655</name>
      <uri>https://old.reddit.com/user/Informal-Victory8655</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;... Any information on availability of qwen3 embeddings in ollama models? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Informal-Victory8655"&gt; /u/Informal-Victory8655 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l4nmrc/qwen3_embeddings_model_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l4nmrc/qwen3_embeddings_model_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l4nmrc/qwen3_embeddings_model_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-06T08:51:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1l4vak2</id>
    <title>[D] Which LLM architecture Implementation would you suggest ?</title>
    <updated>2025-06-06T15:28:51+00:00</updated>
    <author>
      <name>/u/theMonarch776</name>
      <uri>https://old.reddit.com/user/theMonarch776</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theMonarch776"&gt; /u/theMonarch776 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/MachineLearning/comments/1l4url3/d_which_llm_architecture_implementation_would_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l4vak2/d_which_llm_architecture_implementation_would_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l4vak2/d_which_llm_architecture_implementation_would_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-06T15:28:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1l4qh7u</id>
    <title>ollama context quantization</title>
    <updated>2025-06-06T11:53:09+00:00</updated>
    <author>
      <name>/u/cipherninjabyte</name>
      <uri>https://old.reddit.com/user/cipherninjabyte</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see a video about ollama context quantization, running commands (ollama flash attention and ollama kv cache type) to set some values which would reduce memory usage. That video was from 2024. Did ollama include those changes in their recent builds? or should we run those commands still? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cipherninjabyte"&gt; /u/cipherninjabyte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l4qh7u/ollama_context_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l4qh7u/ollama_context_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l4qh7u/ollama_context_quantization/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-06T11:53:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1l4oavm</id>
    <title>Ollama's Context Window (Granite 3.3 128K Model)</title>
    <updated>2025-06-06T09:39:44+00:00</updated>
    <author>
      <name>/u/mufasathetiger</name>
      <uri>https://old.reddit.com/user/mufasathetiger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I have a few questions regarding how Ollama handles the context window when running models.&lt;/p&gt; &lt;p&gt;Why does Ollama run models with a 2K token context window when some models, like Granite 3.3, support up to 128K tokens?&lt;/p&gt; &lt;p&gt;How can I configure the context window value for a specific model and verify such context window is actually effective?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mufasathetiger"&gt; /u/mufasathetiger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l4oavm/ollamas_context_window_granite_33_128k_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l4oavm/ollamas_context_window_granite_33_128k_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l4oavm/ollamas_context_window_granite_33_128k_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-06T09:39:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1l51g6s</id>
    <title>Ollama is using CPU and not GPU</title>
    <updated>2025-06-06T19:39:26+00:00</updated>
    <author>
      <name>/u/TheMicrosoftMan</name>
      <uri>https://old.reddit.com/user/TheMicrosoftMan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama keeps using the CPU instead of the Nvidia GPU. I have 32gb of vram so it shouldn't be a problem. Pls help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheMicrosoftMan"&gt; /u/TheMicrosoftMan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l51g6s/ollama_is_using_cpu_and_not_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l51g6s/ollama_is_using_cpu_and_not_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l51g6s/ollama_is_using_cpu_and_not_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-06T19:39:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1l4xgs4</id>
    <title>CPU only AI - Help!</title>
    <updated>2025-06-06T16:57:14+00:00</updated>
    <author>
      <name>/u/AngeloNino</name>
      <uri>https://old.reddit.com/user/AngeloNino</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dual Xeon Gold and no AI model performance&lt;/p&gt; &lt;p&gt;I'm so frustrated. I have dual Xeon Gold (56 cores) and 256 GB RAM with TBs of space and can't get Qwen 2.5 to return a JavaScript function in reasonable time that simply adds two integers.&lt;/p&gt; &lt;p&gt;Ideas? I have enough CPU to do so many other things. Not trying to do a one shot application just a basic JavaScript function.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AngeloNino"&gt; /u/AngeloNino &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l4xgs4/cpu_only_ai_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l4xgs4/cpu_only_ai_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l4xgs4/cpu_only_ai_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-06T16:57:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1l4wfon</id>
    <title>Ollama vs Llamacpp: Different output for same model</title>
    <updated>2025-06-06T16:15:01+00:00</updated>
    <author>
      <name>/u/Plus_Factor7011</name>
      <uri>https://old.reddit.com/user/Plus_Factor7011</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l4wfon/ollama_vs_llamacpp_different_output_for_same_model/"&gt; &lt;img alt="Ollama vs Llamacpp: Different output for same model" src="https://b.thumbs.redditmedia.com/8YTuLdC2AuS3-ztcdxp8khbZiDmM7NL8GNUWPGWS49E.jpg" title="Ollama vs Llamacpp: Different output for same model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! For my master's thesis project, I use LLMs to generate behaviour trees for robot control. I used local models in the gguf format, and most of the time, I used llamacpp. But it became hell to consistently get it to use GPUs in different systems, so I also integrated ollama into my framework, and it has been a blessing for running with GPU out of the box.&lt;/p&gt; &lt;p&gt;For llamacpp, I directly feed the path to my local gguf file, while for ollama, I instead provide the HF URL where the model is stored (so both are the same model), and ollama pulls it and uses it for prompting. I run it in both ollama and llamacpp using the same parameters, system, and user prompt, but somehow I get different responses even with the same seed and temperature.&lt;/p&gt; &lt;p&gt;To be clear, I finetuned my model using unsloth notebooks, which do finetuning + quantt+ conversion to gguf. Any detail or advice is welcome. Find below my implementation of both libraries' setup and prompting.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rzkfveizxb5f1.png?width=895&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=edc0497626459e6be4213858b385ce9623d2258a"&gt;Llamacpp Initialization&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/27homhjqzb5f1.png?width=851&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1bb1963dd4ebf95b6f39bb50fbfbe41f1c668455"&gt;Ollama init&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/grdb4ltbyb5f1.png?width=582&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8ae5e1c621d062a1afd2687d6d9ead0fcdde53a"&gt;Prompting for both ollama and llamacpp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Plus_Factor7011"&gt; /u/Plus_Factor7011 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l4wfon/ollama_vs_llamacpp_different_output_for_same_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l4wfon/ollama_vs_llamacpp_different_output_for_same_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l4wfon/ollama_vs_llamacpp_different_output_for_same_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-06T16:15:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1l50tq2</id>
    <title>Agno Now Supports Dual Model Output (Reasoning + Structure)</title>
    <updated>2025-06-06T19:13:18+00:00</updated>
    <author>
      <name>/u/superconductiveKyle</name>
      <uri>https://old.reddit.com/user/superconductiveKyle</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l50tq2/agno_now_supports_dual_model_output_reasoning/"&gt; &lt;img alt="Agno Now Supports Dual Model Output (Reasoning + Structure)" src="https://external-preview.redd.it/it3L19k1xh50Kb_ccYW3CVpt4x8hg7QgkgkZ9388zN4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b769290d3e5cce48d225c0e5af4f54138038244e" title="Agno Now Supports Dual Model Output (Reasoning + Structure)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw this from Ashpreet CEO of Agno today and it’s genuinely clever:&lt;/p&gt; &lt;p&gt;They’ve added support for using a &lt;strong&gt;separate parser_model&lt;/strong&gt; for structured output.&lt;/p&gt; &lt;p&gt;Basically, you can now let your main model focus on reasoning/creativity, and handle structured formatting with a second, specialized model.&lt;/p&gt; &lt;p&gt;This matters because structured output modes often hurt reasoning performance.&lt;br /&gt; By decoupling the two steps, you get the best of both worlds.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Works with &lt;strong&gt;any model&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Demo uses the new &lt;strong&gt;Osmosis-Structure-0.6B&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feels like a big unlock for anyone working on evals, agent chaining, or structured outputs from open-ended prompts.&lt;/p&gt; &lt;p&gt;Curious to see what people build with this.&lt;/p&gt; &lt;p&gt;Here's one of the recipes using Ollama: &lt;a href="https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/other/parse_model_ollama.py"&gt;https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/other/parse_model_ollama.py&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/superconductiveKyle"&gt; /u/superconductiveKyle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/ashpreetbedi/status/1931064945686544759"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l50tq2/agno_now_supports_dual_model_output_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l50tq2/agno_now_supports_dual_model_output_reasoning/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-06T19:13:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1l4gcpx</id>
    <title>Building a Text Adventure Game with Persistent AI Agents Using Ollama</title>
    <updated>2025-06-06T01:33:36+00:00</updated>
    <author>
      <name>/u/cyb3rofficial</name>
      <uri>https://old.reddit.com/user/cyb3rofficial</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l4gcpx/building_a_text_adventure_game_with_persistent_ai/"&gt; &lt;img alt="Building a Text Adventure Game with Persistent AI Agents Using Ollama" src="https://a.thumbs.redditmedia.com/4aRIHqLn_KCKF9SuojGdg1FsS0SY9RMPI9GZj2OgtF4.jpg" title="Building a Text Adventure Game with Persistent AI Agents Using Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;! I've been working on a project that I think this community might find interesting - a locally-hosted text adventure game where the game it self is basically a craftable file system.&lt;/p&gt; &lt;h1&gt;What makes it special?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Every NPC is powered by Ollama&lt;/strong&gt; - Each agent has their own personality, persistent memory, and individual conversation contexts that survive between sessions&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Smart token management&lt;/strong&gt; - Uses dual models (I'm running &lt;code&gt;qwen3:8b&lt;/code&gt; for main conversations, &lt;code&gt;qwen3:4b&lt;/code&gt; for summaries) with automatic context compression when approaching limits&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Everything persists&lt;/strong&gt; - Agent memories are stored in CSV files, conversations in pickle files, and the entire world state can be saved/loaded with full backups&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Filesystem-based world&lt;/strong&gt; - Each folder is a location, each JSON file is an agent or item. Want to add a new NPC? Just drop a JSON file in a folder!&lt;/p&gt; &lt;h1&gt;Technical highlights:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Token-aware design&lt;/strong&gt;: Real-time monitoring with automatic compression before hitting limits&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Isolated agent contexts&lt;/strong&gt;: Each NPC maintains separate conversation history&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context sharing&lt;/strong&gt;: Agents can share experiences within the same location&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Complete privacy&lt;/strong&gt;: Everything runs locally, no external API calls&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Robust save system&lt;/strong&gt;: With automatic backups&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Quick example:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; /say alice Hello there! *wipes down a mug with practiced ease* Well hello there, stranger! Welcome to the Prancing Pony. What brings you to our little town? &amp;gt; /memory alice Alice's recent memories: Said: &amp;quot;Welcome to the tavern!&amp;quot;; Observed: &amp;quot;A new traveler arrived&amp;quot;; Felt: &amp;quot;Curious about newcomer&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The whole thing runs on local Ollama models, and I've tested it extensively with various model sizes. The token management system really shines - it automatically compresses contexts when needed while preserving important conversation history.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Models used: &lt;code&gt;qwen3:8b&lt;/code&gt; (main), &lt;code&gt;qwen3:4b&lt;/code&gt; (summary model)&lt;/li&gt; &lt;li&gt;Requires: Python 3.13, Ollama&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The summary model will take contextual stuff and try to make decent summaries of stuff happened.&lt;/p&gt; &lt;p&gt;You can use other models, but I've been liking qwen3. It's not too overwhelming and has that simplicity to it. (yes there is &amp;lt;think&amp;gt; suppression too, so you can enable or disable &amp;lt;think&amp;gt; tags in the outputs)&lt;/p&gt; &lt;p&gt;I plan on releasing it soon as a proof of concept on GitHub.&lt;/p&gt; &lt;p&gt;The entire thing is trying to make the people or monsters 'self aware' of their surroundings and other things. Context does matter and so does tokens more importantly the story, so the entire system is made up to help keep things in check via ranking systems.&lt;/p&gt; &lt;p&gt;The compression system uses a dual-model approach with smart token management:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Continuously monitors token usage for each agent's conversation context&lt;/li&gt; &lt;li&gt;When approaching 85% of model's token limit, automatically triggers compression&lt;/li&gt; &lt;li&gt;Uses smaller/faster model (qwen3:4b) to create intelligent summaries&lt;/li&gt; &lt;li&gt;Preserves recent messages (last 8 exchanges) in full detail for continuity&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Ranking/Priority system:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;HIGH PRIORITY:&lt;/strong&gt; Recent interactions, character personality traits, plot developments, relationship changes&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MEDIUM PRIORITY:&lt;/strong&gt; Emotional context, world state changes, important dialogue&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LOW PRIORITY:&lt;/strong&gt; Casual chatter, repetitive conversations, older small talk&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Example compression:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Before (7,500 tokens):&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Turn 1: &amp;quot;Hello Alice, I'm a traveling merchant&amp;quot; Turn 2: &amp;quot;Welcome! I run this tavern with my husband&amp;quot; Turn 3: &amp;quot;What goods do you sell?&amp;quot; Turn 4: &amp;quot;Mainly spices and cloth from the eastern kingdoms&amp;quot; ...40 more turns of detailed conversation... Turn 45: &amp;quot;The bandits have been troubling travelers lately&amp;quot; Turn 46: &amp;quot;I've noticed that too, very concerning&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;After compression (2,000 tokens):&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;SUMMARY: &amp;quot;Alice learned the player is a traveling merchant selling spices and cloth. They discussed her tavern business, shared concerns about recent bandit activity affecting travelers. Alice is friendly and trusting.&amp;quot; RECENT MESSAGES (last 8 turns preserved in full): Turn 39: &amp;quot;The weather has been strange lately&amp;quot; Turn 40: &amp;quot;Yes, unseasonably cold for this time of year&amp;quot; ... Turn 45: &amp;quot;The bandits have been troubling travelers lately&amp;quot; Turn 46: &amp;quot;I've noticed that too, very concerning&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt; Agent still knows you're a merchant, remembers the bandit discussion, maintains her personality, but saves 70% tokens. Conversation flows naturally without any &amp;quot;who are you again?&amp;quot; moments.&lt;/p&gt; &lt;p&gt;Yes, I know there are plenty of things like this that are way way way 10 fold better, but I'm trying to make it more fun and interactive dynamic and more creative and be able to have a full battle system and automated events, I've tried many other role play systems, but I haven't gotten that itch for full (scripted or unscripted) role and battle events. the code base is very messy right now, need to make it more readable and friendly to look at or improve upon. This took me like over 2 weeks to make, and I hope once it push it out to public, It pays off. Also need to make a documented guide on how to actually world build and have that more advanced touch to it. I might make a world editor or something easier to make but I want to release the main project first.&lt;/p&gt; &lt;p&gt;I'll be glad to answer any questions (or concerns) you may have, or requests (if its not already implemented that is.)&lt;/p&gt; &lt;p&gt;Everything will be open source, nothing hidden or behind a weird api or website. Fully 100% free &amp;amp; offline and on your system. &lt;/p&gt; &lt;p&gt;Also To Note; In the images, that starting box can be changed to your liking, so you can call it anything to give it that more personal touch. Also plan to make it 'portable' so you can just open an exe and not worry about installing python.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cyb3rofficial"&gt; /u/cyb3rofficial &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1l4gcpx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l4gcpx/building_a_text_adventure_game_with_persistent_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l4gcpx/building_a_text_adventure_game_with_persistent_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-06T01:33:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1l57w4e</id>
    <title>PSA - Pytorch 2.6 and lower with CUDA 12.8 - causes silent low-level failures.</title>
    <updated>2025-06-07T00:27:19+00:00</updated>
    <author>
      <name>/u/ctfish70</name>
      <uri>https://old.reddit.com/user/ctfish70</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;PSA: PyTorch 2.6 (&amp;amp; dependent apps e.g Ollama) are silently failing on new RTX 50-series GPUs. &lt;/p&gt; &lt;p&gt;Manifestation: Silent low-level unraveling with sm_120 CUDA errors. &lt;/p&gt; &lt;p&gt;Problem: PyTorch 2.6&amp;gt; builds lack Blackwell architecture support.&lt;/p&gt; &lt;p&gt;Solution: Upgrade to PyTorch 2.7 and CUDA 12.8.&lt;/p&gt; &lt;p&gt;It is truly a ghost in the machine and causes zombie processes, etc. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ctfish70"&gt; /u/ctfish70 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l57w4e/psa_pytorch_26_and_lower_with_cuda_128_causes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l57w4e/psa_pytorch_26_and_lower_with_cuda_128_causes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l57w4e/psa_pytorch_26_and_lower_with_cuda_128_causes/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-07T00:27:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1l554d2</id>
    <title>I need help using open web UI with Ollama. Help installing and getting it running win 11</title>
    <updated>2025-06-06T22:17:00+00:00</updated>
    <author>
      <name>/u/FlatImpact4554</name>
      <uri>https://old.reddit.com/user/FlatImpact4554</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/open-webui/open-webui?tab=readme-ov-file"&gt;GitHub - open-webui/open-webui: User-friendly AI Interface (Supports Ollama, OpenAI API, ...)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;this is the file on github; it might as well be a foreign language to me. i run ollama through the command prompt on my 5090. I just learned about this, and I'm loving the photos I'm seeing. having a UI is a game changer for me. when it says open a terminal and type,&amp;quot; Isn't &amp;quot;cmd&amp;quot; a terminal? or are they talking about unix or Linux? What's the easiest way for a win 11 Ollama command prompt user like myself to step by step break these instructions down to get it operational ? any help will be GREATLY appreciated. you have no idea how badly I need this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FlatImpact4554"&gt; /u/FlatImpact4554 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l554d2/i_need_help_using_open_web_ui_with_ollama_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l554d2/i_need_help_using_open_web_ui_with_ollama_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l554d2/i_need_help_using_open_web_ui_with_ollama_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-06T22:17:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1l5eyx1</id>
    <title>Vector Chat Client</title>
    <updated>2025-06-07T07:19:16+00:00</updated>
    <author>
      <name>/u/doornailbarley</name>
      <uri>https://old.reddit.com/user/doornailbarley</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, just thought I'd share a little python ollama front end I made. I added a tool in it this week that saves your chat in real time to a qdrant vector database.... this lets AI learn about you and develop as a assistant over time. Basically RAG for Chat (*cough* vitual gf anyone?)&lt;/p&gt; &lt;p&gt;Anyway, check it out if ya bored, source code included. Feedback welcome.&lt;/p&gt; &lt;p&gt;&lt;a href="https://aimultifool.com/"&gt;https://aimultifool.com/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doornailbarley"&gt; /u/doornailbarley &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l5eyx1/vector_chat_client/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l5eyx1/vector_chat_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l5eyx1/vector_chat_client/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-07T07:19:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1l5hfjt</id>
    <title>How to get ollama to work with anything to automate tasks?</title>
    <updated>2025-06-07T10:10:18+00:00</updated>
    <author>
      <name>/u/Otherwise-Dot-3460</name>
      <uri>https://old.reddit.com/user/Otherwise-Dot-3460</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried to get it working with Open WebUI, which it can't even access the images I literally upload to it, nor can it access the web. &lt;/p&gt; &lt;p&gt;If I use Langflow it can access the web but still can't access any local files, even if I use URL style locations to where it is at. &lt;/p&gt; &lt;p&gt;Is there anyway to use Ollama with something that will let it access local files or just which program like Open WebUI or Langflow should I be using that is easiest to work with? Should I be using a different model? Do I need some kind of addons? I would like to be able to have it identify and sort local files and possibly even upload local files to places like dropbox or google drive or something. Is anything like that even possible?&lt;/p&gt; &lt;p&gt;I'm just not sure what all is even possible. I don't understand why people said Open WebUI was easy when I can't get it to do anything at all. Maybe I need a different model but all the instructions in places said to use Qwen2.5 so that's what I'm using. &lt;/p&gt; &lt;p&gt;This is a couple of my attempts in Open WebUI after setting it up to work with Ollama... &amp;quot;qwen2.5:latest&lt;/p&gt; &lt;p&gt;I see that you've attached an image, but as an AI, I don't have direct access to any files or images from your device. However, if you describe what's in the image or provide more details, I can certainly help you identify what it might be!&lt;/p&gt; &lt;p&gt;&amp;gt;Do you have access to the web?&lt;/p&gt; &lt;p&gt;qwen2.5:latestToday at 5:58 AM&lt;/p&gt; &lt;p&gt;No, as an AI assistant, I don't have direct access to the internet or any files from your device. However, if you describe the content of the image or provide details about it, I can certainly help you analyze and interpret what it might be!&amp;quot;&lt;/p&gt; &lt;p&gt;So as you can see, not helpful in the slightest. With Langflow it says it does have access to the web and it can summarize websites and find things online for me which is useful but I was hoping that it could do things locally on my computer as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Otherwise-Dot-3460"&gt; /u/Otherwise-Dot-3460 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l5hfjt/how_to_get_ollama_to_work_with_anything_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l5hfjt/how_to_get_ollama_to_work_with_anything_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l5hfjt/how_to_get_ollama_to_work_with_anything_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-07T10:10:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1l5gyk5</id>
    <title>Some advice please</title>
    <updated>2025-06-07T09:37:55+00:00</updated>
    <author>
      <name>/u/RegularYak2236</name>
      <uri>https://old.reddit.com/user/RegularYak2236</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey All,&lt;/p&gt; &lt;p&gt;So I have been setting up/creating multiple models each with different prompts etc for a platform I’m creating.&lt;/p&gt; &lt;p&gt;The one thing on my mind is speed/performance. The issue is the reason I’m using local models is because of privacy, the data I will be putting through the models is pretty sensitive.&lt;/p&gt; &lt;p&gt;Without spending huge amounts on maybe lambdas or dedicated gpu servers/renting time based servers e.g run the server for as long as the model takes to process the request, how can I ensure speed/performance is respectable (I will be using queues etc).&lt;/p&gt; &lt;p&gt;Is there any privacy first kind of services available that don’t cost a fortune? &lt;/p&gt; &lt;p&gt;I need some of your guru minds please offering some suggestions please and thank you.&lt;/p&gt; &lt;p&gt;Fyi I am a developer and development etc isn’t an issue and neither is languages used. I’m currently combining laravel laragent with ollama/openweb.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RegularYak2236"&gt; /u/RegularYak2236 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l5gyk5/some_advice_please/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l5gyk5/some_advice_please/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l5gyk5/some_advice_please/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-07T09:37:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1l5ie5j</id>
    <title>Ollama/AnythingLLM on Windows 11 with AMD RX 6600: GPU Not Utilized for LLM Inference - Help!</title>
    <updated>2025-06-07T11:13:46+00:00</updated>
    <author>
      <name>/u/AreBee73</name>
      <uri>https://old.reddit.com/user/AreBee73</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm trying to set up a local LLM on my Windows 11 PC and I'm encountering issues with GPU acceleration, despite having an AMD card. I hope someone with a similar experience can help me out.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My hardware configuration:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Operating System:&lt;/strong&gt; Windows 11 Pro (64-bit)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; AMD Ryzen 5 5600X&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; AMD Radeon RX 6600 (8GB VRAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 32GB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Storage:&lt;/strong&gt; SSD (for OS and programs, I've configured Ollama and AnythingLLM to save heavier data to an HDD to preserve the SSD)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Software installed and purpose:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I have installed &lt;strong&gt;Ollama&lt;/strong&gt; and &lt;strong&gt;AnythingLLM Desktop&lt;/strong&gt;. My goal is to use a local LLM (specifically &lt;strong&gt;Llama 3 8B Instruct&lt;/strong&gt;) to analyze emails and legal documentation, with maximum privacy and reliability.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Despite my AMD Radeon RX 6600 having 8GB of VRAM, Ollama doesn't seem to be utilizing it for Llama 3 model inference. I've checked GPU usage via &lt;strong&gt;Windows Task Manager (Performance tab, GPU section, monitoring &amp;quot;Compute&amp;quot; or &amp;quot;3D&amp;quot;)&lt;/strong&gt; while the model processes a complex request: GPU usage remains at 0-5%, while the CPU spikes to 100%. This makes inference (response generation) very slow.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I've already tried for the GPU:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I performed a &lt;strong&gt;clean and complete reinstallation of the &amp;quot;AMD Software: Adrenalin Edition&amp;quot; package&lt;/strong&gt; (the latest version available for my RX 6600).&lt;/li&gt; &lt;li&gt;During installation, I selected the &lt;strong&gt;&amp;quot;Factory Reset&amp;quot;&lt;/strong&gt; option to ensure all previous drivers and configurations were completely removed.&lt;/li&gt; &lt;li&gt;I restarted the PC after driver installation.&lt;/li&gt; &lt;li&gt;I also tried updating Ollama via &lt;code&gt;ollama update&lt;/code&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The final result is that the GPU is still not being utilized.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Has anyone with an AMD GPU (particularly an RX 6000 series) on Windows 11 successfully enabled GPU acceleration with Ollama?&lt;/li&gt; &lt;li&gt;Are there specific steps or additional ROCm configurations on Windows that I might have missed for consumer GPUs?&lt;/li&gt; &lt;li&gt;Is there an environment variable or a specific Ollama configuration I need to set to force AMD GPU usage, beyond what Ollama should automatically detect?&lt;/li&gt; &lt;li&gt;Is it possible that the RX 6600 has insufficient or problematic ROCm support on Windows for this type of workload?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any advice or shared experience would be greatly appreciated. Thank you in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AreBee73"&gt; /u/AreBee73 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l5ie5j/ollamaanythingllm_on_windows_11_with_amd_rx_6600/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l5ie5j/ollamaanythingllm_on_windows_11_with_amd_rx_6600/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l5ie5j/ollamaanythingllm_on_windows_11_with_amd_rx_6600/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-07T11:13:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1l5iu3l</id>
    <title>For task-specific agents use task-specific LLMs for routing and hand off - NOT semantic techniques.</title>
    <updated>2025-06-07T11:41:11+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you are building caching techniques for LLMs or developing a router to handle certain queries by select LLMs/agents - know that semantic caching and routing is a broken approach. Here is why.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Follow-ups or Elliptical Queries: Same issue as embeddings — &amp;quot;And Boston?&amp;quot; doesn't carry meaning on its own. Clustering will likely put it in a generic or wrong cluster unless context is encoded.&lt;/li&gt; &lt;li&gt;Semantic Drift and Negation: Clustering can’t capture logical distinctions like negation, sarcasm, or intent reversal. “I don’t want a refund” may fall in the same cluster as “I want a refund.”&lt;/li&gt; &lt;li&gt;Unseen or Low-Frequency Queries: Sparse or emerging intents won’t form tight clusters. Outliers may get dropped or grouped incorrectly, leading to intent “blind spots.”&lt;/li&gt; &lt;li&gt;Over-clustering / Under-clustering: Setting the right number of clusters is non-trivial. Fine-grained intents often end up merged unless you do manual tuning or post-labeling.&lt;/li&gt; &lt;li&gt;Short Utterances: Queries like “cancel,” “report,” “yes” often land in huge ambiguous clusters. Clustering lacks precision for atomic expressions.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What can you do instead? You are far better off in using a LLM and instruct it to predict the scenario for you (like here is a user query, does it overlap with recent list of queries here) or build a very small and highly capable TLM (Task-specific LLM).&lt;/p&gt; &lt;p&gt;I wrote a guide on how to do this with TLMs via a gateway for agents. Links to the guide and the proejct in the comments. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l5iu3l/for_taskspecific_agents_use_taskspecific_llms_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l5iu3l/for_taskspecific_agents_use_taskspecific_llms_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l5iu3l/for_taskspecific_agents_use_taskspecific_llms_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-07T11:41:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1l5hfeq</id>
    <title>What is the best and affordable uncensored model to fine tune with your own data?</title>
    <updated>2025-06-07T10:10:01+00:00</updated>
    <author>
      <name>/u/sprmgtrb</name>
      <uri>https://old.reddit.com/user/sprmgtrb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Imagine I have 10,000 projects, they each have a title, description, and 6 metadata fields. I want to train an LLM to know about these projects where I can have a search input on my site to ask for a certain type of project and the LLM knows which projects to list. Which models do most people use for my type of case? It has to be an uncensored model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sprmgtrb"&gt; /u/sprmgtrb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l5hfeq/what_is_the_best_and_affordable_uncensored_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l5hfeq/what_is_the_best_and_affordable_uncensored_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l5hfeq/what_is_the_best_and_affordable_uncensored_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-07T10:10:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1l60vhh</id>
    <title>Librechat issues with ollama</title>
    <updated>2025-06-08T01:48:15+00:00</updated>
    <author>
      <name>/u/Large_Yams</name>
      <uri>https://old.reddit.com/user/Large_Yams</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone have advice for why librechat needs to remain in the foreground while responses are generating? As soon as I change apps for a few seconds, when I go back to librechat the output fails. I would've thought it would keep generating and show me the output when I open it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Large_Yams"&gt; /u/Large_Yams &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l60vhh/librechat_issues_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l60vhh/librechat_issues_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l60vhh/librechat_issues_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-08T01:48:15+00:00</published>
  </entry>
</feed>
