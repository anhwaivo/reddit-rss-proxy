<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-04-29T04:07:36+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1k8euyg</id>
    <title>2x 64GB M2 Mac Studio Ultra for hosting locally</title>
    <updated>2025-04-26T14:57:45+00:00</updated>
    <author>
      <name>/u/Mountain_Desk_767</name>
      <uri>https://old.reddit.com/user/Mountain_Desk_767</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have these 2x Macs, and i am thinking of combining them (cluster) to host &amp;gt;70B models.&lt;br /&gt; The question is, is it possible i combine both of them to be able to utilize their VRAM, improve performance and use large models. Can i set them up as a server and only have my laptop access it. I will have the open web ui on my laptop and connect to them.&lt;/p&gt; &lt;p&gt;Is it worth the consideration.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mountain_Desk_767"&gt; /u/Mountain_Desk_767 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8euyg/2x_64gb_m2_mac_studio_ultra_for_hosting_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8euyg/2x_64gb_m2_mac_studio_ultra_for_hosting_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8euyg/2x_64gb_m2_mac_studio_ultra_for_hosting_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-26T14:57:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8nqfe</id>
    <title>"flash attention enabled but not supported by model"</title>
    <updated>2025-04-26T21:29:35+00:00</updated>
    <author>
      <name>/u/the_renaissance_jack</name>
      <uri>https://old.reddit.com/user/the_renaissance_jack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've got flash attention and KV cache enabled in my environment variables, but I can't figure out which models do or don't support it.&lt;/p&gt; &lt;p&gt;Is there some special trigger to enable it?&lt;/p&gt; &lt;p&gt;I've tried granite3.3:2b, mistral:7b, and gemma3:4b (multiple).&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# ollama export OLLAMA\_FLASH\_ATTENTION=&amp;quot;1&amp;quot; export OLLAMA\_CONTEXT\_LENGTH=&amp;quot;8192&amp;quot; export OLLAMA\_KV\_CACHE\_TYPE=&amp;quot;q4\_0&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/the_renaissance_jack"&gt; /u/the_renaissance_jack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8nqfe/flash_attention_enabled_but_not_supported_by_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8nqfe/flash_attention_enabled_but_not_supported_by_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8nqfe/flash_attention_enabled_but_not_supported_by_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-26T21:29:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8jubs</id>
    <title>I installed a Tesla M10 into an r740</title>
    <updated>2025-04-26T18:34:25+00:00</updated>
    <author>
      <name>/u/dopey_se</name>
      <uri>https://old.reddit.com/user/dopey_se</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am aware this card was ancient.&lt;/p&gt; &lt;p&gt;The main reason I bought it since it as 'officially' supported on an r740 and it would let me confirm the parts/working setup before I experiment with newer/unsupported cards. I did think I'd atleast find *some* use for it, and that it beat pure CPUs though..&lt;/p&gt; &lt;p&gt;I do have some questions; but for those that are searching later on r740s + gpus -- It is common folks are asking what parts are needed so thought i'd share.&lt;/p&gt; &lt;p&gt;----&lt;/p&gt; &lt;p&gt;My r740 came with a PM3YD riser on the right side - So without power provided. The middle riser is for the raid controller, so it's not usable. &lt;/p&gt; &lt;p&gt;The PSUs are 750w, I only have 1 of two connected.&lt;/p&gt; &lt;p&gt;Aside from the M10 card itself the only thing I ordered was TR5TP cable - However this cable is too short to go from the motherboard connection to the card on the right riser (I believe these two connections are meant for the middle and left riser, they are not meant to power the first riser's card). I used an pCIE 8 pin extension cable. &lt;/p&gt; &lt;p&gt;I did *not* change the PSU to an 1100watt, add fans, or change risers - Or anything else that is in the gpu enablement kit. &lt;/p&gt; &lt;p&gt;Worth noting(obvious I suppose), you will lose a pcie slot on the riser if the card is 2x width - Likely. Nevermind bifurcation/performance, but just thought i'd share. &lt;/p&gt; &lt;p&gt;TLDR; TR5TP + Extension cable is all I needed.&lt;/p&gt; &lt;p&gt;-----&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results + Question&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The M10 performs &lt;strong&gt;worse&lt;/strong&gt; than the cpus for me so far ! :) I've tried smaller models that can fit within 1 of it's GPU, i've tried setting env variables to only use 1 gpu, etc. Even tried numa setting to one or other cpu incase that was the issue. &lt;/p&gt; &lt;p&gt;I am very much a newbie to do LLM at home -- So before I bash my head against the wall more. Is this expected ? I know the Tesla M10 is ancient, but would dual Intel(R) Xeon(R) Gold 6126 with half a TB of ram really *outperform* The M10? &lt;/p&gt; &lt;p&gt;I've tested with arch and ubuntu, and on ubuntu have compiled llama-cpp from source. I do see the GPU being used per nvidia-smi it just sucks at performance :) I have not tried to downgrade cuda/drivers to something that 'officially' supported the M10 -- but since I do see the card being utilized I don't think that would matter? &lt;/p&gt; &lt;p&gt;Here is using the GPU&lt;br /&gt; &lt;code&gt;llama_perf_sampler_print: sampling time = 1.96 ms / 38 runs ( 0.05 ms per token, 19387.76 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama_perf_context_print: load time = 3048.63 ms&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama_perf_context_print: prompt eval time = 1028.66 ms / 17 tokens ( 60.51 ms per token, 16.53 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama_perf_context_print: eval time = 4358.45 ms / 20 runs ( 217.92 ms per token, 4.59 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama_perf_context_print: total time = 9361.87 ms / 37 tokens&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Here is using CPU&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama_perf_sampler_print: sampling time = 10.60 ms / 79 runs ( 0.13 ms per token, 7452.13 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama_perf_context_print: load time = 1853.95 ms&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama_perf_context_print: prompt eval time = 414.58 ms / 17 tokens ( 24.39 ms per token, 41.01 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama_perf_context_print: eval time = 10234.78 ms / 61 runs ( 167.78 ms per token, 5.96 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama_perf_context_print: total time = 11537.87 ms / 78 tokens&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;dopey@sonny:~/models$&lt;/code&gt; &lt;/p&gt; &lt;p&gt;Here is ollama with GPU&lt;/p&gt; &lt;p&gt;&lt;code&gt;dopey@sonny:~/models$ ollama run tinyllama --verbose&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; tell me a joke&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Sure, here's a classic joke for you:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;A person walks into a bar and sits down at a single chair. The bartender approaches him and asks, &amp;quot;Excuse me, do you need anything?&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;The person replies, &amp;quot;Yes! I just need some company.&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;The bartender smiles and says, &amp;quot;That's not something that's available in a bar these days. But I have good news - we have a few chairs left over from last night.&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;The person laughs and says, &amp;quot;Awesome! Thanks for the compliment. That was just what I needed. Let me sit here with you for a little while.&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;The bartender grins and nods, then turns to another customer. The joke ends with the bartender saying to the new customer, &amp;quot;Oh, sorry about that - we had an extra chair left over from last night.&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;total duration: 5.845741618s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;load duration: 62.907712ms&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval count: 40 token(s)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval duration: 433.397307ms&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval rate: 92.29 tokens/s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval count: 202 token(s)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval duration: 5.347443728s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval rate: 37.78 tokens/s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And with &lt;code&gt;CUDA_VISIBLE_DEVICES=-1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;dopey@sonny:~/models$ sudo systemctl daemon-reload ;sudo systemctl restart ollama&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;dopey@sonny:~/models$ ollama run tinyllama --verbose&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; tell me a joke&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;(Laughs) Well, that was a close one! But now here's another one for you:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;What do you call a happy-go-lucky AI with a sense of humor?&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;(Sighs) Oh, well. I guess that'll have to do then...&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;total duration: 1.6980198s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;load duration: 62.293307ms&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval count: 40 token(s)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval duration: 168.484526ms&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval rate: 237.41 tokens/s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval count: 67 token(s)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval duration: 1.465694164s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval rate: 45.71 tokens/s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; Send a message (/? for help)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;It's comical. The first Anoxiom/llama-3-8b-Instruct-Q6_K-GGUF:Q6_K as I thought/read that model be better for the M10. If I do very small models, the performance is even large gap. I've yet to find a model the M10 outperforms my CPU :) &lt;/p&gt; &lt;p&gt;I've spent better part of the day tinkering with both ollama and llama.cpp, thought i'd share/ask here before going further down the rabbit hole! &amp;lt;3 &lt;/p&gt; &lt;p&gt;Feel free to laugh that I bought an M10 in 2025 -- It did accomplish it's goal of confirming what I needed to setup a GPU on an r740. Rather have a working setup *before* in terms of cables/risers I buy a expensive card. I just thought I could *atleast* use it on a small model for genai frigate, or home assistant, or something.. but so far it's performing worse than pure CPU :D :D :D &lt;/p&gt; &lt;p&gt;(I ordered a P100 as well, it too is officially supported. Any bets if it'll be paper weight or atleast beat the CPUs?)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dopey_se"&gt; /u/dopey_se &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8jubs/i_installed_a_tesla_m10_into_an_r740/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8jubs/i_installed_a_tesla_m10_into_an_r740/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8jubs/i_installed_a_tesla_m10_into_an_r740/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-26T18:34:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8kbjn</id>
    <title>Function calling - guidance</title>
    <updated>2025-04-26T18:55:41+00:00</updated>
    <author>
      <name>/u/jagauthier</name>
      <uri>https://old.reddit.com/user/jagauthier</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay, some of this post may be laced with ignorance. That is because I am ignorant. But, I am pretty sure what I want to do is function calling.&lt;/p&gt; &lt;p&gt;I am using ollama (but I am not married to it), and I also have 2 gemma3 models running that support tools. I have a couple frontends that use different APIs. &lt;/p&gt; &lt;p&gt;I primarily use open-webui, but also have my ollama instance connected to home assistance for control. I have a couple objectives. &lt;/p&gt; &lt;p&gt;I want to ask &amp;quot;current&amp;quot; questions like &amp;quot;What is the weather&amp;quot;, but I also want to be able to integrate a whole source code project while I'm developing (vscode+continue.dev perhaps)&lt;/p&gt; &lt;p&gt;So, I've been reading about function calling and experimenting. I am pretty sure I am missing a piece. The piece is: Where is the function that gets called and how is that implemented?&lt;/p&gt; &lt;p&gt;I've seen a lot of python examples, that seem to actually call the function itself. But that seems to be the client, call, and response. That doesn't work from two different API endpoints (open-webui and home assistance), or even any of them singularly.&lt;/p&gt; &lt;p&gt;I have multiple different endpoints I feel like this needs to happen all in one place on the server itself. Like, ollama itself has to actually call the function. But that doesn't make much sense how it would do that.&lt;/p&gt; &lt;p&gt;I am trying to expand my understanding on how this would work. So, would definitely appreciate any input. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jagauthier"&gt; /u/jagauthier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8kbjn/function_calling_guidance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8kbjn/function_calling_guidance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8kbjn/function_calling_guidance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-26T18:55:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8u27c</id>
    <title>Ok, I have a project, is Ollama what I want, here?</title>
    <updated>2025-04-27T02:54:23+00:00</updated>
    <author>
      <name>/u/Feral_Guardian</name>
      <uri>https://old.reddit.com/user/Feral_Guardian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. Ok. I've been using a tablet with google assistant on it as an alarm clock, and I'd like to branch out. What I'm looking to do is have an alarm clock that will ring, with a customizeable UI, (yeah, google's alarm clock controls aren't very good. They're tiny. Exactly what I need to focus on first thing in the morning without my glasses, right?) and then go through a routine. Ideally.... the Babylon 5 style &amp;quot;Good Morning. The time is.... yadda yadda.&amp;quot; Maybe list time, outside weather conditions, new emails, and then go on to play a news podcast or three. That sort of thing. Is using an LLM for this overkill? It seems like using the cleaned up DeepSeek or something would be a good idea. I'd be running this on an older surface tablet under Linux. Is this hardware too limited? Yes, it's limited, no GPU or anything, but on the other hand I'm not intending on training it or anything, just run some simple, preset commands.&lt;/p&gt; &lt;p&gt;Any thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Feral_Guardian"&gt; /u/Feral_Guardian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8u27c/ok_i_have_a_project_is_ollama_what_i_want_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8u27c/ok_i_have_a_project_is_ollama_what_i_want_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8u27c/ok_i_have_a_project_is_ollama_what_i_want_here/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-27T02:54:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8itfb</id>
    <title>Best model for synthetic data</title>
    <updated>2025-04-26T17:49:57+00:00</updated>
    <author>
      <name>/u/No_Wind7503</name>
      <uri>https://old.reddit.com/user/No_Wind7503</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I working on synthetic data generation system and I need small models (3-8B) to generate the data, anyone know best model can do that or specific to do that&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Wind7503"&gt; /u/No_Wind7503 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8itfb/best_model_for_synthetic_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8itfb/best_model_for_synthetic_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8itfb/best_model_for_synthetic_data/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-26T17:49:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8cub6</id>
    <title>Any UI for Local Fine-Tuning of Open-Source LLMs?</title>
    <updated>2025-04-26T13:22:31+00:00</updated>
    <author>
      <name>/u/who_is_erik</name>
      <uri>https://old.reddit.com/user/who_is_erik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey AI experts!&lt;/p&gt; &lt;p&gt;I'm exploring local fine-tuning of open-source LLMs. We've seen tools like AI-Toolkit, Kohya SS, and Flux Gym enable local training and fine-tuning of diffusion models. &lt;/p&gt; &lt;p&gt;Specifically: Are there frameworks or libraries that support local fine-tuning of open-source LLMs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/who_is_erik"&gt; /u/who_is_erik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8cub6/any_ui_for_local_finetuning_of_opensource_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8cub6/any_ui_for_local_finetuning_of_opensource_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8cub6/any_ui_for_local_finetuning_of_opensource_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-26T13:22:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8tuh7</id>
    <title>ollama not using cuda devices, despite detecting them</title>
    <updated>2025-04-27T02:42:02+00:00</updated>
    <author>
      <name>/u/gamamoder</name>
      <uri>https://old.reddit.com/user/gamamoder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k8tuh7/ollama_not_using_cuda_devices_despite_detecting/"&gt; &lt;img alt="ollama not using cuda devices, despite detecting them" src="https://external-preview.redd.it/0UzN0vIXsJTeFPJFGuUQaRFlHQzJDLgosVrv2azwCHg.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=514a843c8a98fe4681b9053b5005311e071dceed" title="ollama not using cuda devices, despite detecting them" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gamamoder"&gt; /u/gamamoder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://pixeldrain.com/u/WcLHj62p"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8tuh7/ollama_not_using_cuda_devices_despite_detecting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8tuh7/ollama_not_using_cuda_devices_despite_detecting/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-27T02:42:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8fti9</id>
    <title>Best model for Web Development?</title>
    <updated>2025-04-26T15:40:29+00:00</updated>
    <author>
      <name>/u/Akila_Kavinga</name>
      <uri>https://old.reddit.com/user/Akila_Kavinga</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! What's a model that best suited for web development? I just want a model that can read documentation for me. If that's not possible, a model that can reason an answer with minimal hallucinating will do.&lt;/p&gt; &lt;p&gt;PC Specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;4060 8GB Laptop GPU&lt;/li&gt; &lt;li&gt;16GB RAM&lt;/li&gt; &lt;li&gt;i7-13620H&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Akila_Kavinga"&gt; /u/Akila_Kavinga &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8fti9/best_model_for_web_development/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8fti9/best_model_for_web_development/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8fti9/best_model_for_web_development/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-26T15:40:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8qazj</id>
    <title>Train or get database to ai for analysis</title>
    <updated>2025-04-26T23:34:40+00:00</updated>
    <author>
      <name>/u/i74ifa</name>
      <uri>https://old.reddit.com/user/i74ifa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a simple query regarding Ai I want to train or give all the database in a rather large project (approximately 5gb) and I want him to give me reports about it by prompt input&lt;/p&gt; &lt;p&gt;What are the tools that help me, and if I use openai, is there any way I can give him this huge data? For the project, there are many detailed reports.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/i74ifa"&gt; /u/i74ifa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8qazj/train_or_get_database_to_ai_for_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8qazj/train_or_get_database_to_ai_for_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8qazj/train_or_get_database_to_ai_for_analysis/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-26T23:34:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8zy5u</id>
    <title>Need Advice on Content Writing Agents</title>
    <updated>2025-04-27T09:23:19+00:00</updated>
    <author>
      <name>/u/karachiwala</name>
      <uri>https://old.reddit.com/user/karachiwala</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I am building a content production pipeline with three agents (outliner, writer, and editor). My stack is &lt;/p&gt; &lt;p&gt;LangChain&lt;br /&gt; CrewAI&lt;/p&gt; &lt;p&gt;Ollama running DeepSeek R1:1.5b&lt;/p&gt; &lt;p&gt;It is a very simple project that I meant to expand with a Streamlit UI and tools to help the agents access the search engine data.&lt;br /&gt; I am getting mediocre results at best with writer agent either not following the outline or producing junk. What can i do to improve the quality of the output. I suspect the issue lies in how i have worded the task and agent description. However, i would appreciate any advice on how i can get better quality results with this basic pipeline. &lt;/p&gt; &lt;p&gt;For reference, here is my code:&lt;br /&gt; &lt;a href="https://smalldev.tools/share-bin/059pTIBK"&gt;https://smalldev.tools/share-bin/059pTIBK&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/karachiwala"&gt; /u/karachiwala &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8zy5u/need_advice_on_content_writing_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8zy5u/need_advice_on_content_writing_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8zy5u/need_advice_on_content_writing_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-27T09:23:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8cprt</id>
    <title>Free GPU for Openwebui</title>
    <updated>2025-04-26T13:16:20+00:00</updated>
    <author>
      <name>/u/guuidx</name>
      <uri>https://old.reddit.com/user/guuidx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi people!&lt;/p&gt; &lt;p&gt;I wrote a post two days ago about using google colab cpu for free to use for Ollama. It was kinda aimed at developers but many webui users were interested. It was not supported, I had to add that functionality. So, that's done now!&lt;/p&gt; &lt;p&gt;Also, by request, i made a video now. The video is full length and you can see that the setup is only a few steps and a few minutes to complete in total! In the video you'll see me happily using a super fast qwen2.5 using openwebui! I'm showing the openwebui config. &lt;/p&gt; &lt;p&gt;The link mentioned in the video as 'my post' is: &lt;a href="https://www.reddit.com/r/ollama/comments/1k674xf/free_ollama_gpu/"&gt;https://www.reddit.com/r/ollama/comments/1k674xf/free_ollama_gpu/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know your experience! &lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k8cprt/video/43794nq7i6xe1/player"&gt;https://reddit.com/link/1k8cprt/video/43794nq7i6xe1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/guuidx"&gt; /u/guuidx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8cprt/free_gpu_for_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8cprt/free_gpu_for_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8cprt/free_gpu_for_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-26T13:16:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8z194</id>
    <title>Garbage / garbled responses</title>
    <updated>2025-04-27T08:18:55+00:00</updated>
    <author>
      <name>/u/mrocty</name>
      <uri>https://old.reddit.com/user/mrocty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running Open WebUI, and Ollama, in two separate docker containers. Responses were working fine when I was using the Open WebUI built in Ollama (&lt;code&gt;ghcr.io/open-webui/open-webui:ollama&lt;/code&gt;), but running a separate container, I get responses like this: &lt;a href="https://imgur.com/a/KoZ8Pgj"&gt;https://imgur.com/a/KoZ8Pgj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All the results I get with &amp;quot;Ollama garbage responses&amp;quot; or anything like that, seem to all be about third party tools that use Ollama, or suggesting that the model is corrupted, or saying I need to adjust the quantization (which I didn't need to do with &lt;code&gt;open-webui:ollama&lt;/code&gt;), so either I'm using the wrong search terms, or I'm the first person in the world that this has happened to.&lt;/p&gt; &lt;p&gt;I've deleted all of the models, and re-downloaded them, but that didn't help.&lt;/p&gt; &lt;p&gt;My docker-compose files are below, but does anyone know wtf would be causing this? &lt;/p&gt; &lt;pre&gt;&lt;code&gt;services: open-webui: container_name: open-webui image: ghcr.io/open-webui/open-webui:main volumes: - ./data:/app/backend/data restart: always environment: - OLLAMA_HOST=http://ollama.my-local-domain.com:11434 services: ollama: volumes: - ./ollama:/root/.ollama container_name: ollama pull_policy: always tty: true restart: unless-stopped image: docker.io/ollama/ollama:latest environment: - OLLAMA_KEEP_ALIVE=24h ports: - 11434:11434 deploy: resources: reservations: devices: - driver: nvidia count: 1 capabilities: [gpu] &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Edit&lt;/h3&gt; &lt;p&gt;&amp;quot;Solved&amp;quot; - issue is with Ollama 0.6.6 only, 0.6.5 and earlier works fine&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrocty"&gt; /u/mrocty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8z194/garbage_garbled_responses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k8z194/garbage_garbled_responses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k8z194/garbage_garbled_responses/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-27T08:18:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1k98rmq</id>
    <title>Attempt at RAG setup</title>
    <updated>2025-04-27T17:01:38+00:00</updated>
    <author>
      <name>/u/terramot</name>
      <uri>https://old.reddit.com/user/terramot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Intro:&lt;/strong&gt;&lt;br /&gt; I've recently read an article about some guy setting up an AI assistant to report his emails, events and other stuff. I liked the idea so i started to setup something with the intention of being similar.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;br /&gt; I have an instance of ollama running with granite3.1-dense:2b (waiting on bitnet support), nomic-embed-text v1.5 and some other modules&lt;br /&gt; duckdb with a file containing the emails table with the following rows:&lt;br /&gt; id&lt;br /&gt; message_id_hash&lt;br /&gt; email_date&lt;br /&gt; from_addr&lt;br /&gt; to_addr,subject,&lt;br /&gt; body&lt;br /&gt; fetch_date&lt;br /&gt; embeddings&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Description:&lt;/strong&gt;&lt;br /&gt; I have a script that fetches the emails from my mailbox, extracts the content and stores in a duckdb file. Then generates the embeddings ( at first i was only using body content, then i added subject and i've also tried including the from address to see if it would improve the result )&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;br /&gt; Let's say i have some emails from ebay about new matches, i tried searching for:&lt;br /&gt; &amp;quot;what are the new matches on ebay?&amp;quot; &lt;/p&gt; &lt;p&gt;&lt;em&gt;using only similiarity function (no AI envolved besides the embeddings)&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt;&lt;br /&gt; I noticed that while some emails from ebay were at the top, others were at the bottom of the top 10, while unrelated emails were in between. I understand it will never be 100% accurate i just found it odd this happens even when i just searched for &amp;quot;ebay&amp;quot;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt;&lt;br /&gt; Because i'm a complete novice in this, i'm not sure what should be my next step. &lt;/p&gt; &lt;p&gt;Should i only extract the keywords from the body content and generate embeddings for them? This way, if i search for something ebay related the connectors (words) will not be part of the embeddings distance measure.&lt;/p&gt; &lt;p&gt;Is this the way to go about it or is there something else i'm missing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/terramot"&gt; /u/terramot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k98rmq/attempt_at_rag_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k98rmq/attempt_at_rag_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k98rmq/attempt_at_rag_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-27T17:01:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1k95j79</id>
    <title>Work Buddy: Local Ollama Chat &amp; RAG Extension for Raycast - Demo &amp; Feedback Request!</title>
    <updated>2025-04-27T14:42:35+00:00</updated>
    <author>
      <name>/u/deathrow902</name>
      <uri>https://old.reddit.com/user/deathrow902</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k95j79/work_buddy_local_ollama_chat_rag_extension_for/"&gt; &lt;img alt="Work Buddy: Local Ollama Chat &amp;amp; RAG Extension for Raycast - Demo &amp;amp; Feedback Request!" src="https://external-preview.redd.it/ZKTvp0y2mnON5TAMl65cMaLkfPhJNameI8Ql-iKjZB4.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e53e7cd22c55024cc9234b822427e8d43c94fd8" title="Work Buddy: Local Ollama Chat &amp;amp; RAG Extension for Raycast - Demo &amp;amp; Feedback Request!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I wanted to share a Raycast extension I've been developing called &lt;strong&gt;Work Buddy&lt;/strong&gt;, which tightly integrates local AI models (via Ollama) into the Raycast productivity tool for macOS. &lt;/p&gt; &lt;p&gt;For those unfamiliar, &lt;strong&gt;Raycast is a blazingly fast, extensible application launcher and productivity booster for macOS, often seen as a powerful alternative to Spotlight.&lt;/strong&gt; It allows you to perform various actions quickly using keyboard commands.&lt;/p&gt; &lt;p&gt;My Work Buddy extension brings the power of local AI directly into this environment, with a strong emphasis on keeping your data private and local. Here are the key features:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local Chat Storage:&lt;/strong&gt; Work Buddy saves all your chat conversations directly on your Mac. It creates and manages chat history files locally, ensuring your interactions remain private and under your control.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Powered by Local AI Models (Ollama):&lt;/strong&gt; The extension harnesses Ollama to run AI models directly on your machine. This means your queries and conversations are processed locally, without relying on external AI services.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Self-Hosted RAG Infrastructure:&lt;/strong&gt; For the &amp;quot;RAG Talk&amp;quot; feature, Work Buddy uses a local backend server (built with Express) and a PostgreSQL database with the pgvector extension. This entire setup runs on your system via Docker, keeping your document processing and data retrieval local and private.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Here are the two main ways you can interact with Work Buddy:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Talk - Simple Chat with Local AI:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Engage in direct conversations with your downloaded Ollama models. Just type &amp;quot;Talk&amp;quot; in Raycast to start chatting! You can even select different models within the chat view (&lt;code&gt;mistral:latest&lt;/code&gt;, &lt;code&gt;codegemma:7b&lt;/code&gt;, &lt;code&gt;deepseek-r1:1.5b&lt;/code&gt;, &lt;code&gt;llama3.2:latest&lt;/code&gt; currently supported). All chat history from &amp;quot;Talk&amp;quot; is saved locally.&lt;/p&gt; &lt;p&gt;Demo:&lt;br /&gt; &lt;a href="https://share.zight.com/kpurNPLQ"&gt;Demo Video (Zight Link)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k95j79/video/65j1jeok2exe1/player"&gt;AI Chat - Raycast&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. RAG Talk - Context-Aware Chat with Your Documents:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;This feature allows you to upload your own documents and have conversations grounded in their content, all within Raycast. Work Buddy currently supports these file types:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;.json&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;.jsonl&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;.txt&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;.ts&lt;/code&gt; / &lt;code&gt;.tsx&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;.js&lt;/code&gt; / &lt;code&gt;.jsx&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;.md&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;.csv&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;.docx&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;.pptx&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;.pdf&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It uses a local backend server (built with Express) and a PostgreSQL database with pgvector, all easily set up with Docker Compose. The chat history for &amp;quot;RAG Talk&amp;quot; is also stored locally.&lt;/p&gt; &lt;p&gt;Demo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://share.zight.com/7Kulbe0l"&gt;Demo Video (Zight Link)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k95j79/video/nobebtpm2exe1/player"&gt;Rag Chat - Raycast&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm really excited about the potential of having a fully local and private AI assistant integrated directly into Raycast, powered by Ollama. Before I open-source the repository, I'd love to get your initial thoughts and feedback on the concept and the features, especially from an Ollama user's perspective.&lt;/p&gt; &lt;p&gt;What do you think of:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The overall idea of a local Ollama-powered AI assistant within Raycast?&lt;/li&gt; &lt;li&gt;The two core features: simple chat and RAG with local documents?&lt;/li&gt; &lt;li&gt;The supported document types for RAG Talk?&lt;/li&gt; &lt;li&gt;The focus on local data storage and privacy, including the use of local AI models and a self-hosted RAG infrastructure using Ollama?&lt;/li&gt; &lt;li&gt;Are there any features you'd love to see in such an extension that leverages Ollama within Raycast?&lt;/li&gt; &lt;li&gt;Any initial usability thoughts based on the demos, considering you might be new to Raycast?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking forward to hearing your valuable feedback!&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/deathrow902"&gt; /u/deathrow902 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k95j79/work_buddy_local_ollama_chat_rag_extension_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k95j79/work_buddy_local_ollama_chat_rag_extension_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k95j79/work_buddy_local_ollama_chat_rag_extension_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-27T14:42:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1k96sml</id>
    <title>MBA deepseek-coder-v2</title>
    <updated>2025-04-27T15:37:20+00:00</updated>
    <author>
      <name>/u/EqualNew2143</name>
      <uri>https://old.reddit.com/user/EqualNew2143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to buy a macbook air 24gb ram. Will it be able to run deepseek-coder-v2 16b parameters daily ??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EqualNew2143"&gt; /u/EqualNew2143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k96sml/mba_deepseekcoderv2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k96sml/mba_deepseekcoderv2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k96sml/mba_deepseekcoderv2/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-27T15:37:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1k9opqc</id>
    <title>AI Model that learns to reflect my personality or learn a new one</title>
    <updated>2025-04-28T06:11:06+00:00</updated>
    <author>
      <name>/u/claushill777</name>
      <uri>https://old.reddit.com/user/claushill777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like in the title, i'm trying to make Dolphin3 have any but it forgets and i'm now to the thing so i whould like to try a model that's created for this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/claushill777"&gt; /u/claushill777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k9opqc/ai_model_that_learns_to_reflect_my_personality_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k9opqc/ai_model_that_learns_to_reflect_my_personality_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k9opqc/ai_model_that_learns_to_reflect_my_personality_or/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-28T06:11:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1k98dsa</id>
    <title>What’s the best way to handle multiple users connecting to Ollama at the same time? (Ubuntu 22 + RTX 4060)</title>
    <updated>2025-04-27T16:45:24+00:00</updated>
    <author>
      <name>/u/a3zdv</name>
      <uri>https://old.reddit.com/user/a3zdv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I’m currently working on a project using Ollama, and I need to allow multiple users to interact with the model simultaneously in a stable and efficient way.&lt;/p&gt; &lt;p&gt;Here are my system specs: OS: Ubuntu 22.04 GPU: NVIDIA GeForce RTX 4060 CPU: Ryzen 7 5700G RAM: 32GB&lt;/p&gt; &lt;p&gt;Right now, I’m running Ollama locally on my machine. What’s the best practice or recommended setup for handling multiple concurrent users? For example: Should I create an intermediate API layer? Or is there a built-in way to support multiple sessions? Any tips, suggestions, or shared experiences would be highly appreciated!&lt;/p&gt; &lt;p&gt;Thanks a lot in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a3zdv"&gt; /u/a3zdv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k98dsa/whats_the_best_way_to_handle_multiple_users/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k98dsa/whats_the_best_way_to_handle_multiple_users/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k98dsa/whats_the_best_way_to_handle_multiple_users/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-27T16:45:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1k9h0cv</id>
    <title>Ollama bash completions</title>
    <updated>2025-04-27T23:00:26+00:00</updated>
    <author>
      <name>/u/ehrlz</name>
      <uri>https://old.reddit.com/user/ehrlz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k9h0cv/ollama_bash_completions/"&gt; &lt;img alt="Ollama bash completions" src="https://a.thumbs.redditmedia.com/KDm4RpwJME_ZTche8FAMGcjwgvhPpTz9bHqEW1gnI80.jpg" title="Ollama bash completions" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever find yourself typing ollama run and then... blanking on the exact model name you downloaded? Or constantly breaking your terminal flow to run ollama ps just to see your list of local models?&lt;/p&gt; &lt;p&gt;Yeah, me too. That's why I created &lt;strong&gt;Sherpa&lt;/strong&gt; (I have to name everything, sorry): a tiny Bash plugin that adds &lt;strong&gt;autocompletion&lt;/strong&gt; for Ollama commands and, more importantly, your locally installed model names!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What does Sherpa autocompletes?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Ollama commands:&lt;/strong&gt; Type &lt;code&gt;ollama&lt;/code&gt; and hit &lt;em&gt;Tab&lt;/em&gt; to see available commands like &lt;code&gt;run&lt;/code&gt;, &lt;code&gt;rm&lt;/code&gt;, &lt;code&gt;show&lt;/code&gt;, &lt;code&gt;create&lt;/code&gt;, &lt;code&gt;stop&lt;/code&gt;, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Your LOCAL model names:&lt;/strong&gt; When you type &lt;code&gt;ollama run&lt;/code&gt;, &lt;code&gt;ollama rm&lt;/code&gt; or &lt;code&gt;ollama show&lt;/code&gt;, hitting &lt;code&gt;Tab&lt;/code&gt; will show you &lt;strong&gt;a list of the models you actually have downloaded&lt;/strong&gt;. No more guesswork or copy-pasting!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RUNNING models to stop&lt;/strong&gt;: The best part! A model is slowing your entire machine and you didn't remember the exact quantization. No problem, type &lt;code&gt;ollama stop&lt;/code&gt; and select the running model &lt;strong&gt;tabbing&lt;/strong&gt;. Done, no more pain.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Modelfiles:&lt;/strong&gt; Helps find your Modelfile paths when using &lt;code&gt;ollama create&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Check the repo!&lt;/strong&gt; &lt;a href="https://github.com/ehrlz/ollama-bash-completion-plugin"&gt;https://github.com/ehrlz/ollama-bash-completion-plugin&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Save time and stay in the Unix &amp;quot;tab flow&amp;quot;.&lt;/strong&gt; Let &lt;code&gt;Tab&lt;/code&gt; do the heavy lifting!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ehrlz"&gt; /u/ehrlz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k9h0cv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k9h0cv/ollama_bash_completions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k9h0cv/ollama_bash_completions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-27T23:00:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1k9nwwu</id>
    <title>How can i make Dolpin3 learn to have a personality</title>
    <updated>2025-04-28T05:17:17+00:00</updated>
    <author>
      <name>/u/claushill777</name>
      <uri>https://old.reddit.com/user/claushill777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OK i installed Dolpin3, i got AnythingLLM, im new to this, i tryed to teach it how to respond and what is my anme and his name but he forgets, how can i seed this information in him ? any easy way ? i saw in option menu, chat setting that there is a prompt window, how can i use it ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/claushill777"&gt; /u/claushill777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k9nwwu/how_can_i_make_dolpin3_learn_to_have_a_personality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k9nwwu/how_can_i_make_dolpin3_learn_to_have_a_personality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k9nwwu/how_can_i_make_dolpin3_learn_to_have_a_personality/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-28T05:17:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1k97v6f</id>
    <title>Open-source Granola with Ollama support</title>
    <updated>2025-04-27T16:22:48+00:00</updated>
    <author>
      <name>/u/beerbellyman4vr</name>
      <uri>https://old.reddit.com/user/beerbellyman4vr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k97v6f/opensource_granola_with_ollama_support/"&gt; &lt;img alt="Open-source Granola with Ollama support" src="https://external-preview.redd.it/MHo5ejJpNDVsZXhlMWaRkUleLtaN4pxDUmVIQYRN1Wk-W-MvDQhEw6jc8qAM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=42236ed58f0bd683e27670f6fdb0f768a38a0caf" title="Open-source Granola with Ollama support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently open-sourced my project Hyprnote; &lt;strong&gt;&lt;em&gt;a smart AI notepad designed for people in back-to-back meetings&lt;/em&gt;&lt;/strong&gt;. Hyprnote is an open source alternative for Granola AI.&lt;/p&gt; &lt;p&gt;Hyprnote uses the computer's system audio and microphone, so you don't need to add any bots to your meetings.&lt;/p&gt; &lt;p&gt;Try it for free, forever.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/fastrepl/hyprnote"&gt;https://github.com/fastrepl/hyprnote&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beerbellyman4vr"&gt; /u/beerbellyman4vr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hbha9i45lexe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k97v6f/opensource_granola_with_ollama_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k97v6f/opensource_granola_with_ollama_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-27T16:22:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ka26nk</id>
    <title>Janitor.ai + Deepseek has the right flavor of character RP for me. How do I go about tweaking my offline experience to mimic that type of chatbot?</title>
    <updated>2025-04-28T18:03:42+00:00</updated>
    <author>
      <name>/u/BigHeavySlowThing</name>
      <uri>https://old.reddit.com/user/BigHeavySlowThing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm coming from Janitor AI, which I'm using Openrouter to proxy in an instance of &amp;quot;Deepseek V3 0324 (free)&amp;quot;.&lt;/p&gt; &lt;p&gt;I'm still a noob at local llms, but I have followed a couple of tutorials and got the following technically working:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ollama&lt;/li&gt; &lt;li&gt;Chatbox AI&lt;/li&gt; &lt;li&gt;deepseek-r1:14b&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;My Ollama + Chatbox setup seems to work quite well, but it doesn't seem to strictly adhere to my system prompts. For example, I explicitly tell it to respond &lt;strong&gt;&lt;em&gt;only&lt;/em&gt;&lt;/strong&gt; for the AI character, but it won't stop responding for the both of us.&lt;/p&gt; &lt;p&gt;I can't tell if this is a limitation of the model I'm using, or if I've failed to set something up somewhere. Or, if my formatting is just incorrect.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;I'm happy to change tools&lt;/em&gt;&lt;/strong&gt; (if an existing tutorial suggests something other than Ollama and/or Chatbox). But, super eager to mimic my JAI experience offline if any of you can point me in the right direction.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;If it matters, here's my system specs (in case that helps point to a specific optimal model):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU: 9800X3D&lt;/li&gt; &lt;li&gt;RAM: 64GB&lt;/li&gt; &lt;li&gt;GPU: 4080 Super (16gb)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BigHeavySlowThing"&gt; /u/BigHeavySlowThing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ka26nk/janitorai_deepseek_has_the_right_flavor_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ka26nk/janitorai_deepseek_has_the_right_flavor_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ka26nk/janitorai_deepseek_has_the_right_flavor_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-28T18:03:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1k9zgm7</id>
    <title>Introducing CleverChatty – An AI Assistant Package for Go</title>
    <updated>2025-04-28T16:13:26+00:00</updated>
    <author>
      <name>/u/gelembjuk</name>
      <uri>https://old.reddit.com/user/gelembjuk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm excited to introduce a new package for Go developers: &lt;a href="https://github.com/Gelembjuk/cleverchatty"&gt;&lt;strong&gt;CleverChatty&lt;/strong&gt;&lt;/a&gt;.&lt;br /&gt; &lt;strong&gt;CleverChatty&lt;/strong&gt; implements the core functionality of an AI chat system. It encapsulates the essential business logic required for building AI-powered assistants or chatbots — all while remaining independent of any specific user interface (UI).&lt;/p&gt; &lt;p&gt;In short, &lt;strong&gt;CleverChatty&lt;/strong&gt; is a fully working AI chat backend — just without a graphical UI. It supports many popular LLM providers, including OpenAI, Claude, Ollama, and others. It also integrates with external tools using the &lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://gelembjuk.hashnode.dev/introducing-cleverchatty-an-ai-assistant-package-for-go"&gt;https://gelembjuk.hashnode.dev/introducing-cleverchatty-an-ai-assistant-package-for-go&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Roadmap for CleverChatty&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Upcoming features include:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;AI Assistant Memory via MCP&lt;/strong&gt;: Introducing persistent, modular, vendor-agnostic memory for AI chats using an external MCP server.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full Support for Updated MCP&lt;/strong&gt;: Implementing new MCP features, HTTP Streaming transport, and OAuth2 authentication.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A2A Protocol Support&lt;/strong&gt;: Adding the A2A protocol for more efficient AI assistant integration.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The ultimate goal is to make CleverChatty a full-featured, easily embeddable AI chat system.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gelembjuk"&gt; /u/gelembjuk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k9zgm7/introducing_cleverchatty_an_ai_assistant_package/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k9zgm7/introducing_cleverchatty_an_ai_assistant_package/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k9zgm7/introducing_cleverchatty_an_ai_assistant_package/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-28T16:13:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kadwr3</id>
    <title>Dynamic Multi-Function Calling Locally with Gemma 3 + Ollama – Full Demo Walkthrough</title>
    <updated>2025-04-29T02:57:37+00:00</updated>
    <author>
      <name>/u/srireddit2020</name>
      <uri>https://old.reddit.com/user/srireddit2020</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kadwr3/dynamic_multifunction_calling_locally_with_gemma/"&gt; &lt;img alt="Dynamic Multi-Function Calling Locally with Gemma 3 + Ollama – Full Demo Walkthrough" src="https://b.thumbs.redditmedia.com/fiILQuAL42v-bRINrhQFdrRhZpV4zPvH-wAfn-Fj_Wc.jpg" title="Dynamic Multi-Function Calling Locally with Gemma 3 + Ollama – Full Demo Walkthrough" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! 👋&lt;/p&gt; &lt;p&gt;I recently worked on &lt;strong&gt;dynamic function calling&lt;/strong&gt; using &lt;strong&gt;Gemma 3 (1B)&lt;/strong&gt; running &lt;strong&gt;locally&lt;/strong&gt; via &lt;strong&gt;Ollama&lt;/strong&gt; — allowing the LLM to &lt;strong&gt;trigger real-time Search, Translation, and Weather retrieval&lt;/strong&gt; dynamically based on user input.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo Video:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1kadwr3/video/7wansdahvoxe1/player"&gt;https://reddit.com/link/1kadwr3/video/7wansdahvoxe1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dynamic Function Calling Flow Diagram :&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/v07jd81ivoxe1.png?width=959&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c1ad77ddf9ca5731046a3c0e18de300adf07ec4"&gt;https://preview.redd.it/v07jd81ivoxe1.png?width=959&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c1ad77ddf9ca5731046a3c0e18de300adf07ec4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Instead of only answering from memory, the model smartly decides when to:&lt;/p&gt; &lt;p&gt;🔍 Perform a &lt;strong&gt;Google Searc&lt;/strong&gt;h (using &lt;a href="http://serper.dev/"&gt;Serper.dev&lt;/a&gt; API)&lt;br /&gt; 🌐 &lt;strong&gt;Translate tex&lt;/strong&gt;t live (using MyMemory API)&lt;br /&gt; ⛅ &lt;strong&gt;Fetch weather&lt;/strong&gt; in real-time (using OpenWeatherMap API)&lt;br /&gt; 🧠 &lt;strong&gt;Answer directl&lt;/strong&gt;y if internal memory is sufficient&lt;/p&gt; &lt;p&gt;This showcases how &lt;strong&gt;structured function calling&lt;/strong&gt; can make local LLMs smarter and much more flexible!&lt;/p&gt; &lt;p&gt;💡 &lt;strong&gt;Key Highlights&lt;/strong&gt;:&lt;br /&gt; ✅ JSON-structured function calls for safe external tool invocation&lt;br /&gt; ✅ Local-first architecture — no cloud LLM inference&lt;br /&gt; ✅ Ollama + Gemma 3 1B combo works great even on modest hardware&lt;br /&gt; ✅ Fully modular — easy to plug in more tools beyond search, translate, weather&lt;/p&gt; &lt;p&gt;🛠 &lt;strong&gt;Tech Stack&lt;/strong&gt;:&lt;br /&gt; ⚡ &lt;a href="https://ollama.com/library/gemma3:1b"&gt;Gemma 3 (1B)&lt;/a&gt; via &lt;strong&gt;Ollama&lt;/strong&gt;&lt;br /&gt; ⚡ &lt;strong&gt;Gradio&lt;/strong&gt; (Chatbot Frontend)&lt;br /&gt; ⚡ &lt;a href="http://serper.dev/"&gt;&lt;strong&gt;Serper.dev&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;API&lt;/strong&gt; (Search)&lt;br /&gt; ⚡ &lt;strong&gt;MyMemory API&lt;/strong&gt; (Translation)&lt;br /&gt; ⚡ &lt;strong&gt;OpenWeatherMap API&lt;/strong&gt; (Weather)&lt;br /&gt; ⚡ &lt;strong&gt;Pydantic + Python&lt;/strong&gt; (Function parsing &amp;amp; validation)&lt;/p&gt; &lt;p&gt;📌 &lt;strong&gt;Full blog + complete code walkthroug&lt;/strong&gt;h: &lt;a href="https://sridhartech.hashnode.dev/dynamic-multi-function-calling-locally-with-gemma-3-and-ollama"&gt;sridhartech.hashnode.dev/dynamic-multi-function-calling-locally-with-gemma-3-and-ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/srireddit2020"&gt; /u/srireddit2020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kadwr3/dynamic_multifunction_calling_locally_with_gemma/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kadwr3/dynamic_multifunction_calling_locally_with_gemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kadwr3/dynamic_multifunction_calling_locally_with_gemma/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-29T02:57:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ka8s9s</id>
    <title>How to disable thinking with Qwen3?</title>
    <updated>2025-04-28T22:41:05+00:00</updated>
    <author>
      <name>/u/No-Refrigerator-1672</name>
      <uri>https://old.reddit.com/user/No-Refrigerator-1672</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, today Qwen team dropped their new Qwen3 model, &lt;a href="https://ollama.com/library/qwen3"&gt;with official Ollama support&lt;/a&gt;. However, there is one crucial detail missing: Qwen3 is a model which supports switching thinking on/off. Thinking really messes up stuff like caption generation in OpenWebUI, so I would want to have a second copy of Qwen3 with disabled thinking. Does anybody knows how to achieve that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Refrigerator-1672"&gt; /u/No-Refrigerator-1672 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ka8s9s/how_to_disable_thinking_with_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ka8s9s/how_to_disable_thinking_with_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ka8s9s/how_to_disable_thinking_with_qwen3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-28T22:41:05+00:00</published>
  </entry>
</feed>
