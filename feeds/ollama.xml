<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-12T08:41:53+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mmnptg</id>
    <title>How to run multiple versions of same model?</title>
    <updated>2025-08-10T16:57:44+00:00</updated>
    <author>
      <name>/u/petr_bena</name>
      <uri>https://old.reddit.com/user/petr_bena</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Right now ollama always works only with latest version of a model, say Mistral:7b&lt;/p&gt; &lt;p&gt;These models get periodic updates. What if I wanted to retain version from 2024 and 2025 and be able to switch between them? Does ollama supports something like version tagging and maintaining multiple versions of same model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/petr_bena"&gt; /u/petr_bena &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmnptg/how_to_run_multiple_versions_of_same_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmnptg/how_to_run_multiple_versions_of_same_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmnptg/how_to_run_multiple_versions_of_same_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T16:57:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmfhfa</id>
    <title>Fastest and best model for my really low spec hardware</title>
    <updated>2025-08-10T10:50:01+00:00</updated>
    <author>
      <name>/u/PurpleUser0000</name>
      <uri>https://old.reddit.com/user/PurpleUser0000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an I5-4th gen, ddr3 8GB ram 1600hz , no GPU ( IGPU ) &lt;/p&gt; &lt;p&gt;What's the best model I can go with here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PurpleUser0000"&gt; /u/PurpleUser0000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmfhfa/fastest_and_best_model_for_my_really_low_spec/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmfhfa/fastest_and_best_model_for_my_really_low_spec/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmfhfa/fastest_and_best_model_for_my_really_low_spec/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T10:50:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmtawi</id>
    <title>Noob here. Please Help me find the perfect model.</title>
    <updated>2025-08-10T20:32:41+00:00</updated>
    <author>
      <name>/u/Many-Kaleidoscope-72</name>
      <uri>https://old.reddit.com/user/Many-Kaleidoscope-72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone. I’m new to LLMs and ollama. But I’m interested and hyped. I am a programmer (IT technician) and I started dealing with local LLMs and vision based ais like stable diffusion. I invested into a Rog Zephyrus G16 2025 model with 64GB RAM, 5070ti (12GB), Core Ultra 9 285H model. As far as I know, this hardware lets me do light or medium AI work locally. I plan to learn AI development later on University. I will just start my first semester. That’s why I choose these specs. I have a 4070ti, 32GB RAM, i7 13700K desktop pc too. Obviously that’s faster but lacks immense VRAM for large models. I could remote onto the desktop from a weaker laptop, but I’d rather be able to do light / medium work locally, than to always remote onto the home PC.&lt;/p&gt; &lt;p&gt;Question is, what’s the absolute best AI model for text + vision, that can speak English and Hungarian (my native language) well. I know that there are runtime parameters for both ollama and the models, so maybe a bigger model is runable with the right tweaks? Currently gemma3 12B runs fast, knows Hungarian well. 27B runs very poorly. Even when changing model settings (I might set it up incorrectly).&lt;/p&gt; &lt;p&gt;So what are your recommendations, guys? Let me know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Many-Kaleidoscope-72"&gt; /u/Many-Kaleidoscope-72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmtawi/noob_here_please_help_me_find_the_perfect_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmtawi/noob_here_please_help_me_find_the_perfect_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmtawi/noob_here_please_help_me_find_the_perfect_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T20:32:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmr2n1</id>
    <title>is we can run gpt oss in 16gb vram ? why mine is offload to cpu im running in docker in ubuntu 22.04</title>
    <updated>2025-08-10T19:06:17+00:00</updated>
    <author>
      <name>/u/actuallytech</name>
      <uri>https://old.reddit.com/user/actuallytech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mmr2n1/is_we_can_run_gpt_oss_in_16gb_vram_why_mine_is/"&gt; &lt;img alt="is we can run gpt oss in 16gb vram ? why mine is offload to cpu im running in docker in ubuntu 22.04" src="https://a.thumbs.redditmedia.com/T8e2323zafFhBWk7siLTxyY4iplSS1jgitzzPLUcEX8.jpg" title="is we can run gpt oss in 16gb vram ? why mine is offload to cpu im running in docker in ubuntu 22.04" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;gpu used is rtx 5060ti 16gb vram&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/actuallytech"&gt; /u/actuallytech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mmr2n1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmr2n1/is_we_can_run_gpt_oss_in_16gb_vram_why_mine_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmr2n1/is_we_can_run_gpt_oss_in_16gb_vram_why_mine_is/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T19:06:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmj279</id>
    <title>The first time I heard a sound coming from my MacBook was while using GPT-OSS 20B</title>
    <updated>2025-08-10T13:49:09+00:00</updated>
    <author>
      <name>/u/anakedsuperman</name>
      <uri>https://old.reddit.com/user/anakedsuperman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mmj279/the_first_time_i_heard_a_sound_coming_from_my/"&gt; &lt;img alt="The first time I heard a sound coming from my MacBook was while using GPT-OSS 20B" src="https://external-preview.redd.it/ZThhMDNvMmE1N2lmMTSXhaJcSNaMLiLEA491TTFq3lE-Ha0mGK07Lje4LN3h.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7372d0bd82781bd6388dff2cabe3eca39528ceca" title="The first time I heard a sound coming from my MacBook was while using GPT-OSS 20B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running GPT-OSS 20B on my MacBook M4 Max with 36GB RAM. I don't hear anything from other models, though, even with Devtra 140B.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anakedsuperman"&gt; /u/anakedsuperman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vyk9un2a57if1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmj279/the_first_time_i_heard_a_sound_coming_from_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmj279/the_first_time_i_heard_a_sound_coming_from_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T13:49:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn74d7</id>
    <title>Help for creating llm</title>
    <updated>2025-08-11T08:06:54+00:00</updated>
    <author>
      <name>/u/matin1099</name>
      <uri>https://old.reddit.com/user/matin1099</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matin1099"&gt; /u/matin1099 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LLMDevs/comments/1mn736g/help_for_creating_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn74d7/help_for_creating_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mn74d7/help_for_creating_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T08:06:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm4ibk</id>
    <title>I ran OpenAI’s GPT-OSS 20B locally on a 16GB Mac with Ollama — setup, gotchas, and mini demo</title>
    <updated>2025-08-10T00:19:29+00:00</updated>
    <author>
      <name>/u/Spirited-Wind6803</name>
      <uri>https://old.reddit.com/user/Spirited-Wind6803</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mm4ibk/i_ran_openais_gptoss_20b_locally_on_a_16gb_mac/"&gt; &lt;img alt="I ran OpenAI’s GPT-OSS 20B locally on a 16GB Mac with Ollama — setup, gotchas, and mini demo" src="https://external-preview.redd.it/MHQ5M3V2aXQ0M2lmMf_ZwYHO2m1fMNCQy9M-9mV9J_Z510ikdbK6GDGwXk75.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=678ce1f3ab9b2ba28aeffdad56e26d3d77e4258f" title="I ran OpenAI’s GPT-OSS 20B locally on a 16GB Mac with Ollama — setup, gotchas, and mini demo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all — I’ve been testing &lt;strong&gt;GPT-OSS 20B&lt;/strong&gt; locally using &lt;strong&gt;Ollama&lt;/strong&gt; and wanted to share a clean setup path, what worked, what didn’t, and a tiny QA demo. &lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Yes, 20B runs on a 16GB Mac&lt;/strong&gt; with Ollama. Do I have the patience? No, it took toooo long&lt;/li&gt; &lt;li&gt;Should you use 16GB to perform any other tasks such as coding, agent, RAG? No, not worth it - upgrade to 32GB maybe..maybe will give you more. I tried on A100 GPU and still did not meet my expectation &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spirited-Wind6803"&gt; /u/Spirited-Wind6803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qlifjvit43if1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm4ibk/i_ran_openais_gptoss_20b_locally_on_a_16gb_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm4ibk/i_ran_openais_gptoss_20b_locally_on_a_16gb_mac/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T00:19:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmrqgh</id>
    <title>How do I get vision models working in Ollama/LM Studio?</title>
    <updated>2025-08-10T19:31:37+00:00</updated>
    <author>
      <name>/u/avdsrj</name>
      <uri>https://old.reddit.com/user/avdsrj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I've been messing around with Ollama and LM Studio to run LLMs locally, and I'm hitting a wall with vision models.&lt;/p&gt; &lt;p&gt;So here's the deal - I know vision models need these &amp;quot;mmproj&amp;quot; files to actually see pictures, and everything works fine when I grab models straight from Ollama or LM Studio's repos. But the moment I try to use GGUF models from somewhere else (like Hugging Face), I'm completely lost on how to get the mmproj stuff working.&lt;/p&gt; &lt;p&gt;I've been googling this for way too long and honestly can't find a clear answer anywhere. It feels like there's some obvious step I'm missing.&lt;/p&gt; &lt;p&gt;Has anyone figured out how to manually add mmproj files to models? Like, is there a specific way to structure the Modelfile or some command I'm not seeing?&lt;/p&gt; &lt;p&gt;Would really appreciate if someone could point me in the right direction - this is driving me crazy!&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/avdsrj"&gt; /u/avdsrj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmrqgh/how_do_i_get_vision_models_working_in_ollamalm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmrqgh/how_do_i_get_vision_models_working_in_ollamalm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmrqgh/how_do_i_get_vision_models_working_in_ollamalm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T19:31:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn8jot</id>
    <title>Coding model for 4080S + 32Gb RAM</title>
    <updated>2025-08-11T09:41:01+00:00</updated>
    <author>
      <name>/u/tresslessone</name>
      <uri>https://old.reddit.com/user/tresslessone</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Title says most of it - I’m looking for some coding model recommendations for my 4080S (16Gb VRAM) + 32Gb RAM (7800X3D) gaming PC.&lt;/p&gt; &lt;p&gt;I’m currently running QWEN3-coder 30b (Q4_K_XL) and whilst it runs, it’s pretty slow (especially once the context fills up) and I’d like something a bit snappier. &lt;/p&gt; &lt;p&gt;Is there a 14b version of QWEN3-coder out there perhaps that I can’t seem to find?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tresslessone"&gt; /u/tresslessone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn8jot/coding_model_for_4080s_32gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn8jot/coding_model_for_4080s_32gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mn8jot/coding_model_for_4080s_32gb_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T09:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmwz6c</id>
    <title>People with MacBook Pro with 36gb of memory, which models you are running for coding?</title>
    <updated>2025-08-10T23:06:39+00:00</updated>
    <author>
      <name>/u/Sea-Emu2600</name>
      <uri>https://old.reddit.com/user/Sea-Emu2600</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have m3 max 36gb of memory but kinda new to ollama so not sure which model use for coding. Also you are using what as a front-end? Vscode Copilot, cline?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Emu2600"&gt; /u/Sea-Emu2600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmwz6c/people_with_macbook_pro_with_36gb_of_memory_which/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmwz6c/people_with_macbook_pro_with_36gb_of_memory_which/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmwz6c/people_with_macbook_pro_with_36gb_of_memory_which/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T23:06:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmu24w</id>
    <title>What is the Best coding LLM for my system?</title>
    <updated>2025-08-10T21:02:54+00:00</updated>
    <author>
      <name>/u/tarsonis125</name>
      <uri>https://old.reddit.com/user/tarsonis125</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;rtx 3090 24vram + 96gig ram&lt;/p&gt; &lt;p&gt;What is the best local LLM to use on my system?&lt;br /&gt; Do some models do better then other at some tasks?&lt;br /&gt; I am trying out a bunch of them, but its hard for me to properly rate them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarsonis125"&gt; /u/tarsonis125 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmu24w/what_is_the_best_coding_llm_for_my_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmu24w/what_is_the_best_coding_llm_for_my_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmu24w/what_is_the_best_coding_llm_for_my_system/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T21:02:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn1v6v</id>
    <title>Integrated Mistral AI into a vehicle</title>
    <updated>2025-08-11T02:58:38+00:00</updated>
    <author>
      <name>/u/SoftDuckling1</name>
      <uri>https://old.reddit.com/user/SoftDuckling1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently working on building an AI to install into my vehicle as an assistant, NOT a driver. The main purpose is to have an assistant that can tell me if something is wrong with the vehicle, provide casual chat on long drives, provide directions (if i add offline GPS/maps), and make driving overall more enjoyable, all while keeping my safety a top priority. I'm using Mistral AI (local) and I have a RAG memory script somewhat working, still working out some kinks. It's using PIPER as a TTS, and im currently working on STT. I eventually want to try to integrate a dual facing dash cam with facial recognition and record drives with loop recording, like a normal dash cam. It will have access to my vehicles speakers, mic, radio display, and possibly OBD-II. That last part still makes me worry, which is why I'm looking for any advice. I already gave it explicit directives and prompts telling it to never alter anything, only read data, but I still don't fully trust it. I'll be adding failsafes before i install it. I plan to install this AI onto a Jetson Orin Nano Super DEV. (at least that's what I plan on right now, might change later). I'll give it a 1-2TB extreme SD to store &amp;quot;.json&amp;quot; memories that it will be able to use as an index library, retrieving only relative information. Then it will be installed into the glove box. I will renovate the glove box with a vibration-proof housings, heat sink, proper cooling, ceramic heaters for colder months, air filters, and anything else I can think of.&lt;/p&gt; &lt;p&gt;So any thoughts? Fully offline AI as a vehicle companion, an okay idea or a ticking bomb? I'll gladly accept any advice about this project. And again, it WILL NOT be controlling any part of the vehicle, only reading data while providing conversation and info.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SoftDuckling1"&gt; /u/SoftDuckling1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn1v6v/integrated_mistral_ai_into_a_vehicle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn1v6v/integrated_mistral_ai_into_a_vehicle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mn1v6v/integrated_mistral_ai_into_a_vehicle/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T02:58:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnt14e</id>
    <title>dose any one use hostinger</title>
    <updated>2025-08-11T23:40:32+00:00</updated>
    <author>
      <name>/u/wbiggs205</name>
      <uri>https://old.reddit.com/user/wbiggs205</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to know dose any one use hostinger on there VPS to run ollama. And What do you think ? I was thing about using them .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wbiggs205"&gt; /u/wbiggs205 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnt14e/dose_any_one_use_hostinger/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnt14e/dose_any_one_use_hostinger/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mnt14e/dose_any_one_use_hostinger/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T23:40:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnkd5t</id>
    <title>Trouble connecting ollama to docker containers</title>
    <updated>2025-08-11T18:04:01+00:00</updated>
    <author>
      <name>/u/The1TrueSteb</name>
      <uri>https://old.reddit.com/user/The1TrueSteb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, &lt;/p&gt; &lt;p&gt;I am trying to integrate ollama with some self hosted services. I have tried karakeep, linkwarden, and paperless-ai and have no success. Open webui works no problem with no setup. &lt;/p&gt; &lt;p&gt;I must be missing something, I do not understand why these services can't connect to ollama. I have tried setting up ollama on host and in a docker container, but still no luck.&lt;/p&gt; &lt;p&gt;Can anyone point me in the right direction? I have read the documentation for all these services and it just seems like it should just connect with no extra setup?&lt;/p&gt; &lt;p&gt;Any help appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The1TrueSteb"&gt; /u/The1TrueSteb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnkd5t/trouble_connecting_ollama_to_docker_containers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnkd5t/trouble_connecting_ollama_to_docker_containers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mnkd5t/trouble_connecting_ollama_to_docker_containers/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T18:04:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnyfdt</id>
    <title>Is Ollama Stealing Llama.cpp’s Work?</title>
    <updated>2025-08-12T03:50:30+00:00</updated>
    <author>
      <name>/u/Lopsided_Dot_4557</name>
      <uri>https://old.reddit.com/user/Lopsided_Dot_4557</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mnyfdt/is_ollama_stealing_llamacpps_work/"&gt; &lt;img alt="Is Ollama Stealing Llama.cpp’s Work?" src="https://external-preview.redd.it/7zXd5apTO5JtMIq4DoKmbJARiKH9JIn8TeQUfPr8bU8.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2be54c033ac02141742fb21a2b8677f69aab193d" title="Is Ollama Stealing Llama.cpp’s Work?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided_Dot_4557"&gt; /u/Lopsided_Dot_4557 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/9QBgY40wos8?si=6rYGGwN0xNHTRN5q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnyfdt/is_ollama_stealing_llamacpps_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mnyfdt/is_ollama_stealing_llamacpps_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-12T03:50:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn5uvk</id>
    <title>devstral:24b</title>
    <updated>2025-08-11T06:44:42+00:00</updated>
    <author>
      <name>/u/barrulus</name>
      <uri>https://old.reddit.com/user/barrulus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone tried to use devstral:24b for any coding work?&lt;/p&gt; &lt;p&gt;I am interested to know what kind of work I could pass to this model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/barrulus"&gt; /u/barrulus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn5uvk/devstral24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn5uvk/devstral24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mn5uvk/devstral24b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T06:44:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mncy6l</id>
    <title>Could you add a button to upload files or images directly for multimodal LLMs in Ollama?</title>
    <updated>2025-08-11T13:26:43+00:00</updated>
    <author>
      <name>/u/thestreamcode</name>
      <uri>https://old.reddit.com/user/thestreamcode</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thestreamcode"&gt; /u/thestreamcode &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mncy6l/could_you_add_a_button_to_upload_files_or_images/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mncy6l/could_you_add_a_button_to_upload_files_or_images/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mncy6l/could_you_add_a_button_to_upload_files_or_images/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T13:26:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnomo3</id>
    <title>Is it possible to collect ollama metrics from Prometheus?</title>
    <updated>2025-08-11T20:44:33+00:00</updated>
    <author>
      <name>/u/volavi</name>
      <uri>https://old.reddit.com/user/volavi</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/volavi"&gt; /u/volavi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnomo3/is_it_possible_to_collect_ollama_metrics_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnomo3/is_it_possible_to_collect_ollama_metrics_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mnomo3/is_it_possible_to_collect_ollama_metrics_from/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T20:44:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mndl7n</id>
    <title>Intel NPU and Ollama</title>
    <updated>2025-08-11T13:53:09+00:00</updated>
    <author>
      <name>/u/Many-Kaleidoscope-72</name>
      <uri>https://old.reddit.com/user/Many-Kaleidoscope-72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys. I've got an Asus ROG Zephyrus G16 2025 with 5070ti, Core Ultra 9 285H. The Core Ultra has a NPU. No software uses it. Is it possible to load Ollama and the models onto the NPU? What would perform better? NPU or 5070ti? Or NPU works in tandem with CPU? If so, CPU+NPU vs GPU? Or is it possible to use the CPU+NPU+GPU At the same time?&lt;/p&gt; &lt;p&gt;Can anyone help me out how to use the NPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Many-Kaleidoscope-72"&gt; /u/Many-Kaleidoscope-72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mndl7n/intel_npu_and_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mndl7n/intel_npu_and_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mndl7n/intel_npu_and_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T13:53:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnnew1</id>
    <title>What is the best Scientific model</title>
    <updated>2025-08-11T19:59:06+00:00</updated>
    <author>
      <name>/u/moric7</name>
      <uri>https://old.reddit.com/user/moric7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For science questions and projects, physics and math. On 12GB VRAM and 64GB RAM. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moric7"&gt; /u/moric7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnnew1/what_is_the_best_scientific_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnnew1/what_is_the_best_scientific_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mnnew1/what_is_the_best_scientific_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T19:59:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mntmz9</id>
    <title>Can ollama do multi-step RAGs?</title>
    <updated>2025-08-12T00:06:54+00:00</updated>
    <author>
      <name>/u/StraightAd8315</name>
      <uri>https://old.reddit.com/user/StraightAd8315</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to develop a RAG that basically does a dynamic questionnaire with the user, and sometimes the question it would ask would change depending on the previous answer (IE YES/NO; Select List).&lt;/p&gt; &lt;p&gt;Can Ollama do that at,? I'm new to using it to try to build a RAG and feeling so lost on how to utilize this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StraightAd8315"&gt; /u/StraightAd8315 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mntmz9/can_ollama_do_multistep_rags/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mntmz9/can_ollama_do_multistep_rags/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mntmz9/can_ollama_do_multistep_rags/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-12T00:06:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn653l</id>
    <title>PSA: Secure Your Ollama / LLM Ports ( Even on Home LAN )</title>
    <updated>2025-08-11T07:02:47+00:00</updated>
    <author>
      <name>/u/Immediate_Fun4182</name>
      <uri>https://old.reddit.com/user/Immediate_Fun4182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I do not know where to begin. Ok here we go.&lt;/p&gt; &lt;p&gt;Recently someone on &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; &lt;a href="https://www.reddit.com/r/ollama/comments/1k6m1b3/someone_found_my_open_ai_server_and_used_it_to/"&gt;posted&lt;/a&gt; that their Ollama API had been exposed to the public internet for over a month, and strangers used it to process massive amounts of personal data.&lt;/p&gt; &lt;p&gt;I am posting this because Ollama is a beginner friendly platform and not many people do realize that Ollama and similar LLM servers like vLLM bind to &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; by default. Meaning &lt;strong&gt;any network interface&lt;/strong&gt; can accept connections (lan wifi and even an if your router is forwarding). No authentication or rate limiting exists on default. If your router has UPnP or manual port forwarding enabled, the API can be fully exposed to the internet without you realizing it.&lt;/p&gt; &lt;p&gt;Best practices I can recommend for beginners is this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Bind only to Tailscale or localhost and/or bind to tailscale0 only so it is only reachable inside your Tailnet.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Close risky ports by default:&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Use usw or firewalld to block everything except what you explicitly allow:&lt;/p&gt; &lt;p&gt;```&lt;br /&gt; sudo ufw default deny incoming&lt;/p&gt; &lt;p&gt;sudo ufw default allow outgoing&lt;/p&gt; &lt;p&gt;sudo ufw allow from &lt;a href="http://100.64.0.0/10"&gt;100.64.0.0/10&lt;/a&gt; # Tailscale subnet&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;and lastly&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Watch your exposed services on ports:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;sudo sof -i -P -n | grep LISTEN&lt;br /&gt; ```&lt;/p&gt; &lt;p&gt;F.e. I was exposing my 11434 port to public IP which is a terrible mistake. make sure you only open through vpn/tailscale set up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Immediate_Fun4182"&gt; /u/Immediate_Fun4182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn653l/psa_secure_your_ollama_llm_ports_even_on_home_lan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn653l/psa_secure_your_ollama_llm_ports_even_on_home_lan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mn653l/psa_secure_your_ollama_llm_ports_even_on_home_lan/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T07:02:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo2qga</id>
    <title>8x mi60 -- 256GB VRAM Server</title>
    <updated>2025-08-12T08:03:13+00:00</updated>
    <author>
      <name>/u/zekken523</name>
      <uri>https://old.reddit.com/user/zekken523</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zekken523"&gt; /u/zekken523 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mo2lev"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mo2qga/8x_mi60_256gb_vram_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mo2qga/8x_mi60_256gb_vram_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-12T08:03:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnnhmt</id>
    <title>What is the best model for absolutely free uncensored unfiltered chat on any theme</title>
    <updated>2025-08-11T20:01:48+00:00</updated>
    <author>
      <name>/u/moric7</name>
      <uri>https://old.reddit.com/user/moric7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On 12GB VRAM and 64GB RAM. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moric7"&gt; /u/moric7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnnhmt/what_is_the_best_model_for_absolutely_free/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnnhmt/what_is_the_best_model_for_absolutely_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mnnhmt/what_is_the_best_model_for_absolutely_free/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T20:01:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnirls</id>
    <title>Ollama’s copy-paste dev strategy is just PR spin?</title>
    <updated>2025-08-11T17:07:02+00:00</updated>
    <author>
      <name>/u/bllshrfv</name>
      <uri>https://old.reddit.com/user/bllshrfv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mnirls/ollamas_copypaste_dev_strategy_is_just_pr_spin/"&gt; &lt;img alt="Ollama’s copy-paste dev strategy is just PR spin?" src="https://preview.redd.it/g9y4dwqw9fif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9d3d5a417006f4dcba2df0d42db0eb590427a92" title="Ollama’s copy-paste dev strategy is just PR spin?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bllshrfv"&gt; /u/bllshrfv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g9y4dwqw9fif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnirls/ollamas_copypaste_dev_strategy_is_just_pr_spin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mnirls/ollamas_copypaste_dev_strategy_is_just_pr_spin/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T17:07:02+00:00</published>
  </entry>
</feed>
