<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-14T21:24:13+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mp216y</id>
    <title>gptme v0.28.0 major release - agent CLI with local model support</title>
    <updated>2025-08-13T11:38:32+00:00</updated>
    <author>
      <name>/u/ErikBjare</name>
      <uri>https://old.reddit.com/user/ErikBjare</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mp216y/gptme_v0280_major_release_agent_cli_with_local/"&gt; &lt;img alt="gptme v0.28.0 major release - agent CLI with local model support" src="https://external-preview.redd.it/pd8Zgp5hyaUkgDOYatKpFNMomJKv_Ji19N5-m2ttO0s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db81836ce4675ab88cdcc7d4cac45fafb4b8f229" title="gptme v0.28.0 major release - agent CLI with local model support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ErikBjare"&gt; /u/ErikBjare &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/gptme/gptme/releases/tag/v0.28.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mp216y/gptme_v0280_major_release_agent_cli_with_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mp216y/gptme_v0280_major_release_agent_cli_with_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T11:38:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1morfl6</id>
    <title>GPT-OSS 20b runs on a RasPi 5, 16gb</title>
    <updated>2025-08-13T01:38:38+00:00</updated>
    <author>
      <name>/u/RasPiBuilder</name>
      <uri>https://old.reddit.com/user/RasPiBuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got bored and decided to see if GPT-OSS 20b would run on a RasPi 5, 16gb... And it does!&lt;/p&gt; &lt;p&gt;It's slow, hovering just under 1 token per second, so not really usable for conversation.. but could possibly work for some background tasks that aren't time sensitive. (I'll share the verbose output sometime tomorrow.. forgot to turn it on when I ran it).&lt;/p&gt; &lt;p&gt;For those curious, I'm running Ollama headless and bare metal.&lt;/p&gt; &lt;p&gt;And just for the fun of it, this weekend I'm going to set try to setup a little agent and see if I can get it to complete some tasks with Browser Use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RasPiBuilder"&gt; /u/RasPiBuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1morfl6/gptoss_20b_runs_on_a_raspi_5_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1morfl6/gptoss_20b_runs_on_a_raspi_5_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1morfl6/gptoss_20b_runs_on_a_raspi_5_16gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T01:38:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mozna4</id>
    <title>Ollama gui app in v0.11- disabling model auto-pull</title>
    <updated>2025-08-13T09:22:03+00:00</updated>
    <author>
      <name>/u/HashMismatch</name>
      <uri>https://old.reddit.com/user/HashMismatch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Having a look at the gui that v0.11 ships with, and its not bad for a lightweight gui… what bugs me is that in the model dropdown, it will auto install whatever model you select without prompting - which might be convenient for some but I want to set it to not auto-pull models and only show the ones I’ve actually chosen to download. Can’t figure out how to do this. &lt;/p&gt; &lt;p&gt;I asked gpt-oss:20b, which was the default model which self-installed when i first ran a query in the gui and it took me down a rabbithole of setting a config.yaml file - which appears to be a hallucination. You can create the file, sure, but ollama ignores it. Perplexity tells me there is no such config file and no way to configure ollama to do this -which appears to be right. Or is there a way?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HashMismatch"&gt; /u/HashMismatch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mozna4/ollama_gui_app_in_v011_disabling_model_autopull/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mozna4/ollama_gui_app_in_v011_disabling_model_autopull/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mozna4/ollama_gui_app_in_v011_disabling_model_autopull/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T09:22:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpkhih</id>
    <title>Ollama vram and sys ram</title>
    <updated>2025-08-13T23:38:39+00:00</updated>
    <author>
      <name>/u/Squanchy2112</name>
      <uri>https://old.reddit.com/user/Squanchy2112</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a Tesla p40 that means 24gb of vram, I am looking to do something about this but the system also has 80gb of system ram, can I tap into that to allow larger models? Thanks I am still learning. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Squanchy2112"&gt; /u/Squanchy2112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpkhih/ollama_vram_and_sys_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpkhih/ollama_vram_and_sys_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpkhih/ollama_vram_and_sys_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T23:38:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1moy27m</id>
    <title>Finally released the major update I've been working on! LLM Checker now intelligently detects your installed Ollama models and shows you exactly what to run vs what to install</title>
    <updated>2025-08-13T07:38:43+00:00</updated>
    <author>
      <name>/u/pzarevich</name>
      <uri>https://old.reddit.com/user/pzarevich</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1moy27m/finally_released_the_major_update_ive_been/"&gt; &lt;img alt="Finally released the major update I've been working on! LLM Checker now intelligently detects your installed Ollama models and shows you exactly what to run vs what to install" src="https://external-preview.redd.it/cXd0ZzIzY3hwcWlmMT5xSciTw8xlybdYMroPRY8T-TipG3QDyDmXFYor1rLg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=315d01f51218bb9580b8cafb45ac30002d0c57ed" title="Finally released the major update I've been working on! LLM Checker now intelligently detects your installed Ollama models and shows you exactly what to run vs what to install" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt; What's New:&lt;/p&gt; &lt;p&gt; - &lt;strong&gt;--limit flag&lt;/strong&gt;: See top 3, 5, or 10 compatible models instead of just one&lt;/p&gt; &lt;p&gt; - &lt;strong&gt;Smart detection&lt;/strong&gt;: Automatically knows which models you have installed&lt;/p&gt; &lt;p&gt; - &lt;strong&gt;Intelligent Quick Start&lt;/strong&gt;: Shows ollama run for installed models, ollama pull for new ones&lt;/p&gt; &lt;p&gt; - &lt;strong&gt;7 specialized categories&lt;/strong&gt;: coding, creative, reasoning, multimodal, embeddings, talking,&lt;/p&gt; &lt;p&gt; general&lt;/p&gt; &lt;p&gt; - &lt;strong&gt;Real model data&lt;/strong&gt;: 177+ models with actual file sizes from Ollama Hub&lt;/p&gt; &lt;p&gt; - &lt;strong&gt;Hardware-aware filtering&lt;/strong&gt;: No more tiny models on high-end hardware or impossible suggestions&lt;/p&gt; &lt;p&gt;npm: &lt;a href="https://www.npmjs.com/package/llm-checker/v/2.2.0?activeTab=readme"&gt;https://www.npmjs.com/package/llm-checker/v/2.2.0?activeTab=readme&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Pavelevich/llm-checker"&gt;https://github.com/Pavelevich/llm-checker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;*please, help me with test in windows and linux machines &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pzarevich"&gt; /u/pzarevich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fmwja4cxpqif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1moy27m/finally_released_the_major_update_ive_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1moy27m/finally_released_the_major_update_ive_been/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T07:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpmtfo</id>
    <title>CLI agentic team ecosystem</title>
    <updated>2025-08-14T01:22:07+00:00</updated>
    <author>
      <name>/u/Humbrol2</name>
      <uri>https://old.reddit.com/user/Humbrol2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking around, everyone is working on thier own version off a CLI agentic AI team similar to claude code, gemini, etc,, is there a list of the top contenders thta work with ollama anywhere?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Humbrol2"&gt; /u/Humbrol2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpmtfo/cli_agentic_team_ecosystem/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpmtfo/cli_agentic_team_ecosystem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpmtfo/cli_agentic_team_ecosystem/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T01:22:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpt812</id>
    <title>Could you use RAG and Wikidumps to keep AI in the loop?</title>
    <updated>2025-08-14T06:53:15+00:00</updated>
    <author>
      <name>/u/C_S_Student45</name>
      <uri>https://old.reddit.com/user/C_S_Student45</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/C_S_Student45"&gt; /u/C_S_Student45 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/selfhosted/comments/1mpt7tb/could_you_use_rag_and_wikidumps_to_keep_ai_in_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpt812/could_you_use_rag_and_wikidumps_to_keep_ai_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpt812/could_you_use_rag_and_wikidumps_to_keep_ai_in_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T06:53:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqa2ol</id>
    <title>Oss 20B is dumb</title>
    <updated>2025-08-14T19:10:58+00:00</updated>
    <author>
      <name>/u/Playful-Jeweler-1601</name>
      <uri>https://old.reddit.com/user/Playful-Jeweler-1601</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mqa2ol/oss_20b_is_dumb/"&gt; &lt;img alt="Oss 20B is dumb" src="https://a.thumbs.redditmedia.com/a14rRih2rGnby2KkZuBcmcAJln9mMcnpJMAjxt0DoA0.jpg" title="Oss 20B is dumb" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/u81m9u5ra1jf1.png?width=1425&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a36e6423961b45c9e15dd80151ce02bcc7d2d0d"&gt;https://preview.redd.it/u81m9u5ra1jf1.png?width=1425&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a36e6423961b45c9e15dd80151ce02bcc7d2d0d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Playful-Jeweler-1601"&gt; /u/Playful-Jeweler-1601 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqa2ol/oss_20b_is_dumb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqa2ol/oss_20b_is_dumb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mqa2ol/oss_20b_is_dumb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T19:10:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpg9tp</id>
    <title>What are your thoughts on GPT-OSS 120B for programming?</title>
    <updated>2025-08-13T20:51:20+00:00</updated>
    <author>
      <name>/u/ajmusic15</name>
      <uri>https://old.reddit.com/user/ajmusic15</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are your thoughts on GPT-OSS 120B for programming? Specifically, how does it compare to a dense model such as Devstral or a MoE model such as Qwen-Coder 30B?&lt;/p&gt; &lt;p&gt;I am running GPT-OSS 120B on my 96 GB DDR5 + RTX 5080 with MoE weight offloading to the CPU (LM Studio does not allow me to specify how many MoE weights I will send to the CPU) and I am having mixed opinions on coding due to censorship (there are certain pentesting tools that I try to use, but I always run into ethical issues and I don't want to waste time on Advanced Prompting).&lt;/p&gt; &lt;p&gt;But anyway, I'm impressed that once the context is processed (which takes ages), the inference starts running at ~20 tk/s.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajmusic15"&gt; /u/ajmusic15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpg9tp/what_are_your_thoughts_on_gptoss_120b_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpg9tp/what_are_your_thoughts_on_gptoss_120b_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpg9tp/what_are_your_thoughts_on_gptoss_120b_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T20:51:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpqf95</id>
    <title>Pruned GPT-OSS 6.0B kinda works</title>
    <updated>2025-08-14T04:17:01+00:00</updated>
    <author>
      <name>/u/Quiet-Engineer110</name>
      <uri>https://old.reddit.com/user/Quiet-Engineer110</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mpqf95/pruned_gptoss_60b_kinda_works/"&gt; &lt;img alt="Pruned GPT-OSS 6.0B kinda works" src="https://external-preview.redd.it/aaoKLInTgXWvAC3h_YKai0S41TEi4sEQ5dlZR6riJuY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a47d9c1b49d8d7aed9f6e4058ae49360afadc00f" title="Pruned GPT-OSS 6.0B kinda works" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet-Engineer110"&gt; /u/Quiet-Engineer110 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AmanPriyanshu/gpt-oss-6.0b-specialized-all-pruned-moe-only-7-experts"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpqf95/pruned_gptoss_60b_kinda_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpqf95/pruned_gptoss_60b_kinda_works/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T04:17:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpwni3</id>
    <title>Is there a standard oci image format for models?</title>
    <updated>2025-08-14T10:19:35+00:00</updated>
    <author>
      <name>/u/Grouchy-Friend4235</name>
      <uri>https://old.reddit.com/user/Grouchy-Friend4235</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Grouchy-Friend4235"&gt; /u/Grouchy-Friend4235 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1mpwn3f/is_there_a_standard_oci_image_format_for_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpwni3/is_there_a_standard_oci_image_format_for_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpwni3/is_there_a_standard_oci_image_format_for_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T10:19:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpxpwd</id>
    <title>Looking for an ISP in India that allows server hosting (no static IP needed)</title>
    <updated>2025-08-14T11:16:44+00:00</updated>
    <author>
      <name>/u/Formal_Jeweler_488</name>
      <uri>https://old.reddit.com/user/Formal_Jeweler_488</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m currently exploring internet service providers in India that would let me host my own servers from home. I don’t need a static IP at the moment—just a reliable connection that allows inbound traffic and won’t block me from serving content externally.&lt;/p&gt; &lt;p&gt;I’m not looking for anything enterprise-grade, just something solid enough to get my host online and accessible. Preferably something with decent upload speeds and minimal restrictions on port forwarding.&lt;/p&gt; &lt;p&gt;Would love to hear your recommendations on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ISPs that allow this kind of setup&lt;/li&gt; &lt;li&gt;Plans that offer good value for hosting&lt;/li&gt; &lt;li&gt;Any caveats or gotchas I should be aware of&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks in advance for any insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Formal_Jeweler_488"&gt; /u/Formal_Jeweler_488 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpxpwd/looking_for_an_isp_in_india_that_allows_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpxpwd/looking_for_an_isp_in_india_that_allows_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpxpwd/looking_for_an_isp_in_india_that_allows_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T11:16:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpmcpe</id>
    <title>AMD Radeon RX 480 8GB benchmark finally working</title>
    <updated>2025-08-14T01:01:02+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1mpm728/amd_radeon_rx_480_8gb_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpmcpe/amd_radeon_rx_480_8gb_benchmark_finally_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpmcpe/amd_radeon_rx_480_8gb_benchmark_finally_working/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T01:01:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqb2xa</id>
    <title>Trying to buy a house</title>
    <updated>2025-08-14T19:47:17+00:00</updated>
    <author>
      <name>/u/GBT55</name>
      <uri>https://old.reddit.com/user/GBT55</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I’m looking for a house to buy (spanish market 🤮) with the help of chatGPT deep research.&lt;/p&gt; &lt;p&gt;The thing is I am giving very specific parameters to search only the type of houses i’m interested in&lt;/p&gt; &lt;p&gt;It is very good but it has a quota limit so I’m wondering if there’s any other type of model that can scrape a website with very specific parameters and get actual valid urls &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GBT55"&gt; /u/GBT55 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqb2xa/trying_to_buy_a_house/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqb2xa/trying_to_buy_a_house/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mqb2xa/trying_to_buy_a_house/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T19:47:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq2e83</id>
    <title>ollama local model slow</title>
    <updated>2025-08-14T14:34:07+00:00</updated>
    <author>
      <name>/u/Designer_Addendum69</name>
      <uri>https://old.reddit.com/user/Designer_Addendum69</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Designer_Addendum69"&gt; /u/Designer_Addendum69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/CLine/comments/1mq2dzw/ollama_local_model_slow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq2e83/ollama_local_model_slow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mq2e83/ollama_local_model_slow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T14:34:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp00lq</id>
    <title>DataKit + Ollama = Your Data, Your AI, Your Way!</title>
    <updated>2025-08-13T09:45:29+00:00</updated>
    <author>
      <name>/u/Sea-Assignment6371</name>
      <uri>https://old.reddit.com/user/Sea-Assignment6371</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mp00lq/datakit_ollama_your_data_your_ai_your_way/"&gt; &lt;img alt="DataKit + Ollama = Your Data, Your AI, Your Way!" src="https://external-preview.redd.it/MHVraTFpb2RjcmlmMau2qwmbGQWxbzW-5uoWUCNNcejArm0w5kuQ7jVz7rlm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1bc53a8330392c0bda995715e81225dfbd789b81" title="DataKit + Ollama = Your Data, Your AI, Your Way!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt; community! Excited to share that DataKit now has native Ollama integration! Run your favorite local AI models directly in your data workflows. 100% Privacy - Your data NEVER leaves your machine. Zero API Costs - No subscriptions, no surprises. No Rate Limits - Process as much as you want. Full Control - Your infrastructure, your rules.&lt;/p&gt; &lt;p&gt;Install Ollama → &lt;a href="https://ollama.com"&gt;https://ollama.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Run `OLLAMA_ORIGINS=&amp;quot;&lt;a href="https://datakit.page/"&gt;https://datakit.page&lt;/a&gt;&amp;quot; ollama serve`. Jump on Firefox.&lt;/p&gt; &lt;p&gt;Open DataKit → &lt;a href="https://datakit.page"&gt;https://datakit.page&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Start building! - SQL queries + AI, all local&lt;/p&gt; &lt;p&gt;Try it out and let me know what you think! Would love to hear about the workflows you create.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Assignment6371"&gt; /u/Sea-Assignment6371 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/whi92hodcrif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mp00lq/datakit_ollama_your_data_your_ai_your_way/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mp00lq/datakit_ollama_your_data_your_ai_your_way/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T09:45:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpqopt</id>
    <title>Making your prompts better with GEPA-Lite using Ollama!</title>
    <updated>2025-08-14T04:31:22+00:00</updated>
    <author>
      <name>/u/AnyIce3007</name>
      <uri>https://old.reddit.com/user/AnyIce3007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link: &lt;a href="https://github.com/egmaminta/GEPA-Lite"&gt;https://github.com/egmaminta/GEPA-Lite&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;ForTheLoveOfCode&lt;/h1&gt; &lt;p&gt;GEPA-Lite is a lightweight implementation based on the proposed GEPA prompt optimization method that is custom fit for single-task applications. It's built on the core principle of LLM self-reflection, self-improvement, streamlined.&lt;/p&gt; &lt;p&gt;Developed in the spirit of open-source initiatives like Google Summer of Code 2025 and For the Love of Code 2025, this project leverages Gemma (ollama::gemma3n:e4b) as its core model. The project also offers optional support for the Gemini API, allowing access to powerful models like gemini-2.5-flash-lite, gemini-2.5-flash, and gemini-2.5-pro.&lt;/p&gt; &lt;p&gt;Feel free to check it out. I'd also appreciate if you can give a Star ⭐️!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnyIce3007"&gt; /u/AnyIce3007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpqopt/making_your_prompts_better_with_gepalite_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpqopt/making_your_prompts_better_with_gepalite_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpqopt/making_your_prompts_better_with_gepalite_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T04:31:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq5pag</id>
    <title>Ollama but for mobile, with a cloud fallback</title>
    <updated>2025-08-14T16:35:38+00:00</updated>
    <author>
      <name>/u/thecoder12322</name>
      <uri>https://old.reddit.com/user/thecoder12322</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;We’re building something like Ollama, but for mobile. It runs models fully on-device for speed and privacy, and can fall back to the cloud when needed.&lt;/p&gt; &lt;p&gt;I’d love your feedback — especially around how you’re currently using local LLMs and what features you’d want on mobile.&lt;/p&gt; &lt;p&gt;🚀 Check out our Product Hunt launch here: &lt;a href="https://www.producthunt.com/products/runanywhere"&gt;https://www.producthunt.com/products/runanywhere&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’re also working on a complete AI voice flow that runs entirely locally (no internet needed) — updates coming soon.&lt;/p&gt; &lt;p&gt;Cheers, RunAnywhere Team&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecoder12322"&gt; /u/thecoder12322 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq5pag/ollama_but_for_mobile_with_a_cloud_fallback/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq5pag/ollama_but_for_mobile_with_a_cloud_fallback/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mq5pag/ollama_but_for_mobile_with_a_cloud_fallback/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T16:35:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq6itc</id>
    <title>Seeking Feedback on My AI Inference PC Build</title>
    <updated>2025-08-14T17:05:13+00:00</updated>
    <author>
      <name>/u/nightcrawler2164</name>
      <uri>https://old.reddit.com/user/nightcrawler2164</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nightcrawler2164"&gt; /u/nightcrawler2164 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/PcBuild/comments/1mq6gvc/seeking_feedback_on_my_ai_inference_pc_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq6itc/seeking_feedback_on_my_ai_inference_pc_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mq6itc/seeking_feedback_on_my_ai_inference_pc_build/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T17:05:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq5ao9</id>
    <title>Run models on Android.</title>
    <updated>2025-08-14T16:20:50+00:00</updated>
    <author>
      <name>/u/Cobalt_Astronomer</name>
      <uri>https://old.reddit.com/user/Cobalt_Astronomer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any software like ollama or lm studio to run models on Android. I have a phone with decent specifications.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cobalt_Astronomer"&gt; /u/Cobalt_Astronomer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq5ao9/run_models_on_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq5ao9/run_models_on_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mq5ao9/run_models_on_android/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T16:20:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqat3c</id>
    <title>M1 Pro MacBook with 16 GB of RAM</title>
    <updated>2025-08-14T19:37:11+00:00</updated>
    <author>
      <name>/u/CobusGreyling</name>
      <uri>https://old.reddit.com/user/CobusGreyling</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the best model I can run with reasonable latency? I pulled and ran the GPT-OSS-30b model and inference is excruciating slow...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CobusGreyling"&gt; /u/CobusGreyling &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqat3c/m1_pro_macbook_with_16_gb_of_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqat3c/m1_pro_macbook_with_16_gb_of_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mqat3c/m1_pro_macbook_with_16_gb_of_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T19:37:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpi9rq</id>
    <title>I just had my first contributor to my open source AI coding agent and it feels great!</title>
    <updated>2025-08-13T22:08:51+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mpi9rq/i_just_had_my_first_contributor_to_my_open_source/"&gt; &lt;img alt="I just had my first contributor to my open source AI coding agent and it feels great!" src="https://preview.redd.it/xat5ofgh1vif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=814ecd9f096ab0c04978681b31dddbd4374c1779" title="I just had my first contributor to my open source AI coding agent and it feels great!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last week I released a rough-around-the-edges open source AI coding agent that runs in your terminal through Ollama and OpenRouter as well as any OpenAI compatible API. I published about wanting to grow it into a community and after a couple days I had my first contributor with a pull request adding some amazing features! &lt;/p&gt; &lt;p&gt;As my first proper open source project (normally I've built closed source as part of my day job), to get people taking an interest enough to star, fork and contribute is an incredible feeling, even if it is very early days!&lt;/p&gt; &lt;p&gt;This project is totally free and I want to build a community around it. I believe access to AI to help people create should be available to everyone for free and not necessarily controlled by big companies.&lt;/p&gt; &lt;p&gt;I would love your help! Whether you're interested in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Adding support for new AI providers&lt;/li&gt; &lt;li&gt;Improving tool functionality&lt;/li&gt; &lt;li&gt;Enhancing the user experience&lt;/li&gt; &lt;li&gt;Writing documentation&lt;/li&gt; &lt;li&gt;Reporting bugs or suggesting features&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All contributions are welcome! Here is the link if you're interested: &lt;a href="https://github.com/Mote-Software/nanocoder"&gt;https://github.com/Mote-Software/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But yes, this post is just me celebrating 😄&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xat5ofgh1vif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpi9rq/i_just_had_my_first_contributor_to_my_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpi9rq/i_just_had_my_first_contributor_to_my_open_source/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T22:08:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqcsbh</id>
    <title>Ollama AI Therapist</title>
    <updated>2025-08-14T20:48:58+00:00</updated>
    <author>
      <name>/u/IWriteTheBuggyCode</name>
      <uri>https://old.reddit.com/user/IWriteTheBuggyCode</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking to set up Ollama to run a local LLM to be a therapist. I have a couple questions.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;What model to use?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;How do I make it remember our previous conversations?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Can it be set up to work on speech rather than text?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IWriteTheBuggyCode"&gt; /u/IWriteTheBuggyCode &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqcsbh/ollama_ai_therapist/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mqcsbh/ollama_ai_therapist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mqcsbh/ollama_ai_therapist/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T20:48:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq6pi7</id>
    <title>Ollama but for realtime Speech-to-Text</title>
    <updated>2025-08-14T17:11:42+00:00</updated>
    <author>
      <name>/u/AggressiveHunt2300</name>
      <uri>https://old.reddit.com/user/AggressiveHunt2300</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Docs: &lt;a href="https://docs.hyprnote.com/owhisper/what-is-this"&gt;https://docs.hyprnote.com/owhisper/what-is-this&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CLI Demo: &lt;a href="https://asciinema.org/a/733110"&gt;https://asciinema.org/a/733110&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quick Start:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;brew tap fastrepl/hyprnote &amp;amp;&amp;amp; brew install owhisper owhisper pull whisper-cpp-base-q8-en owhisper run whisper-cpp-base-q8-en &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Other model like moonshine is also supported)&lt;/p&gt; &lt;p&gt;Love to hear what you guys think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggressiveHunt2300"&gt; /u/AggressiveHunt2300 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq6pi7/ollama_but_for_realtime_speechtotext/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq6pi7/ollama_but_for_realtime_speechtotext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mq6pi7/ollama_but_for_realtime_speechtotext/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T17:11:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq1izk</id>
    <title>Easy RAG using Ollama</title>
    <updated>2025-08-14T14:01:12+00:00</updated>
    <author>
      <name>/u/gogozad</name>
      <uri>https://old.reddit.com/user/gogozad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Ollama people,&lt;/p&gt; &lt;p&gt;I am the author of &lt;a href="https://github.com/ggozad/oterm"&gt;oterm&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/ggozad/haiku.rag"&gt;haiku.rag&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I created an &lt;a href="https://ggozad.github.io/oterm/rag_example"&gt;example&lt;/a&gt; on how to combine these two to get fully local RAG, running on Ollama and without the need of external vector databases or servers other than Ollama. &lt;/p&gt; &lt;p&gt;You can see a demo and detailed instructions at the &lt;code&gt;oterm&lt;/code&gt;s &lt;a href="https://ggozad.github.io/oterm/rag_example"&gt;docs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking forward to your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gogozad"&gt; /u/gogozad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq1izk/easy_rag_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mq1izk/easy_rag_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mq1izk/easy_rag_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T14:01:12+00:00</published>
  </entry>
</feed>
