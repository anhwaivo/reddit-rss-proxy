<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-04-01T10:49:35+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jnfb6h</id>
    <title>Need help stopping runaway GPU due to inferencing with Ollama and Open WebUI</title>
    <updated>2025-03-30T15:47:01+00:00</updated>
    <author>
      <name>/u/Haunting_Bat_4240</name>
      <uri>https://old.reddit.com/user/Haunting_Bat_4240</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Haunting_Bat_4240"&gt; /u/Haunting_Bat_4240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/unRAID/comments/1jnf9y6/need_help_stopping_runaway_gpu_due_to_inferencing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnfb6h/need_help_stopping_runaway_gpu_due_to_inferencing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jnfb6h/need_help_stopping_runaway_gpu_due_to_inferencing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-30T15:47:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmrw67</id>
    <title>ollama inference 25% faster on Linux than windows</title>
    <updated>2025-03-29T17:48:02+00:00</updated>
    <author>
      <name>/u/AdhesivenessLatter57</name>
      <uri>https://old.reddit.com/user/AdhesivenessLatter57</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;running latest version of ollama 0.6.2 on both systems, updated windows 11 and latest build of kali Linux with kernel 3.11. python 3.12.9, pytorch 2.6, cuda 12.6 on both pc.&lt;/p&gt; &lt;p&gt;I have tested major under 8b models(llama3.2, gemma2, gemma3, qwen2.5 and mistral) available in ollama that inference is 25% faster on Linux pc than windows pc.&lt;/p&gt; &lt;p&gt;nividia quadro rtx 4000 8gb vram, 32gb ram, intel i7&lt;/p&gt; &lt;p&gt;is this a known fact? any benchmarking data or article on this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdhesivenessLatter57"&gt; /u/AdhesivenessLatter57 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmrw67/ollama_inference_25_faster_on_linux_than_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmrw67/ollama_inference_25_faster_on_linux_than_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmrw67/ollama_inference_25_faster_on_linux_than_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T17:48:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jndb9h</id>
    <title>RAG and permissions broken?</title>
    <updated>2025-03-30T14:14:17+00:00</updated>
    <author>
      <name>/u/OrganizationHot731</name>
      <uri>https://old.reddit.com/user/OrganizationHot731</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone&lt;/p&gt; &lt;p&gt;Maybe my expectations on how things work are off... So please correct me if I am wrong&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I have 10 collections of knowledge loaded &lt;/li&gt; &lt;li&gt;I have a model that is to use the collection of knowledge (set in the settings of the model) &lt;/li&gt; &lt;li&gt;I have users loaded that have part of a group 4 that ground is restricted to only access 1-2 knowledge collections&lt;/li&gt; &lt;li&gt;I have the instructions for the model set to only answer questions from the data in the knowledge collections that is accessible by the user.&lt;br /&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Based on that when the user talks with the model it should ONLY reference the knowledge the users/group is assigned. Not all that is available to the model. &lt;/p&gt; &lt;p&gt;Instead the model is pulling data from all collections and not just the 2 that the user should be limited to in the group. &lt;/p&gt; &lt;p&gt;While I type # and only the collections assigned are correct, it's like the backend is ignoring that the user is restricted to that when the model has all knowledge collections.... &lt;/p&gt; &lt;p&gt;What am I missing? Or is something broken? &lt;/p&gt; &lt;p&gt;My end goal is to have 1 model that has access to all the collections but when a user asks it only uses data and references the collection the user has access to. &lt;/p&gt; &lt;p&gt;Example: - User is restricted to collection 3&amp;amp;5 - Model has 1-10 access in its settings - User asks a question that should only be available in collection 6 - Model will pull data from 6 and answer to user, when it shouldn't say it doesn't have access to that data. -User asks a question that's should be available in collection 5 - Model should answer fully without any restriction&lt;/p&gt; &lt;p&gt;Anyone have any idea what I'm missing or what I'm doing wrong. Or is something broken?? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OrganizationHot731"&gt; /u/OrganizationHot731 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jndb9h/rag_and_permissions_broken/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jndb9h/rag_and_permissions_broken/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jndb9h/rag_and_permissions_broken/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-30T14:14:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnbr1c</id>
    <title>Looking for a ChatGPT-like Mac app that supports multiple AI models and MCP protocol</title>
    <updated>2025-03-30T12:52:23+00:00</updated>
    <author>
      <name>/u/MorpheusML</name>
      <uri>https://old.reddit.com/user/MorpheusML</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;I’ve been using the official ChatGPT app for Mac for quite some time now, and honestly, it’s fantastic. The Swift app is responsive, intuitive, and has many features that make it much nicer than the browser version. However, there’s one major limitation: It only works with OpenAI’s models. I’m looking for a similar desktop experience but with the ability to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Connect to Claude models (especially Sonnet 3.7)&lt;/li&gt; &lt;li&gt;Use local models via Ollama&lt;/li&gt; &lt;li&gt;Connect to MCP servers&lt;/li&gt; &lt;li&gt;Switch between different AI providers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’ve tried a few open-source alternatives (for example, &lt;a href="https://github.com/Renset/macai"&gt;https://github.com/Renset/macai&lt;/a&gt;), but none have matched the polish and user experience of the official ChatGPT app. I know browser-based solutions like OpenWebUI, but I prefer a native Mac application.&lt;/p&gt; &lt;p&gt;Do you know of a well-designed Mac app that fits these requirements?&lt;/p&gt; &lt;p&gt;Any recommendations would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MorpheusML"&gt; /u/MorpheusML &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnbr1c/looking_for_a_chatgptlike_mac_app_that_supports/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnbr1c/looking_for_a_chatgptlike_mac_app_that_supports/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jnbr1c/looking_for_a_chatgptlike_mac_app_that_supports/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-30T12:52:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnjh1h</id>
    <title>Ollama on laptop with 2 GPU</title>
    <updated>2025-03-30T18:49:07+00:00</updated>
    <author>
      <name>/u/Successful_Power2125</name>
      <uri>https://old.reddit.com/user/Successful_Power2125</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, good day..is it possible for Olama to use the 2 GPUs in my computer since one is an AMD 780M and a dedicated Nvidia 4070? Thanks for your answers &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Successful_Power2125"&gt; /u/Successful_Power2125 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnjh1h/ollama_on_laptop_with_2_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnjh1h/ollama_on_laptop_with_2_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jnjh1h/ollama_on_laptop_with_2_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-30T18:49:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnnz1l</id>
    <title>Struggling with a simple summary bot</title>
    <updated>2025-03-30T22:05:42+00:00</updated>
    <author>
      <name>/u/why_not_my_email</name>
      <uri>https://old.reddit.com/user/why_not_my_email</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm still very new to Ollama. I'm trying to create a setup that returns a one-sentence summary of a document, as a stepping stone towards identifying and providing key quotations relevant to a project. &lt;/p&gt; &lt;p&gt;I've spent the last couple of hours playing around with different prompts, &lt;code&gt;system&lt;/code&gt; arguments, source documents, and models (primarily llama3.2, gemma3:12b, and a couple different sizes of deepseek-r1). In every case, the model gives a long, articulated summary (along with commentary about how the document is thoughtful or complex or whatever). &lt;/p&gt; &lt;p&gt;I'm using the &lt;code&gt;ollamar&lt;/code&gt; package, since I'm more comfortable with R than bash scripts. FWIW here's the current version: ``` library(ollamar) library(stringr) library(glue) library(pdftools) library(tictoc)&lt;/p&gt; &lt;p&gt;source = '/path/to/doc' |&amp;gt; readLines() |&amp;gt; str_c(collapse = '\n')&lt;/p&gt; &lt;p&gt;system = &amp;quot;You are an academic research assistant. The user will give you the text of a source document. Your job is to provide a one-sentence summary of the overall conclusion of the source. Do not include any other analysis or commentary.&amp;quot;&lt;/p&gt; &lt;p&gt;prompt = glue(&amp;quot;{source}&amp;quot;)&lt;/p&gt; &lt;p&gt;str_length(prompt) / 4&lt;/p&gt; &lt;p&gt;tic() resp = generate('llama3.2', system = system, prompt = prompt, output = 'resp', stream = TRUE, temperature = 0)&lt;/p&gt; &lt;h1&gt;resp = chat('gemma3:12b',&lt;/h1&gt; &lt;h1&gt;messages = list(&lt;/h1&gt; &lt;h1&gt;list(role = 'system', content = system),&lt;/h1&gt; &lt;h1&gt;list(role = 'user', content = prompt)),&lt;/h1&gt; &lt;h1&gt;output = 'text', stream = TRUE)&lt;/h1&gt; &lt;p&gt;toc() ```&lt;/p&gt; &lt;p&gt;Help? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/why_not_my_email"&gt; /u/why_not_my_email &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnnz1l/struggling_with_a_simple_summary_bot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnnz1l/struggling_with_a_simple_summary_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jnnz1l/struggling_with_a_simple_summary_bot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-30T22:05:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnbr3f</id>
    <title>Looking for a ChatGPT-like Mac app that supports multiple AI models and MCP protocol</title>
    <updated>2025-03-30T12:52:27+00:00</updated>
    <author>
      <name>/u/MorpheusML</name>
      <uri>https://old.reddit.com/user/MorpheusML</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;I’ve been using the official ChatGPT app for Mac for quite some time now, and honestly, it’s fantastic. The Swift app is responsive, intuitive, and has many features that make it much nicer than the browser version. However, there’s one major limitation: It only works with OpenAI’s models. I’m looking for a similar desktop experience but with the ability to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Connect to Claude models (especially Sonnet 3.7)&lt;/li&gt; &lt;li&gt;Use local models via Ollama&lt;/li&gt; &lt;li&gt;Connect to MCP servers&lt;/li&gt; &lt;li&gt;Switch between different AI providers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’ve tried a few open-source alternatives (for example, &lt;a href="https://github.com/Renset/macai"&gt;https://github.com/Renset/macai&lt;/a&gt;), but none have matched the polish and user experience of the official ChatGPT app. I know browser-based solutions like OpenWebUI, but I prefer a native Mac application.&lt;/p&gt; &lt;p&gt;Do you know of a well-designed Mac app that fits these requirements?&lt;/p&gt; &lt;p&gt;Any recommendations would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MorpheusML"&gt; /u/MorpheusML &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnbr3f/looking_for_a_chatgptlike_mac_app_that_supports/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnbr3f/looking_for_a_chatgptlike_mac_app_that_supports/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jnbr3f/looking_for_a_chatgptlike_mac_app_that_supports/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-30T12:52:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnjvwh</id>
    <title>Most of the models I have tried got it right. But baby llama triped over itself.</title>
    <updated>2025-03-30T19:06:49+00:00</updated>
    <author>
      <name>/u/tahaan</name>
      <uri>https://old.reddit.com/user/tahaan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jnjvwh/most_of_the_models_i_have_tried_got_it_right_but/"&gt; &lt;img alt="Most of the models I have tried got it right. But baby llama triped over itself." src="https://preview.redd.it/gzui4kpxkvre1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24bfa3ba69eed11474b9e7da24d6ab5d4ed56cc2" title="Most of the models I have tried got it right. But baby llama triped over itself." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tahaan"&gt; /u/tahaan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gzui4kpxkvre1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnjvwh/most_of_the_models_i_have_tried_got_it_right_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jnjvwh/most_of_the_models_i_have_tried_got_it_right_but/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-30T19:06:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jno67z</id>
    <title>API and Local file access</title>
    <updated>2025-03-30T22:14:49+00:00</updated>
    <author>
      <name>/u/LeeAnt74</name>
      <uri>https://old.reddit.com/user/LeeAnt74</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm very new to using Ollama but finally got to the point today where I was able to install the Web UI. However, two things are still causing me headaches.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;How do you use the API to send requests? I've been trying localhost:8080/api/chat and the same on 11414 without success.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Every time I attempt to get Ollama to examine files it tells me that I have to explicitly give authorisation. This makes sense but how do I do this?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Sorry, I'm sure these are going to appear to be problems with obvious answers but I've got nowhere and just ended up frustrated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeeAnt74"&gt; /u/LeeAnt74 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jno67z/api_and_local_file_access/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jno67z/api_and_local_file_access/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jno67z/api_and_local_file_access/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-30T22:14:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jny9q7</id>
    <title>Fine tuning ollama/gemini models</title>
    <updated>2025-03-31T08:14:11+00:00</updated>
    <author>
      <name>/u/Electrical-Button635</name>
      <uri>https://old.reddit.com/user/Electrical-Button635</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guy's im looking for resources for fine tunning ollama or gemini models resources &lt;/p&gt; &lt;p&gt;I'll be greatful if you vsn share your resources. I'm new in this field of AI and ML and wanted to learn.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electrical-Button635"&gt; /u/Electrical-Button635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jny9q7/fine_tuning_ollamagemini_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jny9q7/fine_tuning_ollamagemini_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jny9q7/fine_tuning_ollamagemini_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-31T08:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnv82k</id>
    <title>MacBook M2 16GB + 24h flight time no WiFi</title>
    <updated>2025-03-31T04:32:50+00:00</updated>
    <author>
      <name>/u/boxabirds</name>
      <uri>https://old.reddit.com/user/boxabirds</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What’s the best way to generate code with this base config?&lt;/p&gt; &lt;p&gt;Options seem to be - find a model that works with Cline or RooCode - copy/paste using OpenWebUI&lt;/p&gt; &lt;p&gt;I’m sure I’m missing others. What would others do? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boxabirds"&gt; /u/boxabirds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnv82k/macbook_m2_16gb_24h_flight_time_no_wifi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnv82k/macbook_m2_16gb_24h_flight_time_no_wifi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jnv82k/macbook_m2_16gb_24h_flight_time_no_wifi/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-31T04:32:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jngors</id>
    <title>Agent - A Local Computer-Use Operator for macOS</title>
    <updated>2025-03-30T16:48:19+00:00</updated>
    <author>
      <name>/u/sandropuppo</name>
      <uri>https://old.reddit.com/user/sandropuppo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've just open-sourced Agent, our framework for running computer-use workflows across multiple apps in isolated macOS/Linux sandboxes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Grab the code at&lt;/strong&gt; &lt;a href="https://github.com/trycua/cua"&gt;&lt;strong&gt;https://github.com/trycua/cua&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After launching Computer a few weeks ago, we realized many of you wanted to run complex workflows that span multiple applications. Agent builds on Computer to make this possible. It works with local Ollama models (if you're privacy-minded) or cloud providers like OpenAI, Anthropic, and others.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why we built this:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We kept hitting the same problems when building multi-app AI agents - they'd break in unpredictable ways, work inconsistently across environments, or just fail with complex workflows. So we built Agent to solve these headaches:&lt;/p&gt; &lt;p&gt;•⁠ ⁠It handles complex workflows across multiple apps without falling apart&lt;/p&gt; &lt;p&gt;•⁠ ⁠You can use your preferred model (local or cloud) - we're not locking you into one provider&lt;/p&gt; &lt;p&gt;•⁠ ⁠You can swap between different agent loop implementations depending on what you're building&lt;/p&gt; &lt;p&gt;•⁠ ⁠You get clean, structured responses that work well with other tools&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The code is pretty straightforward:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;async with Computer() as macos_computer:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;agent = ComputerAgent(&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;computer=macos_computer,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;loop=AgentLoop.OPENAI,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;model=LLM(provider=LLMProvider.OPENAI)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;tasks = [&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Look for a repository named trycua/cua on GitHub.&amp;quot;,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Check the open issues, open the most recent one and read it.&amp;quot;,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Clone the repository if it doesn't exist yet.&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;for i, task in enumerate(tasks):&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;print(f&amp;quot;\nTask {i+1}/{len(tasks)}: {task}&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;async for result in agent.run(task):&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;print(result)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;print(f&amp;quot;\nFinished task {i+1}!&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some cool things you can do with it:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;•⁠ ⁠Mix and match agent loops - OpenAI for some tasks, Claude for others, or try our experimental OmniParser&lt;/p&gt; &lt;p&gt;•⁠ ⁠Run it with various models - works great with OpenAI's computer_use_preview, but also with Claude and others&lt;/p&gt; &lt;p&gt;•⁠ ⁠Get detailed logs of what your agent is thinking/doing (super helpful for debugging)&lt;/p&gt; &lt;p&gt;•⁠ ⁠All the sandboxing from Computer means your main system stays protected&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Getting started is easy:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install &amp;quot;cua-agent[all]&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;# Or if you only need specific providers:&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install &amp;quot;cua-agent[openai]&amp;quot; # Just OpenAI&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install &amp;quot;cua-agent[anthropic]&amp;quot; # Just Anthropic&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install &amp;quot;cua-agent[omni]&amp;quot; # Our experimental OmniParser&lt;/code&gt;&lt;/p&gt; &lt;p&gt;We've been dogfooding this internally for weeks now, and it's been a game-changer for automating our workflows. &lt;/p&gt; &lt;p&gt;Would love to hear your thoughts ! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandropuppo"&gt; /u/sandropuppo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jngors/agent_a_local_computeruse_operator_for_macos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jngors/agent_a_local_computeruse_operator_for_macos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jngors/agent_a_local_computeruse_operator_for_macos/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-30T16:48:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jo5rv9</id>
    <title>Menu bar Mac app?</title>
    <updated>2025-03-31T15:23:08+00:00</updated>
    <author>
      <name>/u/KenKaniffsmd</name>
      <uri>https://old.reddit.com/user/KenKaniffsmd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does there exist an Ollama UI where I can access and chat with the models I have downloaded from my menu bar?&lt;/p&gt; &lt;p&gt;I use chatbox right now which is nice, but I haven't been able to find any apps that do this, only with chatgtp.. Does anyone know if one exists? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KenKaniffsmd"&gt; /u/KenKaniffsmd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jo5rv9/menu_bar_mac_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jo5rv9/menu_bar_mac_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jo5rv9/menu_bar_mac_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-31T15:23:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jo1ap8</id>
    <title>Dou (道) - AI powered analysis and feedback for notes and mind maps</title>
    <updated>2025-03-31T11:50:06+00:00</updated>
    <author>
      <name>/u/shokuninstudio</name>
      <uri>https://old.reddit.com/user/shokuninstudio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jo1ap8/dou_道_ai_powered_analysis_and_feedback_for_notes/"&gt; &lt;img alt="Dou (道) - AI powered analysis and feedback for notes and mind maps" src="https://external-preview.redd.it/YYeavqzrlRV9wv0VsdOcofwGugram_uHQiRH-4N5NPs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e92aee0cb0217897da5babb318d8c8a6e22cf5a" title="Dou (道) - AI powered analysis and feedback for notes and mind maps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shokuninstudio"&gt; /u/shokuninstudio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/shokuninstudio/Dou/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jo1ap8/dou_道_ai_powered_analysis_and_feedback_for_notes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jo1ap8/dou_道_ai_powered_analysis_and_feedback_for_notes/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-31T11:50:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jo9dpq</id>
    <title>NVIDIA RTX A4000 vs Quadro RTX 5000</title>
    <updated>2025-03-31T17:51:35+00:00</updated>
    <author>
      <name>/u/randomplebescite</name>
      <uri>https://old.reddit.com/user/randomplebescite</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomplebescite"&gt; /u/randomplebescite &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/buildapc/comments/1jo896e/nvidia_rtx_a4000_vs_quadro_rtx_5000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jo9dpq/nvidia_rtx_a4000_vs_quadro_rtx_5000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jo9dpq/nvidia_rtx_a4000_vs_quadro_rtx_5000/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-31T17:51:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnvnay</id>
    <title>Best local model which can process images and runs on 24GB GPU RAM?</title>
    <updated>2025-03-31T05:00:54+00:00</updated>
    <author>
      <name>/u/OkRide2660</name>
      <uri>https://old.reddit.com/user/OkRide2660</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to extend my local vibe voice model, so I can not just type with my voice, but also get nice LLM suggestions with my voice command and want to send the current screenshot as context.&lt;/p&gt; &lt;p&gt;I have a RTX 3090 and want to know what you consider the best ollama vision model which can run on this card (without being slow / swapping to system RAM etc). &lt;/p&gt; &lt;p&gt;Thank you! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OkRide2660"&gt; /u/OkRide2660 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnvnay/best_local_model_which_can_process_images_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnvnay/best_local_model_which_can_process_images_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jnvnay/best_local_model_which_can_process_images_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-31T05:00:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jojxzq</id>
    <title>App creation ai</title>
    <updated>2025-04-01T01:30:51+00:00</updated>
    <author>
      <name>/u/cuberhino</name>
      <uri>https://old.reddit.com/user/cuberhino</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anything I could use to create an app with ai? Like a web app or iOS app&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cuberhino"&gt; /u/cuberhino &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jojxzq/app_creation_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jojxzq/app_creation_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jojxzq/app_creation_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-01T01:30:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnrmd9</id>
    <title>I built an open-source NotebookLM alternative using Morphik</title>
    <updated>2025-03-31T01:07:54+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like using NoteBook LM, especially when I have a bunch of research papers I'm trying to extract insights from.&lt;/p&gt; &lt;p&gt;For example, if I'm implementing a new feature (like re-ranking) into Morphik, I like to create a notebook with some papers about it, and then compare those models with each other on different benchmarks.&lt;/p&gt; &lt;p&gt;I thought it would be cool to create a free, completely open-source version of it, so that I could use some private docs (like my journal!) and see if a NoteBook LM like system can help with that. I've found it to be insanely helpful, so I added a version of it onto the Morphik UI Component!&lt;/p&gt; &lt;p&gt;Try it out:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clone the repo at: &lt;a href="https://github.com/morphik-org/morphik-core"&gt;https://github.com/morphik-org/morphik-core&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Launch the UI component following instructions here: &lt;a href="https://docs.morphik.ai/using-morphik/morphik-ui"&gt;https://docs.morphik.ai/using-morphik/morphik-ui&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love to hear the &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; community's thoughts and feature requests!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnrmd9/i_built_an_opensource_notebooklm_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnrmd9/i_built_an_opensource_notebooklm_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jnrmd9/i_built_an_opensource_notebooklm_alternative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-31T01:07:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jo46vx</id>
    <title>I built a voice assistant that types for me anywhere with context from screenshots</title>
    <updated>2025-03-31T14:14:31+00:00</updated>
    <author>
      <name>/u/OkRide2660</name>
      <uri>https://old.reddit.com/user/OkRide2660</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Simply hold a button and aks your question:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;your spoken text gets transcribed by a locally running whisper model&lt;/li&gt; &lt;li&gt;a screenshot is made &lt;/li&gt; &lt;li&gt;both is sent to an ollama model of your choice (defaults to Gemma3:27B)&lt;/li&gt; &lt;li&gt;the llm answer is typed into your keyboard&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So you can e. g. say 'reply to this email' and it sees the email and types your response.&lt;/p&gt; &lt;p&gt;Try it out and let me know what you think:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mpaepper/vibevoice"&gt;https://github.com/mpaepper/vibevoice&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OkRide2660"&gt; /u/OkRide2660 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jo46vx/i_built_a_voice_assistant_that_types_for_me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jo46vx/i_built_a_voice_assistant_that_types_for_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jo46vx/i_built_a_voice_assistant_that_types_for_me/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-31T14:14:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jon581</id>
    <title>Copying a Fine-Tuned Model to Another Machine</title>
    <updated>2025-04-01T04:23:12+00:00</updated>
    <author>
      <name>/u/BorisLovesMarishka</name>
      <uri>https://old.reddit.com/user/BorisLovesMarishka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I have been working on fine-tuning an Llama3 model and I want to share it with my colleagues to run on their own machines. What is the best way to send it to them? Would it just be to create a model file and send it? I would prefer not to send it up to Ollama for them to pull it down for themselves if possible. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BorisLovesMarishka"&gt; /u/BorisLovesMarishka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jon581/copying_a_finetuned_model_to_another_machine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jon581/copying_a_finetuned_model_to_another_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jon581/copying_a_finetuned_model_to_another_machine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-01T04:23:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1johtit</id>
    <title>Responses are different</title>
    <updated>2025-03-31T23:45:48+00:00</updated>
    <author>
      <name>/u/Satoshi-Wasabi8520</name>
      <uri>https://old.reddit.com/user/Satoshi-Wasabi8520</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Responses is different using Ollama in console and Ollama models in open-webui. The response in console is straight forward and correct while in open-webui sometimes incorrect, same model, same prompt. Any idea?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Satoshi-Wasabi8520"&gt; /u/Satoshi-Wasabi8520 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1johtit/responses_are_different/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1johtit/responses_are_different/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1johtit/responses_are_different/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-31T23:45:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1josb2j</id>
    <title>New to Ollama, want to integrate it more but keep it portable.</title>
    <updated>2025-04-01T10:28:05+00:00</updated>
    <author>
      <name>/u/elkbond</name>
      <uri>https://old.reddit.com/user/elkbond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, due to work reasons I can’t install applications without approval. So I made a portable version of ollama and I am using llama 3.1 and Deepseek currently just to try out functionality.&lt;/p&gt; &lt;p&gt;I want to configure it to be more assistant-like, such as able to add things to my calendar. Remind me about things, just generally be an always on assistant for research and PA duties.&lt;/p&gt; &lt;p&gt;I don’t mind adding a few programs at home to achieve this, but the biggest issue is how much space these take up and the fact if I want to take my ‘PA’ to work I need to have it run from the drive only. So currently at work I am just command line-ing it, but at home I use MSTY.&lt;/p&gt; &lt;p&gt;Anyone else achieved anything like the above? Also I am average or below-average at python and coding in general. I can get about but use guides aalotttt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elkbond"&gt; /u/elkbond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1josb2j/new_to_ollama_want_to_integrate_it_more_but_keep/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1josb2j/new_to_ollama_want_to_integrate_it_more_but_keep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1josb2j/new_to_ollama_want_to_integrate_it_more_but_keep/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-01T10:28:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jorrgn</id>
    <title>Are RDNA4 GPUs supported yet?</title>
    <updated>2025-04-01T09:51:21+00:00</updated>
    <author>
      <name>/u/T_Play</name>
      <uri>https://old.reddit.com/user/T_Play</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was wondering if Hardware Acceleration with RDNA4 GPUs (9070/9070 XT) is supported as of now. Because when I install ollama locally (Fedora 41) the installer states &amp;quot;AMD GPU ready&amp;quot; but when running a model, it clearly doesn't utilize my GPU&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/T_Play"&gt; /u/T_Play &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jorrgn/are_rdna4_gpus_supported_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jorrgn/are_rdna4_gpus_supported_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jorrgn/are_rdna4_gpus_supported_yet/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-01T09:51:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jo7bzj</id>
    <title>I want an LLM that responds with “I don’t know. How could I possibly do that or know that?” Instead of going into hallucinations</title>
    <updated>2025-03-31T16:28:09+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any recommendations? I tried a honest system prompt, but they are like hardwired to answer at any cost. &lt;/p&gt; &lt;p&gt;Reasoning ones are even worse.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jo7bzj/i_want_an_llm_that_responds_with_i_dont_know_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jo7bzj/i_want_an_llm_that_responds_with_i_dont_know_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jo7bzj/i_want_an_llm_that_responds_with_i_dont_know_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-31T16:28:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1joaaoj</id>
    <title>This project might be the most usable app for using models and image generation locally</title>
    <updated>2025-03-31T18:28:11+00:00</updated>
    <author>
      <name>/u/k1sh0r</name>
      <uri>https://old.reddit.com/user/k1sh0r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1joaaoj/this_project_might_be_the_most_usable_app_for/"&gt; &lt;img alt="This project might be the most usable app for using models and image generation locally" src="https://preview.redd.it/ypi8dajvi2se1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7fd295049f64e5b1474b5a2b5c643ee6a286c426" title="This project might be the most usable app for using models and image generation locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I came across this project called Clara in this subreddit few days ago, honestly it was so easy to setup and run. Previously I tried Open WebUI and it was too technical for me (as a non-tech person) to setup docker and all. I can see new improvements and in-app updates frequently. May be give it a try.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k1sh0r"&gt; /u/k1sh0r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ypi8dajvi2se1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1joaaoj/this_project_might_be_the_most_usable_app_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1joaaoj/this_project_might_be_the_most_usable_app_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-31T18:28:11+00:00</published>
  </entry>
</feed>
