<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-05-05T06:09:35+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1kcybew</id>
    <title>Qwen3 disable thinking in Ollama?</title>
    <updated>2025-05-02T11:12:24+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, How to get instant answer and disable thinking in qwen3 with Ollama?&lt;/p&gt; &lt;p&gt;Qwen3 pages states this is possible: &amp;quot;This flexibility allows users to control how much “thinking” the model performs based on the task at hand. For example, harder problems can be tackled with extended reasoning, while easier ones can be answered directly without delay.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kcybew/qwen3_disable_thinking_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kcybew/qwen3_disable_thinking_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kcybew/qwen3_disable_thinking_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T11:12:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1kd5q7x</id>
    <title>If you have adequate GPU, does the CPU matter?</title>
    <updated>2025-05-02T16:47:55+00:00</updated>
    <author>
      <name>/u/Havanatha_banana</name>
      <uri>https://old.reddit.com/user/Havanatha_banana</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an old xeon server and it has multiple pcie lanes, so I'm planning to get a few cheaper GPUs with high vrams to meet the 50gb vram requirement from 70b.&lt;/p&gt; &lt;p&gt;Context: For work, I want to train an AI to be able to format documents into a specific style, to fill it gaps of our documentations with transcriptions from videos. We have way too many meetings that are actually important but no minutes have been taken.&lt;/p&gt; &lt;p&gt;As such, I wanna start self hosting. I'm not sure if it's appropriate, but 70b seems to be default for my application?&lt;/p&gt; &lt;p&gt;As such, I need to run multiple GPUs to get it to work. I have an old xeon server with multiple pcie lanes. So hopefully that will work? Or should I settle for a smaller model, like 8b? Accuracy is more important here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Havanatha_banana"&gt; /u/Havanatha_banana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kd5q7x/if_you_have_adequate_gpu_does_the_cpu_matter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kd5q7x/if_you_have_adequate_gpu_does_the_cpu_matter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kd5q7x/if_you_have_adequate_gpu_does_the_cpu_matter/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T16:47:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kd95or</id>
    <title>Train Better Computer-Use AI by Creating Human Demonstration Datasets</title>
    <updated>2025-05-02T19:10:53+00:00</updated>
    <author>
      <name>/u/Original-Thanks-8118</name>
      <uri>https://old.reddit.com/user/Original-Thanks-8118</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The C/ua team just released a new tutorial that shows how anyone with macOS can contribute to training better computer-use AI models by recording their own human demonstrations.&lt;/p&gt; &lt;p&gt;Why this matters:&lt;/p&gt; &lt;p&gt;One of the biggest challenges in developing AI that can use computers effectively is the lack of high-quality human demonstration data. Current computer-use models often fail to capture the nuanced ways humans navigate interfaces, recover from errors, and adapt to changing contexts.&lt;/p&gt; &lt;p&gt;This tutorial walks through using C/ua's Computer-Use Interface (CUI) with a Gradio UI to:&lt;/p&gt; &lt;p&gt;- Record your natural computer interactions in a sandbox macOS environment&lt;/p&gt; &lt;p&gt;- Organize and tag your demonstrations for maximum research value&lt;/p&gt; &lt;p&gt;- Share your datasets on Hugging Face to advance computer-use AI research&lt;/p&gt; &lt;p&gt;What makes human demonstrations particularly valuable is that they capture aspects of computer use that synthetic data misses:&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Natural pacing&lt;/strong&gt; - the rhythm of real human computer use&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Error recovery&lt;/strong&gt; - how humans detect and fix mistakes&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Context-sensitive actions&lt;/strong&gt; - adjusting behavior based on changing UI states&lt;/p&gt; &lt;p&gt;You can find the blog-post here: &lt;a href="https://trycua.com/blog/training-computer-use-models-trajectories-1"&gt;https://trycua.com/blog/training-computer-use-models-trajectories-1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The only requirements are Python 3.10+ and macOS Sequoia.&lt;/p&gt; &lt;p&gt;Would love to hear if anyone else has been working on computer-use AI and your thoughts on this approach to building better training datasets!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Original-Thanks-8118"&gt; /u/Original-Thanks-8118 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.trycua.com/blog/training-computer-use-models-trajectories-1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kd95or/train_better_computeruse_ai_by_creating_human/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kd95or/train_better_computeruse_ai_by_creating_human/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T19:10:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1kdaogc</id>
    <title>Ollama Show model gpu/cpu layer</title>
    <updated>2025-05-02T20:15:55+00:00</updated>
    <author>
      <name>/u/lavoie005</name>
      <uri>https://old.reddit.com/user/lavoie005</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I searched a way to Find out many GPU offload layers a model have.&lt;/p&gt; &lt;p&gt;I also want to set the parameter for execute all layer in my gpu.&lt;/p&gt; &lt;p&gt;You can do it with lm studio But I ain't find any way to get how many layers the model have in Ollama&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lavoie005"&gt; /u/lavoie005 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdaogc/ollama_show_model_gpucpu_layer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdaogc/ollama_show_model_gpucpu_layer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kdaogc/ollama_show_model_gpucpu_layer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T20:15:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kd4r8l</id>
    <title>How to use bigger models</title>
    <updated>2025-05-02T16:07:10+00:00</updated>
    <author>
      <name>/u/newz2000</name>
      <uri>https://old.reddit.com/user/newz2000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have found many posts asking a similar question, but the answers don't make sense to me. I do not know what quantization and some of these other terms mean when it comes to the different model formats, and when I get AI tools to explain it to me, they're either too simple or too complex.&lt;/p&gt; &lt;p&gt;I have an older workstation with an 8gb GTX 1070 GPU. I'm having a lot of fun using it with 9b and smaller models (thanks to the suggestion for Gemma 3 4b - it packs quite a bunch). Specifically, I like Qwen 2.5, Gemma 3 and Qwen 3. Most of what I do is process, summarize, and reorganize info, but I have used Qwen 2.5 coder to write some shell scripts and automations.&lt;/p&gt; &lt;p&gt;I have bumped into a project that just fails with the smaller models. By failing, I mean it tries, and thinks its doing a good job, but the output is not nearly the quality of what a human would do. It works in ChatGPT and Gemini and I suspect it would work with bigger models.&lt;/p&gt; &lt;p&gt;I am due for a computer upgrade. My desktop is a 2019 i9 iMac with 64gb of RAM. I think I will replace it with a maxed out Mac mini or a mid-range Mac Studio. Or I could upgrade the graphics card in the workstation that has the 1070 gpu. (or I could do both)&lt;/p&gt; &lt;p&gt;My goal is to simply take legal and technical information and allow a human or an AI to ask questions about the information and generate useful reports on that info. The task that currently fails is having the AI generate follow-up questions of the human to clarify the goals without hallucinating.&lt;/p&gt; &lt;p&gt;What do I need to do to use bigger models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newz2000"&gt; /u/newz2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kd4r8l/how_to_use_bigger_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kd4r8l/how_to_use_bigger_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kd4r8l/how_to_use_bigger_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T16:07:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kdc3z8</id>
    <title>I'm amazed by ollama</title>
    <updated>2025-05-02T21:16:50+00:00</updated>
    <author>
      <name>/u/Sandalwoodincencebur</name>
      <uri>https://old.reddit.com/user/Sandalwoodincencebur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here in my city home I have an old computer from 2008 (i7 920 and DX58so 16GB ddr3, RTX 3050) and LM studio, GPT4All and koboldccp didn't work, I managed to get it kind of working but it was painfully slow (kobold). &lt;/p&gt; &lt;p&gt;Then I tried Ollama, and oh boy is this amazing, installed docker to run open webui and everything is dandy. I run couple of models locally, hermes3b:8, deepseek-r1:7b, llama3.2:1b, samantha-mistral:latest, still trying out different stuff, so I was wondering if you have any recommendations for lightweight models specialized in psychology, philosophy, arts and mythology, religions, metaphysics and poetry? &lt;/p&gt; &lt;p&gt;And I was also wondering if there's any FREE API for image generation I can outsource? I tried dalle3 but it doesn't work without subscription, is there API I could use for free? I wouldn't abuse it only an image here and there, as I'm not really a heavy user. Gemini also didn't work, something wrong with base url. So any recommendations what to try next, I really love tinkering with this stuff, and seeing it work so flawlessly on my old pc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sandalwoodincencebur"&gt; /u/Sandalwoodincencebur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdc3z8/im_amazed_by_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdc3z8/im_amazed_by_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kdc3z8/im_amazed_by_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T21:16:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kdt30w</id>
    <title>Ollama models wont run</title>
    <updated>2025-05-03T13:33:21+00:00</updated>
    <author>
      <name>/u/BKK31</name>
      <uri>https://old.reddit.com/user/BKK31</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I try to get any response from ollama models, I'm getting this error:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;error: post predict: post http://127.0.0.1:54764/completion : read tcp 127.0.0.1:54766-&amp;gt;127.0.0.1:54764: wsarecv: an existing connection was forcibly closed by the remote host. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Does anyone have a fix for this or know what's causing this?&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BKK31"&gt; /u/BKK31 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdt30w/ollama_models_wont_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdt30w/ollama_models_wont_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kdt30w/ollama_models_wont_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-03T13:33:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1kdq2m3</id>
    <title>Curious about the JOSIEFIED versions of models on Ollama—are they safe?</title>
    <updated>2025-05-03T10:42:47+00:00</updated>
    <author>
      <name>/u/KrazyHomosapien</name>
      <uri>https://old.reddit.com/user/KrazyHomosapien</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I'm kinda new to all this AI model stuff and recently came across the &lt;strong&gt;&amp;quot;JOSIEFIED-Qwen3:8b-q3_k_m&amp;quot;&lt;/strong&gt; model on Ollama. It’s supposed to be an uncensored, super-intelligent version created by someone named &lt;strong&gt;Gökdeniz Gülmez&lt;/strong&gt;. I don't know much about him, so I am just taking some precautions.&lt;/p&gt; &lt;p&gt;I’m interested in testing the uncensored version of Qwen 3 just for experimentation purposes, but I’m worried because I’m new to all this and not sure if models in Ollama could have malware when used on my main PC. I don’t want to take any unnecessary risks.&lt;/p&gt; &lt;p&gt;Has anyone tried the JOSIEFIED versions? Any red flags or odd behaviors I should be aware of before I dive in? Is it safe to test, or should I steer clear?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LINK:&lt;/strong&gt; &lt;a href="https://ollama.com/goekdenizguelmez/JOSIEFIED-Qwen3:8b-q3_k_m"&gt;&lt;strong&gt;https://ollama.com/goekdenizguelmez/JOSIEFIED-Qwen3:8b-q3_k_m&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would really appreciate your advice and any insights you might have!&lt;/p&gt; &lt;p&gt;Thanks in advance! 🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KrazyHomosapien"&gt; /u/KrazyHomosapien &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdq2m3/curious_about_the_josiefied_versions_of_models_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdq2m3/curious_about_the_josiefied_versions_of_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kdq2m3/curious_about_the_josiefied_versions_of_models_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-03T10:42:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kdxsyl</id>
    <title>Problem with Obsidian plugin, Zen Browser and Ollama: "Ollama cannot process requests from browser extension"</title>
    <updated>2025-05-03T17:07:31+00:00</updated>
    <author>
      <name>/u/shaiceisonline</name>
      <uri>https://old.reddit.com/user/shaiceisonline</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! I'm new here and I'm stuck with an issue I can't solve on my own. I'm using &lt;strong&gt;Zen Browser&lt;/strong&gt; on &lt;strong&gt;macOS&lt;/strong&gt; with &lt;strong&gt;zsh&lt;/strong&gt;, and the &lt;strong&gt;Obsidian Web Clipper&lt;/strong&gt; plugin is giving me this error: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Ollama cannot process requests originating from a browser extension without setting OLLAMA_ORIGINS. See instructions at &lt;a href="https://help.obsidian.md/web-clipper/interpreter"&gt;https://help.obsidian.md/web-clipper/interpreter&lt;/a&gt;&amp;quot; &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I followed the guide from &lt;a href="https://blog.parente.dev/obsidian-webclipper-config/"&gt;https://blog.parente.dev/obsidian-webclipper-config/&lt;/a&gt; and added this line to my &lt;code&gt;.zshrc&lt;/code&gt;:&lt;br /&gt; &lt;code&gt;bash export OLLAMA_ORIGINS=* &lt;/code&gt;&lt;br /&gt; I reloaded the file with &lt;code&gt;source ~/.zshrc&lt;/code&gt;, restarted Zen Browser and the terminal, but the error keeps appearing. Oddly, it worked &lt;strong&gt;twice&lt;/strong&gt; without issues, but now it's not working again. &lt;/p&gt; &lt;p&gt;Does anyone know why it's not recognizing the origin? Maybe I missed a step? Or is there an issue with how Zen Browser handles environment variables? &lt;/p&gt; &lt;p&gt;Thanks in advance for your help! I'm happy to provide more details if needed. 🙏 &lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Additional details:&lt;/strong&gt;&lt;br /&gt; - Zen Browser version: 1.12b (Firefox 138.0.1) (aarch64)&lt;br /&gt; - Ollama version: 0.6.7&lt;br /&gt; - ➜ ~ echo $OLLAMA_ORIGINS retrurns *&lt;br /&gt; - I restarted Ollama after updating &lt;code&gt;.zshrc&lt;/code&gt; - Obsidian Web Clipper plugin is up to date &lt;/p&gt; &lt;p&gt;I'm a bit confused, but I've never seen this error before. Anyone else experience something similar? 😕&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shaiceisonline"&gt; /u/shaiceisonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdxsyl/problem_with_obsidian_plugin_zen_browser_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdxsyl/problem_with_obsidian_plugin_zen_browser_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kdxsyl/problem_with_obsidian_plugin_zen_browser_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-03T17:07:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kdlx2z</id>
    <title>Multimodal RAG with Cohere + Gemini 2.5 Flash</title>
    <updated>2025-05-03T05:53:40+00:00</updated>
    <author>
      <name>/u/srireddit2020</name>
      <uri>https://old.reddit.com/user/srireddit2020</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kdlx2z/multimodal_rag_with_cohere_gemini_25_flash/"&gt; &lt;img alt="Multimodal RAG with Cohere + Gemini 2.5 Flash" src="https://b.thumbs.redditmedia.com/d-wP6pvXUDk5oABXaAUFpi10nFiAdpj7HllucJMOFqY.jpg" title="Multimodal RAG with Cohere + Gemini 2.5 Flash" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hi everyone! �&lt;/strong&gt;�&lt;/p&gt; &lt;p&gt;I recently built a &lt;strong&gt;Multimodal RAG (Retrieval-Augmented Generation)&lt;/strong&gt; system that can extract insights from &lt;strong&gt;both text and images inside PDFs&lt;/strong&gt; — using &lt;strong&gt;Cohere’s multimodal embeddings&lt;/strong&gt; and &lt;strong&gt;Gemini 2.5 Flash&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;💡 &lt;strong&gt;Why this matters&lt;/strong&gt;:&lt;br /&gt; Traditional RAG systems completely miss visual data — like &lt;strong&gt;pie charts, tables, or infographics&lt;/strong&gt; — that are critical in financial or research PDFs.&lt;/p&gt; &lt;p&gt;📽️ &lt;strong&gt;Demo Video&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1kdlx2z/video/r5z2kawhaiye1/player"&gt;https://reddit.com/link/1kdlx2z/video/r5z2kawhaiye1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;📊 &lt;strong&gt;Multimodal RAG in Action&lt;/strong&gt;:&lt;br /&gt; ✅ Upload a financial PDF&lt;br /&gt; ✅ Embed both text and images&lt;br /&gt; ✅ Ask any question — e.g., &amp;quot;How much % is Apple in S&amp;amp;P 500?&amp;quot;&lt;br /&gt; ✅ Gemini gives image-grounded answers like reading from a chart&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cyguzinkaiye1.png?width=1989&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0452882b565badbf93863b376d99eb97567f79e2"&gt;https://preview.redd.it/cyguzinkaiye1.png?width=1989&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0452882b565badbf93863b376d99eb97567f79e2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🧠 &lt;strong&gt;Key Highlights&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mixed FAISS index (text + image embeddings)&lt;/li&gt; &lt;li&gt;Visual grounding via Gemini 2.5 Flash&lt;/li&gt; &lt;li&gt;Handles questions from tables, charts, and even timelines&lt;/li&gt; &lt;li&gt;Fully local setup using Streamlit + FAISS&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🛠️ &lt;strong&gt;Tech Stack&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cohere embed-v4.0&lt;/strong&gt; (text + image embeddings)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemini 2.5 Flash&lt;/strong&gt; (visual question answering)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;FAISS&lt;/strong&gt; (for retrieval)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;pdf2image&lt;/strong&gt; + &lt;strong&gt;PIL&lt;/strong&gt; (image conversion)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Streamlit UI&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;📌 &lt;strong&gt;Full blog + source code + side-by-side demo&lt;/strong&gt;:&lt;br /&gt; 🔗 &lt;a href="https://sridhartech.hashnode.dev/beyond-text-building-multimodal-rag-systems-with-cohere-and-gemini"&gt;sridhartech.hashnode.dev/beyond-text-building-multimodal-rag-systems-with-cohere-and-gemini&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts or any feedback! 😊&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/srireddit2020"&gt; /u/srireddit2020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdlx2z/multimodal_rag_with_cohere_gemini_25_flash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdlx2z/multimodal_rag_with_cohere_gemini_25_flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kdlx2z/multimodal_rag_with_cohere_gemini_25_flash/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-03T05:53:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1kd4woq</id>
    <title>zero dolars vibe debugging menace</title>
    <updated>2025-05-02T16:13:34+00:00</updated>
    <author>
      <name>/u/AntelopeEntire9191</name>
      <uri>https://old.reddit.com/user/AntelopeEntire9191</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kd4woq/zero_dolars_vibe_debugging_menace/"&gt; &lt;img alt="zero dolars vibe debugging menace" src="https://preview.redd.it/yg8mgjgt7eye1.gif?width=640&amp;amp;crop=smart&amp;amp;s=ee84534512b5deabf250bb21516f287f281c3f40" title="zero dolars vibe debugging menace" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been tweaking on building &lt;strong&gt;Cloi&lt;/strong&gt; its local debugging agent that runs in your terminal &lt;/p&gt; &lt;p&gt;cursor's o3 got me down astronomical ($0.30 per request??) and claude 3.7 still taking my lunch money ($0.05 a pop) so made something that's zero dollar sign vibes, just pure on-device cooking.&lt;/p&gt; &lt;p&gt;The technical breakdown is pretty straightforward: cloi deadass catches your error tracebacks, spins up a local LLM (zero api key nonsense, no cloud tax) and only with your permission (we respectin boundaries) drops some clean af patches directly to ur files.&lt;/p&gt; &lt;p&gt;Been working on this during my research downtime. If anyone's interested in exploring the implementation or wants to issue feedback, cloi its open source: &lt;a href="https://github.com/cloi-ai/cloi"&gt;https://github.com/cloi-ai/cloi&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AntelopeEntire9191"&gt; /u/AntelopeEntire9191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yg8mgjgt7eye1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kd4woq/zero_dolars_vibe_debugging_menace/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kd4woq/zero_dolars_vibe_debugging_menace/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T16:13:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1kdlh3z</id>
    <title>What is a real use of local AI for business?</title>
    <updated>2025-05-03T05:24:26+00:00</updated>
    <author>
      <name>/u/RepaBali</name>
      <uri>https://old.reddit.com/user/RepaBali</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a medium sized B2B business distributing petfood. What kind of use cases can you recommend running an LLM locally?&lt;/p&gt; &lt;p&gt;I was thinking of - Product knowledge base (but still haven’t figured that out) - Sales Rep Training&lt;/p&gt; &lt;p&gt;I am curious to know what would you suggest?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RepaBali"&gt; /u/RepaBali &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdlh3z/what_is_a_real_use_of_local_ai_for_business/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdlh3z/what_is_a_real_use_of_local_ai_for_business/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kdlh3z/what_is_a_real_use_of_local_ai_for_business/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-03T05:24:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kdfc3k</id>
    <title>I was confused at first about what model types mean, but this clarified it, I found 5-bit works the best on my system without sacrificing speed or accuracy. 16 bit works, but sluggish. If you're new to this...explanations of terminology in post.</title>
    <updated>2025-05-02T23:44:04+00:00</updated>
    <author>
      <name>/u/Sandalwoodincencebur</name>
      <uri>https://old.reddit.com/user/Sandalwoodincencebur</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kdfc3k/i_was_confused_at_first_about_what_model_types/"&gt; &lt;img alt="I was confused at first about what model types mean, but this clarified it, I found 5-bit works the best on my system without sacrificing speed or accuracy. 16 bit works, but sluggish. If you're new to this...explanations of terminology in post." src="https://preview.redd.it/fbv2s5a0fgye1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2149c48778b4bb213734368aa65fe75a9c1a5747" title="I was confused at first about what model types mean, but this clarified it, I found 5-bit works the best on my system without sacrificing speed or accuracy. 16 bit works, but sluggish. If you're new to this...explanations of terminology in post." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These are different versions (tags) of the &lt;strong&gt;Llama3.2&lt;/strong&gt; model, each optimized for specific use cases, sizes, and quantization levels. Here's a breakdown of what each part of the naming convention means:&lt;/p&gt; &lt;h1&gt;1. Model Size (1b, 3b)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;1b&lt;/code&gt;: A 1-billion-parameter version of the model (smaller, faster, less resource-intensive).&lt;/li&gt; &lt;li&gt;&lt;code&gt;3b&lt;/code&gt;: A 3-billion-parameter version (larger, more capable, but requires more RAM/VRAM).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. Model Type (text, instruct)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;text&lt;/code&gt;: A base model trained for general text generation (like autocompletion or story writing).&lt;/li&gt; &lt;li&gt;&lt;code&gt;instruct&lt;/code&gt;: Fine-tuned for instruction-following (better at following prompts like chatbots or assistants).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. Precision &amp;amp; Quantization (fp16, q2_K, q4_K_M, etc.)&lt;/h1&gt; &lt;p&gt;Quantization reduces model size by lowering numerical precision, trading off some accuracy for efficiency.&lt;/p&gt; &lt;h1&gt;Full Precision (No Quantization)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;fp16&lt;/code&gt;: Full 16-bit floating-point precision (highest quality, largest file size).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What q5_K_M What q5_K_M Specifically Means&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;code&gt;q5&lt;/code&gt; → 5-bit quantization &lt;ul&gt; &lt;li&gt;Weights stored in 5 bits (vs. 32 bits in &lt;code&gt;fp32&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Balances size and accuracy (better than &lt;code&gt;q4&lt;/code&gt;, smaller than &lt;code&gt;q6&lt;/code&gt;).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;_K&lt;/code&gt; → &amp;quot;K-means&amp;quot; clustering &lt;ul&gt; &lt;li&gt;Groups similar weights together to minimize precision loss.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;_M&lt;/code&gt; → &amp;quot;Middle&amp;quot; precision tier &lt;ul&gt; &lt;li&gt;Optimized for &lt;strong&gt;balanced&lt;/strong&gt; performance (other options: &lt;code&gt;_S&lt;/code&gt; for small, &lt;code&gt;_L&lt;/code&gt; for large).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sandalwoodincencebur"&gt; /u/Sandalwoodincencebur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fbv2s5a0fgye1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdfc3k/i_was_confused_at_first_about_what_model_types/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kdfc3k/i_was_confused_at_first_about_what_model_types/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T23:44:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kekkkc</id>
    <title>Built a LinkedIn lead gen system with automation + AI scraped 300M profiles (painful but worth it)</title>
    <updated>2025-05-04T14:04:20+00:00</updated>
    <author>
      <name>/u/Dreamer_made</name>
      <uri>https://old.reddit.com/user/Dreamer_made</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been deep in the weeds of marketing automation and AI for over a year now. Recently wrapped up building a large-scale system that scraped and enriched over &lt;strong&gt;300 million LinkedIn leads&lt;/strong&gt;. It involved:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multiple Sales Navigator accounts&lt;/li&gt; &lt;li&gt;Rotating proxies + headless browser automation&lt;/li&gt; &lt;li&gt;Queue-based architecture to avoid bans&lt;/li&gt; &lt;li&gt;ChatGPT and DeepSeek used for enrichment and parsing&lt;/li&gt; &lt;li&gt;Custom JavaScript for data cleanup + deduplication&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;LinkedIn really doesn't make it easy (lots of anti-bot mechanisms), but with enough retries and tweaks, it started flowing. The data pipelines, retry queues, and proxy rotation logic were the toughest parts.&lt;/p&gt; &lt;p&gt; If you're into large-scale scraping, lead gen, or just curious how this stuff works under the hood, happy to chat.&lt;/p&gt; &lt;p&gt;I packaged everything into a cleaned database way cheaper than ZoomInfo/Apollo if anyone ever needs it. It’s up at Leadady .com, &lt;strong&gt;one-time payment&lt;/strong&gt;, no fluff.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dreamer_made"&gt; /u/Dreamer_made &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kekkkc/built_a_linkedin_lead_gen_system_with_automation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kekkkc/built_a_linkedin_lead_gen_system_with_automation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kekkkc/built_a_linkedin_lead_gen_system_with_automation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-04T14:04:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ke0etl</id>
    <title>kb-ai-bot: probably another bot scraping sites and replies to questions (i did this)</title>
    <updated>2025-05-03T19:00:23+00:00</updated>
    <author>
      <name>/u/dowmeister_trucky</name>
      <uri>https://old.reddit.com/user/dowmeister_trucky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;during the last week i've worked on creating a small project as playground for site scraping + knowledge retrieval + vectors embedding and LLM text generation.&lt;/p&gt; &lt;p&gt;Basically I did this because i wanted to learn on my skin about LLM and KB bots but also because i have a KB site for my application with about 100 articles. After evaluated different AI bots on the market (with crazy pricing), I wanted to investigate directly what i could build.&lt;/p&gt; &lt;p&gt;Source code is available here: &lt;a href="https://github.com/dowmeister/kb-ai-bot"&gt;https://github.com/dowmeister/kb-ai-bot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Scrape recursively a site with a pluggable Site Scraper identifying the site type and applying the correct extractor for each type (currently Echo KB, Wordpress, Mediawiki and a Generic one)&lt;/p&gt; &lt;p&gt;- Create embeddings via HuggingFace MiniLM&lt;/p&gt; &lt;p&gt;- Store embeddings in QDrant&lt;/p&gt; &lt;p&gt;- Use vector search for retrieving affordable and matching content&lt;/p&gt; &lt;p&gt;- The content retrieved is used to generate a Context and a Prompt for an AI LLM and getting a natural language reply&lt;/p&gt; &lt;p&gt;- Multiple AI providers supported: Ollama, OpenAI, Claude, Cloudflare AI&lt;/p&gt; &lt;p&gt;- CLI console for asking questions&lt;/p&gt; &lt;p&gt;- Discord Bot with slash commands and automatic detection of questions\help requests&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;While the site scraping and embedding process is quite easy, having good results from LLM is another story.&lt;/p&gt; &lt;p&gt;OpenAI and Claude are good enough, Ollama has alternate replies depending on the model used, Cloudflare AI seems like Ollama but some models are really bad. Not tested on Amazon Bedrock.&lt;/p&gt; &lt;p&gt;If i would use Ollama in production, naturally the problem would be: where host Ollama at a reasonable price?&lt;/p&gt; &lt;p&gt;I'm searching for suggestions, comments, hints.&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dowmeister_trucky"&gt; /u/dowmeister_trucky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ke0etl/kbaibot_probably_another_bot_scraping_sites_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ke0etl/kbaibot_probably_another_bot_scraping_sites_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ke0etl/kbaibot_probably_another_bot_scraping_sites_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-03T19:00:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kdsv32</id>
    <title>The feature I hate the bug in Ollama</title>
    <updated>2025-05-03T13:22:45+00:00</updated>
    <author>
      <name>/u/Informal-Victory8655</name>
      <uri>https://old.reddit.com/user/Informal-Victory8655</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kdsv32/the_feature_i_hate_the_bug_in_ollama/"&gt; &lt;img alt="The feature I hate the bug in Ollama" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="The feature I hate the bug in Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The default ctx is 2048 even for the embeddings model loaded using langchain. I mean, the persons who don't deep dive into the things, can't see why they are not getting any good results by using an embeddings model that supports input sequence up to 8192. :/&lt;/p&gt; &lt;p&gt;I'm using &lt;a href="https://ollama.com/library/snowflake-arctic-embed2"&gt;snowflake-arctic-embed2&lt;/a&gt;, which supports 8192 length, but default set is 2048.&lt;/p&gt; &lt;p&gt;The reason I select &lt;a href="https://ollama.com/library/snowflake-arctic-embed2"&gt;snowflake-arctic-embed2&lt;/a&gt; is longer context length, so I can avoid chunking.&lt;/p&gt; &lt;p&gt;Its crucial to monitor and see every log of the application/model you are running, don't trust anything.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dcvnzbqpikye1.png?width=1131&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=83cc4017541e7d26c45093020136dab211f8f145"&gt;https://preview.redd.it/dcvnzbqpikye1.png?width=1131&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=83cc4017541e7d26c45093020136dab211f8f145&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Informal-Victory8655"&gt; /u/Informal-Victory8655 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdsv32/the_feature_i_hate_the_bug_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdsv32/the_feature_i_hate_the_bug_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kdsv32/the_feature_i_hate_the_bug_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-03T13:22:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ke9vl8</id>
    <title>ollama question : I cannot get system " If asked anything unrelated, respond with: ‘I only answer questions related." working</title>
    <updated>2025-05-04T02:52:40+00:00</updated>
    <author>
      <name>/u/jlsilicon9</name>
      <uri>https://old.reddit.com/user/jlsilicon9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have seen directions to specify :&lt;/p&gt; &lt;p&gt;SYSTEM &amp;quot;&lt;br /&gt; Only answer questions related to programming.&lt;br /&gt; If asked anything unrelated, respond with: `I only answer questions related to programming.'&lt;br /&gt; &amp;quot;&lt;/p&gt; &lt;p&gt;But, this does not seem to work.&lt;/p&gt; &lt;p&gt;If you specify the above in the Model :&lt;br /&gt; Then ask: &amp;quot;Tell me about daffy&amp;quot;&lt;br /&gt; ... it just explains about the character named daffy.&lt;/p&gt; &lt;p&gt;What am I missing ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jlsilicon9"&gt; /u/jlsilicon9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ke9vl8/ollama_question_i_cannot_get_system_if_asked/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ke9vl8/ollama_question_i_cannot_get_system_if_asked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ke9vl8/ollama_question_i_cannot_get_system_if_asked/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-04T02:52:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1kdyaq7</id>
    <title>How to move on from Ollama?</title>
    <updated>2025-05-03T17:28:52+00:00</updated>
    <author>
      <name>/u/jerasu_</name>
      <uri>https://old.reddit.com/user/jerasu_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been having so many problems with Ollama like Gemma3 performing worse than Gemma2 and Ollama getting stuck on some LLM calls or I have to restart ollama server once a day because it stops working. I wanna start using vLLM or llama.cpp but I couldn't make it work.vLLMt gives me &amp;quot;out of memory&amp;quot; error even though I have enough vramandt I couldn't figure out why llama.cpp won't work well. It is too slow like 5x slower than Ollama for me. I use a Linux machine with 2x 4070 Ti Super how can I stop using Ollama and make these other programs work? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jerasu_"&gt; /u/jerasu_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdyaq7/how_to_move_on_from_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdyaq7/how_to_move_on_from_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kdyaq7/how_to_move_on_from_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-03T17:28:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ked8x2</id>
    <title>Feeding tool output back to LLM</title>
    <updated>2025-05-04T06:25:17+00:00</updated>
    <author>
      <name>/u/Bradymodion</name>
      <uri>https://old.reddit.com/user/Bradymodion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;I'm trying to write a program that uses the tool calling API from ollama. There is plenty of information available on the way to inform the model about the tools and the format of the tool calls (the tool_calls array). All of this works. But: what do I do then? I want to return the tool call results back to the LLM. What is the proper format? An array as well? Or several messages, one for each called tool? If a tool gets called twice (didn't happen yet, but possible), how would I handle this? Greetings!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bradymodion"&gt; /u/Bradymodion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ked8x2/feeding_tool_output_back_to_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ked8x2/feeding_tool_output_back_to_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ked8x2/feeding_tool_output_back_to_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-04T06:25:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ken0os</id>
    <title>Trouble running Ollama with Intel Arc GPU – BSOD with VIDEO_SCHEDULER_INTERNAL_ERROR</title>
    <updated>2025-05-04T15:51:37+00:00</updated>
    <author>
      <name>/u/Mr_B1rd</name>
      <uri>https://old.reddit.com/user/Mr_B1rd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm trying to run Ollama using my Intel Arc GPU because it has more VRAM than my Nvidia card. Here's my setup:&lt;/p&gt; &lt;p&gt;Dell PC with a - Nvidia GPU with 8 GB VRAM - Intel Arc A770 GPU with 16 GB VRAM&lt;/p&gt; &lt;p&gt;I wanted to use the Intel GPU for Ollama, so I tried using the IPEX (Intel Extension for PyTorch) version of Ollama. However, every time I try to load a model, I get a bluescreen with the stopcode: VIDEO_SCHEDULER_INTERNAL_ERROR.&lt;/p&gt; &lt;p&gt;Has anyone run into this issue or know how to fix it? I'd really appreciate any help or pointers!&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_B1rd"&gt; /u/Mr_B1rd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ken0os/trouble_running_ollama_with_intel_arc_gpu_bsod/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ken0os/trouble_running_ollama_with_intel_arc_gpu_bsod/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ken0os/trouble_running_ollama_with_intel_arc_gpu_bsod/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-04T15:51:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kevih4</id>
    <title>New User: Can I add attachments for analysis to Ollama with WebUI?</title>
    <updated>2025-05-04T21:54:11+00:00</updated>
    <author>
      <name>/u/IamAlotOfMe</name>
      <uri>https://old.reddit.com/user/IamAlotOfMe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently downloaded and installed and my language model seemed to be outdated and I can't get current information from them that might be a separate problem but I'm trying to understand is there a way that I can add any attachment such as Excel sheets or Pdfs so I can analyze trading results and financial analysis?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IamAlotOfMe"&gt; /u/IamAlotOfMe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kevih4/new_user_can_i_add_attachments_for_analysis_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kevih4/new_user_can_i_add_attachments_for_analysis_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kevih4/new_user_can_i_add_attachments_for_analysis_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-04T21:54:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1keyg9k</id>
    <title>LLM not following instructions</title>
    <updated>2025-05-05T00:13:51+00:00</updated>
    <author>
      <name>/u/buttered-toasst</name>
      <uri>https://old.reddit.com/user/buttered-toasst</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am building this chatbot that uses streamlit for frontend and python with postgres for the backend, I have a vector table in my db with fragments so I can use RAG. I am trying to give memory to the bot and I found this approach that doesn't use any lanchain memory stuff and is to use the LLM to view a chat history and reformulate the user question. Like this, question -&amp;gt; first LLM -&amp;gt; reformulated question -&amp;gt; embedding and retrieval of documents in the db -&amp;gt; second LLM -&amp;gt; answer. The problem I'm facing is that the first LLM answers the question and it's not supposed to do it. I can't find a solution and If anyone wants to give me a hand, I'd really appreciate it.&lt;/p&gt; &lt;p&gt;This is the code if anybody could help:&lt;/p&gt; &lt;p&gt;from sentence_transformers import SentenceTransformer from fragmentsDAO import FragmentDAO from langchain.prompts import PromptTemplate from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain_core.messages import AIMessage, HumanMessage from langchain_community.chat_models import ChatOllama from langchain.schema.output_parser import StrOutputParser&lt;/p&gt; &lt;p&gt;class ChatOllamabot: def &lt;strong&gt;init&lt;/strong&gt;(self): self.model = SentenceTransformer(&amp;quot;all-mpnet-base-v2&amp;quot;) self.max_turns = 5&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def chat(self, question, memory): instruction_to_system = &amp;quot;&amp;quot;&amp;quot; Do NOT answer the question. Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question under ANY circumstance , just reformulate it if needed and otherwise return it as it is. Examples: 1.History: &amp;quot;Human: Wgat is a beginner friendly exercise that targets biceps? AI: A begginer friendly exercise that targets biceps is Concentration Curls?&amp;quot; Question: &amp;quot;Human: What are the steps to perform this exercise?&amp;quot; Output: &amp;quot;What are the steps to perform the Concentration Curls exercise?&amp;quot; 2.History: &amp;quot;Human: What is the category of bench press? AI: The category of bench press is strength.&amp;quot; Question: &amp;quot;Human: What are the steps to perform the child pose exercise?&amp;quot; Output: &amp;quot;What are the steps to perform the child pose exercise?&amp;quot; &amp;quot;&amp;quot;&amp;quot; llm = ChatOllama(model=&amp;quot;llama3.2&amp;quot;, temperature=0) question_maker_prompt = ChatPromptTemplate.from_messages( [ (&amp;quot;system&amp;quot;, instruction_to_system), MessagesPlaceholder(variable_name=&amp;quot;chat_history&amp;quot;), (&amp;quot;human&amp;quot;, &amp;quot;{question}&amp;quot;), ] ) question_chain = question_maker_prompt | llm | StrOutputParser() newQuestion = question_chain.invoke({&amp;quot;question&amp;quot;: question, &amp;quot;chat_history&amp;quot;: memory}) actual_question = self.contextualized_question(memory, newQuestion, question) emb = self.model.encode(actual_question) dao = FragmentDAO() fragments = dao.getFragments(str(emb.tolist())) context = [f[3] for f in fragments] for f in fragments: context.append(f[3]) documents = &amp;quot;\n\n---\n\n&amp;quot;.join(c for c in context) prompt = PromptTemplate( template=&amp;quot;&amp;quot;&amp;quot;You are an assistant for question answering tasks. Use the following documents to answer the question. If you dont know the answers, just say that you dont know. Use five sentences maximum and keep the answer concise: Documents: {documents} Question: {question} Answer:&amp;quot;&amp;quot;&amp;quot;, input_variables=[&amp;quot;documents&amp;quot;, &amp;quot;question&amp;quot;], ) llm = ChatOllama(model=&amp;quot;llama3.2&amp;quot;, temperature=0) rag_chain = prompt | llm | StrOutputParser() answer = rag_chain.invoke({ &amp;quot;question&amp;quot;: actual_question, &amp;quot;documents&amp;quot;: documents, }) # Keep only the last N turns (each turn = 2 messages) if len(memory) &amp;gt; 2 * self.max_turns: memory = memory[-2 * self.max_turns:] # Add new interaction as direct messages memory.append( HumanMessage(content=actual_question)) memory.append( AIMessage(content=answer)) print(newQuestion + &amp;quot; -&amp;gt; &amp;quot; + answer) for interactions in memory: print(interactions) print() return answer, memory def contextualized_question(self, chat_history, new_question, question): if chat_history: return new_question else: return question &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/buttered-toasst"&gt; /u/buttered-toasst &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1keyg9k/llm_not_following_instructions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1keyg9k/llm_not_following_instructions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1keyg9k/llm_not_following_instructions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-05T00:13:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1keoxav</id>
    <title>UI-Tars-1.5 reasoning never fails to entertain me.</title>
    <updated>2025-05-04T17:12:18+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1keoxav/uitars15_reasoning_never_fails_to_entertain_me/"&gt; &lt;img alt="UI-Tars-1.5 reasoning never fails to entertain me." src="https://preview.redd.it/ugrfilvlssye1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e9b3d998fa1d7c1ac8db262966dd43f77e7e9a6f" title="UI-Tars-1.5 reasoning never fails to entertain me." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;7B parameter computer use agent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ugrfilvlssye1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1keoxav/uitars15_reasoning_never_fails_to_entertain_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1keoxav/uitars15_reasoning_never_fails_to_entertain_me/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-04T17:12:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1kemlk9</id>
    <title>Run AI Agents with Near-Native Speed on macOS—Introducing C/ua</title>
    <updated>2025-05-04T15:33:32+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share an exciting open-source framework called C/ua, specifically optimized for Apple Silicon Macs. C/ua allows AI agents to seamlessly control entire operating systems running inside high-performance, lightweight virtual containers.&lt;/p&gt; &lt;p&gt;Key Highlights:&lt;/p&gt; &lt;p&gt;Performance: Achieves up to 97% of native CPU speed on Apple Silicon. Compatibility: Works smoothly with any AI language model. Open Source: Fully available on GitHub for customization and community contributions.&lt;/p&gt; &lt;p&gt;Whether you're into automation, AI experimentation, or just curious about pushing your Mac's capabilities, check it out here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts and see what innovative use cases the macOS community can come up with!&lt;/p&gt; &lt;p&gt;Happy hacking!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kemlk9/run_ai_agents_with_nearnative_speed_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kemlk9/run_ai_agents_with_nearnative_speed_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kemlk9/run_ai_agents_with_nearnative_speed_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-04T15:33:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf371s</id>
    <title>LLM finetuning</title>
    <updated>2025-05-05T04:34:23+00:00</updated>
    <author>
      <name>/u/Unique_Yogurtcloset8</name>
      <uri>https://old.reddit.com/user/Unique_Yogurtcloset8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Given 22 image+JSON datasets that are mostly similar, what is the most cost-effective and time-efficient approach for LLM fine-tuning?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Train using all 22 datasets at once.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Train each dataset one by one in a sequential manner.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Start by training on the first dataset, and for subsequent training rounds, use a mixed sample: 20% from previously seen datasets and 80% from the current one.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unique_Yogurtcloset8"&gt; /u/Unique_Yogurtcloset8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kf371s/llm_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kf371s/llm_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kf371s/llm_finetuning/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-05T04:34:23+00:00</published>
  </entry>
</feed>
