<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-08T04:06:46+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ijqoco</id>
    <title>One Quadro RTX 5000, or Two Quadro RTX 4000, for deepseek-coder-v2?</title>
    <updated>2025-02-07T08:51:57+00:00</updated>
    <author>
      <name>/u/alsutton</name>
      <uri>https://old.reddit.com/user/alsutton</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking to run &lt;a href="https://ollama.com/library/deepseek-coder-v2"&gt;deepseek-coder-v2&lt;/a&gt; locally (yes, I'm late to the game on that one), and it looks like the 16b param model won't fit into a single RTX 4000. My local eBay pricing is such that a single Turing based RTX 5000 is roughly 1.8 times the price of a Turing based RTX 4000, so I'm wondering if I should just spend a little extra, get 2xRTX 4000s, use them in my motherboards two 16 lane PCIe 3.0 slots, and bask in the joy of a lot more CUDA cores and the same RAM.&lt;/p&gt; &lt;p&gt;Is that reasonable, or is ollama limited to a single card for memory and/or processing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alsutton"&gt; /u/alsutton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijqoco/one_quadro_rtx_5000_or_two_quadro_rtx_4000_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijqoco/one_quadro_rtx_5000_or_two_quadro_rtx_4000_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijqoco/one_quadro_rtx_5000_or_two_quadro_rtx_4000_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T08:51:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijh27y</id>
    <title>üìùüßµ Introducing Text Loom: A Node-Based Text Processing Playground!</title>
    <updated>2025-02-06T23:39:49+00:00</updated>
    <author>
      <name>/u/kleer001</name>
      <uri>https://old.reddit.com/user/kleer001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;&lt;a href="https://github.com/kleer001/Text_Loom"&gt;TEXT LOOM!&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/kleer001/Text_Loom"&gt;https://github.com/kleer001/Text_Loom&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey text wranglers! üëã Ever wanted to slice, dice, and weave text like a digital textile artist? &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/kleer001/Text_Loom/blob/main/images/leaderloop_trim_4.gif?raw=true"&gt;https://github.com/kleer001/Text_Loom/blob/main/images/leaderloop_trim_4.gif?raw=true&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Text Loom is your new best friend! It's a &lt;strong&gt;node-based workspace&lt;/strong&gt; where you can build awesome text processing pipelines by connecting simple, powerful nodes. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Want to split a script into scenes? Done. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Need to process a batch of files through an LLM? Easy peasy. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;How about automatically formatting numbered lists or merging multiple documents? We've got you covered! &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each node is like a tiny text-processing specialist: the &lt;a href="https://github.com/kleer001/Text_Loom/wiki/Section-Node"&gt;Section Node&lt;/a&gt; slices text based on patterns, the &lt;a href="https://github.com/kleer001/Text_Loom/wiki/Query-Node"&gt;Query Node&lt;/a&gt; talks to AI models, and the &lt;a href="https://github.com/kleer001/Text_Loom/wiki/Looper-Node"&gt;Looper Node&lt;/a&gt; handles all your iteration needs. &lt;/p&gt; &lt;p&gt;Mix and match to create your perfect text processing flow! Check out our &lt;a href="https://github.com/kleer001/Text_Loom/wiki"&gt;wiki&lt;/a&gt; to see what's possible. üöÄ&lt;/p&gt; &lt;h2&gt;Why Terminal? Because Hackers Know Best! üíª&lt;/h2&gt; &lt;p&gt;Remember those awesome 1900's movies where hackers typed furiously on glowing green screens, making magic happen with just their keyboards? &lt;/p&gt; &lt;p&gt;&lt;em&gt;Turns out they were onto something!&lt;/em&gt; &lt;/p&gt; &lt;p&gt;While Text Loom's got a cool node-based interface, it's running on good old-fashioned terminal power. Just like Matthew Broderick in &lt;em&gt;WarGames&lt;/em&gt; or the crew in &lt;em&gt;Hackers&lt;/em&gt;, we're keeping it real with that sweet, sweet command line efficiency. No fancy GUI bloat, no mouse-hunting required ‚Äì just you, your keyboard, and pure text-processing power. Want to feel like you're hacking the Gibson while actually getting real work done? We've got you covered! üïπÔ∏è&lt;/p&gt; &lt;p&gt;&lt;em&gt;Because text should flow, not fight you.&lt;/em&gt; ‚ú®&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kleer001"&gt; /u/kleer001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijh27y/introducing_text_loom_a_nodebased_text_processing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijh27y/introducing_text_loom_a_nodebased_text_processing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijh27y/introducing_text_loom_a_nodebased_text_processing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T23:39:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijmfao</id>
    <title>Can I run 70b with my RTX5090 (32GB GDDR7 VRAM) and 64GB DDR5-6000?</title>
    <updated>2025-02-07T04:10:02+00:00</updated>
    <author>
      <name>/u/AzysLla</name>
      <uri>https://old.reddit.com/user/AzysLla</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;CPU is 9800X3D.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AzysLla"&gt; /u/AzysLla &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijmfao/can_i_run_70b_with_my_rtx5090_32gb_gddr7_vram_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijmfao/can_i_run_70b_with_my_rtx5090_32gb_gddr7_vram_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijmfao/can_i_run_70b_with_my_rtx5090_32gb_gddr7_vram_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T04:10:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijtt8v</id>
    <title>Actually Benefiting from Structured Output Support with Ollama and LangChainJS</title>
    <updated>2025-02-07T12:25:00+00:00</updated>
    <author>
      <name>/u/Inevitable-Judge2642</name>
      <uri>https://old.reddit.com/user/Inevitable-Judge2642</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ijtt8v/actually_benefiting_from_structured_output/"&gt; &lt;img alt="Actually Benefiting from Structured Output Support with Ollama and LangChainJS" src="https://external-preview.redd.it/dtRSrdeN5WqSxGmu3zNM8x-79n94KvvGaZcUGyRp3w8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0317bb9986fdd1b1a3a33c47f4aa6b82315f80a6" title="Actually Benefiting from Structured Output Support with Ollama and LangChainJS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Judge2642"&gt; /u/Inevitable-Judge2642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://k33g.hashnode.dev/actually-benefiting-from-structured-output-support-with-ollama-and-langchainjs"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijtt8v/actually_benefiting_from_structured_output/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijtt8v/actually_benefiting_from_structured_output/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T12:25:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1iju74a</id>
    <title>Sharing Ollama models between users in macOS?</title>
    <updated>2025-02-07T12:47:37+00:00</updated>
    <author>
      <name>/u/joyfulsparrow</name>
      <uri>https://old.reddit.com/user/joyfulsparrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a way to shared Ollama models between users? They're pretty big, so I don't want to fill up the hard disk with duplicates. Can I put them in '/Users/Shared'?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joyfulsparrow"&gt; /u/joyfulsparrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iju74a/sharing_ollama_models_between_users_in_macos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iju74a/sharing_ollama_models_between_users_in_macos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iju74a/sharing_ollama_models_between_users_in_macos/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T12:47:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijuah4</id>
    <title>How to Handle Missing Parameters and Chained Tool Calls in LangChain with Ollama Llama 3.2:8B?</title>
    <updated>2025-02-07T12:52:56+00:00</updated>
    <author>
      <name>/u/MindIndividual4397</name>
      <uri>https://old.reddit.com/user/MindIndividual4397</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve built a simple call tool setup using Ollama Llama 3.2:8B and LangChain, but I‚Äôm facing some issues when calling tools that depend on each other.&lt;/p&gt; &lt;p&gt;Problem 1: Handling Missing Parameters&lt;/p&gt; &lt;p&gt;I have a tool user_status(user_id: int), which requires an integer user ID. However, when I say something like:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Check user status for test&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;LangChain doesn‚Äôt detect an integer in the prompt and instead assigns a random user ID like 1 or 1234.&lt;/p&gt; &lt;p&gt;How can I make it force the user to provide a user ID explicitly, instead of assuming a random one? Ideally, it should either ask for the missing parameter or refuse execution.&lt;/p&gt; &lt;p&gt;Problem 2: Automatically Resolving Dependencies&lt;/p&gt; &lt;p&gt;I also have another tool:&lt;/p&gt; &lt;p&gt;get_user_id(username: str) -&amp;gt; int&lt;/p&gt; &lt;p&gt;I want the system to automatically call get_user_id(&amp;quot;test&amp;quot;) first and use the returned value as input for user_status(user_id).&lt;/p&gt; &lt;p&gt;Do I need to implement a custom agent executor for this? If so, how can I handle similar cases when multiple tools depend on each other?&lt;/p&gt; &lt;p&gt;Would love to hear your approaches! Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MindIndividual4397"&gt; /u/MindIndividual4397 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijuah4/how_to_handle_missing_parameters_and_chained_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijuah4/how_to_handle_missing_parameters_and_chained_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijuah4/how_to_handle_missing_parameters_and_chained_tool/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T12:52:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijnqb9</id>
    <title>VRAM usage is still high after running /bye, how to release that capacity?</title>
    <updated>2025-02-07T05:25:28+00:00</updated>
    <author>
      <name>/u/SpecialistPear755</name>
      <uri>https://old.reddit.com/user/SpecialistPear755</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was running llama 3.2 3b on a laptop, u ubuntu.&lt;/p&gt; &lt;p&gt;After I run /bye and closed the termina, the Vram usage is still high like the model is still running. (Around 4or5 g)&lt;/p&gt; &lt;p&gt;Is there a way to release that burden?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpecialistPear755"&gt; /u/SpecialistPear755 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijnqb9/vram_usage_is_still_high_after_running_bye_how_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijnqb9/vram_usage_is_still_high_after_running_bye_how_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijnqb9/vram_usage_is_still_high_after_running_bye_how_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T05:25:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijspg3</id>
    <title>Is it normal for ollama to use CPU when OLLAMA_KEEP_ALIVE=-1</title>
    <updated>2025-02-07T11:15:47+00:00</updated>
    <author>
      <name>/u/gmetothemoongodspeed</name>
      <uri>https://old.reddit.com/user/gmetothemoongodspeed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm using the Windows client and when setting OLLAMA_KEEP_ALIVE=-1 my CPU usage doesn‚Äôt stop at the end of the query. Is this normal? I would say it uses CPU for approximately 5 minutes after the query ends. Then the CPU drops to minimal as expected.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gmetothemoongodspeed"&gt; /u/gmetothemoongodspeed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijspg3/is_it_normal_for_ollama_to_use_cpu_when_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijspg3/is_it_normal_for_ollama_to_use_cpu_when_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijspg3/is_it_normal_for_ollama_to_use_cpu_when_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T11:15:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijxthe</id>
    <title>Ollama setup: GPU load fails.</title>
    <updated>2025-02-07T15:39:30+00:00</updated>
    <author>
      <name>/u/kayakyakr</name>
      <uri>https://old.reddit.com/user/kayakyakr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Final update, for posterity: If you copy/paste a docker_compose.yml file off of the internet and are using an nvidia GPU, make sure you are using the ollama/ollama docker image instead of ollama/ollama:rcom. Hope that this helps someone searching for this issue discover the fix.&lt;/p&gt; &lt;p&gt;&lt;del&gt;Local LLM newb, but not server newb. Been trying to bring ollama up on my server to mess around with. Have it running in a proxmox LXC container, docker hosted, with nvidia-container-toolkit working as expected. I've tested the easy nvidia-smi container, as well as put it through its paces using the dockerized gpu_burn project. Same setup works as a gaming server with the same GPU.&lt;/del&gt;&lt;/p&gt; &lt;p&gt;edit2: a ha. I had copied a compose that was installing rocm, which is for amd processors &amp;gt;_&amp;lt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;edit: I found something that seems weird:&lt;/del&gt; &lt;code&gt; time=2025-02-07T17:00:57.303Z level=INFO source=routes.go:1267 msg=&amp;quot;Dynamic LLM libraries&amp;quot; runners=&amp;quot;[cpu cpu_avx cpu_avx2 rocm_avx]&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;returns only CPU runners, there's no cuda_vXX runner available there like I've seen in other logs&lt;/del&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;old:&lt;/del&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;Ollama finds the GPU and &lt;code&gt;ollama ps&lt;/code&gt; even gives a result of &lt;code&gt;100% GPU&lt;/code&gt; for the loaded model.&lt;/del&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;Best I can tell, these are the relevant lines where it fails to load into GPU and instead switches to CPU:&lt;/del&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt; ollama | time=2025-02-07T05:51:38.953Z level=INFO source=memory.go:356 msg=&amp;quot;offload to cuda&amp;quot; layers.requested=-1 layers.model=29 layers.offload=29 layers.split=&amp;quot;&amp;quot; memory.available=&amp;quot;\[7.7 GiB\]&amp;quot; memory.gpu\_overhead=&amp;quot;0 B&amp;quot; memory.required.full=&amp;quot;2.5 GiB&amp;quot; memory.required.partial=&amp;quot;2.5 GiB&amp;quot; memory.required.kv=&amp;quot;224.0 MiB&amp;quot; memory.required.allocations=&amp;quot;\[2.5 GiB\]&amp;quot; memory.weights.total=&amp;quot;1.5 GiB&amp;quot; memory.weights.repeating=&amp;quot;1.3 GiB&amp;quot; memory.weights.nonrepeating=&amp;quot;236.5 MiB&amp;quot; memory.graph.full=&amp;quot;299.8 MiB&amp;quot; memory.graph.partial=&amp;quot;482.3 MiB&amp;quot; ollama | time=2025-02-07T05:51:38.954Z level=INFO source=server.go:376 msg=&amp;quot;starting llama server&amp;quot; cmd=&amp;quot;/usr/lib/ollama/runners/cpu\_avx2/ollama\_llama\_server runner --model /root/.ollama/models/blobs/sha256-4c132839f93a189e3d8fa196e3324adf94335971104a578470197ea7e11d8e70 --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 28 --parallel 4 --port 39375&amp;quot; ollama | time=2025-02-07T05:51:38.955Z level=INFO source=sched.go:449 msg=&amp;quot;loaded runners&amp;quot; count=2 ollama | time=2025-02-07T05:51:38.955Z level=INFO source=server.go:555 msg=&amp;quot;waiting for llama runner to start responding&amp;quot; ollama | time=2025-02-07T05:51:38.956Z level=INFO source=server.go:589 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server error&amp;quot; ollama | time=2025-02-07T05:51:38.966Z level=INFO source=runner.go:936 msg=&amp;quot;starting go runner&amp;quot; ollama | time=2025-02-07T05:51:38.971Z level=INFO source=runner.go:937 msg=system info=&amp;quot;CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | AARCH64\_REPACK = 1 | cgo(gcc)&amp;quot; threads=28 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;I see the line with &amp;quot;llm server error&amp;quot; but for the life of me, I haven't been able to figure out where I might find that error. Adding OLLAMA_DEBUG doesn't add anything illuminating:&lt;/del&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt; ollama | time=2025-02-07T15:31:26.233Z level=DEBUG source=gpu.go:713 msg=&amp;quot;no filter required for library cpu&amp;quot; ollama | time=2025-02-07T15:31:26.234Z level=INFO source=server.go:376 msg=&amp;quot;starting llama server&amp;quot; cmd=&amp;quot;/usr/lib/ollama/runners/cpu\_avx2/ollama\_llama\_server runner --model /root/.ollama/models/blobs/sha256-4c132839f93a189e3d8fa196e3324adf94335971104a578470197ea7e11d8e70 --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --verbose --threads 28 --parallel 4 --port 41131&amp;quot; ollama | time=2025-02-07T15:31:26.234Z level=DEBUG source=server.go:393 msg=subprocess environment=&amp;quot;\[PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HSA\_OVERRIDE\_GFX\_VERSION='9.0.0' CUDA\_ERROR\_LEVEL=50 LD\_LIBRARY\_PATH=/usr/lib/ollama:/usr/lib/ollama:/usr/lib/ollama/runners/cpu\_avx2\]&amp;quot; ollama | time=2025-02-07T15:31:26.235Z level=INFO source=sched.go:449 msg=&amp;quot;loaded runners&amp;quot; count=1 ollama | time=2025-02-07T15:31:26.235Z level=DEBUG source=sched.go:575 msg=&amp;quot;evaluating already loaded&amp;quot; model=/root/.ollama/models/blobs/sha256-4c132839f93a189e3d8fa196e3324adf94335971104a578470197ea7e11d8e70 ollama | time=2025-02-07T15:31:26.235Z level=INFO source=server.go:555 msg=&amp;quot;waiting for llama runner to start responding&amp;quot; ollama | time=2025-02-07T15:31:26.235Z level=INFO source=server.go:589 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server error&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;host dmesg doesn't contain any error messages. /dev/nvidia-uvm is passed through to all levels.&lt;/del&gt;&lt;/p&gt; &lt;p&gt;&lt;del&gt;Open to any suggestions that might shed light on the mystery error that's keeping me from using my GPU.&lt;/del&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kayakyakr"&gt; /u/kayakyakr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijxthe/ollama_setup_gpu_load_fails/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijxthe/ollama_setup_gpu_load_fails/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijxthe/ollama_setup_gpu_load_fails/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T15:39:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij7nuo</id>
    <title>UNCENSORED AI MODELS</title>
    <updated>2025-02-06T17:12:07+00:00</updated>
    <author>
      <name>/u/yng_kydd</name>
      <uri>https://old.reddit.com/user/yng_kydd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some months ago i tried for the first time wizard vicuna and i was ok with it being a lil slow and not that optimized, i wasn't even complaining cause at least i had an AI uncensored, something i could ask for everything.&lt;/p&gt; &lt;p&gt;This week i've seen a post talking about other new models that are pretty much better like tiger gemma, dolphin and others&lt;/p&gt; &lt;p&gt;i've been searching about this for quite a lot and i'd want to ask y'all what is the best uncensored AI model right now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yng_kydd"&gt; /u/yng_kydd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij7nuo/uncensored_ai_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij7nuo/uncensored_ai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij7nuo/uncensored_ai_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T17:12:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijye71</id>
    <title>Some good options for a deepseek r1 local interface?</title>
    <updated>2025-02-07T16:03:22+00:00</updated>
    <author>
      <name>/u/Game-Lover44</name>
      <uri>https://old.reddit.com/user/Game-Lover44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im using windows sadly, i need something with a Internet search and is light but is also totally free/local.&lt;/p&gt; &lt;p&gt;What are some local choices i should look into that would work great with r1?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Game-Lover44"&gt; /u/Game-Lover44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijye71/some_good_options_for_a_deepseek_r1_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijye71/some_good_options_for_a_deepseek_r1_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijye71/some_good_options_for_a_deepseek_r1_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T16:03:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij2pw7</id>
    <title>üéâ Being Thankful for Everyone Who Made This Project a Super Hit! üöÄ</title>
    <updated>2025-02-06T13:37:59+00:00</updated>
    <author>
      <name>/u/akhilpanja</name>
      <uri>https://old.reddit.com/user/akhilpanja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are thrilled to announce that our project, DeepSeek-RAG-Chatbot, has officially hit 100 stars on GitHub repo: &lt;a href="https://github.com/SaiAkhil066/DeepSeek-RAG-Chatbot.git"&gt;https://github.com/SaiAkhil066/DeepSeek-RAG-Chatbot.git&lt;/a&gt; üåü‚ú® &lt;/p&gt; &lt;p&gt;This journey has been incredible, and we couldn‚Äôt have achieved this milestone without the support of our amazing community. Your contributions, feedback, and enthusiasm have helped shape this project into what it is today!&lt;/p&gt; &lt;p&gt;üîç Performance Boost The graph above showcases the significant improvements in Graph Context Relevancy and Graph Context Recall after integrating GraphRAG and further advancements. Our system is now more accurate, contextually aware, and efficient in retrieving relevant information.&lt;/p&gt; &lt;p&gt;We are committed to making this project even better and look forward to the next milestones! üöÄ&lt;/p&gt; &lt;p&gt;Thank you all once again for being part of this journey. Let‚Äôs keep building together! üí°üî• Ôøº&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akhilpanja"&gt; /u/akhilpanja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij2pw7/being_thankful_for_everyone_who_made_this_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij2pw7/being_thankful_for_everyone_who_made_this_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij2pw7/being_thankful_for_everyone_who_made_this_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T13:37:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijs34u</id>
    <title>LLMs as Embeddings?</title>
    <updated>2025-02-07T10:33:09+00:00</updated>
    <author>
      <name>/u/Better-Designer-8904</name>
      <uri>https://old.reddit.com/user/Better-Designer-8904</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ijs34u/llms_as_embeddings/"&gt; &lt;img alt="LLMs as Embeddings?" src="https://b.thumbs.redditmedia.com/ytaJOU70XQ_OdGIs6ff3q5cDHpWWu6pRAi8DYLo9CwY.jpg" title="LLMs as Embeddings?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/pu81il0r2phe1.png?width=832&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=539f8ec6328553c328a6bced0c730e87952fb5ba"&gt;https://preview.redd.it/pu81il0r2phe1.png?width=832&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=539f8ec6328553c328a6bced0c730e87952fb5ba&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been using LangChain to run LLMs as embeddings through Ollama, and it actually works pretty well. But I‚Äôm kinda wondering‚Ä¶ how does it actually work? And does it even make sense to use an LLM for embeddings instead of a dedicated model?&lt;/p&gt; &lt;p&gt;If anyone understands the details, I‚Äôd love an explanation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Better-Designer-8904"&gt; /u/Better-Designer-8904 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijs34u/llms_as_embeddings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijs34u/llms_as_embeddings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijs34u/llms_as_embeddings/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T10:33:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijm8m0</id>
    <title>Dora - Local Drive Semantic Search</title>
    <updated>2025-02-07T03:59:52+00:00</updated>
    <author>
      <name>/u/ranoutofusernames__</name>
      <uri>https://old.reddit.com/user/ranoutofusernames__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Sharing Dora, an alternative to the Mac Explorer app that I wrote today so you can retrieve files using natural language. It runs a local crawler at the target directory to index file names and paths recursively, embeds them and then lets you retrieve them using a chat window (semantic search). You can then open the files directly from the results as well.&lt;/p&gt; &lt;p&gt;It runs completely local and no data is sent out.&lt;/p&gt; &lt;p&gt;Adding file content embedding for plaintext, PDFs and images on the next update for even better results. The goal is to do deep-research with local files eventually.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/space0blaster/dora"&gt;https://github.com/space0blaster/dora&lt;/a&gt;&lt;/p&gt; &lt;p&gt;License: MIT&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ranoutofusernames__"&gt; /u/ranoutofusernames__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijm8m0/dora_local_drive_semantic_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijm8m0/dora_local_drive_semantic_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijm8m0/dora_local_drive_semantic_search/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T03:59:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik4grv</id>
    <title>Is there something similar to operator what runs locally?</title>
    <updated>2025-02-07T20:13:25+00:00</updated>
    <author>
      <name>/u/Anyusername7294</name>
      <uri>https://old.reddit.com/user/Anyusername7294</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would love to try operator, but $200/month is too much for me. Also I don't want to give access to my entire computer to OpenAI. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anyusername7294"&gt; /u/Anyusername7294 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik4grv/is_there_something_similar_to_operator_what_runs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik4grv/is_there_something_similar_to_operator_what_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ik4grv/is_there_something_similar_to_operator_what_runs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T20:13:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik4xrz</id>
    <title>Help picking a GPU</title>
    <updated>2025-02-07T20:33:38+00:00</updated>
    <author>
      <name>/u/phantom6047</name>
      <uri>https://old.reddit.com/user/phantom6047</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking to start messing around with llms and ollama and need to purchase a gpu for my machine. I am running a Precision t7810 with dual E5-2690 cpus and 256gb 2400 ECC ram. The psu in this machine has only one free 8 pin connector and I originally hoped to purchase a 4070 as that seemed to be my best option, but I've realized that getting ahold of a 4070 is practically impossible. There's no used market around me with anything nvidia for sale so that's out too. I'm hoping to get something with lots of vram that will also hold up well for some light 2k gaming, and I've pretty much settled on a 7800xt. &lt;/p&gt; &lt;p&gt;I run arch on my systems and whatever gpu I get will be passed through to a windows vm for gaming or another arch vm/docker configuration for llms. &lt;/p&gt; &lt;p&gt;At this point I'm about to pull the trigger on a newegg deal for a 7800xt and psu for $550, pretty much maxing out my budget. I'm looking to hear your thoughts on how well this would or wouldn't work and if I should consider something else. Look forward to your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phantom6047"&gt; /u/phantom6047 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik4xrz/help_picking_a_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik4xrz/help_picking_a_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ik4xrz/help_picking_a_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T20:33:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijwoxn</id>
    <title>Exposing ollamas 11434 port for api use</title>
    <updated>2025-02-07T14:50:30+00:00</updated>
    <author>
      <name>/u/epigen01</name>
      <uri>https://old.reddit.com/user/epigen01</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys ive been using ngrok (free) for use on my homelab but the monthly limit for http requests was just hit ( i didnt know about that).&lt;/p&gt; &lt;p&gt;Any free alternatives to ngrok? Ideally something easy (otherwise i might have to use tailscale)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/epigen01"&gt; /u/epigen01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijwoxn/exposing_ollamas_11434_port_for_api_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijwoxn/exposing_ollamas_11434_port_for_api_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijwoxn/exposing_ollamas_11434_port_for_api_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T14:50:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikb7nq</id>
    <title>New Proyect...</title>
    <updated>2025-02-08T01:11:57+00:00</updated>
    <author>
      <name>/u/tech215</name>
      <uri>https://old.reddit.com/user/tech215</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Would it be a good idea to create something lightweight and would you use it?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/poll/1ikb7nq"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tech215"&gt; /u/tech215 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikb7nq/new_proyect/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikb7nq/new_proyect/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ikb7nq/new_proyect/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T01:11:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik6nml</id>
    <title>My Chat App Supports DeepSeek-R1 &amp; Works on All Platforms Now [Open Source]</title>
    <updated>2025-02-07T21:46:10+00:00</updated>
    <author>
      <name>/u/pozitronx</name>
      <uri>https://old.reddit.com/user/pozitronx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ik6nml/my_chat_app_supports_deepseekr1_works_on_all/"&gt; &lt;img alt="My Chat App Supports DeepSeek-R1 &amp;amp; Works on All Platforms Now [Open Source]" src="https://external-preview.redd.it/kifZ0Ldmdn6gGLz2rDPo3mLeb0SO0q-89k2FTXEJFUo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=865102b36718797f6b74ce3c7a0bbe6bfb735df2" title="My Chat App Supports DeepSeek-R1 &amp;amp; Works on All Platforms Now [Open Source]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, last month I share my app &lt;strong&gt;Reins: Chat for Ollama&lt;/strong&gt;. It simplifies configurations of conversations like customizing system prompt per chat or tweaking advanced options. Now, it shows DeepSeek-R1 thought messages separately and runs on iOS, Android, macOS and Windows. You can learn more from &lt;a href="https://github.com/ibrahimcetin/reins"&gt;GitHub&lt;/a&gt; and the &lt;a href="https://www.reddit.com/r/ollama/comments/1hv1vlh/opensource_app_for_easy_ollama_chat_configuration/"&gt;previous post&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://apps.apple.com/tr/app/reins-chat-for-ollama/id6739738501"&gt;iOS App&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ibrahimcetin/reins/releases/tag/1.2.0"&gt;Android App&lt;/a&gt; (I need testers for Google Play. If you send me your Google Play email, I will add you testers and send the link of the app. Your help is much appreciated.)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ibrahimcetin/reins/releases/tag/1.2.0"&gt;Windows App&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ibrahimcetin/reins/releases/tag/1.2.0"&gt;MacOS App&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ibrahimcetin/reins"&gt;GitHub Link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1mpito51fshe1.png?width=3618&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09b0d28f7e5c9dd3586733fd1048c135bcc1edca"&gt;https://preview.redd.it/1mpito51fshe1.png?width=3618&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09b0d28f7e5c9dd3586733fd1048c135bcc1edca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j7d859g0cshe1.png?width=2544&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3238936ae201b52c15ca39de17ac01ff3c109625"&gt;https://preview.redd.it/j7d859g0cshe1.png?width=2544&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3238936ae201b52c15ca39de17ac01ff3c109625&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/453hndz0cshe1.png?width=2546&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31c585b337a6dbaf7071132b3ffad73334da3013"&gt;https://preview.redd.it/453hndz0cshe1.png?width=2546&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31c585b337a6dbaf7071132b3ffad73334da3013&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pozitronx"&gt; /u/pozitronx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik6nml/my_chat_app_supports_deepseekr1_works_on_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik6nml/my_chat_app_supports_deepseekr1_works_on_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ik6nml/my_chat_app_supports_deepseekr1_works_on_all/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T21:46:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik1p0d</id>
    <title>Local Cursor.ai</title>
    <updated>2025-02-07T18:18:19+00:00</updated>
    <author>
      <name>/u/Kind_Ad_2866</name>
      <uri>https://old.reddit.com/user/Kind_Ad_2866</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since cursor only supports online models such as Claude and OpenAI, I‚Äôm surprised no one has created an alternative for local models yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kind_Ad_2866"&gt; /u/Kind_Ad_2866 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik1p0d/local_cursorai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik1p0d/local_cursorai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ik1p0d/local_cursorai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T18:18:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik8i7x</id>
    <title>What type of models can my machine run, any coding models?</title>
    <updated>2025-02-07T23:06:14+00:00</updated>
    <author>
      <name>/u/Game-Lover44</name>
      <uri>https://old.reddit.com/user/Game-Lover44</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ik8i7x/what_type_of_models_can_my_machine_run_any_coding/"&gt; &lt;img alt="What type of models can my machine run, any coding models?" src="https://b.thumbs.redditmedia.com/4U4r8A_jnxDMPM5aPlZjCwocQ_o7ifejWkeDLm6HnWQ.jpg" title="What type of models can my machine run, any coding models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im mostly looking for a model for coding and general questions. im just not sure what the largest model i can run is while still having a ok speed. any suggestions would be great, also i know my machine isint the greatest thing out there. Ive tried some 7b models or less but i feel they are not powerful enough.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5mftuja6tshe1.png?width=373&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f94fc8466e34c449a1f649fa53d8b28ebe023503"&gt;https://preview.redd.it/5mftuja6tshe1.png?width=373&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f94fc8466e34c449a1f649fa53d8b28ebe023503&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z3s2cyi6tshe1.png?width=374&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cff5d8bdb2d9f0fc303a8f90c7d06320159943ca"&gt;https://preview.redd.it/z3s2cyi6tshe1.png?width=374&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cff5d8bdb2d9f0fc303a8f90c7d06320159943ca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5ojhh3q6tshe1.png?width=403&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e5bbcb30bd63218d5bc0d482c877a66309f9aae3"&gt;https://preview.redd.it/5ojhh3q6tshe1.png?width=403&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e5bbcb30bd63218d5bc0d482c877a66309f9aae3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Game-Lover44"&gt; /u/Game-Lover44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik8i7x/what_type_of_models_can_my_machine_run_any_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik8i7x/what_type_of_models_can_my_machine_run_any_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ik8i7x/what_type_of_models_can_my_machine_run_any_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T23:06:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik3m98</id>
    <title>How do I make chatting about documents not suck?</title>
    <updated>2025-02-07T19:37:51+00:00</updated>
    <author>
      <name>/u/cunasmoker69420</name>
      <uri>https://old.reddit.com/user/cunasmoker69420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;: Asking various 22b-32b sized models questions about an insurance policy document that is about 40 pages long&lt;/p&gt; &lt;p&gt;The various models I've tried mostly fail miserably, often telling me the information I'm looking for is not in the document (it is) or returning incomplete information. &lt;/p&gt; &lt;p&gt;I'm assuming I'm doing something wrong since other people rave about using their local LLMs for document analysis. I guess its probably not as simple as uploading documents and asking away, so I'm grateful for any advice&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Models tried&lt;/strong&gt;: gemma2, mistral, &amp;quot;deepseek-r1&amp;quot;, qwen2-5, and more&lt;/p&gt; &lt;p&gt;EDIT: I am learning now that there is indeed a lot more to this than just using Open WebUI and uploading documents and chatting about them. I have learned the defaults in Open WebUI with Ollama are very basic and more research needs to be done on on my part configuring embedding models and reranking models beyond the default settings.&lt;/p&gt; &lt;p&gt;If anyone has a guide they can point me to that would be great&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cunasmoker69420"&gt; /u/cunasmoker69420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik3m98/how_do_i_make_chatting_about_documents_not_suck/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ik3m98/how_do_i_make_chatting_about_documents_not_suck/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ik3m98/how_do_i_make_chatting_about_documents_not_suck/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T19:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ika79l</id>
    <title>Help with testing the 0.5.8 pre-release</title>
    <updated>2025-02-08T00:23:25+00:00</updated>
    <author>
      <name>/u/jmorganca</name>
      <uri>https://old.reddit.com/user/jmorganca</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;The next version of Ollama has overhauled how acceleration libraries are packaged. This adds support for non-AVX+GPU and AVX2+GPU combos. It also adds AVX512 instruction support for high-end CPUs like the AMD threadripper processors.&lt;/p&gt; &lt;p&gt;We'd love your help testing it out before marking it as a final release to work out any kinks with GPU support. You can download it here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.5.8-rc11"&gt;https://github.com/ollama/ollama/releases/tag/v0.5.8-rc11&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On Linux, you can run:&lt;/p&gt; &lt;p&gt;&lt;code&gt; curl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.5.8-rc11 sh &lt;/code&gt;&lt;/p&gt; &lt;p&gt;For Docker, you can use the following command to pull the new version:&lt;/p&gt; &lt;p&gt;&lt;code&gt; docker pull ollama/ollama:0.5.8-rc11 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;or for ROCm users:&lt;/p&gt; &lt;p&gt;&lt;code&gt; docker pull ollama/ollama:0.5.8-rc11-rocm &lt;/code&gt;&lt;/p&gt; &lt;p&gt;If you hit any issues feel free to DM me or create a GitHub issue letting us know you are on the 0.5.8 RC version. Thanks so much!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jmorganca"&gt; /u/jmorganca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ika79l/help_with_testing_the_058_prerelease/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ika79l/help_with_testing_the_058_prerelease/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ika79l/help_with_testing_the_058_prerelease/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T00:23:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijrwas</id>
    <title>Best LLM for Coding</title>
    <updated>2025-02-07T10:20:01+00:00</updated>
    <author>
      <name>/u/anshul2k</name>
      <uri>https://old.reddit.com/user/anshul2k</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for LLM for coding i got 32GB ram and 4080&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anshul2k"&gt; /u/anshul2k &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijrwas/best_llm_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijrwas/best_llm_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijrwas/best_llm_for_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T10:20:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijx0ll</id>
    <title>PDF to JSON</title>
    <updated>2025-02-07T15:04:34+00:00</updated>
    <author>
      <name>/u/hotdone</name>
      <uri>https://old.reddit.com/user/hotdone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ijx0ll/pdf_to_json/"&gt; &lt;img alt="PDF to JSON" src="https://b.thumbs.redditmedia.com/XWID2_aUBd2Bw1zOtjvRHQY1gy9OSXlnPe9v7pfEUUc.jpg" title="PDF to JSON" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, i am looking for guidance on how i can upload a pdf file and get the contents in JSON code. For example, a new patient form that asks for name, address and phone number. This is a pdf that I will feed to the LLM and then I would like it to analyze the content of the form and output code. Like in the picture. That code will output a fillable textbox call First Name. Currently i am looking at a pdf form that was provided to me and I have to translate it into code, like the example in the picture. For each item in the new patient form. First name, last name, address etc. This is a very time consuming process at the moment and I would like to see if i can use AI to optimize it. Thank you, any help/advise is appreciated &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hotdone"&gt; /u/hotdone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ijx0ll"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijx0ll/pdf_to_json/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijx0ll/pdf_to_json/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T15:04:34+00:00</published>
  </entry>
</feed>
