<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-06T17:05:40+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1iikoqb</id>
    <title>Handy scripts for local use</title>
    <updated>2025-02-05T20:59:09+00:00</updated>
    <author>
      <name>/u/Diligent_Property_39</name>
      <uri>https://old.reddit.com/user/Diligent_Property_39</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last few days i created some scripts that might also be interesting for this subreddit users. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;A local chat script for ollama that uses the available llm's from your local install.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Link: &lt;a href="https://github.com/xdep/Ollama-Chat-Client"&gt;https://github.com/xdep/Ollama-Chat-Client&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Another script to test the llm security for any flaws left by its creators. It will try multiple promps to see if it can bypass its default security measures.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Link: &lt;a href="https://github.com/xdep/llm-security-checks"&gt;https://github.com/xdep/llm-security-checks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;*** Screenshots available for both scripts to get an impression of the functionality.&lt;/p&gt; &lt;p&gt;Hope you guys like it :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Diligent_Property_39"&gt; /u/Diligent_Property_39 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iikoqb/handy_scripts_for_local_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iikoqb/handy_scripts_for_local_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iikoqb/handy_scripts_for_local_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T20:59:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii6y3n</id>
    <title>Which model is best for RAG or chatting document?</title>
    <updated>2025-02-05T10:16:59+00:00</updated>
    <author>
      <name>/u/Interesting_Music464</name>
      <uri>https://old.reddit.com/user/Interesting_Music464</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to train a model locally on my Macbook Pro M1 32GB based on a technical standard/specifications that is written in a document format like PDF. Which model would you recommend for this case? I saw that MLX is best for Apple Silicon so that is my only lead on how to properly choose a model aside from choosing the number of parameters and available unified memory.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting_Music464"&gt; /u/Interesting_Music464 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6y3n/which_model_is_best_for_rag_or_chatting_document/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6y3n/which_model_is_best_for_rag_or_chatting_document/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii6y3n/which_model_is_best_for_rag_or_chatting_document/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T10:16:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iioy7r</id>
    <title>Another Ollama+OpenWebUI Docker post...</title>
    <updated>2025-02-06T00:00:45+00:00</updated>
    <author>
      <name>/u/PaulLee420</name>
      <uri>https://old.reddit.com/user/PaulLee420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So... I'm on MacOS and can install Ollama using the MacOS download. Then I can spin up an Open WebUI docker container and it works perfectly.&lt;/p&gt; &lt;p&gt;However, I'd like to have ALL my AI data in docker containers - but when I install Ollama via docker, Open WebUI can see the LLMs but is gets some 500 error when I try to send a prompt...&lt;/p&gt; &lt;p&gt;I have followed the troubleshooting stuff on Open WebUI about this issue, changing the Docker run commands - but it simply doesn't work. Has there been any movement on this issue?&lt;/p&gt; &lt;p&gt;Do I need to post more details??? Will update if needed...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaulLee420"&gt; /u/PaulLee420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iioy7r/another_ollamaopenwebui_docker_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iioy7r/another_ollamaopenwebui_docker_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iioy7r/another_ollamaopenwebui_docker_post/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T00:00:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij2iry</id>
    <title>What am I missing about DeepSeek? I asked it to tell me a knock knock joke...</title>
    <updated>2025-02-06T13:28:18+00:00</updated>
    <author>
      <name>/u/thegreatcerebral</name>
      <uri>https://old.reddit.com/user/thegreatcerebral</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't have a great setup or anything. No GPU so I'm running llama3.2:3b and asking it things usually takes roughly 2-3 minutes for an answer which is fine for what I am doing currently. This DeepSeek came along and so I thought I would give it a whirl. Now, the one thing I will say is that it is very interesting to see what it is thinking but honestly I don't know if I can trust it.&lt;/p&gt; &lt;p&gt;First thing I asked it: &amp;quot;Can you tell me a simple knock knock joke?&amp;quot;&lt;/p&gt; &lt;p&gt;It thought about it for 2 minutes and returned the following: &lt;strong&gt;Why did the scarecrow get sick? Because he couldn't boil a perfect cup of tea!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;That is on par with a standard 1-3 year old kid just telling you random jokes but using words and phrasing that an adult would use. &lt;/p&gt; &lt;p&gt;Now, mind you I was using the 1.5b model so I stepped it up to the 7b model while also starting a whole new context so each was fresh. Here is my knock knock joke: &lt;strong&gt;Why did the scarecrow become a farmer? Because he had an honest reaction when asked if he ever worried about being caught without a umbrella!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;What is even more funny is the thought process it goes though. It second, third, up to fifth guessing itself only to go back to the original garbage it first came up with. &lt;/p&gt; &lt;p&gt;Now, interestingly enough when I kept the context, I started with DS1.5b's bad response. I then brought in llama which gave me a real knock knock joke. I then brought in DS7b and it cheated seeing what llama had brought me and gave me this gem:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;knock, knock!&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Who's there?&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;The scarecrow.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Scarecrow: &amp;quot;Whoa!&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;What is the fascination with scarecrows? Why is it so bad? I mean we all know the background of DS and how it was done for way cheaper and trained on so much more and faster etc. but, did they secretly just find the MP3 version of AI to everyone else's WAV by doing massive trimming to stuff that it found less relevant?&lt;/p&gt; &lt;p&gt;Didn't know if I am overthinking this one or just stumbled upon one of those things that this AI just doesn't do well. If you want more silly fun try asking it for guitar tablature for any song, then do another. I know that Chat-GPT gives you nearly identical responses and they are all massively wrong.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thegreatcerebral"&gt; /u/thegreatcerebral &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij2iry/what_am_i_missing_about_deepseek_i_asked_it_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij2iry/what_am_i_missing_about_deepseek_i_asked_it_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij2iry/what_am_i_missing_about_deepseek_i_asked_it_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T13:28:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1iixbpm</id>
    <title>Roast/critique/improve my plan: Pre-training a model for a specific JSON structure</title>
    <updated>2025-02-06T07:44:31+00:00</updated>
    <author>
      <name>/u/anderssewerin</name>
      <uri>https://old.reddit.com/user/anderssewerin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to implement an agent that can help generate, edit and enhance what's basically a graph defined in a JSON structure with a given syntax. I have access to a pretty large chunk of existing examples.&lt;/p&gt; &lt;p&gt;My thinking is that the best way to go would be to specialize a model to understand this structure, so I won't have to include a description in every prompt. Downside would be cost and coding effort to train, but running costs would be reduced. Format is not likely to change significantly.&lt;/p&gt; &lt;p&gt;My thinking was to generate training pairs by knockong our nodes or chunks of the existing examples and generate completion pairs for training this way.&lt;/p&gt; &lt;p&gt;Does that make sense, and are there any good examples of this out there that I can learn from?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anderssewerin"&gt; /u/anderssewerin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iixbpm/roastcritiqueimprove_my_plan_pretraining_a_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iixbpm/roastcritiqueimprove_my_plan_pretraining_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iixbpm/roastcritiqueimprove_my_plan_pretraining_a_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T07:44:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1iixtss</id>
    <title>As a beginner, where to start to setup &amp; run AI locally on your system?</title>
    <updated>2025-02-06T08:22:13+00:00</updated>
    <author>
      <name>/u/ExtremePresence3030</name>
      <uri>https://old.reddit.com/user/ExtremePresence3030</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are dozens of different apps for client and server side or some doing both, and different AI models as well. It makes me confused where to start. Is there any guideline ever written anywhere about how to choose the right app for you and how to install it through step-by-step guideline?&lt;/p&gt; &lt;p&gt;(My system is 6GB GPU, and 40GB Ram . My usage is research, brainstorming and research)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExtremePresence3030"&gt; /u/ExtremePresence3030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iixtss/as_a_beginner_where_to_start_to_setup_run_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iixtss/as_a_beginner_where_to_start_to_setup_run_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iixtss/as_a_beginner_where_to_start_to_setup_run_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T08:22:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1iijkmy</id>
    <title>Step-by-Step Guide to Running Open Deep Research with smolagents</title>
    <updated>2025-02-05T20:13:05+00:00</updated>
    <author>
      <name>/u/KonradFreeman</name>
      <uri>https://old.reddit.com/user/KonradFreeman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I had heard something about OpenAI's Deep Research&lt;/p&gt; &lt;p&gt;OpenAI’s Deep Research represents a leap toward AGI by enabling AI to independently discover and synthesize knowledge. While still evolving, its ability to automate expert-level research has transformative potential across industries. For users, however, its current instability and access limitations temper immediate utility, signaling a need for ongoing refinement.&lt;/p&gt; &lt;p&gt;Then I got an email this morning about this new open source project which reverse engineered how Deep Research works:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/blog/open-deep-research"&gt;https://huggingface.co/blog/open-deep-research&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I ran it and wrote a guide on how to run it:&lt;/p&gt; &lt;p&gt;&lt;a href="https://danielkliewer.com/2025/02/05/open-deep-research"&gt;https://danielkliewer.com/2025/02/05/open-deep-research&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You just run this command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;smolagent &amp;quot;{PROMPT}&amp;quot; \ --model-type &amp;quot;HfApiModel&amp;quot; \ --model-id &amp;quot;Qwen/Qwen2.5-Coder-32B-Instruct&amp;quot; \ --imports &amp;quot;pandas numpy&amp;quot; \ --tools &amp;quot;web_search translation&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I was surprised by some of what you can do with it and am interested in applying it and using it as a starting point for some other project.&lt;/p&gt; &lt;p&gt;The interesting thing about it is how it interacts with code instead of JSON and how this increases its accuracy considerably on the benchmarks.&lt;/p&gt; &lt;p&gt;I am planning on adapting this framework to work with Ollama and run local models. I am organizing a Hackathon on the 13th to do just that and more.&lt;/p&gt; &lt;p&gt;The idea is to develop software that benefits humanity using the reverse engineering of ClosedAI's latest model locally using Ollama.&lt;/p&gt; &lt;p&gt;Or whichever inference engine you like.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KonradFreeman"&gt; /u/KonradFreeman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iijkmy/stepbystep_guide_to_running_open_deep_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iijkmy/stepbystep_guide_to_running_open_deep_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iijkmy/stepbystep_guide_to_running_open_deep_research/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T20:13:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1iivo62</id>
    <title>Why are ollama package downloads at NPM zero?</title>
    <updated>2025-02-06T05:50:45+00:00</updated>
    <author>
      <name>/u/quantum-aey-ai</name>
      <uri>https://old.reddit.com/user/quantum-aey-ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iivo62/why_are_ollama_package_downloads_at_npm_zero/"&gt; &lt;img alt="Why are ollama package downloads at NPM zero?" src="https://external-preview.redd.it/sr7XqdeKF73E4m8CFm57jK-VSCmixf5xr3cX1tdw1SY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ac9ce6a0d25fae7f6d6d78c177a7289c0eb8c68" title="Why are ollama package downloads at NPM zero?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/1d9c22cijghe1.png?width=1348&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cf8fde4921fb9e3b1436e30d19089a2f55119ffb"&gt;ollama downloads are 0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Is it NPM? What's happening?&lt;/p&gt; &lt;p&gt;[SOLVED] It is a bug with NPM. Download count is 0 for every package on &lt;a href="http://npmjs.com"&gt;npmjs.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantum-aey-ai"&gt; /u/quantum-aey-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iivo62/why_are_ollama_package_downloads_at_npm_zero/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iivo62/why_are_ollama_package_downloads_at_npm_zero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iivo62/why_are_ollama_package_downloads_at_npm_zero/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T05:50:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1iifmpr</id>
    <title>qwen 2.5 VL on Ollama</title>
    <updated>2025-02-05T17:34:19+00:00</updated>
    <author>
      <name>/u/mans-987</name>
      <uri>https://old.reddit.com/user/mans-987</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any way to use qwen 2.5 VL with Ollama? The model is open source and can be found here on hugging face: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"&gt;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;also on github: &lt;a href="https://github.com/QwenLM/Qwen2.5-VL"&gt;https://github.com/QwenLM/Qwen2.5-VL&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mans-987"&gt; /u/mans-987 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iifmpr/qwen_25_vl_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iifmpr/qwen_25_vl_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iifmpr/qwen_25_vl_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T17:34:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1iihpxs</id>
    <title>Which small models are better than the original ChatGPT (based on GPT 3.5 released in November 2022) ?</title>
    <updated>2025-02-05T18:59:06+00:00</updated>
    <author>
      <name>/u/hn-mc</name>
      <uri>https://old.reddit.com/user/hn-mc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm wondering about those small models that we can run on our PCs. How many parameters do you need to have better performance than the first version of ChatGPT that was released in November 2022?&lt;/p&gt; &lt;p&gt;Are parameters the only measures that count? &lt;strong&gt;&lt;em&gt;Perhaps newer models can achieve the same performance with less parameters?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm asking this because I consider the original ChatGPT to be kind of first serious model. Everything below it, seems like a toy model.&lt;/p&gt; &lt;p&gt;So I'm wondering now in 2025, do we have any models that we can run on PC as good as the first ChatGPT?&lt;/p&gt; &lt;p&gt;If I recall, GPT 3, a predecessor to GPT 3.5, already had 175 billion parameters when it was released in 2020! And it was a long time ago!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hn-mc"&gt; /u/hn-mc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iihpxs/which_small_models_are_better_than_the_original/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iihpxs/which_small_models_are_better_than_the_original/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iihpxs/which_small_models_are_better_than_the_original/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T18:59:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij0kq6</id>
    <title>Ollama commands: How to use Ollama in the command line [Part 2]</title>
    <updated>2025-02-06T11:38:54+00:00</updated>
    <author>
      <name>/u/geshan</name>
      <uri>https://old.reddit.com/user/geshan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ij0kq6/ollama_commands_how_to_use_ollama_in_the_command/"&gt; &lt;img alt="Ollama commands: How to use Ollama in the command line [Part 2]" src="https://external-preview.redd.it/jlW-oWEX_HQ97HQZemLXUJ798r1KAbkE9X8R5aBgHBs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6af01b870b8ee04789b7d38be82e984b4d9b80c0" title="Ollama commands: How to use Ollama in the command line [Part 2]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geshan"&gt; /u/geshan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://geshan.com.np/blog/2025/02/ollama-commands/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij0kq6/ollama_commands_how_to_use_ollama_in_the_command/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij0kq6/ollama_commands_how_to_use_ollama_in_the_command/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T11:38:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1iitfiv</id>
    <title>Function Calling in Terminal + DeepSeek-R1-Distill-Llama-70B-Q_8 + vLLM -&gt; Sometimes...</title>
    <updated>2025-02-06T03:43:17+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7h5utciiwfhe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iitfiv/function_calling_in_terminal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iitfiv/function_calling_in_terminal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T03:43:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij4cr1</id>
    <title>[Guide] Mac Pro 2019 (MacPro7,1) w/ Linux &amp; Local LLM/AI [Link]</title>
    <updated>2025-02-06T14:54:53+00:00</updated>
    <author>
      <name>/u/Faisal_Biyari</name>
      <uri>https://old.reddit.com/user/Faisal_Biyari</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/macpro/comments/1ij3k4s/guide_mac_pro_2019_macpro71_w_linux_local_llmai/"&gt;https://www.reddit.com/r/macpro/comments/1ij3k4s/guide_mac_pro_2019_macpro71_w_linux_local_llmai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Worth Checking out. I did not know the best subreddit to post.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Faisal_Biyari"&gt; /u/Faisal_Biyari &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij4cr1/guide_mac_pro_2019_macpro71_w_linux_local_llmai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij4cr1/guide_mac_pro_2019_macpro71_w_linux_local_llmai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij4cr1/guide_mac_pro_2019_macpro71_w_linux_local_llmai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T14:54:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij5biv</id>
    <title>How can I run ollama webUI using an RX 7900 XTX?</title>
    <updated>2025-02-06T15:36:28+00:00</updated>
    <author>
      <name>/u/Altruistic-Pickle-57</name>
      <uri>https://old.reddit.com/user/Altruistic-Pickle-57</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is only using the cpu and it is really slow, is there any way to make it run with the gpu?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altruistic-Pickle-57"&gt; /u/Altruistic-Pickle-57 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij5biv/how_can_i_run_ollama_webui_using_an_rx_7900_xtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij5biv/how_can_i_run_ollama_webui_using_an_rx_7900_xtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij5biv/how_can_i_run_ollama_webui_using_an_rx_7900_xtx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T15:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1iilug6</id>
    <title>Ollama + DataBridge: Creating an interactive learning platform under 2 minutes!</title>
    <updated>2025-02-05T21:46:26+00:00</updated>
    <author>
      <name>/u/yes-no-maybe_idk</name>
      <uri>https://old.reddit.com/user/yes-no-maybe_idk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=tfqIa_6lqQU"&gt;https://www.youtube.com/watch?v=tfqIa_6lqQU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Learn how to turn any video into an interactive learning tool with Databridge! In this demo, we'll show you how to ingest a lecture video and generate engaging questions with DataBridge, all locally Using Ollama and DataBridge.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/databridge-org/databridge-core"&gt;https://github.com/databridge-org/databridge-core&lt;/a&gt;&lt;br /&gt; Docs: &lt;a href="https://databridge.gitbook.io/databridge-docs"&gt;https://databridge.gitbook.io/databridge-docs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear comments, see you build cool stuff (or maybe even contribute to our OSS library).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yes-no-maybe_idk"&gt; /u/yes-no-maybe_idk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iilug6/ollama_databridge_creating_an_interactive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iilug6/ollama_databridge_creating_an_interactive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iilug6/ollama_databridge_creating_an_interactive/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T21:46:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij5pvy</id>
    <title>Getting "@@@@@@" output while making inference on deepseek-r1:32b locally with Ollama. Does anyone knows how to solve it?</title>
    <updated>2025-02-06T15:53:33+00:00</updated>
    <author>
      <name>/u/D4v3-X</name>
      <uri>https://old.reddit.com/user/D4v3-X</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ij5pvy/getting_output_while_making_inference_on/"&gt; &lt;img alt="Getting &amp;quot;@@@@@@&amp;quot; output while making inference on deepseek-r1:32b locally with Ollama. Does anyone knows how to solve it?" src="https://preview.redd.it/a280k0d2jjhe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f38230ca5b703d2749fa39ebf56f9097b76e3141" title="Getting &amp;quot;@@@@@@&amp;quot; output while making inference on deepseek-r1:32b locally with Ollama. Does anyone knows how to solve it?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/D4v3-X"&gt; /u/D4v3-X &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a280k0d2jjhe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij5pvy/getting_output_while_making_inference_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij5pvy/getting_output_while_making_inference_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T15:53:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij1aaz</id>
    <title>Best coding models for Consumer Hardware</title>
    <updated>2025-02-06T12:22:41+00:00</updated>
    <author>
      <name>/u/Willbo_Bagg1ns</name>
      <uri>https://old.reddit.com/user/Willbo_Bagg1ns</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve recently been experimenting with Deepseek-r1:32b and Deepseek-coderv2:16b on my RTX 4090. I can run both these models with really good performance locally, but want to explore other models that can handle coding tasks.&lt;/p&gt; &lt;p&gt;What are the best coding models available through ollama? I plan to integrate one of these models with my IDE (vscode).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Willbo_Bagg1ns"&gt; /u/Willbo_Bagg1ns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij1aaz/best_coding_models_for_consumer_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij1aaz/best_coding_models_for_consumer_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij1aaz/best_coding_models_for_consumer_hardware/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T12:22:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij1d1l</id>
    <title>Do I need local AI for Electrical Engineering studying ?</title>
    <updated>2025-02-06T12:27:04+00:00</updated>
    <author>
      <name>/u/wolfson10</name>
      <uri>https://old.reddit.com/user/wolfson10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey !&lt;/p&gt; &lt;p&gt;I'm currently using ChaptGPT plus and Deepseek for Electrical Engineering studying , I'm paying for ChatGPT and still have limitations for example the O1 model , meanwhile Deepseek has a lot of &amp;quot;Server busy&amp;quot; and it's annoying but it's solving math and electrical problems great and I think even better than Chatgpt O4 model.&lt;/p&gt; &lt;p&gt;I thought maybe to try Ollama + Open WebUI + Deppseek R1 as a local chat .&lt;/p&gt; &lt;p&gt;Will I able to upload math/electrical problems images and it will analyze them and solve ?&lt;/p&gt; &lt;p&gt;What will be the best for me ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wolfson10"&gt; /u/wolfson10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij1d1l/do_i_need_local_ai_for_electrical_engineering/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij1d1l/do_i_need_local_ai_for_electrical_engineering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij1d1l/do_i_need_local_ai_for_electrical_engineering/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T12:27:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij6xse</id>
    <title>Crashing after playing with num_ctx</title>
    <updated>2025-02-06T16:43:06+00:00</updated>
    <author>
      <name>/u/ActionzheZ</name>
      <uri>https://old.reddit.com/user/ActionzheZ</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running the distilled deepseek r1:32b on a 12GB VRAM 3080Ti and 64GB of RAM. The model when running uses about 10.9GB VRAM+11GB RAM. &lt;/p&gt; &lt;p&gt;I was a bit annoyed by the model losing context clues, so I created a new modelfile with the parameter set to 10240, increasing the default 2K to 10K. This new modelfile ran fine with test scenario in terminal, though slower compared to 2K as expected.&lt;/p&gt; &lt;p&gt;I'm using Ollama with Chatbox AI, so I opened an existing conversation with roughly 15k existing tokens, switched to the new modelfile, and gave prompt. It worked fine the for the first prompt requiring some previous context larger than 2K. I saw it working, so then I gave it a follow up, the model crashed. Repeating would crash again. I figured ok maybe the context is too much, so I created another one with context 4k. To my surprise, this crashed as well. I then simply used the original model, but it is now also crashing! Wtf? The original model was running fine on my machine before. &lt;/p&gt; &lt;p&gt;I have plenty of RAM left to spare, so while I may be out of dedicated VRAM I'm certainly nowhere near exceeding shared VRAM. Even if somehow the larger context does not behave when dedicated VRAM is full, running the default settings should be fine as it was working before...? Why I'm I suddenly crashing on everything? The crashing seems to be happening right as the &amp;lt;think&amp;gt; is about to start populate texts...&lt;/p&gt; &lt;p&gt;Did I accidentally mess up a setting somewhere in the original model when I did all that modelfiles ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ActionzheZ"&gt; /u/ActionzheZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij6xse/crashing_after_playing_with_num_ctx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij6xse/crashing_after_playing_with_num_ctx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij6xse/crashing_after_playing_with_num_ctx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T16:43:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij71ow</id>
    <title>[Help] Why I can't pull this model from offical ollama model page?</title>
    <updated>2025-02-06T16:47:22+00:00</updated>
    <author>
      <name>/u/josephwang123</name>
      <uri>https://old.reddit.com/user/josephwang123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to pull this model: &lt;a href="https://ollama.com/daiseur/darkidol-llama-3.1-8b_instruct-1.2"&gt;https://ollama.com/daiseur/darkidol-llama-3.1-8b_instruct-1.2&lt;/a&gt;&lt;br /&gt; Both &lt;code&gt;ollama pull&lt;/code&gt; and &lt;code&gt;ollama serve&lt;/code&gt; is failed.&lt;br /&gt; Can anybody help?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/josephwang123"&gt; /u/josephwang123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij71ow/help_why_i_cant_pull_this_model_from_offical/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij71ow/help_why_i_cant_pull_this_model_from_offical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij71ow/help_why_i_cant_pull_this_model_from_offical/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T16:47:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij7dmw</id>
    <title>Please help me out with this issue in downloading ollama deepseek model</title>
    <updated>2025-02-06T17:00:47+00:00</updated>
    <author>
      <name>/u/oye_ap</name>
      <uri>https://old.reddit.com/user/oye_ap</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's my first time trying to download ollama and deepseek, I'm not much into tech &amp;amp; while I try to run deepseek, this is the issue that I'm facing &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oye_ap"&gt; /u/oye_ap &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij7dmw/please_help_me_out_with_this_issue_in_downloading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij7dmw/please_help_me_out_with_this_issue_in_downloading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij7dmw/please_help_me_out_with_this_issue_in_downloading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T17:00:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiycpb</id>
    <title>How to Ollama on-demand?</title>
    <updated>2025-02-06T09:02:35+00:00</updated>
    <author>
      <name>/u/RamenKomplex</name>
      <uri>https://old.reddit.com/user/RamenKomplex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;We would like to run a ollama server for our team and let team members use it via the openweb ui. We can create a new VM on Aws and run it there but since it won't be used all the time, this would be a waste of the rather expensive resources. &lt;/p&gt; &lt;p&gt;Are there any smarter ways of deploying ollama based openwebui for team use in a way that we only pay for the GPU/CPU only when chat is being used?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RamenKomplex"&gt; /u/RamenKomplex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iiycpb/how_to_ollama_ondemand/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iiycpb/how_to_ollama_ondemand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iiycpb/how_to_ollama_ondemand/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T09:02:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij23pm</id>
    <title>Can a malicious model execute code ?</title>
    <updated>2025-02-06T13:07:27+00:00</updated>
    <author>
      <name>/u/niilzon</name>
      <uri>https://old.reddit.com/user/niilzon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Could a hypothetical malicious model (such as Deepseek or any other) execute code on a machine running Ollama (without exploiting a vulnerability in Ollama) ?&lt;/p&gt; &lt;p&gt;It's not clear to me if models used by Ollama are &amp;quot;just weights&amp;quot; or if they also embed custom dependencies, executables etc&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/niilzon"&gt; /u/niilzon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij23pm/can_a_malicious_model_execute_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij23pm/can_a_malicious_model_execute_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij23pm/can_a_malicious_model_execute_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T13:07:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij2pw7</id>
    <title>🎉 Being Thankful for Everyone Who Made This Project a Super Hit! 🚀</title>
    <updated>2025-02-06T13:37:59+00:00</updated>
    <author>
      <name>/u/akhilpanja</name>
      <uri>https://old.reddit.com/user/akhilpanja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are thrilled to announce that our project, DeepSeek-RAG-Chatbot, has officially hit 100 stars on GitHub repo: &lt;a href="https://github.com/SaiAkhil066/DeepSeek-RAG-Chatbot.git"&gt;https://github.com/SaiAkhil066/DeepSeek-RAG-Chatbot.git&lt;/a&gt; 🌟✨ &lt;/p&gt; &lt;p&gt;This journey has been incredible, and we couldn’t have achieved this milestone without the support of our amazing community. Your contributions, feedback, and enthusiasm have helped shape this project into what it is today!&lt;/p&gt; &lt;p&gt;🔍 Performance Boost The graph above showcases the significant improvements in Graph Context Relevancy and Graph Context Recall after integrating GraphRAG and further advancements. Our system is now more accurate, contextually aware, and efficient in retrieving relevant information.&lt;/p&gt; &lt;p&gt;We are committed to making this project even better and look forward to the next milestones! 🚀&lt;/p&gt; &lt;p&gt;Thank you all once again for being part of this journey. Let’s keep building together! 💡🔥 ￼&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akhilpanja"&gt; /u/akhilpanja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij2pw7/being_thankful_for_everyone_who_made_this_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij2pw7/being_thankful_for_everyone_who_made_this_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij2pw7/being_thankful_for_everyone_who_made_this_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T13:37:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij345i</id>
    <title>Can anyone help me? i really don't understand what i'm doing wrong.</title>
    <updated>2025-02-06T13:57:13+00:00</updated>
    <author>
      <name>/u/RevolutionaryBus4545</name>
      <uri>https://old.reddit.com/user/RevolutionaryBus4545</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ij345i/can_anyone_help_me_i_really_dont_understand_what/"&gt; &lt;img alt="Can anyone help me? i really don't understand what i'm doing wrong." src="https://preview.redd.it/gurn3vbgyihe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81c1c17805ce018b0fc08afcf7535c8018e411f7" title="Can anyone help me? i really don't understand what i'm doing wrong." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RevolutionaryBus4545"&gt; /u/RevolutionaryBus4545 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gurn3vbgyihe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij345i/can_anyone_help_me_i_really_dont_understand_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij345i/can_anyone_help_me_i_really_dont_understand_what/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T13:57:13+00:00</published>
  </entry>
</feed>
