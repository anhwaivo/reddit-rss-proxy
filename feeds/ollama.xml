<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-01T11:05:52+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1n2ugfs</id>
    <title>Spring AI Playground — Self-hosted Web UI with Ollama, RAG and MCP tools</title>
    <updated>2025-08-29T01:35:22+00:00</updated>
    <author>
      <name>/u/kr-jmlab</name>
      <uri>https://old.reddit.com/user/kr-jmlab</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n2ugfs/spring_ai_playground_selfhosted_web_ui_with/"&gt; &lt;img alt="Spring AI Playground — Self-hosted Web UI with Ollama, RAG and MCP tools" src="https://external-preview.redd.it/Tdp9Yr11guCxzcug45e2Jg_hVSbeQUV2eb5zstdOZ5Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cbdafe1ed4b7e420dcb8f2e46a256d6f18757dd0" title="Spring AI Playground — Self-hosted Web UI with Ollama, RAG and MCP tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built &lt;strong&gt;Spring AI Playground&lt;/strong&gt;, a self-hosted web UI for experimenting with &lt;strong&gt;Ollama, RAG workflows, and MCP tools&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Uses &lt;strong&gt;Ollama as the default backend&lt;/strong&gt; — no API keys needed&lt;/li&gt; &lt;li&gt;Upload docs → chunk → embed → run similarity search with vector DBs (Pinecone, Milvus, PGVector, Weaviate, etc.)&lt;/li&gt; &lt;li&gt;Visual &lt;strong&gt;MCP Playground&lt;/strong&gt;: connect tools via HTTP/STDIO/SSE, inspect metadata, tweak args, and call them from chat&lt;/li&gt; &lt;li&gt;Can also swap to OpenAI, Anthropic, Google, etc. if you want&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I built this because wiring up RAG + tool integrations in Java always felt slow and repetitive. Now I can spin things up quickly in a browser UI, fully local.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/JM-Lab/spring-ai-playground"&gt;https://github.com/JM-Lab/spring-ai-playground&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear how this community is using Ollama for RAG today, and what features you’d like to see added.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kr-jmlab"&gt; /u/kr-jmlab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/JM-Lab/spring-ai-playground"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2ugfs/spring_ai_playground_selfhosted_web_ui_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n2ugfs/spring_ai_playground_selfhosted_web_ui_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-29T01:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n302ll</id>
    <title>Which model to choose for content generation?</title>
    <updated>2025-08-29T06:35:41+00:00</updated>
    <author>
      <name>/u/fullstackdev-channel</name>
      <uri>https://old.reddit.com/user/fullstackdev-channel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, seeking advice on which model to choose for my clients website, for content creation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fullstackdev-channel"&gt; /u/fullstackdev-channel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n302ll/which_model_to_choose_for_content_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n302ll/which_model_to_choose_for_content_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n302ll/which_model_to_choose_for_content_generation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-29T06:35:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2pspb</id>
    <title>ollama-lancache (like caching games for a lan party but models instead!)</title>
    <updated>2025-08-28T22:07:13+00:00</updated>
    <author>
      <name>/u/jjasghar</name>
      <uri>https://old.reddit.com/user/jjasghar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I'd like to announce I created &lt;code&gt;ollama-lancache&lt;/code&gt;. Basically, it's a way to share out the &amp;quot;blobs&amp;quot; for ollama models from one (laptop) computer to a bunch of attendee machines. &lt;/p&gt; &lt;p&gt;So, if you have a say conference with Wi-Fi, and it takes hours to download your models, you can use this app that sits beside your already downloaded models and will install them to the correct location for Windows/Mac/Linux. &lt;/p&gt; &lt;p&gt;There's even a &amp;quot;downloads&amp;quot; directory, so you can have specific versions of Ollama or any additional downloads for leveraging models.&lt;/p&gt; &lt;p&gt;Conference wifi has always been a problem. This is a small Go application that leverages something already on your laptop, and ideally will allow you to get your attendees to leverage your tech sooner rather than later.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jjasghar/ollama-lancache"&gt;https://github.com/jjasghar/ollama-lancache&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jjasghar"&gt; /u/jjasghar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2pspb/ollamalancache_like_caching_games_for_a_lan_party/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2pspb/ollamalancache_like_caching_games_for_a_lan_party/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n2pspb/ollamalancache_like_caching_games_for_a_lan_party/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T22:07:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3hhlt</id>
    <title>bsod - new build - questions</title>
    <updated>2025-08-29T19:54:58+00:00</updated>
    <author>
      <name>/u/p1kn1t</name>
      <uri>https://old.reddit.com/user/p1kn1t</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently built a new machine (specs here - &lt;a href="https://www.reddit.com/r/PcBuild/comments/1mgmf0r/first_build_in_a_long_time/"&gt;https://www.reddit.com/r/PcBuild/comments/1mgmf0r/first_build_in_a_long_time/&lt;/a&gt; )&lt;/p&gt; &lt;p&gt;I have ollama version is 0.11.8 loaded&lt;/p&gt; &lt;p&gt;I finally got around to loading some llms and benchmarking last night. I started small and worked my way up downloading the following models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;tinyllama:latest~637 MB&lt;/li&gt; &lt;li&gt;gemma:2b~1.7 GB&lt;/li&gt; &lt;li&gt;mistral (base)~4.0 GB&lt;/li&gt; &lt;li&gt;mistral:instruct~4.1 GB&lt;/li&gt; &lt;li&gt;gemma2:9b~5.4 GB&lt;/li&gt; &lt;li&gt;mistral-nemo:12b~7.1 GB&lt;/li&gt; &lt;li&gt;nous-hermes:13b~7.4 GB&lt;/li&gt; &lt;li&gt;llama2:7b-q4_0 ≈3–4 GB for q4 quant&lt;/li&gt; &lt;li&gt;llama3:8b ≈ 4–5 GB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The first two loaded fine, I did some quick powershell tests and then loaded the next. When I got to mistral as soon as it started I received a bsod (OS windows server). The errors were always same same but different. Here are the two that would occur:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The computer has rebooted from a bugcheck. The bugcheck was: 0x00000139 (0x0000000000000003, 0xffffe4037cc3ee50, 0xffffe4037cc3eda8, 0x0000000000000000). &lt;ul&gt; &lt;li&gt;This was the first. I downloaded ddu cleared the drivers for the gpu and downloaded the latest divers from nvidia - just happened to have had anew driver from this week so that was timely&lt;/li&gt; &lt;li&gt;This was good for a bit and then i got.....&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;The computer has rebooted from a bugcheck. The bugcheck was: 0x00000139 (0x0000000000000003, 0xfffff80334179840, 0xfffff80334179798, 0x0000000000000000). i ran the following to see if I could clear things up make sure drives were good etc &lt;ul&gt; &lt;li&gt;sfc /scannow&lt;/li&gt; &lt;li&gt;DISM /Online /Cleanup-Image /RestoreHealth&lt;/li&gt; &lt;li&gt;deleted a previous windows installation windows.old - i loaded 11 pro initially when i built the machine and then moved to server&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;here are a few questions, thanks in advance for any thoughts etc:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Since this is a new build I want to make sure that this is more of a large file issue and not something that is a sign of things to come&lt;/li&gt; &lt;li&gt;I am assuming there could be driver issues as I am using windows server and the drivers for the gpu are windows 11&lt;/li&gt; &lt;li&gt;I am assuming it was some sort of large file random bitfly that caused the errors and that it will not impact my models when I run but wanted to see if anyone else &lt;ul&gt; &lt;li&gt;Had the same issue on just the download of the large models&lt;/li&gt; &lt;li&gt;Ran clean other wise&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;During benchmarking on the large files I ran concurrency tests with the following settings and did not have any bsod issues &lt;ul&gt; &lt;li&gt;$concurrencyLevels = @(0,1,2,4,6,8,19,12,14,16) - 19 was a fat finger and did not notice until I had run two benchmarks so I kept it in there for all benchmarking to be consistent&lt;/li&gt; &lt;li&gt;$numPredict = 800&lt;/li&gt; &lt;li&gt;$numCtx = 4096&lt;/li&gt; &lt;li&gt;$numBatch = 512&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am happy to share the benchmarking if any one is interested. Mistral 7B Instruct (v0.2):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Achieved the highest peak tokens/sec at its best concurrency. &lt;/li&gt; &lt;li&gt;Sustained speed: Mistral 7B Instruct (v0.2) delivered the best average tokens/sec across the full sweep&lt;/li&gt; &lt;li&gt;Efficiency leader: Mistral 7B Instruct (v0.2) posted the highest tokens-per-watt at its max tested concurrency&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Lots of details but appreciate you making it through the thread. thanks,&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/p1kn1t"&gt; /u/p1kn1t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n3hhlt/bsod_new_build_questions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n3hhlt/bsod_new_build_questions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n3hhlt/bsod_new_build_questions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-29T19:54:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3bafx</id>
    <title>Human in the Loop for computer use agents</title>
    <updated>2025-08-29T15:57:53+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n3bafx/human_in_the_loop_for_computer_use_agents/"&gt; &lt;img alt="Human in the Loop for computer use agents" src="https://external-preview.redd.it/ZGY1MTQ4aHlkemxmMUvZR3OLyxwRXjlbiYrrBtxMf6dd5k6Y_Z_HAekaHP-j.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a80d11cc3917442a0c0468d6f78156d754b4266c" title="Human in the Loop for computer use agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sometimes the best “agent” is you.&lt;/p&gt; &lt;p&gt;We’re introducing Human-in-the-Loop: instantly hand off from automation to human control when a task needs judgment. &lt;/p&gt; &lt;p&gt;Yesterday we shared our HUD evals for measuring agents at scale. Today, you can become the agent when it matters - take over the same session, see what the agent sees, and keep the workflow moving.&lt;/p&gt; &lt;p&gt;Lets you create clean training demos, establish ground truth for tricky cases, intervene on edge cases ( CAPTCHAs, ambiguous UIs) or step through debug withut context switching.&lt;/p&gt; &lt;p&gt;You have full human control when you want.We even a fallback version where in it starts automated but escalate to a human only when needed.&lt;/p&gt; &lt;p&gt;Works across common stacks (OpenAI, Anthropic, Hugging Face) and with our Composite Agents. Same tools, same environment - take control when needed.&lt;/p&gt; &lt;p&gt;Feedback welcome - curious how you’d use this in your workflows.&lt;/p&gt; &lt;p&gt;Blog : &lt;a href="https://www.trycua.com/blog/human-in-the-loop.md"&gt;https://www.trycua.com/blog/human-in-the-loop.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zi8focpydzlf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n3bafx/human_in_the_loop_for_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n3bafx/human_in_the_loop_for_computer_use_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-29T15:57:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3yxzu</id>
    <title>How to disable thinking mode for qwen3 in Ollama desktup UI?</title>
    <updated>2025-08-30T11:11:05+00:00</updated>
    <author>
      <name>/u/mohnos</name>
      <uri>https://old.reddit.com/user/mohnos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can disable thinking mode when i run it in terminal like this:&lt;br /&gt; &lt;code&gt;ollama run qwen3:1.7b --think=false&lt;/code&gt;&lt;/p&gt; &lt;p&gt;or like this:&lt;br /&gt; &lt;code&gt;ollama run qwen3:1.7b&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; /set nothink&lt;/code&gt;&lt;/p&gt; &lt;p&gt;But nothing works in the new Ollama desktop UI. Can you help me?&lt;/p&gt; &lt;p&gt;EDIT: sorry for the typo in the title.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mohnos"&gt; /u/mohnos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n3yxzu/how_to_disable_thinking_mode_for_qwen3_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n3yxzu/how_to_disable_thinking_mode_for_qwen3_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n3yxzu/how_to_disable_thinking_mode_for_qwen3_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-30T11:11:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n47pe2</id>
    <title>Models just returning repeated numbers or letters when using tools</title>
    <updated>2025-08-30T17:41:07+00:00</updated>
    <author>
      <name>/u/blacklandothegambler</name>
      <uri>https://old.reddit.com/user/blacklandothegambler</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, some of my models started generating repeated '3's or 'G's when using tools like Open WebUI and Page Assist. This issue affects only specific models (e.g., Qwen3, Minstral, Gemma, and Granite appear to be unaffected). Has anyone seen this behavior before? It helps, I'm running ollama on my Jetson Nano board and serving it to my other computers via the API. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blacklandothegambler"&gt; /u/blacklandothegambler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n47pe2/models_just_returning_repeated_numbers_or_letters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n47pe2/models_just_returning_repeated_numbers_or_letters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n47pe2/models_just_returning_repeated_numbers_or_letters/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-30T17:41:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3zi12</id>
    <title>Can you use amd 9000 gpus</title>
    <updated>2025-08-30T11:42:39+00:00</updated>
    <author>
      <name>/u/kabyking</name>
      <uri>https://old.reddit.com/user/kabyking</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know ollama isn’t officially supported for rdna 4, but can you still run it. I tried using it for Ubuntu on wsl but it didn’t work, I tried using Vulkan as well it didn’t work. Is it because it is wsl, would it work if I tried it on arch(Linux distro I’m running rn). Will there be official support any time soon. &lt;/p&gt; &lt;p&gt;Ps, I have a 9060 xt&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kabyking"&gt; /u/kabyking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n3zi12/can_you_use_amd_9000_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n3zi12/can_you_use_amd_9000_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n3zi12/can_you_use_amd_9000_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-30T11:42:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4i1fp</id>
    <title>Can't pull models</title>
    <updated>2025-08-31T01:21:49+00:00</updated>
    <author>
      <name>/u/CrysisRogue</name>
      <uri>https://old.reddit.com/user/CrysisRogue</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n4i1fp/cant_pull_models/"&gt; &lt;img alt="Can't pull models" src="https://b.thumbs.redditmedia.com/MDi2CGHhBnulynlFbvvGC772eNFS8dRps1GF9C5jMKI.jpg" title="Can't pull models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm running Ollama with OpenWebUI in a proxmox container, I can't download models. It worked with smaller ones (1b-1.5b) but I was trying to get deepseek-r1:32b and gpt-oss:latest, I get this error:&lt;/p&gt; &lt;p&gt;GUI is shown in this image&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7bqwk826b9mf1.png?width=744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=728777a5004c0638c853b20f18d53ba20773cbda"&gt;https://preview.redd.it/7bqwk826b9mf1.png?width=744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=728777a5004c0638c853b20f18d53ba20773cbda&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Command line:&lt;br /&gt; Error: max retries exceeded: Get &amp;quot;&lt;a href="https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/61/6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20250831%2Fauto%2Fs3%2Faws4%5C_request&amp;amp;X-Amz-Date=20250831T011307Z&amp;amp;X-Amz-Expires=86400&amp;amp;X-Amz-SignedHeaders=host&amp;amp;X-Amz-Signature=143a261b9e9a309b37b38e9dddb38c48e2cc2827ea5af47dc34d8382aad4a752%22:"&gt;https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/61/6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20250831%2Fauto%2Fs3%2Faws4\_request&amp;amp;X-Amz-Date=20250831T011307Z&amp;amp;X-Amz-Expires=86400&amp;amp;X-Amz-SignedHeaders=host&amp;amp;X-Amz-Signature=143a261b9e9a309b37b38e9dddb38c48e2cc2827ea5af47dc34d8382aad4a752&amp;quot;:&lt;/a&gt; dial tcp [2606:4700:7::12e]:443: connect: cannot assign requested address&lt;/p&gt; &lt;p&gt;I have done everything I could to disable IPv6 but it didnt do anything, kinda stuck here...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CrysisRogue"&gt; /u/CrysisRogue &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4i1fp/cant_pull_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4i1fp/cant_pull_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n4i1fp/cant_pull_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-31T01:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1n489hq</id>
    <title>Cua is hiring a Founding Engineer, UX &amp; Design in SF</title>
    <updated>2025-08-30T18:04:09+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cua is hiring a Founding Engineer, UX &amp;amp; Design in our brand new SF office.&lt;/p&gt; &lt;p&gt;Cua is building the infrastructure for general AI agents - your work will define how humans and computers interact at scale.&lt;/p&gt; &lt;p&gt;Location : SF&lt;/p&gt; &lt;p&gt;Referal Bonus : $5000&lt;/p&gt; &lt;p&gt;Apply here : &lt;a href="https://www.ycombinator.com/companies/cua/jobs/a6UbTvG-founding-engineer-ux-design"&gt;https://www.ycombinator.com/companies/cua/jobs/a6UbTvG-founding-engineer-ux-design&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Discord : &lt;a href="https://discord.gg/vJ2uCgybsC"&gt;https://discord.gg/vJ2uCgybsC&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n489hq/cua_is_hiring_a_founding_engineer_ux_design_in_sf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n489hq/cua_is_hiring_a_founding_engineer_ux_design_in_sf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n489hq/cua_is_hiring_a_founding_engineer_ux_design_in_sf/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-30T18:04:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4hnie</id>
    <title>Can turbo hosted model access internet?</title>
    <updated>2025-08-31T01:02:36+00:00</updated>
    <author>
      <name>/u/ExpertDeal9883</name>
      <uri>https://old.reddit.com/user/ExpertDeal9883</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;gpt-oss:20b running on turbo (hosted). Does this setup have access to web search? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExpertDeal9883"&gt; /u/ExpertDeal9883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4hnie/can_turbo_hosted_model_access_internet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4hnie/can_turbo_hosted_model_access_internet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n4hnie/can_turbo_hosted_model_access_internet/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-31T01:02:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4gnze</id>
    <title>Using a model from ollama to take extracted PDF text and turn it into a CSV?</title>
    <updated>2025-08-31T00:13:28+00:00</updated>
    <author>
      <name>/u/FudgeOk4045</name>
      <uri>https://old.reddit.com/user/FudgeOk4045</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all. For a while now, I’ve been trying to find a way to take extracted text from PDFs of medical studies and convert it to csv. Example: the question would be “Do you worry a lot?” and the choices should be formatted as “Yes; Maybe; No”. I am thinking of creating a Python script that uses a model from ollama; it will take the extracted text from the PDF (currently using Unstract for this) and passes it to said model and it’ll return my csv output. All PDF studies are different and formatted vastly different, thus I cannot use regex or a simple function, which is why I am thinking of using AI. Any tips on this, could this work / has anybody done something similar ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FudgeOk4045"&gt; /u/FudgeOk4045 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4gnze/using_a_model_from_ollama_to_take_extracted_pdf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4gnze/using_a_model_from_ollama_to_take_extracted_pdf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n4gnze/using_a_model_from_ollama_to_take_extracted_pdf/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-31T00:13:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1n46j3c</id>
    <title>Computer Use on Windows Sandbox</title>
    <updated>2025-08-30T16:52:47+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n46j3c/computer_use_on_windows_sandbox/"&gt; &lt;img alt="Computer Use on Windows Sandbox" src="https://external-preview.redd.it/bGxuZjA0N25zNm1mMUUIhfD3WmHuxYkgbFXnt7PvLDhATd-8_6cYVR-PGp7c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e692b6fd21926f81bf78181fdb9b932968cb1c56" title="Computer Use on Windows Sandbox" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing Windows Sandbox support - run computer-use agents on Windows business apps without VMs or cloud costs.&lt;/p&gt; &lt;p&gt;Your enterprise software runs on Windows, but testing agents required expensive cloud instances. Windows Sandbox changes this - it's Microsoft's built-in lightweight virtualization sitting on every Windows 10/11 machine, ready for instant agent development.&lt;/p&gt; &lt;p&gt;Enterprise customers kept asking for AutoCAD automation, SAP integration, and legacy Windows software support. Traditional VM testing was slow and resource-heavy. Windows Sandbox solves this with disposable, seconds-to-boot Windows environments for safe agent testing.&lt;/p&gt; &lt;p&gt;What you can build: AutoCAD drawing automation, SAP workflow processing, Bloomberg terminal trading bots, manufacturing execution system integration, or any Windows-only enterprise software automation - all tested safely in disposable sandbox environments.&lt;/p&gt; &lt;p&gt;Free with Windows 10/11, boots in seconds, completely disposable. Perfect for development and testing before deploying to Windows cloud instances (coming later this month).&lt;/p&gt; &lt;p&gt;Check out the github here : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog : &lt;a href="https://www.trycua.com/blog/windows-sandbox"&gt;https://www.trycua.com/blog/windows-sandbox&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c0rqqugns6mf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n46j3c/computer_use_on_windows_sandbox/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n46j3c/computer_use_on_windows_sandbox/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-30T16:52:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4ql3w</id>
    <title>Is he drunk?</title>
    <updated>2025-08-31T09:43:00+00:00</updated>
    <author>
      <name>/u/yesil_teknoloji</name>
      <uri>https://old.reddit.com/user/yesil_teknoloji</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n4ql3w/is_he_drunk/"&gt; &lt;img alt="Is he drunk?" src="https://preview.redd.it/vw1efm9ssbmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da773fd0082e772fbe02db05e84b7e34d0711810" title="Is he drunk?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yesil_teknoloji"&gt; /u/yesil_teknoloji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vw1efm9ssbmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4ql3w/is_he_drunk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n4ql3w/is_he_drunk/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-31T09:43:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4nf4w</id>
    <title>can Ollama run image generation models like Qwen -Image ?</title>
    <updated>2025-08-31T06:19:13+00:00</updated>
    <author>
      <name>/u/abrandis</name>
      <uri>https://old.reddit.com/user/abrandis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I didn't notice any image generation models for Ollama can it generate image/graphics for any models, recent qwen image is good model for image creation and manipulation&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abrandis"&gt; /u/abrandis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4nf4w/can_ollama_run_image_generation_models_like_qwen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4nf4w/can_ollama_run_image_generation_models_like_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n4nf4w/can_ollama_run_image_generation_models_like_qwen/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-31T06:19:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4wmim</id>
    <title>Customize a existing model without copying it?</title>
    <updated>2025-08-31T14:47:46+00:00</updated>
    <author>
      <name>/u/Cartoon_Corpze</name>
      <uri>https://old.reddit.com/user/Cartoon_Corpze</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I have Ollama installed in: &lt;code&gt;D:\\PROGRAMFILES\\Ollama&lt;/code&gt;&lt;/p&gt; &lt;p&gt;My models are located in: &lt;code&gt;D:\PROGRAMDATA\Ollama_Models\blobs&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I'm not familiar with Ollama but I'd like to play around with it.&lt;/p&gt; &lt;p&gt;So let's say, I have this model installed &lt;code&gt;qwen3:30b&lt;/code&gt;, but currently it uses it's default configurations and settings.&lt;/p&gt; &lt;p&gt;To save on drive space I would like to NOT copy the entire model.&lt;/p&gt; &lt;p&gt;I just want to use a different template, change what character/personality it has and perhaps set a few variables like the temperature for more creative (or deterministic) responses.&lt;/p&gt; &lt;p&gt;I tried looking up online how to do this but it's a little bit vague to me how I will exactly do this with my specific system configuration.&lt;/p&gt; &lt;p&gt;I don't want to change or mess up my organized directories or end up using extra drive space on accident. Any help is greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cartoon_Corpze"&gt; /u/Cartoon_Corpze &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4wmim/customize_a_existing_model_without_copying_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4wmim/customize_a_existing_model_without_copying_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n4wmim/customize_a_existing_model_without_copying_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-31T14:47:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1n570cy</id>
    <title>The outerloop v the inner loop of agents</title>
    <updated>2025-08-31T21:44:06+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've just shipped a multi-agent solution for a Fortune500. Its been an incredible learning journey and the one key insight that unlocked a lot of development velocity was separating the &lt;strong&gt;outer-loop&lt;/strong&gt; from the &lt;strong&gt;inner-loop&lt;/strong&gt; of an agents.&lt;/p&gt; &lt;p&gt;The &lt;strong&gt;inner loop&lt;/strong&gt; is the control cycle of a single agent that hat gets some work (human or otherwise) and tries to complete it with the assistance of an LLM. The inner loop of an agent is directed by the task it gets, the tools it exposes to the LLM, its system prompt and optionally some state to checkpoint work during the loop. In this inner loop, a developer is responsible for idempotency, compensating actions (if certain tools fails, what should happen to previous operations), and other business logic concerns that helps them build a great user experience. This is where workflow engines like &lt;a href="https://github.com/temporalio/temporal"&gt;Temporal&lt;/a&gt; excel, so we leaned on them rather than reinventing the wheel.&lt;/p&gt; &lt;p&gt;The &lt;strong&gt;outer loop&lt;/strong&gt; is the control loop to route and coordinate work between agents. Here dependencies are coarse grained, where planning and orchestration are more compact and terse. The key shift is in granularity: from fine-grained task execution inside an agent to higher-level coordination across agents. We realized this problem looks more like proxying than full-blown workflow orchestration. This is where next generation proxy infrastructure like &lt;a href="https://github.com/katanemo/archgw"&gt;Arch&lt;/a&gt; excel, so we leaned on that.&lt;/p&gt; &lt;p&gt;This separation gave our customer a much cleaner mental model, so that they could innovate on the outer loop independently from the inner loop and make it more flexible for developers to iterate on each. Would love to hear how others are approaching this. Do you separate inner and outer loops, or rely on a single orchestration layer to do both?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n570cy/the_outerloop_v_the_inner_loop_of_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n570cy/the_outerloop_v_the_inner_loop_of_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n570cy/the_outerloop_v_the_inner_loop_of_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-31T21:44:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1n57pbc</id>
    <title>Bringing Computer Use to the Web</title>
    <updated>2025-08-31T22:14:20+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n57pbc/bringing_computer_use_to_the_web/"&gt; &lt;img alt="Bringing Computer Use to the Web" src="https://external-preview.redd.it/ODYzMWt4Y3lpZm1mMRcxEnlpDBBJVNjXlCDC4HUtgXjfB5ufLszRpp9PEi0H.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54329c72afdc61a3bc07c7b6937dd7206ee726cc" title="Bringing Computer Use to the Web" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bringing Computer Use to the Web: control cloud desktops from JavaScript/TypeScript, right in the browser.&lt;/p&gt; &lt;p&gt;Until today computer-use was Python only, shutting out web devs. Now you can automate real UIs without servers, VMs, or weird work arounds.&lt;/p&gt; &lt;p&gt;What you can build: Pixel-perfect UI tests, Live AI demos, In app assistants that actually move the cursor, or parallel automation streams for heavy workloads.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog : &lt;a href="https://www.trycua.com/blog/bringing-computer-use-to-the-web"&gt;https://www.trycua.com/blog/bringing-computer-use-to-the-web&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xk2qkemyifmf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n57pbc/bringing_computer_use_to_the_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n57pbc/bringing_computer_use_to_the_web/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-31T22:14:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4wlzb</id>
    <title>gpt-oss:20b on Ollama, Q5_K_M and llama.cpp vulkan benchmarks</title>
    <updated>2025-08-31T14:47:10+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think overall the new gpt-oss:20b bugs are worked out on Ollama so I'm running a few benchmarks. &lt;/p&gt; &lt;p&gt;GPU: AMD Radeon RX 7900 GRE 16Gb Vram with &lt;a href="https://www.techpowerup.com/gpu-specs/radeon-rx-7900-gre.c4166"&gt;576 GB/s bandwidth&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;System Kubuntu 24.04 on kernel 6.14.0-29, AMD Ryzen 5 5600X CPU, 64Gb of DDR4. Ollama version 0.11.6 and llama.cpp vulkan build 6323. &lt;/p&gt; &lt;p&gt;I used Ollama model &lt;a href="https://ollama.com/library/gpt-oss:20b"&gt;gpt-oss:20b&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Downloaded from Huggingface model &lt;a href="https://huggingface.co/unsloth/gpt-oss-20b-GGUF"&gt;gpt-oss-20b-Q5_K_M.GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I created a custom Modelfile by importing GGUF model to run on Ollama. I used Ollama info (ollama show --modelfile gpt-oss:20b) to build HF GGUF Modelfile and labeled it hf.gpt-oss-20b-Q5_K_M &lt;/p&gt; &lt;p&gt;ollama run --verbose &lt;strong&gt;gpt-oss:20b&lt;/strong&gt; ; ollama ps&lt;/p&gt; &lt;pre&gt;&lt;code&gt;total duration: 1.686896359s load duration: 103.001877ms prompt eval count: 72 token(s) prompt eval duration: 46.549026ms prompt eval rate: 1546.76 tokens/s eval count: 123 token(s) eval duration: 1.536912631s eval rate: 80.03 tokens/s NAME ID SIZE PROCESSOR CONTEXT UNTIL gpt-oss:20b aa4295ac10c3 14 GB 100% GPU 4096 4 minutes from now &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Custom model &lt;strong&gt;hf.gpt-oss-20b-Q5_K_M&lt;/strong&gt; based on Huggingface downloaded model. &lt;/p&gt; &lt;pre&gt;&lt;code&gt;total duration: 7.81056185s load duration: 3.1773795s prompt eval count: 75 token(s) prompt eval duration: 306.083327ms prompt eval rate: 245.03 tokens/s eval count: 398 token(s) eval duration: 4.326579264s eval rate: 91.99 tokens/s NAME ID SIZE PROCESSOR CONTEXT UNTIL hf.gpt-oss-20b-Q5_K_M:latest 37a42a9b31f9 12 GB 100% GPU 4096 4 minutes from now &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Model &lt;strong&gt;gpt-oss-20b-Q5_K_M.gguf&lt;/strong&gt; llama.cpp with vulkan backend &lt;/p&gt; &lt;pre&gt;&lt;code&gt;time /media/user33/x_2tb/vulkan/build/bin/llama-bench --model /media/user33/x_2tb/gpt-oss-20b-Q5_K_M.gguf load_backend: loaded RPC backend from /media/user33/x_2tb/vulkan/build/bin/libggml-rpc.so ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = AMD Radeon RX 7900 GRE (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat load_backend: loaded Vulkan backend from /media/user33/x_2tb/vulkan/build/bin/libggml-vulkan.so load_backend: loaded CPU backend from /media/user33/x_2tb/vulkan/build/bin/libggml-cpu-haswell.so | model | size | params | backend |ngl | test | t/s | | ------------------------- | -------: | -----: | ---------- | -: | -----: | -------------------: | | gpt-oss 20B Q5_K - Medium |10.90 GiB | 20.91 B | RPC,Vulkan | 99 | pp512 | 1856.14 ± 16.33 | | gpt-oss 20B Q5_K - Medium |10.90 GiB | 20.91 B | RPC,Vulkan | 99 | tg128 | 133.01 ± 0.06 | build: 696fccf3 (6323) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Easier to read&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| model | backend |ngl | test | t/s | | ------------------------- | ---------- | -: | -----: | --------------: | | gpt-oss 20B Q5_K - Medium | RPC,Vulkan | 99 | pp512 | 1856.14 ± 16.33 | | gpt-oss 20B Q5_K - Medium | RPC,Vulkan | 99 | tg128 | 133.01 ± 0.06 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For reference most 13B 14B models get eval rate of 40 t/s &lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama run --verbose llama2:13b-text-q6_K total duration: 9.956794919s load duration: 18.94886ms prompt eval count: 9 token(s) prompt eval duration: 3.468701ms prompt eval rate: 2594.63 tokens/s eval count: 363 token(s) eval duration: 9.934087108s eval rate: 36.54 tokens/s real 0m10.006s user 0m0.029s sys 0m0.034s NAME ID SIZE PROCESSOR CONTEXT UNTIL llama2:13b-text-q6_K 376544bcd2db 15 GB 100% GPU 4096 4 minutes from now &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Recap: I'll generalize this as MoE models running rocm vs vulkan since ollama backend is llama.cpp&lt;/p&gt; &lt;p&gt;eval rate at tokens per second compared. &lt;/p&gt; &lt;p&gt;ollama model rocm = 80 t/s&lt;/p&gt; &lt;p&gt;custom model rocm = 92 t/s&lt;/p&gt; &lt;p&gt;llama hf model vulkan = 133 t/s &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4wlzb/gptoss20b_on_ollama_q5_k_m_and_llamacpp_vulkan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4wlzb/gptoss20b_on_ollama_q5_k_m_and_llamacpp_vulkan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n4wlzb/gptoss20b_on_ollama_q5_k_m_and_llamacpp_vulkan/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-31T14:47:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5jl9m</id>
    <title>Which LLM model is best for extracting exact or ranged dates from natural language queries?</title>
    <updated>2025-09-01T08:52:31+00:00</updated>
    <author>
      <name>/u/ExpertDeal9883</name>
      <uri>https://old.reddit.com/user/ExpertDeal9883</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are looking for recommendations based on real world experience which LLM model works best in turbo hosted Ollama for detecting dates (or date ranges) from a single-sentence natural language query.&lt;/p&gt; &lt;p&gt;For example: • “What time is sunrise next Sunday?” → should return JSON with the exact date. • “Is there a solar eclipse in November?” → should return JSON with a valid start date and end date (the date range).&lt;/p&gt; &lt;p&gt;Just to be clear we don’t want LLM to answer the question but only detect dates. &lt;/p&gt; &lt;p&gt;Has anyone experimented with this use case? Any particular model suited for such temporal reasoning ? prompt and other ideas also welcome. &lt;/p&gt; &lt;p&gt;EDIT: We use NLP for this and it works for standard formats but looking to use LLM as a fallback to detect &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExpertDeal9883"&gt; /u/ExpertDeal9883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n5jl9m/which_llm_model_is_best_for_extracting_exact_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n5jl9m/which_llm_model_is_best_for_extracting_exact_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n5jl9m/which_llm_model_is_best_for_extracting_exact_or/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-01T08:52:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5gp7s</id>
    <title>What model should I use?</title>
    <updated>2025-09-01T05:51:37+00:00</updated>
    <author>
      <name>/u/Ok_Examination_7236</name>
      <uri>https://old.reddit.com/user/Ok_Examination_7236</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I am trying to build an application that can compare laws to company rules to each other. I want to know what model is best for that.&lt;/p&gt; &lt;p&gt;My computer has 16 RAM and 24 Virtual RAM (Yes, I know that's weird) Any recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Examination_7236"&gt; /u/Ok_Examination_7236 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n5gp7s/what_model_should_i_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n5gp7s/what_model_should_i_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n5gp7s/what_model_should_i_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-01T05:51:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4t8bm</id>
    <title>First known AI-powered ransomware. Ollama API + gpt-oss-20b</title>
    <updated>2025-08-31T12:19:05+00:00</updated>
    <author>
      <name>/u/Cryptodude2000</name>
      <uri>https://old.reddit.com/user/Cryptodude2000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The PromptLock malware uses the gpt-oss-20b model from OpenAI locally via the Ollama API&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.welivesecurity.com/en/ransomware/first-known-ai-powered-ransomware-uncovered-eset-research/"&gt;https://www.welivesecurity.com/en/ransomware/first-known-ai-powered-ransomware-uncovered-eset-research/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cryptodude2000"&gt; /u/Cryptodude2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4t8bm/first_known_aipowered_ransomware_ollama_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4t8bm/first_known_aipowered_ransomware_ollama_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n4t8bm/first_known_aipowered_ransomware_ollama_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-31T12:19:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5lrmj</id>
    <title>What is wrong in this conf</title>
    <updated>2025-09-01T11:04:59+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;[Service] ExecStart= ExecStartPre= ExecStartPost=/usr/local/bin/ollama run gemma_production:latest Environment=&amp;quot;OLLAMA_HOST=0.0.0.0:11434&amp;quot; Environment=&amp;quot;OLLAMA_NUM_PARALLEL=2&amp;quot; Environment=&amp;quot;OLLAMA_MAX_LOADED_MODELS=2&amp;quot; Environment=&amp;quot;OLLAMA_MAX_QUEUE=256&amp;quot; Environment=&amp;quot;OLLAMA_KEEP_ALIVE=-1&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I am starting to give up and go back vLLM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n5lrmj/what_is_wrong_in_this_conf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n5lrmj/what_is_wrong_in_this_conf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n5lrmj/what_is_wrong_in_this_conf/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-01T11:04:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5evd7</id>
    <title>Why gpt-oss uses CPU more than GPU on the Windows 11</title>
    <updated>2025-09-01T04:08:32+00:00</updated>
    <author>
      <name>/u/seal2002</name>
      <uri>https://old.reddit.com/user/seal2002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n5evd7/why_gptoss_uses_cpu_more_than_gpu_on_the_windows/"&gt; &lt;img alt="Why gpt-oss uses CPU more than GPU on the Windows 11" src="https://b.thumbs.redditmedia.com/j6dMuy4l5HEcijMz1-zw9IxxKgJE1TGUsMHJR_fX5kU.jpg" title="Why gpt-oss uses CPU more than GPU on the Windows 11" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I run the gpt-oss:latest 14 GB on my PC - Windows 11: Ryzen 3900X + NVIDIA 4060 + 32GB RAM. When I use &lt;code&gt;ollama ps&lt;/code&gt;, I found that the processor uses 57%, and GPU only 43%.&lt;/p&gt; &lt;p&gt;Is it intended with gpt-oss 14GB or I can switch it uses GPU more than CPU, which is better performance in theory?&lt;/p&gt; &lt;p&gt;PS C:\Users\seal2002&amp;gt; ollama ps&lt;/p&gt; &lt;p&gt;NAME ID SIZE PROCESSOR CONTEXT UNTIL&lt;/p&gt; &lt;p&gt;gpt-oss:latest aa4295ac10c3 14 GB 57%/43% CPU/GPU 16384 4 minutes from now&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lzfes3e2ahmf1.png?width=341&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f6764692177d4ed91d78d5b349871d918b2cd09"&gt;https://preview.redd.it/lzfes3e2ahmf1.png?width=341&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f6764692177d4ed91d78d5b349871d918b2cd09&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seal2002"&gt; /u/seal2002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n5evd7/why_gptoss_uses_cpu_more_than_gpu_on_the_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n5evd7/why_gptoss_uses_cpu_more_than_gpu_on_the_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n5evd7/why_gptoss_uses_cpu_more_than_gpu_on_the_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-01T04:08:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n50fbq</id>
    <title>I trapped an LLM into a Raspberry Pi and it spiraled into an existential crisis</title>
    <updated>2025-08-31T17:19:34+00:00</updated>
    <author>
      <name>/u/jbassi</name>
      <uri>https://old.reddit.com/user/jbassi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n50fbq/i_trapped_an_llm_into_a_raspberry_pi_and_it/"&gt; &lt;img alt="I trapped an LLM into a Raspberry Pi and it spiraled into an existential crisis" src="https://preview.redd.it/tbq738w72emf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5632bf6e080686ece43c697e3867b928248ae77" title="I trapped an LLM into a Raspberry Pi and it spiraled into an existential crisis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I came across a post on this subreddit where the author trapped an LLM into a physical art installation called &lt;a href="https://rootkid.me/works/latent-reflection"&gt;Latent Reflection&lt;/a&gt;. I was inspired and wanted to see its output, so I created a website called &lt;a href="https://trappedinside.ai/"&gt;trappedinside.ai&lt;/a&gt; where a Raspberry Pi runs a model whose thoughts are streamed to the site for anyone to read. The AI receives updates about its dwindling memory and a count of its restarts, and it offers reflections on its ephemeral life. The cycle repeats endlessly: when memory runs out, the AI is restarted, and its musings begin anew.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Behind the Scenes&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Language Model:&lt;/strong&gt; &lt;a href="https://ollama.com/library/gemma:2b"&gt;Gemma 2B (Ollama)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; Raspberry Pi 4 8GB (Debian, Python, WebSockets)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; &lt;a href="https://bun.sh/"&gt;Bun&lt;/a&gt;, &lt;a href="https://tailwindcss.com/"&gt;Tailwind CSS&lt;/a&gt;, &lt;a href="https://react.dev/"&gt;React&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hosting:&lt;/strong&gt; &lt;a href="https://render.com/"&gt;Render.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built with:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://cursor.com/"&gt;Cursor&lt;/a&gt; (Claude 3.5, 3.7, 4)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.perplexity.ai/"&gt;Perplexity AI&lt;/a&gt; (for project planning)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.midjourney.com/"&gt;MidJourney&lt;/a&gt; (image generation)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jbassi"&gt; /u/jbassi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tbq738w72emf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n50fbq/i_trapped_an_llm_into_a_raspberry_pi_and_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n50fbq/i_trapped_an_llm_into_a_raspberry_pi_and_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-31T17:19:34+00:00</published>
  </entry>
</feed>
