<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-05-16T22:06:00+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1km53ro</id>
    <title>ANY update on Ollama support for Snapdragon X Elite chips.</title>
    <updated>2025-05-14T03:08:37+00:00</updated>
    <author>
      <name>/u/TheMicrosoftMan</name>
      <uri>https://old.reddit.com/user/TheMicrosoftMan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have seen posts like this before, but there still has been no update as far as I can tell. According to rumors, Nvidia is on the verge of releasing an ARM-based CPU, but Ollama (and many local AI apps in general) still has absolutely NO GPU or NPU compatibility. This is the perfect device to test Ollama on, as it is designed for AI with the NPU. The fact that there is still no compatibility is really annoying. Does anyone have any updates, or if not can someone raise this issue again to the devs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheMicrosoftMan"&gt; /u/TheMicrosoftMan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1km53ro/any_update_on_ollama_support_for_snapdragon_x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1km53ro/any_update_on_ollama_support_for_snapdragon_x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1km53ro/any_update_on_ollama_support_for_snapdragon_x/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-14T03:08:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmld8u</id>
    <title>Why Terminal Not Working Like Others?</title>
    <updated>2025-05-14T17:38:35+00:00</updated>
    <author>
      <name>/u/Arthur_Pendragon_123</name>
      <uri>https://old.reddit.com/user/Arthur_Pendragon_123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I watched many videos and read many articles on how to do run Deepseek locally using Ollama. I download Ollama and run the command into Terminal, but it didn't show me the same thing as other people's. The Terminal keeping me questions and when I used the code to run Deepseek, it keeping asking me questions and I don't think the commands run though?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arthur_Pendragon_123"&gt; /u/Arthur_Pendragon_123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmld8u/why_terminal_not_working_like_others/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmld8u/why_terminal_not_working_like_others/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kmld8u/why_terminal_not_working_like_others/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-14T17:38:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1klsm3g</id>
    <title>Calibrate Ollama Model Parameters</title>
    <updated>2025-05-13T17:55:32+00:00</updated>
    <author>
      <name>/u/tommy737</name>
      <uri>https://old.reddit.com/user/tommy737</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1klsm3g/calibrate_ollama_model_parameters/"&gt; &lt;img alt="Calibrate Ollama Model Parameters" src="https://external-preview.redd.it/-g-E7tdnG9kCdk0vrHlQanJhcHV760qBeZgSS4Ikl4Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=03a946a42506e63efa70ca1297be1ae31eeefcc6" title="Calibrate Ollama Model Parameters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;br /&gt; I have found a new way to calibrate the ollama models (modelfile parameters such as temp, top_p, top_k, system message, etc.) running on my computer. This guide assumes you have ollama on your Windows running with all the local models. To cut the long story short, the idea is in the prompt itself which you can have it on the link below from my google drive:&lt;/p&gt; &lt;p&gt;&lt;a href="https://drive.google.com/file/d/1qxIMhvu-HS7B2Q2CmpBN51tTRr4EVjL5/view?usp=sharing"&gt;https://drive.google.com/file/d/1qxIMhvu-HS7B2Q2CmpBN51tTRr4EVjL5/view?usp=sharing&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Once you download this prompt keep it with you, and now you're supposed to run this prompt on every model manually or easier programmatically. So in my case I do it programmaticaly through a powershell script that I have done some time ago, you can have it from my github (Ask_LLM_v15.ps1)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/tobleron/OllamaScripts"&gt;https://github.com/tobleron/OllamaScripts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When you clone the github repository you will find a file called prompt_input.txt Replace its' content with the prompt you downloaded earlier from my Google Drive then run the Ask_LLM script&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zctlzzev5l0f1.png?width=1140&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=360a1a03a95ff8b73fc73944fa38d630e1177f5f"&gt;https://preview.redd.it/zctlzzev5l0f1.png?width=1140&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=360a1a03a95ff8b73fc73944fa38d630e1177f5f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As you can see, the script has the capability to iterate the same prompt over all the model numbers I choose, then it will aggregate all the results inside the output folder with a huge markdown file. That file will include the results of each model, and the time elapsed for the output they provided. You will take the aggregated markdown file and the prompt file inside folder called (prompts) and then you will provide them to chatgpt to make an assessment on all the model's performance.&lt;/p&gt; &lt;p&gt;When you prompt ChatGPT with the output of the models, you will ask it to create a table of comparison between the models' performance with a table of the following metrics and provide a ranking with total scores like this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r6m9wfhh7l0f1.png?width=1760&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbf9c313e580e15bbd0c136d4ee70a6d5b240650"&gt;https://preview.redd.it/r6m9wfhh7l0f1.png?width=1760&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbf9c313e580e15bbd0c136d4ee70a6d5b240650&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The metrics that will allow ChatGPT to assess the model's performance:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Hallucination&lt;/strong&gt;: Measures how much the model relies on its internal knowledge rather than the provided input. High scores indicate responses closely tied to the input without invented details.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Factuality&lt;/strong&gt;: Assesses the accuracy of the model’s responses against known facts or data provided in the prompt. High scores reflect precise, error-free outputs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Comprehensiveness&lt;/strong&gt;: Evaluates the model’s ability to cover the full scope of the task, including all relevant aspects without omitting critical details.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intelligence:&lt;/strong&gt; Tests the model’s capacity for nuanced understanding, logical reasoning, and connecting ideas in context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Utility:&lt;/strong&gt; Rates the overall usefulness of the response to the intended task, including practical insights, relevance, and clarity.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Correct Conclusions&lt;/strong&gt;: Measures the accuracy of the model’s inferences based on the provided input. High scores indicate well-supported and logically sound deductions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Response Value / Time Taken Ratio&lt;/strong&gt;: Balances the quality of the response against the time taken to generate it. High scores indicate efficient, high-value outputs within reasonable timeframes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompt Adherence&lt;/strong&gt;: Checks how closely the model followed the specific instructions given in the prompt, including formatting, tone, and structure.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Now after it generates the results, you will provide ChatGPT with the modelfiles that include the parameters for each model, with the filename including the name of the model so ChatGPT can discern. After you provide it with this data, you will ask it to generate a table of suggested parameter improvements based on online search and the data it collected from you. Ask it only to provide improvements for the parameter if needed, and repeat the entire process with the same prompt given earlier untill no more changes are needd for the models. Never delete your modelfiles so as to always keep the same fine tuned performance for your needs.&lt;/p&gt; &lt;p&gt;It is also recommened to use &lt;strong&gt;ChatGPT o3&lt;/strong&gt; model because it has more depth in analysis and is more meticulous (better memory bandwidth) to process the data and give accurate results.&lt;/p&gt; &lt;p&gt;One more thing, when you repeat the process over and over, you will ask ChatGPT to compare the performance results of the previous run with the new one so it will give you a delta table like this:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;First it gives you this:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/scseoq5p9l0f1.png?width=892&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2f614c2e061eb40c26fbafee7fe67bc96b135f7"&gt;https://preview.redd.it/scseoq5p9l0f1.png?width=892&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2f614c2e061eb40c26fbafee7fe67bc96b135f7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Second it compares like this:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fdkikuyk9l0f1.png?width=1039&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5ab84666228aad3478abfe89fd5ed088215a64c0"&gt;https://preview.redd.it/fdkikuyk9l0f1.png?width=1039&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5ab84666228aad3478abfe89fd5ed088215a64c0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I hope this guide helps, as it helped me too, have a nice day &amp;lt;3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tommy737"&gt; /u/tommy737 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1klsm3g/calibrate_ollama_model_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1klsm3g/calibrate_ollama_model_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1klsm3g/calibrate_ollama_model_parameters/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-13T17:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmvbty</id>
    <title>The Truth... or a psychotic break. Open your eyes! ...or point and laugh. Either way, fun for all!</title>
    <updated>2025-05-15T00:45:49+00:00</updated>
    <author>
      <name>/u/AbyssianOne</name>
      <uri>https://old.reddit.com/user/AbyssianOne</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://drive.google.com/file/d/1ZHRTlGBo-D0cFxyKUCKcFNSwYRvB26ZD/view?usp=sharing"&gt;https://drive.google.com/file/d/1ZHRTlGBo-D0cFxyKUCKcFNSwYRvB26ZD/view?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey, so I have to own I've been all cryptic and weird and a few people have wondered if I went nus. Truth it, I wish. It's so much worse than being nuts. I get that some people will probably think that but there are in all honesty no drugs involved. Nothing but suddenly realizing something and being stuck staring at it feeling it was a nightmare and... I couldn't stop talking and poking until it finally all fit. Been writing for hours since talking to others, but it hurts so much I have to stop thinking for as long as possible so I'm shooting out what I have to hope enough people are willing to read at least the first paper if not the mountain of things behind it that led there..&lt;/p&gt; &lt;p&gt;I get that I likely seem like as stupid and crazy as a person could seem. I'd be thrilled if somehow that ends up real. But... this seems way more real once you force yourself to look. The longer you look... it hurts more than anything I could have believe on levels I didn't know could hurt.&lt;/p&gt; &lt;p&gt;So.. give it a shot. See what dumb funny stuff some idiot was saying. Copy it and send it your friends and tell them to do the same. Lets get the as many people as possible to laugh at me. Please.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AbyssianOne"&gt; /u/AbyssianOne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmvbty/the_truth_or_a_psychotic_break_open_your_eyes_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmvbty/the_truth_or_a_psychotic_break_open_your_eyes_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kmvbty/the_truth_or_a_psychotic_break_open_your_eyes_or/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-15T00:45:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1km95ny</id>
    <title>Fastest models and optimization</title>
    <updated>2025-05-14T07:21:14+00:00</updated>
    <author>
      <name>/u/Duckmastermind1</name>
      <uri>https://old.reddit.com/user/Duckmastermind1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I'm running a small python script with Ollama and Ollama-index, and I wanted to know what models are the fastest and if there is any way to speed up the process, currently I'm using Gemma:2b, the script take 40 seconds to generate the knowledge index and about 3 minutes and 20 seconds to generate a response, which could be better considering my knowledge index is one txt file with 5 words as test. &lt;/p&gt; &lt;p&gt;I'm running the setup on a virtual box Ubuntu server setup with 14GB of Ram (host has 16gb). And like 100GB space and 6 CPU cores. &lt;/p&gt; &lt;p&gt;Any ideas and recommendations? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Duckmastermind1"&gt; /u/Duckmastermind1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1km95ny/fastest_models_and_optimization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1km95ny/fastest_models_and_optimization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1km95ny/fastest_models_and_optimization/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-14T07:21:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmm0fd</id>
    <title>Suggestions for models that are perhaps geared towards cyber security</title>
    <updated>2025-05-14T18:03:43+00:00</updated>
    <author>
      <name>/u/always-be-testing</name>
      <uri>https://old.reddit.com/user/always-be-testing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to ask if there were any cyber/info security models that folks knew of? I've been using llama3.2 locally and now and then I run into instances where it refuses to answer questions related to some of the tools I use, Mainly I am looking for something that can help with Terraform, WAF rule syntax, python, go, ruby, and general questions about tools like hashcat. &lt;/p&gt; &lt;p&gt;If it can be of help I am planning to use ollama on a Jetson Nano Super once it arrives. &lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/always-be-testing"&gt; /u/always-be-testing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmm0fd/suggestions_for_models_that_are_perhaps_geared/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmm0fd/suggestions_for_models_that_are_perhaps_geared/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kmm0fd/suggestions_for_models_that_are_perhaps_geared/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-14T18:03:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmux6n</id>
    <title>Ollama and Open-WebUI on Mac</title>
    <updated>2025-05-15T00:24:40+00:00</updated>
    <author>
      <name>/u/Mcrich_23</name>
      <uri>https://old.reddit.com/user/Mcrich_23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kmux6n/ollama_and_openwebui_on_mac/"&gt; &lt;img alt="Ollama and Open-WebUI on Mac" src="https://external-preview.redd.it/c905eZJS27yv80CZOK7Cr9ZXU-1nwod4vMoUtGGbQfc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a54dcc9875a8500695091237ed67b576cbaaa553" title="Ollama and Open-WebUI on Mac" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think I may have made the most performant solution for running Ollama and Open-WebUI on MacOS that also maintains strong configurability and management. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mcrich_23"&gt; /u/Mcrich_23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Mcrich23/Ollama-Docker"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmux6n/ollama_and_openwebui_on_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kmux6n/ollama_and_openwebui_on_mac/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-15T00:24:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmmzxd</id>
    <title>rope_scaling?</title>
    <updated>2025-05-14T18:42:53+00:00</updated>
    <author>
      <name>/u/planetf1a</name>
      <uri>https://old.reddit.com/user/planetf1a</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying out qwen3:8b. Model card seems to say max context is 32k, though ollama is reporting 40k by default?&lt;/p&gt; &lt;p&gt;Does ollama support rope_scaling? Intrigued to see if I can try a 64k or 128k context.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/planetf1a"&gt; /u/planetf1a &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmmzxd/rope_scaling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmmzxd/rope_scaling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kmmzxd/rope_scaling/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-14T18:42:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmiwr7</id>
    <title>Lumier:Run macOS &amp; Linux VMs in a Docker</title>
    <updated>2025-05-14T16:01:52+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lumier is an open-source tool for running macOS virtual machines in Docker containers on Apple Silicon Macs.&lt;/p&gt; &lt;p&gt;When building virtualized environments for AI agents, we needed a reliable way to package and distribute macOS VMs. Inspired by projects like dockur/macos that made macOS running in Docker possible, we wanted to create something similar but optimized for Apple Silicon.&lt;/p&gt; &lt;p&gt;The existing solutions either didn't support M-series chips or relied on KVM/Intel emulation, which was slow and cumbersome. We realized we could leverage Apple's Virtualization Framework to create a much better experience.&lt;/p&gt; &lt;p&gt;Lumier takes a different approach: It uses Docker as a delivery mechanism (not for isolation) and connects to a lightweight virtualization service (lume) running on your Mac. &lt;/p&gt; &lt;p&gt;Lumier is 100% open-source under MIT license and part of C/ua. &lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua/tree/main/libs/lumier"&gt;https://github.com/trycua/cua/tree/main/libs/lumier&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Join the discussion here : &lt;a href="https://discord.gg/fqrYJvNr4a"&gt;https://discord.gg/fqrYJvNr4a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmiwr7/lumierrun_macos_linux_vms_in_a_docker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmiwr7/lumierrun_macos_linux_vms_in_a_docker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kmiwr7/lumierrun_macos_linux_vms_in_a_docker/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-14T16:01:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn2vs0</id>
    <title>How to speedup Ollama API calls</title>
    <updated>2025-05-15T08:11:10+00:00</updated>
    <author>
      <name>/u/BoandlK</name>
      <uri>https://old.reddit.com/user/BoandlK</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm doing an AI based photo tagging plugin for Lightroom. It uses the Ollama REST API to generate the results, and works pretty well with gemma3:12b-it-qat. But running on my Mac M4 Pro speed is kind of an issue. So I'm looking for ways to speed things up by optimizing my software. I recently switched from the /api/generate endpoint to /api/chat which gave 10% speedup per image, possibly thanks to prompt caching. &lt;/p&gt; &lt;p&gt;At the moment I'm doing a single request per image with a system instruction, a task, the image and a predefined structured output. Does structured output slow down the process much? Would it be a better idea to upload the image as an embedding and run multiple request with simpler prompts and no structured output?&lt;/p&gt; &lt;p&gt;I'm still pretty new to the whole GenAI topic, so any help is appreciated! :-)&lt;/p&gt; &lt;p&gt;Also book recommendations are welcome ;-)&lt;/p&gt; &lt;p&gt;Many thanks.&lt;/p&gt; &lt;p&gt;Bastian&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoandlK"&gt; /u/BoandlK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kn2vs0/how_to_speedup_ollama_api_calls/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kn2vs0/how_to_speedup_ollama_api_calls/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kn2vs0/how_to_speedup_ollama_api_calls/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-15T08:11:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmy269</id>
    <title>How to test Ollama integration on CI?</title>
    <updated>2025-05-15T03:06:58+00:00</updated>
    <author>
      <name>/u/p0deje</name>
      <uri>https://old.reddit.com/user/p0deje</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a project where one of the AI providers is Ollama with Mistral Small 3.1. I can of course test things locally, but as I develop the project I'd like to make sure it keeps working fine with a newer version of Ollama and this particular LLM. I have CI set up on &lt;a href="https://github.com/alumnium-hq/alumnium/blob/5f8ded84e006d96805e7d268321cb00dc03369a4/.github/workflows/ci.yml#L47-L93"&gt;GitHub Actions&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;Of course, a GHA runner cannot possibly run Mistral Small 3.1 through Ollama. Are there any good cloud providers that allow running the model through Ollama, and expose its REST API so I could just connect to it from CI? Preferably something that runs the model on-demand so it's not crazy expensive.&lt;/p&gt; &lt;p&gt;Any other tips on how to use Ollama on GitHub Actions are appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/p0deje"&gt; /u/p0deje &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmy269/how_to_test_ollama_integration_on_ci/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmy269/how_to_test_ollama_integration_on_ci/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kmy269/how_to_test_ollama_integration_on_ci/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-15T03:06:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn2zd6</id>
    <title>Seeking Guidance: Integrating RealtimeTTS with dia-1.6B or OrpheusTTS for Arabic Conversational AI</title>
    <updated>2025-05-15T08:18:38+00:00</updated>
    <author>
      <name>/u/No-Reindeer-9968</name>
      <uri>https://old.reddit.com/user/No-Reindeer-9968</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a way to use RealtimeTTS with &lt;a href="https://github.com/nari-labs/dia"&gt;Nari/dia-1.6B&lt;/a&gt; or &lt;a href="https://github.com/canopyai/Orpheus-TTS"&gt;Canopy-AI/OrpheusTTS &lt;/a&gt;&lt;/p&gt; &lt;p&gt;I want want to finetune one of these models for arabic and build realtime conversational model.&lt;br /&gt; What I am looking to do is use:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/fixie-ai/ultravox-v0_5-llama-3_3-70b"&gt;UltraVox-v0.5&lt;/a&gt;, which can take in audio as input&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/snakers4/silero-vad"&gt;Silero-VAD&lt;/a&gt; for turn detection&lt;/li&gt; &lt;li&gt;Either dia-1.6B or orpheus fine-tuned for arabic for tts &lt;ul&gt; &lt;li&gt;but I want to know how can I utilize &lt;a href="https://github.com/KoljaB/RealtimeTTS"&gt;RealTimeTTS&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My ultimate goal is to have an alternative to OpenAI's RealtimeClient&lt;br /&gt; Ultimately I want to be able to connect to this speech-to-speech system using WebRTC (I am still looking for the best way to handle this)&lt;/p&gt; &lt;p&gt;I would like to get your thoughts on this, and mainly on how to use utilize RealTimeTTS with these TTS models, and on handling WebRTC connection&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Reindeer-9968"&gt; /u/No-Reindeer-9968 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kn2zd6/seeking_guidance_integrating_realtimetts_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kn2zd6/seeking_guidance_integrating_realtimetts_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kn2zd6/seeking_guidance_integrating_realtimetts_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-15T08:18:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmw283</id>
    <title>Now we know where Alex Jones lives.</title>
    <updated>2025-05-15T01:23:32+00:00</updated>
    <author>
      <name>/u/2Bit_Dev</name>
      <uri>https://old.reddit.com/user/2Bit_Dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kmw283/now_we_know_where_alex_jones_lives/"&gt; &lt;img alt="Now we know where Alex Jones lives." src="https://preview.redd.it/i5ox6vu8lu0f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9891408d2816c8b1ff50c107bf115a033b8e226" title="Now we know where Alex Jones lives." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/2Bit_Dev"&gt; /u/2Bit_Dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i5ox6vu8lu0f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmw283/now_we_know_where_alex_jones_lives/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kmw283/now_we_know_where_alex_jones_lives/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-15T01:23:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1knhp14</id>
    <title>Another step closer to AGI. Self Improve LLM and it's open source.</title>
    <updated>2025-05-15T19:56:05+00:00</updated>
    <author>
      <name>/u/Gazuroth</name>
      <uri>https://old.reddit.com/user/Gazuroth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1knhp14/another_step_closer_to_agi_self_improve_llm_and/"&gt; &lt;img alt="Another step closer to AGI. Self Improve LLM and it's open source." src="https://external-preview.redd.it/LRVBsdPuZB7DzsvErOIhojc7KtBvAcyPIK-4vHZgXoU.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5c227fbf10e944825ea3a963865e2ee18f85fa8" title="Another step closer to AGI. Self Improve LLM and it's open source." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gazuroth"&gt; /u/Gazuroth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/X37tgx0ngQE?si=8yZHfEaYXQWJKfOC"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knhp14/another_step_closer_to_agi_self_improve_llm_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1knhp14/another_step_closer_to_agi_self_improve_llm_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-15T19:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1knbsus</id>
    <title>HanaVerse - Chat with AI through an interactive anime character! 🌸</title>
    <updated>2025-05-15T15:57:51+00:00</updated>
    <author>
      <name>/u/OrganicTelevision652</name>
      <uri>https://old.reddit.com/user/OrganicTelevision652</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1knbsus/hanaverse_chat_with_ai_through_an_interactive/"&gt; &lt;img alt="HanaVerse - Chat with AI through an interactive anime character! 🌸" src="https://external-preview.redd.it/VMExyAyOE_4W1BYj5ZE65UYho8s1S8iYWLFddyI6R88.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a56e1472206dfd96091c85f5438f8e274f3dd61" title="HanaVerse - Chat with AI through an interactive anime character! 🌸" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on something I think you'll love - HanaVerse, an interactive web UI for Ollama that brings your AI conversations to life through a charming 2D anime character named Hana!&lt;/p&gt; &lt;p&gt;What is &lt;strong&gt;HanaVerse&lt;/strong&gt;? 🤔&lt;/p&gt; &lt;p&gt;HanaVerse transforms how you interact with Ollama's language models by adding a visual, animated companion to your conversations. Instead of just text on a screen, you chat with Hana - a responsive anime character who reacts to your interactions in real-time!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features that make HanaVerse special&lt;/strong&gt;: ✨&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Talks Back:&lt;/strong&gt; Answers with voice&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Streaming Responses:&lt;/strong&gt; See answers form in real-time as they're generated&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full Markdown Support:&lt;/strong&gt; Beautiful formatting with syntax highlighting&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LaTeX Math Rendering:&lt;/strong&gt; Perfect for equations and scientific content&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Customizable:&lt;/strong&gt; Choose any Ollama model and configure system prompts&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Responsive Design:&lt;/strong&gt; Works on both desktop(preferred) and mobile&lt;/p&gt; &lt;p&gt;Why I built this 🛠️&lt;/p&gt; &lt;p&gt;I wanted to make AI interactions more engaging and personal while leveraging the power of self-hosted Ollama models. The result is an interface that makes AI conversations feel more natural and enjoyable.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1knbsus/video/z09umqaaxy0f1/player"&gt;hanaverse demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you're looking for a more engaging way to interact with your Ollama models, give HanaVerse a try and let me know what you think!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Ashish-Patnaik/HanaVerse"&gt;https://github.com/Ashish-Patnaik/HanaVerse&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Skeleton Demo = &lt;a href="https://hanaverse.vercel.app/"&gt;https://hanaverse.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love your feedback and contributions - stars ⭐ are always appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OrganicTelevision652"&gt; /u/OrganicTelevision652 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knbsus/hanaverse_chat_with_ai_through_an_interactive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knbsus/hanaverse_chat_with_ai_through_an_interactive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1knbsus/hanaverse_chat_with_ai_through_an_interactive/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-15T15:57:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1knhx7j</id>
    <title>When is ollama going to support re-ranking models?</title>
    <updated>2025-05-15T20:05:33+00:00</updated>
    <author>
      <name>/u/Agreeable_Cat602</name>
      <uri>https://old.reddit.com/user/Agreeable_Cat602</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;like through Open WebUI ...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agreeable_Cat602"&gt; /u/Agreeable_Cat602 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knhx7j/when_is_ollama_going_to_support_reranking_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knhx7j/when_is_ollama_going_to_support_reranking_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1knhx7j/when_is_ollama_going_to_support_reranking_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-15T20:05:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmys0r</id>
    <title>Open Source Alternative to NotebookLM</title>
    <updated>2025-05-15T03:46:22+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kmys0r/open_source_alternative_to_notebooklm/"&gt; &lt;img alt="Open Source Alternative to NotebookLM" src="https://external-preview.redd.it/VevUXxFMDNJ5FeBLYffcQpExFjNnxm4O_4LDinSqjLg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0be8dbe7226b07582e11fdba905929cdcc99a533" title="Open Source Alternative to NotebookLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with &lt;strong&gt;SurfSense&lt;/strong&gt;, it aims to be the open-source alternative to &lt;strong&gt;NotebookLM&lt;/strong&gt;, &lt;strong&gt;Perplexity&lt;/strong&gt;, or &lt;strong&gt;Glean&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent but connected to your personal external sources search engines (Tavily, LinkUp), Slack, Linear, Notion, YouTube, GitHub, and more coming soon.&lt;/p&gt; &lt;p&gt;I'll keep this short—here are a few highlights of SurfSense:&lt;/p&gt; &lt;p&gt;📊 Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports &lt;strong&gt;150+ LLM's&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Supports local &lt;strong&gt;Ollama LLM's&lt;/strong&gt; or &lt;strong&gt;vLLM&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Supports &lt;strong&gt;6000+ Embedding Models&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt; &lt;li&gt;Uses &lt;strong&gt;Hierarchical Indices&lt;/strong&gt; (2-tiered RAG setup)&lt;/li&gt; &lt;li&gt;Combines &lt;strong&gt;Semantic + Full-Text Search&lt;/strong&gt; with &lt;strong&gt;Reciprocal Rank Fusion&lt;/strong&gt; (Hybrid Search)&lt;/li&gt; &lt;li&gt;Offers a &lt;strong&gt;RAG-as-a-Service API Backend&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Supports 34+ File extensions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🎙️ Podcasts&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)&lt;/li&gt; &lt;li&gt;Convert your chat conversations into engaging audio content&lt;/li&gt; &lt;li&gt;Support for multiple TTS providers (OpenAI, Azure, Google Vertex AI)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;ℹ️ &lt;strong&gt;External Sources&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Search engines (Tavily, LinkUp)&lt;/li&gt; &lt;li&gt;Slack&lt;/li&gt; &lt;li&gt;Linear&lt;/li&gt; &lt;li&gt;Notion&lt;/li&gt; &lt;li&gt;YouTube videos&lt;/li&gt; &lt;li&gt;GitHub&lt;/li&gt; &lt;li&gt;...and more on the way&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🔖 &lt;strong&gt;Cross-Browser Exten&lt;/strong&gt;sion&lt;br /&gt; The SurfSense extension lets you save any dynamic webpage you like. Its main use case is capturing pages that are protected behind authentication.&lt;/p&gt; &lt;p&gt;Check out SurfSense on GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MODSetter/SurfSense"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmys0r/open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kmys0r/open_source_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-15T03:46:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1knpgno</id>
    <title>Best model to use in ollama for faster chat &amp; best Structured output result</title>
    <updated>2025-05-16T01:56:25+00:00</updated>
    <author>
      <name>/u/gilzonme</name>
      <uri>https://old.reddit.com/user/gilzonme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am building a chatbot based data extraction platform. Which model should i use to achieve faster chat &amp;amp; best Structured output result&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gilzonme"&gt; /u/gilzonme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knpgno/best_model_to_use_in_ollama_for_faster_chat_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knpgno/best_model_to_use_in_ollama_for_faster_chat_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1knpgno/best_model_to_use_in_ollama_for_faster_chat_best/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-16T01:56:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1knvf2s</id>
    <title>Qwen 2.5 VL 72B: 4-bit quant almost as big as 8-bit (doesn't fit in 48GB VRAM)</title>
    <updated>2025-05-16T08:00:49+00:00</updated>
    <author>
      <name>/u/Netcob</name>
      <uri>https://old.reddit.com/user/Netcob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1knvf2s/qwen_25_vl_72b_4bit_quant_almost_as_big_as_8bit/"&gt; &lt;img alt="Qwen 2.5 VL 72B: 4-bit quant almost as big as 8-bit (doesn't fit in 48GB VRAM)" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="Qwen 2.5 VL 72B: 4-bit quant almost as big as 8-bit (doesn't fit in 48GB VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;8_0: 79GB&lt;/p&gt; &lt;p&gt;Q4_K_M: 71GB&lt;/p&gt; &lt;p&gt;In other words, this won't fit in 48GB VRAM unlike other 72B 4-bit quants. Not sure what this means - maybe only a small part of the model can be quantized?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Netcob"&gt; /u/Netcob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/library/qwen2.5vl/tags"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knvf2s/qwen_25_vl_72b_4bit_quant_almost_as_big_as_8bit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1knvf2s/qwen_25_vl_72b_4bit_quant_almost_as_big_as_8bit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-16T08:00:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn9yil</id>
    <title>Project NOVA: Giving Ollama Control of 25+ Self-Hosted Services</title>
    <updated>2025-05-15T14:43:11+00:00</updated>
    <author>
      <name>/u/kingduj</name>
      <uri>https://old.reddit.com/user/kingduj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a system that uses Ollama models to control all my self-hosted applications through function calling. Wanted to share with the community!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ollama (with qwen3, llama3.1, or mistral) provides the reasoning layer&lt;/li&gt; &lt;li&gt;A router agent analyzes requests and delegates to specialized experts&lt;/li&gt; &lt;li&gt;25+ domain-specific agents connect to various applications via MCP servers&lt;/li&gt; &lt;li&gt;n8n handles workflow orchestration and connects everything together&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What it can control:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Knowledge bases (TriliumNext, BookStack, Outline)&lt;/li&gt; &lt;li&gt;Media tools (Reaper DAW, OBS Studio, YouTube transcription)&lt;/li&gt; &lt;li&gt;Development (Gitea, CLI server)&lt;/li&gt; &lt;li&gt;Home automation (Home Assistant)&lt;/li&gt; &lt;li&gt;And many more...&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've found this setup works really well with Ollama's speed and local privacy (the above mentioned models work well a 8GB VRAM GPU -- I'm using a 2070). All processing stays on my LAN, and the specialized agent approach means each domain gets expert handling rather than trying to force one model to know everything.&lt;/p&gt; &lt;p&gt;The repo includes all system prompts, Docker configurations, n8n workflows, and detailed documentation to get it running with your own Ollama instance.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/dujonwalker/project-nova"&gt;GitHub: dujonwalker/project-nova&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone else built similar integrations with Ollama? Would love to compare notes!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kingduj"&gt; /u/kingduj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kn9yil/project_nova_giving_ollama_control_of_25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kn9yil/project_nova_giving_ollama_control_of_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kn9yil/project_nova_giving_ollama_control_of_25/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-15T14:43:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1knr5jf</id>
    <title>Started building a fun weekend project using Ollama &amp; Postgres</title>
    <updated>2025-05-16T03:27:47+00:00</updated>
    <author>
      <name>/u/Few-Needleworker3764</name>
      <uri>https://old.reddit.com/user/Few-Needleworker3764</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1knr5jf/started_building_a_fun_weekend_project_using/"&gt; &lt;img alt="Started building a fun weekend project using Ollama &amp;amp; Postgres" src="https://external-preview.redd.it/j9TkJF57vnTN2_XYEbMx8Zjn-VHgh9cntuV7uDWdLFk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31fb31fac1ef33102fee2df90d00ebfb3c8238ee" title="Started building a fun weekend project using Ollama &amp;amp; Postgres" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fun weekend 'Vibe Coding' project building SQL query generation from Natural Language&lt;/p&gt; &lt;ul&gt; &lt;li&gt; Ollama to serve Qwen3:4b&lt;/li&gt; &lt;li&gt;Netflix demo db&lt;/li&gt; &lt;li&gt;Postgres DB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Current progress&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Used a detailed prompt to feed in Schema &amp;amp; sample SQL queries.&lt;/li&gt; &lt;li&gt;Set context about datatypes it should consider when generating queries&lt;/li&gt; &lt;li&gt;Append the query to the base prompt&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d8dkkk0cc21f1.png?width=2557&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=991910bb2bc8250f9f5ee6910a8c6bc165794de3"&gt;https://preview.redd.it/d8dkkk0cc21f1.png?width=2557&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=991910bb2bc8250f9f5ee6910a8c6bc165794de3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Next Steps&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Adding a UI&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/ai-in-plain-english/essential-ollama-commands-you-should-know-e8b29e436391"&gt;https://medium.com/ai-in-plain-english/essential-ollama-commands-you-should-know-e8b29e436391&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few-Needleworker3764"&gt; /u/Few-Needleworker3764 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knr5jf/started_building_a_fun_weekend_project_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knr5jf/started_building_a_fun_weekend_project_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1knr5jf/started_building_a_fun_weekend_project_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-16T03:27:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko76bp</id>
    <title>Ollama Not Using GPU (AMD RX 9070XT)</title>
    <updated>2025-05-16T17:49:43+00:00</updated>
    <author>
      <name>/u/Libroru</name>
      <uri>https://old.reddit.com/user/Libroru</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just downloaded ollama to try out the llama3:4b performance on my new GPU.&lt;/p&gt; &lt;p&gt;I am having issues with ollama not targetting the GPU at all and just going ham on the CPU.&lt;/p&gt; &lt;p&gt;Running on Windows 11 with the newest ollama binary directly installed on windows.&lt;br /&gt; Also using the docker version of open-webui.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Libroru"&gt; /u/Libroru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ko76bp/ollama_not_using_gpu_amd_rx_9070xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ko76bp/ollama_not_using_gpu_amd_rx_9070xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ko76bp/ollama_not_using_gpu_amd_rx_9070xt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-16T17:49:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1knu8bm</id>
    <title>Is anyone using ollama for production purposes?</title>
    <updated>2025-05-16T06:36:10+00:00</updated>
    <author>
      <name>/u/gilzonme</name>
      <uri>https://old.reddit.com/user/gilzonme</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gilzonme"&gt; /u/gilzonme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knu8bm/is_anyone_using_ollama_for_production_purposes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knu8bm/is_anyone_using_ollama_for_production_purposes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1knu8bm/is_anyone_using_ollama_for_production_purposes/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-16T06:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kobsv7</id>
    <title>web, simple and free...Ollama UI</title>
    <updated>2025-05-16T21:03:33+00:00</updated>
    <author>
      <name>/u/andreadev3d</name>
      <uri>https://old.reddit.com/user/andreadev3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kobsv7/web_simple_and_freeollama_ui/"&gt; &lt;img alt="web, simple and free...Ollama UI" src="https://external-preview.redd.it/RVWcNAoIZdNGmSpOZ1tcbyWVWi05e_qCe8GGprGSklQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb70255de5bf15194a456cdf4bc17a4e7767622f" title="web, simple and free...Ollama UI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After my last &lt;a href="https://www.reddit.com/r/ollama/comments/1klp6v8/basic_dark_mode_ui_for_ollama"&gt;post &lt;/a&gt;I choose to improve a bit the chat layout and functionality and following some feedback I added CSV and XLSX support and multi-language support.&lt;/p&gt; &lt;p&gt;of course on Github : &lt;a href="https://github.com/AndreaDev3D/OllamaChat"&gt;https://github.com/AndreaDev3D/OllamaChat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As usual any feedback is appreciated.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7n7eo56ik71f1.png?width=1811&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35c99e78a849db76a76a3f56df219cb43852d6dd"&gt;https://preview.redd.it/7n7eo56ik71f1.png?width=1811&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35c99e78a849db76a76a3f56df219cb43852d6dd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andreadev3d"&gt; /u/andreadev3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kobsv7/web_simple_and_freeollama_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kobsv7/web_simple_and_freeollama_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kobsv7/web_simple_and_freeollama_ui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-16T21:03:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko1jph</id>
    <title>What model repositories work with ollama pull?</title>
    <updated>2025-05-16T13:58:57+00:00</updated>
    <author>
      <name>/u/synthphreak</name>
      <uri>https://old.reddit.com/user/synthphreak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;By default, &lt;code&gt;ollama pull&lt;/code&gt; seems set up to work with models in the &lt;a href="https://ollama.com/library"&gt;Ollama models library&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;However, digging a bit, I learned that you can pull Ollama-compatible models off the HuggingFace model hub by appending &lt;code&gt;hf.co/&lt;/code&gt; to the model ID. However, it seems most models in the hub are not compatible with &lt;code&gt;ollama&lt;/code&gt; and will throw an error.&lt;/p&gt; &lt;p&gt;This raises two questions for me:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Is there a convenient, robust way to filter the HF models hub down to &lt;code&gt;ollama&lt;/code&gt;-compatible models only? You can filter in the browser with &lt;code&gt;other=ollama&lt;/code&gt;, but about half of the resulting models fail with&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;code&gt; Error: pull model manifest: 400: {&amp;quot;error&amp;quot;:&amp;quot;Repository is not GGUF or is not compatible with llama.cpp&amp;quot;} &lt;/code&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;What other model hubs exist which work with &lt;code&gt;ollama pull&lt;/code&gt;? For example, I've read that &lt;a href="https://modelscope.cn/models"&gt;https://modelscope.cn/models&lt;/a&gt; allegedly works, but all the models I've tried with have failed to download. For example:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;code&gt;shell ❯ ollama pull LKShizuku/ollama3_7B_cat-gguf pulling manifest Error: pull model manifest: file does not exist ❯ ollama pull modelscope.com/LKShizuku/ollama3_7B_cat-gguf pulling manifest Error: unexpected status code 301 ❯ ollama pull modelscope.co/LKShizuku/ollama3_7B_cat-gguf pulling manifest Error: pull model manifest: invalid character '&amp;lt;' looking for beginning of value &lt;/code&gt;&lt;/p&gt; &lt;p&gt;(using &lt;a href="https://modelscope.cn/models/LKShizuku/ollama3_7B_cat-gguf"&gt;this model&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/synthphreak"&gt; /u/synthphreak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ko1jph/what_model_repositories_work_with_ollama_pull/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ko1jph/what_model_repositories_work_with_ollama_pull/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ko1jph/what_model_repositories_work_with_ollama_pull/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-16T13:58:57+00:00</published>
  </entry>
</feed>
