<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-24T10:48:49+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ivd3ip</id>
    <title>Ollama frontend using ChatterUI</title>
    <updated>2025-02-22T07:08:47+00:00</updated>
    <author>
      <name>/u/----Val----</name>
      <uri>https://old.reddit.com/user/----Val----</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ivd3ip/ollama_frontend_using_chatterui/"&gt; &lt;img alt="Ollama frontend using ChatterUI" src="https://external-preview.redd.it/OWU1M2MwNzc0bmtlMdFQRLeEpBBtEHSSn-qbmJ4l5ADqGikYUAGhEl3DS_ow.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aaea36dd077ddc1ee7db47b15f1d2574fca9e4ad" title="Ollama frontend using ChatterUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! I've been working on my app, ChatterUI for a while now, and I just wanted to show off its use as a frontend for various LLM services, including a few open source projects like Ollama!&lt;/p&gt; &lt;p&gt;You can get the app here (android only): &lt;a href="https://github.com/Vali-98/ChatterUI/releases/latest"&gt;https://github.com/Vali-98/ChatterUI/releases/latest&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/----Val----"&gt; /u/----Val---- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/oy6s7lf74nke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivd3ip/ollama_frontend_using_chatterui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivd3ip/ollama_frontend_using_chatterui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T07:08:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivrfjt</id>
    <title>8x AMD Instinct Mi50 Server + Llama-3.3-70B-Instruct + vLLM + Tensor Parallelism -&gt; 25t/s</title>
    <updated>2025-02-22T19:53:29+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ort8fxcawqke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivrfjt/8x_amd_instinct_mi50_server_llama3370binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivrfjt/8x_amd_instinct_mi50_server_llama3370binstruct/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T19:53:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivl5fo</id>
    <title>Is it worth running 7b dpsk r1 or should I buy more ram?</title>
    <updated>2025-02-22T15:26:57+00:00</updated>
    <author>
      <name>/u/dTechAnimeGamingGuy</name>
      <uri>https://old.reddit.com/user/dTechAnimeGamingGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My pc specs Amd ryzen 5600g Gpu rx6600 8gb vram Ram 16gb &lt;/p&gt; &lt;p&gt;I usually work with code and reasoning for copywriting or learning. I’m a no code developer / designer and using mainly for generating scripts.&lt;/p&gt; &lt;p&gt;Been using ChatGPT free version till now but thinking to upgrading but I’m not sure if I should buy plus subscription/ get OpenAI/deepseek api or just upgrade my pc for local llm.&lt;/p&gt; &lt;p&gt;My current setup can run bartkowski’s dpsk r1 Q_6 7b/8b somewhat well. &lt;/p&gt; &lt;p&gt;P.S. I know my gpu isn’t officially supported. Found a GitHub repo that bypasses that so it’s ok. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dTechAnimeGamingGuy"&gt; /u/dTechAnimeGamingGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivl5fo/is_it_worth_running_7b_dpsk_r1_or_should_i_buy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivl5fo/is_it_worth_running_7b_dpsk_r1_or_should_i_buy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivl5fo/is_it_worth_running_7b_dpsk_r1_or_should_i_buy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T15:26:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivmel8</id>
    <title>I designed “Prompt Targets” - a higher level abstraction than function-calling. Route to downstream agents, clarify questions and trigger common agentic scenarios</title>
    <updated>2025-02-22T16:21:50+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ivmel8/i_designed_prompt_targets_a_higher_level/"&gt; &lt;img alt="I designed “Prompt Targets” - a higher level abstraction than function-calling. Route to downstream agents, clarify questions and trigger common agentic scenarios" src="https://preview.redd.it/lyhvhusuupke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a73aee9d1cfdc50799b4ac3c8ed979288c5a940" title="I designed “Prompt Targets” - a higher level abstraction than function-calling. Route to downstream agents, clarify questions and trigger common agentic scenarios" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Function calling is now a core primitive now in building agentic applications - but there is still alot of engineering muck and duck tape required to build an accurate conversational experience. Meaning - sometimes you need to forward a prompt to the right down stream agent to handle the query, or ask for clarifying questions before you can trigger/ complete an agentic task.&lt;/p&gt; &lt;p&gt;I’ve designed a higher level abstraction called &amp;quot;prompt targets&amp;quot; inspired and modeled after how load balancers direct traffic to backend servers. The idea is to process prompts, extract critical information from them and effectively route to a downstream agent or task to handle the user prompt. The devex doesn’t deviate too much from function calling semantics - but the functionality operates at a higher level of abstraction to simplify building agentic systems&lt;/p&gt; &lt;p&gt;So how do you get started? Check out the OSS project: &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt; for more &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lyhvhusuupke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivmel8/i_designed_prompt_targets_a_higher_level/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivmel8/i_designed_prompt_targets_a_higher_level/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T16:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivx91h</id>
    <title>Can someone help me figure out how to do this?</title>
    <updated>2025-02-23T00:20:47+00:00</updated>
    <author>
      <name>/u/MrWinterCreates</name>
      <uri>https://old.reddit.com/user/MrWinterCreates</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm wanting to set something up with ollama that all it does is I can add a pdf, then ask it questions and get accurate answers. While being ran from my own pc. How do I do this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrWinterCreates"&gt; /u/MrWinterCreates &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivx91h/can_someone_help_me_figure_out_how_to_do_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivx91h/can_someone_help_me_figure_out_how_to_do_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivx91h/can_someone_help_me_figure_out_how_to_do_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T00:20:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw63re</id>
    <title>Is it possible to deploy Ollama on AWS and allow access to specific IPS only?</title>
    <updated>2025-02-23T09:06:24+00:00</updated>
    <author>
      <name>/u/asji4</name>
      <uri>https://old.reddit.com/user/asji4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a very simple app that just setups Ollama on flask. Works fine locally and on a public EC2 DNS, but I can't seem to figure out how to get it to run with AWS cloudfront. Here's what I have done so far:&lt;/p&gt; &lt;p&gt;Application Configuration: - Flask application running on localhost:8080. - Ollama service running on localhost:11434.&lt;/p&gt; &lt;p&gt;Deployment Environment: - Both services are hosted on a single EC2 instance. - AWS CloudFront is used as a content delivery network.&lt;/p&gt; &lt;p&gt;What works - the application works perfectly locally and when deployed on a public ec2 DNS on HTTP - I have a security group setup so that only flask is accessible via public, and Ollama has no access except for being called by flask internally via port number &lt;/p&gt; &lt;p&gt;Issue Encountered: - Post-deployment on cloudfront the Flask application is unable to communicate with the Ollama service because of my security group restrictions to block 0.0.0.0 but allow inbound traffic within the security group - CloudFront operates over standard HTTP (port 80) and HTTPS (port 443) ports and doesn't support forwarding traffic to custom ports.&lt;/p&gt; &lt;p&gt;Constraints: - I need Ollama endpoint only accessible via a private IP for security reasons - The Ollama endpoint should only be called by the flask app - I cannot make modifications to client-side endpoints.&lt;/p&gt; &lt;p&gt;What I have tried so far: - tried nginx reverse proxies: didn't work - setup Ollama on a separate EC2 server but now it's accessible to the public which I don't want&lt;/p&gt; &lt;p&gt;Any help or advice would be appreciated as I have used chatgpt but it's starting to hallucinate wrong answers &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asji4"&gt; /u/asji4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iw63re/is_it_possible_to_deploy_ollama_on_aws_and_allow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iw63re/is_it_possible_to_deploy_ollama_on_aws_and_allow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iw63re/is_it_possible_to_deploy_ollama_on_aws_and_allow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T09:06:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivsl80</id>
    <title>Wired on 240v - Test time!</title>
    <updated>2025-02-22T20:44:22+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ivsl80/wired_on_240v_test_time/"&gt; &lt;img alt="Wired on 240v - Test time!" src="https://preview.redd.it/c8wvltbciqke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a50bd31faa4ee94d41203a9b4a07dcfbdf1e6fd0" title="Wired on 240v - Test time!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c8wvltbciqke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivsl80/wired_on_240v_test_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivsl80/wired_on_240v_test_time/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T20:44:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivxody</id>
    <title>What should I build with this?</title>
    <updated>2025-02-23T00:41:43+00:00</updated>
    <author>
      <name>/u/_astronerd</name>
      <uri>https://old.reddit.com/user/_astronerd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ivxody/what_should_i_build_with_this/"&gt; &lt;img alt="What should I build with this?" src="https://preview.redd.it/161j8f92cske1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0bbfffaf8eaceb0b0dec8e8a1473f5b046a9169" title="What should I build with this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I prefer to run everything locally and have built multiple AI agents, but I struggle with the next step—how to share or sell them effectively. While I enjoy developing and experimenting with different ideas, I often find it difficult to determine when a project is &amp;quot;good enough&amp;quot; to be put in front of users. I tend to keep refining and iterating, unsure of when to stop.&lt;/p&gt; &lt;p&gt;Another challenge I face is originality. Whenever I come up with what I believe is a novel idea, I often discover that someone else has already built something similar. This makes me question whether my work is truly innovative or valuable enough to stand out.&lt;/p&gt; &lt;p&gt;One of my strengths is having access to powerful tools and the ability to rigorously test and push AI models—something that many others may not have. However, despite these advantages, I feel stuck. I don't know how to move forward, how to bring my work to an audience, or how to turn my projects into something meaningful and shareable.&lt;/p&gt; &lt;p&gt;Any guidance on how to break through this stagnation would be greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_astronerd"&gt; /u/_astronerd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/161j8f92cske1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivxody/what_should_i_build_with_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivxody/what_should_i_build_with_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T00:41:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivpm8m</id>
    <title>I Make a Customized RAG Chatbot to Talk to CSV File Using Ollama DeepSeek and Streamlit Full Tutorial Part 2</title>
    <updated>2025-02-22T18:35:42+00:00</updated>
    <author>
      <name>/u/Spirited-Wind6803</name>
      <uri>https://old.reddit.com/user/Spirited-Wind6803</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ivpm8m/i_make_a_customized_rag_chatbot_to_talk_to_csv/"&gt; &lt;img alt="I Make a Customized RAG Chatbot to Talk to CSV File Using Ollama DeepSeek and Streamlit Full Tutorial Part 2" src="https://preview.redd.it/3zxqk8oqiqke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4bd9122479810ab06552684685b83cd3d6fe122" title="I Make a Customized RAG Chatbot to Talk to CSV File Using Ollama DeepSeek and Streamlit Full Tutorial Part 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spirited-Wind6803"&gt; /u/Spirited-Wind6803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3zxqk8oqiqke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivpm8m/i_make_a_customized_rag_chatbot_to_talk_to_csv/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivpm8m/i_make_a_customized_rag_chatbot_to_talk_to_csv/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T18:35:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwgsen</id>
    <title>Problem connecting remotely [Windows]</title>
    <updated>2025-02-23T18:27:49+00:00</updated>
    <author>
      <name>/u/sraasch</name>
      <uri>https://old.reddit.com/user/sraasch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I've checked all the obvious stuff already. And... It worked a month or so ago, when I last tried it!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I'm running Ollama native on windows (not WSL)&lt;/li&gt; &lt;li&gt;The server indicates that it is listening: &amp;quot;Listening on [::]:11434 (version 0.5.11)&amp;quot;&lt;/li&gt; &lt;li&gt;I have IPV6 disabled&lt;/li&gt; &lt;li&gt;I have windows firewall turned off&lt;/li&gt; &lt;li&gt;The OLLAMA_HOST variable is set to &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;I've rebooted several times and made sure I'm updated&lt;/li&gt; &lt;li&gt;I can access the localhost:11434 from the local machine&lt;/li&gt; &lt;li&gt;Running wireshark, I can see that the remote machine's attempt to open the connection *does* arrive at the windows machine. I see the original SYN and 3 retries.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I need to get smarter on TCP/IP to better understand the connection attempt as that *may* provide a clue, but I'm not optomistic.&lt;/p&gt; &lt;p&gt;If anyone has seen something like this, or has a thought on how to debug this, I'd be very grateful.&lt;/p&gt; &lt;p&gt;Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sraasch"&gt; /u/sraasch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwgsen/problem_connecting_remotely_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwgsen/problem_connecting_remotely_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwgsen/problem_connecting_remotely_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T18:27:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw51wo</id>
    <title>Self hosted LLM cpu non-gpu AVX512 importance ?</title>
    <updated>2025-02-23T07:51:38+00:00</updated>
    <author>
      <name>/u/centminmod</name>
      <uri>https://old.reddit.com/user/centminmod</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fairly new to self hosted LLM side. I use LM Studio on my 14&amp;quot; MacBook Pro M4 Pro with 48GB and 1TB drive and save LLM models to JEYI 2464 Pro fan edition USB4 NVMe external enclosure with 2TB Kingston KC3000.&lt;/p&gt; &lt;p&gt;However just started self hosted journey on my existing dedicated web servers developing my or-cli.py python client script that supports Openrouter.ai API + local Ollama &lt;a href="https://github.com/centminmod/or-cli"&gt;https://github.com/centminmod/or-cli&lt;/a&gt; and plan on adding vLLM support.&lt;/p&gt; &lt;p&gt;But the dedicated servers are fairly old and ram limited and lack AVX512 support. AMD Ryzen 5950X and Intel Xeon E-2276G with 64GB and 32GB memory respectively.&lt;/p&gt; &lt;p&gt;Short of GPU hosted servers, how much difference in performance would cpu only based usage for Ollama and vLLM and the like would there be if server supported AVX512 instructions for x86_64 based servers? Anyone got any past performance benchmark/results?&lt;/p&gt; &lt;p&gt;Even for GPU hosted, any noticeable difference pairing with/without cpu support for AVX512?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/centminmod"&gt; /u/centminmod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iw51wo/self_hosted_llm_cpu_nongpu_avx512_importance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iw51wo/self_hosted_llm_cpu_nongpu_avx512_importance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iw51wo/self_hosted_llm_cpu_nongpu_avx512_importance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T07:51:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1iweyjw</id>
    <title>Help: SyntaxError: Unexpected token '&lt;', "&lt;!DOCTYPE "... is not valid JSON</title>
    <updated>2025-02-23T17:10:35+00:00</updated>
    <author>
      <name>/u/flamingreaper1</name>
      <uri>https://old.reddit.com/user/flamingreaper1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, has anyone ever gotten this error when using Ollama through Openwebui?&lt;/p&gt; &lt;p&gt;I recently got a 7900xt, but after a few uses of it working fine, I run into this error and can only get past it by rebooting Unraid.&lt;/p&gt; &lt;p&gt;If this isn't the right community, please point me in the right direction! Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/flamingreaper1"&gt; /u/flamingreaper1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iweyjw/help_syntaxerror_unexpected_token_doctype_is_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iweyjw/help_syntaxerror_unexpected_token_doctype_is_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iweyjw/help_syntaxerror_unexpected_token_doctype_is_not/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T17:10:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwp77w</id>
    <title>Exporting an Ollama Model to Hugging Face Format</title>
    <updated>2025-02-24T00:36:31+00:00</updated>
    <author>
      <name>/u/naza01</name>
      <uri>https://old.reddit.com/user/naza01</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;The first disclaimer of this post is that I'm super new into this world so forgive me in advance if my question is silly.&lt;br /&gt; I looked a lot over the internet but haven't found anything useful so far.&lt;br /&gt; I was looking to fine-tune a model locally from my laptop.&lt;br /&gt; I'm using &lt;code&gt;qwen2.5-coder:1.5b&lt;/code&gt; model and I have already preprocessed the data I want to add to that model and have it in a JSONL format, which I read, is needed in order to successfully fine tune the LLM.&lt;br /&gt; Nevertheless, I'm having an error when trying to train the LLM with this data because apparently my model is not compatible with hugging face.&lt;br /&gt; I was hoping to have some built-in command from ollama to accomplish this, something like: &lt;code&gt;ollama fine-tune --model model_name --data data_to_finetune.jsonl&lt;/code&gt; but there's no native solution, therefore I read I can do this with hugging face, but then I'm having these incompatibilities.&lt;/p&gt; &lt;p&gt;Could someone care to explain what am I'm missing or what can I do differently to fine-tune my ollama model locally please?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/naza01"&gt; /u/naza01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwp77w/exporting_an_ollama_model_to_hugging_face_format/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwp77w/exporting_an_ollama_model_to_hugging_face_format/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwp77w/exporting_an_ollama_model_to_hugging_face_format/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T00:36:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwpbbt</id>
    <title>Quick &amp; Clean Web Data for Your Local LLMs? 👋 Introducing LexiCrawler (Binaries Inside!)</title>
    <updated>2025-02-24T00:42:10+00:00</updated>
    <author>
      <name>/u/Embarrassed-Way-1350</name>
      <uri>https://old.reddit.com/user/Embarrassed-Way-1350</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed-Way-1350"&gt; /u/Embarrassed-Way-1350 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1iwowsz/quick_clean_web_data_for_your_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwpbbt/quick_clean_web_data_for_your_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwpbbt/quick_clean_web_data_for_your_local_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T00:42:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwpjfp</id>
    <title>2nd GPU: VRAM overhead and available</title>
    <updated>2025-02-24T00:53:14+00:00</updated>
    <author>
      <name>/u/jujubre</name>
      <uri>https://old.reddit.com/user/jujubre</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all!&lt;br /&gt; Does someone could explain me why Ollama says that VRAM available is 11GB instead of 12GB?&lt;/p&gt; &lt;p&gt;Is there a way to have the 12GB available?&lt;/p&gt; &lt;p&gt;I have search quite a lot about this and I still do not understand why. Here are the facts: &lt;/p&gt; &lt;ul&gt; &lt;li&gt; I run ollama in win 11, both up to date.&lt;/li&gt; &lt;li&gt; Win 11 display: integrated GPU (AMD 7700X).&lt;/li&gt; &lt;li&gt; RTX 3060 12GB VRAM, as 2nd graphic card, no display attached.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ollama starting log: &lt;code&gt; time=2025-02-23T19:42:19.412-05:00 level=INFO source=images.go:432 msg=&amp;quot;total blobs: 64&amp;quot; time=2025-02-23T19:42:19.414-05:00 level=INFO source=images.go:439 msg=&amp;quot;total unused blobs removed: 0&amp;quot; time=2025-02-23T19:42:19.416-05:00 level=INFO source=routes.go:1237 msg=&amp;quot;Listening on [::]:11434 (version 0.5.11)&amp;quot; time=2025-02-23T19:42:19.416-05:00 level=INFO source=gpu.go:217 msg=&amp;quot;looking for compatible GPUs&amp;quot; time=2025-02-23T19:42:19.416-05:00 level=INFO source=gpu_windows.go:167 msg=packages count=1 time=2025-02-23T19:42:19.416-05:00 level=INFO source=gpu_windows.go:214 msg=&amp;quot;&amp;quot; package=0 cores=8 efficiency=0 threads=16 time=2025-02-23T19:42:19.539-05:00 level=INFO source=gpu.go:319 msg=&amp;quot;detected OS VRAM overhead&amp;quot; id=GPU-25c2f227-db2e-9f0b-b32a-ecff37fac3d0 library=cuda compute=8.6 driver=12.8 name=&amp;quot;NVIDIA GeForce RTX 3060&amp;quot; overhead=&amp;quot;867.3 MiB&amp;quot; time=2025-02-23T19:42:19.952-05:00 level=INFO source=amd_windows.go:127 msg=&amp;quot;unsupported Radeon iGPU detected skipping&amp;quot; id=0 total=&amp;quot;24.0 GiB&amp;quot; time=2025-02-23T19:42:19.954-05:00 level=INFO source=types.go:130 msg=&amp;quot;inference compute&amp;quot; id=GPU-25c2f227-db2e-9f0b-b32a-ecff37fac3d0 library=cuda variant=v12 compute=8.6 driver=12.8 name=&amp;quot;NVIDIA GeForce RTX 3060&amp;quot; total=&amp;quot;12.0 GiB&amp;quot; available=&amp;quot;11.0 GiB&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jujubre"&gt; /u/jujubre &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwpjfp/2nd_gpu_vram_overhead_and_available/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwpjfp/2nd_gpu_vram_overhead_and_available/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwpjfp/2nd_gpu_vram_overhead_and_available/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T00:53:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwc049</id>
    <title>Moe for LLMs</title>
    <updated>2025-02-23T15:01:10+00:00</updated>
    <author>
      <name>/u/wahnsinnwanscene</name>
      <uri>https://old.reddit.com/user/wahnsinnwanscene</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What does it mean to have a mixture of experts in llama.cpp? Does it mean parts of weights are loaded when the mixture router decides on the expert, or is the entire model loaded and is partitioned programmatically ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wahnsinnwanscene"&gt; /u/wahnsinnwanscene &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwc049/moe_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwc049/moe_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwc049/moe_for_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T15:01:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwmxf6</id>
    <title>External Ollama API Support has been added in Notate. RAG web &amp; vector store search, data ingestion pipeline and more!</title>
    <updated>2025-02-23T22:49:37+00:00</updated>
    <author>
      <name>/u/Hairetsu</name>
      <uri>https://old.reddit.com/user/Hairetsu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iwmxf6/external_ollama_api_support_has_been_added_in/"&gt; &lt;img alt="External Ollama API Support has been added in Notate. RAG web &amp;amp; vector store search, data ingestion pipeline and more!" src="https://external-preview.redd.it/uJ03FCBmwin1Bs6kmUTNhxfLIQ4gRfplVkP8UGh6jTs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb4e5c1bc213426afae9759155975f1cbdd10bc5" title="External Ollama API Support has been added in Notate. RAG web &amp;amp; vector store search, data ingestion pipeline and more!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hairetsu"&gt; /u/Hairetsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/CNTRLAI/Notate"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwmxf6/external_ollama_api_support_has_been_added_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwmxf6/external_ollama_api_support_has_been_added_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T22:49:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwyigw</id>
    <title>I need help to boost the results</title>
    <updated>2025-02-24T09:54:08+00:00</updated>
    <author>
      <name>/u/Eliahhigh787</name>
      <uri>https://old.reddit.com/user/Eliahhigh787</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using ollama with different models such as llama3, phi and mistra but the results take so long to show up. I use this model on a laptop.. should i upload it some where for better performance?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eliahhigh787"&gt; /u/Eliahhigh787 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwyigw/i_need_help_to_boost_the_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwyigw/i_need_help_to_boost_the_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwyigw/i_need_help_to_boost_the_results/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T09:54:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwh3iu</id>
    <title>Back at it again..</title>
    <updated>2025-02-23T18:40:56+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iwh3iu/back_at_it_again/"&gt; &lt;img alt="Back at it again.." src="https://preview.redd.it/5er15txqkxke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ba76a55522110f381f14f58cc97ad34e09bd0c9" title="Back at it again.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5er15txqkxke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwh3iu/back_at_it_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwh3iu/back_at_it_again/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T18:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwu3ap</id>
    <title>Just Released v1 of My AI-Powered VS Code Extension – Looking for Feedback!</title>
    <updated>2025-02-24T04:53:59+00:00</updated>
    <author>
      <name>/u/Matrix_030</name>
      <uri>https://old.reddit.com/user/Matrix_030</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Matrix_030"&gt; /u/Matrix_030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/opensource/comments/1iuz8jk/just_released_v1_of_my_aipowered_vs_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwu3ap/just_released_v1_of_my_aipowered_vs_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwu3ap/just_released_v1_of_my_aipowered_vs_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T04:53:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwpy32</id>
    <title>Look Closely - 8x Mi50 (left) + 8x Mi60 (right) - Llama-3.3-70B - Do the Mi50s use less power ?!?!</title>
    <updated>2025-02-24T01:13:24+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6wvh9swlsxke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwpy32/look_closely_8x_mi50_left_8x_mi60_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwpy32/look_closely_8x_mi50_left_8x_mi60_right/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T01:13:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwvj07</id>
    <title>Llama with no gpu and 120 gb RAM</title>
    <updated>2025-02-24T06:23:17+00:00</updated>
    <author>
      <name>/u/rock_db_saanu</name>
      <uri>https://old.reddit.com/user/rock_db_saanu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can Llama work efficiently with 120 GB RAM and no GPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rock_db_saanu"&gt; /u/rock_db_saanu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwvj07/llama_with_no_gpu_and_120_gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwvj07/llama_with_no_gpu_and_120_gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwvj07/llama_with_no_gpu_and_120_gb_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T06:23:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwn9hk</id>
    <title>I created an open-source planning assistant that works with Ollama models that supports structured output</title>
    <updated>2025-02-23T23:04:23+00:00</updated>
    <author>
      <name>/u/neoneye2</name>
      <uri>https://old.reddit.com/user/neoneye2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iwn9hk/i_created_an_opensource_planning_assistant_that/"&gt; &lt;img alt="I created an open-source planning assistant that works with Ollama models that supports structured output" src="https://external-preview.redd.it/oy_g42SiRHj8efbYTgaxUdlRuNoUk8FyHzjh0Dar9K4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f87689dfecd43f7f8b9c7d55d48522c70d744e9e" title="I created an open-source planning assistant that works with Ollama models that supports structured output" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neoneye2"&gt; /u/neoneye2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/neoneye/PlanExe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwn9hk/i_created_an_opensource_planning_assistant_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwn9hk/i_created_an_opensource_planning_assistant_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T23:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwtdr7</id>
    <title>How good is a 7-14b model finetuned for a super specific use case (i.e. a spdcific sql dialect, or transforming data with pandas or pyspark)?</title>
    <updated>2025-02-24T04:12:39+00:00</updated>
    <author>
      <name>/u/juan_berger</name>
      <uri>https://old.reddit.com/user/juan_berger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like would it make sense to have a bunch of smaller models running locally, fined tuned to the specific task you are currently working on, and switching between them?&lt;/p&gt; &lt;p&gt;Would this even be that useful (or too much hastle switching between models and only working for that specific use case...)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juan_berger"&gt; /u/juan_berger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwtdr7/how_good_is_a_714b_model_finetuned_for_a_super/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwtdr7/how_good_is_a_714b_model_finetuned_for_a_super/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwtdr7/how_good_is_a_714b_model_finetuned_for_a_super/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T04:12:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwm09j</id>
    <title>I created an open-source tool for using ANY Ollama model for real-time financial analysis</title>
    <updated>2025-02-23T22:08:44+00:00</updated>
    <author>
      <name>/u/No-Definition-2886</name>
      <uri>https://old.reddit.com/user/No-Definition-2886</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iwm09j/i_created_an_opensource_tool_for_using_any_ollama/"&gt; &lt;img alt="I created an open-source tool for using ANY Ollama model for real-time financial analysis" src="https://external-preview.redd.it/Ila7NpPu5TjU1zt5yIdrVeYFtcNrMgsyMgU6l0x4FVc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76ff6c15337ec12e3f3fb9b6b0962ffd1d2a31b6" title="I created an open-source tool for using ANY Ollama model for real-time financial analysis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Definition-2886"&gt; /u/No-Definition-2886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/austin-starks/FinAnGPT-Pro"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwm09j/i_created_an_opensource_tool_for_using_any_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwm09j/i_created_an_opensource_tool_for_using_any_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T22:08:44+00:00</published>
  </entry>
</feed>
