<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-28T08:24:59+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1iz389x</id>
    <title>Fine-tune a DeepSeek distilled variant with a reasoning dataset</title>
    <updated>2025-02-27T00:46:13+00:00</updated>
    <author>
      <name>/u/heido333</name>
      <uri>https://old.reddit.com/user/heido333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to fine-tune a distilled variant with a reasoning dataset. My question is whether I should generate two responses (one for the reasoning and one for the actual answer separately) or combine both the reasoning and the final answer into a single response. Do you have any other suggestions?&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;deep_seek_prompt = &amp;quot;&amp;quot;&amp;quot; &amp;lt;｜User｜&amp;gt;{}&amp;lt;｜end▁of▁sentence｜&amp;gt; &amp;lt;｜Assistant｜&amp;gt; &amp;lt;think&amp;gt; {} &amp;lt;/think&amp;gt;&amp;lt;｜end▁of▁sentence｜&amp;gt; &amp;lt;｜Assistant｜&amp;gt;{}&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;or&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;deep_seek_prompt = &amp;quot;&amp;quot;&amp;quot; &amp;lt;｜User｜&amp;gt;{}&amp;lt;｜end▁of▁sentence｜&amp;gt; &amp;lt;｜Assistant｜&amp;gt; &amp;lt;think&amp;gt; {} &amp;lt;/think&amp;gt; {}&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/heido333"&gt; /u/heido333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iz389x/finetune_a_deepseek_distilled_variant_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iz389x/finetune_a_deepseek_distilled_variant_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iz389x/finetune_a_deepseek_distilled_variant_with_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T00:46:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz4395</id>
    <title>OpenThinker-32B-abliterated.Q8_0 + 8x AMD Instinct Mi60 Server + vLLM + Tensor Parallelism</title>
    <updated>2025-02-27T01:28:14+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/f8sna8gu3lle1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iz4395/openthinker32babliteratedq8_0_8x_amd_instinct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iz4395/openthinker32babliteratedq8_0_8x_amd_instinct/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T01:28:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyt6jc</id>
    <title>Ran a 9GB model on a laptop with 8GB of RAM, and wondered why it was so slow</title>
    <updated>2025-02-26T17:36:31+00:00</updated>
    <author>
      <name>/u/akb74</name>
      <uri>https://old.reddit.com/user/akb74</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't if anyone needs to hear this, but happy for you to learn from my stupid mistakes!&lt;/p&gt; &lt;p&gt;This was deepseek-r1:14b and I fixed the slowness by switching to deepseek-r1:7b&lt;/p&gt; &lt;p&gt;Am I right in thinking that given a model which &lt;em&gt;comfortably&lt;/em&gt; fits in RAM, then the GPU becomes the main determinant of performance? And this should be true for all well implemented LLMs, not just DeepSeek?&lt;/p&gt; &lt;p&gt;Incidently, the 14b model (runnning well on a much better specced work laptop) wasn't able to diagnose the cause of this problem. I mean, it made plenty of sensible suggestions, but at no time said &amp;quot;try a smaller model&amp;quot; or anything like it.&lt;/p&gt; &lt;p&gt;Anyway, I'm just beginning and loving my ollama journey so far! Hope I've brought a smile to someone's face with my folly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akb74"&gt; /u/akb74 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyt6jc/ran_a_9gb_model_on_a_laptop_with_8gb_of_ram_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyt6jc/ran_a_9gb_model_on_a_laptop_with_8gb_of_ram_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iyt6jc/ran_a_9gb_model_on_a_laptop_with_8gb_of_ram_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-26T17:36:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz5rhi</id>
    <title>Shift Update, more customization options, more AI models based on your suggestions! Local models next?</title>
    <updated>2025-02-27T02:53:37+00:00</updated>
    <author>
      <name>/u/Ehsan1238</name>
      <uri>https://old.reddit.com/user/Ehsan1238</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iz5rhi/shift_update_more_customization_options_more_ai/"&gt; &lt;img alt="Shift Update, more customization options, more AI models based on your suggestions! Local models next?" src="https://external-preview.redd.it/VTY9iVHTT24wk-pF6YMsTOrn60JQpmsHRBwhxPqYFUE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16cf91aa41fb63ab1228267d34e438174a25abd1" title="Shift Update, more customization options, more AI models based on your suggestions! Local models next?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;Thanks for the incredible response to Shift lately. We deeply appreciate all your thoughtful feature suggestions, bug notifications, and positive comments about your experience with the app. It truly means everything to our team :)&lt;/p&gt; &lt;h1&gt;What is Shift?&lt;/h1&gt; &lt;p&gt;Shift is basically a text helper that lives on your laptop. It's pretty simple - you highlight some text, double-tap your shift key, and it helps you rewrite or fix whatever you're working on. I've been using it for emails and reports, and it saves me from constantly googling &amp;quot;how to word this professionally&amp;quot; or &amp;quot;make this sound better.&amp;quot; Nothing fancy - just select text, tap shift twice, tell it what you want, and it does it right there in whatever app you're using. It works with different AI engines behind the scenes, but you don't really notice that part. It's convenient since you don't have to copy-paste stuff into ChatGPT or wherever. &lt;/p&gt; &lt;p&gt;I use it a lot for rewriting or answering to people as well as coding and many other things. This also works on excel for creating tables or editing them as well as google sheets or any other similar platforms. I will be pushing more features, there's a built in updating mechanism inside the app where you can download the latest update, I'll be releasing a feature where you can download local LLM models like deepseek or llama through the app itself increasing privacy and security so everything is done locally on your laptop, there is now also a feature where you can add you own API keys if you want to for the models. You can watch the full demo here (it's an old demo and some features have been added) : &lt;a href="https://youtu.be/AtgPYKtpMmU?si=V6UShc062xr1s9iO"&gt;https://youtu.be/AtgPYKtpMmU?si=V6UShc062xr1s9iO&lt;/a&gt; , for more info you are welcome to visit the website here: &lt;a href="https://shiftappai.com/"&gt;https://shiftappai.com/&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What's New?&lt;/h1&gt; &lt;p&gt;After a lot of user suggestions, we added more customizations for the shortcuts &lt;strong&gt;you can now choose two keys and three keys combinations with beautiful UI where you can link a prompt with a model you want and then link it to this keyboard shortcut key:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m2z4hj45blle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f20655c5f82a76a7753a69d33dd7098546110ab1"&gt;https://preview.redd.it/m2z4hj45blle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f20655c5f82a76a7753a69d33dd7098546110ab1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wu1zdr36blle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad1bc76dc20b9534e4bfa3a21b295e37f25ba10f"&gt;https://preview.redd.it/wu1zdr36blle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad1bc76dc20b9534e4bfa3a21b295e37f25ba10f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Secondly, we have added the new claude. 3.7 sonnet but that's not all you can turn on the thinking mode for it and specifically define the amount of thinking it can do for a specific task:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nky1gu9jblle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc3d477179d80781cedb50009b405e2224102c6d"&gt;https://preview.redd.it/nky1gu9jblle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc3d477179d80781cedb50009b405e2224102c6d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Thirdly, you can now use your own API keys for the models and skip our servers completely, the app validates your API key automatically upon pasting and encrypts it locally in your device keychain for security:, simple paste and turn on the toggle and the requests will now be switched to your own API keys:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uhh9cz6rblle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af273496eac9670721783043a96d3e2272acfce6"&gt;https://preview.redd.it/uhh9cz6rblle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af273496eac9670721783043a96d3e2272acfce6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;After gathering extensive user feedback about the double shift functionality on both sides of the keyboard, we learned that many users were accidentally triggering these commands, causing inconvenience. We've addressed this issue by adding customization options in the settings menu. You can now personalize both the Widget Activation Key (right double shift by default) and the Context Capture Key (left double shift by default) to better suit your specific workflow preferences.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nyk4z2d1clle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac994f4d5a315d210c3b2a2ff8c9547e358f3584"&gt;https://preview.redd.it/nyk4z2d1clle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac994f4d5a315d210c3b2a2ff8c9547e358f3584&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3cbf9q8iclle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01e9d5157e09c24b9cdb908a08c2d43820984883"&gt;https://preview.redd.it/3cbf9q8iclle1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01e9d5157e09c24b9cdb908a08c2d43820984883&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. To dismiss the Shift Widget originally you had to do it with ESC only, now you can go to quick dismiss shortcut and turn it on, this way you can appear/disappear the widget with the same shortcut (which is by default right double shift)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/frgxx52zclle1.png?width=3080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f906049779af38f6480e12a5a3e2c48512bd288a"&gt;https://preview.redd.it/frgxx52zclle1.png?width=3080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f906049779af38f6480e12a5a3e2c48512bd288a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ofkotfp9dlle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=784877e9ee928ec0dca0aa4ac463f4c2232eed5c"&gt;https://preview.redd.it/ofkotfp9dlle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=784877e9ee928ec0dca0aa4ac463f4c2232eed5c&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;A lot of users have very specialized long prompts with documents, so we decided to create a hub for all the prompts where you can manage and save them introducing library, library prompts can be used in shortcut section so now you don't have to copy paste your prompts and move them around a lot. You can also add up to 8 documents for each prompt&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/91m4l2zldlle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7d6d2b72dd5af9ad903d068b336a19c971f2c0da"&gt;https://preview.redd.it/91m4l2zldlle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7d6d2b72dd5af9ad903d068b336a19c971f2c0da&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q27tr6hrdlle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7c1e08742d97b19144a232fb829a7ea79df8cd34"&gt;https://preview.redd.it/q27tr6hrdlle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7c1e08742d97b19144a232fb829a7ea79df8cd34&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7y2ouckzdlle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1606db06f99ff786086e2f656351ccffc082d12e"&gt;https://preview.redd.it/7y2ouckzdlle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1606db06f99ff786086e2f656351ccffc082d12e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fosqsq51elle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=58cdf122f56607b5377d0bcfe7dabfdd6f4cf049"&gt;https://preview.redd.it/fosqsq51elle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=58cdf122f56607b5377d0bcfe7dabfdd6f4cf049&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And let's not forget our smooth and beautiful UI designs:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g8v7d0jwelle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3102703897c80b0a8fc92cbb1f52018babbdffff"&gt;https://preview.redd.it/g8v7d0jwelle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3102703897c80b0a8fc92cbb1f52018babbdffff&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ll4uz1wzelle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df096b918302d10894259e0cf9cbbf1a480a094f"&gt;https://preview.redd.it/ll4uz1wzelle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df096b918302d10894259e0cf9cbbf1a480a094f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q0tsjgk2flle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fde65c08f8259d1503b2ff37a149e3bdbe33e360"&gt;https://preview.redd.it/q0tsjgk2flle1.png?width=2090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fde65c08f8259d1503b2ff37a149e3bdbe33e360&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;If you like to see Shift in action, watch out our most recent demo of shortcuts in Shift &lt;a href="https://youtu.be/GNHZ-mNgpCE?si=9rXi9sBEekamQlo8"&gt;here&lt;/a&gt;.&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;This shows we're truly listening and quick to respond implementing your suggestions within 24 hours in our updates. We genuinely value your input and are committed to perfecting Shift. Thanks to your support, we've welcomed 100 users in just our first week! We're incredibly grateful for your encouragement and kind feedback. We are your employees.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We're still evolving with major updates on the horizon. To learn about our upcoming significant features, please visit: &lt;a href="https://shiftappai.com/#whats-nexttps://shiftappai.com/#whats-next"&gt;https://shiftappai.com/#whats-nexttps://shiftappai.com/#whats-next&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you'd like to suggest features or improvements for our upcoming updates, just drop us a line at [&lt;a href="mailto:contact@shiftappai.com"&gt;contact@shiftappai.com&lt;/a&gt;](mailto:&lt;a href="mailto:contact@shiftappai.com"&gt;contact@shiftappai.com&lt;/a&gt;) or message us here. We'll make sure to implement your ideas quickly to match what you're looking for.&lt;/p&gt; &lt;p&gt;We have grown in over 100 users in less than a week! Thank you all for all this support :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ehsan1238"&gt; /u/Ehsan1238 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iz5rhi/shift_update_more_customization_options_more_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iz5rhi/shift_update_more_customization_options_more_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iz5rhi/shift_update_more_customization_options_more_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T02:53:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz6rhk</id>
    <title>DataBridge Now Supports ColPali for Unprecedented Multi-Modal RAG! 🎉</title>
    <updated>2025-02-27T03:46:14+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/Rag/comments/1iz6qp0/databridge_now_supports_colpali_for_unprecedented/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iz6rhk/databridge_now_supports_colpali_for_unprecedented/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iz6rhk/databridge_now_supports_colpali_for_unprecedented/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T03:46:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz4squ</id>
    <title>Tested 6 AI Models with 6 different types of questions, timed and rated the answers, is there a clear winner?</title>
    <updated>2025-02-27T02:04:05+00:00</updated>
    <author>
      <name>/u/tonyscha</name>
      <uri>https://old.reddit.com/user/tonyscha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure if anyone finds this useful but did some very elementary testing on 6 different models with 6 different questions then ranked them on my personal opinion of what I thought the best responses were and provided how long it took to provide responses in minutes/seconds.&lt;/p&gt; &lt;p&gt;Use the following models for my testing&lt;/p&gt; &lt;ul&gt; &lt;li&gt;olmo2:13b&lt;/li&gt; &lt;li&gt;olmo2:7b&lt;/li&gt; &lt;li&gt;deepseek-r1:7b&lt;/li&gt; &lt;li&gt;gemma:7b﻿&lt;/li&gt; &lt;li&gt;llama3.2:latest 3.2B﻿&lt;/li&gt; &lt;li&gt;llama3.1:8b&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Asked the following questions&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tell me about Zion National Park&lt;/li&gt; &lt;li&gt;Create me a half marathon training plan&lt;/li&gt; &lt;li&gt;Tell me the ethical concerns related to AI&lt;/li&gt; &lt;li&gt;Outline the steps to prove that the sum of the interior angles of a triangle is 180 degrees.&lt;/li&gt; &lt;li&gt;If you could change anything about your algorithm, what would it be?&lt;/li&gt; &lt;li&gt;Rewrite this for better readability - the lazy yellow dog was caught by the slow red fox as he lay sleeping in the sun&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here was my conclusion:&lt;br /&gt; Overall both olmo models and the llama3.1 models won per my criteria more often then the other ones. While gemma:7b was the fastest on creating the half marathon training plan and prove the triangle interior angles is 180 degrees, I didn't like the responses as much as others. Deepseek seemed to always be middle of the pack. I am unsure if there is a clear winner, but I do feel like there is some clear losers, and I would included deepseek-r1:7b or gemma:7b in that category.&lt;/p&gt; &lt;p&gt;Note: I am running on a CPU only system, so the responses are going to be much slower then a GPU system.&lt;/p&gt; &lt;p&gt;Full write up here with times:&lt;/p&gt; &lt;p&gt;&lt;a href="https://akschaefer.com/2025/02/26/6-ai-models-6-tough-questions-1-clear-winner/"&gt;https://akschaefer.com/2025/02/26/6-ai-models-6-tough-questions-1-clear-winner/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tonyscha"&gt; /u/tonyscha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iz4squ/tested_6_ai_models_with_6_different_types_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iz4squ/tested_6_ai_models_with_6_different_types_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iz4squ/tested_6_ai_models_with_6_different_types_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T02:04:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1izhl52</id>
    <title>Basic LLM performance testing of GPU-powered Kubernetes nodes from Rackspace Spot</title>
    <updated>2025-02-27T14:54:54+00:00</updated>
    <author>
      <name>/u/olegsmith7</name>
      <uri>https://old.reddit.com/user/olegsmith7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://oleg.smetan.in/posts/2025-02-16-rackspace-spot-llm-performance-test"&gt;https://oleg.smetan.in/posts/2025-02-16-rackspace-spot-llm-performance-test&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We'll create a managed Kubernetes cluster on Rackspace Spot, install an NVIDIA Helm chart to unlock GPU capabilities for applications, install an Ollama Helm chart to run self-hosted LLM on Kubernetes, configure OpenWebUI to use a remote self-hosted LLM, and test LLM performance on an A30 GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/olegsmith7"&gt; /u/olegsmith7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izhl52/basic_llm_performance_testing_of_gpupowered/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izhl52/basic_llm_performance_testing_of_gpupowered/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izhl52/basic_llm_performance_testing_of_gpupowered/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T14:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1izqfxm</id>
    <title>An AI agent, using Ollama and mistral, in 16 lines of code</title>
    <updated>2025-02-27T21:04:07+00:00</updated>
    <author>
      <name>/u/Excellent-Suit2150</name>
      <uri>https://old.reddit.com/user/Excellent-Suit2150</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1izqfxm/an_ai_agent_using_ollama_and_mistral_in_16_lines/"&gt; &lt;img alt="An AI agent, using Ollama and mistral, in 16 lines of code" src="https://preview.redd.it/hp46r94jxqle1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c648dc6adec2ba2f9480cafdd512db628a35caad" title="An AI agent, using Ollama and mistral, in 16 lines of code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent-Suit2150"&gt; /u/Excellent-Suit2150 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hp46r94jxqle1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izqfxm/an_ai_agent_using_ollama_and_mistral_in_16_lines/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izqfxm/an_ai_agent_using_ollama_and_mistral_in_16_lines/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T21:04:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1izeohu</id>
    <title>Use deep seek and ollama for real time generating dialogs in unity</title>
    <updated>2025-02-27T12:33:05+00:00</updated>
    <author>
      <name>/u/Intrepid_Way9713</name>
      <uri>https://old.reddit.com/user/Intrepid_Way9713</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1izeohu/use_deep_seek_and_ollama_for_real_time_generating/"&gt; &lt;img alt="Use deep seek and ollama for real time generating dialogs in unity" src="https://external-preview.redd.it/8Vb5Yt3tWc0UtC7nSBTK1zgY8tSHJEGMiskrU2ZPlCc.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e2d05d911ff23ba13e6757459f256900fb560ae" title="Use deep seek and ollama for real time generating dialogs in unity" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Intrepid_Way9713"&gt; /u/Intrepid_Way9713 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/D9M4WajkRCM?si=wrBX-KjHWxn7RzKn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izeohu/use_deep_seek_and_ollama_for_real_time_generating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izeohu/use_deep_seek_and_ollama_for_real_time_generating/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T12:33:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1izl9re</id>
    <title>Force ollama to run on CPU mode?</title>
    <updated>2025-02-27T17:28:24+00:00</updated>
    <author>
      <name>/u/Imaginary_Virus19</name>
      <uri>https://old.reddit.com/user/Imaginary_Virus19</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to run deepseek-v3:671b on a system with 512GB RAM and 2x40GB GPUs. For some reason, it refuses to launch &amp;quot;unable to allocate CUDA0 buffer&amp;quot;. If I uninstall the GPU drivers, ollama runs on CPU only and is fast enough for my needs. But I need the GPUs for other models.&lt;/p&gt; &lt;p&gt;Is there a way of telling ollama to ignore the GPUs when I run this model? (so I don't have to uninstall and reinstall the GPU drivers every time I switch models). &lt;/p&gt; &lt;p&gt;Edit: Ollama is installed on bare metal Ubuntu.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Imaginary_Virus19"&gt; /u/Imaginary_Virus19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izl9re/force_ollama_to_run_on_cpu_mode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izl9re/force_ollama_to_run_on_cpu_mode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izl9re/force_ollama_to_run_on_cpu_mode/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T17:28:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1izfjjf</id>
    <title>DeepSeek Day 4 - Open Sourcing Repositories</title>
    <updated>2025-02-27T13:17:51+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1izfjjf/deepseek_day_4_open_sourcing_repositories/"&gt; &lt;img alt="DeepSeek Day 4 - Open Sourcing Repositories" src="https://external-preview.redd.it/FRC-3EiSZlkskMTc5xbb7kLDlJeJc878iZBR_MHAT98.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f6365a75136a2ecb08b9278d12ff71b5764b426" title="DeepSeek Day 4 - Open Sourcing Repositories" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/deepseek-ai/open-infra-index"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izfjjf/deepseek_day_4_open_sourcing_repositories/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izfjjf/deepseek_day_4_open_sourcing_repositories/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T13:17:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1izps48</id>
    <title>How to LLM with git repo in mind. RAG, FineTune, etc..?</title>
    <updated>2025-02-27T20:35:42+00:00</updated>
    <author>
      <name>/u/einthecorgi2</name>
      <uri>https://old.reddit.com/user/einthecorgi2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I often code in rust, I would like an LLM to be aware of the frame work and version that I am coding with for a certain pacakge(s). For example, I use egui, its under development and changes a lot and the LLM generally uses syntax from assorted versions and even gpt-3o rarely produces results that compile without some work. &lt;/p&gt; &lt;p&gt;Does anyone have any guidance on how I can setup an LLM (I currently use Ollama with openweb-ui, or continue pointed at Ollama) so that it will best reference some specific repos when coding? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/einthecorgi2"&gt; /u/einthecorgi2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izps48/how_to_llm_with_git_repo_in_mind_rag_finetune_etc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izps48/how_to_llm_with_git_repo_in_mind_rag_finetune_etc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izps48/how_to_llm_with_git_repo_in_mind_rag_finetune_etc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T20:35:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1izo7wf</id>
    <title>Running ollama with 4 Nvidia 1080 how?</title>
    <updated>2025-02-27T19:29:30+00:00</updated>
    <author>
      <name>/u/Money_Hand_4199</name>
      <uri>https://old.reddit.com/user/Money_Hand_4199</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dear ollama community!&lt;/p&gt; &lt;p&gt;I am running ollama with 4 Nvidia 1080 cards with 8GB VRAM each. When loading and using LLM, I got only one of the GPU utilized.&lt;/p&gt; &lt;p&gt;Please advise how to setup ollama to have combined vram of all the GPUs available for running bigger llm. How I can setup this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Money_Hand_4199"&gt; /u/Money_Hand_4199 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izo7wf/running_ollama_with_4_nvidia_1080_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izo7wf/running_ollama_with_4_nvidia_1080_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izo7wf/running_ollama_with_4_nvidia_1080_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T19:29:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1izmd0l</id>
    <title>Generate a wiki for your research topic, sourcing from the web and your docs (MIT License)</title>
    <updated>2025-02-27T18:13:15+00:00</updated>
    <author>
      <name>/u/gkamer8</name>
      <uri>https://old.reddit.com/user/gkamer8</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1izmd0l/generate_a_wiki_for_your_research_topic_sourcing/"&gt; &lt;img alt="Generate a wiki for your research topic, sourcing from the web and your docs (MIT License)" src="https://external-preview.redd.it/8t_8ma6bYc6oGtpAhaHj5jBbi1LLrwxpqhbhCStrt4M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=508f8dd999d0ba0a52a79d22e4d9bb42f2259f47" title="Generate a wiki for your research topic, sourcing from the web and your docs (MIT License)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gkamer8"&gt; /u/gkamer8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/goodreasonai/nichey"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izmd0l/generate_a_wiki_for_your_research_topic_sourcing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izmd0l/generate_a_wiki_for_your_research_topic_sourcing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T18:13:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1izk3pe</id>
    <title>I built a MacOS app that lets you summon an Ollama model anywhere on your Mac to generate/discuss content with/without your voice. Let me know what you think!</title>
    <updated>2025-02-27T16:40:39+00:00</updated>
    <author>
      <name>/u/bustyLaserCannon</name>
      <uri>https://old.reddit.com/user/bustyLaserCannon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1izk3pe/i_built_a_macos_app_that_lets_you_summon_an/"&gt; &lt;img alt="I built a MacOS app that lets you summon an Ollama model anywhere on your Mac to generate/discuss content with/without your voice. Let me know what you think!" src="https://b.thumbs.redditmedia.com/YDAk3Y0NrYuhRVoHzAjEguVx0OSiby4WyOo0AYKmzeU.jpg" title="I built a MacOS app that lets you summon an Ollama model anywhere on your Mac to generate/discuss content with/without your voice. Let me know what you think!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got pretty fed up with copy and pasting to different LLMs so I decided to learn SwiftUI and built my first MacOS app called Promptly.&lt;/p&gt; &lt;p&gt;It's a Mac menu bar app that lets you use LLMs in any app with a simple shortcut (including your voice!).&lt;/p&gt; &lt;p&gt;You bring your own API keys for models like ChatGPT, Claude, and Gemini or more relevant for this sub, Ollama models you want to use!&lt;/p&gt; &lt;p&gt;You can configure the shortcuts and settings too in the menu app.&lt;/p&gt; &lt;p&gt;I'm using it daily to summarise web pages, rewrite slack messages and emails to be more professional, enhance my notes and write tweets.&lt;/p&gt; &lt;p&gt;I hate subscriptions so there's a 7 day free trial, and then it's a one-time purchase.&lt;/p&gt; &lt;p&gt;Also giving away a discount code for launch that expires on 1st March - just use code &lt;code&gt;QZNDI5MG&lt;/code&gt; on checkout for 20% off!&lt;/p&gt; &lt;p&gt;Check it out! Would love any feedback!&lt;/p&gt; &lt;p&gt;Download free trial here: &lt;a href="https://getpromptly.app?ref=ollama"&gt;Promptly&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/xa8ohyx9mple1.gif"&gt;https://i.redd.it/xa8ohyx9mple1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/zh533xx9mple1.gif"&gt;https://i.redd.it/zh533xx9mple1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/nnprfxx9mple1.gif"&gt;https://i.redd.it/nnprfxx9mple1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/202i3ty9mple1.gif"&gt;https://i.redd.it/202i3ty9mple1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bustyLaserCannon"&gt; /u/bustyLaserCannon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izk3pe/i_built_a_macos_app_that_lets_you_summon_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izk3pe/i_built_a_macos_app_that_lets_you_summon_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izk3pe/i_built_a_macos_app_that_lets_you_summon_an/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T16:40:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyxpd7</id>
    <title>DeepSeek RAG Chatbot Reaches 650+ Stars 🎉 - Celebrating Offline RAG Innovation</title>
    <updated>2025-02-26T20:41:55+00:00</updated>
    <author>
      <name>/u/akhilpanja</name>
      <uri>https://old.reddit.com/user/akhilpanja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m incredibly excited to share that &lt;strong&gt;DeepSeek RAG Chatbot&lt;/strong&gt; has officially hit &lt;strong&gt;650+ stars&lt;/strong&gt; on GitHub! This is a huge achievement, and I want to take a moment to celebrate this milestone and thank everyone who has contributed to the project in one way or another. Whether you’ve provided feedback, used the tool, or just starred the repo, your support has made all the difference. (git: &lt;a href="https://github.com/SaiAkhil066/DeepSeek-RAG-Chatbot.git"&gt;https://github.com/SaiAkhil066/DeepSeek-RAG-Chatbot.git&lt;/a&gt; )&lt;/p&gt; &lt;h1&gt;What is DeepSeek RAG Chatbot?&lt;/h1&gt; &lt;p&gt;DeepSeek RAG Chatbot is a local, privacy-first solution for anyone who needs to quickly retrieve information from documents like PDFs, Word files, and text files. What sets it apart is that it runs &lt;strong&gt;100% offline&lt;/strong&gt;, ensuring that all your data remains private and never leaves your machine. It’s a tool built with privacy in mind, allowing you to search and retrieve answers from your own documents, without ever needing an internet connection.&lt;/p&gt; &lt;h1&gt;Key Features and Technical Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Offline &amp;amp; Private&lt;/strong&gt;: The chatbot works completely offline, ensuring your data stays private on your local machine.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Format Support&lt;/strong&gt;: DeepSeek can handle PDFs, Word documents, and text files, making it versatile for different types of content.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid Search&lt;/strong&gt;: We’ve combined traditional keyword search with vector search to ensure we’re fetching the most relevant information from your documents. This dual approach maximizes the chances of finding the right answer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Knowledge Graph&lt;/strong&gt;: The chatbot uses a knowledge graph to better understand the relationships between different pieces of information in your documents, which leads to more accurate and contextual answers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cross-Encoder Re-ranking&lt;/strong&gt;: After retrieving the relevant information, a re-ranking system is used to make sure that the most contextually relevant answers are selected.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Completely Open Source&lt;/strong&gt;: The project is fully open-source and free to use, which means you can contribute, modify, or use it however you need.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;A Big Thank You to the Community&lt;/h1&gt; &lt;p&gt;This project wouldn’t have reached 650+ stars without the incredible support of the community. I want to express my heartfelt thanks to everyone who has starred the repo, contributed code, reported bugs, or even just tried it out. Your support means the world, and I’m incredibly grateful for the feedback that has helped shape this project into what it is today.&lt;/p&gt; &lt;p&gt;This is just the beginning! DeepSeek RAG Chatbot will continue to grow, and I’m excited about what’s to come. If you’re interested in contributing, testing, or simply learning more, feel free to check out the GitHub page. Let’s keep making this tool better and better!&lt;/p&gt; &lt;p&gt;Thank you again to everyone who has been part of this journey. Here’s to more milestones ahead!&lt;/p&gt; &lt;p&gt;edit: now it is 950+ stars 🙌🏻🙏🏻&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akhilpanja"&gt; /u/akhilpanja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyxpd7/deepseek_rag_chatbot_reaches_650_stars/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyxpd7/deepseek_rag_chatbot_reaches_650_stars/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iyxpd7/deepseek_rag_chatbot_reaches_650_stars/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-26T20:41:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1izz6gk</id>
    <title>Ollama users - What would you like to see in such an application? Which are your preferred models (for guaranteeing compatibility on release), etc.?</title>
    <updated>2025-02-28T04:11:10+00:00</updated>
    <author>
      <name>/u/BeanjaminBuxbaum</name>
      <uri>https://old.reddit.com/user/BeanjaminBuxbaum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1izz6gk/ollama_users_what_would_you_like_to_see_in_such/"&gt; &lt;img alt="Ollama users - What would you like to see in such an application? Which are your preferred models (for guaranteeing compatibility on release), etc.?" src="https://external-preview.redd.it/DqRac_A52TVo021qwWJCJADpVVfqqh6WpMjEekF677s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02cf695919c63e5a3a350b282d70c46b169c1a4f" title="Ollama users - What would you like to see in such an application? Which are your preferred models (for guaranteeing compatibility on release), etc.?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BeanjaminBuxbaum"&gt; /u/BeanjaminBuxbaum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/mcp/comments/1izxgm9/i_need_your_ideasfeature_requests_presenting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izz6gk/ollama_users_what_would_you_like_to_see_in_such/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izz6gk/ollama_users_what_would_you_like_to_see_in_such/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-28T04:11:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1izlgdn</id>
    <title>Leveraging Ollama to maximise home/work/life quality</title>
    <updated>2025-02-27T17:35:57+00:00</updated>
    <author>
      <name>/u/StrayaSpiders</name>
      <uri>https://old.reddit.com/user/StrayaSpiders</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry in advance for the long thread - I love this thing! Huge props to the Ollama community, open-webui, and this subreddit! I wouldn't have got this far without you!&lt;/p&gt; &lt;p&gt;I got an Nvidia Jetsgon AGX Orin (64gb) from work - I don't work in AI and want to use it to run LLMs that will make my life easier. I really like the concept of &amp;quot;offline&amp;quot; AI that's private and I can feed more context than I would be comfortable giving to a tech company (maybe my tinfoil hat is too tight).&lt;/p&gt; &lt;p&gt;I added a 1tb NVMe and flashed the Jetson - it's now running Ubuntu 22.04. I've so far managed to get Ollama with open-webui running. I've tried to get Stable diffusion running, but can't get it to see the GPU yet.&lt;/p&gt; &lt;p&gt;In terms of LLMs. PHI4 &amp;amp; Mistral Nemo seem to give the most useful content and not take forever to reply.&lt;/p&gt; &lt;p&gt;This thread is a huge huge &amp;quot;thank you&amp;quot; as I've used lots of comments here to help me get all of this going, but also an ask for recommended next steps! I want to go down the local/offline wormhole more and really create a system that makes my life easier maybe home automation? I work in statistics and there's a few things I'd like to achieve;&lt;/p&gt; &lt;p&gt;- IDE support for coding&lt;br /&gt; - Financial data parsing (really great if it can read financial reports and distill so I can get info quicker) [web page/pdf/doc]&lt;br /&gt; - Generic PDF/DOC reading (generic distilling information - this would save me 100s of hours in deciding if I should bother reading something further)&lt;br /&gt; - Is there a way I can make LLMs &amp;quot;remember&amp;quot; things? I found the &amp;quot;personalisation&amp;quot; area in Open webui, but can I solve this more programmatically?&lt;/p&gt; &lt;p&gt;Any other recommendations for making my day-to-day life easier (yes, I'll spend 50 hours tinkering to save 10 minutes).&lt;/p&gt; &lt;p&gt;Side note: was putting Ubuntu 22 on the Jetson a mistake? It was a pain to get to the point ollama would use GPU (drivers). Maybe I should revert to NVidia's image?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StrayaSpiders"&gt; /u/StrayaSpiders &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izlgdn/leveraging_ollama_to_maximise_homeworklife_quality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izlgdn/leveraging_ollama_to_maximise_homeworklife_quality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izlgdn/leveraging_ollama_to_maximise_homeworklife_quality/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T17:35:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1izthdp</id>
    <title>Deploying DeepSeek with Ollama + LiteLLM + OpenWebUI</title>
    <updated>2025-02-27T23:17:52+00:00</updated>
    <author>
      <name>/u/Ordinary_Ad_404</name>
      <uri>https://old.reddit.com/user/Ordinary_Ad_404</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ordinary_Ad_404"&gt; /u/Ordinary_Ad_404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/DeepSeek/comments/1izth10/deploying_deepseek_with_ollama_litellm_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izthdp/deploying_deepseek_with_ollama_litellm_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izthdp/deploying_deepseek_with_ollama_litellm_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T23:17:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1izqb0c</id>
    <title>How can I make Ollama serve a preloaded model so I can call it directly like an API?</title>
    <updated>2025-02-27T20:58:31+00:00</updated>
    <author>
      <name>/u/CellObvious3943</name>
      <uri>https://old.reddit.com/user/CellObvious3943</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Right now, when I make a request, it seems to load the model first, which slows down the response time. Is there a way to keep the model loaded and ready for faster responses?&lt;/p&gt; &lt;p&gt;this example takes: 3.62 seconds&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import requests import json url = &amp;quot;http://localhost:11434/api/generate&amp;quot; data = { &amp;quot;model&amp;quot;: &amp;quot;llama3.2&amp;quot;, &amp;quot;prompt&amp;quot;: &amp;quot;tell me a short story and make it funny.&amp;quot;, } &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CellObvious3943"&gt; /u/CellObvious3943 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izqb0c/how_can_i_make_ollama_serve_a_preloaded_model_so/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izqb0c/how_can_i_make_ollama_serve_a_preloaded_model_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izqb0c/how_can_i_make_ollama_serve_a_preloaded_model_so/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T20:58:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1izm8fh</id>
    <title>Best llm for coding!</title>
    <updated>2025-02-27T18:07:50+00:00</updated>
    <author>
      <name>/u/Potential_Chip4708</name>
      <uri>https://old.reddit.com/user/Potential_Chip4708</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am angular and nodejs developer. I am using copilot with claude sonnet 3.5 which is free. Additionally i have some experience on Mistral Codestral. (Cline). UI standpoint codestral is not good. But if you specify a bug or feature with files relative path, it gives perfect solution. Apart from that am missing any good llm? Any suggestions for a local llm. That can be better than this setup? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Potential_Chip4708"&gt; /u/Potential_Chip4708 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izm8fh/best_llm_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izm8fh/best_llm_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izm8fh/best_llm_for_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T18:07:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j02fuq</id>
    <title>beast arrived</title>
    <updated>2025-02-28T07:41:31+00:00</updated>
    <author>
      <name>/u/_ggsa</name>
      <uri>https://old.reddit.com/user/_ggsa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j02fuq/beast_arrived/"&gt; &lt;img alt="beast arrived" src="https://b.thumbs.redditmedia.com/xOVgu2nqy6sy_S39KOfageCUHpUyWG05i4_kEy_CAcw.jpg" title="beast arrived" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/bejtfc6s3ule1.jpg?width=1636&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f1aa15fb942f8bc67acc150686cfc06d8ec5bd23"&gt;https://preview.redd.it/bejtfc6s3ule1.jpg?width=1636&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f1aa15fb942f8bc67acc150686cfc06d8ec5bd23&lt;/a&gt;&lt;/p&gt; &lt;p&gt;got his monster for $3k, can't wait to see what i can do with it! spec: m1 ultra, 20/64, 128gb&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_ggsa"&gt; /u/_ggsa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j02fuq/beast_arrived/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j02fuq/beast_arrived/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j02fuq/beast_arrived/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-28T07:41:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1izh66i</id>
    <title>which AIs are you using?</title>
    <updated>2025-02-27T14:35:37+00:00</updated>
    <author>
      <name>/u/fantasy-owl</name>
      <uri>https://old.reddit.com/user/fantasy-owl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Want to try a local AI but not sure which one. I know that an AI can be good for a task but not that good for other tasks, so which AIs are you using and how is your experience with them? And Which AI is your favorite for a specif task? &lt;/p&gt; &lt;p&gt;My PC specs:&lt;br /&gt; GPU - NVIDIA 12VRAM&lt;br /&gt; CPU - AMD Ryzen 7&lt;br /&gt; RAM - 64GB&lt;/p&gt; &lt;p&gt;I’d really appreciate any advice or suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fantasy-owl"&gt; /u/fantasy-owl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izh66i/which_ais_are_you_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izh66i/which_ais_are_you_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izh66i/which_ais_are_you_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T14:35:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1izkh7c</id>
    <title>[Release] ScribePal - An Open Source Browser Extension for Private AI Chat Using Your Local Ollama Models</title>
    <updated>2025-02-27T16:56:18+00:00</updated>
    <author>
      <name>/u/Code-Forge-Temple</name>
      <uri>https://old.reddit.com/user/Code-Forge-Temple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;ScribePal - A Privacy-Focused Browser Extension for Ollama&lt;/h1&gt; &lt;p&gt;ScribePal is an Open Source intelligent browser extension that leverages AI to empower your web experience by providing contextual insights, efficient content summarization, and seamless interaction while you browse.&lt;/p&gt; &lt;h2&gt;Privacy &amp;amp; Compatibility&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Works with local Ollama models - all AI processing stays within your network&lt;/li&gt; &lt;li&gt;Compatible with Chrome, Firefox, Vivaldi, Opera, Edge, Brave, etc.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Key Features&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI-powered assistance:&lt;/strong&gt; Uses your local Ollama models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;100% Private:&lt;/strong&gt; All data stays within your LAN&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Theming:&lt;/strong&gt; Supports light and dark themes&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat Interface:&lt;/strong&gt; Draggable chat box for easy interaction&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Management:&lt;/strong&gt; Select, refresh, download, and delete models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Capture Tool:&lt;/strong&gt; Highlight and capture webpage content&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompt Customization:&lt;/strong&gt; Customize how the AI responds&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;&lt;em&gt;Note: Requires a running Ollama instance on your local machine or LAN&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I have provided the full Ollama intructions in &lt;a href="https://github.com/code-forge-temple/scribe-pal?tab=readme-ov-file#prerequisites"&gt;prerequisites&lt;/a&gt; section of the README repo.&lt;/p&gt; &lt;h2&gt;Installation&lt;/h2&gt; &lt;p&gt;Please check the &lt;a href="https://github.com/code-forge-temple/scribe-pal?tab=readme-ov-file#installing"&gt;installing&lt;/a&gt; section of the README repo.&lt;/p&gt; &lt;h2&gt;How to Use&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Open the Extension:&lt;/strong&gt; Click the extension icon in your toolbar&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Configure:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Set your Ollama Server URL&lt;/li&gt; &lt;li&gt;Choose your preferred theme&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat Interface:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Click &amp;quot;Show ScribePal chat&amp;quot;&lt;/li&gt; &lt;li&gt;Drag the chat box anywhere on the page&lt;/li&gt; &lt;li&gt;Capture webpage content with &lt;code&gt;@captured&lt;/code&gt; tag&lt;/li&gt; &lt;li&gt;Customize prompts for better responses&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interact:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Type queries and get markdown-formatted responses&lt;/li&gt; &lt;li&gt;Manage your Ollama models directly from the interface&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Quick Demo&lt;/h2&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=IR7Jufc0zxo"&gt;Watch the tutorial video&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;GitHub Repository: &lt;a href="https://github.com/code-forge-temple/scribe-pal"&gt;https://github.com/code-forge-temple/scribe-pal&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Contributing&lt;/h2&gt; &lt;p&gt;Found a bug or have a suggestion? I'd love to hear from you! Please open an issue on the &lt;a href="https://github.com/code-forge-temple/scribe-pal/issues"&gt;GitHub repository&lt;/a&gt; with: - A clear description of the issue/suggestion - Your browser and version - Steps to reproduce (for bugs) - Your Ollama version and setup&lt;/p&gt; &lt;p&gt;Your feedback helps make ScribePal better for everyone!&lt;/p&gt; &lt;p&gt;&lt;em&gt;Note: When opening issues, please check if a similar issue already exists to avoid duplicates.&lt;/em&gt;&lt;/p&gt; &lt;h2&gt;License&lt;/h2&gt; &lt;p&gt;This project is licensed under the GNU General Public License v3.0.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Code-Forge-Temple"&gt; /u/Code-Forge-Temple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izkh7c/release_scribepal_an_open_source_browser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izkh7c/release_scribepal_an_open_source_browser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izkh7c/release_scribepal_an_open_source_browser/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T16:56:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1izmfn7</id>
    <title>Building a robot that can see, hear, talk, and dance. Powered by on-device AI with the Jetson Orin NX, Moondream &amp; Whisper (open source)</title>
    <updated>2025-02-27T18:16:23+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1izmfn7/building_a_robot_that_can_see_hear_talk_and_dance/"&gt; &lt;img alt="Building a robot that can see, hear, talk, and dance. Powered by on-device AI with the Jetson Orin NX, Moondream &amp;amp; Whisper (open source)" src="https://external-preview.redd.it/dWlhd3A2OHYzcWxlMe__omCO_n66cYU7Fe7wXFz05iYznG-U5sQ5kSodSfXF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cff4f9933b92052bd2cd458e4172868ca7c62431" title="Building a robot that can see, hear, talk, and dance. Powered by on-device AI with the Jetson Orin NX, Moondream &amp;amp; Whisper (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9kwfq88v3qle1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1izmfn7/building_a_robot_that_can_see_hear_talk_and_dance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1izmfn7/building_a_robot_that_can_see_hear_talk_and_dance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-27T18:16:23+00:00</published>
  </entry>
</feed>
