<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-10T13:09:47+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ikxer9</id>
    <title>Supercharge Your Document Processing: DataBridge Rules + DeepSeek = Magic!</title>
    <updated>2025-02-08T20:54:58+00:00</updated>
    <author>
      <name>/u/yes-no-maybe_idk</name>
      <uri>https://old.reddit.com/user/yes-no-maybe_idk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey r/ollama! I'm excited to present DataBridge's rules system - a powerful way to process documents &lt;strong&gt;exactly&lt;/strong&gt; how you want, completely locally!&lt;/p&gt; &lt;h1&gt;What's Cool About It?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;100% Local Processing&lt;/strong&gt;: Works beautifully with DeepSeek/Llama2 through Ollama&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Document Processing&lt;/strong&gt;: Extract metadata and transform content automatically&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Super Simple Setup&lt;/strong&gt;: Just modify &lt;code&gt;databridge.toml&lt;/code&gt; to use your preferred model:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[rules] provider = &amp;quot;ollama&amp;quot; model_name = &amp;quot;deepseek-coder&amp;quot; # or any other model you prefer &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Builtin Rules:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Metadata Rules&lt;/strong&gt;: Automatically extract structured data&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;metadata_rule = MetadataExtractionRule(schema={ &amp;quot;title&amp;quot;: str, &amp;quot;category&amp;quot;: str, &amp;quot;priority&amp;quot;: str }) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;2. &lt;strong&gt;Natural Language Rules&lt;/strong&gt;: Transform content using plain English&lt;/p&gt; &lt;pre&gt;&lt;code&gt;clean_rule = NaturalLanguageRule( prompt=&amp;quot;Remove PII and standardize formatting&amp;quot; ) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Totally Customizable!&lt;/h1&gt; &lt;p&gt;You can create your own rules! Here's a quick example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;class KeywordRule(BaseRule): &amp;quot;&amp;quot;&amp;quot;Extract keywords from documents&amp;quot;&amp;quot;&amp;quot; async def apply(self, content: str): # Your custom logic here return {&amp;quot;keywords&amp;quot;: extracted_keywords}, content &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Real-World Use Cases:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;PII removal&lt;/li&gt; &lt;li&gt;Content classification&lt;/li&gt; &lt;li&gt;Auto-summarization&lt;/li&gt; &lt;li&gt;Format standardization&lt;/li&gt; &lt;li&gt;Custom metadata extraction&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All this running on your hardware, your rules, your way. Works amazingly well with smaller models! üéâ&lt;/p&gt; &lt;p&gt;Let me know what custom rules you'd like to see implemented or if you have any questions!&lt;/p&gt; &lt;p&gt;Checkout &lt;a href="https://github.com/databridge-org/databridge-core"&gt;DatBridge&lt;/a&gt; and our &lt;a href="https://databridge.gitbook.io/databridge-docs"&gt;docs&lt;/a&gt;. Leave a ‚≠ê if you like it, feel free to submit a PR for your rules :).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yes-no-maybe_idk"&gt; /u/yes-no-maybe_idk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikxer9/supercharge_your_document_processing_databridge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikxer9/supercharge_your_document_processing_databridge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ikxer9/supercharge_your_document_processing_databridge/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T20:54:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilhdi0</id>
    <title>Pdf, images in Local Model</title>
    <updated>2025-02-09T15:37:54+00:00</updated>
    <author>
      <name>/u/Visual_Locksmith_997</name>
      <uri>https://old.reddit.com/user/Visual_Locksmith_997</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any way to upload pdf, images in deepseek r1 (8b ) local model. I run it using powershell /web-ui.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Visual_Locksmith_997"&gt; /u/Visual_Locksmith_997 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilhdi0/pdf_images_in_local_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilhdi0/pdf_images_in_local_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilhdi0/pdf_images_in_local_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T15:37:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilhwh2</id>
    <title>Is pulling models not working at the moment?</title>
    <updated>2025-02-09T16:01:06+00:00</updated>
    <author>
      <name>/u/Nabukadnezar</name>
      <uri>https://old.reddit.com/user/Nabukadnezar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I only get this: &lt;code&gt; ... pulling manifest pulling 2bada8a74506... 0% ‚ñï ‚ñè 0 B/4.7 GB Error: max retries exceeded: ... &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nabukadnezar"&gt; /u/Nabukadnezar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilhwh2/is_pulling_models_not_working_at_the_moment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilhwh2/is_pulling_models_not_working_at_the_moment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilhwh2/is_pulling_models_not_working_at_the_moment/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T16:01:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilakbj</id>
    <title>"Structured Output" with Ollama and LangChainJS [the return]</title>
    <updated>2025-02-09T08:49:48+00:00</updated>
    <author>
      <name>/u/Inevitable-Judge2642</name>
      <uri>https://old.reddit.com/user/Inevitable-Judge2642</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ilakbj/structured_output_with_ollama_and_langchainjs_the/"&gt; &lt;img alt="&amp;quot;Structured Output&amp;quot; with Ollama and LangChainJS [the return]" src="https://external-preview.redd.it/v8th63x1NCCLZjxABxP5OPU2v457BOEOTLqo9VEhnk4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac7bd7b01c348ab2f4a3654ac57684204869a258" title="&amp;quot;Structured Output&amp;quot; with Ollama and LangChainJS [the return]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Judge2642"&gt; /u/Inevitable-Judge2642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://k33g.hashnode.dev/structured-output-with-ollama-and-langchainjs-the-return"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilakbj/structured_output_with_ollama_and_langchainjs_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilakbj/structured_output_with_ollama_and_langchainjs_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T08:49:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1il7rwy</id>
    <title>ParLlama v0.3.15 released. Now supports Ollama, OpenAI, GoogleAI, Anthropic, Groq, xAI, Bedrock, OpenRouter</title>
    <updated>2025-02-09T05:36:51+00:00</updated>
    <author>
      <name>/u/probello</name>
      <uri>https://old.reddit.com/user/probello</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1il7rwy/parllama_v0315_released_now_supports_ollama/"&gt; &lt;img alt="ParLlama v0.3.15 released. Now supports Ollama, OpenAI, GoogleAI, Anthropic, Groq, xAI, Bedrock, OpenRouter" src="https://external-preview.redd.it/g5LJqgM5VjbIVUFBSAgJzLXbf1SuAHgtPCr33OTJjkY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6abaac865026b7320f41dbbdbdda11366edf04c" title="ParLlama v0.3.15 released. Now supports Ollama, OpenAI, GoogleAI, Anthropic, Groq, xAI, Bedrock, OpenRouter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/7n0k1sywv1ie1.png?width=1400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b7e8775111b28eb3b6af3cb35b15027ce485448"&gt;https://preview.redd.it/7n0k1sywv1ie1.png?width=1400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b7e8775111b28eb3b6af3cb35b15027ce485448&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What My project Does:&lt;/h1&gt; &lt;p&gt;PAR LLAMA is a powerful TUI (Text User Interface) written in Python and designed for easy management and use of Ollama and Large Language Models as well as interfacing with online Providers such as Ollama, OpenAI, GoogleAI, Anthropic, Bedrock, Groq, xAI, OpenRouter&lt;/p&gt; &lt;h1&gt;Whats New:&lt;/h1&gt; &lt;h1&gt;v0.3.15&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Added copy button to the fence blocks in chat markdown for easy code copy.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;v0.3.14&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Fix crash caused some models having some missing fields in model file&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;v0.3.13&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Handle clipboard errors&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;v0.3.12&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Fixed bug where changing providers that have custom urls would break other providers&lt;/li&gt; &lt;li&gt;Fixed bug where changing Ollama base url would cause connection timed out&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Easy-to-use interface for interacting with Ollama and cloud hosted LLMs&lt;/li&gt; &lt;li&gt;Dark and Light mode support, plus custom themes&lt;/li&gt; &lt;li&gt;Flexible installation options (uv, pipx, pip or dev mode)&lt;/li&gt; &lt;li&gt;Chat session management&lt;/li&gt; &lt;li&gt;Custom prompt library support&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;GitHub and PyPI&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;PAR LLAMA is under active development and getting new features all the time.&lt;/li&gt; &lt;li&gt;Check out the project on GitHub or for full documentation, installation instructions, and to contribute: &lt;a href="https://github.com/paulrobello/parllama"&gt;https://github.com/paulrobello/parllama&lt;/a&gt;&lt;/li&gt; &lt;li&gt;PyPI &lt;a href="https://pypi.org/project/parllama/"&gt;https://pypi.org/project/parllama/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Comparison:&lt;/h1&gt; &lt;p&gt;I have seem many command line and web applications for interacting with LLM's but have not found any TUI related applications&lt;/p&gt; &lt;h1&gt;Target Audience&lt;/h1&gt; &lt;p&gt;Anybody that loves or wants to love terminal interactions and LLM's&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/probello"&gt; /u/probello &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1il7rwy/parllama_v0315_released_now_supports_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1il7rwy/parllama_v0315_released_now_supports_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1il7rwy/parllama_v0315_released_now_supports_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T05:36:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ill1uf</id>
    <title>Becoming an AI solopreneur: Seeking advice on essential tools, learning paths, and prioritization</title>
    <updated>2025-02-09T18:13:22+00:00</updated>
    <author>
      <name>/u/loloamoravain</name>
      <uri>https://old.reddit.com/user/loloamoravain</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, 36 yo, always worked in startup as Growth Marketing. I quit my job a month ago and decided to start learning about AI.&lt;/p&gt; &lt;p&gt;For the last two weeks, I've been watching a huge amount of content and I'm really enjoying it. I discovered ollama, downloaded models, modified prompts systems, discovered python, installed cursor, followed tutorials to fine tun a model with lora, to create a rag chatbot, ... &lt;/p&gt; &lt;p&gt;I'm now pretty convinced that there's a lot of potential for solopreneur and/or to create startups.&lt;/p&gt; &lt;p&gt;Now that I have explored various topics but only scratched the surface, what would you recommend I study in depth? Which tools, models, or trends should I focus on mastering? Which websites / forums should I bookmark ?&lt;/p&gt; &lt;p&gt;Thx a lot for your help !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/loloamoravain"&gt; /u/loloamoravain &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ill1uf/becoming_an_ai_solopreneur_seeking_advice_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ill1uf/becoming_an_ai_solopreneur_seeking_advice_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ill1uf/becoming_an_ai_solopreneur_seeking_advice_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T18:13:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilg690</id>
    <title>Start chat with message from model.</title>
    <updated>2025-02-09T14:43:23+00:00</updated>
    <author>
      <name>/u/Velskadi</name>
      <uri>https://old.reddit.com/user/Velskadi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm having a hard time finding any info on this, so I am hoping someone here might have some guidance. I would like to start a chat with a model using ollama start &amp;lt;MODEL NAME&amp;gt;, and have the model start the conversation with a response before I give it a prompt.&lt;/p&gt; &lt;p&gt;Preferably I'd like this message to be static, something like &amp;quot;I am your workshop assistant. Please give me these pieces of information so I can assist. etc. etc&amp;quot;&lt;/p&gt; &lt;p&gt;Is this possible using Ollama? If so, would it be possible to do this in Openwebui as well? Any advice would be appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Velskadi"&gt; /u/Velskadi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilg690/start_chat_with_message_from_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilg690/start_chat_with_message_from_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilg690/start_chat_with_message_from_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T14:43:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilfj2d</id>
    <title>hardware question</title>
    <updated>2025-02-09T14:11:48+00:00</updated>
    <author>
      <name>/u/quantrpeter</name>
      <uri>https://old.reddit.com/user/quantrpeter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Jetson Orin Nano Super = 1024 CUDA&lt;br /&gt;&lt;/li&gt; &lt;li&gt;2070 = 2560 CUDA&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Telsa K80 24GB = 4992 CUDA&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For second hand price, K80 &amp;lt; 2070 &amp;lt; Jetson. For real ollama performance, isn't it more cuda core must win? If so, Jetson is not valuable.&lt;/p&gt; &lt;p&gt;thanks&lt;br /&gt; Peter&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantrpeter"&gt; /u/quantrpeter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilfj2d/hardware_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilfj2d/hardware_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilfj2d/hardware_question/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T14:11:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilo8is</id>
    <title>UNCENSORED AI MODELS</title>
    <updated>2025-02-09T20:25:10+00:00</updated>
    <author>
      <name>/u/Infamous_5563</name>
      <uri>https://old.reddit.com/user/Infamous_5563</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just have an 8gb Ram system, no GPU. I wanted to ask what is the best uncensored AI LLM Models, like heard about Ollama Lexi uncensored but the model was too heavy for my machine for my kind of system to run locally, and if not Ollama Model, Is there any other? I searched that can run either in terminal or LM studio, Is there a better chat interface UI? Need advice from the Gods&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Infamous_5563"&gt; /u/Infamous_5563 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilo8is/uncensored_ai_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilo8is/uncensored_ai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilo8is/uncensored_ai_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T20:25:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1il0zea</id>
    <title>Just released an open-source Mac client for Ollama built with Swift/SwiftUI</title>
    <updated>2025-02-08T23:34:21+00:00</updated>
    <author>
      <name>/u/billythepark</name>
      <uri>https://old.reddit.com/user/billythepark</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently created a new Mac app using Swift. Last year, I released an open-source iPhone client for Ollama (a program for running LLMs locally) called MyOllama using Flutter. I planned to make a Mac version too, but when I tried with Flutter, the design didn't feel very Mac-native, so I put it aside.&lt;/p&gt; &lt;p&gt;Early this year, I decided to rebuild it from scratch using Swift/SwiftUI. This app lets you install and chat with LLMs like Deepseek on your Mac using Ollama. Features include:&lt;/p&gt; &lt;p&gt;- Contextual conversations&lt;/p&gt; &lt;p&gt;- Save and search chat history&lt;/p&gt; &lt;p&gt;- Customize system prompts&lt;/p&gt; &lt;p&gt;- And more...&lt;/p&gt; &lt;p&gt;It's completely open-source! Check out the code here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/bipark/mac_ollama_client"&gt;https://github.com/bipark/mac_ollama_client&lt;/a&gt;&lt;/p&gt; &lt;p&gt;#Ollama #LLMHippo&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/billythepark"&gt; /u/billythepark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1il0zea/just_released_an_opensource_mac_client_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1il0zea/just_released_an_opensource_mac_client_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1il0zea/just_released_an_opensource_mac_client_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T23:34:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1iljej9</id>
    <title>Model occasionally continues to use CPU despite having finished responding.</title>
    <updated>2025-02-09T17:04:21+00:00</updated>
    <author>
      <name>/u/Velskadi</name>
      <uri>https://old.reddit.com/user/Velskadi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty much the title. I am running the magnum-v4-9b model through Open-webui, using my CPU (Ryzen 9 5900X). The model runs well, but brings my CPU usage to about 80-90% while it is generating a response. After it finishes it will sometimes keep my CPU usage pegged at these levels.&lt;/p&gt; &lt;p&gt;The last time this happened I tried stopping it with &lt;code&gt;ollama stop &amp;lt;model name&amp;gt;&lt;/code&gt; but it was then stuck in the &amp;quot;Stopping&amp;quot; state, and my CPU useage stayed high. I had to restart the Ollama service to fix this issue.&lt;/p&gt; &lt;p&gt;I may have seen this issue with other models as well but not realized it, as it was only today that I started monitoring the CPU usage. Any advice is appreciated!&lt;/p&gt; &lt;p&gt;-SPECS-&lt;br /&gt; CPU: Ryzen 9 5900X&lt;br /&gt; GPU (Unused): AMD Radeon 6700 XT&lt;br /&gt; RAM: 33GB DDR4&lt;br /&gt; OS: Arch Linux&lt;/p&gt; &lt;p&gt;EDIT: I'd like to note that all I had prompted when this happened was &amp;quot;This is a test. Please respond with Hello&amp;quot;, which it did.&lt;/p&gt; &lt;p&gt;While it is stuck like this the model takes a long time to start responding to any new prompts, and it generates it much slower. The CPU stays almost maxed out even after these subsequent prompts as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Velskadi"&gt; /u/Velskadi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iljej9/model_occasionally_continues_to_use_cpu_despite/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iljej9/model_occasionally_continues_to_use_cpu_despite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iljej9/model_occasionally_continues_to_use_cpu_despite/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T17:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilnx95</id>
    <title>Training a local model w/ Confluence?</title>
    <updated>2025-02-09T20:11:50+00:00</updated>
    <author>
      <name>/u/thenyx</name>
      <uri>https://old.reddit.com/user/thenyx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to train llama3.2:8b with content from Confluence - what would be the best way to go about this?&lt;/p&gt; &lt;p&gt;I've seen mention of RAG, but how would this apply? Fairly new to this part of LLMs. Running MacOS if this matters.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thenyx"&gt; /u/thenyx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilnx95/training_a_local_model_w_confluence/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilnx95/training_a_local_model_w_confluence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilnx95/training_a_local_model_w_confluence/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T20:11:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ildvgc</id>
    <title>Local TTS (text-to-speech) AI model with a human voice and file output?</title>
    <updated>2025-02-09T12:41:08+00:00</updated>
    <author>
      <name>/u/simo41993</name>
      <uri>https://old.reddit.com/user/simo41993</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don't know if this is the right place to ask, but... i was looking for a text to speech alternative to the quite expensive online ones i was looking for recently.&lt;/p&gt; &lt;p&gt;I'm partially blind and it would be of great help to have a recorded and narrated version of some technical e-books i own.&lt;/p&gt; &lt;p&gt;As i was saying, models like Elevenlabs and similar are really quite good but absolutely too expensive in terms of ‚Ç¨/time for what i need to do (and the books are quite long too).&lt;/p&gt; &lt;p&gt;I was wondering, because of that, &lt;strong&gt;if there was a good (the normal TTS is quite abismal and distracting) alternative to run locally that can transpose the book in audio&lt;/strong&gt; and let me save a mp3 or similar file for later use.&lt;/p&gt; &lt;p&gt;I have to say, also, that i'm not a programmer whatsoever, so i should be able to follow simple instructions but, sadly, nothing more. so... a ready to use solution would be quite nice (or a detailed, like i'm a 3yo, set of instructions).&lt;/p&gt; &lt;p&gt;i'm using ollama + docker and free open web-ui for playing (literally) with some offline models and also thinking about using something compatible with this already running system... hopefully, possibly?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Another complication it's that i'm italian, so... the probably unexisting model should be capable to use italian language too...&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The following are my PC specs, if needed:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Processor: intel i7 13700k&lt;/li&gt; &lt;li&gt;MB: Asus ROG Z790-H&lt;/li&gt; &lt;li&gt;Ram: 64gb Corsair 5600 MT/S&lt;/li&gt; &lt;li&gt;Gpu: RTX 4070TI 12gb - MSI Ventus 3X&lt;/li&gt; &lt;li&gt;Storage: Samsung 970EVO NVME SSD + others&lt;/li&gt; &lt;li&gt;Windows 11 PRO 64bit&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Sorry for the long post and thank you for any help :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simo41993"&gt; /u/simo41993 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ildvgc/local_tts_texttospeech_ai_model_with_a_human/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ildvgc/local_tts_texttospeech_ai_model_with_a_human/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ildvgc/local_tts_texttospeech_ai_model_with_a_human/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T12:41:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilp9bc</id>
    <title>script to import / export models between devices locally</title>
    <updated>2025-02-09T21:08:35+00:00</updated>
    <author>
      <name>/u/nahushrk</name>
      <uri>https://old.reddit.com/user/nahushrk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;wanted to share this simple scrip that lets you export the models downloaded to a machine to another machine without re-downloading it again&lt;/p&gt; &lt;p&gt;particularly useful when models are large and/or you want to share the models locally, saves time and bandwidth&lt;/p&gt; &lt;p&gt;just make sure the ollama version is same on both machines in case the storage mechanism changes&lt;/p&gt; &lt;p&gt;&lt;a href="https://gist.github.com/nahushrk/5d980e676c4f2762ca385bd6fb9498a9"&gt;https://gist.github.com/nahushrk/5d980e676c4f2762ca385bd6fb9498a9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the way this works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;export a model by name and size&lt;/li&gt; &lt;li&gt;a .tar file is created in dir where you ran this script&lt;/li&gt; &lt;li&gt;copy .tar file and this script to another machine&lt;/li&gt; &lt;li&gt;run import subcommand pointing to .tar file&lt;/li&gt; &lt;li&gt;run ollama list to see new model being added&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nahushrk"&gt; /u/nahushrk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilp9bc/script_to_import_export_models_between_devices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilp9bc/script_to_import_export_models_between_devices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilp9bc/script_to_import_export_models_between_devices/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T21:08:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilwrme</id>
    <title>Ollama Research Agent | Agentic AI | DeepSeek and Llama 3.2 based Local ...</title>
    <updated>2025-02-10T03:27:43+00:00</updated>
    <author>
      <name>/u/Sangwan70</name>
      <uri>https://old.reddit.com/user/Sangwan70</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ilwrme/ollama_research_agent_agentic_ai_deepseek_and/"&gt; &lt;img alt="Ollama Research Agent | Agentic AI | DeepSeek and Llama 3.2 based Local ..." src="https://external-preview.redd.it/oodCpe-QxVzXvuRh_YIKkrAcNgKdJZhqS2aP14QHVOE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1bd07b4f716cef864694e37af43772f2ac4bdc74" title="Ollama Research Agent | Agentic AI | DeepSeek and Llama 3.2 based Local ..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sangwan70"&gt; /u/Sangwan70 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=lXjSEi6cmDE&amp;amp;si=gwQsckmdXyXL4qTO"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilwrme/ollama_research_agent_agentic_ai_deepseek_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilwrme/ollama_research_agent_agentic_ai_deepseek_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T03:27:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilxe14</id>
    <title>What's the smartest reasoning &amp; non reasoning LLM i can run on a 4090Mobile?</title>
    <updated>2025-02-10T04:01:59+00:00</updated>
    <author>
      <name>/u/Tricky_Reflection_75</name>
      <uri>https://old.reddit.com/user/Tricky_Reflection_75</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i am just a little overwhelmed with the all the different paramaters of models, and then the quantized GGUF versions off them wiith all the different size variants , etc etc.&lt;/p&gt; &lt;p&gt;and when i say &amp;quot;smartest&amp;quot; , i primarily mean just general intelligence and math , code and other factual information&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tricky_Reflection_75"&gt; /u/Tricky_Reflection_75 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilxe14/whats_the_smartest_reasoning_non_reasoning_llm_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilxe14/whats_the_smartest_reasoning_non_reasoning_llm_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilxe14/whats_the_smartest_reasoning_non_reasoning_llm_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T04:01:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilzgwn</id>
    <title>Controlling model swapping</title>
    <updated>2025-02-10T06:05:16+00:00</updated>
    <author>
      <name>/u/Bukt</name>
      <uri>https://old.reddit.com/user/Bukt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the best way to control model swapping? For example, if a user sends a request with the context size, I don't want the model to unload and reload. Can Ollama ignore certain parameters to prevent this? Or would I need to have a sanatizing proxy?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bukt"&gt; /u/Bukt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilzgwn/controlling_model_swapping/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilzgwn/controlling_model_swapping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilzgwn/controlling_model_swapping/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T06:05:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilzwvr</id>
    <title>3B model with a N100 and 32GB DDR4 RAM</title>
    <updated>2025-02-10T06:34:05+00:00</updated>
    <author>
      <name>/u/Tuxedotux83</name>
      <uri>https://old.reddit.com/user/Tuxedotux83</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone here tried a 3B model (e.g. as Q8) with Intel N100, 32GB of DDR4 RAM and NVMe storage? CPU inference. What kind of t/s were you able to get?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tuxedotux83"&gt; /u/Tuxedotux83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilzwvr/3b_model_with_a_n100_and_32gb_ddr4_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilzwvr/3b_model_with_a_n100_and_32gb_ddr4_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilzwvr/3b_model_with_a_n100_and_32gb_ddr4_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T06:34:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilo9cz</id>
    <title>I built an agentic Spotify app with 50 lines of YAML and ollama-supported LLMs</title>
    <updated>2025-02-09T20:26:11+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ilo9cz/i_built_an_agentic_spotify_app_with_50_lines_of/"&gt; &lt;img alt="I built an agentic Spotify app with 50 lines of YAML and ollama-supported LLMs" src="https://external-preview.redd.it/NHM0bW05dGphNmllMceFbxA9VETScbYkQkd_Y6Vr5Y0XlSdRTwjLPPGK61FC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e806715355fc40a96e65c9307c08a71adecc04be" title="I built an agentic Spotify app with 50 lines of YAML and ollama-supported LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a Spotify agent with 50 lines of YAML and an open source model.&lt;/p&gt; &lt;p&gt;The second most requested feature for Arch Gateway was bearer authorization for function calling scenarios to secure business APIs.&lt;/p&gt; &lt;p&gt;So when we added support for bearer authorization it opened up new possibilities- including connecting to third-party APIs so that user queries can be fulfilled via existing SaaS tools. Or consumer apps like Spotify. &lt;/p&gt; &lt;p&gt;For those not familiar with the project - Arch is an intelligent (edge and LLM) proxy designed for agentic apps and prompts - it handles the pesky stuff in handling, processing and routing prompts so that you can focus on the core business objectives is your AI app. You can read more here: &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;here is the 20+ lines of yaml that can help you achieve the above experience. Of course, you need the Gradio app too.&lt;/p&gt; &lt;p&gt;prompt_targets: - name: get_new_releases description: Get a list of new album releases featured in Spotify (shown, for example, on a Spotify player‚Äôs ‚ÄúBrowse‚Äù tab). parameters: - name: country description: the country where the album is released required: true type: str in_path: true - name: limit type: integer description: The maximum number of results to return default: &amp;quot;5&amp;quot; endpoint: name: spotify path: /v1/browse/new-releases http_headers: Authorization: &amp;quot;Bearer $SPOTIFY_CLIENT_KEY&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rqoxdb4ka6ie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilo9cz/i_built_an_agentic_spotify_app_with_50_lines_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilo9cz/i_built_an_agentic_spotify_app_with_50_lines_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T20:26:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1im1njg</id>
    <title>second hand 2080 vs brand new jetson</title>
    <updated>2025-02-10T08:42:24+00:00</updated>
    <author>
      <name>/u/quantrpeter</name>
      <uri>https://old.reddit.com/user/quantrpeter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi&lt;/p&gt; &lt;p&gt;second hand 2080 vs brand new jetson. which on can run ollama faster?&lt;/p&gt; &lt;p&gt;thanks&lt;br /&gt; Peter&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantrpeter"&gt; /u/quantrpeter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1im1njg/second_hand_2080_vs_brand_new_jetson/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1im1njg/second_hand_2080_vs_brand_new_jetson/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1im1njg/second_hand_2080_vs_brand_new_jetson/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T08:42:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1im52z2</id>
    <title>Has anyone had ollama download deepseek-r1:70b by itsself and then all other models get deleted.</title>
    <updated>2025-02-10T12:33:57+00:00</updated>
    <author>
      <name>/u/StressOwn</name>
      <uri>https://old.reddit.com/user/StressOwn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1im52z2/has_anyone_had_ollama_download_deepseekr170b_by/"&gt; &lt;img alt="Has anyone had ollama download deepseek-r1:70b by itsself and then all other models get deleted." src="https://b.thumbs.redditmedia.com/r85fwXLhtXLKmsG8O-TW-TJityUs7VIQRVRocubhyPw.jpg" title="Has anyone had ollama download deepseek-r1:70b by itsself and then all other models get deleted." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/b3ypue203bie1.png?width=699&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4eedc738adedd0148e2040f7247fa2f6ac196345"&gt;https://preview.redd.it/b3ypue203bie1.png?width=699&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4eedc738adedd0148e2040f7247fa2f6ac196345&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ollama somehow downloaded this by itself, then deleted all other models, llmam3.2 deepseekr1-14b, bllava,qwencoder etc.was running openui but not open to outside traffic, but was using port forwarding to expose the api&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StressOwn"&gt; /u/StressOwn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1im52z2/has_anyone_had_ollama_download_deepseekr170b_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1im52z2/has_anyone_had_ollama_download_deepseekr170b_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1im52z2/has_anyone_had_ollama_download_deepseekr170b_by/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T12:33:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilq0gd</id>
    <title>Goose + Ollama best model for agent coding</title>
    <updated>2025-02-09T21:40:17+00:00</updated>
    <author>
      <name>/u/einthecorgi2</name>
      <uri>https://old.reddit.com/user/einthecorgi2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just starting to mess around with goose, would love to start using it more. Current daily driver is cursor. Just wondering if anyone has any feedback on which model would work the best for code generation. I have been experimenting with a couple but I do not have a machine setup to run anything larger yet. So far my experience has been (all these are through Groq)&lt;br /&gt; - llama 3: would not maintain the main purpose of the app as the prompting lengthened and eventually just do whatever to make the code run.&lt;br /&gt; - Deepseek R1: would not actually edit or change any code (i think there is a specific &amp;quot;action&amp;quot; version of the model that is needed). But would run CLI commands, and if I kept asking would eventually put some code in a file. &lt;/p&gt; &lt;p&gt;Will update my progress as Goose gets better and I test more models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/einthecorgi2"&gt; /u/einthecorgi2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilq0gd/goose_ollama_best_model_for_agent_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilq0gd/goose_ollama_best_model_for_agent_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilq0gd/goose_ollama_best_model_for_agent_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T21:40:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1im3m1p</id>
    <title>Ollama windows run uses gpu however through running on api does not</title>
    <updated>2025-02-10T11:03:41+00:00</updated>
    <author>
      <name>/u/i-have-the-stash</name>
      <uri>https://old.reddit.com/user/i-have-the-stash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am using ollama mistral model as a classifier on my project which is around 4.1 gb. I have around 16gb vram&lt;/p&gt; &lt;p&gt;When i use it with ollama run and inference via cli, it is properly set up and utilizes gpu on inference run time but keeps it on system memory when not used.&lt;/p&gt; &lt;p&gt;When i api call its just uses it on memory, does not utilize gpu at all.&lt;/p&gt; &lt;p&gt;I would like to keep the model always on vram, and never utilize system memory if possible. Are there settings for that ?&lt;/p&gt; &lt;p&gt;Edit: Is there also an option to make it greedy ? I would like to fetch into vram when its inference time and dump it the moment it completely executes. I dont want gradual decrease at all since i will always call it via api and it stacks up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/i-have-the-stash"&gt; /u/i-have-the-stash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1im3m1p/ollama_windows_run_uses_gpu_however_through/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1im3m1p/ollama_windows_run_uses_gpu_however_through/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1im3m1p/ollama_windows_run_uses_gpu_however_through/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T11:03:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilwupn</id>
    <title>sqluniversal</title>
    <updated>2025-02-10T03:32:18+00:00</updated>
    <author>
      <name>/u/tech215</name>
      <uri>https://old.reddit.com/user/tech215</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ilwupn/sqluniversal/"&gt; &lt;img alt="sqluniversal" src="https://b.thumbs.redditmedia.com/dFkexJfNAWq3YjMNTqLgonhLsU2UtWzEpih2qdx8ooI.jpg" title="sqluniversal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Great news! Here is the corrected and improved text:&lt;/p&gt; &lt;p&gt;&amp;quot;Important announcement! I am excited to share with you that SQLUniversal is compatible with all databases. This means that you will be able to use our tool to manage and analyze data efficiently, no matter what type of database you use.&lt;/p&gt; &lt;p&gt;I am currently working on developing the front-end, but I wanted to share this news with you so that you know that we are making progress on the project.&lt;/p&gt; &lt;p&gt;Also, I want to highlight that we have tested SQLUniversal on Granite3.1-moe:1b-instruct-fp16 and the results have been excellent. This gives us confidence that our tool will be able to handle large amounts of data efficiently.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; Python and the library are used: pip install flask ---{ &amp;quot;prompt&amp;quot;: &amp;quot;Get all users whose name is 'John'&amp;quot;, &amp;quot;database&amp;quot;: &amp;quot;postgresql&amp;quot; } { &amp;quot;output&amp;quot;: &amp;quot;SELECT * FROM users WHERE name = 'John';&amp;quot; }&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;/h2&gt; &lt;p&gt;&lt;a href="https://github.com/techindev/sqluniversal/tree/main"&gt;https://github.com/techindev/sqluniversal/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here you choose MySQL sqlite etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tech215"&gt; /u/tech215 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ilwupn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilwupn/sqluniversal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilwupn/sqluniversal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T03:32:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilq95h</id>
    <title>Why does ollama use the port 11434 as the default port?</title>
    <updated>2025-02-09T21:50:33+00:00</updated>
    <author>
      <name>/u/mozophe</name>
      <uri>https://old.reddit.com/user/mozophe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Because, (0)11434 = (o)llama in leetspeak.&lt;/p&gt; &lt;p&gt;For info, the max port number is 65535, so having the first zero as well is not possible as it is a 6 digit number and greater than 65535.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mozophe"&gt; /u/mozophe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilq95h/why_does_ollama_use_the_port_11434_as_the_default/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilq95h/why_does_ollama_use_the_port_11434_as_the_default/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilq95h/why_does_ollama_use_the_port_11434_as_the_default/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T21:50:33+00:00</published>
  </entry>
</feed>
