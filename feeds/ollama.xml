<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-30T11:20:45+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1n1hyr5</id>
    <title>Building a local Ai PC</title>
    <updated>2025-08-27T14:04:19+00:00</updated>
    <author>
      <name>/u/KCCarpenter5739</name>
      <uri>https://old.reddit.com/user/KCCarpenter5739</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Advice needed: I’m looking at micro center, building my own pc. I’m thinking of using Ryzen 9 cpu, Msi pro x870e-p wifi mobo, Corsair 32gb ram (128gb total), Samsung pro 4 tb nvme, liquid cooling aio, 1300w Psu, LIAN li O11D XL case.&lt;/p&gt; &lt;p&gt;GPU is where I’m getting stuck, the mobo has 3 slots (yes I know the secondary slots are bottlenecked), I’m thinking of running a 5060 TI 16gb primary, 3060 rtx for offloading and my old 1070ti for offloading more. Is this a good setup? Am I completely wrong? Never built custom before&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KCCarpenter5739"&gt; /u/KCCarpenter5739 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1hyr5/building_a_local_ai_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1hyr5/building_a_local_ai_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1hyr5/building_a_local_ai_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T14:04:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1n18h8q</id>
    <title>GPT-OSS Web Search</title>
    <updated>2025-08-27T05:19:42+00:00</updated>
    <author>
      <name>/u/No-Engineering3583</name>
      <uri>https://old.reddit.com/user/No-Engineering3583</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The updates and blog posts about gpt-oss support and Ollama v0.11 mention web search support: &amp;quot;Ollama is providing a built-in web search that can be optionally enabled to augment the model with the latest information&amp;quot;&lt;/p&gt; &lt;p&gt;How is this being provided? How is it enabled/disabled? Is it only in the Ollama app or is it available when using the CLI or python libraries to access the model hosted on a local Ollama instance?&lt;/p&gt; &lt;p&gt;EDIT for clarity: I am aware there are other ways to do this, I've even coded personal solutions. My inquiry is about how a feature they semi-announced works, if it is available, and how to use it. I would like to be able to compare it against other solutions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Engineering3583"&gt; /u/No-Engineering3583 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n18h8q/gptoss_web_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n18h8q/gptoss_web_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n18h8q/gptoss_web_search/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T05:19:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1mg7v</id>
    <title>Tool calls keep ending up as responses</title>
    <updated>2025-08-27T16:53:07+00:00</updated>
    <author>
      <name>/u/thewiirocks</name>
      <uri>https://old.reddit.com/user/thewiirocks</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n1mg7v/tool_calls_keep_ending_up_as_responses/"&gt; &lt;img alt="Tool calls keep ending up as responses" src="https://preview.redd.it/rtfarn7pcllf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=228e5a9c14dd8871eb8431600e3a1762b39ccd05" title="Tool calls keep ending up as responses" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've given llama3.2 a tool to run reports using an OLAP schema. When the LLM triggers the tool call, everything works well. The problem I'm having is that the tool call is often ending up as a regular response rather than a tool call. &lt;/p&gt; &lt;p&gt;Here is the exact response text:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;model&amp;quot;: &amp;quot;llama3.2&amp;quot;, &amp;quot;created_at&amp;quot;: &amp;quot;2025-08-27T16:48:54.552815Z&amp;quot;, &amp;quot;message&amp;quot;: { &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;{\&amp;quot;name\&amp;quot;: \&amp;quot;generateReport\&amp;quot;, \&amp;quot;parameters\&amp;quot;: {\&amp;quot;arg0\&amp;quot;: \&amp;quot;[\\\&amp;quot;Franchise Name\\\&amp;quot;, \\\&amp;quot;Product Name\\\&amp;quot;]\&amp;quot;, \&amp;quot;arg1\&amp;quot;: \&amp;quot;[\\\&amp;quot;Units Sold\\\&amp;quot;, \\\&amp;quot;Total Sale \\$\\\&amp;quot;]\&amp;quot;}}&amp;quot; }, &amp;quot;done&amp;quot;: false } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is becoming a huge frustration to reliable operation. I could try and intercept these situations, but that feels like a bit of a hack. (Which I supposed describes a lot of LLM interactions. 😅)&lt;/p&gt; &lt;p&gt;Does anyone know why this is happening and how to resolve? Or do you just intercept the call yourself?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thewiirocks"&gt; /u/thewiirocks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rtfarn7pcllf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1mg7v/tool_calls_keep_ending_up_as_responses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1mg7v/tool_calls_keep_ending_up_as_responses/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T16:53:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1yfeu</id>
    <title>Questions about Agents</title>
    <updated>2025-08-28T00:50:13+00:00</updated>
    <author>
      <name>/u/Street_Equivalent_45</name>
      <uri>https://old.reddit.com/user/Street_Equivalent_45</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Fellow ai experts. &lt;/p&gt; &lt;p&gt;I am currently making agent using Ollama in local agent with langchains Because of costs😂 Is there anyways to make agent better not using chatgpt or claudes or having no coat issues? I know maybe impossible but I really know what you guys think&lt;/p&gt; &lt;p&gt;Thanks for reading my comments&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Street_Equivalent_45"&gt; /u/Street_Equivalent_45 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1yfeu/questions_about_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1yfeu/questions_about_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1yfeu/questions_about_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T00:50:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1rc2y</id>
    <title>llama.ui - minimal, privacy focused chat interface</title>
    <updated>2025-08-27T19:55:53+00:00</updated>
    <author>
      <name>/u/COBECT</name>
      <uri>https://old.reddit.com/user/COBECT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n1rc2y/llamaui_minimal_privacy_focused_chat_interface/"&gt; &lt;img alt="llama.ui - minimal, privacy focused chat interface" src="https://preview.redd.it/6g2icqwi96lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93145b5e6ac2c5f127d14e540cb4261819454a6b" title="llama.ui - minimal, privacy focused chat interface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/COBECT"&gt; /u/COBECT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6g2icqwi96lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1rc2y/llamaui_minimal_privacy_focused_chat_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1rc2y/llamaui_minimal_privacy_focused_chat_interface/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T19:55:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n25rhd</id>
    <title>Ollama help!</title>
    <updated>2025-08-28T07:27:46+00:00</updated>
    <author>
      <name>/u/IndependentBug490</name>
      <uri>https://old.reddit.com/user/IndependentBug490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n25rhd/ollama_help/"&gt; &lt;img alt="Ollama help!" src="https://b.thumbs.redditmedia.com/PwS1yOmD3a8ondnY40EzM0_SuNAdB78gHX0Oa5mdvbw.jpg" title="Ollama help!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im making a workflow in n8n using the ollama chat model, i used it as an alternative to google gemini chat model. But it keeps in erroring and the output is fetch failed. Im self host, and im only using chatgpt to help me. im a totally beginner when it comes to n8n, hopefully anyone can help me with this. thank you. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ypp0m2h1qplf1.png?width=1382&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9f53630bf8fcbcb53f4ac0f979039b75f9d1196c"&gt;https://preview.redd.it/ypp0m2h1qplf1.png?width=1382&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9f53630bf8fcbcb53f4ac0f979039b75f9d1196c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IndependentBug490"&gt; /u/IndependentBug490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n25rhd/ollama_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n25rhd/ollama_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n25rhd/ollama_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T07:27:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1n27qcn</id>
    <title>Qwen3 rbit rl finetuned for stromger reasoning</title>
    <updated>2025-08-28T09:37:04+00:00</updated>
    <author>
      <name>/u/adeelahmadch</name>
      <uri>https://old.reddit.com/user/adeelahmadch</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adeelahmadch"&gt; /u/adeelahmadch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1n27p5g/qwen3_rbit_rl_finetuned_for_stromger_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n27qcn/qwen3_rbit_rl_finetuned_for_stromger_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n27qcn/qwen3_rbit_rl_finetuned_for_stromger_reasoning/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T09:37:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1xjnb</id>
    <title>Agentic: Your 3B local model becomes a thoughtful research partner.</title>
    <updated>2025-08-28T00:09:22+00:00</updated>
    <author>
      <name>/u/Thin_Beat_9072</name>
      <uri>https://old.reddit.com/user/Thin_Beat_9072</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n1xjnb/agentic_your_3b_local_model_becomes_a_thoughtful/"&gt; &lt;img alt="Agentic: Your 3B local model becomes a thoughtful research partner." src="https://preview.redd.it/915rf37hpmlf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=7d625aa9b27cf3e56d522a230d24e40702faf46e" title="Agentic: Your 3B local model becomes a thoughtful research partner." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thin_Beat_9072"&gt; /u/Thin_Beat_9072 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/915rf37hpmlf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1xjnb/agentic_your_3b_local_model_becomes_a_thoughtful/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1xjnb/agentic_your_3b_local_model_becomes_a_thoughtful/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T00:09:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1m9fk</id>
    <title>Pair a vision grounding model with a reasoning LLM with Cua</title>
    <updated>2025-08-27T16:46:06+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n1m9fk/pair_a_vision_grounding_model_with_a_reasoning/"&gt; &lt;img alt="Pair a vision grounding model with a reasoning LLM with Cua" src="https://external-preview.redd.it/dzUycDIzdHFjbGxmMbzuqtV-zsPSC2s-Lu_18m-UGy8cX2XwaXvrFiOhDTxh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d3f7d72bb82103be882d2f660f7910185af8eb8" title="Pair a vision grounding model with a reasoning LLM with Cua" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cua just shipped v0.4 of the Cua Agent framework with Composite Agents - you can now pair a vision/grounding model with a reasoning LLM using a simple modelA+modelB syntax. Best clicks + best plans.&lt;/p&gt; &lt;p&gt;The problem: every GUI model speaks a different dialect. • some want pixel coordinates • others want percentages • a few spit out cursed tokens like &amp;lt;|loc095|&amp;gt;&lt;/p&gt; &lt;p&gt;We built a universal interface that works the same across Anthropic, OpenAI, Hugging Face, etc.:&lt;/p&gt; &lt;p&gt;agent = ComputerAgent( model=&amp;quot;anthropic/claude-3-5-sonnet-20241022&amp;quot;, tools=[computer] )&lt;/p&gt; &lt;p&gt;But here’s the fun part: you can combine models by specialization. Grounding model (sees + clicks) + Planning model (reasons + decides) →&lt;/p&gt; &lt;p&gt;agent = ComputerAgent( model=&amp;quot;huggingface-local/HelloKKMe/GTA1-7B+openai/gpt-4o&amp;quot;, tools=[computer] )&lt;/p&gt; &lt;p&gt;This gives GUI skills to models that were never built for computer use. One handles the eyes/hands, the other the brain. Think driver + navigator working together.&lt;/p&gt; &lt;p&gt;Two specialists beat one generalist. We’ve got a ready-to-run notebook demo - curious what combos you all will try.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog : &lt;a href="https://www.trycua.com/blog/composite-agents"&gt;https://www.trycua.com/blog/composite-agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7io8lg1rcllf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1m9fk/pair_a_vision_grounding_model_with_a_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1m9fk/pair_a_vision_grounding_model_with_a_reasoning/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T16:46:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2exp5</id>
    <title>Ollama having server timeout issues (on Mac), need help</title>
    <updated>2025-08-28T15:11:59+00:00</updated>
    <author>
      <name>/u/solarsflare</name>
      <uri>https://old.reddit.com/user/solarsflare</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My Ollama isn't starting, it just shows a blank screen. I checked through the terminal and it keeps server timing out. I tried an alternative way to get it to run by directly making Ollama run through the terminal, but whenever i tried to select a model and send a message, it wouldn't do anything. It'd continue to time out. I tried restarting and re-installing Ollama multiple times. I even restarted my Macbook. It's an M4 with plenty of RAM and space. Currently on the 26 beta. Does anyone know how to fix this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/solarsflare"&gt; /u/solarsflare &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2exp5/ollama_having_server_timeout_issues_on_mac_need/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2exp5/ollama_having_server_timeout_issues_on_mac_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n2exp5/ollama_having_server_timeout_issues_on_mac_need/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T15:11:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2eqc1</id>
    <title>Coquette Mobile - Android App, Ollama with Agentic Properties - desktop control.</title>
    <updated>2025-08-28T15:04:29+00:00</updated>
    <author>
      <name>/u/Fimeg</name>
      <uri>https://old.reddit.com/user/Fimeg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n2eqc1/coquette_mobile_android_app_ollama_with_agentic/"&gt; &lt;img alt="Coquette Mobile - Android App, Ollama with Agentic Properties - desktop control." src="https://b.thumbs.redditmedia.com/5JVeogvyCM6U8FWg9rpAnrZ9CgUGteXfiawa1DSs9Do.jpg" title="Coquette Mobile - Android App, Ollama with Agentic Properties - desktop control." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am quite hesitant to tell anyone about my projects for fear of having to work on them xD&lt;/p&gt; &lt;p&gt;&lt;a href="https://GitHub.com/Fimeg/CoquetteMobile"&gt;https://GitHub.com/Fimeg/CoquetteMobile&lt;/a&gt; is agentic AI on android. I built this running on a GTX 1070ti, Jan 4b model, and a Pixel 3. I would love to do this post a bit more justice but I am on mobile today. I see so many of you are creating amazing agentic desktop tools - and I want them EVERYWHERE.&lt;/p&gt; &lt;p&gt;This is just a proof of concept. I want more people working on this for FREE for us all... Fork it, give suggestions. Data Sovereignty is key for our futures.&lt;/p&gt; &lt;p&gt;Extracted from Github: ⚠️ DEVELOPMENT SOFTWARE - HIGHLY EXPERIMENTAL ⚠️&lt;/p&gt; &lt;p&gt;Your data. Your models. Your control.&lt;/p&gt; &lt;p&gt;Transform mobile AI from a black box into a transparent, user-controlled system. This isn't just another AI app - it's a demonstration of how technology should work: empowering users without extracting their data, putting complete control in your hands, and maintaining full transparency in every operation.&lt;/p&gt; &lt;p&gt;No data harvesting. No cloud dependencies. No hidden algorithms. Just pure, transparent AI assistance that you own and control. Just a privacy-first Android AI assistant built on principles of data sovereignty and technological autonomy. Features complete operational transparency - because you deserve to understand exactly how your AI assistant works.&lt;/p&gt; &lt;p&gt;🚧 Current Status: Active Development&lt;/p&gt; &lt;pre&gt;&lt;code&gt;This software is in early development and contains bugs Features may not work as expected Updates will be frequent and may introduce breaking changes Use at your own risk - not ready for production use &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;🔐 SECURITY WARNING: HID Device Control&lt;/p&gt; &lt;pre&gt;&lt;code&gt;This app can inject keyboard/mouse commands into connected computers Supports DuckyScript and similar automation protocols Can execute arbitrary commands on target systems Use only on systems you own or have explicit permission to control Malicious use is prohibited - for legitimate automation only &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fimeg"&gt; /u/Fimeg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n2eqc1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2eqc1/coquette_mobile_android_app_ollama_with_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n2eqc1/coquette_mobile_android_app_ollama_with_agentic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T15:04:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2f3mp</id>
    <title>Evaluate any computer-use agent with HUD + OSWorld-Verified</title>
    <updated>2025-08-28T15:17:50+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We integrated Cua with HUD so you can run OSWorld-Verified and other computer-/browser-use benchmarks at scale.&lt;/p&gt; &lt;p&gt;Different runners and logs made results hard to compare. Cua × HUD gives you a consistent runner, reliable traces, and comparable metrics across setups.&lt;/p&gt; &lt;p&gt;Bring your stack (OpenAI, Anthropic, Hugging Face) — or Composite Agents (grounder + planner) from Day 3. Pick the dataset and keep the same workflow.&lt;/p&gt; &lt;p&gt;See the notebook for the code: run OSWorld-Verified (~369 tasks) by XLang Labs to benchmark on real desktop apps (Chrome, LibreOffice, VS Code, GIMP).&lt;/p&gt; &lt;p&gt;Heading to Hack the North? Enter our on-site computer-use agent track — the top OSWorld-Verified score earns a guaranteed interview with a YC partner in the next batch.&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://www.trycua.com/blog/hud-agent-evals"&gt;https://www.trycua.com/blog/hud-agent-evals&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs: &lt;a href="https://docs.trycua.com/docs/agent-sdk/integrations/hud"&gt;https://docs.trycua.com/docs/agent-sdk/integrations/hud&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Notebook: &lt;a href="https://github.com/trycua/cua/blob/main/notebooks/eval_osworld.ipynb"&gt;https://github.com/trycua/cua/blob/main/notebooks/eval_osworld.ipynb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2f3mp/evaluate_any_computeruse_agent_with_hud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2f3mp/evaluate_any_computeruse_agent_with_hud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n2f3mp/evaluate_any_computeruse_agent_with_hud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T15:17:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1tfj4</id>
    <title>Ollama + PostgreSQL: Your Local LLM Can Now Query Production Databases</title>
    <updated>2025-08-27T21:17:07+00:00</updated>
    <author>
      <name>/u/Sea-Assignment6371</name>
      <uri>https://old.reddit.com/user/Sea-Assignment6371</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n1tfj4/ollama_postgresql_your_local_llm_can_now_query/"&gt; &lt;img alt="Ollama + PostgreSQL: Your Local LLM Can Now Query Production Databases" src="https://external-preview.redd.it/d2huZzF2ZHZvbWxmMblmm4Un8wPzClWvKIqbCd-O0tnIDr4HTaJ8aTVXM2nL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fdb7439b91fc4e3bf69069c2c0e5c06b049adfcd" title="Ollama + PostgreSQL: Your Local LLM Can Now Query Production Databases" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt;! Quick update - DataKit now lets you query PostgreSQL databases with Ollama's help.&lt;/p&gt; &lt;p&gt;Well the best part: Your data/schema NEVER goes to OpenAI/Claude. Your local LLM generates the SQL just by looking at the schema of the file.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What this enables:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;• &amp;quot;Show me all users who signed up last month but haven't made a purchase&amp;quot;&lt;/p&gt; &lt;p&gt;• &amp;quot;Find orders with unusual patterns&amp;quot;&lt;/p&gt; &lt;p&gt;• &amp;quot;Generate a cohort analysis query&amp;quot;&lt;/p&gt; &lt;p&gt;All happens locally. Ollama writes the SQL, DuckDB executes it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Run: `OLLAMA_ORIGINS=&amp;quot;&lt;a href="https://datakit.page"&gt;https://datakit.page&lt;/a&gt;&amp;quot; ollama serve`&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Connect your PostgreSQL&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Ask questions in plain English&lt;/p&gt; &lt;p&gt;Try it at &lt;a href="http://datakit.page"&gt;datakit.page&lt;/a&gt; - would love feedback on what models work best for SQL generation!&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Assignment6371"&gt; /u/Sea-Assignment6371 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9qyoordvomlf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1tfj4/ollama_postgresql_your_local_llm_can_now_query/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1tfj4/ollama_postgresql_your_local_llm_can_now_query/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T21:17:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1v1uw</id>
    <title>Just released version 1.4 of Nanocoder built in Ink - such an epic framework for CLI applications!</title>
    <updated>2025-08-27T22:21:00+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n1v1uw/just_released_version_14_of_nanocoder_built_in/"&gt; &lt;img alt="Just released version 1.4 of Nanocoder built in Ink - such an epic framework for CLI applications!" src="https://preview.redd.it/vkqp6dgi0nlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7c9616bc2e60e27283ed7ab441d6f9aa60448d1" title="Just released version 1.4 of Nanocoder built in Ink - such an epic framework for CLI applications!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don’t know why I didn’t build the previous versions of Nanocoder from the start in Ink, it has been so powerful in building a beautiful next-gen version of my open source coding agent.&lt;/p&gt; &lt;p&gt;It helps create some incredible UIs around the terminal and is pretty much pick up and go if you’re already fluent in React. The only challenge has been getting to the UI to scale when you resize the terminal window - any tips let me know!&lt;/p&gt; &lt;p&gt;We’re almost on 100 stars on GitHub which I know is small but I really believe in the philosophies behind this small community! It would make my day to get it there!&lt;/p&gt; &lt;p&gt;All contributors and feedback welcome - people have been so amazing already! I’m trying to get people involved to build a piece of software that is owned and pushed by the community - not big tech companies! 😄&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Link&lt;/strong&gt;: &lt;a href="https://github.com/Mote-Software/nanocoder"&gt;https://github.com/Mote-Software/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discord Link to Get Involved&lt;/strong&gt;: &lt;a href="https://discord.gg/ktPDV6rekE"&gt;https://discord.gg/ktPDV6rekE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vkqp6dgi0nlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1v1uw/just_released_version_14_of_nanocoder_built_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1v1uw/just_released_version_14_of_nanocoder_built_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T22:21:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2f514</id>
    <title>Private AI by Proton</title>
    <updated>2025-08-28T15:19:18+00:00</updated>
    <author>
      <name>/u/naperwind</name>
      <uri>https://old.reddit.com/user/naperwind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n2f514/private_ai_by_proton/"&gt; &lt;img alt="Private AI by Proton" src="https://external-preview.redd.it/qwgh9izd63uNNKC7KnYNGMWh58yq5A1iL3wjdvP3PgM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2cd0508e37d8af974808c811adb21f6ed8bbd68c" title="Private AI by Proton" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anybody tried? Can it be put on Ollama? Thank you in advance for your thoughts. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/naperwind"&gt; /u/naperwind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://proton.me/blog/lumo-1-1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2f514/private_ai_by_proton/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n2f514/private_ai_by_proton/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T15:19:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3hatn</id>
    <title>My code isn't working for my discord bot. Any help?</title>
    <updated>2025-08-29T19:47:31+00:00</updated>
    <author>
      <name>/u/NickValent710</name>
      <uri>https://old.reddit.com/user/NickValent710</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;// index.js&lt;/p&gt; &lt;p&gt;import 'dotenv/config';&lt;/p&gt; &lt;p&gt;import { Client, GatewayIntentBits, Partials, PermissionsBitField } from 'discord.js';&lt;/p&gt; &lt;p&gt;import ollama from 'ollama';&lt;/p&gt; &lt;p&gt;// ----- CONFIG -----&lt;/p&gt; &lt;p&gt;const SPAM_WINDOW_MS = 10000;&lt;/p&gt; &lt;p&gt;const SPAM_MAX_MSGS = 6;&lt;/p&gt; &lt;p&gt;const SPAM_TIMEOUT_MS = 10 * 60_000;&lt;/p&gt; &lt;p&gt;const HATE_TIMEOUT_MS = 60 * 60_000;&lt;/p&gt; &lt;p&gt;const AI_COOLDOWN_MS = 5000;&lt;/p&gt; &lt;p&gt;const AI_RETRY_INTERVAL = 5000; // Retry every 5s if model is offline&lt;/p&gt; &lt;p&gt;const userMsgTimes = new Map();&lt;/p&gt; &lt;p&gt;const userWarnings = new Map();&lt;/p&gt; &lt;p&gt;const userAICooldown = new Map();&lt;/p&gt; &lt;p&gt;const BLOCKLIST = ['nigga','Nigga','nigger','Nigger','cunt','Cunt','sperm','Sperm'];&lt;/p&gt; &lt;p&gt;// ----- CREATE CLIENT -----&lt;/p&gt; &lt;p&gt;const client = new Client({&lt;/p&gt; &lt;p&gt;intents: [&lt;/p&gt; &lt;p&gt;GatewayIntentBits.Guilds,&lt;/p&gt; &lt;p&gt;GatewayIntentBits.GuildMessages,&lt;/p&gt; &lt;p&gt;GatewayIntentBits.MessageContent,&lt;/p&gt; &lt;p&gt;GatewayIntentBits.GuildMembers&lt;/p&gt; &lt;p&gt;],&lt;/p&gt; &lt;p&gt;partials: [Partials.Channel, Partials.Message, Partials.User]&lt;/p&gt; &lt;p&gt;});&lt;/p&gt; &lt;p&gt;// ----- HELPERS -----&lt;/p&gt; &lt;p&gt;function normalizeForMatch(str) {&lt;/p&gt; &lt;p&gt;const map = { '0':'o','1':'i','3':'e','4':'a','5':'s','7':'t','@':'a','$':'s' };&lt;/p&gt; &lt;p&gt;return str&lt;/p&gt; &lt;p&gt;.normalize('NFKD').replace(/\p{Diacritic}/gu,'')&lt;/p&gt; &lt;p&gt;.toLowerCase()&lt;/p&gt; &lt;p&gt;.replace(/[0|1|3|4|5|7@\$]/g,ch =&amp;gt; map[ch] ?? ch)&lt;/p&gt; &lt;p&gt;.replace(/[^a-z0-9]/g,'');&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;function containsBannedTerm(content) {&lt;/p&gt; &lt;p&gt;const norm = normalizeForMatch(content);&lt;/p&gt; &lt;p&gt;return BLOCKLIST.some(term =&amp;gt; norm.includes(normalizeForMatch(term)));&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;function bumpWarning(userId, kind) {&lt;/p&gt; &lt;p&gt;const entry = userWarnings.get(userId) ?? { spam:0, hate:0 };&lt;/p&gt; &lt;p&gt;entry[kind] = (entry[kind] ?? 0) + 1;&lt;/p&gt; &lt;p&gt;userWarnings.set(userId, entry);&lt;/p&gt; &lt;p&gt;return entry[kind];&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;async function warnUser(message, text) {&lt;/p&gt; &lt;p&gt;try { await message.reply({ content: text }); } catch (err) { logError(`warnUser error: ${err.message}`); }&lt;/p&gt; &lt;p&gt;try { await message.member?.send?.(`Heads up: ${text}\nServer: ${message.guild?.name}`); } catch {}&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;async function timeoutMember(member, ms, reason) {&lt;/p&gt; &lt;p&gt;try { await member.timeout(ms, reason); return true; }&lt;/p&gt; &lt;p&gt;catch (err) { logError(`timeoutMember error: ${err.message}`); return false; }&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;async function warnMods(message, reason) {&lt;/p&gt; &lt;p&gt;const modChannelId = '1390751780228436084';&lt;/p&gt; &lt;p&gt;const channel = message.guild.channels.cache.get(modChannelId);&lt;/p&gt; &lt;p&gt;if (!channel) return logError('Mod channel not found!');&lt;/p&gt; &lt;p&gt;try { await channel.send(`⚠️ Moderator Alert: ${reason}\nUser: &amp;lt;@${message.author.id}&amp;gt;`); }&lt;/p&gt; &lt;p&gt;catch (err) { logError(`Failed to notify mods: ${err.message}`); }&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;async function logError(text) {&lt;/p&gt; &lt;p&gt;const logChannelId = '1390751780228436082';&lt;/p&gt; &lt;p&gt;const channel = client.channels.cache.get(logChannelId);&lt;/p&gt; &lt;p&gt;if (!channel) return console.error('Error log channel not found!');&lt;/p&gt; &lt;p&gt;try { await channel.send(`🛑 Bot Error: ${text}`); } catch (err) { console.error(err); }&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;// ----- AI HELPER: check if model is online -----&lt;/p&gt; &lt;p&gt;async function generateAI(prompt) {&lt;/p&gt; &lt;p&gt;try {&lt;/p&gt; &lt;p&gt;const response = await ollama.generate({&lt;/p&gt; &lt;p&gt;model: 'llama3:8b',&lt;/p&gt; &lt;p&gt;prompt,&lt;/p&gt; &lt;p&gt;baseURL: '&lt;a href="http://127.0.0.1:11435"&gt;http://127.0.0.1:11435&lt;/a&gt;'&lt;/p&gt; &lt;p&gt;});&lt;/p&gt; &lt;p&gt;return response.generations?.[0]?.text || null;&lt;/p&gt; &lt;p&gt;} catch {&lt;/p&gt; &lt;p&gt;return null; // Model offline or not responding&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;// ----- EVENTS -----&lt;/p&gt; &lt;p&gt;client.on('ready', () =&amp;gt; console.log(`Logged in as ${client.user.tag}`));&lt;/p&gt; &lt;p&gt;client.on('messageCreate', async (message) =&amp;gt; {&lt;/p&gt; &lt;p&gt;try {&lt;/p&gt; &lt;p&gt;if (!message.guild || message.author.bot) return;&lt;/p&gt; &lt;p&gt;const now = Date.now();&lt;/p&gt; &lt;p&gt;// ----- HATE SPEECH -----&lt;/p&gt; &lt;p&gt;if (containsBannedTerm(message.content)) {&lt;/p&gt; &lt;p&gt;if (message.guild.members.me?.permissions.has(PermissionsBitField.Flags.ManageMessages)) {&lt;/p&gt; &lt;p&gt;try { await message.delete(); } catch {}&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;const count = bumpWarning(message.author.id,'hate');&lt;/p&gt; &lt;p&gt;if(count===1) await warnUser(message,&amp;quot;That word is not allowed here. First warning.&amp;quot;);&lt;/p&gt; &lt;p&gt;else if(count===2) { await warnUser(message,&amp;quot;Second violation. Timing out user.&amp;quot;); await timeoutMember(message.member,HATE_TIMEOUT_MS,'Hate speech'); }&lt;/p&gt; &lt;p&gt;else { await timeoutMember(message.member,HATE_TIMEOUT_MS,'Repeated hate speech'); await warnMods(message,&amp;quot;Repeated hate speech detected&amp;quot;); }&lt;/p&gt; &lt;p&gt;return;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;// ----- SPAM -----&lt;/p&gt; &lt;p&gt;const times = userMsgTimes.get(message.author.id) ?? [];&lt;/p&gt; &lt;p&gt;times.push(now);&lt;/p&gt; &lt;p&gt;const recent = times.filter(t =&amp;gt; now-t &amp;lt;= SPAM_WINDOW_MS);&lt;/p&gt; &lt;p&gt;userMsgTimes.set(message.author.id,recent);&lt;/p&gt; &lt;p&gt;if(recent.length&amp;gt;SPAM_MAX_MSGS){&lt;/p&gt; &lt;p&gt;const count=bumpWarning(message.author.id,'spam');&lt;/p&gt; &lt;p&gt;if(count===1) await warnUser(message,`You're sending messages too quickly (warning ${count}).`);&lt;/p&gt; &lt;p&gt;else { await warnUser(message,&amp;quot;Spam detected again. Timing out user.&amp;quot;); await timeoutMember(message.member,SPAM_TIMEOUT_MS,'Spam'); }&lt;/p&gt; &lt;p&gt;return;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;// ----- AI RESPONSE -----&lt;/p&gt; &lt;p&gt;const lastAI = userAICooldown.get(message.author.id) ?? 0;&lt;/p&gt; &lt;p&gt;if(now - lastAI &amp;gt;= AI_COOLDOWN_MS){&lt;/p&gt; &lt;p&gt;let aiText = await generateAI(message.content);&lt;/p&gt; &lt;p&gt;// Retry after interval if offline&lt;/p&gt; &lt;p&gt;if(!aiText){&lt;/p&gt; &lt;p&gt;setTimeout(async ()=&amp;gt;{&lt;/p&gt; &lt;p&gt;aiText = await generateAI(message.content);&lt;/p&gt; &lt;p&gt;if(aiText){&lt;/p&gt; &lt;p&gt;await message.channel.send(aiText);&lt;/p&gt; &lt;p&gt;userAICooldown.set(message.author.id,Date.now());&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;},AI_RETRY_INTERVAL);&lt;/p&gt; &lt;p&gt;await message.channel.send('🤖 AI is currently offline. Retrying in 5 seconds...');&lt;/p&gt; &lt;p&gt;await logError(`Ollama AI offline for message: &amp;quot;${message.content}&amp;quot;`);&lt;/p&gt; &lt;p&gt;return;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;if(aiText){&lt;/p&gt; &lt;p&gt;await message.channel.send(aiText);&lt;/p&gt; &lt;p&gt;userAICooldown.set(message.author.id, now);&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;} catch(err){&lt;/p&gt; &lt;p&gt;console.error('messageCreate error:',err);&lt;/p&gt; &lt;p&gt;await logError(`messageCreate error: ${err.message}\n${err.stack}`);&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;});&lt;/p&gt; &lt;p&gt;// ----- LOGIN -----&lt;/p&gt; &lt;p&gt;client.login(process.env.TOKEN);&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NickValent710"&gt; /u/NickValent710 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n3hatn/my_code_isnt_working_for_my_discord_bot_any_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n3hatn/my_code_isnt_working_for_my_discord_bot_any_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n3hatn/my_code_isnt_working_for_my_discord_bot_any_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-29T19:47:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2c5mw</id>
    <title>I’ve Debugged 100+ RAG/LLM Pipelines. These 16 Bugs Always Come Back. (70 days, 800 stars)</title>
    <updated>2025-08-28T13:25:01+00:00</updated>
    <author>
      <name>/u/onestardao</name>
      <uri>https://old.reddit.com/user/onestardao</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i used to think RAG was mostly “pick better embeddings, tune chunk size, choose a faster vector db.” then production happened.&lt;/p&gt; &lt;p&gt;what i thought&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;switch cosine to dot, increase chunk length, rerun.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;try another vector store and RPS goes up, so answers should improve.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;hybrid retrieval must be strictly better than a single retriever.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;what really happened&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;high similarity with wrong meaning. facts exist in the corpus but never surface.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;answers look right while citations silently drift to the wrong section.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;first call after deploy fails because secrets are not ready.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;hybrid sometimes performs worse than a single strong retriever with a clean contract.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;after 100+ pipelines across ollama stacks, the same patterns kept returning. none of this was random. they were structural failure modes. so i wrote them down as a Problem Map with 16 reproducible slots, each with a permanent fix. examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;No.5 embedding ≠ semantic. high similarity, wrong meaning.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No.8 retrieval traceability. answer looks fine, citations do not align to the exact offsets.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No.14 bootstrap ordering. first call after deploy crashes or uses stale env because infra is not warmed.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No.15 deployment deadlock. retriever or merge waits forever on an index that is still building.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;i shared the map and the community response was surprisingly strong. 70 days, 800 stars, and even the tesseract.js author starred it. more important than stars though, the map made bugs stop repeating. once a slot is fixed structurally, it stays fixed.&lt;/p&gt; &lt;p&gt;👉 Problem Map, 16 failure modes with fixes (link above)&lt;/p&gt; &lt;h2&gt;a concrete ollama workflow you can try in 60 seconds&lt;/h2&gt; &lt;p&gt;open a fresh ollama chat with your model. paste this diagnostic prompt as is:&lt;/p&gt; &lt;p&gt;You are a RAG pipeline auditor. Classify the current failure into the Problem Map slots (No.5 embedding≠semantic, No.8 retrieval traceability, No.14 bootstrap ordering, No.15 deployment deadlock, or other). Return a short JSON plan with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;slot&amp;quot;: &amp;quot;No.x&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;why&amp;quot;: one-sentence symptom match&lt;/li&gt; &lt;li&gt;&amp;quot;checks&amp;quot;: ordered steps I can run now&lt;/li&gt; &lt;li&gt;&amp;quot;fix&amp;quot;: the minimal structural change&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Rules:&lt;/p&gt; &lt;p&gt;1) enforce cite-then-explain. if citations or offsets are missing, fail fast and say &amp;quot;add traceability contract&amp;quot;.&lt;/p&gt; &lt;p&gt;2) if coverage &amp;lt; 0.70 or alignment is inconsistent across 3 paraphrases, flag &amp;quot;needs retriever repair&amp;quot;.&lt;/p&gt; &lt;p&gt;3) do not change my infra. propose guardrails I can add at the text and contract layer.&lt;/p&gt; &lt;p&gt;Keep it terse and auditable.&lt;/p&gt; &lt;p&gt;now ask your real question, or paste a failing trace. the model should classify into one of the slots and return a tiny, checkable plan.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;minimal guardrails you can add today&lt;/h2&gt; &lt;p&gt;acceptance targets&lt;/p&gt; &lt;ul&gt; &lt;li&gt;coverage for the target section ≥ 0.70&lt;/li&gt; &lt;li&gt;enforce cite then explain&lt;/li&gt; &lt;li&gt;stop on missing fields: snippet_id, section_id, source_url, offsets, tokens&lt;/li&gt; &lt;li&gt;flag instability if the answer flips across 3 paraphrases with identical inputs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;bootstrap fence&lt;/p&gt; &lt;ul&gt; &lt;li&gt;before any retrieval or generation, assert env and secrets are present. if not, short circuit with a wait and a capped retry counter. this prevents No.14.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;traceability contract&lt;/p&gt; &lt;ul&gt; &lt;li&gt;require snippet level ids and offsets. reject answers that cannot point back to the exact span. this prevents No.8 from hiding for weeks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;retriever sanity&lt;/p&gt; &lt;ul&gt; &lt;li&gt;verify the analyzer and normalization used to write the index matches the one used in retrieval. a mismatch often masquerades as No.5.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;single writer&lt;/p&gt; &lt;ul&gt; &lt;li&gt;queue or mutex all index writes. many “random” 500s are actually No.15 race conditions.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;why this matters to ollama users&lt;/h2&gt; &lt;p&gt;ollama gives you control and speed. the failure modes above sneak in precisely when you move fast. if you keep a short checklist and a common language for the bugs, you do not waste cycles arguing about tools. you fix the structure once, and it stays fixed.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;global fix map, work in progress&lt;/h2&gt; &lt;p&gt;problem map 1.0 is the foundation. i am now drafting a global fix map that spans ollama, langchain, llamaindex, qdrant, weaviate, milvus, and common automation stacks. same idea, one level broader. minimal recipes, clean guardrails, no infra rewrites.&lt;/p&gt; &lt;p&gt;what would you want included besides “common bugs + fixes”?&lt;/p&gt; &lt;p&gt;metrics you actually check, copy paste recipes, deployment checklists, or something else you wish you had before prod went live? ( will be launched soon) &lt;/p&gt; &lt;p&gt;🫡 Thank you in advance &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onestardao"&gt; /u/onestardao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/onestardao/WFGY/tree/main/ProblemMap/README.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2c5mw/ive_debugged_100_ragllm_pipelines_these_16_bugs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n2c5mw/ive_debugged_100_ragllm_pipelines_these_16_bugs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T13:25:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ugfs</id>
    <title>Spring AI Playground — Self-hosted Web UI with Ollama, RAG and MCP tools</title>
    <updated>2025-08-29T01:35:22+00:00</updated>
    <author>
      <name>/u/kr-jmlab</name>
      <uri>https://old.reddit.com/user/kr-jmlab</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n2ugfs/spring_ai_playground_selfhosted_web_ui_with/"&gt; &lt;img alt="Spring AI Playground — Self-hosted Web UI with Ollama, RAG and MCP tools" src="https://external-preview.redd.it/Tdp9Yr11guCxzcug45e2Jg_hVSbeQUV2eb5zstdOZ5Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cbdafe1ed4b7e420dcb8f2e46a256d6f18757dd0" title="Spring AI Playground — Self-hosted Web UI with Ollama, RAG and MCP tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built &lt;strong&gt;Spring AI Playground&lt;/strong&gt;, a self-hosted web UI for experimenting with &lt;strong&gt;Ollama, RAG workflows, and MCP tools&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Uses &lt;strong&gt;Ollama as the default backend&lt;/strong&gt; — no API keys needed&lt;/li&gt; &lt;li&gt;Upload docs → chunk → embed → run similarity search with vector DBs (Pinecone, Milvus, PGVector, Weaviate, etc.)&lt;/li&gt; &lt;li&gt;Visual &lt;strong&gt;MCP Playground&lt;/strong&gt;: connect tools via HTTP/STDIO/SSE, inspect metadata, tweak args, and call them from chat&lt;/li&gt; &lt;li&gt;Can also swap to OpenAI, Anthropic, Google, etc. if you want&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I built this because wiring up RAG + tool integrations in Java always felt slow and repetitive. Now I can spin things up quickly in a browser UI, fully local.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/JM-Lab/spring-ai-playground"&gt;https://github.com/JM-Lab/spring-ai-playground&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear how this community is using Ollama for RAG today, and what features you’d like to see added.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kr-jmlab"&gt; /u/kr-jmlab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/JM-Lab/spring-ai-playground"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2ugfs/spring_ai_playground_selfhosted_web_ui_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n2ugfs/spring_ai_playground_selfhosted_web_ui_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-29T01:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n318fr</id>
    <title>🚀 Built semantic related posts for my Astro blog using local Ollama embeddings</title>
    <updated>2025-08-29T07:50:43+00:00</updated>
    <author>
      <name>/u/ComplexScary8689</name>
      <uri>https://old.reddit.com/user/ComplexScary8689</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComplexScary8689"&gt; /u/ComplexScary8689 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/astrojs/comments/1n313p6/built_semantic_related_posts_for_my_astro_blog/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n318fr/built_semantic_related_posts_for_my_astro_blog/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n318fr/built_semantic_related_posts_for_my_astro_blog/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-29T07:50:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2omfl</id>
    <title>[Guide + Code] Fine-Tuning a Vision-Language Model on a Single GPU (Yes, With Code)</title>
    <updated>2025-08-28T21:19:23+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n2omfl/guide_code_finetuning_a_visionlanguage_model_on_a/"&gt; &lt;img alt="[Guide + Code] Fine-Tuning a Vision-Language Model on a Single GPU (Yes, With Code)" src="https://preview.redd.it/086kei6dutlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da7b7c3f635056e44d3dedb5a3e573006bf0f4bb" title="[Guide + Code] Fine-Tuning a Vision-Language Model on a Single GPU (Yes, With Code)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote a step-by-step guide (with code) on how to fine-tune SmolVLM-256M-Instruct using Hugging Face TRL + PEFT. It covers lazy dataset streaming (no OOM), LoRA/DoRA explained simply, ChartQA for verifiable evaluation, and how to deploy via vLLM. Runs fine on a single consumer GPU like a 3060/4070.&lt;/p&gt; &lt;p&gt;Guide: &lt;a href="https://pavankunchalapk.medium.com/the-definitive-guide-to-fine-tuning-a-vision-language-model-on-a-single-gpu-with-code-79f7aa914fc6?utm_source=chatgpt.com"&gt;https://pavankunchalapk.medium.com/the-definitive-guide-to-fine-tuning-a-vision-language-model-on-a-single-gpu-with-code-79f7aa914fc6&lt;/a&gt;&lt;br /&gt; Code: &lt;a href="https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/vllm-fine-tuning-smolvlm?utm_source=chatgpt.com"&gt;https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/vllm-fine-tuning-smolvlm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also — I’m open to roles! Hands-on with real-time pose estimation, LLMs, and deep learning architectures. Resume: &lt;a href="https://pavan-portfolio-tawny.vercel.app/"&gt;https://pavan-portfolio-tawny.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/086kei6dutlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2omfl/guide_code_finetuning_a_visionlanguage_model_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n2omfl/guide_code_finetuning_a_visionlanguage_model_on_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T21:19:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2pspb</id>
    <title>ollama-lancache (like caching games for a lan party but models instead!)</title>
    <updated>2025-08-28T22:07:13+00:00</updated>
    <author>
      <name>/u/jjasghar</name>
      <uri>https://old.reddit.com/user/jjasghar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I'd like to announce I created &lt;code&gt;ollama-lancache&lt;/code&gt;. Basically, it's a way to share out the &amp;quot;blobs&amp;quot; for ollama models from one (laptop) computer to a bunch of attendee machines. &lt;/p&gt; &lt;p&gt;So, if you have a say conference with Wi-Fi, and it takes hours to download your models, you can use this app that sits beside your already downloaded models and will install them to the correct location for Windows/Mac/Linux. &lt;/p&gt; &lt;p&gt;There's even a &amp;quot;downloads&amp;quot; directory, so you can have specific versions of Ollama or any additional downloads for leveraging models.&lt;/p&gt; &lt;p&gt;Conference wifi has always been a problem. This is a small Go application that leverages something already on your laptop, and ideally will allow you to get your attendees to leverage your tech sooner rather than later.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jjasghar/ollama-lancache"&gt;https://github.com/jjasghar/ollama-lancache&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jjasghar"&gt; /u/jjasghar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2pspb/ollamalancache_like_caching_games_for_a_lan_party/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2pspb/ollamalancache_like_caching_games_for_a_lan_party/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n2pspb/ollamalancache_like_caching_games_for_a_lan_party/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T22:07:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1n302ll</id>
    <title>Which model to choose for content generation?</title>
    <updated>2025-08-29T06:35:41+00:00</updated>
    <author>
      <name>/u/fullstackdev-channel</name>
      <uri>https://old.reddit.com/user/fullstackdev-channel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, seeking advice on which model to choose for my clients website, for content creation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fullstackdev-channel"&gt; /u/fullstackdev-channel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n302ll/which_model_to_choose_for_content_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n302ll/which_model_to_choose_for_content_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n302ll/which_model_to_choose_for_content_generation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-29T06:35:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3hhlt</id>
    <title>bsod - new build - questions</title>
    <updated>2025-08-29T19:54:58+00:00</updated>
    <author>
      <name>/u/p1kn1t</name>
      <uri>https://old.reddit.com/user/p1kn1t</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently built a new machine (specs here - &lt;a href="https://www.reddit.com/r/PcBuild/comments/1mgmf0r/first_build_in_a_long_time/"&gt;https://www.reddit.com/r/PcBuild/comments/1mgmf0r/first_build_in_a_long_time/&lt;/a&gt; )&lt;/p&gt; &lt;p&gt;I have ollama version is 0.11.8 loaded&lt;/p&gt; &lt;p&gt;I finally got around to loading some llms and benchmarking last night. I started small and worked my way up downloading the following models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;tinyllama:latest~637 MB&lt;/li&gt; &lt;li&gt;gemma:2b~1.7 GB&lt;/li&gt; &lt;li&gt;mistral (base)~4.0 GB&lt;/li&gt; &lt;li&gt;mistral:instruct~4.1 GB&lt;/li&gt; &lt;li&gt;gemma2:9b~5.4 GB&lt;/li&gt; &lt;li&gt;mistral-nemo:12b~7.1 GB&lt;/li&gt; &lt;li&gt;nous-hermes:13b~7.4 GB&lt;/li&gt; &lt;li&gt;llama2:7b-q4_0 ≈3–4 GB for q4 quant&lt;/li&gt; &lt;li&gt;llama3:8b ≈ 4–5 GB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The first two loaded fine, I did some quick powershell tests and then loaded the next. When I got to mistral as soon as it started I received a bsod (OS windows server). The errors were always same same but different. Here are the two that would occur:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The computer has rebooted from a bugcheck. The bugcheck was: 0x00000139 (0x0000000000000003, 0xffffe4037cc3ee50, 0xffffe4037cc3eda8, 0x0000000000000000). &lt;ul&gt; &lt;li&gt;This was the first. I downloaded ddu cleared the drivers for the gpu and downloaded the latest divers from nvidia - just happened to have had anew driver from this week so that was timely&lt;/li&gt; &lt;li&gt;This was good for a bit and then i got.....&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;The computer has rebooted from a bugcheck. The bugcheck was: 0x00000139 (0x0000000000000003, 0xfffff80334179840, 0xfffff80334179798, 0x0000000000000000). i ran the following to see if I could clear things up make sure drives were good etc &lt;ul&gt; &lt;li&gt;sfc /scannow&lt;/li&gt; &lt;li&gt;DISM /Online /Cleanup-Image /RestoreHealth&lt;/li&gt; &lt;li&gt;deleted a previous windows installation windows.old - i loaded 11 pro initially when i built the machine and then moved to server&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;here are a few questions, thanks in advance for any thoughts etc:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Since this is a new build I want to make sure that this is more of a large file issue and not something that is a sign of things to come&lt;/li&gt; &lt;li&gt;I am assuming there could be driver issues as I am using windows server and the drivers for the gpu are windows 11&lt;/li&gt; &lt;li&gt;I am assuming it was some sort of large file random bitfly that caused the errors and that it will not impact my models when I run but wanted to see if anyone else &lt;ul&gt; &lt;li&gt;Had the same issue on just the download of the large models&lt;/li&gt; &lt;li&gt;Ran clean other wise&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;During benchmarking on the large files I ran concurrency tests with the following settings and did not have any bsod issues &lt;ul&gt; &lt;li&gt;$concurrencyLevels = @(0,1,2,4,6,8,19,12,14,16) - 19 was a fat finger and did not notice until I had run two benchmarks so I kept it in there for all benchmarking to be consistent&lt;/li&gt; &lt;li&gt;$numPredict = 800&lt;/li&gt; &lt;li&gt;$numCtx = 4096&lt;/li&gt; &lt;li&gt;$numBatch = 512&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am happy to share the benchmarking if any one is interested. Mistral 7B Instruct (v0.2):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Achieved the highest peak tokens/sec at its best concurrency. &lt;/li&gt; &lt;li&gt;Sustained speed: Mistral 7B Instruct (v0.2) delivered the best average tokens/sec across the full sweep&lt;/li&gt; &lt;li&gt;Efficiency leader: Mistral 7B Instruct (v0.2) posted the highest tokens-per-watt at its max tested concurrency&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Lots of details but appreciate you making it through the thread. thanks,&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/p1kn1t"&gt; /u/p1kn1t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n3hhlt/bsod_new_build_questions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n3hhlt/bsod_new_build_questions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n3hhlt/bsod_new_build_questions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-29T19:54:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3bafx</id>
    <title>Human in the Loop for computer use agents</title>
    <updated>2025-08-29T15:57:53+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n3bafx/human_in_the_loop_for_computer_use_agents/"&gt; &lt;img alt="Human in the Loop for computer use agents" src="https://external-preview.redd.it/ZGY1MTQ4aHlkemxmMUvZR3OLyxwRXjlbiYrrBtxMf6dd5k6Y_Z_HAekaHP-j.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a80d11cc3917442a0c0468d6f78156d754b4266c" title="Human in the Loop for computer use agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sometimes the best “agent” is you.&lt;/p&gt; &lt;p&gt;We’re introducing Human-in-the-Loop: instantly hand off from automation to human control when a task needs judgment. &lt;/p&gt; &lt;p&gt;Yesterday we shared our HUD evals for measuring agents at scale. Today, you can become the agent when it matters - take over the same session, see what the agent sees, and keep the workflow moving.&lt;/p&gt; &lt;p&gt;Lets you create clean training demos, establish ground truth for tricky cases, intervene on edge cases ( CAPTCHAs, ambiguous UIs) or step through debug withut context switching.&lt;/p&gt; &lt;p&gt;You have full human control when you want.We even a fallback version where in it starts automated but escalate to a human only when needed.&lt;/p&gt; &lt;p&gt;Works across common stacks (OpenAI, Anthropic, Hugging Face) and with our Composite Agents. Same tools, same environment - take control when needed.&lt;/p&gt; &lt;p&gt;Feedback welcome - curious how you’d use this in your workflows.&lt;/p&gt; &lt;p&gt;Blog : &lt;a href="https://www.trycua.com/blog/human-in-the-loop.md"&gt;https://www.trycua.com/blog/human-in-the-loop.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zi8focpydzlf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n3bafx/human_in_the_loop_for_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n3bafx/human_in_the_loop_for_computer_use_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-29T15:57:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3yxzu</id>
    <title>How to disable thinking mode for qwen3 in Ollama desktup UI?</title>
    <updated>2025-08-30T11:11:05+00:00</updated>
    <author>
      <name>/u/mohnos</name>
      <uri>https://old.reddit.com/user/mohnos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can disable thinking mode when i run it in terminal like this:&lt;br /&gt; &lt;code&gt;ollama run qwen3:1.7b --think=false&lt;/code&gt;&lt;/p&gt; &lt;p&gt;or like this:&lt;br /&gt; &lt;code&gt;ollama run qwen3:1.7b&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; /set nothink&lt;/code&gt;&lt;/p&gt; &lt;p&gt;But nothing works in the new Ollama desktop UI. Can you help me?&lt;/p&gt; &lt;p&gt;EDIT: sorry for the typo in the title.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mohnos"&gt; /u/mohnos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n3yxzu/how_to_disable_thinking_mode_for_qwen3_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n3yxzu/how_to_disable_thinking_mode_for_qwen3_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n3yxzu/how_to_disable_thinking_mode_for_qwen3_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-30T11:11:05+00:00</published>
  </entry>
</feed>
