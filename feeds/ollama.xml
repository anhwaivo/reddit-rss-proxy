<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-13T02:26:05+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mn8jot</id>
    <title>Coding model for 4080S + 32Gb RAM</title>
    <updated>2025-08-11T09:41:01+00:00</updated>
    <author>
      <name>/u/tresslessone</name>
      <uri>https://old.reddit.com/user/tresslessone</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Title says most of it - I’m looking for some coding model recommendations for my 4080S (16Gb VRAM) + 32Gb RAM (7800X3D) gaming PC.&lt;/p&gt; &lt;p&gt;I’m currently running QWEN3-coder 30b (Q4_K_XL) and whilst it runs, it’s pretty slow (especially once the context fills up) and I’d like something a bit snappier. &lt;/p&gt; &lt;p&gt;Is there a 14b version of QWEN3-coder out there perhaps that I can’t seem to find?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tresslessone"&gt; /u/tresslessone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn8jot/coding_model_for_4080s_32gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn8jot/coding_model_for_4080s_32gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mn8jot/coding_model_for_4080s_32gb_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T09:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmwz6c</id>
    <title>People with MacBook Pro with 36gb of memory, which models you are running for coding?</title>
    <updated>2025-08-10T23:06:39+00:00</updated>
    <author>
      <name>/u/Sea-Emu2600</name>
      <uri>https://old.reddit.com/user/Sea-Emu2600</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have m3 max 36gb of memory but kinda new to ollama so not sure which model use for coding. Also you are using what as a front-end? Vscode Copilot, cline?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Emu2600"&gt; /u/Sea-Emu2600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmwz6c/people_with_macbook_pro_with_36gb_of_memory_which/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmwz6c/people_with_macbook_pro_with_36gb_of_memory_which/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmwz6c/people_with_macbook_pro_with_36gb_of_memory_which/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T23:06:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmu24w</id>
    <title>What is the Best coding LLM for my system?</title>
    <updated>2025-08-10T21:02:54+00:00</updated>
    <author>
      <name>/u/tarsonis125</name>
      <uri>https://old.reddit.com/user/tarsonis125</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;rtx 3090 24vram + 96gig ram&lt;/p&gt; &lt;p&gt;What is the best local LLM to use on my system?&lt;br /&gt; Do some models do better then other at some tasks?&lt;br /&gt; I am trying out a bunch of them, but its hard for me to properly rate them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarsonis125"&gt; /u/tarsonis125 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmu24w/what_is_the_best_coding_llm_for_my_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmu24w/what_is_the_best_coding_llm_for_my_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmu24w/what_is_the_best_coding_llm_for_my_system/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T21:02:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnt14e</id>
    <title>dose any one use hostinger</title>
    <updated>2025-08-11T23:40:32+00:00</updated>
    <author>
      <name>/u/wbiggs205</name>
      <uri>https://old.reddit.com/user/wbiggs205</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to know dose any one use hostinger on there VPS to run ollama. And What do you think ? I was thing about using them .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wbiggs205"&gt; /u/wbiggs205 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnt14e/dose_any_one_use_hostinger/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnt14e/dose_any_one_use_hostinger/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mnt14e/dose_any_one_use_hostinger/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T23:40:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn1v6v</id>
    <title>Integrated Mistral AI into a vehicle</title>
    <updated>2025-08-11T02:58:38+00:00</updated>
    <author>
      <name>/u/SoftDuckling1</name>
      <uri>https://old.reddit.com/user/SoftDuckling1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently working on building an AI to install into my vehicle as an assistant, NOT a driver. The main purpose is to have an assistant that can tell me if something is wrong with the vehicle, provide casual chat on long drives, provide directions (if i add offline GPS/maps), and make driving overall more enjoyable, all while keeping my safety a top priority. I'm using Mistral AI (local) and I have a RAG memory script somewhat working, still working out some kinks. It's using PIPER as a TTS, and im currently working on STT. I eventually want to try to integrate a dual facing dash cam with facial recognition and record drives with loop recording, like a normal dash cam. It will have access to my vehicles speakers, mic, radio display, and possibly OBD-II. That last part still makes me worry, which is why I'm looking for any advice. I already gave it explicit directives and prompts telling it to never alter anything, only read data, but I still don't fully trust it. I'll be adding failsafes before i install it. I plan to install this AI onto a Jetson Orin Nano Super DEV. (at least that's what I plan on right now, might change later). I'll give it a 1-2TB extreme SD to store &amp;quot;.json&amp;quot; memories that it will be able to use as an index library, retrieving only relative information. Then it will be installed into the glove box. I will renovate the glove box with a vibration-proof housings, heat sink, proper cooling, ceramic heaters for colder months, air filters, and anything else I can think of.&lt;/p&gt; &lt;p&gt;So any thoughts? Fully offline AI as a vehicle companion, an okay idea or a ticking bomb? I'll gladly accept any advice about this project. And again, it WILL NOT be controlling any part of the vehicle, only reading data while providing conversation and info.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SoftDuckling1"&gt; /u/SoftDuckling1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn1v6v/integrated_mistral_ai_into_a_vehicle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn1v6v/integrated_mistral_ai_into_a_vehicle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mn1v6v/integrated_mistral_ai_into_a_vehicle/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T02:58:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnyfdt</id>
    <title>Is Ollama Stealing Llama.cpp’s Work?</title>
    <updated>2025-08-12T03:50:30+00:00</updated>
    <author>
      <name>/u/Lopsided_Dot_4557</name>
      <uri>https://old.reddit.com/user/Lopsided_Dot_4557</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mnyfdt/is_ollama_stealing_llamacpps_work/"&gt; &lt;img alt="Is Ollama Stealing Llama.cpp’s Work?" src="https://external-preview.redd.it/7zXd5apTO5JtMIq4DoKmbJARiKH9JIn8TeQUfPr8bU8.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2be54c033ac02141742fb21a2b8677f69aab193d" title="Is Ollama Stealing Llama.cpp’s Work?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided_Dot_4557"&gt; /u/Lopsided_Dot_4557 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/9QBgY40wos8?si=6rYGGwN0xNHTRN5q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnyfdt/is_ollama_stealing_llamacpps_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mnyfdt/is_ollama_stealing_llamacpps_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-12T03:50:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnkd5t</id>
    <title>Trouble connecting ollama to docker containers</title>
    <updated>2025-08-11T18:04:01+00:00</updated>
    <author>
      <name>/u/The1TrueSteb</name>
      <uri>https://old.reddit.com/user/The1TrueSteb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, &lt;/p&gt; &lt;p&gt;I am trying to integrate ollama with some self hosted services. I have tried karakeep, linkwarden, and paperless-ai and have no success. Open webui works no problem with no setup. &lt;/p&gt; &lt;p&gt;I must be missing something, I do not understand why these services can't connect to ollama. I have tried setting up ollama on host and in a docker container, but still no luck.&lt;/p&gt; &lt;p&gt;Can anyone point me in the right direction? I have read the documentation for all these services and it just seems like it should just connect with no extra setup?&lt;/p&gt; &lt;p&gt;Any help appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The1TrueSteb"&gt; /u/The1TrueSteb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnkd5t/trouble_connecting_ollama_to_docker_containers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnkd5t/trouble_connecting_ollama_to_docker_containers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mnkd5t/trouble_connecting_ollama_to_docker_containers/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T18:04:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mncy6l</id>
    <title>Could you add a button to upload files or images directly for multimodal LLMs in Ollama?</title>
    <updated>2025-08-11T13:26:43+00:00</updated>
    <author>
      <name>/u/thestreamcode</name>
      <uri>https://old.reddit.com/user/thestreamcode</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thestreamcode"&gt; /u/thestreamcode &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mncy6l/could_you_add_a_button_to_upload_files_or_images/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mncy6l/could_you_add_a_button_to_upload_files_or_images/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mncy6l/could_you_add_a_button_to_upload_files_or_images/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T13:26:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn5uvk</id>
    <title>devstral:24b</title>
    <updated>2025-08-11T06:44:42+00:00</updated>
    <author>
      <name>/u/barrulus</name>
      <uri>https://old.reddit.com/user/barrulus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone tried to use devstral:24b for any coding work?&lt;/p&gt; &lt;p&gt;I am interested to know what kind of work I could pass to this model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/barrulus"&gt; /u/barrulus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn5uvk/devstral24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn5uvk/devstral24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mn5uvk/devstral24b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T06:44:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnnew1</id>
    <title>What is the best Scientific model</title>
    <updated>2025-08-11T19:59:06+00:00</updated>
    <author>
      <name>/u/moric7</name>
      <uri>https://old.reddit.com/user/moric7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For science questions and projects, physics and math. On 12GB VRAM and 64GB RAM. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moric7"&gt; /u/moric7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnnew1/what_is_the_best_scientific_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnnew1/what_is_the_best_scientific_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mnnew1/what_is_the_best_scientific_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T19:59:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mndl7n</id>
    <title>Intel NPU and Ollama</title>
    <updated>2025-08-11T13:53:09+00:00</updated>
    <author>
      <name>/u/Many-Kaleidoscope-72</name>
      <uri>https://old.reddit.com/user/Many-Kaleidoscope-72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys. I've got an Asus ROG Zephyrus G16 2025 with 5070ti, Core Ultra 9 285H. The Core Ultra has a NPU. No software uses it. Is it possible to load Ollama and the models onto the NPU? What would perform better? NPU or 5070ti? Or NPU works in tandem with CPU? If so, CPU+NPU vs GPU? Or is it possible to use the CPU+NPU+GPU At the same time?&lt;/p&gt; &lt;p&gt;Can anyone help me out how to use the NPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Many-Kaleidoscope-72"&gt; /u/Many-Kaleidoscope-72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mndl7n/intel_npu_and_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mndl7n/intel_npu_and_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mndl7n/intel_npu_and_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T13:53:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnomo3</id>
    <title>Is it possible to collect ollama metrics from Prometheus?</title>
    <updated>2025-08-11T20:44:33+00:00</updated>
    <author>
      <name>/u/volavi</name>
      <uri>https://old.reddit.com/user/volavi</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/volavi"&gt; /u/volavi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnomo3/is_it_possible_to_collect_ollama_metrics_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnomo3/is_it_possible_to_collect_ollama_metrics_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mnomo3/is_it_possible_to_collect_ollama_metrics_from/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T20:44:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mntmz9</id>
    <title>Can ollama do multi-step RAGs?</title>
    <updated>2025-08-12T00:06:54+00:00</updated>
    <author>
      <name>/u/StraightAd8315</name>
      <uri>https://old.reddit.com/user/StraightAd8315</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to develop a RAG that basically does a dynamic questionnaire with the user, and sometimes the question it would ask would change depending on the previous answer (IE YES/NO; Select List).&lt;/p&gt; &lt;p&gt;Can Ollama do that at,? I'm new to using it to try to build a RAG and feeling so lost on how to utilize this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StraightAd8315"&gt; /u/StraightAd8315 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mntmz9/can_ollama_do_multistep_rags/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mntmz9/can_ollama_do_multistep_rags/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mntmz9/can_ollama_do_multistep_rags/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-12T00:06:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn653l</id>
    <title>PSA: Secure Your Ollama / LLM Ports ( Even on Home LAN )</title>
    <updated>2025-08-11T07:02:47+00:00</updated>
    <author>
      <name>/u/Immediate_Fun4182</name>
      <uri>https://old.reddit.com/user/Immediate_Fun4182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I do not know where to begin. Ok here we go.&lt;/p&gt; &lt;p&gt;Recently someone on &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; &lt;a href="https://www.reddit.com/r/ollama/comments/1k6m1b3/someone_found_my_open_ai_server_and_used_it_to/"&gt;posted&lt;/a&gt; that their Ollama API had been exposed to the public internet for over a month, and strangers used it to process massive amounts of personal data.&lt;/p&gt; &lt;p&gt;I am posting this because Ollama is a beginner friendly platform and not many people do realize that Ollama and similar LLM servers like vLLM bind to &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; by default. Meaning &lt;strong&gt;any network interface&lt;/strong&gt; can accept connections (lan wifi and even an if your router is forwarding). No authentication or rate limiting exists on default. If your router has UPnP or manual port forwarding enabled, the API can be fully exposed to the internet without you realizing it.&lt;/p&gt; &lt;p&gt;Best practices I can recommend for beginners is this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Bind only to Tailscale or localhost and/or bind to tailscale0 only so it is only reachable inside your Tailnet.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Close risky ports by default:&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Use usw or firewalld to block everything except what you explicitly allow:&lt;/p&gt; &lt;p&gt;```&lt;br /&gt; sudo ufw default deny incoming&lt;/p&gt; &lt;p&gt;sudo ufw default allow outgoing&lt;/p&gt; &lt;p&gt;sudo ufw allow from &lt;a href="http://100.64.0.0/10"&gt;100.64.0.0/10&lt;/a&gt; # Tailscale subnet&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;and lastly&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Watch your exposed services on ports:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;sudo sof -i -P -n | grep LISTEN&lt;br /&gt; ```&lt;/p&gt; &lt;p&gt;F.e. I was exposing my 11434 port to public IP which is a terrible mistake. make sure you only open through vpn/tailscale set up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Immediate_Fun4182"&gt; /u/Immediate_Fun4182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn653l/psa_secure_your_ollama_llm_ports_even_on_home_lan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn653l/psa_secure_your_ollama_llm_ports_even_on_home_lan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mn653l/psa_secure_your_ollama_llm_ports_even_on_home_lan/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T07:02:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo4pmh</id>
    <title>Lightweight Model for fuzzy matching + structured output in Colab?</title>
    <updated>2025-08-12T10:09:55+00:00</updated>
    <author>
      <name>/u/Living-Boat2566</name>
      <uri>https://old.reddit.com/user/Living-Boat2566</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; I’m looking for suggestions on which model would be best to run in Google Colab (or similar environments) for a temporary research project.&lt;/p&gt; &lt;p&gt;My local setup is a Lenovo ThinkPad T14s Gen 5 (Intel Core Ultra 7 155U, 32 GB RAM, Intel Graphics), but I’m fine with running the model in Colab or similar cloud-based environments.&lt;/p&gt; &lt;p&gt;The use case:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The model receives a &lt;strong&gt;report&lt;/strong&gt; (JSON) along with a &lt;strong&gt;list of pre-approved items&lt;/strong&gt; for a specific customer (plain text).&lt;/li&gt; &lt;li&gt;It checks whether the report matches any item in the list, using a &lt;strong&gt;fuzzy matching&lt;/strong&gt; approach (e.g., allowing small spelling variations).&lt;/li&gt; &lt;li&gt;It then generates a &lt;strong&gt;structured output&lt;/strong&gt; in a predefined format, which will later be sent by email.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Accuracy is important, but I also need something lightweight enough for Colab.&lt;/p&gt; &lt;p&gt;Any recommendations for a model that could work well for this scenario?&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Living-Boat2566"&gt; /u/Living-Boat2566 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mo4pmh/lightweight_model_for_fuzzy_matching_structured/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mo4pmh/lightweight_model_for_fuzzy_matching_structured/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mo4pmh/lightweight_model_for_fuzzy_matching_structured/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-12T10:09:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo61s5</id>
    <title>When you installed a model do you have to run it?</title>
    <updated>2025-08-12T11:24:55+00:00</updated>
    <author>
      <name>/u/DarqOnReddit</name>
      <uri>https://old.reddit.com/user/DarqOnReddit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 3 models installed locally &lt;code&gt; ollama list NAME ID SIZE MODIFIED qwen3-coder:latest ad67f85ca250 18 GB 18 hours ago gpt-oss:latest e95023cf3b7b 13 GB 22 hours ago deepseek-coder-v2:16b 63fb193b3a9b 8.9 GB 9 days ago &lt;/code&gt;&lt;/p&gt; &lt;p&gt;I have a systemd service for &lt;code&gt;ollama serve&lt;/code&gt; running as root, and also a user service for qwen3-coder, however I'm not sure if that user service for qwen3-coder is required.&lt;/p&gt; &lt;p&gt;I'm using &amp;quot;continue&amp;quot; with jetbrains and vs code. &lt;code&gt;yaml name: Local Assistant version: 1.0.0 schema: v1 models: - name: Autodetect provider: ollama model: AUTODETECT context: - provider: code - provider: docs - provider: diff - provider: terminal - provider: problems - provider: folder - provider: codebase &lt;/code&gt;&lt;/p&gt; &lt;p&gt;However autocomplete is not working in Jetbrains, so I'm reading &lt;a href="https://docs.continue.dev/customize/deep-dives/autocomplete"&gt;https://docs.continue.dev/customize/deep-dives/autocomplete&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have to disable thinking for the autocomplete role.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I don't know which models are &amp;quot;thinking switchable&amp;quot;.&lt;/li&gt; &lt;li&gt;Do I have to run qwen3 or does the ollama service do that for me?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarqOnReddit"&gt; /u/DarqOnReddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mo61s5/when_you_installed_a_model_do_you_have_to_run_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mo61s5/when_you_installed_a_model_do_you_have_to_run_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mo61s5/when_you_installed_a_model_do_you_have_to_run_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-12T11:24:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo4qbf</id>
    <title>How to expose Ollama API from my local PC to a remote host?</title>
    <updated>2025-08-12T10:11:02+00:00</updated>
    <author>
      <name>/u/Formal_Jeweler_488</name>
      <uri>https://old.reddit.com/user/Formal_Jeweler_488</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve got Ollama running locally on a pretty powerful PC, and I want to make its API (&lt;code&gt;http://localhost:11434&lt;/code&gt;) accessible from a remote host so I can integrate it into a web app. The catch is: my ISP doesn’t allow port forwarding, so I can’t open ports on my router.&lt;/p&gt; &lt;p&gt;I tried using Cloudflare Tunnel (&lt;code&gt;cloudflared&lt;/code&gt;) but couldn’t get it working properly—either the tunnel wouldn’t stay up or the public URL wouldn’t forward requests correctly to the local API.&lt;/p&gt; &lt;p&gt;Ideally, I want a stable public endpoint that forwards traffic to my local Ollama instance. I’m open to using tunneling services like Ngrok or LocalTunnel, or even setting up a reverse proxy if needed. I also considered deploying Ollama on a VPS, but I’d prefer to leverage the hardware I already have.&lt;/p&gt; &lt;p&gt;Has anyone successfully done this? What’s the most reliable way to expose a local API to the public when port forwarding isn’t an option?&lt;/p&gt; &lt;p&gt;Appreciate any tips or walkthroughs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Formal_Jeweler_488"&gt; /u/Formal_Jeweler_488 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mo4qbf/how_to_expose_ollama_api_from_my_local_pc_to_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mo4qbf/how_to_expose_ollama_api_from_my_local_pc_to_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mo4qbf/how_to_expose_ollama_api_from_my_local_pc_to_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-12T10:11:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnnhmt</id>
    <title>What is the best model for absolutely free uncensored unfiltered chat on any theme</title>
    <updated>2025-08-11T20:01:48+00:00</updated>
    <author>
      <name>/u/moric7</name>
      <uri>https://old.reddit.com/user/moric7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On 12GB VRAM and 64GB RAM. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moric7"&gt; /u/moric7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnnhmt/what_is_the_best_model_for_absolutely_free/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnnhmt/what_is_the_best_model_for_absolutely_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mnnhmt/what_is_the_best_model_for_absolutely_free/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T20:01:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo6pbf</id>
    <title>Image generation</title>
    <updated>2025-08-12T11:57:55+00:00</updated>
    <author>
      <name>/u/Odd-Suggestion4292</name>
      <uri>https://old.reddit.com/user/Odd-Suggestion4292</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wouldn’t it be great if ollama added image and video generation models to its list? They’re a big pain to install manually (through hugging face) and open source UI options are terrible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Suggestion4292"&gt; /u/Odd-Suggestion4292 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mo6pbf/image_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mo6pbf/image_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mo6pbf/image_generation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-12T11:57:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo6k1v</id>
    <title>Bookseerr - My first vibe-coded application</title>
    <updated>2025-08-12T11:50:45+00:00</updated>
    <author>
      <name>/u/vir_db</name>
      <uri>https://old.reddit.com/user/vir_db</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mo6k1v/bookseerr_my_first_vibecoded_application/"&gt; &lt;img alt="Bookseerr - My first vibe-coded application" src="https://external-preview.redd.it/Vw3bBu31aiMSJTxYckiYR6DzhREt281xzPoVcP66tdI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4823ce403ce0b384bf5becab4cef89e37ca8a14" title="Bookseerr - My first vibe-coded application" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks!&lt;br /&gt; I'm happy to share my first vibe-coded application, &lt;strong&gt;Bookseerr&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;It's a full stack, easy to deploy, application that connect your &lt;strong&gt;Calibre&lt;/strong&gt; database and use an &lt;strong&gt;Ollama&lt;/strong&gt; served model (default gemma3:27b) to suggest you your next book to read. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hn08xul9ukif1.png?width=1854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4064add3e264cab255e898e050df80f15ccfd0d0"&gt;https://preview.redd.it/hn08xul9ukif1.png?width=1854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4064add3e264cab255e898e050df80f15ccfd0d0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Inspired by Jellyseer, it's totally &lt;em&gt;vibe-coded&lt;/em&gt; with a Python backend and a React frontend.&lt;/p&gt; &lt;p&gt;The code is available on &lt;a href="https://gitlab.bertorello.info/marco/bookseerr"&gt;my Gitlab&lt;/a&gt; and it's released under GPLv3 and later. Feel free to suggest any kind of improvment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vir_db"&gt; /u/vir_db &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mo6k1v/bookseerr_my_first_vibecoded_application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mo6k1v/bookseerr_my_first_vibecoded_application/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mo6k1v/bookseerr_my_first_vibecoded_application/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-12T11:50:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1moa1qu</id>
    <title>Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp; Streamlit</title>
    <updated>2025-08-12T14:20:52+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1moa1qu/build_a_local_ai_agent_with_mcp_tools_using/"&gt; &lt;img alt="Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp;amp; Streamlit" src="https://external-preview.redd.it/rq8k6bkBVDqS3EaB-6PmZwrrp9mjAeoX2Tt37ubIdpg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76f91a095d5d6782424d54183f94d9fb060dd411" title="Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp;amp; Streamlit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=Baa-z7cum1g&amp;amp;list=PLp01ObP3udmq2quR-RfrX4zNut_t_kNot&amp;amp;index=25"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1moa1qu/build_a_local_ai_agent_with_mcp_tools_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1moa1qu/build_a_local_ai_agent_with_mcp_tools_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-12T14:20:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnirls</id>
    <title>Ollama’s copy-paste dev strategy is just PR spin?</title>
    <updated>2025-08-11T17:07:02+00:00</updated>
    <author>
      <name>/u/bllshrfv</name>
      <uri>https://old.reddit.com/user/bllshrfv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mnirls/ollamas_copypaste_dev_strategy_is_just_pr_spin/"&gt; &lt;img alt="Ollama’s copy-paste dev strategy is just PR spin?" src="https://preview.redd.it/g9y4dwqw9fif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9d3d5a417006f4dcba2df0d42db0eb590427a92" title="Ollama’s copy-paste dev strategy is just PR spin?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bllshrfv"&gt; /u/bllshrfv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g9y4dwqw9fif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnirls/ollamas_copypaste_dev_strategy_is_just_pr_spin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mnirls/ollamas_copypaste_dev_strategy_is_just_pr_spin/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T17:07:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo2qga</id>
    <title>8x mi60 -- 256GB VRAM Server</title>
    <updated>2025-08-12T08:03:13+00:00</updated>
    <author>
      <name>/u/zekken523</name>
      <uri>https://old.reddit.com/user/zekken523</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zekken523"&gt; /u/zekken523 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mo2lev"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mo2qga/8x_mi60_256gb_vram_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mo2qga/8x_mi60_256gb_vram_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-12T08:03:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1morfl6</id>
    <title>GPT-OSS 20b runs on a RasPi 5, 16gb</title>
    <updated>2025-08-13T01:38:38+00:00</updated>
    <author>
      <name>/u/RasPiBuilder</name>
      <uri>https://old.reddit.com/user/RasPiBuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got bored and decided to see if GPT-OSS 20b would run on a RasPi 5, 16gb... And it does!&lt;/p&gt; &lt;p&gt;It's slow, hovering just under 1 token per second, so not really usable for conversation.. but could possibly work for some background tasks that aren't time sensitive. (I'll share the verbose output sometime tomorrow.. forgot to turn it on when I ran it).&lt;/p&gt; &lt;p&gt;For those curious, I'm running Ollama headless and bare metal.&lt;/p&gt; &lt;p&gt;And just for the fun of it, this weekend I'm going to set try to setup a little agent and see if I can get it to complete some tasks with Browser Use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RasPiBuilder"&gt; /u/RasPiBuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1morfl6/gptoss_20b_runs_on_a_raspi_5_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1morfl6/gptoss_20b_runs_on_a_raspi_5_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1morfl6/gptoss_20b_runs_on_a_raspi_5_16gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T01:38:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mog599</id>
    <title>Open Source GLM-4.5V model with the Cua Agent framework.</title>
    <updated>2025-08-12T18:06:41+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mog599/open_source_glm45v_model_with_the_cua_agent/"&gt; &lt;img alt="Open Source GLM-4.5V model with the Cua Agent framework." src="https://external-preview.redd.it/bGFzc2dlMGdwbWlmMZtBXPQuBBghVYkEG23VKH2rdUK_y7uZuqgwTRJo1CZN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dccfd6c23783f2c590f73d5cef3088157ae540e6" title="Open Source GLM-4.5V model with the Cua Agent framework." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On OSWorld-V, it scores 35.8% - beating UI-TARS-1.5, matching Claude-3.7-Sonnet-20250219, and setting SOTA for fully open-source computer-use models.&lt;/p&gt; &lt;p&gt;Run it with Cua either: Locally via Hugging Face Remotely via OpenRouter&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs + examples: &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9mp1x4agpmif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mog599/open_source_glm45v_model_with_the_cua_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mog599/open_source_glm45v_model_with_the_cua_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-12T18:06:41+00:00</published>
  </entry>
</feed>
