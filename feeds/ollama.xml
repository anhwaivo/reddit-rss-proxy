<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-05-27T03:01:54+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ktjind</id>
    <title>Ollama is running on AMD GPU, despite ROCM not being installed</title>
    <updated>2025-05-23T13:25:31+00:00</updated>
    <author>
      <name>/u/Xatraxalian</name>
      <uri>https://old.reddit.com/user/Xatraxalian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I've started to experiment with running local LLM's. It seems Ollama runs on the AMD GPU even without ROCM installed. This is what I did:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPU: AMD RX 6750 XT&lt;/li&gt; &lt;li&gt;OS: Debian Trixie 13 (currently testing)&lt;/li&gt; &lt;li&gt;Kernel: 6.14.x, Xanmod&lt;/li&gt; &lt;li&gt;Installed the Debian Trixie ROCM 6.1 libraries (bear with me here)&lt;/li&gt; &lt;li&gt;Set: HSA_OVERRIDE_GFX_VERSION=10.3.0 (in the systemd unit file)&lt;/li&gt; &lt;li&gt;Installed Ollama, and have it started with Systemd.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It ran, and it ran the models on the GPU, as 'ollama ps' said &amp;quot;100% GPU&amp;quot;. I can see the GPU being fully loaded when Ollama is doing something like generating code.&lt;/p&gt; &lt;p&gt;Then I wanted to install the latest version of ROCM from AMD, but it doesn't support Debian Trixie 13 yet. So I did this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Quit everything&lt;/li&gt; &lt;li&gt;Removed Ollama from my host system &lt;a href="https://github.com/ollama/ollama/issues/986"&gt;see here&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Installed Distrobox.&lt;/li&gt; &lt;li&gt;Created a box running Debian 12&lt;/li&gt; &lt;li&gt;Installed Ollama in it and 'exported' the binary to the host system&lt;/li&gt; &lt;li&gt;Had the box and the ollama server started by systemd&lt;/li&gt; &lt;li&gt;I still set HSA_OVERRIDE_GFX_VERSION=10.3.0&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything works: The ollama box and the server starts, and I can use the exported binary to control ollama within the distrobox. It still runs 100% on the GPU, probably because ROCM is installed on the host. (Distrobox first uses libraries in the box; if they're not there, it uses the system libraries, as far as I understand.)&lt;/p&gt; &lt;p&gt;Then I removed all the rocm libraries from my host system and rebooted the system, intending to re-install ROCM 6.4.1 in the distrobox. However, I first ran Ollama, expecting it to now run 100% on the CPU.&lt;/p&gt; &lt;p&gt;But surprise... when I restarted and then fired up a model, it was STILL running 100% on the GPU. All the ROCM libraries on the host are gone, and they where never installed in the distrobox. When grepping for 'rocm' in the 'dpkg --list' output, no ROCM packages are found; not in the host, not in the distrobox.&lt;/p&gt; &lt;p&gt;How's that possible? Does Ollama not actually require ROCM to just run the model, and only needs it to train new models? Does Ollama now include its own ROCM when installing on Linux? Is it able to run on the GPU all by itself if it detects it correctly?&lt;/p&gt; &lt;p&gt;Can anyone enlighten me here? Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xatraxalian"&gt; /u/Xatraxalian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ktjind/ollama_is_running_on_amd_gpu_despite_rocm_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ktjind/ollama_is_running_on_amd_gpu_despite_rocm_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ktjind/ollama_is_running_on_amd_gpu_despite_rocm_not/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-23T13:25:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kts6tn</id>
    <title>Tested Qwen3 all models on CPU (i5-10210U), RTX 3060 12GB, and RTX 3090 24GB</title>
    <updated>2025-05-23T19:23:11+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1ktqgk0/tested_qwen3_all_models_on_cpu_i510210u_rtx_3060/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kts6tn/tested_qwen3_all_models_on_cpu_i510210u_rtx_3060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kts6tn/tested_qwen3_all_models_on_cpu_i510210u_rtx_3060/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-23T19:23:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktiap6</id>
    <title>32GB vs 48GB RAM MBP for local LLM experimentation - real world experiences?</title>
    <updated>2025-05-23T12:27:26+00:00</updated>
    <author>
      <name>/u/SampleSalty</name>
      <uri>https://old.reddit.com/user/SampleSalty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently torn between two MacBook Pro M4 configs at the same price (€2850):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option A:&lt;/strong&gt; M4 + 32GB RAM + 2TB storage&lt;br /&gt; &lt;strong&gt;Option B:&lt;/strong&gt; M4 Pro + 48GB RAM + 1TB storage&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My use case:&lt;/strong&gt; Web research, development POCs, and increasingly interested in local LLM experimentation. I know 64GB+ is ideal for the biggest models, but that's €4500+ which is out of budget.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What's the largest/most useful model you've successfully run on 32GB vs 48GB?&lt;/li&gt; &lt;li&gt;Does the extra 16GB make a meaningful difference in your day-to-day LLM usage?&lt;/li&gt; &lt;li&gt;Any M4 vs M4 Pro performance differences you've noticed with inference?&lt;/li&gt; &lt;li&gt;Is 1TB enough storage for model experimentation, or do you find yourself constantly managing space?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm particularly interested in hearing from anyone who's made a similar choice or upgraded from 32GB to 48GB. I am between the chairs, because I also value the better efficiency of the normal M4, otherwise choice would be much easier.&lt;/p&gt; &lt;p&gt;What would you do?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SampleSalty"&gt; /u/SampleSalty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ktiap6/32gb_vs_48gb_ram_mbp_for_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ktiap6/32gb_vs_48gb_ram_mbp_for_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ktiap6/32gb_vs_48gb_ram_mbp_for_local_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-23T12:27:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ku5pon</id>
    <title>[R] The Gamechanger of Performer Attention Mechanism</title>
    <updated>2025-05-24T07:13:35+00:00</updated>
    <author>
      <name>/u/theMonarch776</name>
      <uri>https://old.reddit.com/user/theMonarch776</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ku5pon/r_the_gamechanger_of_performer_attention_mechanism/"&gt; &lt;img alt="[R] The Gamechanger of Performer Attention Mechanism" src="https://preview.redd.it/db5poy74jo2f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a99bdce3ce42be6537f49d7b7de9f14d5f137749" title="[R] The Gamechanger of Performer Attention Mechanism" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theMonarch776"&gt; /u/theMonarch776 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/db5poy74jo2f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ku5pon/r_the_gamechanger_of_performer_attention_mechanism/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ku5pon/r_the_gamechanger_of_performer_attention_mechanism/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-24T07:13:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ku0b3j</id>
    <title>I'm Building an AI Interview Prep Tool to Get Real Feedback on Your Answers - Using Ollama and Multi Agents using Agno</title>
    <updated>2025-05-24T01:44:40+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ku0b3j/im_building_an_ai_interview_prep_tool_to_get_real/"&gt; &lt;img alt="I'm Building an AI Interview Prep Tool to Get Real Feedback on Your Answers - Using Ollama and Multi Agents using Agno" src="https://external-preview.redd.it/c2szYzUyMzh4bTJmMZnC2Xmlj1tbjjNvTSMwNtpmFzPX21OzgpZxL5ufaAPv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ce977bce7b42f72dc539a696d83eb092d76ac3a" title="I'm Building an AI Interview Prep Tool to Get Real Feedback on Your Answers - Using Ollama and Multi Agents using Agno" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm developing an AI-powered interview preparation tool because I know how tough it can be to get good, specific feedback when practising for technical interviews.&lt;/p&gt; &lt;p&gt;The idea is to use local Large Language Models (via Ollama) to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Analyse your resume and extract key skills.&lt;/li&gt; &lt;li&gt;Generate dynamic interview questions based on those skills and chosen difficulty.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;And most importantly: Evaluate your answers!&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;After you go through a mock interview session (answering questions in the app), you'll go to an Evaluation Page. Here, an AI &amp;quot;coach&amp;quot; will analyze all your answers and give you feedback like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An overall score.&lt;/li&gt; &lt;li&gt;What you did well.&lt;/li&gt; &lt;li&gt;Where you can improve.&lt;/li&gt; &lt;li&gt;How you scored on things like accuracy, completeness, and clarity.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;I'd love your input:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;As someone practicing for interviews, would you prefer feedback immediately after each question, or all at the end?&lt;/li&gt; &lt;li&gt;What kind of feedback is most helpful to you? Just a score? Specific examples of what to say differently?&lt;/li&gt; &lt;li&gt;Are there any particular pain points in interview prep that you wish an AI tool could solve?&lt;/li&gt; &lt;li&gt;What would make an AI interview coach truly valuable for you?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is a passion project (using Python/FastAPI on the backend, React/TypeScript on the frontend), and I'm keen to build something genuinely useful. Any thoughts or feature requests would be amazing!&lt;/p&gt; &lt;p&gt;🚀 P.S. This project was a ton of fun, and I'm itching for my next AI challenge! If you or your team are doing innovative work in &lt;strong&gt;Computer Vision or LLM&lt;/strong&gt;S and are looking for a passionate dev, I'd love to chat.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;My Email:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=mailto%3Apavankunchalaofficial%40gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My GitHub Profile (for more projects):&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My Resume:&lt;/strong&gt; &lt;a href="https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view"&gt;https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nschui18xm2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ku0b3j/im_building_an_ai_interview_prep_tool_to_get_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ku0b3j/im_building_an_ai_interview_prep_tool_to_get_real/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-24T01:44:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktt4yc</id>
    <title>2x RTX 6000 ADA vs 4x RTX 5000 ADA</title>
    <updated>2025-05-23T20:04:05+00:00</updated>
    <author>
      <name>/u/Personal-Library4908</name>
      <uri>https://old.reddit.com/user/Personal-Library4908</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey,&lt;/p&gt; &lt;p&gt;I'm working on getting a local LLM machine due to compliance reasons.&lt;/p&gt; &lt;p&gt;As I have a budget of around 20k USD, I was able to configure a DELL 7960 in two different ways:&lt;/p&gt; &lt;p&gt;2x RTX6000 ADA 48gb (96gb) + Xeon 3433 + 128Gb DDR5 4800MT/s = 19,5k USD&lt;/p&gt; &lt;p&gt;4x RTX5000 ADA 32gb (128gb) + Xeon 3433 + 64Gb DDR5 4800MT/s = 21k USD&lt;/p&gt; &lt;p&gt;Jumping over to 3x RTX 6000 brings the amount to over 23k and is too much of a stretch for my budget.&lt;/p&gt; &lt;p&gt;I plan to serve a LLM as a Wise Man for our internal documents with no more than 10-20 simultaneous users (company have 300 administrative workers). &lt;/p&gt; &lt;p&gt;I thought of going for 4x RTX 5000 due to the possibility of loading the LLM into 3 and getting a diffusion model to run on the last one, allowing usage for both. &lt;/p&gt; &lt;p&gt;Both models don't need to be too big as we already have Copilot (GPT4 Turbo) available for all users for general questions. &lt;/p&gt; &lt;p&gt;Can you help me choose one and give some insights why?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Personal-Library4908"&gt; /u/Personal-Library4908 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ktt4yc/2x_rtx_6000_ada_vs_4x_rtx_5000_ada/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ktt4yc/2x_rtx_6000_ada_vs_4x_rtx_5000_ada/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ktt4yc/2x_rtx_6000_ada_vs_4x_rtx_5000_ada/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-23T20:04:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktx7wo</id>
    <title>Every time i send something to ollama a scary alien sound plays</title>
    <updated>2025-05-23T23:05:36+00:00</updated>
    <author>
      <name>/u/HUG0gamingHD</name>
      <uri>https://old.reddit.com/user/HUG0gamingHD</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ktx7wo/every_time_i_send_something_to_ollama_a_scary/"&gt; &lt;img alt="Every time i send something to ollama a scary alien sound plays" src="https://external-preview.redd.it/OGFucHg0eGk0bTJmMatMiBRxjAnYt8WEmWZKSJhbyvGfL8s_9B5JCBZKSlOO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80e97b149492940a1415ffd280275cdd99e0b0ed" title="Every time i send something to ollama a scary alien sound plays" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GTX 1060 6GB from msi, Think it is coil whine and I didn't hear it on my 2070 but that could have been because the fans are really loud.&lt;/p&gt; &lt;p&gt;Does anyone know what this weird sound is? It is power delivery? Coil whine? It's been really annoying me, and it's actually the loudest sound the computer makes, because I optimised it to be very quiet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HUG0gamingHD"&gt; /u/HUG0gamingHD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0xk316xi4m2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ktx7wo/every_time_i_send_something_to_ollama_a_scary/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ktx7wo/every_time_i_send_something_to_ollama_a_scary/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-23T23:05:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ku7157</id>
    <title>Knowledge cut off of models and there stupid behavior</title>
    <updated>2025-05-24T08:47:06+00:00</updated>
    <author>
      <name>/u/sudo_solvedit</name>
      <uri>https://old.reddit.com/user/sudo_solvedit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a general question if there is already a well known approach how to handle knowledge cut off of models where models reject to give a answer even if they have access web search tools and the internet but don't give a good answer and instead complain about it can't be because what I demand is in the future and it can't give me information about events happening in the future. &lt;/p&gt; &lt;p&gt;For clarification I am using OpenWeb UI with a local hosted searxng instance that works without problems only the model behavior about things that happened after some models knowledge cut off sucks and I didn't find a reliable solution for it. &lt;/p&gt; &lt;p&gt;Someone have tips or know a good working workaround for that problem?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sudo_solvedit"&gt; /u/sudo_solvedit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ku7157/knowledge_cut_off_of_models_and_there_stupid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ku7157/knowledge_cut_off_of_models_and_there_stupid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ku7157/knowledge_cut_off_of_models_and_there_stupid/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-24T08:47:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktszhz</id>
    <title>Tome (open source local LLM + MCP client) now has Windows support!</title>
    <updated>2025-05-23T19:57:49+00:00</updated>
    <author>
      <name>/u/WalrusVegetable4506</name>
      <uri>https://old.reddit.com/user/WalrusVegetable4506</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ktszhz/tome_open_source_local_llm_mcp_client_now_has/"&gt; &lt;img alt="Tome (open source local LLM + MCP client) now has Windows support!" src="https://external-preview.redd.it/ZWQ2eDl0d2Y1bDJmMVX2QN0t6HDMdnLouEWsCxzgzK2OMiUh-wZOozvL9zll.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70f6a6d32e0f41587a0e43f341001d5ca7307ecb" title="Tome (open source local LLM + MCP client) now has Windows support!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Y'all gave us awesome feedback a few weeks ago when we shared our project so I wanted to share that we added support for Windows in our latest release: &lt;a href="https://github.com/runebookai/tome/releases/tag/0.5.0"&gt;https://github.com/runebookai/tome/releases/tag/0.5.0&lt;/a&gt; This was our most requested feature so I'm hoping more of you get a chance to try it out!&lt;/p&gt; &lt;p&gt;If you didn't see &lt;a href="https://www.reddit.com/r/ollama/comments/1kiw05t/built_a_simple_way_to_oneclick_install_and/"&gt;our last post&lt;/a&gt; here's a quick refresher - Tome is a local LLM desktop client that enables you to one-click install and connect MCP servers to Ollama, without having to manage uv/npm or any json config.&lt;/p&gt; &lt;p&gt;All you have to do is install Tome, connect to Ollama (it'll auto-connect if it's localhost, otherwise you can set a remote URL), and then add an MCP server either by pasting a command like &amp;quot;uvx mcp-server-fetch&amp;quot; or using the in-app registry to one-click install thousands of servers.&lt;/p&gt; &lt;p&gt;The demo video uses Qwen3 1.7B, which calls the Scryfall MCP server (it has an API that has access to all Magic the Gathering cards), fetches one at random and then writes a song about that card in the style of Sum 41.&lt;/p&gt; &lt;p&gt;If you get a chance to try it out we would love any feedback (good or bad!) here or &lt;a href="https://discord.gg/9CH6us29YA"&gt;on our Discord&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We also added support for OpenAI and Gemini, and we're also going to be adding better error handling soon. It's still rough around the edges but (hopefully) getting better by the week, thanks to all of your feedback. :)&lt;/p&gt; &lt;p&gt;GitHub here: &lt;a href="https://github.com/runebookai/tome"&gt;https://github.com/runebookai/tome&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WalrusVegetable4506"&gt; /u/WalrusVegetable4506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/oi4bptwf5l2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ktszhz/tome_open_source_local_llm_mcp_client_now_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ktszhz/tome_open_source_local_llm_mcp_client_now_has/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-23T19:57:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kujxz9</id>
    <title>Template issue for Nvidia Nemotron</title>
    <updated>2025-05-24T19:37:59+00:00</updated>
    <author>
      <name>/u/iadanos</name>
      <uri>https://old.reddit.com/user/iadanos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt; &lt;p&gt;I was trying to use &lt;a href="https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-8B-v1"&gt;nvidia/Llama-3.1-Nemotron-Nano-8B-v1&lt;/a&gt; (basically, &lt;a href="https://huggingface.co/unsloth/Llama-3.1-Nemotron-Nano-8B-v1-GGUF"&gt;Unsloth's quant&lt;/a&gt;) and didn't really managed to make it work properly due to template issue:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Running ollama pull with HF model path pulls model, but default promt template is not really usable, so custom modelfile is required&lt;/li&gt; &lt;li&gt;Using the same template as for LLama 3.1 fails since it is different for Nemotron and detailed thinking and tools calling does not work properly.&lt;/li&gt; &lt;li&gt;I tried to re-write original chat template from Jinja and it's better, it discovers and identifies tools correctly, it starts thinking and tries to use it, but not getting to use the proper format, as I understand.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Maybe detailed thinking is somehow conflicting with tool call, but I didn't manage to fix it so it would work with Continue.dev.&lt;/p&gt; &lt;p&gt;Looking for helpers, testers and advisers, since the result seem very close and the model should be much more useful than a stock one.&lt;/p&gt; &lt;p&gt;The modelfile I have now:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;FROM hf.co/unsloth/Llama-3.1-Nemotron-Nano-8B-v1-GGUF:Q4_k_M TEMPLATE &amp;quot;&amp;quot;&amp;quot;{{- /* Initialize system message */ -}} {{- $system_message := .System -}} {{- $system_message_found := false -}} {{- /* Find system message priority: .System &amp;gt; first system message in Messages */ -}} {{- if not $system_message -}} {{- range .Messages -}} {{- if and (eq .Role &amp;quot;system&amp;quot;) (not $system_message_found) -}} {{- $system_message = .Content -}} {{- $system_message_found = true -}} {{- end -}} {{- end -}} {{- end -}} {{- /* System section rendering */ -}} {{- if or $system_message .Tools -}} &amp;lt;|start_header_id|&amp;gt;system&amp;lt;|end_header_id|&amp;gt; {{- if $system_message -}} {{ $system_message }}{{- end -}} {{- if .Tools -}} {{if $system_message}} {{end -}} You are a helpful assistant with tool calling capabilities. When you receive a tool call response, use the output to format an answer to the orginal user question. Make tool calls after your detailed thinking and reasoning output. &amp;lt;AVAILABLE_TOOLS&amp;gt; {{- $firstTool := true -}} {{- range .Tools -}} {{- if not $firstTool -}},{{- end -}} {{- json .Function -}} {{- $firstTool = false -}} {{- end -}} &amp;lt;/AVAILABLE_TOOLS&amp;gt; {{- end -}}&amp;lt;|eot_id|&amp;gt; {{- end -}} {{- /* Process messages */ -}} {{- $lastRole := &amp;quot;&amp;quot; -}} {{- range $index, $message := .Messages -}} {{- $last := eq (len (slice $.Messages $index)) 1 }} {{- /* Track last role for final prompt */ -}} {{- $lastRole = .Role -}} {{- /* Skip system messages when using .System */ -}} {{- if and $system_message (eq .Role &amp;quot;system&amp;quot;) }}{{ continue }}{{ end -}} {{- if eq .Role &amp;quot;user&amp;quot; -}} &amp;lt;|start_header_id|&amp;gt;user&amp;lt;|end_header_id|&amp;gt; {{- if and $.Tools $last }} Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt. Respond in the format {&amp;quot;name&amp;quot;: function name, &amp;quot;parameters&amp;quot;: dictionary of argument name and its value}. Do not use variables. {{ range $.Tools }} {{- . }} {{ end }} Question: {{ .Content }}&amp;lt;|eot_id|&amp;gt; {{- else }} {{ .Content }}&amp;lt;|eot_id|&amp;gt; {{ end }} {{- else if eq .Role &amp;quot;tool&amp;quot; -}} &amp;lt;|start_header_id|&amp;gt;user&amp;lt;|end_header_id|&amp;gt; &amp;lt;TOOL_RESPONSE&amp;gt;[{{ .Content }}]&amp;lt;/TOOL_RESPONSE&amp;gt;&amp;lt;|eot_id|&amp;gt; {{- else if eq .Role &amp;quot;assistant&amp;quot; -}} {{- if .ToolCalls -}} &amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt; &amp;lt;TOOLCALL&amp;gt;[ {{- $firstCall := true -}} {{- range .ToolCalls -}} {{- if not $firstCall -}},{{- end -}} {&amp;quot;name&amp;quot;: &amp;quot;{{ .Function.Name }}&amp;quot;, &amp;quot;parameters&amp;quot;: {{ .Function.Arguments }}} {{- $firstCall = false -}} {{- end -}} ]&amp;lt;/TOOLCALL&amp;gt;&amp;lt;|eot_id|&amp;gt; {{- else -}} &amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt; {{ .Content }}&amp;lt;|eot_id|&amp;gt; {{- end -}} {{- end -}} {{- end -}} &amp;quot;&amp;quot;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iadanos"&gt; /u/iadanos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kujxz9/template_issue_for_nvidia_nemotron/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kujxz9/template_issue_for_nvidia_nemotron/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kujxz9/template_issue_for_nvidia_nemotron/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-24T19:37:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ku4ejf</id>
    <title>Open source model which good at tool calling?</title>
    <updated>2025-05-24T05:44:59+00:00</updated>
    <author>
      <name>/u/Superb_Practice_4544</name>
      <uri>https://old.reddit.com/user/Superb_Practice_4544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am working on small project which involves MCP and some custom tools. Which open source model should I use ? Preferably smaller models. Thanks for the help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Superb_Practice_4544"&gt; /u/Superb_Practice_4544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ku4ejf/open_source_model_which_good_at_tool_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ku4ejf/open_source_model_which_good_at_tool_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ku4ejf/open_source_model_which_good_at_tool_calling/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-24T05:44:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kutppv</id>
    <title>can a model be connected to github?</title>
    <updated>2025-05-25T04:05:35+00:00</updated>
    <author>
      <name>/u/onemorequickchange</name>
      <uri>https://old.reddit.com/user/onemorequickchange</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a way to connect &lt;strong&gt;Qwen 3&lt;/strong&gt; to a github repository so it can analyze existing code and add features?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onemorequickchange"&gt; /u/onemorequickchange &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kutppv/can_a_model_be_connected_to_github/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kutppv/can_a_model_be_connected_to_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kutppv/can_a_model_be_connected_to_github/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-25T04:05:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kudn6h</id>
    <title>how is MCP tool calling different form basic function calling?</title>
    <updated>2025-05-24T15:01:36+00:00</updated>
    <author>
      <name>/u/benxben13</name>
      <uri>https://old.reddit.com/user/benxben13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to figure out if MCP is doing native tool calling or it's the same standard function calling using multiple llm calls but just more universally standardized and organized.&lt;/p&gt; &lt;p&gt;let's take the following example of an message only travel agency:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;travel agency&amp;gt; &amp;lt;tools&amp;gt; async def search\_hotels(query) ---&amp;gt; calls a rest api and generates a json containing a set of hotels async def select_hotels(hotels_list, criteria) ---&amp;gt; calls a rest api and generates a json containing top choice hotel and two alternatives async def book_hotel(hotel_id) ---&amp;gt; calls a rest api and books a hotel return a json containing fail or success &amp;lt;/tools&amp;gt; &amp;lt;pipeline&amp;gt; #step 0 query = str(input()) # example input is 'book for me the best hotel closest to the Empire State Building' #step 1 prompt1 = f&amp;quot;given the users query {query} you have to do the following: 1- study the search_hotels tool {hotel_search_doc_string} 2- study the select_hotels tool {select_hotels_doc_string} task: generate a json containing the set of query parameter for the search_hotels tool and the criteria parameter for the select_hotels so we can execute the user's query output format { 'qeury': 'put here the generated query for search_hotels', 'criteria': 'put here the generated query for select_hotels' } &amp;quot; params = llm(prompt1) params = json.loads(params) #step 2 hotels_search_list = await search_hotels(params['query']) #step 3 selected_hotels = await select_hotels(hotels_search_list, params['criteria']) selected_hotels = json.loads(selected_hotels) #step 4 show the results to the user print(f&amp;quot;here is the list of hotels which do you wish to book? the top choice is {selected_hotels['top']} the alternatives are {selected_hotels['alternatives'][0]} and {selected_hotels['alternatives'][1]} let me know which one to book? &amp;quot; #step 5 users_choice = str(input()) # example input is &amp;quot;go for the top the choice&amp;quot; prompt2 = f&amp;quot; given the list of the hotels: {selected_hotels} and the user's answer {users_choice} give an json output containing the id of the hotel selected by the user output format: { 'id': 'put here the id of the hotel selected by the user' } &amp;quot; id = llm(prompt2) id = json.loads(id) #step 6 user confirmation print(f&amp;quot;do you wish to book hotel {hotels_search_list[id['id']]} ?&amp;quot;) users_choice = str(input()) # example answer: yes please prompt3 = f&amp;quot;given the user's answer reply with a json confirming the user wants to book the given hotel or not output format: { 'confirm': 'put here true or false depending on the users answer' } confirm = llm(prompt3) confirm = json.loads(confirm) if confirm['confirm']: book_hotel(id['id']) else: print('booking failed, lets try again') #go to step 5 again &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;let's assume that the user responses in both cases are parsable only by an llm and we can't figure them out using the ui. What's the version of this using MCP looks like? does it make the same 3 llm calls ? or somehow it calls them natively? &lt;/p&gt; &lt;p&gt;If I understand correctly:&lt;br /&gt; et's say an llm call is : &lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;llm_call&amp;gt; prompt = 'usr: hello' llm_response = 'assistant: hi how are you ' &amp;lt;/llm_call&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;correct me if I'm wrong but an llm is next token generation correct so in sense it's doing a series of micro class like :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;llm_call&amp;gt; prompt = 'user: hello how are you assistant: ' llm_response_1 = ''user: hello how are you assistant: hi&amp;quot; llm_response_2 = ''user: hello how are you assistant: hi how &amp;quot; llm_response_3 = ''user: hello how are you assistant: hi how are &amp;quot; llm_response_4 = ''user: hello how are you assistant: hi how are you&amp;quot; &amp;lt;/llm_call&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;like in this way: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;‘user: hello assitant:’ —&amp;gt; ‘user: hello, assitant: hi’ ‘user: hello, assitant: hi’ —&amp;gt; ‘user: hello, assitant: hi how’ ‘user: hello, assitant: hi how’ —&amp;gt; ‘user: hello, assitant: hi how are’ ‘user: hello, assitant: hi how are’ —&amp;gt; ‘user: hello, assitant: hi how are you’ ‘user: hello, assitant: hi how are you’ —&amp;gt; ‘user: hello, assitant: hi how are you &amp;lt;stop_token&amp;gt; ’ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;so in case of a tool use using mcp does it work using which approach out of the following:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;lt;/llm_call_approach_1&amp;gt; prompt = 'user: hello how is today weather in austin' llm_response_1 = ''user: hello how is today weather in Austin, assistant: hi&amp;quot; ... llm_response_n = ''user: hello how is today weather in Austin, assistant: hi let me use tool weather with params {Austin, today's date}&amp;quot; # can we do like a mini pause here run the tool and inject it here like: llm_response_n_plus1 = ''user: hello how is today weather in Austin, assistant: hi let me use tool weather with params {Austin, today's date} {tool_response --&amp;gt; it's sunny in austin}&amp;quot; llm_response_n_plus1 = ''user: hello how is today weather in Austin , assistant: hi let me use tool weather with params {Austin, today's date} {tool_response --&amp;gt; it's sunny in Austin} according&amp;quot; llm_response_n_plus2 = ''user:hello how is today weather in austin , assistant: hi let me use tool weather with params {Austin, today's date} {tool_response --&amp;gt; it's sunny in Austin} according to&amp;quot; llm_response_n_plus3 = ''user: hello how is today weather in austin , assistant: hi let me use tool weather with params {Austin, today's date} {tool_response --&amp;gt; it's sunny in Austin} according to tool&amp;quot; .... llm_response_n_plus_m = ''user: hello how is today weather in austin , assistant: hi let me use tool weather with params {Austin, today's date} {tool_response --&amp;gt; it's sunny in Austin} according to tool the weather is sunny to today Austin. &amp;quot; &amp;lt;/llm_call_approach_1&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;or does it do it in this way:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;llm_call_approach_2&amp;gt; prompt = ''user: hello how is today weather in austin&amp;quot; intermediary_response = &amp;quot; I must use tool {waather} wit params ...&amp;quot; # await wather tool intermediary_prompt = f&amp;quot;using the results of the wather tool {weather_results} reply to the users question: {prompt}&amp;quot; llm_response = 'it's sunny in austin' &amp;lt;/llm_call_approach_2&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;what I mean to say is that: does mcp execute the tools at the level of the next token generation and inject the results to the generation process so the llm can adapt its response on the fly or does it make separate calls in the same way as the manual way just organized way ensuring coherent input output format?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/benxben13"&gt; /u/benxben13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kudn6h/how_is_mcp_tool_calling_different_form_basic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kudn6h/how_is_mcp_tool_calling_different_form_basic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kudn6h/how_is_mcp_tool_calling_different_form_basic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-24T15:01:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuku1g</id>
    <title>What are the most capable LLM models to run with NVIDIA GeForce RTX 4060 8GB Laptop GPU and AMD Ryzen 9 8945HS CPU and 32 RAM</title>
    <updated>2025-05-24T20:19:35+00:00</updated>
    <author>
      <name>/u/Happysedits</name>
      <uri>https://old.reddit.com/user/Happysedits</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Happysedits"&gt; /u/Happysedits &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kuku1g/what_are_the_most_capable_llm_models_to_run_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kuku1g/what_are_the_most_capable_llm_models_to_run_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kuku1g/what_are_the_most_capable_llm_models_to_run_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-24T20:19:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kufitg</id>
    <title>Cua : Docker Container for Computer Use Agents</title>
    <updated>2025-05-24T16:23:57+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kufitg/cua_docker_container_for_computer_use_agents/"&gt; &lt;img alt="Cua : Docker Container for Computer Use Agents" src="https://external-preview.redd.it/NWdkb3VuNDZhcjJmMcsvHa0C_XuOSkhUSfxPH2wNUS_IzERNrp7qS2qcV3Nx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d4005034643f2814d24f0385eb4e46b274bf994" title="Cua : Docker Container for Computer Use Agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cua is the Docker for Computer-Use Agent, an open-source framework that enables AI agents to control full operating systems within high-performance, lightweight virtual containers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8kzntcf6ar2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kufitg/cua_docker_container_for_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kufitg/cua_docker_container_for_computer_use_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-24T16:23:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kut4v8</id>
    <title>2x 3090 cards - ollama installed with multiple models</title>
    <updated>2025-05-25T03:30:58+00:00</updated>
    <author>
      <name>/u/onemorequickchange</name>
      <uri>https://old.reddit.com/user/onemorequickchange</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My mb has 64GB RAM and an i9-12900k CPU. I've gotten deepseek-r1:70b and llama3.3:latest to use both cards.&lt;br /&gt; qwen2.5-coder:32b is my goto for coding. So the real question is, what is the next best coding model that I can still run with these specs? And what would be a model to justify a upgraded hardware?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onemorequickchange"&gt; /u/onemorequickchange &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kut4v8/2x_3090_cards_ollama_installed_with_multiple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kut4v8/2x_3090_cards_ollama_installed_with_multiple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kut4v8/2x_3090_cards_ollama_installed_with_multiple/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-25T03:30:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuq0mt</id>
    <title>Is there any easy way to get up and running with chatgpt-like capabilities at home?</title>
    <updated>2025-05-25T00:33:29+00:00</updated>
    <author>
      <name>/u/MeYaj1111</name>
      <uri>https://old.reddit.com/user/MeYaj1111</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a noob, running Windows 10 on a 32GB i5-9600K w/ 8GB GTX 3070&lt;/p&gt; &lt;p&gt;I do not care about performance, I only care about capability. &lt;/p&gt; &lt;p&gt;Is there any way to get up and running with a chatgpt-like interface that I can use for general purpose things like doing research with real-time data from internet searches, &amp;quot;deep research&amp;quot; where it will take the time to think about its answer before finalizing it, basic image generation, etc? As close to the chatgpt experience as possible, aside from the performance since I know my system is crap.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MeYaj1111"&gt; /u/MeYaj1111 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kuq0mt/is_there_any_easy_way_to_get_up_and_running_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kuq0mt/is_there_any_easy_way_to_get_up_and_running_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kuq0mt/is_there_any_easy_way_to_get_up_and_running_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-25T00:33:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvb4eo</id>
    <title>Graphical card</title>
    <updated>2025-05-25T19:40:14+00:00</updated>
    <author>
      <name>/u/Zealousideal-One5210</name>
      <uri>https://old.reddit.com/user/Zealousideal-One5210</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Because I'm a complete noob for graphical cards... Couple of months ago I bought a beelink Intel Arc with this docking station &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.bee-link.com/products/beelink-ex-docking-station?variant=46659193241842"&gt;https://www.bee-link.com/products/beelink-ex-docking-station?variant=46659193241842&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now I'm looking for a graphical card that can run perfectly with ollama. Not looking for those massive big models. I'm happy with the smaller ones, because I also see the smaller ones getting better and better. And not want to spend to much (max 350 euro). So I found this card for example &lt;a href="https://amzn.eu/d/6D5vaQ8"&gt;https://amzn.eu/d/6D5vaQ8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would this work? Is this one any good for Running gemma3:8b for example?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-One5210"&gt; /u/Zealousideal-One5210 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kvb4eo/graphical_card/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kvb4eo/graphical_card/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kvb4eo/graphical_card/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-25T19:40:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kuy23x</id>
    <title>Updated jarvis project .</title>
    <updated>2025-05-25T08:55:07+00:00</updated>
    <author>
      <name>/u/cython_boy</name>
      <uri>https://old.reddit.com/user/cython_boy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After weeks of upgrades and modular refinements, I'm thrilled to unveil the latest version of &lt;strong&gt;Jarvis&lt;/strong&gt;, my personal AI assistant built with Streamlit, LangChain, Gemini, Ollama, and custom ML/LLM agents. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ganeshnikhil/J.A.R.V.I.S.2.0"&gt;JARVIS&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Normal&lt;/strong&gt;: Understands natural queries and executes dynamic function calls.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Personal Chat&lt;/strong&gt;: Keeps track of important conversations and responds contextually using Ollama + memory logic.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAG Chat&lt;/strong&gt;: Ask deep questions across topics like Finance, AI, Disaster, Space Tech using embedded knowledge via LangChain + FAISS.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data Analysis&lt;/strong&gt;: Upload a CSV, ask in plain English, and Jarvis will auto-generate insightful Python code (with fallback logic if API fails!).&lt;/li&gt; &lt;li&gt;Toggle voice replies on/off.&lt;/li&gt; &lt;li&gt;Use voice input via audio capture.&lt;/li&gt; &lt;li&gt;Speech output uses real-time TTS with Streamlit rendering.&lt;/li&gt; &lt;li&gt; Enable Developer Mode, turn on USB Debugging, connect via USB, and run &lt;code&gt;adb devices&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cython_boy"&gt; /u/cython_boy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kuy23x/updated_jarvis_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kuy23x/updated_jarvis_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kuy23x/updated_jarvis_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-25T08:55:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvixua</id>
    <title>Title: Seeking Help: A "Deep Research" Project for a Retired Mathematician (Recoll, Langchain, Ollama)</title>
    <updated>2025-05-26T02:00:10+00:00</updated>
    <author>
      <name>/u/DigiDadaist</name>
      <uri>https://old.reddit.com/user/DigiDadaist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Reddit!&lt;/p&gt; &lt;p&gt;I'm a 70-year-old retired mathematician from Poland. I have a large collection of digital books and articles, indexed using Recoll. I want to build a tool that can help me explore and understand this information in more depth.&lt;/p&gt; &lt;p&gt;My idea is to create a &amp;quot;deep research&amp;quot; application that works like this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;**Find Documents:** Use Recoll (through its web interface's API) to find documents related to a topic.&lt;/li&gt; &lt;li&gt;**Ask Questions:** Use a computer program (Langchain and Ollama) to automatically generate questions about these documents. The program should be able to ask many different questions to really understand the topic.&lt;/li&gt; &lt;li&gt;**Answer Questions:** Use the same program (Langchain and Ollama) to answer the questions, using the documents as a source of information.&lt;/li&gt; &lt;li&gt;**Learn and Repeat:** The program should learn from the answers and use that knowledge to ask even better questions. It should repeat this process several times.&lt;/li&gt; &lt;li&gt;**Create Summary:** Finally, the program should create a summary of everything it has learned.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I am inspired by this project: &lt;a href="https://github.com/u14app/deep-research"&gt;https://github.com/u14app/deep-research&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I want to use:&lt;/p&gt; &lt;p&gt;* **Recoll:** Because I already use it to index my documents.&lt;/p&gt; &lt;p&gt;* **Langchain:** A framework to help build the program.&lt;/p&gt; &lt;p&gt;* **Ollama:** To run a &amp;quot;Large Language Model&amp;quot; locally on my computer (no internet needed). This model will help generate and answer questions.&lt;/p&gt; &lt;p&gt;The problems I have are:&lt;/p&gt; &lt;p&gt;* **My English is not very good.**&lt;/p&gt; &lt;p&gt;* **I am not a strong programmer.** I know some basic programming, but not enough to build this myself.&lt;/p&gt; &lt;p&gt;* **Connecting Recoll with Langchain:** I don't know how to get the information from Recoll into Langchain.&lt;/p&gt; &lt;p&gt;* **Making the program ask good questions:** I need help making the program generate questions that are interesting and useful.&lt;/p&gt; &lt;p&gt;I am looking for help from the community. I would like:&lt;/p&gt; &lt;p&gt;* **Advice and ideas:** Any suggestions are welcome!&lt;/p&gt; &lt;p&gt;* **Example code:** Especially for connecting Recoll with Langchain.&lt;/p&gt; &lt;p&gt;* **Someone to collaborate with:** If you are interested in helping me build this project, please contact me! I am willing to learn and contribute as much as I can.&lt;/p&gt; &lt;p&gt;I plan to make this project open source so that others can use it.&lt;/p&gt; &lt;p&gt;Thank you for your time and help!&lt;/p&gt; &lt;p&gt;TL;DR: Retired mathematician needs help building a &amp;quot;deep research&amp;quot; tool using Recoll, Langchain, and Ollama. Low programming skills, needs help with Recoll integration and question generation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DigiDadaist"&gt; /u/DigiDadaist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kvixua/title_seeking_help_a_deep_research_project_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kvixua/title_seeking_help_a_deep_research_project_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kvixua/title_seeking_help_a_deep_research_project_for_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-26T02:00:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kv61te</id>
    <title>Local-first AI + SearXNG in one place - reclaim your autonomy (Cognito AI Search v1.1.0)</title>
    <updated>2025-05-25T16:03:18+00:00</updated>
    <author>
      <name>/u/kekePower</name>
      <uri>https://old.reddit.com/user/kekePower</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;After many late nights and a lot of caffeine, I’m proud to share something I’ve been quietly building for a while: &lt;a href="https://github.com/kekePower/cognito-ai-search"&gt;&lt;strong&gt;Cognito AI Search&lt;/strong&gt;&lt;/a&gt;, a self-hosted, local-first tool that combines &lt;strong&gt;private AI chat&lt;/strong&gt; (via Ollama) with &lt;strong&gt;anonymous web search&lt;/strong&gt; (via SearXNG) in one clean interface.&lt;/p&gt; &lt;p&gt;I wanted something that would let me:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ask questions to a fast, local LLM without my data ever leaving my machine&lt;/li&gt; &lt;li&gt;Search the web anonymously without all the bloat, tracking, or noise&lt;/li&gt; &lt;li&gt;Use a single, simple UI, not two disconnected tabs or systems&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I built it.&lt;br /&gt; No ads, no logging, no cloud dependencies, just pure function. The blog post dives a little deeper into the thinking behind it and shows a screenshot:&lt;br /&gt; 👉 &lt;a href="https://blog.kekepower.com/blog/2025/may/25/cognito_ai_search_110_-_where_precision_meets_polish.html"&gt;Cognito AI Search 1.1.0 - Where Precision Meets Polish&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I built this for people like me, people who want control, speed, and clarity in how they interact with both AI and the web. It’s open source, minimal, and actively being improved.&lt;/p&gt; &lt;p&gt;Would love to hear your feedback, ideas, or criticism. If it’s useful to even a handful of people here, I’ll consider that a win. 🙌&lt;/p&gt; &lt;p&gt;Thanks for checking it out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kekePower"&gt; /u/kekePower &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kv61te/localfirst_ai_searxng_in_one_place_reclaim_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kv61te/localfirst_ai_searxng_in_one_place_reclaim_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kv61te/localfirst_ai_searxng_in_one_place_reclaim_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-25T16:03:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvjt36</id>
    <title>Looking to learn about hosting my first local LLM</title>
    <updated>2025-05-26T02:48:55+00:00</updated>
    <author>
      <name>/u/anmolmanchanda</name>
      <uri>https://old.reddit.com/user/anmolmanchanda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I have been a huge ChatGPT user since day 1. I am confident that I have been the top 1% user, using it several hours daily for personal and work; solving every problem in life with it. I ended up sharing more and more personal and sensitive information to give context and the more i gave, the better it was able to help me until I realised the privacy implications.&lt;/p&gt; &lt;p&gt;I am now looking to replace my experience with ChatGPT 4o as long as I can get close to accuracy. I am okay with being twice or three times as slow which would be understandable.&lt;/p&gt; &lt;p&gt;I also understand that it runs on millions of dollars of infrastructure, my goal is not get exactly there, just as close as I can.&lt;/p&gt; &lt;p&gt;I experimented with LLama 3 8B Q4 on my MacBook Pro, speed was acceptable but the responses left a bit to be desired. Then I moved to Deepseek r1 distilled 14B Q5 which was streching the limit of my laptop, but I was able to run it and responses were better.&lt;/p&gt; &lt;p&gt;I am currently thinking of buying a new or very likely used PC (or used parts for a PC separately) to run LLama 3.3 70B Q4. Q5 would be slightly better but I don't want to spend crazy from the start.&lt;/p&gt; &lt;p&gt;And I am hoping to upgrade in 1-2 months so the PC can run FP16 for the same model.&lt;/p&gt; &lt;p&gt;I am also considering Llama 4 and I need to read more about it to understand it's benefits and costs.&lt;/p&gt; &lt;p&gt;My budget initially preferably would be $3500 CAD, but would be willing to go to $4000 CAD for a solid foundation that I can build upon.&lt;/p&gt; &lt;p&gt;I use ChatGPT for work a lot, I would like accuracy and reliabiltiy to be as high as 4o; so part of me wants to build for FP16 from the get go.&lt;/p&gt; &lt;p&gt;For coding, I pay seperately for Cursor and that I am willing to keep paying until I have FP16 at least or even after as Claude Sonnet 4 is unbeatable. I am curious what open source model is as good in coding to that?&lt;/p&gt; &lt;p&gt;For the update in 1-2 months, budget I am thinking is $2000-2500 CAD&lt;/p&gt; &lt;p&gt;I am looking to hear which of my assumptions are wrong? What resources I should read more? What hardware specifications I should buy for my first AI PC? Which model is best suited for my needs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anmolmanchanda"&gt; /u/anmolmanchanda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kvjt36/looking_to_learn_about_hosting_my_first_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kvjt36/looking_to_learn_about_hosting_my_first_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kvjt36/looking_to_learn_about_hosting_my_first_local_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-26T02:48:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvj89n</id>
    <title>What's the best I can get from Ollama with my setup? Looking for model &amp; workflow suggestions</title>
    <updated>2025-05-26T02:15:34+00:00</updated>
    <author>
      <name>/u/Calebe94</name>
      <uri>https://old.reddit.com/user/Calebe94</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I'm diving deeper into local LLM workflows with Ollama and wanted to tap into the community's collective brainpower for some guidance and inspiration.&lt;/p&gt; &lt;p&gt;Here’s what I’m working with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🧠 &lt;strong&gt;CPU&lt;/strong&gt;: Ryzen 5 5600X&lt;/li&gt; &lt;li&gt;🧠 &lt;strong&gt;RAM&lt;/strong&gt;: 64GB DDR4 @ 3600MHz&lt;/li&gt; &lt;li&gt;🎮 &lt;strong&gt;GPU&lt;/strong&gt;: Radeon RX6600 (so yeah, ROCm is &lt;em&gt;meh&lt;/em&gt;, I’m mostly CPU-bound)&lt;/li&gt; &lt;li&gt;🐧 &lt;strong&gt;OS&lt;/strong&gt;: Debian Sid&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I work as a senior cloud developer and also do embedded/hardware stuff (KiCAD, electronics prototyping, custom mechanical keyboards, etc). I’m also neurodivergent (ADHD, autism), and I’ve been trying to integrate LLMs into my workflow not just for productivity, but also for cognitive scaffolding — like breaking down complex tasks, context retention, journaling, decision trees, automations, and reminders.&lt;/p&gt; &lt;p&gt;So I’m wondering:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Given my setup, what’s the best I can realistically run smoothly with Ollama?&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What models do you recommend for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Coding (Python, Terraform, Bash, KiCAD-related tasks)&lt;/li&gt; &lt;li&gt;Thought organization (task breakdown, long-context support)&lt;/li&gt; &lt;li&gt;Automation planning (like agents / planners that actually work offline-ish)&lt;/li&gt; &lt;li&gt;General chat and productivity assistance&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Any tools you’d recommend pairing with Ollama for local workflows?&lt;/li&gt; &lt;li&gt;Anyone doing automations with shell scripts or hooking LLMs into daily tools like &lt;code&gt;todo.txt&lt;/code&gt;, &lt;code&gt;obsidian&lt;/code&gt;, &lt;code&gt;cron&lt;/code&gt;, or even custom scripts?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I know my GPU limits me with current ROCm support, but with 64GB RAM, I figure there’s still a lot I can do. I’m also fine running things in CPU-only mode, if it means more flexibility or compatibility.&lt;/p&gt; &lt;p&gt;Would love to hear what kind of setups you folks are running, and what models/tools/flows are actually &lt;em&gt;worth it&lt;/em&gt; right now in the local LLM scene.&lt;/p&gt; &lt;p&gt;Appreciate any tips or setups you’re willing to share. 🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Calebe94"&gt; /u/Calebe94 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kvj89n/whats_the_best_i_can_get_from_ollama_with_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kvj89n/whats_the_best_i_can_get_from_ollama_with_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kvj89n/whats_the_best_i_can_get_from_ollama_with_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-26T02:15:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvsgeg</id>
    <title>AI vision on windows with Ollama</title>
    <updated>2025-05-26T11:53:32+00:00</updated>
    <author>
      <name>/u/LivingSignificant452</name>
      <uri>https://old.reddit.com/user/LivingSignificant452</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;br /&gt; in case you prefer the speed of a native application for windows, Obviousidea just announced they support Ollama with Light Image Editor :&lt;br /&gt; &lt;a href="https://www.obviousidea.com/light-image-resizer-ollama-support-ai-vision/"&gt;https://www.obviousidea.com/light-image-resizer-ollama-support-ai-vision/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;it speed up the upload part and directly save in the metadata the description. there is an automode to speed up the description on a set of photos.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LivingSignificant452"&gt; /u/LivingSignificant452 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kvsgeg/ai_vision_on_windows_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kvsgeg/ai_vision_on_windows_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kvsgeg/ai_vision_on_windows_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-26T11:53:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kw8woc</id>
    <title>gemma3:12b-it-qat vs gemma3:12b memory usage using Ollama</title>
    <updated>2025-05-26T23:35:26+00:00</updated>
    <author>
      <name>/u/LithuanianAmerican</name>
      <uri>https://old.reddit.com/user/LithuanianAmerican</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;gemma3:12b-it-qat is advertised to use 3x less memory than gemma3:12b yet in my testing on my Mac I'm seeing that Ollama is actually using 11.55gb of memory for the quantized model and 9.74gb for the regular variant. Why is the quantized model actually using more memory? How can I &amp;quot;find&amp;quot; those memory savings?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LithuanianAmerican"&gt; /u/LithuanianAmerican &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kw8woc/gemma312bitqat_vs_gemma312b_memory_usage_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kw8woc/gemma312bitqat_vs_gemma312b_memory_usage_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kw8woc/gemma312bitqat_vs_gemma312b_memory_usage_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-26T23:35:26+00:00</published>
  </entry>
</feed>
