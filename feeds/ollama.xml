<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-04T07:48:43+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1j1q26x</id>
    <title>Avoid placeholders</title>
    <updated>2025-03-02T12:45:42+00:00</updated>
    <author>
      <name>/u/samftijazwaro</name>
      <uri>https://old.reddit.com/user/samftijazwaro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No matter what prompt I use, no matter what system prompt I give, deepseek 14b and qwen-coder14b ALWAYS use placeholder text.&lt;/p&gt; &lt;p&gt;I want to be asked &amp;quot;what is the path to the file, what is your username, what is the URL?&amp;quot; and then once it has the information, provide complete terminal commands.&lt;/p&gt; &lt;p&gt;I just cannot get it to work. Meanwhile, I have 0 such issues with Grok 3/ChatGPT. Is it simply a limitation of weaker models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samftijazwaro"&gt; /u/samftijazwaro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1q26x/avoid_placeholders/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1q26x/avoid_placeholders/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1q26x/avoid_placeholders/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T12:45:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1n84y</id>
    <title>Experiment Reddit + small local LLM</title>
    <updated>2025-03-02T09:36:28+00:00</updated>
    <author>
      <name>/u/raul3820</name>
      <uri>https://old.reddit.com/user/raul3820</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to test the possibility of filtering content with small local models, just reading the text multiple times, filtering few things at a time. In this case I use &lt;code&gt;mistral-small:24b&lt;/code&gt;&lt;/p&gt; &lt;p&gt;To test the idea, I made a reddit account &lt;a href="/u/osoconfesoso007"&gt;u/osoconfesoso007&lt;/a&gt; that receives stories and publishes them anonimously.&lt;/p&gt; &lt;p&gt;It's supposed to filter out personal data and only publish interesting stories. I want to test if the filters are reliable, so feel free to poke at it.&lt;/p&gt; &lt;p&gt;It's open source: &lt;a href="https://github.com/raul3820/oso"&gt;github&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/raul3820"&gt; /u/raul3820 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1n84y/experiment_reddit_small_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1n84y/experiment_reddit_small_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1n84y/experiment_reddit_small_local_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T09:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1yh4j</id>
    <title>Suggestions for a coding model for a MacBook M4 Pro 24gb</title>
    <updated>2025-03-02T19:02:03+00:00</updated>
    <author>
      <name>/u/PLCLINK</name>
      <uri>https://old.reddit.com/user/PLCLINK</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Would be pleased to hear your suggestions or experiences. Thanks in advance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PLCLINK"&gt; /u/PLCLINK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1yh4j/suggestions_for_a_coding_model_for_a_macbook_m4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1yh4j/suggestions_for_a_coding_model_for_a_macbook_m4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1yh4j/suggestions_for_a_coding_model_for_a_macbook_m4/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T19:02:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1ree1</id>
    <title>WASImancer, an MCP server with SSE transport, powered by WebAssembly</title>
    <updated>2025-03-02T13:57:20+00:00</updated>
    <author>
      <name>/u/Inevitable-Judge2642</name>
      <uri>https://old.reddit.com/user/Inevitable-Judge2642</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j1ree1/wasimancer_an_mcp_server_with_sse_transport/"&gt; &lt;img alt="WASImancer, an MCP server with SSE transport, powered by WebAssembly" src="https://external-preview.redd.it/1x8HVEC_omAWU9QM9VtDofQPOgkR7qQwNIfTXFBoRX4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=852f2ce1959ef1e202cbadd2d9dcfc833d460148" title="WASImancer, an MCP server with SSE transport, powered by WebAssembly" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Judge2642"&gt; /u/Inevitable-Judge2642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://k33g.hashnode.dev/wasimancer-an-mcp-server-with-sse-transport-powered-by-webassembly"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1ree1/wasimancer_an_mcp_server_with_sse_transport/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1ree1/wasimancer_an_mcp_server_with_sse_transport/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T13:57:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1h4mz</id>
    <title>What do you actually use local LLM's for?</title>
    <updated>2025-03-02T03:04:07+00:00</updated>
    <author>
      <name>/u/ivkemilioner</name>
      <uri>https://old.reddit.com/user/ivkemilioner</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivkemilioner"&gt; /u/ivkemilioner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1h4mz/what_do_you_actually_use_local_llms_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1h4mz/what_do_you_actually_use_local_llms_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1h4mz/what_do_you_actually_use_local_llms_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T03:04:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j21vzv</id>
    <title>What's the difference between Ollama and LMstudio for hosting?</title>
    <updated>2025-03-02T21:24:36+00:00</updated>
    <author>
      <name>/u/DuelShockX</name>
      <uri>https://old.reddit.com/user/DuelShockX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm still trying to get Ollama downloaded on my D drive instead of C drive so I've only experienced LMstudio so far. Anyone here can tell me what's the difference between the two? Does Ollama offer a way to connect the models to the internetfor real-time data?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DuelShockX"&gt; /u/DuelShockX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j21vzv/whats_the_difference_between_ollama_and_lmstudio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j21vzv/whats_the_difference_between_ollama_and_lmstudio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j21vzv/whats_the_difference_between_ollama_and_lmstudio/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T21:24:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j24poa</id>
    <title>I did some poking, but didn't see a lot of info. Ollama and graphics.</title>
    <updated>2025-03-02T23:28:02+00:00</updated>
    <author>
      <name>/u/Ravenseye</name>
      <uri>https://old.reddit.com/user/Ravenseye</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a pipeline for getting image generation llms to work under the ollama umbrella?&lt;/p&gt; &lt;p&gt;Can they be run offline as well?&lt;/p&gt; &lt;p&gt;Thank you in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ravenseye"&gt; /u/Ravenseye &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j24poa/i_did_some_poking_but_didnt_see_a_lot_of_info/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j24poa/i_did_some_poking_but_didnt_see_a_lot_of_info/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j24poa/i_did_some_poking_but_didnt_see_a_lot_of_info/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T23:28:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1e5k8</id>
    <title>For Mac users, Ollama is getting MLX support!</title>
    <updated>2025-03-02T00:29:13+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama has officially started work on MLX support! For those who don't know, this is huge for anyone running models locally on their Mac. MLX is designed to fully utilize Apple's unified memory and GPU. Expect faster, more efficient LLM training, execution and inference speeds.&lt;/p&gt; &lt;p&gt;You can watch the progress here:&lt;br /&gt; &lt;a href="https://github.com/ollama/ollama/pull/9118"&gt;https://github.com/ollama/ollama/pull/9118&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Development is still early but you can now pull it down and run it yourself by running the following (as mentioned in the PR)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cmake -S . -B build cmake --build build -j go build . OLLAMA_NEW_ENGINE=1 OLLAMA_BACKEND=mlx ollama serve &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let me know your thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1e5k8/for_mac_users_ollama_is_getting_mlx_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1e5k8/for_mac_users_ollama_is_getting_mlx_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1e5k8/for_mac_users_ollama_is_getting_mlx_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T00:29:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2fygi</id>
    <title>Accessing an LLM Across the home network</title>
    <updated>2025-03-03T10:45:11+00:00</updated>
    <author>
      <name>/u/Birdinhandandbush</name>
      <uri>https://old.reddit.com/user/Birdinhandandbush</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sure this is obvious to some, but wondering what I need to do myself. &lt;/p&gt; &lt;p&gt;If I have an LLM running on my home system, running the Ollama Serve command, can I access the local LLM on my tablet in another room for example. &lt;/p&gt; &lt;p&gt;In the future I was hoping to have a desktop/server setup in one room with the LLM running, and that I could connect with my other laptop or tablets as needed. &lt;/p&gt; &lt;p&gt;Any advice or feedback appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Birdinhandandbush"&gt; /u/Birdinhandandbush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2fygi/accessing_an_llm_across_the_home_network/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2fygi/accessing_an_llm_across_the_home_network/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2fygi/accessing_an_llm_across_the_home_network/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T10:45:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2cc8o</id>
    <title>Small Models for android</title>
    <updated>2025-03-03T06:20:43+00:00</updated>
    <author>
      <name>/u/Loveandfucklife</name>
      <uri>https://old.reddit.com/user/Loveandfucklife</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can someone suggest &amp;lt;6GB or &amp;lt;8GB models to run on android ?&lt;/p&gt; &lt;p&gt;Condition - 1. For general purpose QnA or Infobased 2. Knowledge cut of date near 2024 3. Unfiltered or Uncensored&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loveandfucklife"&gt; /u/Loveandfucklife &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2cc8o/small_models_for_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2cc8o/small_models_for_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2cc8o/small_models_for_android/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T06:20:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2hw55</id>
    <title>I just downloaded cuda. How should I now be able to give ollama access to the power of my gpu?</title>
    <updated>2025-03-03T12:47:47+00:00</updated>
    <author>
      <name>/u/Dalar42</name>
      <uri>https://old.reddit.com/user/Dalar42</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dalar42"&gt; /u/Dalar42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2hw55/i_just_downloaded_cuda_how_should_i_now_be_able/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2hw55/i_just_downloaded_cuda_how_should_i_now_be_able/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2hw55/i_just_downloaded_cuda_how_should_i_now_be_able/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T12:47:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j20l5m</id>
    <title>ANY UPDATES ON THE APPLE SILICON (M1,M2,M3,M4) CRITICAL FLAW?</title>
    <updated>2025-03-02T20:29:57+00:00</updated>
    <author>
      <name>/u/fremenmuaddib</name>
      <uri>https://old.reddit.com/user/fremenmuaddib</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone have some news about this issue? I have 2 thunderbolt SSD drives connected to my MacMini M4 Pro 64GB, and this is still a huge source of troubles for me, with continuous and unpredictable resets of the machine while I'm using mlx models, as you can read here: &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/neobundy/Deep-Dive-Into-AI-With-MLX-PyTorch/blob/main/NOTES_ON_METAL_BUGS.md"&gt;NOTES ON METAL BUGS by neobundy&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Neobundy is a smart Korean guy who wrote 3 technical books on MLX, hundreds of web articles and tutorials, and even developed two stable diffusion apps that use different SD models on apple silicon. He was one of the most prominent supporter of the architecture, but after discovering and reporting the critical issue with the M chips, Apple ignored his requests for an entire year, until he finally announced his decision to abandon any R&amp;amp;D work on the Apple Silicon since he now believes that Apple does not have any plan to address the issue.&lt;/p&gt; &lt;p&gt;I don't understand. Is Apple going to admit the design flaws in the M processors and start working on a software fix or on a improved hardware architecture?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fremenmuaddib"&gt; /u/fremenmuaddib &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j20l5m/any_updates_on_the_apple_silicon_m1m2m3m4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j20l5m/any_updates_on_the_apple_silicon_m1m2m3m4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j20l5m/any_updates_on_the_apple_silicon_m1m2m3m4/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T20:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2kz8u</id>
    <title>Ollama models for translation</title>
    <updated>2025-03-03T15:19:02+00:00</updated>
    <author>
      <name>/u/Zalupik98</name>
      <uri>https://old.reddit.com/user/Zalupik98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was trying using DeepSeek model to translate English text to Norwegian but it works terribly, is there any models which would work better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zalupik98"&gt; /u/Zalupik98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2kz8u/ollama_models_for_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2kz8u/ollama_models_for_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2kz8u/ollama_models_for_translation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T15:19:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2i2bv</id>
    <title>Best open source models to try out different RAG techniques.</title>
    <updated>2025-03-03T12:57:14+00:00</updated>
    <author>
      <name>/u/Superb_Practice_4544</name>
      <uri>https://old.reddit.com/user/Superb_Practice_4544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hii all, hope you guys are doing great. Recently I am learning about RAGs and want to try out different RAG techniques and their differences.&lt;/p&gt; &lt;p&gt;Which 7b parameter model works best for RAG use case ? Also need suggestion on best Open source embedding models. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Superb_Practice_4544"&gt; /u/Superb_Practice_4544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2i2bv/best_open_source_models_to_try_out_different_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2i2bv/best_open_source_models_to_try_out_different_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2i2bv/best_open_source_models_to_try_out_different_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T12:57:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2rfky</id>
    <title>Newbie Question - Ollama with Open-Webui and deepscaler / deepseek r1</title>
    <updated>2025-03-03T19:42:44+00:00</updated>
    <author>
      <name>/u/Lipora</name>
      <uri>https://old.reddit.com/user/Lipora</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running the above and running into some interesting issues.&lt;/p&gt; &lt;p&gt;I need some help understanding where my problem actually exists. Uploading some Word Documents as part of my query to the LLM and wanting it to combine and use the best information from all the documents to create essentially a distilled version of the information that aligns with the question being asked. Think of the example: Here are a bunch of my old resumes. Help me take the information from these resumes and compile them into a resume I can use to apply on the following position... And then listing all the details of the job posting. Deekseek R1 seems to be able to read &amp;quot;parts of the documents&amp;quot; and provide a reasonable response, but other models don't even seem to be able to open the documents or understand what is in them. Is this a tool that's needed to be added to Open-WebUI to assist with taking the uploaded content and getting it into a format that the LLM can understand? or the LLM itself? or some addition to Ollama that is needed? I guess I'm just trying to truly understand how the three tools, ollama, the LLM models themselves and Open-WebUI work together.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lipora"&gt; /u/Lipora &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2rfky/newbie_question_ollama_with_openwebui_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2rfky/newbie_question_ollama_with_openwebui_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2rfky/newbie_question_ollama_with_openwebui_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T19:42:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j26bbr</id>
    <title>Impressed with how well Ollama runs on the RasPi, this is Granite3.1 MoE</title>
    <updated>2025-03-03T00:45:13+00:00</updated>
    <author>
      <name>/u/RasPiBuilder</name>
      <uri>https://old.reddit.com/user/RasPiBuilder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j26bbr/impressed_with_how_well_ollama_runs_on_the_raspi/"&gt; &lt;img alt="Impressed with how well Ollama runs on the RasPi, this is Granite3.1 MoE" src="https://preview.redd.it/u1578jgzfdme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70291d2ca9084c471397d78898f453ae38d57293" title="Impressed with how well Ollama runs on the RasPi, this is Granite3.1 MoE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RasPiBuilder"&gt; /u/RasPiBuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u1578jgzfdme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j26bbr/impressed_with_how_well_ollama_runs_on_the_raspi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j26bbr/impressed_with_how_well_ollama_runs_on_the_raspi/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T00:45:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2nyut</id>
    <title>Toolmaker – A Tool SDK to Standardize AI Agent Capabilities</title>
    <updated>2025-03-03T17:23:13+00:00</updated>
    <author>
      <name>/u/suvsuvsuv</name>
      <uri>https://old.reddit.com/user/suvsuvsuv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI agents are powerful, but building tools for them is still chaotic.&lt;/p&gt; &lt;p&gt;We built Toolmaker, an SDK that provides a structured, scalable way to create and manage AI agent tools.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/suvsuvsuv"&gt; /u/suvsuvsuv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://docs.try-synaptic.ai/atm/toolmaker"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2nyut/toolmaker_a_tool_sdk_to_standardize_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2nyut/toolmaker_a_tool_sdk_to_standardize_ai_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T17:23:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2udva</id>
    <title>Get Started Easily with LangchainJS and Ollama</title>
    <updated>2025-03-03T21:45:22+00:00</updated>
    <author>
      <name>/u/Inevitable-Judge2642</name>
      <uri>https://old.reddit.com/user/Inevitable-Judge2642</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j2udva/get_started_easily_with_langchainjs_and_ollama/"&gt; &lt;img alt="Get Started Easily with LangchainJS and Ollama" src="https://external-preview.redd.it/dSL_MDQNqcxsEs3DXiQe3svJ7ZmGtARuqD1bteUJo8c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7046a30c4ff8975305969bdd0d4420225b41c3c" title="Get Started Easily with LangchainJS and Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Judge2642"&gt; /u/Inevitable-Judge2642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://k33g.hashnode.dev/get-started-easily-with-langchainjs-and-ollama"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2udva/get_started_easily_with_langchainjs_and_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2udva/get_started_easily_with_langchainjs_and_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T21:45:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2v629</id>
    <title>Python libs for local LLM + use cases</title>
    <updated>2025-03-03T22:18:47+00:00</updated>
    <author>
      <name>/u/jshre</name>
      <uri>https://old.reddit.com/user/jshre</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New to local LLM but with some background in machine learning (good old Scikit Learn). Does anyone have pointers for powerful python libraries / tools to use with Ollama server? What could be practical use cases?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jshre"&gt; /u/jshre &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2v629/python_libs_for_local_llm_use_cases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2v629/python_libs_for_local_llm_use_cases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2v629/python_libs_for_local_llm_use_cases/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T22:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j32eir</id>
    <title>Need help with an error message</title>
    <updated>2025-03-04T04:13:55+00:00</updated>
    <author>
      <name>/u/First_Handle_7722</name>
      <uri>https://old.reddit.com/user/First_Handle_7722</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m getting an error “model requires more system memory (747.4 MiB) than is available (694.8 MiB)” how can I fix it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/First_Handle_7722"&gt; /u/First_Handle_7722 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j32eir/need_help_with_an_error_message/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j32eir/need_help_with_an_error_message/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j32eir/need_help_with_an_error_message/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T04:13:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2ibxv</id>
    <title>Chat with my own PDF documents</title>
    <updated>2025-03-03T13:11:14+00:00</updated>
    <author>
      <name>/u/9elpi8</name>
      <uri>https://old.reddit.com/user/9elpi8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, as title says I would like to chat with my PDF documents. Which model would you recommend me to use? Best would be with multilanguage support. I have Nvidia 4060Ti 16GB.&lt;/p&gt; &lt;p&gt;My idea is make several threads inside AnythingLLM where I would have my receipts in other thread books related to engineering or some other learning stuff.&lt;/p&gt; &lt;p&gt;Thank you for your recommendation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/9elpi8"&gt; /u/9elpi8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2ibxv/chat_with_my_own_pdf_documents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2ibxv/chat_with_my_own_pdf_documents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2ibxv/chat_with_my_own_pdf_documents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T13:11:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3550x</id>
    <title>Exam Questions Ai Saas</title>
    <updated>2025-03-04T07:04:55+00:00</updated>
    <author>
      <name>/u/Dry_Ingenuity_8009</name>
      <uri>https://old.reddit.com/user/Dry_Ingenuity_8009</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I working on make Saas that help teachers to put exam questions for high school In Egypt I have noticed that this process take from teachers a lot of time and money so the system is like this I ask the ai for like 100 low-level,mid-level or high-level questions for lets say physics subject so it has to give me exactly the 100 question with the chosen level or a group of levels so I want also this to be generated question not just retrieved from the knowledge base to prevent any copyright issues so what is the best technique to achieve this using fine tune only or rag or both of them and if there any one have the right way to do it please tell me &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Ingenuity_8009"&gt; /u/Dry_Ingenuity_8009 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3550x/exam_questions_ai_saas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3550x/exam_questions_ai_saas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3550x/exam_questions_ai_saas/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T07:04:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1j324wb</id>
    <title>Proper local llm</title>
    <updated>2025-03-04T04:00:47+00:00</updated>
    <author>
      <name>/u/Daedric800</name>
      <uri>https://old.reddit.com/user/Daedric800</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys i need help to find local light weight Ilm that is specified and fine-tuned just for the task of coding, which means a model that is trained only for coding and nothing else which make it very light weight and small in size since it does not do chat, math, etc.. which makes it small in size yet powerful in coding like claude or deepseek models, i cant see why i havent came across a model like that yet, why are not people making a specific coding models, we are at 2025, so please if you have a model with these specs please do tell me, so i could use it for a proper coding tasks on my low end gpu locally or maybe someonw of you guys could train a simple unsloth model for just coding and upload it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daedric800"&gt; /u/Daedric800 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j324wb/proper_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j324wb/proper_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j324wb/proper_local_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T04:00:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j30893</id>
    <title>Qwen2.5 32b will start to put the tool calls in the content instead of the tool_calls</title>
    <updated>2025-03-04T02:20:23+00:00</updated>
    <author>
      <name>/u/nstevnc77</name>
      <uri>https://old.reddit.com/user/nstevnc77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I've been building a small application with Ollama for personal use that involves tool calling. I've been really impressed with Qwen2.5's ability to figure out when to do tool calls, which tools to use, and its overall reliability.&lt;/p&gt; &lt;p&gt;The only problem I've been running into is that Qwen2.5 will start putting its tool calls (JSON) in the content instead of the proper tool_calls part of the JSON. This is frustrating because it works so well otherwise.&lt;/p&gt; &lt;p&gt;It always seems to get the tool calls correct in the beginning, but about 20-40 messages in, it just starts putting the JSON in the content. Has anyone found a solution to this issue? I'm thinking that maybe because I'm saving those tool call messages in its list of messages or I'm adding &amp;quot;toolresult&amp;quot; responses that maybe it's getting confused?&lt;/p&gt; &lt;p&gt;Just wanted to see if anybody has had a similar experience!&lt;/p&gt; &lt;p&gt;Edit: I've tried llama models but they will ALWAYS call tools given the chance. Not very useful for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nstevnc77"&gt; /u/nstevnc77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j30893/qwen25_32b_will_start_to_put_the_tool_calls_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j30893/qwen25_32b_will_start_to_put_the_tool_calls_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j30893/qwen25_32b_will_start_to_put_the_tool_calls_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T02:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2j8nt</id>
    <title>I open-sourced Klee today, an Ollama GUI designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities.</title>
    <updated>2025-03-03T13:58:46+00:00</updated>
    <author>
      <name>/u/w-zhong</name>
      <uri>https://old.reddit.com/user/w-zhong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j2j8nt/i_opensourced_klee_today_an_ollama_gui_designed/"&gt; &lt;img alt="I open-sourced Klee today, an Ollama GUI designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." src="https://preview.redd.it/e9n94wxjdhme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cea4c13efc5bd2a090237ef7c3fe7065ceb8f0d9" title="I open-sourced Klee today, an Ollama GUI designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w-zhong"&gt; /u/w-zhong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e9n94wxjdhme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2j8nt/i_opensourced_klee_today_an_ollama_gui_designed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2j8nt/i_opensourced_klee_today_an_ollama_gui_designed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T13:58:46+00:00</published>
  </entry>
</feed>
