<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-07-27T17:49:47+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1m7ufom</id>
    <title>My new Chrome extension lets you easily query Ollama and copy any text with a click.</title>
    <updated>2025-07-24T04:04:57+00:00</updated>
    <author>
      <name>/u/Sea-Reception-2697</name>
      <uri>https://old.reddit.com/user/Sea-Reception-2697</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Reception-2697"&gt; /u/Sea-Reception-2697 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m7u9fz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7ufom/my_new_chrome_extension_lets_you_easily_query/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7ufom/my_new_chrome_extension_lets_you_easily_query/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T04:04:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8cjcj</id>
    <title>How I got Ollama to use my GPU in Docker &amp; WSL2 (RTX 3090TI)</title>
    <updated>2025-07-24T18:39:16+00:00</updated>
    <author>
      <name>/u/lid_z</name>
      <uri>https://old.reddit.com/user/lid_z</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;Background: &lt;ol&gt; &lt;li&gt;I use &lt;a href="https://github.com/louislam/dockge"&gt;Dockge&lt;/a&gt; for managing my containers&lt;/li&gt; &lt;li&gt;I'm using my gaming PC so it needs to stay windows (until SteamOS is publicly available)&lt;/li&gt; &lt;li&gt;When I say WSL I mean WSL2. dont feel like typing the 2 every time.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Install Nvidia tools onto WSL (See instructions here: &lt;a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation"&gt;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation&lt;/a&gt; or here: &lt;a href="https://hub.docker.com/r/ollama/ollama#nvidia-gpu"&gt;https://hub.docker.com/r/ollama/ollama#nvidia-gpu&lt;/a&gt; ) &lt;ol&gt; &lt;li&gt;Open WSL terminal on the host machine&lt;/li&gt; &lt;li&gt;Follow the instructions in either of the guides linked above&lt;/li&gt; &lt;li&gt;go into docker desktop and restart the docker engine (See more here about how to do that: &lt;a href="https://docs.docker.com/reference/cli/docker/desktop/restart/"&gt;https://docs.docker.com/reference/cli/docker/desktop/restart/&lt;/a&gt; )&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Use this compose file with special attention (you shouldn't need to change anything just highlighting what makes the Nvidia GPU available in the compose) to the &amp;quot;deploy&amp;quot; &amp;amp; &amp;quot;environment&amp;quot; keys: &lt;/li&gt; &lt;/ol&gt; &lt;blockquote&gt; &lt;p&gt;&lt;code&gt;services:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;webui:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;image:&lt;/code&gt; &lt;a href="http://ghcr.io/open-webui/open-webui:main"&gt;&lt;code&gt;ghcr.io/open-webui/open-webui:main&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;container_name: webui&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ports:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- 7000:8080/tcp&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;volumes:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- open-webui:/app/backend/data&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;extra_hosts:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- host.docker.internal:host-gateway&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;depends_on:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- ollama&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;restart: unless-stopped&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;image: ollama/ollama&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;container_name: ollama&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;deploy:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;resources:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;reservations:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;devices:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- driver: nvidia&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;count: 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;capabilities:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- gpu&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;environment:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- TZ=America/New_York&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- gpus=all&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;expose:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- 11434/tcp&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ports:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- 11434:11434/tcp&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;healthcheck:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;test: ollama --version || exit 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;volumes:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- ollama:/root/.ollama&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;restart: unless-stopped&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;volumes:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama: null&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;open-webui: null&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;networks: {}&lt;/code&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lid_z"&gt; /u/lid_z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8cjcj/how_i_got_ollama_to_use_my_gpu_in_docker_wsl2_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8cjcj/how_i_got_ollama_to_use_my_gpu_in_docker_wsl2_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m8cjcj/how_i_got_ollama_to_use_my_gpu_in_docker_wsl2_rtx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T18:39:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7wryh</id>
    <title>RAG project fails to retrieve info from large Excel files – data ingested but not found at query time. Need help debugging.</title>
    <updated>2025-07-24T06:17:48+00:00</updated>
    <author>
      <name>/u/One-Will5139</name>
      <uri>https://old.reddit.com/user/One-Will5139</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a beginner building a RAG system and running into a strange issue with large Excel files.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt;&lt;br /&gt; When I ingest large Excel files, the system appears to extract and process the data correctly during ingestion. However, when I later query the system for specific information from those files, it responds as if the data doesn’t exist.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Details of my tech stack and setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Django&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAG/LLM Orchestration:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;LangChain for managing LLM calls, embeddings, and retrieval&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vector Store:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Qdrant (accessed via langchain-qdrant + qdrant-client)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;File Parsing:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Excel/CSV: &lt;code&gt;pandas&lt;/code&gt;, &lt;code&gt;openpyxl&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM Details:&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat Model:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;gpt-4o&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Embedding Model:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;text-embedding-ada-002&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One-Will5139"&gt; /u/One-Will5139 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7wryh/rag_project_fails_to_retrieve_info_from_large/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7wryh/rag_project_fails_to_retrieve_info_from_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7wryh/rag_project_fails_to_retrieve_info_from_large/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T06:17:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7pahj</id>
    <title>How do HF models get to "ollama pull"?</title>
    <updated>2025-07-23T23:54:24+00:00</updated>
    <author>
      <name>/u/neurostream</name>
      <uri>https://old.reddit.com/user/neurostream</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like Hugging Face is sort of the main release hub for new models.&lt;/p&gt; &lt;p&gt;Can I point the ollama cli with an env var or other config method to pull directly from HF? &lt;/p&gt; &lt;p&gt;How do models make their way from HF to the ollama.com registry where one can access them with an &amp;quot;ollama pull&amp;quot;?&lt;/p&gt; &lt;p&gt;Are the gemma, deepseek, mistral, and qwen models on ollama.com posted there by the same official owners that first release them through HF? Like, are the popular/top listings still the &amp;quot;official&amp;quot; model, or are they re-releases by other specialty users and teams?&lt;/p&gt; &lt;p&gt;Does the GGUF format they end up in - also split in to parts/layers with the ORAS registry storage scheme used by ollama.com - entail any loss of quality or features for the same quant/architecture the HF version is?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neurostream"&gt; /u/neurostream &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7pahj/how_do_hf_models_get_to_ollama_pull/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7pahj/how_do_hf_models_get_to_ollama_pull/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7pahj/how_do_hf_models_get_to_ollama_pull/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-23T23:54:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8660w</id>
    <title>Usecase for 16GB MacBook Air M4</title>
    <updated>2025-07-24T14:39:06+00:00</updated>
    <author>
      <name>/u/Fluffy-Platform5153</name>
      <uri>https://old.reddit.com/user/Fluffy-Platform5153</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all,&lt;/p&gt; &lt;p&gt;I am looking for a model that works best for the following-&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Letter writing&lt;/li&gt; &lt;li&gt;English correction&lt;/li&gt; &lt;li&gt;Analysing images/ pdfs and extracting text&lt;/li&gt; &lt;li&gt;Answering Questions from text in PDF/ images and drafting written content based on extractions from the doc&lt;/li&gt; &lt;li&gt;NO Excel related stuff. Pure text based work&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Typical office stuff but i need a local one since data is company confidential&lt;/p&gt; &lt;p&gt;Kindly advise?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluffy-Platform5153"&gt; /u/Fluffy-Platform5153 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8660w/usecase_for_16gb_macbook_air_m4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8660w/usecase_for_16gb_macbook_air_m4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m8660w/usecase_for_16gb_macbook_air_m4/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T14:39:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8fa1j</id>
    <title>How does Ollama stream tokens to the CLI?</title>
    <updated>2025-07-24T20:24:55+00:00</updated>
    <author>
      <name>/u/TheBroseph69</name>
      <uri>https://old.reddit.com/user/TheBroseph69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does it use websockets, or something else?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheBroseph69"&gt; /u/TheBroseph69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8fa1j/how_does_ollama_stream_tokens_to_the_cli/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8fa1j/how_does_ollama_stream_tokens_to_the_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m8fa1j/how_does_ollama_stream_tokens_to_the_cli/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T20:24:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8s7zi</id>
    <title>Copy Model to another Server</title>
    <updated>2025-07-25T06:49:13+00:00</updated>
    <author>
      <name>/u/Internal_Junket_25</name>
      <uri>https://old.reddit.com/user/Internal_Junket_25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How to copy a Downloaded LLM to another Server (without Internet)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Internal_Junket_25"&gt; /u/Internal_Junket_25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8s7zi/copy_model_to_another_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8s7zi/copy_model_to_another_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m8s7zi/copy_model_to_another_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-25T06:49:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8gpls</id>
    <title>Ollama plugin for zsh</title>
    <updated>2025-07-24T21:21:44+00:00</updated>
    <author>
      <name>/u/kstopa</name>
      <uri>https://old.reddit.com/user/kstopa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m8gpls/ollama_plugin_for_zsh/"&gt; &lt;img alt="Ollama plugin for zsh" src="https://external-preview.redd.it/6v2VzO7p7_e9watftsiCareTaSopiFqOUVdrqXVL9XU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d30543bf30b613d15b2e91d975c96ebedcdaf40" title="Ollama plugin for zsh" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A great ZSH plugin that enables to ask for a specific command directly on the terminal. Just write what you need and press Ctrl+B to get some command options.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kstopa"&gt; /u/kstopa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kstopa/zsh-ollama-command/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8gpls/ollama_plugin_for_zsh/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m8gpls/ollama_plugin_for_zsh/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T21:21:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8o2t5</id>
    <title>Computron now has a "virtual computer"</title>
    <updated>2025-07-25T02:57:08+00:00</updated>
    <author>
      <name>/u/Individual_Ad_1453</name>
      <uri>https://old.reddit.com/user/Individual_Ad_1453</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m8o2t5/computron_now_has_a_virtual_computer/"&gt; &lt;img alt="Computron now has a &amp;quot;virtual computer&amp;quot;" src="https://external-preview.redd.it/zKvT4i91dfJNwcERvaNS61Y5ke_CwLezyMeGUGi7MPY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b4bc19e06cfd8dd9248b2b08b575d40648cabc3" title="Computron now has a &amp;quot;virtual computer&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm giving my personal AI agent a virtual computer so it can do computer stuff.&lt;/p&gt; &lt;p&gt;One example is it can now write a multi-file program if I say something like &amp;quot;create a multi-file side scroller game inspired by mario, using only pygame and do not include any external assets&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sg7b7uiyqxef1.png?width=1853&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d28dbdca4ff5c592812474b34b55292a67fe308d"&gt;https://preview.redd.it/sg7b7uiyqxef1.png?width=1853&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d28dbdca4ff5c592812474b34b55292a67fe308d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It also has a rudimentary &amp;quot;deep research&amp;quot; agent you can ask it do do things like &amp;quot;research how to run LLMs on local hardware using ollama&amp;quot;. It'll do a bunch of steps including googling and searching reddit then synthesize the results.&lt;/p&gt; &lt;p&gt;It's no open AI agent but it's also running on two 3090s and using Qwen3:30b-a3b and getting pretty good results.&lt;/p&gt; &lt;p&gt;Check it out on github &lt;a href="https://github.com/lefoulkrod/computron_9000/"&gt;https://github.com/lefoulkrod/computron_9000/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My readme isn't very good because I'm mostly doing this for myself but if you want to run it and you get stuck message me and I'll help you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Individual_Ad_1453"&gt; /u/Individual_Ad_1453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8o2t5/computron_now_has_a_virtual_computer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8o2t5/computron_now_has_a_virtual_computer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m8o2t5/computron_now_has_a_virtual_computer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-25T02:57:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1m989fv</id>
    <title>How to use open-source LLMs in a Microsoft Azure-heavy company?</title>
    <updated>2025-07-25T19:19:39+00:00</updated>
    <author>
      <name>/u/liljuden</name>
      <uri>https://old.reddit.com/user/liljuden</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I work in a company that is heavily invested in the Microsoft Azure ecosystem. Currently I use Azure OpenAI and it works great, but I also want to explore open-source LLMs (like LLaMA, Mistral, etc.) for internal applications but struggle to understand exactly how to do it.&lt;/p&gt; &lt;p&gt;I’m trying to understand how I can deploy open-source LLMs in Azure and also what is needed for it to work, like for example, do I need to spin up my own inference endpoints on Azure VMs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liljuden"&gt; /u/liljuden &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m989fv/how_to_use_opensource_llms_in_a_microsoft/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m989fv/how_to_use_opensource_llms_in_a_microsoft/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m989fv/how_to_use_opensource_llms_in_a_microsoft/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-25T19:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8wpjx</id>
    <title>Key Takeaways for LLM Input Length</title>
    <updated>2025-07-25T11:32:12+00:00</updated>
    <author>
      <name>/u/Modders_Arena</name>
      <uri>https://old.reddit.com/user/Modders_Arena</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here’s a brief summary of a recent analysis on how large language models (LLMs) perform as input size increases:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Accuracy Drops with Length:&lt;/strong&gt; LLMs get less reliable as prompts grow, especially after a few thousand tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;More Distractors = More Hallucinations:&lt;/strong&gt; Irrelevant text in the input causes more mistakes and hallucinated answers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semantic Similarity Matters:&lt;/strong&gt; If the query and answer are strongly related, performance degrades less.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Shuffling Helps:&lt;/strong&gt; Randomizing input order can sometimes improve retrieval.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Behaviors Differ:&lt;/strong&gt; Some abstain (Claude), others guess confidently (GPT).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tip:&lt;/strong&gt; For best results, keep prompts focused, filter out irrelevant info, and experiment with input order.&lt;/p&gt; &lt;p&gt;Read more here: &lt;a href="https://synehq.com/blog/facing-the-token-tide-how-increasing-input-tokens-impacts-llm-performance"&gt;Click here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Modders_Arena"&gt; /u/Modders_Arena &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8wpjx/key_takeaways_for_llm_input_length/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8wpjx/key_takeaways_for_llm_input_length/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m8wpjx/key_takeaways_for_llm_input_length/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-25T11:32:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8yyzd</id>
    <title>Alright, I am done with vLLM. Will Ollama get tensor parallel?</title>
    <updated>2025-07-25T13:19:26+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Will Ollama get tensor parallel or anything which would utilize multiple GPUs simultaneusly?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8yyzd/alright_i_am_done_with_vllm_will_ollama_get/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8yyzd/alright_i_am_done_with_vllm_will_ollama_get/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m8yyzd/alright_i_am_done_with_vllm_will_ollama_get/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-25T13:19:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1m93gt2</id>
    <title>Any good QW3-coder models for Ollama yet?</title>
    <updated>2025-07-25T16:15:50+00:00</updated>
    <author>
      <name>/u/beedunc</name>
      <uri>https://old.reddit.com/user/beedunc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama's model download site appears to be stuck in June. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beedunc"&gt; /u/beedunc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m93gt2/any_good_qw3coder_models_for_ollama_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m93gt2/any_good_qw3coder_models_for_ollama_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m93gt2/any_good_qw3coder_models_for_ollama_yet/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-25T16:15:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9mw82</id>
    <title>How to Convert Fine-Tuned Qwen 2.5 VL 3B Model to Ollama? (Mungert/Qwen2.5-VL-3B-Instruct-GGUF)</title>
    <updated>2025-07-26T07:03:29+00:00</updated>
    <author>
      <name>/u/DSN_CV</name>
      <uri>https://old.reddit.com/user/DSN_CV</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I recently fine-tuned the Qwen 2.5 VL 3B model for a custom vision-language task and now I’d like to convert it to run locally using Ollama. I found the GGUF version of the model here:&lt;/p&gt; &lt;p&gt;🔗 &lt;a href="https://huggingface.co/Mungert/Qwen2.5-VL-3B-Instruct-GGUF"&gt;&lt;code&gt;Mungert/Qwen2.5-VL-3B-Instruct-GGUF&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I want to load this model in Ollama for local inference. However, I’m a bit stuck on how to properly structure and configure everything to make this work.&lt;/p&gt; &lt;h1&gt;Here's what I have:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;My fine-tuned model is based on Qwen2.5 VL 3B.&lt;/li&gt; &lt;li&gt;I downloaded the &lt;code&gt;.gguf&lt;/code&gt; mmproj model files from the Hugging Face repo above.&lt;/li&gt; &lt;li&gt;I have converted the main file into '.gguf' model files.&lt;/li&gt; &lt;li&gt;I have Ollama installed and running successfully (tested with other models like LLaMA, Mistral, etc.).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I need help with:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;How do I properly create a &lt;code&gt;Modelfile&lt;/code&gt; for this Qwen2.5-VL-3B-Instruct model?&lt;/li&gt; &lt;li&gt;Do I need any special preprocessing or metadata configuration?&lt;/li&gt; &lt;li&gt;Are there known limitations when using vision-language GGUF models in Ollama?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any guidance or example &lt;code&gt;Modelfile&lt;/code&gt; structure would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DSN_CV"&gt; /u/DSN_CV &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m9mw82/how_to_convert_finetuned_qwen_25_vl_3b_model_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m9mw82/how_to_convert_finetuned_qwen_25_vl_3b_model_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m9mw82/how_to_convert_finetuned_qwen_25_vl_3b_model_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-26T07:03:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ma9wss</id>
    <title>Help with setting a global timeout default or adding the timeout parameter to brave AI chat</title>
    <updated>2025-07-27T01:29:01+00:00</updated>
    <author>
      <name>/u/Super-Chip-6714</name>
      <uri>https://old.reddit.com/user/Super-Chip-6714</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to use brave browsers inbuilt AI chat server to use a model im hosting with ollama on the same machine.&lt;br /&gt; But it doesnt have the correct parameters to set timeout.&lt;a href="https://i.imgur.com/oPGq5NS.png"&gt; looks like this&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Other than figuring that out, I was thinking I could just set the global default to whatever I want. But I dont know where that config is stored.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Super-Chip-6714"&gt; /u/Super-Chip-6714 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ma9wss/help_with_setting_a_global_timeout_default_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ma9wss/help_with_setting_a_global_timeout_default_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ma9wss/help_with_setting_a_global_timeout_default_or/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-27T01:29:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9rswu</id>
    <title>Which is the best for coding?</title>
    <updated>2025-07-26T12:12:37+00:00</updated>
    <author>
      <name>/u/Terabaccha</name>
      <uri>https://old.reddit.com/user/Terabaccha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im new to ollama so Im bit confused. I'm using it on my laptop with weaker gpu (rtx 4050 6gb). Which is the best that I can use for coding and Ide integration?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terabaccha"&gt; /u/Terabaccha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m9rswu/which_is_the_best_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m9rswu/which_is_the_best_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m9rswu/which_is_the_best_for_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-26T12:12:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ma47kk</id>
    <title>Any good models for coding (Python and JS) to run on a 16 GB 5080?</title>
    <updated>2025-07-26T21:01:10+00:00</updated>
    <author>
      <name>/u/ajmusic15</name>
      <uri>https://old.reddit.com/user/ajmusic15</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So far, I can run models such as Qwen3-30B-A3B on IQ3_XXS at 90-110 tk/s. I can also run Devstral Small and Mistral Small 3.2 on IQ3_XXS and Q3_K_L at ~48 tk/s in 60K context.&lt;/p&gt; &lt;p&gt;I was trying to run Deepseek Coder V2 Lite, but no matter how hard I try, it won't start, and Gemma is memory-hungry.&lt;/p&gt; &lt;p&gt;Update: Qwen3-30B-A3B run at ~144 tk/s&lt;/p&gt; &lt;p&gt;Update 2: Looks like I'm going to stick with Devstral Small 2507, it's a 24B model that I can run with 90K of context at a modest 38-50 tk/s with no offload. I wish there was a version of QW3-30B-A3B with 128K (without YARN Scaling) especially for programming, since that model is running at ~145tk/s.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajmusic15"&gt; /u/ajmusic15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ma47kk/any_good_models_for_coding_python_and_js_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ma47kk/any_good_models_for_coding_python_and_js_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ma47kk/any_good_models_for_coding_python_and_js_to_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-26T21:01:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mai9fz</id>
    <title>What max model you can run locally on today's Linux laptops?</title>
    <updated>2025-07-27T09:42:04+00:00</updated>
    <author>
      <name>/u/jcubic</name>
      <uri>https://old.reddit.com/user/jcubic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I plan to buy a new laptop, because my 7 years old Dell is starting to show its age. I wanted to have something that will make me able to run bigger local models with Ollama.&lt;/p&gt; &lt;p&gt;What is the biggest model you can run locally with a laptop. Or what kind of model you're able to run yourself? Best if you use Linux. And I would like to use other things on my computer, I don't want the model to consume all available resources.&lt;/p&gt; &lt;p&gt;I'm particularly interested in models that can write code, and can be used with Agentic code writing tools, that I wanted to try.&lt;/p&gt; &lt;p&gt;I'm using Linux, and right the status of AMD NPU, that I wanted to purchase, in Ollama is unknown. It seems that &lt;a href="https://www.phoronix.com/news/AMD-Linux-RT-Preview-Ryzen-AI"&gt;Linux supports AMD NPU from version 6.14&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jcubic"&gt; /u/jcubic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mai9fz/what_max_model_you_can_run_locally_on_todays/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mai9fz/what_max_model_you_can_run_locally_on_todays/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mai9fz/what_max_model_you_can_run_locally_on_todays/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-27T09:42:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ma2sty</id>
    <title>Now you can pull LLMs directly from the browser (works both Ollama and huggingface models)</title>
    <updated>2025-07-26T20:01:02+00:00</updated>
    <author>
      <name>/u/Sea-Reception-2697</name>
      <uri>https://old.reddit.com/user/Sea-Reception-2697</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ma2sty/now_you_can_pull_llms_directly_from_the_browser/"&gt; &lt;img alt="Now you can pull LLMs directly from the browser (works both Ollama and huggingface models)" src="https://external-preview.redd.it/yp9kcsLE0b5eEfAA34Kw6OoIldEMhA7bxI4wXZrUv18.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c8fd5d3f489a2b1f80cbf70e36067b43588bc88" title="Now you can pull LLMs directly from the browser (works both Ollama and huggingface models)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a extension that Allows you to use your LLM from any page on the browser, now I added the capability of pulling and deleting models directly from the browser&lt;/p&gt; &lt;p&gt;If you want to help me or star my project here is the link (100% open-source):&lt;br /&gt; &lt;a href="https://github.com/Aletech-Solutions/XandAI-Extension"&gt;https://github.com/Aletech-Solutions/XandAI-Extension&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Reception-2697"&gt; /u/Sea-Reception-2697 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ma2sty/now_you_can_pull_llms_directly_from_the_browser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ma2sty/now_you_can_pull_llms_directly_from_the_browser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ma2sty/now_you_can_pull_llms_directly_from_the_browser/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-26T20:01:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1manvne</id>
    <title>Qwen3 235B 2507 adding its own questions to mine, and thinking despite being Instruct model?</title>
    <updated>2025-07-27T14:36:43+00:00</updated>
    <author>
      <name>/u/MrMattSz</name>
      <uri>https://old.reddit.com/user/MrMattSz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;Have been slowly trying to build up my daily computer and getting more experienced with running local llm models before I go nuts on a dedicated box for me and the family.&lt;/p&gt; &lt;p&gt;Wanted to try something a bit more up there (have been on Llama 3.3 70B Ablated for a while), so have been trying to run Qwen3-235B-2507 Instruct (tried Thinking too, but had pretty much the same issues).&lt;/p&gt; &lt;p&gt;System Specs:&lt;br /&gt; -Windows 11 - 24H2&lt;br /&gt; -i9-12900K&lt;br /&gt; -128gb DDR5-5200 RAM&lt;br /&gt; -RTX 4090&lt;br /&gt; -Samsung 990 Pro SSD&lt;br /&gt; -OpenWebUI for Interface - 0.6.18&lt;br /&gt; -Ollama to run the model - 0.9.6&lt;/p&gt; &lt;p&gt;Have gotten the best T/S (4.17) with:&lt;br /&gt; -unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF - IQ4_XS&lt;br /&gt; -Stop Sequence - &amp;quot;&amp;lt;|im_start|&amp;gt;&amp;quot;,&amp;quot;&amp;lt;|im_end|&amp;gt;&amp;quot;&lt;br /&gt; -top_k - 20&lt;br /&gt; -top_p - 0.8&lt;br /&gt; -min_p - 0&lt;br /&gt; -presence_penalty - 1&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Main two issues I run into, when I do an initial question, Qwen starts by adding it's own question, and then proceeds as though that was part of my question:&lt;/p&gt; &lt;p&gt;Are you familiar with Schrödinger's cat? And how it implies that reality is not set until it’s observed?&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;The second issue I'm noticing is it appears to be thinking before providing it's answer. This is the updated instruct model which isn't supposed to think? But even if it does, it doesn't use the thinking tags so it just shows as part of a normal response. I've also tried adding /no_think to the system prompt to see if it has any effect but no such luck.&lt;/p&gt; &lt;p&gt;Can I get any advice or recommendations for what I should be doing differently? (aside from not running Windows haha, will do that with the dedicated box)&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMattSz"&gt; /u/MrMattSz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1manvne/qwen3_235b_2507_adding_its_own_questions_to_mine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1manvne/qwen3_235b_2507_adding_its_own_questions_to_mine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1manvne/qwen3_235b_2507_adding_its_own_questions_to_mine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-27T14:36:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mapgrj</id>
    <title>How to Make AI Agents Collaborate with ACP (Agent Communication Protocol)</title>
    <updated>2025-07-27T15:41:41+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mapgrj/how_to_make_ai_agents_collaborate_with_acp_agent/"&gt; &lt;img alt="How to Make AI Agents Collaborate with ACP (Agent Communication Protocol)" src="https://external-preview.redd.it/MXDpTBzv6lKzmG6f-1q4qryOhuwTPxMubw3Trkf7Zas.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b39ee9a6080e139b574789fd5880534a4fa886f" title="How to Make AI Agents Collaborate with ACP (Agent Communication Protocol)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=fABcNHKVqYM&amp;amp;list=PLp01ObP3udmq2quR-RfrX4zNut_t_kNot&amp;amp;index=23"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mapgrj/how_to_make_ai_agents_collaborate_with_acp_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mapgrj/how_to_make_ai_agents_collaborate_with_acp_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-27T15:41:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1macmip</id>
    <title>Claude Code Alternative Recommendations?</title>
    <updated>2025-07-27T03:51:47+00:00</updated>
    <author>
      <name>/u/VashyTheNexian</name>
      <uri>https://old.reddit.com/user/VashyTheNexian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, I'm a self-hosting noob looking for recommendations for good self-hosted/foss/local/private/etc alternative to Claude Code's CLI tool. I recently started using at work and am blown away by how good it is. Would love to have something similar for myself. I have a 12GB VRAM RTX 3060 GPU with Ollama running in a docker container.&lt;/p&gt; &lt;p&gt;I haven't done extensive research to be honest, but I did try searching for a bit in general. I found a tool called Aider that was similar that I tried installing and using. It was okay, not as polished as Claude Code imo (and had a lot of, imo, poor choices for default settings; e.g. auto commit to git and not asking for permission first before editing files).&lt;/p&gt; &lt;p&gt;Anyway, I'm going to keep searching - I've come across a few articles with recommendations but I thought I'd ask here since you folks probably are more in line with my personal philosophy/requirements than some random articles (probably written by some AI itself) recommending tools. Otherwise, I'm going to have to go through these lists and try out the ones that look interesting and potentially liter my system with useless tools lol.&lt;/p&gt; &lt;p&gt;Thanks in advance for any pointers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VashyTheNexian"&gt; /u/VashyTheNexian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1macmip/claude_code_alternative_recommendations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1macmip/claude_code_alternative_recommendations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1macmip/claude_code_alternative_recommendations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-27T03:51:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1maa0uh</id>
    <title>It’s been a month since a new Ollama “official” model post. Anyone have any news on when we’ll see support for all the new SOTA models dropping lately?</title>
    <updated>2025-07-27T01:34:53+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Love Ollama, huge fan, but lately it kinda feels like they aren’t keeping up feature parity with LMStudio or Llama.cpp changes. The last few weeks we’ve seen models being released left and right, but I’ve found myself pulling more and more from HF or random Ollama user repos because Ollama hasn’t had any model releases since Mistral Small 3.2. Is this by design? Are they trying to push us towards HF for model downloads now or is the team just too busy?&lt;/p&gt; &lt;p&gt;Again, not trying to throw shade or anything, I know the Ollama team doesn’t owe us anything, just hoping all is well and that we start to see official support for some of the new SOTA open source models being released on the daily over the last few weeks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1maa0uh/its_been_a_month_since_a_new_ollama_official/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1maa0uh/its_been_a_month_since_a_new_ollama_official/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1maa0uh/its_been_a_month_since_a_new_ollama_official/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-27T01:34:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1maps05</id>
    <title>Why isn't ollama using gpu?</title>
    <updated>2025-07-27T15:54:12+00:00</updated>
    <author>
      <name>/u/Routine_Author961</name>
      <uri>https://old.reddit.com/user/Routine_Author961</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1maps05/why_isnt_ollama_using_gpu/"&gt; &lt;img alt="Why isn't ollama using gpu?" src="https://b.thumbs.redditmedia.com/jhUINj4kU2710ij8qu1xfLoOTQv42kP_NTES0B0xCNw.jpg" title="Why isn't ollama using gpu?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys!&lt;/p&gt; &lt;p&gt;i'm trying to run a local server with fedora and open web ui.&lt;/p&gt; &lt;p&gt;doenloaded ollama and openmwebui and everything works great, i have nvidia drivers and cuda installed but every tme i run models i see 100% use of the cpu. I want them to run on my gpu, how can I change it? would love your help thank you!!!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gjfgshs7vfff1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a13357f17e92a84718a66903e0356eb8fd7ecf1a"&gt;https://preview.redd.it/gjfgshs7vfff1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a13357f17e92a84718a66903e0356eb8fd7ecf1a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Routine_Author961"&gt; /u/Routine_Author961 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1maps05/why_isnt_ollama_using_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1maps05/why_isnt_ollama_using_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1maps05/why_isnt_ollama_using_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-27T15:54:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1magiqm</id>
    <title>Open source AI presentation generator with custom layouts support for custom presentation design</title>
    <updated>2025-07-27T07:46:45+00:00</updated>
    <author>
      <name>/u/goodboydhrn</name>
      <uri>https://old.reddit.com/user/goodboydhrn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1magiqm/open_source_ai_presentation_generator_with_custom/"&gt; &lt;img alt="Open source AI presentation generator with custom layouts support for custom presentation design" src="https://preview.redd.it/f45w62s4gdff1.gif?width=640&amp;amp;crop=smart&amp;amp;s=47cf8ebe3ee03b45923e23f2ba6ddd49f5982ebc" title="Open source AI presentation generator with custom layouts support for custom presentation design" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Presenton, the open source AI presentation generator that can run locally over Ollama.&lt;/p&gt; &lt;p&gt;Presenton now supports custom AI layouts. Create custom templates with HTML, Tailwind and Zod for schema. Then, use it to create presentations over AI.&lt;/p&gt; &lt;p&gt;We've added a lot more improvements with this release on Presenton:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Stunning in-built layouts to create AI presentations with&lt;/li&gt; &lt;li&gt;Custom HTML layouts/ themes/ templates&lt;/li&gt; &lt;li&gt;Workflow to create custom templates for developers&lt;/li&gt; &lt;li&gt;API support for custom templates&lt;/li&gt; &lt;li&gt;Choose text and image models separately giving much more flexibility&lt;/li&gt; &lt;li&gt;Better support for local llama&lt;/li&gt; &lt;li&gt;Support for external SQL database if you want to deploy for enterprise use (you don't need our permission. apache 2.0, remember! )&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can learn more about how to create custom layouts here: &lt;a href="https://docs.presenton.ai/tutorial/create-custom-presentation-layouts"&gt;https://docs.presenton.ai/tutorial/create-custom-presentation-layouts&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We'll soon release template vibe-coding guide.(I recently vibe-coded a stunning template within an hour.)&lt;/p&gt; &lt;p&gt;Do checkout and try out github if you haven't: &lt;a href="https://github.com/presenton/presenton"&gt;https://github.com/presenton/presenton&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if you have any feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goodboydhrn"&gt; /u/goodboydhrn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f45w62s4gdff1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1magiqm/open_source_ai_presentation_generator_with_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1magiqm/open_source_ai_presentation_generator_with_custom/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-27T07:46:45+00:00</published>
  </entry>
</feed>
