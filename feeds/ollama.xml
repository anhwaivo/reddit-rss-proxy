<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-05T22:51:29+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mhvfm0</id>
    <title>Expose port</title>
    <updated>2025-08-05T01:03:47+00:00</updated>
    <author>
      <name>/u/Far_Satisfaction6405</name>
      <uri>https://old.reddit.com/user/Far_Satisfaction6405</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hay I am new to ollama and I have a Ubuntu machine with it installed and I been trying to expose my ollama api but when I do it as localhost:11434 or 0.0.0.0:11434 it works but the moment I try my servers ip 1.1.1.1:11434 it refuses to connect error any ideas how to fix. I followed everything for install to a t and it is taking the port when I check&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far_Satisfaction6405"&gt; /u/Far_Satisfaction6405 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhvfm0/expose_port/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhvfm0/expose_port/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mhvfm0/expose_port/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T01:03:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mh9jac</id>
    <title>Is this the best value machine to run Local LLMs?</title>
    <updated>2025-08-04T10:14:19+00:00</updated>
    <author>
      <name>/u/optimism0007</name>
      <uri>https://old.reddit.com/user/optimism0007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mh9jac/is_this_the_best_value_machine_to_run_local_llms/"&gt; &lt;img alt="Is this the best value machine to run Local LLMs?" src="https://preview.redd.it/4m8omr1w9zgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc0a9517363a39bcaaf77f833193098a6e7ae1dd" title="Is this the best value machine to run Local LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/optimism0007"&gt; /u/optimism0007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4m8omr1w9zgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mh9jac/is_this_the_best_value_machine_to_run_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mh9jac/is_this_the_best_value_machine_to_run_local_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-04T10:14:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mif6k5</id>
    <title>A qestion.</title>
    <updated>2025-08-05T17:16:45+00:00</updated>
    <author>
      <name>/u/warmarduk</name>
      <uri>https://old.reddit.com/user/warmarduk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello... if this is not the right place to ask such question i apologize. I found in my garage my old &amp;quot;toaster&amp;quot;: i5 4570k 16gbram and a rx470-4gb. Can i run any local models on this old junk? Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/warmarduk"&gt; /u/warmarduk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mif6k5/a_qestion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mif6k5/a_qestion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mif6k5/a_qestion/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T17:16:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mifr38</id>
    <title>hallucinations - model specific ?</title>
    <updated>2025-08-05T17:37:16+00:00</updated>
    <author>
      <name>/u/rh4beakyd</name>
      <uri>https://old.reddit.com/user/rh4beakyd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;set gemma3 up and basically every answer has just been not only wildly incorrect but the model has stuck to it's guns and continued being wrong when challenged.&lt;/p&gt; &lt;p&gt;example - best books for RAG implementation using python. model listed three books, none of which exist. gave links to github project which didnt exist, apparently developed by either someone who doesnt exist or ( at a push ) a top coach of a US ladies basketball team. on multiple challenges it flipped from github to git lab, then back to git hub - this all continued a few times before I just gave up.&lt;/p&gt; &lt;p&gt;are they all needing medication or is Gemma3 just 'special' ?&lt;/p&gt; &lt;p&gt;ta&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rh4beakyd"&gt; /u/rh4beakyd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mifr38/hallucinations_model_specific/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mifr38/hallucinations_model_specific/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mifr38/hallucinations_model_specific/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T17:37:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mifs86</id>
    <title>Help! How to remove a model using MacOS app?</title>
    <updated>2025-08-05T17:38:23+00:00</updated>
    <author>
      <name>/u/lost_in_that_moment</name>
      <uri>https://old.reddit.com/user/lost_in_that_moment</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lost_in_that_moment"&gt; /u/lost_in_that_moment &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mifs86/help_how_to_remove_a_model_using_macos_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mifs86/help_how_to_remove_a_model_using_macos_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mifs86/help_how_to_remove_a_model_using_macos_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T17:38:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mifym9</id>
    <title>gpt-oss:20b ollama needs a newer version but i just updated to the latest one?</title>
    <updated>2025-08-05T17:44:54+00:00</updated>
    <author>
      <name>/u/veryhasselglad</name>
      <uri>https://old.reddit.com/user/veryhasselglad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;ollama pull gpt-oss:20b&lt;/p&gt; &lt;p&gt;pulling manifest &lt;/p&gt; &lt;p&gt;Error: pull model manifest: 412: &lt;/p&gt; &lt;p&gt;The model you are attempting to pull requires a newer version of Ollama.&lt;/p&gt; &lt;p&gt;Please download the latest version at:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;https://ollama.com/download&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/veryhasselglad"&gt; /u/veryhasselglad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mifym9/gptoss20b_ollama_needs_a_newer_version_but_i_just/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mifym9/gptoss20b_ollama_needs_a_newer_version_but_i_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mifym9/gptoss20b_ollama_needs_a_newer_version_but_i_just/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T17:44:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mibizg</id>
    <title>Cosine Similarity on Llama 3.2 model</title>
    <updated>2025-08-05T14:58:58+00:00</updated>
    <author>
      <name>/u/thewiirocks</name>
      <uri>https://old.reddit.com/user/thewiirocks</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm testing the embedding functionality to get a feel for working with it. But the results I'm getting aren't making much sense and I'm hoping someone can explain what's going on.&lt;/p&gt; &lt;p&gt;I have the following document for lookup:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;The sky is blue because of a magic spell cast by the space wizard Obi-Wan Kenobi&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;My expectation would be that this would be fairly close to the question:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Why is the sky blue?&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;(Yes, the results are hilarious when you convince Llama to roll with it. 😉)&lt;/p&gt; &lt;p&gt;I would expect to get a cosine distance relatively close to 1.0, such as 0.7 - 0.8. But what I actually get is 0.35399102976301283. Which seems pretty dang far away from the question!&lt;/p&gt; &lt;p&gt;Worse yet, the following document:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Under the sea, under the sea! Down where it's wetter, down where it's better, take it from meeee!!!&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;...computes as 0.45021770805463773. CLOSER to &lt;em&gt;&amp;quot;Why is the sky blue?&amp;quot;&lt;/em&gt; than the actual answer to why the sky is blue!&lt;/p&gt; &lt;p&gt;Digging further, I find that the cosine similarity between &lt;em&gt;&amp;quot;Why Is the sky blue?&amp;quot;&lt;/em&gt; and &lt;em&gt;&amp;quot;The sky is blue&amp;quot;&lt;/em&gt; is 0.418049006847794. Which makes no sense to me.&lt;/p&gt; &lt;p&gt;Am I misunderstanding something here or is this a bad example where I'm fighting the model's knowledge about why the sky is blue?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thewiirocks"&gt; /u/thewiirocks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mibizg/cosine_similarity_on_llama_32_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mibizg/cosine_similarity_on_llama_32_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mibizg/cosine_similarity_on_llama_32_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T14:58:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1miib62</id>
    <title>Up-to-date models. How can we know?</title>
    <updated>2025-08-05T19:10:01+00:00</updated>
    <author>
      <name>/u/Visible_Importance68</name>
      <uri>https://old.reddit.com/user/Visible_Importance68</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1miib62/uptodate_models_how_can_we_know/"&gt; &lt;img alt="Up-to-date models. How can we know?" src="https://b.thumbs.redditmedia.com/ejufC56qbiQjTOhJKcdBIuYyhGioAvoXUudQLRLjnpA.jpg" title="Up-to-date models. How can we know?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I would really like to know how to determine a model's most recent training date. For example, in the image I uploaded, the model responds by stating it was updated in 2023. If there were a way to download models where we could see the exact training date, I would prefer to download the most recently updated version instead of an older one. I really appreciate any help you can provide.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ns4enyeb29hf1.png?width=3902&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=164d9036b4f88dad2d7a8376d43a31c617177265"&gt;https://preview.redd.it/ns4enyeb29hf1.png?width=3902&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=164d9036b4f88dad2d7a8376d43a31c617177265&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Visible_Importance68"&gt; /u/Visible_Importance68 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miib62/uptodate_models_how_can_we_know/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miib62/uptodate_models_how_can_we_know/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1miib62/uptodate_models_how_can_we_know/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T19:10:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1miel5x</id>
    <title>Building a basic AI bot using Ollama, Angular and Node.js (Beginners )</title>
    <updated>2025-08-05T16:55:18+00:00</updated>
    <author>
      <name>/u/iamsausi</name>
      <uri>https://old.reddit.com/user/iamsausi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1miel5x/building_a_basic_ai_bot_using_ollama_angular_and/"&gt; &lt;img alt="Building a basic AI bot using Ollama, Angular and Node.js (Beginners )" src="https://external-preview.redd.it/0aUhnG-aPIcbdebztln4-6BC3XuZX3qsNFXppAhm1x8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e484a9e25e25eee60dd1ac867cdf5ed6fcbfc4a" title="Building a basic AI bot using Ollama, Angular and Node.js (Beginners )" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamsausi"&gt; /u/iamsausi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/p/629f25f52687"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miel5x/building_a_basic_ai_bot_using_ollama_angular_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1miel5x/building_a_basic_ai_bot_using_ollama_angular_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T16:55:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mimuiz</id>
    <title>Ollama app is weak.</title>
    <updated>2025-08-05T22:03:33+00:00</updated>
    <author>
      <name>/u/VictorCTavernari</name>
      <uri>https://old.reddit.com/user/VictorCTavernari</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I read about websearch, I thought it was for all models with agents approach, but, unfortunately, it is just for the GPT-OSS and it doesn't work well. I asked &amp;quot;Qual a temperatura em Resende RJ?&amp;quot; which means to check the weather in a Brazil city, and it is thinking for more than 3 minutes to just check the weather and I stopped it before, so I didn't get any result from that.&lt;/p&gt; &lt;p&gt;Bad, bad, bad, bad..&lt;/p&gt; &lt;p&gt;Ollama App has the same interface as OpenAI App, so my guess is that was developed by them.&lt;/p&gt; &lt;p&gt;Ps. I like the Ollama project... I contributed with my tentative of fine-tuning models &lt;a href="https://ollama.com/tavernari/git-commit-message"&gt;https://ollama.com/tavernari/git-commit-message&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VictorCTavernari"&gt; /u/VictorCTavernari &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mimuiz/ollama_app_is_weak/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mimuiz/ollama_app_is_weak/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mimuiz/ollama_app_is_weak/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T22:03:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mimw05</id>
    <title>gpt-oss:20b crashes: CUDA illegal memory access on Ollama 0.11.0</title>
    <updated>2025-08-05T22:05:10+00:00</updated>
    <author>
      <name>/u/Diegam</name>
      <uri>https://old.reddit.com/user/Diegam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;br /&gt; I’m trying to run the new &lt;code&gt;gpt-oss:20b&lt;/code&gt; model on Ollama (v0.11.0), but it crashes immediately with a CUDA illegal memory access error.&lt;/p&gt; &lt;p&gt;&lt;code&gt; ollama run gpt-oss:20b \&amp;gt;&amp;gt;&amp;gt; Hi Error: model runner has unexpectedly stopped, this may be due to resource limitations or an internal error, check ollama server logs for details &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Here’s the relevant part of the log:&lt;br /&gt; CUDA error: an illegal memory access was encountered&lt;br /&gt; current device: 0, in function ggml_cuda_mul_mat_id at &lt;a href="http://ggml-cuda.cu:2052"&gt;ggml-cuda.cu:2052&lt;/a&gt;&lt;br /&gt; SIGSEGV: segmentation violation&lt;/p&gt; &lt;p&gt;&lt;code&gt; CUDA error: an illegal memory access was encountered current device: 0, in function ggml\_cuda\_mul\_mat\_id at //ml/backend/ggml/ggml/src/ggml-cuda/ggml-cuda.cu:2052 cudaMemcpyAsync(ids\_host.data(), ids-&amp;gt;data, ggml\_nbytes(ids), cudaMemcpyDeviceToHost, stream) //ml/backend/ggml/ggml/src/ggml-cuda/ggml-cuda.cu:77: CUDA error SIGSEGV: segmentation violation PC=0x76964e1ebea7 m=11 sigcode=1 addr=0x204803f90 signal arrived during cgo execution goroutine 7 gp=0xc000582e00 m=11 mp=0xc000580808 \[syscall\]: runtime.cgocall(0x593306cf7180, 0xc001ad7a58) &lt;/code&gt;&lt;/p&gt; &lt;p&gt;My setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;NVIDIA GPU with 24 GB VRAM (rtx3090)&lt;/li&gt; &lt;li&gt;Driver: 560.28.03&lt;/li&gt; &lt;li&gt;CUDA: 12.6&lt;/li&gt; &lt;li&gt;OS: [Ubuntu Server 24.04]&lt;/li&gt; &lt;li&gt;Ollama version: 0.11.0&lt;/li&gt; &lt;li&gt;Other models like &lt;code&gt;llama3&lt;/code&gt;.1, qwen3:* work perfectly&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Diegam"&gt; /u/Diegam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mimw05/gptoss20b_crashes_cuda_illegal_memory_access_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mimw05/gptoss20b_crashes_cuda_illegal_memory_access_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mimw05/gptoss20b_crashes_cuda_illegal_memory_access_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T22:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi63i6</id>
    <title>Any plans to support image generation models in ollama?</title>
    <updated>2025-08-05T11:01:45+00:00</updated>
    <author>
      <name>/u/sh_tomer</name>
      <uri>https://old.reddit.com/user/sh_tomer</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sh_tomer"&gt; /u/sh_tomer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mi63i6/any_plans_to_support_image_generation_models_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mi63i6/any_plans_to_support_image_generation_models_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mi63i6/any_plans_to_support_image_generation_models_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T11:01:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mij9gu</id>
    <title>Open AI GPT-OSS:20b is bullshit</title>
    <updated>2025-08-05T19:45:48+00:00</updated>
    <author>
      <name>/u/Embarrassed-Way-1350</name>
      <uri>https://old.reddit.com/user/Embarrassed-Way-1350</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have just tried GPT-OSS:20b on my machine. This is the stupidest COT MOE model I have ever interacted with. Open AI chose to shit on the open-source community by releasing this abomination of a model.&lt;/p&gt; &lt;p&gt;Cannot perform basic arithmetic reasoning tasks, Thinks too much, and thinking traits remind me of deepseek-distill:70b, Would have been a great model 3 generations ago. As of today there are a ton of better models out there GLM is a far better alternative. Do not even try this model, Pure shit spray dried into fine powder.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed-Way-1350"&gt; /u/Embarrassed-Way-1350 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mij9gu/open_ai_gptoss20b_is_bullshit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mij9gu/open_ai_gptoss20b_is_bullshit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mij9gu/open_ai_gptoss20b_is_bullshit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T19:45:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mifzoc</id>
    <title>gpt-oss OpenAI’s open-weight models</title>
    <updated>2025-08-05T17:45:55+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ollama.com/library/gpt-oss"&gt;https://ollama.com/library/gpt-oss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;OpenAI’s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases. &lt;/p&gt; &lt;p&gt;Ollama partners with OpenAI to bring its latest state-of-the-art open weight models to Ollama. The two models, 20B and 120B, bring a whole new local chat experience, and are designed for powerful reasoning, agentic tasks, and versatile developer use cases.&lt;/p&gt; &lt;p&gt;ollama run gpt-oss:20b ollama run gpt-oss:120b&lt;/p&gt; &lt;p&gt;Edit: v0.11.0 required &lt;a href="https://github.com/ollama/ollama/releases/tag/v0.11.0"&gt;https://github.com/ollama/ollama/releases/tag/v0.11.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit2: v0.11.2 fix crash &lt;a href="https://github.com/ollama/ollama/releases/tag/v0.11.2"&gt;https://github.com/ollama/ollama/releases/tag/v0.11.2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mifzoc/gptoss_openais_openweight_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mifzoc/gptoss_openais_openweight_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mifzoc/gptoss_openais_openweight_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T17:45:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi7f4l</id>
    <title>llama3.2-vision prompt for OCR</title>
    <updated>2025-08-05T12:09:24+00:00</updated>
    <author>
      <name>/u/vir_db</name>
      <uri>https://old.reddit.com/user/vir_db</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to get llama3.2-vision act like an OCR system, in order to transcribe the text inside an image.&lt;/p&gt; &lt;p&gt;The source image is like the page of a book, or a image-only PDF. The text is not handwritten, however I cannot find a working combination of system/user prompt that just report the full text in the image, without adding notes or information about what the image look like. Sometimes the model return the text, but with notes and explanation, sometimes the model return (with the same prompt, often) a lot of strange nonsense character sequences. I tried both simple prompts like&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Extract all text from the image and return it as markdown.\n Do not describe the image or add extra text.\n Only return the text found in the image. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and more complex ones like&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;You are a text extraction expert. Your task is to analyze the provided image and extract all visible text with maximum accuracy. Organize the extracted text into a structured Markdown format. Follow these rules:\n\n 1. Headers: If a section of the text appears larger, bold, or like a heading, format it as a Markdown header (#, ##, or ###).\n 2. Lists: Format bullets or numbered items using Markdown syntax.\n 3. Tables: Use Markdown table format.\n 4. Paragraphs: Keep normal text blocks as paragraphs.\n 5. Emphasis: Use _italics_ and **bold** where needed.\n 6. Links: Format links like [text](url).\n Ensure the extracted text mirrors the document\’s structure and formatting.\n Provide only the transcription without any additional comments.&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But none of them is working as expected. Somebody have ideas? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vir_db"&gt; /u/vir_db &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mi7f4l/llama32vision_prompt_for_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mi7f4l/llama32vision_prompt_for_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mi7f4l/llama32vision_prompt_for_ocr/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T12:09:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mil1lu</id>
    <title>GPT-OSS no response</title>
    <updated>2025-08-05T20:52:47+00:00</updated>
    <author>
      <name>/u/No_Thing8294</name>
      <uri>https://old.reddit.com/user/No_Thing8294</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded the new GPT-OSS 12B in Ollama. But I do not get an answer in the chat. Using it via OpenWebUI with my Ollama results in some strange error (some function is not declared 🤷‍♂️)&lt;/p&gt; &lt;p&gt;Someone running into similar issues?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Thing8294"&gt; /u/No_Thing8294 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mil1lu/gptoss_no_response/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mil1lu/gptoss_no_response/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mil1lu/gptoss_no_response/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T20:52:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1minhkr</id>
    <title>Hardware upgrade advice for local Ollama + Frigate GenAI + HA Voice (Budget: $1000–$1200)</title>
    <updated>2025-08-05T22:29:50+00:00</updated>
    <author>
      <name>/u/ResponsibleSyrup6556</name>
      <uri>https://old.reddit.com/user/ResponsibleSyrup6556</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m hoping to get some advice from folks who are running Ollama and similar local AI setups. I’m trying to figure out what my next hardware move should be. My budget is around $1000 to $1200.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unraid as the host OS&lt;/li&gt; &lt;li&gt;Running Ollama, Frigate, and Kokoro (TTS) in Docker&lt;/li&gt; &lt;li&gt;Home Assistant OS in a VM&lt;/li&gt; &lt;li&gt;Hardware: &lt;ul&gt; &lt;li&gt;ASUS PRIME B450M-A II&lt;/li&gt; &lt;li&gt;Ryzen 5 4500 (6-core)&lt;/li&gt; &lt;li&gt;64GB DDR4 3600MHz (2x32GB)&lt;/li&gt; &lt;li&gt;Old 4GB NVIDIA GPU (will get the model later)&lt;/li&gt; &lt;li&gt;500W PSU (also old)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything runs surprisingly well using cloud services for the AI stuff (Frigate GenAI and HA Voice Assistant). I also have a &lt;strong&gt;Coral TPU&lt;/strong&gt;, which is handling Frigate’s object detection just fine, so I’m not relying on CPU or GPU for that part.&lt;/p&gt; &lt;p&gt;What I want to do now is &lt;strong&gt;run everything locally&lt;/strong&gt; — LLMs for Frigate summaries and Home Assistant Voice responses, and eventually try out some image models like LLaVA or Qwen-image. Right now I’m stuck using smaller 3B models, and I want to go beyond that without relying on the cloud.&lt;/p&gt; &lt;p&gt;That said, I’m still pretty new to all this AI/LLM stuff. I don’t see myself training models or messing with massive 70B setups. I just want something solid that gives me room to grow and experiment. I like tinkering and learning — I’m not a pro, but I’m having fun.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here are the options I’m considering:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;GPU upgrade in my current system&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Probably need to upgrade the PSU too (850W or so)&lt;/li&gt; &lt;li&gt;Looking at something like an &lt;strong&gt;RTX 5060 Ti 16GB&lt;/strong&gt; or a used &lt;strong&gt;RTX 3090&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Might be the cheapest and simplest path&lt;/li&gt; &lt;li&gt;But: is it worth it, or am I throwing a Porsche engine in a Pinto?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Used gaming PC on Facebook Marketplace&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;i9-12900K&lt;/li&gt; &lt;li&gt;ASUS ROG Z690-E&lt;/li&gt; &lt;li&gt;RTX 3090 (24GB)&lt;/li&gt; &lt;li&gt;32GB DDR5&lt;/li&gt; &lt;li&gt;Dual NVMe Gen4 SSDs&lt;/li&gt; &lt;li&gt;850W Gold PSU&lt;/li&gt; &lt;li&gt;Corsair liquid cooler&lt;/li&gt; &lt;li&gt;RGB fans (some are busted, but whatever)&lt;/li&gt; &lt;li&gt;Asking $1300 but open to offers — might be able to grab it for $1000–$1200 cash&lt;/li&gt; &lt;li&gt;Would definitely want to test it before buying&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mac Mini M4 (24–32GB)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;$999 base model&lt;/li&gt; &lt;li&gt;Tempting since we use Apple gear in the house&lt;/li&gt; &lt;li&gt;But the lack of upgradability is a concern. Once you hit a ceiling, that’s it — unless you want to build a cluster.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Jetson Orin NX from Seeed Studio&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;reComputer J4012 w/ Jetson Orin NX 16GB&lt;/li&gt; &lt;li&gt;Around $950&lt;/li&gt; &lt;li&gt;Seems built for AI, but maybe more focused on edge CV stuff?&lt;/li&gt; &lt;li&gt;Not sure it’s great for LLMs, especially beyond 3B models.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ryzen AI NUC (HX 370)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;GMKtec EVO-X1 with 64GB RAM and 1TB SSD is ~$1029&lt;/li&gt; &lt;li&gt;Nice compact form factor&lt;/li&gt; &lt;li&gt;But I haven’t seen many people running real-world LLM workloads on these yet. Is the NPU actually useful today?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;So here’s what I’m trying to figure out:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is it worth throwing a nice GPU in my current system and rolling with it?&lt;/li&gt; &lt;li&gt;Or is it smarter to grab that used 3090 rig and start fresh?&lt;/li&gt; &lt;li&gt;Are the edge devices like the Jetson or AI NUCs actually viable for LLMs?&lt;/li&gt; &lt;li&gt;Anyone running a Mac Mini M4 for local LLMs — how far can it go?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any advice would be appreciated, especially from folks doing similar things with Frigate, HA Voice, or Ollama. I'm not trying to max out some giant model — just want to stop relying on the cloud and have enough horsepower to explore and grow.&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleSyrup6556"&gt; /u/ResponsibleSyrup6556 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1minhkr/hardware_upgrade_advice_for_local_ollama_frigate/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1minhkr/hardware_upgrade_advice_for_local_ollama_frigate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1minhkr/hardware_upgrade_advice_for_local_ollama_frigate/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T22:29:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mihumw</id>
    <title>gpt-oss-20b WAY too slow on M1 MacBook Pro (2020)</title>
    <updated>2025-08-05T18:53:10+00:00</updated>
    <author>
      <name>/u/rafa3790543246789</name>
      <uri>https://old.reddit.com/user/rafa3790543246789</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I just saw the new open-weight models that OpenAI released, and I wanted to try them on my M1 MacBook Pro (from 2020, 16GB). OpenAI said the gpt-oss-20b model can run on most desktops and laptops, but I'm having trouble running it on my Mac.&lt;/p&gt; &lt;p&gt;When I try to run gpt-oss-20b (after closing every app, making room for the 13GB model), it just takes ages to generate single tokens. It's definitely not usable and cannot run on my Mac.&lt;/p&gt; &lt;p&gt;Curious to know if anyone had similar experiences.&lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rafa3790543246789"&gt; /u/rafa3790543246789 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mihumw/gptoss20b_way_too_slow_on_m1_macbook_pro_2020/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mihumw/gptoss20b_way_too_slow_on_m1_macbook_pro_2020/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mihumw/gptoss20b_way_too_slow_on_m1_macbook_pro_2020/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T18:53:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mim247</id>
    <title>Running OpenAI’s new GPT‑OSS‑20B locally with Ollama</title>
    <updated>2025-08-05T21:32:06+00:00</updated>
    <author>
      <name>/u/Sumanth_077</name>
      <uri>https://old.reddit.com/user/Sumanth_077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI released two new open‑source models, GPT‑OSS‑120B and GPT‑OSS‑20B, focused on reasoning tasks.&lt;/p&gt; &lt;p&gt;We put together a short walkthrough showing how to:&lt;/p&gt; &lt;p&gt;• Pull and run GPT‑OSS‑20B locally using Ollama&lt;br /&gt; • Expose it through a standard OpenAI‑compatible API with Local Runners&lt;/p&gt; &lt;p&gt;This makes it easier to experiment with the model locally while still accessing it programmatically via an API.&lt;/p&gt; &lt;p&gt;Watch the walkthrough &lt;a href="https://youtu.be/TfS2p8LZYBE"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sumanth_077"&gt; /u/Sumanth_077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mim247/running_openais_new_gptoss20b_locally_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mim247/running_openais_new_gptoss20b_locally_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mim247/running_openais_new_gptoss20b_locally_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T21:32:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mijg4m</id>
    <title>Ollama removed the link to GitHub</title>
    <updated>2025-08-05T19:52:42+00:00</updated>
    <author>
      <name>/u/waescher</name>
      <uri>https://old.reddit.com/user/waescher</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mijg4m/ollama_removed_the_link_to_github/"&gt; &lt;img alt="Ollama removed the link to GitHub" src="https://preview.redd.it/bk6utn9v99hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cac4875cf871bdc5c59cca05a4063a0f9a11ae0b" title="Ollama removed the link to GitHub" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama added a link to their paid cloud &amp;quot;Turbo&amp;quot; subscription and removed the link to their GitHub repository. I don't like where this is going ...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waescher"&gt; /u/waescher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bk6utn9v99hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mijg4m/ollama_removed_the_link_to_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mijg4m/ollama_removed_the_link_to_github/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T19:52:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mig4bu</id>
    <title>OpenAI Open Source Models Released!</title>
    <updated>2025-08-05T17:50:35+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI has unleashed two new open‑weight models:&lt;br /&gt; - &lt;strong&gt;GPT‑OSS‑120b (120B parameters)&lt;/strong&gt;&lt;br /&gt; - &lt;strong&gt;GPT‑OSS‑20b (20B parameters)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;It's their first to be actually downloadable and customizable models since GPT‑2 in 2019. It has a &lt;strong&gt;GPL‑friendly license&lt;/strong&gt; (Apache 2.0), allows free modification and commercial use. They're also Chain‑of‑thought enabled, supports code generation, browsing, and agent use via OpenAI API&lt;/p&gt; &lt;p&gt;&lt;a href="https://openai.com/open-models/"&gt;https://openai.com/open-models/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mig4bu/openai_open_source_models_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mig4bu/openai_open_source_models_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mig4bu/openai_open_source_models_released/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T17:50:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi9zex</id>
    <title>Ollama's new app makes using local AI LLMs on your Windows 11 PC a breeze — no more need to chat in the terminal</title>
    <updated>2025-08-05T14:00:04+00:00</updated>
    <author>
      <name>/u/rkhunter_</name>
      <uri>https://old.reddit.com/user/rkhunter_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mi9zex/ollamas_new_app_makes_using_local_ai_llms_on_your/"&gt; &lt;img alt="Ollama's new app makes using local AI LLMs on your Windows 11 PC a breeze — no more need to chat in the terminal" src="https://external-preview.redd.it/BhemsEH58Hdd5wlj8RIXGA-DVEqtIWcM1c0-0glb5O8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2963f63bbb5ff18828ac8e0ab68dbd3e71338ac3" title="Ollama's new app makes using local AI LLMs on your Windows 11 PC a breeze — no more need to chat in the terminal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rkhunter_"&gt; /u/rkhunter_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.windowscentral.com/artificial-intelligence/ollamas-new-app-makes-using-local-ai-llms-on-your-windows-11-pc-a-breeze-no-more-need-to-chat-in-the-terminal"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mi9zex/ollamas_new_app_makes_using_local_ai_llms_on_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mi9zex/ollamas_new_app_makes_using_local_ai_llms_on_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T14:00:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1miis70</id>
    <title>Why does web search require an ollama account? That's pretty lame</title>
    <updated>2025-08-05T19:27:47+00:00</updated>
    <author>
      <name>/u/Anxious-Bottle7468</name>
      <uri>https://old.reddit.com/user/Anxious-Bottle7468</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1miis70/why_does_web_search_require_an_ollama_account/"&gt; &lt;img alt="Why does web search require an ollama account? That's pretty lame" src="https://preview.redd.it/2x8cvu7i59hf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=514e04fbc3a672eba9c3d94cf3db31d0455e246d" title="Why does web search require an ollama account? That's pretty lame" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxious-Bottle7468"&gt; /u/Anxious-Bottle7468 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2x8cvu7i59hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miis70/why_does_web_search_require_an_ollama_account/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1miis70/why_does_web_search_require_an_ollama_account/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T19:27:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi6rqk</id>
    <title>Built a lightweight picker that finds the right Ollama model for your hardware (surprisingly useful!)</title>
    <updated>2025-08-05T11:37:10+00:00</updated>
    <author>
      <name>/u/pzarevich</name>
      <uri>https://old.reddit.com/user/pzarevich</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mi6rqk/built_a_lightweight_picker_that_finds_the_right/"&gt; &lt;img alt="Built a lightweight picker that finds the right Ollama model for your hardware (surprisingly useful!)" src="https://external-preview.redd.it/Ymhhd3RucXBzNmhmMUg6vjulbZIeLFSczOGwuN3tGRr8QNKtfAF_eyKX2Orf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfef104774b195d4b72123ead92016e96e4c61d5" title="Built a lightweight picker that finds the right Ollama model for your hardware (surprisingly useful!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pzarevich"&gt; /u/pzarevich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/st0gjoqps6hf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mi6rqk/built_a_lightweight_picker_that_finds_the_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mi6rqk/built_a_lightweight_picker_that_finds_the_right/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T11:37:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mig251</id>
    <title>gpt-oss now available on Ollama</title>
    <updated>2025-08-05T17:48:20+00:00</updated>
    <author>
      <name>/u/john_rage</name>
      <uri>https://old.reddit.com/user/john_rage</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mig251/gptoss_now_available_on_ollama/"&gt; &lt;img alt="gpt-oss now available on Ollama" src="https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417" title="gpt-oss now available on Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI has published their opensource gpt model on Ollama.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/john_rage"&gt; /u/john_rage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/library/gpt-oss"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mig251/gptoss_now_available_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mig251/gptoss_now_available_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T17:48:20+00:00</published>
  </entry>
</feed>
