<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-06-06T03:36:10+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1l2pyrd</id>
    <title>bug in qwen 3 chat template?</title>
    <updated>2025-06-03T22:52:43+00:00</updated>
    <author>
      <name>/u/Expensive-Apricot-25</name>
      <uri>https://old.reddit.com/user/Expensive-Apricot-25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I noticed that when ever qwen 3 calls tools, it thinks that the user called the tool, or is talking to the model. I looked into the chat template and it turns out that for a tool response, it is labeled as a user message:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{{- else if eq .Role &amp;quot;tool&amp;quot; }}&amp;lt;|im_start|&amp;gt;user &amp;lt;tool_response&amp;gt; {{ .Content }} &amp;lt;/tool_response&amp;gt;&amp;lt;|im_end|&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I looked at the chat template for the official qwen page on hugging face, and the `user` marker is not there for a tool response.&lt;/p&gt; &lt;p&gt;Is this a bug? or is this intended behavior?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expensive-Apricot-25"&gt; /u/Expensive-Apricot-25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2pyrd/bug_in_qwen_3_chat_template/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2pyrd/bug_in_qwen_3_chat_template/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l2pyrd/bug_in_qwen_3_chat_template/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-03T22:52:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2m8t7</id>
    <title>starting off using Ollama</title>
    <updated>2025-06-03T20:20:30+00:00</updated>
    <author>
      <name>/u/Available-Ad1878</name>
      <uri>https://old.reddit.com/user/Available-Ad1878</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey I'm a masters student working in clinical research as a side project while im in school. &lt;/p&gt; &lt;p&gt;one of the post docs in my lab told me to use Ollama to process our data and output graphs + written papers as well. the way they do this is basically by uploading huge files of data that we have extracted from surgery records (looking at times vs outcomes vs costs of materials etc.) alongside papers on similar topics and previous papers from the lab to their Ollama and then prompting it heavily until they get what they need. some of the data is HIPAA protected as well, so im rly too sure about how this works but they told me that its fine to use it as long as its locally hosted and not in the cloud. &lt;/p&gt; &lt;p&gt;im working on an M2 MacBook Air right now, so let me know if that is going to restrict my usage heavily. but im here just to learn more about what model I should be using and how to go about that. thanks!&lt;/p&gt; &lt;p&gt;I also have to do a ton of reading (journal articles) so if theres models that could help with that in terms of giving me summaries or being able to recall anything I need, that would be great too. I know this is a lot but thanks again!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available-Ad1878"&gt; /u/Available-Ad1878 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2m8t7/starting_off_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2m8t7/starting_off_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l2m8t7/starting_off_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-03T20:20:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2i71o</id>
    <title>Is anyone productively using Aider and Ollama together?</title>
    <updated>2025-06-03T17:37:58+00:00</updated>
    <author>
      <name>/u/chanfle12</name>
      <uri>https://old.reddit.com/user/chanfle12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was experimenting with Aider yesterday and discovered a potential bug with its Ollama support. It appears the available models are hardcoded, and Aider isn't fetching the list of models directly from Ollama. This makes it seem broken.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Aider-AI/aider/issues/3081"&gt;https://github.com/Aider-AI/aider/issues/3081&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Is anyone else successfully using Aider with Ollama? If not, what alternatives are people using for local LLM integration?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chanfle12"&gt; /u/chanfle12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2i71o/is_anyone_productively_using_aider_and_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2i71o/is_anyone_productively_using_aider_and_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l2i71o/is_anyone_productively_using_aider_and_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-03T17:37:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2xq0p</id>
    <title>Locally downloading Qwen pretrained weights for finetuning</title>
    <updated>2025-06-04T05:35:49+00:00</updated>
    <author>
      <name>/u/hendy0</name>
      <uri>https://old.reddit.com/user/hendy0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm trying to load the pretrained weights of LLMs (Qwen2.5-0.5B for now) into a custom model architecture I created manually. I'm trying to mimic &lt;a href="https://github.com/rasbt/LLMs-from-scratch/blob/c278745aff419ae6c1d6409ca4279aa57ea749e4/ch06/01_main-chapter-code/previous_chapters.py#L251"&gt;this code&lt;/a&gt;. However, I wasn't able to find the checkpoints of the pretrained model online. Could someone help me with that or refer me to a place where I can load the pretrained weights? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hendy0"&gt; /u/hendy0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2xq0p/locally_downloading_qwen_pretrained_weights_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2xq0p/locally_downloading_qwen_pretrained_weights_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l2xq0p/locally_downloading_qwen_pretrained_weights_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-04T05:35:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2r6kb</id>
    <title>Building an extension that lets you try ANY clothing on with AI. Open sourcing it...</title>
    <updated>2025-06-03T23:48:11+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l2r6kb/building_an_extension_that_lets_you_try_any/"&gt; &lt;img alt="Building an extension that lets you try ANY clothing on with AI. Open sourcing it..." src="https://external-preview.redd.it/Y3IyeXJ5Zmt1czRmMZEFLO9nCJikC7mtBpPcIQAr59c4sK2P034UkenC8j1x.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=577a0bce4c774caddeb043ad3e991d8b058d5489" title="Building an extension that lets you try ANY clothing on with AI. Open sourcing it..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/iy5wdxekus4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2r6kb/building_an_extension_that_lets_you_try_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l2r6kb/building_an_extension_that_lets_you_try_any/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-03T23:48:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1l38ign</id>
    <title>geekom a6 mini PC 32gb ram *internal gpu* r7 6800h</title>
    <updated>2025-06-04T15:29:03+00:00</updated>
    <author>
      <name>/u/3d_printing_kid</name>
      <uri>https://old.reddit.com/user/3d_printing_kid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ok so what is the best llm i could run at maybe 5 tokens/second? also how do i make it use my integrated graphics?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3d_printing_kid"&gt; /u/3d_printing_kid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l38ign/geekom_a6_mini_pc_32gb_ram_internal_gpu_r7_6800h/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l38ign/geekom_a6_mini_pc_32gb_ram_internal_gpu_r7_6800h/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l38ign/geekom_a6_mini_pc_32gb_ram_internal_gpu_r7_6800h/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-04T15:29:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2vm2g</id>
    <title>AI Runner v4.11.0: web browsing with contextually aware agent + search via duckduckgo</title>
    <updated>2025-06-04T03:32:55+00:00</updated>
    <author>
      <name>/u/w00fl35</name>
      <uri>https://old.reddit.com/user/w00fl35</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yesterday I showed you a preview of the web browser tool I was working on for my AI Runner application. Today I have released it with &lt;a href="https://github.com/Capsize-Games/airunner/releases/tag/v4.11.0"&gt;v4.11.0 - you can see the full release notes here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Some key changes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The LLM can search via duckduckgo without an API key. The search can be extended to include other search engines (and will be in upcoming releases).&lt;/li&gt; &lt;li&gt;Integrated web browser with private browsing, bookmarks, history, keyboard controls and most importantly a contextually aware LLM&lt;/li&gt; &lt;li&gt;Completely reworked the chat area which was very sluggish in previous versions. Now its fast.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There are some known bugs&lt;/p&gt; &lt;ul&gt; &lt;li&gt;chat doesn't always show up on first load&lt;/li&gt; &lt;li&gt;browser is in its alpha stage - i tried to make it robust, but it probably needs some polish&lt;/li&gt; &lt;li&gt;the LLM will screw up a lot right now&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll be working on everything heavily over the next couple of days and will update you as I release. If you want a more stable LLM experience use a version prior to v4.11.0, but polishing the agent and giving it more tools is my primary focus for the next few days.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;AI Runner is a desktop application I built with Python. It allows you to run AI models offline on your own hardware. You can generate images, have voice conversations, create custom bots, and much more.&lt;/p&gt; &lt;p&gt;Check it out and if you like what you see, consider supporting the project by giving me a star.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Capsize-Games/airunner"&gt;https://github.com/Capsize-Games/airunner&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w00fl35"&gt; /u/w00fl35 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2vm2g/ai_runner_v4110_web_browsing_with_contextually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2vm2g/ai_runner_v4110_web_browsing_with_contextually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l2vm2g/ai_runner_v4110_web_browsing_with_contextually/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-04T03:32:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3ms1a</id>
    <title>Question would a mini PC with a ryzen 7 5700u with a radeon rx vega and 32 gb of ram work for ai llm? something like a quantitized Claude?</title>
    <updated>2025-06-05T01:23:27+00:00</updated>
    <author>
      <name>/u/Inside-Minute4184</name>
      <uri>https://old.reddit.com/user/Inside-Minute4184</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inside-Minute4184"&gt; /u/Inside-Minute4184 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3ms1a/question_would_a_mini_pc_with_a_ryzen_7_5700u/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3ms1a/question_would_a_mini_pc_with_a_ryzen_7_5700u/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l3ms1a/question_would_a_mini_pc_with_a_ryzen_7_5700u/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-05T01:23:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3b50r</id>
    <title>Guys, can we use any locally hosted LLM as a coding agent on CodeGPT VS ?</title>
    <updated>2025-06-04T17:11:12+00:00</updated>
    <author>
      <name>/u/Constantinos_bou</name>
      <uri>https://old.reddit.com/user/Constantinos_bou</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l3b50r/guys_can_we_use_any_locally_hosted_llm_as_a/"&gt; &lt;img alt="Guys, can we use any locally hosted LLM as a coding agent on CodeGPT VS ?" src="https://preview.redd.it/b4jyogdn0y4f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01ba3bfa95e877f9da00ce2d1d43ea17c5972825" title="Guys, can we use any locally hosted LLM as a coding agent on CodeGPT VS ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Constantinos_bou"&gt; /u/Constantinos_bou &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b4jyogdn0y4f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3b50r/guys_can_we_use_any_locally_hosted_llm_as_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l3b50r/guys_can_we_use_any_locally_hosted_llm_as_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-04T17:11:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3ismx</id>
    <title>Can I run NVILA-8B-Video</title>
    <updated>2025-06-04T22:16:20+00:00</updated>
    <author>
      <name>/u/bubukiki</name>
      <uri>https://old.reddit.com/user/bubukiki</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;Just started using ollama. Worked well for LLaVA:13B, but I want to test NVILA on some videos.&lt;/p&gt; &lt;p&gt;I did not find it on the ollama repo, I heard I can convert them from .safetensor to .gguf but the ollama.cpp did not work. Any leads?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bubukiki"&gt; /u/bubukiki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3ismx/can_i_run_nvila8bvideo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3ismx/can_i_run_nvila8bvideo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l3ismx/can_i_run_nvila8bvideo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-04T22:16:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3dozn</id>
    <title>PC or android Phone which is enough??</title>
    <updated>2025-06-04T18:50:40+00:00</updated>
    <author>
      <name>/u/_Ninefox_</name>
      <uri>https://old.reddit.com/user/_Ninefox_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Soo I have an old Athlon 3000G and a 8GB Stick, I need to buy the rest for a PC.&lt;/p&gt; &lt;p&gt;But I thought to maybe build a small budget AI PC.&lt;br /&gt; Question is, is it worth it?&lt;/p&gt; &lt;p&gt;Or is an android Smartphone with the &amp;quot;PocketPal AI&amp;quot; app more reasonable?&lt;/p&gt; &lt;p&gt;For context I want to be able to use the LLM offline and play around with it a bit (not much coding just learning with it and training it}&lt;/p&gt; &lt;p&gt;Let me guess a Laptop is the best solution? 🤣&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_Ninefox_"&gt; /u/_Ninefox_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3dozn/pc_or_android_phone_which_is_enough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3dozn/pc_or_android_phone_which_is_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l3dozn/pc_or_android_phone_which_is_enough/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-04T18:50:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1l36vlk</id>
    <title>What are some features missing from the Ollama API that you would like to see?</title>
    <updated>2025-06-04T14:24:13+00:00</updated>
    <author>
      <name>/u/TheBroseph69</name>
      <uri>https://old.reddit.com/user/TheBroseph69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I plan on building an improved API for Ollama that would have features not currently found in the Ollama API. What are some features you’d like to see?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheBroseph69"&gt; /u/TheBroseph69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l36vlk/what_are_some_features_missing_from_the_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l36vlk/what_are_some_features_missing_from_the_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l36vlk/what_are_some_features_missing_from_the_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-04T14:24:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3nuor</id>
    <title>Context window in python</title>
    <updated>2025-06-05T02:16:17+00:00</updated>
    <author>
      <name>/u/Gadrakmtg</name>
      <uri>https://old.reddit.com/user/Gadrakmtg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It there any way to set a context window with ollama python or any way to impliment it withough appending the last message to a history? How does the cli manage it without a great cost to performance?&lt;/p&gt; &lt;p&gt;Thank in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gadrakmtg"&gt; /u/Gadrakmtg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3nuor/context_window_in_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3nuor/context_window_in_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l3nuor/context_window_in_python/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-05T02:16:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3ex51</id>
    <title>What models can I run well with a 3060 12gb?</title>
    <updated>2025-06-04T19:38:42+00:00</updated>
    <author>
      <name>/u/TheBroseph69</name>
      <uri>https://old.reddit.com/user/TheBroseph69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Found a cheap 3060 for sale, thinking of picking it up. What would I be able to run (well)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheBroseph69"&gt; /u/TheBroseph69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3ex51/what_models_can_i_run_well_with_a_3060_12gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3ex51/what_models_can_i_run_well_with_a_3060_12gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l3ex51/what_models_can_i_run_well_with_a_3060_12gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-04T19:38:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2qdt2</id>
    <title>Ollama Video Editor</title>
    <updated>2025-06-03T23:11:21+00:00</updated>
    <author>
      <name>/u/AdamHYE</name>
      <uri>https://old.reddit.com/user/AdamHYE</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l2qdt2/ollama_video_editor/"&gt; &lt;img alt="Ollama Video Editor" src="https://preview.redd.it/9bqo8jd0os4f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7b0ba4f6ffdd7bcf5ef30581ec7a4a27d8affd60" title="Ollama Video Editor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Created an Ollama MCP to give ffmpeg’s advanced video/audio editing to an agent.&lt;/p&gt; &lt;p&gt;Runs 100% locally. React Vite frontend, Node Express mcp, Python Flask backend, simple Ollama agent. Scaffolded by Dyad.&lt;/p&gt; &lt;p&gt;When I’m ready to do sophisticated editing, I’ll wire this up to CrewAI. But if you just want to do single command requests, it’s solid. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/hyepartners-gmail/vibevideo-mcp"&gt;https://github.com/hyepartners-gmail/vibevideo-mcp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdamHYE"&gt; /u/AdamHYE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9bqo8jd0os4f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2qdt2/ollama_video_editor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l2qdt2/ollama_video_editor/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-03T23:11:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3z6tu</id>
    <title>Reccomandations on budget GPU</title>
    <updated>2025-06-05T13:27:53+00:00</updated>
    <author>
      <name>/u/Oridium_</name>
      <uri>https://old.reddit.com/user/Oridium_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am looking to create a local LLM on my machine but I am unsure on which GPU should I use since I am not that affiliated with the requirements. Currently I am using an NVIDIA RTX 3060 Ti with 8 GB of VRAM but I am looking to upgrade to an RX 6800 xt with 16GB of vram. I've heard that the CUDA cores on the nvidia gpus outperform any radeon counterparts in the same price range. Also, regarding general storage, what would be the general amount of storage i should allocate for it. Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Oridium_"&gt; /u/Oridium_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3z6tu/reccomandations_on_budget_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3z6tu/reccomandations_on_budget_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l3z6tu/reccomandations_on_budget_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-05T13:27:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3zhgh</id>
    <title>llama3.2:3b is also slightly crazy</title>
    <updated>2025-06-05T13:41:05+00:00</updated>
    <author>
      <name>/u/3d_printing_kid</name>
      <uri>https://old.reddit.com/user/3d_printing_kid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l3zhgh/llama323b_is_also_slightly_crazy/"&gt; &lt;img alt="llama3.2:3b is also slightly crazy" src="https://external-preview.redd.it/NzVwdmFvanUzNDVmMeb6P8aQ6QCzNmwawQBJ6zWy2AxVJFGMrDcwEXK5iJlO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0001e6317abbaf38e5eb54c6debeaabb6380b2c" title="llama3.2:3b is also slightly crazy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3d_printing_kid"&gt; /u/3d_printing_kid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/66f49oju345f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3zhgh/llama323b_is_also_slightly_crazy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l3zhgh/llama323b_is_also_slightly_crazy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-05T13:41:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3fcrw</id>
    <title>I made an LLM tool to let you search offline Wikipedia/StackExchange/DevDocs ZIM files (llm-tools-kiwix, works with Python &amp; LLM cli)</title>
    <updated>2025-06-04T19:56:05+00:00</updated>
    <author>
      <name>/u/mozanunal</name>
      <uri>https://old.reddit.com/user/mozanunal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I just released &lt;a href="https://github.com/mozanunal/llm-tools-kiwix"&gt;&lt;code&gt;llm-tools-kiwix&lt;/code&gt;&lt;/a&gt;, a plugin for the &lt;a href="https://llm.datasette.io/"&gt;&lt;code&gt;llm&lt;/code&gt; CLI&lt;/a&gt; and Python that lets LLMs read and search offline ZIM archives (i.e., Wikipedia, DevDocs, StackExchange, and more) &lt;strong&gt;totally offline&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt;&lt;br /&gt; A lot of local LLM use cases could benefit from RAG using big knowledge bases, but most solutions require network calls. Kiwix makes it possible to have huge websites (Wikipedia, StackExchange, etc.) stored as &lt;code&gt;.zim&lt;/code&gt; files on your disk. Now you can let your LLM access those—no Internet needed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What does it do?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Discovers your ZIM files&lt;/strong&gt; (in the cwd or a folder via &lt;code&gt;KIWIX_HOME&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Exposes tools so the LLM can search articles or read full content&lt;/li&gt; &lt;li&gt;Works on the command line or from Python (supports GPT-4o, ollama, Llama.cpp, etc via the &lt;code&gt;llm&lt;/code&gt; tool)&lt;/li&gt; &lt;li&gt;No cloud or browser needed, just pure local retrieval&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Example use-case:&lt;/strong&gt;&lt;br /&gt; Say you have &lt;code&gt;wikipedia_en_all_nopic_2023-10.zim&lt;/code&gt; downloaded and want your LLM to answer questions using it:&lt;/p&gt; &lt;p&gt;&lt;code&gt; llm install llm-tools-kiwix # (one-time setup) llm -m ollama:llama3 --tool kiwix_search_and_collect \ &amp;quot;Summarize notable attempts at human-powered flight from Wikipedia.&amp;quot; \ --tools-debug &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Or use the Docker/DevDocs ZIMs for local developer documentation search.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to try:&lt;/strong&gt; 1. Download some ZIM files from &lt;a href="https://download.kiwix.org/zim/"&gt;https://download.kiwix.org/zim/&lt;/a&gt; 2. Put them in your project dir, or set &lt;code&gt;KIWIX_HOME&lt;/code&gt; 3. &lt;code&gt;llm install llm-tools-kiwix&lt;/code&gt; 4. Use tool mode as above!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Open source, Apache 2.0.&lt;/strong&gt;&lt;br /&gt; Repo + docs: &lt;a href="https://github.com/mozanunal/llm-tools-kiwix"&gt;https://github.com/mozanunal/llm-tools-kiwix&lt;/a&gt;&lt;br /&gt; PyPI: &lt;a href="https://pypi.org/project/llm-tools-kiwix/"&gt;https://pypi.org/project/llm-tools-kiwix/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think! Would love feedback, bug reports, or ideas for more offline tools.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mozanunal"&gt; /u/mozanunal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3fcrw/i_made_an_llm_tool_to_let_you_search_offline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3fcrw/i_made_an_llm_tool_to_let_you_search_offline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l3fcrw/i_made_an_llm_tool_to_let_you_search_offline/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-04T19:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3ug2z</id>
    <title>Ollama on an old server using openVINO? How does it work?</title>
    <updated>2025-06-05T09:01:00+00:00</updated>
    <author>
      <name>/u/Palova98</name>
      <uri>https://old.reddit.com/user/Palova98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt; &lt;p&gt;I have a 15 yo server that runs ollama with some models. &lt;/p&gt; &lt;p&gt;Let's make it short: it takes about 5 minutes to do anything. &lt;/p&gt; &lt;p&gt;I heard of some &amp;quot;middleware&amp;quot; for Intel CPUs called&lt;a href="https://docs.openvino.ai/2025/index.html"&gt; openVINO. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;My ollama instance runs on a docker container in a Ubuntu proxmox VM.&lt;/p&gt; &lt;p&gt;Anyone had any experience with this sort of optimization for old hardware? &lt;/p&gt; &lt;p&gt;Apparently you &lt;a href="https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/download.html?PACKAGE=OPENVINO_BASE&amp;amp;VERSION=v_2025_1_0&amp;amp;OP_SYSTEM=LINUX&amp;amp;DISTRIBUTION=DOCKER"&gt;CAN run openVINO in a docker container&lt;/a&gt;, but does it still work with ollama if ollama is on a different container? Does it work if it is on the main VM instead? What about PyTorch? &lt;/p&gt; &lt;p&gt;I have found &lt;a href="https://blog.openvino.ai/blog-posts/ollama-integrated-with-openvino-accelerating-deepseek-inference"&gt;THIS &lt;/a&gt;article somewhere but it does not explain much, or whatever it explains is beyond my knowledge (basically none). It makes you &amp;quot;create&amp;quot; a model compatible with ollama or something similar. &lt;/p&gt; &lt;p&gt;Sorry for my lack of knowledge, I'm doing R&amp;amp;D for work and they don't give me more than &amp;quot;we must make it run on our hardware, not buying new gpu&amp;quot;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Palova98"&gt; /u/Palova98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3ug2z/ollama_on_an_old_server_using_openvino_how_does/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3ug2z/ollama_on_an_old_server_using_openvino_how_does/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l3ug2z/ollama_on_an_old_server_using_openvino_how_does/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-05T09:01:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1l36kvb</id>
    <title>smollm is crazy</title>
    <updated>2025-06-04T14:11:39+00:00</updated>
    <author>
      <name>/u/3d_printing_kid</name>
      <uri>https://old.reddit.com/user/3d_printing_kid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l36kvb/smollm_is_crazy/"&gt; &lt;img alt="smollm is crazy" src="https://external-preview.redd.it/NG9mN3N2ZGw0eDRmMYC1oXT879drMGhz7A_iST_bdDJ62X2-qbCshqC67I28.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=faaff70ac5a371991a4c0aa1609505f081603776" title="smollm is crazy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i was bored one day so i dicided to run smollm 135 m parameters. here is a video of the result:&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3d_printing_kid"&gt; /u/3d_printing_kid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fesotwdl4x4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l36kvb/smollm_is_crazy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l36kvb/smollm_is_crazy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-04T14:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3rbac</id>
    <title>local models need a lot of hand holding when prompting ?</title>
    <updated>2025-06-05T05:30:50+00:00</updated>
    <author>
      <name>/u/AntelopeEntire9191</name>
      <uri>https://old.reddit.com/user/AntelopeEntire9191</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is it just me or does local models that are around the size of 14b just need a lot of hand holding when prompting them? like it requires you to be meticulous in the prompt otherwise the outputs ends up being lackluster. ik ollama released &lt;a href="https://ollama.com/blog/structured-outputs"&gt;https://ollama.com/blog/structured-outputs&lt;/a&gt; structured outputs that significantly helped from having to force the llm to have attention to detail to every sort of items such as spacing, missing commas, unnecessary syntax, but still this is annoying to have to hand hold. at times i think the extra cost of frontier models is just so much more worth that sort of already handle these edge cases for you? its just annoying and im just wonder im using these models wrong? my bullet point of instructions feels like its starting to become a never ending list and as a result only making the invoke time even longer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AntelopeEntire9191"&gt; /u/AntelopeEntire9191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3rbac/local_models_need_a_lot_of_hand_holding_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3rbac/local_models_need_a_lot_of_hand_holding_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l3rbac/local_models_need_a_lot_of_hand_holding_when/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-05T05:30:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3zpxd</id>
    <title>smollm is crazier (older version is worse)</title>
    <updated>2025-06-05T13:51:25+00:00</updated>
    <author>
      <name>/u/3d_printing_kid</name>
      <uri>https://old.reddit.com/user/3d_printing_kid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l3zpxd/smollm_is_crazier_older_version_is_worse/"&gt; &lt;img alt="smollm is crazier (older version is worse)" src="https://external-preview.redd.it/NHUycjJwenc1NDVmMZsBk0Wxe9S7tv9zfFAxZfkxM-dQVhjCUDDDFpUA1CGh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16c890621f1461a0c81e1346c489d3ee6889ad8d" title="smollm is crazier (older version is worse)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3d_printing_kid"&gt; /u/3d_printing_kid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eao3tqzw545f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3zpxd/smollm_is_crazier_older_version_is_worse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l3zpxd/smollm_is_crazier_older_version_is_worse/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-05T13:51:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1l4bcqd</id>
    <title>Any way to translate text from images with local AIs?</title>
    <updated>2025-06-05T21:35:33+00:00</updated>
    <author>
      <name>/u/iTrejoMX</name>
      <uri>https://old.reddit.com/user/iTrejoMX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to locally have something similar to sider.ai . I haven't been able to find anything that i can use for this use case or something similar. Anyone have any experience in extracting text from images and translating it? (optionally: putting translated text into the image to replace original text)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iTrejoMX"&gt; /u/iTrejoMX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l4bcqd/any_way_to_translate_text_from_images_with_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l4bcqd/any_way_to_translate_text_from_images_with_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l4bcqd/any_way_to_translate_text_from_images_with_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-05T21:35:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3tcon</id>
    <title>Open Source Alternative to Perplexity</title>
    <updated>2025-06-05T07:43:10+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with &lt;strong&gt;SurfSense&lt;/strong&gt;, it aims to be the open-source alternative to &lt;strong&gt;NotebookLM&lt;/strong&gt;, &lt;strong&gt;Perplexity&lt;/strong&gt;, or &lt;strong&gt;Glean&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent but connected to your personal external sources search engines (Tavily, LinkUp), Slack, Linear, Notion, YouTube, GitHub, Discord and more coming soon.&lt;/p&gt; &lt;p&gt;I'll keep this short—here are a few highlights of SurfSense:&lt;/p&gt; &lt;p&gt;📊 Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports &lt;strong&gt;150+ LLM's&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Supports local &lt;strong&gt;Ollama LLM's&lt;/strong&gt; or &lt;strong&gt;vLLM&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Supports &lt;strong&gt;6000+ Embedding Models&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt; &lt;li&gt;Uses &lt;strong&gt;Hierarchical Indices&lt;/strong&gt; (2-tiered RAG setup)&lt;/li&gt; &lt;li&gt;Combines &lt;strong&gt;Semantic + Full-Text Search&lt;/strong&gt; with &lt;strong&gt;Reciprocal Rank Fusion&lt;/strong&gt; (Hybrid Search)&lt;/li&gt; &lt;li&gt;Offers a &lt;strong&gt;RAG-as-a-Service API Backend&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Supports 50+ File extensions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🎙️ Podcasts&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)&lt;/li&gt; &lt;li&gt;Convert your chat conversations into engaging audio content&lt;/li&gt; &lt;li&gt;Support for multiple TTS providers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;ℹ️ &lt;strong&gt;External Sources&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Search engines (Tavily, LinkUp)&lt;/li&gt; &lt;li&gt;Slack&lt;/li&gt; &lt;li&gt;Linear&lt;/li&gt; &lt;li&gt;Notion&lt;/li&gt; &lt;li&gt;YouTube videos&lt;/li&gt; &lt;li&gt;GitHub&lt;/li&gt; &lt;li&gt;Discord&lt;/li&gt; &lt;li&gt;...and more on the way&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🔖 &lt;strong&gt;Cross-Browse&lt;/strong&gt;r Extension&lt;br /&gt; The SurfSense extension lets you save any dynamic webpage you like. Its main use case is capturing pages that are protected behind authentication.&lt;/p&gt; &lt;p&gt;Check out SurfSense on GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3tcon/open_source_alternative_to_perplexity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l3tcon/open_source_alternative_to_perplexity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l3tcon/open_source_alternative_to_perplexity/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-05T07:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1l4gcpx</id>
    <title>Building a Text Adventure Game with Persistent AI Agents Using Ollama</title>
    <updated>2025-06-06T01:33:36+00:00</updated>
    <author>
      <name>/u/cyb3rofficial</name>
      <uri>https://old.reddit.com/user/cyb3rofficial</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l4gcpx/building_a_text_adventure_game_with_persistent_ai/"&gt; &lt;img alt="Building a Text Adventure Game with Persistent AI Agents Using Ollama" src="https://a.thumbs.redditmedia.com/4aRIHqLn_KCKF9SuojGdg1FsS0SY9RMPI9GZj2OgtF4.jpg" title="Building a Text Adventure Game with Persistent AI Agents Using Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;! I've been working on a project that I think this community might find interesting - a locally-hosted text adventure game where the game it self is basically a craftable file system.&lt;/p&gt; &lt;h1&gt;What makes it special?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Every NPC is powered by Ollama&lt;/strong&gt; - Each agent has their own personality, persistent memory, and individual conversation contexts that survive between sessions&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Smart token management&lt;/strong&gt; - Uses dual models (I'm running &lt;code&gt;qwen3:8b&lt;/code&gt; for main conversations, &lt;code&gt;qwen3:4b&lt;/code&gt; for summaries) with automatic context compression when approaching limits&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Everything persists&lt;/strong&gt; - Agent memories are stored in CSV files, conversations in pickle files, and the entire world state can be saved/loaded with full backups&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Filesystem-based world&lt;/strong&gt; - Each folder is a location, each JSON file is an agent or item. Want to add a new NPC? Just drop a JSON file in a folder!&lt;/p&gt; &lt;h1&gt;Technical highlights:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Token-aware design&lt;/strong&gt;: Real-time monitoring with automatic compression before hitting limits&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Isolated agent contexts&lt;/strong&gt;: Each NPC maintains separate conversation history&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context sharing&lt;/strong&gt;: Agents can share experiences within the same location&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Complete privacy&lt;/strong&gt;: Everything runs locally, no external API calls&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Robust save system&lt;/strong&gt;: With automatic backups&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Quick example:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; /say alice Hello there! *wipes down a mug with practiced ease* Well hello there, stranger! Welcome to the Prancing Pony. What brings you to our little town? &amp;gt; /memory alice Alice's recent memories: Said: &amp;quot;Welcome to the tavern!&amp;quot;; Observed: &amp;quot;A new traveler arrived&amp;quot;; Felt: &amp;quot;Curious about newcomer&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The whole thing runs on local Ollama models, and I've tested it extensively with various model sizes. The token management system really shines - it automatically compresses contexts when needed while preserving important conversation history.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Models used: &lt;code&gt;qwen3:8b&lt;/code&gt; (main), &lt;code&gt;qwen3:4b&lt;/code&gt; (summary model)&lt;/li&gt; &lt;li&gt;Requires: Python 3.13, Ollama&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The summary model will take contextual stuff and try to make decent summaries of stuff happened.&lt;/p&gt; &lt;p&gt;You can use other models, but I've been liking qwen3. It's not too overwhelming and has that simplicity to it. (yes there is &amp;lt;think&amp;gt; suppression too, so you can enable or disable &amp;lt;think&amp;gt; tags in the outputs)&lt;/p&gt; &lt;p&gt;I plan on releasing it soon as a proof of concept on GitHub.&lt;/p&gt; &lt;p&gt;The entire thing is trying to make the people or monsters 'self aware' of their surroundings and other things. Context does matter and so does tokens more importantly the story, so the entire system is made up to help keep things in check via ranking systems.&lt;/p&gt; &lt;p&gt;The compression system uses a dual-model approach with smart token management:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Continuously monitors token usage for each agent's conversation context&lt;/li&gt; &lt;li&gt;When approaching 85% of model's token limit, automatically triggers compression&lt;/li&gt; &lt;li&gt;Uses smaller/faster model (qwen3:4b) to create intelligent summaries&lt;/li&gt; &lt;li&gt;Preserves recent messages (last 8 exchanges) in full detail for continuity&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Ranking/Priority system:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;HIGH PRIORITY:&lt;/strong&gt; Recent interactions, character personality traits, plot developments, relationship changes&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MEDIUM PRIORITY:&lt;/strong&gt; Emotional context, world state changes, important dialogue&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LOW PRIORITY:&lt;/strong&gt; Casual chatter, repetitive conversations, older small talk&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Example compression:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Before (7,500 tokens):&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Turn 1: &amp;quot;Hello Alice, I'm a traveling merchant&amp;quot; Turn 2: &amp;quot;Welcome! I run this tavern with my husband&amp;quot; Turn 3: &amp;quot;What goods do you sell?&amp;quot; Turn 4: &amp;quot;Mainly spices and cloth from the eastern kingdoms&amp;quot; ...40 more turns of detailed conversation... Turn 45: &amp;quot;The bandits have been troubling travelers lately&amp;quot; Turn 46: &amp;quot;I've noticed that too, very concerning&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;After compression (2,000 tokens):&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;SUMMARY: &amp;quot;Alice learned the player is a traveling merchant selling spices and cloth. They discussed her tavern business, shared concerns about recent bandit activity affecting travelers. Alice is friendly and trusting.&amp;quot; RECENT MESSAGES (last 8 turns preserved in full): Turn 39: &amp;quot;The weather has been strange lately&amp;quot; Turn 40: &amp;quot;Yes, unseasonably cold for this time of year&amp;quot; ... Turn 45: &amp;quot;The bandits have been troubling travelers lately&amp;quot; Turn 46: &amp;quot;I've noticed that too, very concerning&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt; Agent still knows you're a merchant, remembers the bandit discussion, maintains her personality, but saves 70% tokens. Conversation flows naturally without any &amp;quot;who are you again?&amp;quot; moments.&lt;/p&gt; &lt;p&gt;Yes, I know there are plenty of things like this that are way way way 10 fold better, but I'm trying to make it more fun and interactive dynamic and more creative and be able to have a full battle system and automated events, I've tried many other role play systems, but I haven't gotten that itch for full (scripted or unscripted) role and battle events. the code base is very messy right now, need to make it more readable and friendly to look at or improve upon. This took me like over 2 weeks to make, and I hope once it push it out to public, It pays off. Also need to make a documented guide on how to actually world build and have that more advanced touch to it. I might make a world editor or something easier to make but I want to release the main project first.&lt;/p&gt; &lt;p&gt;I'll be glad to answer any questions (or concerns) you may have, or requests (if its not already implemented that is.)&lt;/p&gt; &lt;p&gt;Everything will be open source, nothing hidden or behind a weird api or website. Fully 100% free &amp;amp; offline and on your system. &lt;/p&gt; &lt;p&gt;Also To Note; In the images, that starting box can be changed to your liking, so you can call it anything to give it that more personal touch. Also plan to make it 'portable' so you can just open an exe and not worry about installing python.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cyb3rofficial"&gt; /u/cyb3rofficial &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1l4gcpx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l4gcpx/building_a_text_adventure_game_with_persistent_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l4gcpx/building_a_text_adventure_game_with_persistent_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-06T01:33:36+00:00</published>
  </entry>
</feed>
