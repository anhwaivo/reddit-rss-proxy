<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-06-14T20:48:46+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1l9gvu5</id>
    <title>Run Ollama in your documents with Writeopia. Windows app now available!</title>
    <updated>2025-06-12T08:05:54+00:00</updated>
    <author>
      <name>/u/lehen01</name>
      <uri>https://old.reddit.com/user/lehen01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l9gvu5/run_ollama_in_your_documents_with_writeopia/"&gt; &lt;img alt="Run Ollama in your documents with Writeopia. Windows app now available!" src="https://external-preview.redd.it/dTdpM2pnNGhlZzZmMVhZcvYgeFNt2CRk4Y47xoCdZZ4Xui688StqpxFWpMYy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e9e4f38f67b08aa746c08a2a69827378720736a" title="Run Ollama in your documents with Writeopia. Windows app now available!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello hello. &lt;/p&gt; &lt;p&gt;Sometime ago, I shared my project Writeopia in &lt;a href="https://www.reddit.com/r/ollama/comments/1jdb4zm/i_created_a_text_editor_that_integrates_with/"&gt;this post&lt;/a&gt; and it had a super nice reception. Many users asked about the Windows app, because at that time, only macOS and Linux were available. &lt;/p&gt; &lt;p&gt;We are happy to announce that the Windows app is finally available. You can download it from the &lt;a href="https://apps.microsoft.com/detail/9NW5WL8NRM4H?hl=en-us&amp;amp;gl=NL&amp;amp;ocid=pdpshare"&gt;Windows Store&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;If you like the project, don't forget to star us on Github: &lt;a href="https://github.com/Writeopia/Writeopia"&gt;https://github.com/Writeopia/Writeopia&lt;/a&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lehen01"&gt; /u/lehen01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1i0njg4heg6f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9gvu5/run_ollama_in_your_documents_with_writeopia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l9gvu5/run_ollama_in_your_documents_with_writeopia/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-12T08:05:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1la6i0u</id>
    <title>[First Release!] Serene Pub - 0.1.0 Alpha - Linux/MacOS/Windows - Silly Tavern alternative</title>
    <updated>2025-06-13T03:57:34+00:00</updated>
    <author>
      <name>/u/doolijb</name>
      <uri>https://old.reddit.com/user/doolijb</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doolijb"&gt; /u/doolijb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1la6h3y"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1la6i0u/first_release_serene_pub_010_alpha/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1la6i0u/first_release_serene_pub_010_alpha/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-13T03:57:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1labu0o</id>
    <title>Building a pc for local llm (help needed)</title>
    <updated>2025-06-13T09:45:25+00:00</updated>
    <author>
      <name>/u/anirudhisonline</name>
      <uri>https://old.reddit.com/user/anirudhisonline</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anirudhisonline"&gt; /u/anirudhisonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1labraz/building_a_pc_for_local_llm_help_needed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1labu0o/building_a_pc_for_local_llm_help_needed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1labu0o/building_a_pc_for_local_llm_help_needed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-13T09:45:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9wvnr</id>
    <title>What's the best model for RAG with docs?</title>
    <updated>2025-06-12T20:23:29+00:00</updated>
    <author>
      <name>/u/Green-Ad-3964</name>
      <uri>https://old.reddit.com/user/Green-Ad-3964</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for the best model to use with llama.cpp or ollama on a RAG project. &lt;/p&gt; &lt;p&gt;I need it to never (ehm) allucinate and to be able to answer simple, plain questions about the docs both in a [yes/no] way and in a descriptive way, i.e. explaining something from the doc. &lt;/p&gt; &lt;p&gt;I have a 5090 so 32GB local memory. What's the best I could use? With or without reasoning? Is the more parameter the better for this task?&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Green-Ad-3964"&gt; /u/Green-Ad-3964 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9wvnr/whats_the_best_model_for_rag_with_docs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9wvnr/whats_the_best_model_for_rag_with_docs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l9wvnr/whats_the_best_model_for_rag_with_docs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-12T20:23:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1labojz</id>
    <title>Are there any small models (7B or smaller) that are good with German copywriting?</title>
    <updated>2025-06-13T09:34:42+00:00</updated>
    <author>
      <name>/u/n0nikk</name>
      <uri>https://old.reddit.com/user/n0nikk</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/n0nikk"&gt; /u/n0nikk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1labojz/are_there_any_small_models_7b_or_smaller_that_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1labojz/are_there_any_small_models_7b_or_smaller_that_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1labojz/are_there_any_small_models_7b_or_smaller_that_are/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-13T09:34:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9py3c</id>
    <title>üßô‚Äç‚ôÇÔ∏è I Built a Local AI Dungeon Master ‚Äì Meet Dungeo_ai (Open Source &amp; Powered by ollama)</title>
    <updated>2025-06-12T15:52:05+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l9py3c/i_built_a_local_ai_dungeon_master_meet_dungeo_ai/"&gt; &lt;img alt="üßô‚Äç‚ôÇÔ∏è I Built a Local AI Dungeon Master ‚Äì Meet Dungeo_ai (Open Source &amp;amp; Powered by ollama)" src="https://external-preview.redd.it/VKXieTbOFzGU2aZe9EDyviI58NBmBHkcxoJcwzIpt0A.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78f390b5728c07a17be669aafa3632a5c3408b7e" title="üßô‚Äç‚ôÇÔ∏è I Built a Local AI Dungeon Master ‚Äì Meet Dungeo_ai (Open Source &amp;amp; Powered by ollama)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1l9py3c/video/cswkxr8rpi6f1/player"&gt;https://reddit.com/link/1l9py3c/video/cswkxr8rpi6f1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey folks!&lt;br /&gt; I‚Äôve been building something I'm super excited to finally share:&lt;br /&gt; üé≤ &lt;a href="https://github.com/Laszlobeer/Dungeo_ai/tree/main"&gt;&lt;strong&gt;Dungeo_ai&lt;/strong&gt;&lt;/a&gt; ‚Äì a fully local, AI-powered Dungeon Master designed for immersive solo RPGs, worldbuilding, and roleplay.&lt;/p&gt; &lt;p&gt;This project it's free and for now it connect to ollama(llm) and alltalktts(tts) &lt;/p&gt; &lt;p&gt;üõ†Ô∏è What it can do:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üíª Runs entirely &lt;strong&gt;locally&lt;/strong&gt; (with support for Ollama )&lt;/li&gt; &lt;li&gt;üß† Persists memory, character state, and custom personalities&lt;/li&gt; &lt;li&gt;üìú Simulates D&amp;amp;D-like dialogue and encounters dynamically&lt;/li&gt; &lt;li&gt;üó∫Ô∏è Expands lore over time with each interaction&lt;/li&gt; &lt;li&gt;üßô Great for solo campaigns, worldbuilding, or even prototyping NPCs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It‚Äôs still early days, but it‚Äôs usable and growing. I‚Äôd love feedback, collab ideas, or even just to know what kind of characters &lt;em&gt;you‚Äôd&lt;/em&gt; throw into it.&lt;/p&gt; &lt;p&gt;Here‚Äôs the link again:&lt;br /&gt; üëâ &lt;a href="https://github.com/Laszlobeer/Dungeo_ai/tree/main"&gt;https://github.com/Laszlobeer/Dungeo_ai/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for checking it out‚Äîand if you give it a spin, let me know how your first AI encounter goes. üòÑ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9py3c/i_built_a_local_ai_dungeon_master_meet_dungeo_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9py3c/i_built_a_local_ai_dungeon_master_meet_dungeo_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l9py3c/i_built_a_local_ai_dungeon_master_meet_dungeo_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-12T15:52:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9v3lk</id>
    <title>Are there any good models of less than 8Gb we can trust for simple tasks?</title>
    <updated>2025-06-12T19:11:46+00:00</updated>
    <author>
      <name>/u/New_Cranberry_6451</name>
      <uri>https://old.reddit.com/user/New_Cranberry_6451</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been testing models with a very simple set of tests, things like &amp;quot;Write the word Atom reversed&amp;quot; and I am quite dissapointed with the results as almost no models I have tested (Gemma3, Qwen3, Qwen2.5 in their small versions around 4.7Gb or 8Gb in the case of Gemma3) got it right on the first try. I am wondering if I am using Ollama the right way. I have made a simple JS client to work against the API, nothing fancy, just the common things following the official documentation. Do you have any advise? Or am I directly wasting my time with small models? If small models can't handle something as trivial as this, is there any real application for them? I feel like the enterprise closed models are light years ahead of what is being released in the open source community...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Cranberry_6451"&gt; /u/New_Cranberry_6451 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9v3lk/are_there_any_good_models_of_less_than_8gb_we_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9v3lk/are_there_any_good_models_of_less_than_8gb_we_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l9v3lk/are_there_any_good_models_of_less_than_8gb_we_can/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-12T19:11:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9z4vh</id>
    <title>New Agent Creator with Observer AI üöÄ!</title>
    <updated>2025-06-12T21:56:31+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l9z4vh/new_agent_creator_with_observer_ai/"&gt; &lt;img alt="New Agent Creator with Observer AI üöÄ!" src="https://external-preview.redd.it/eTI4dHYzZXRpazZmMdAGXmZBeVc_QZpDk3TS6rDj0o4TMoRZ42ebNhgLEVSd.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d831b0b22e36f4d633ee6a0ca0f405b74f3c747a" title="New Agent Creator with Observer AI üöÄ!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey ollama family! first of all I wanted to thank you so much for your support and feedback on running ollama with ObserverAI! I'm super grateful for your support and i'll keep adding features! Here are some features i just added:&lt;br /&gt; * AI Agent Builder&lt;br /&gt; * Template Agent Builder&lt;br /&gt; * SMS message notifications&lt;br /&gt; * Camera input&lt;br /&gt; * Microphone input (still needs work)&lt;br /&gt; * Whatsapp message notifiaction (rolled back but coming soon!, still needs work, got Meta account flagged for spam hahaha)&lt;br /&gt; * Computer audio transcription (beta, coming soon!)&lt;/p&gt; &lt;p&gt;Please check it out at &lt;a href="http://app.observer-ai.com"&gt;app.observer-ai.com&lt;/a&gt;, the project is 100% Open Source, and you can run it locally! (inference with ollama and webapp) &lt;a href="http://github.com/Roy3838/Observer"&gt;github.com/Roy3838/Observer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks so much Ollama community! You guys are awesome, I hope you can check it out and give me feedback on what to add next!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ltagz5etik6f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9z4vh/new_agent_creator_with_observer_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l9z4vh/new_agent_creator_with_observer_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-12T21:56:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lafs6v</id>
    <title>Need help on RAG based project in legal domain.</title>
    <updated>2025-06-13T13:21:20+00:00</updated>
    <author>
      <name>/u/amitsingh80108</name>
      <uri>https://old.reddit.com/user/amitsingh80108</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I am currently learning RAG and trying to make domain specific RAG. &lt;/p&gt; &lt;p&gt;In legal domain the laws are very much similar and one word can change entire meaning. Hence the query from me is not able to retrieve the correct laws as I don't have knowledge of laws. &lt;/p&gt; &lt;p&gt;Instead I took case details, passed it to LLM and asked write 5 rag queries to retrieve relevant laws from vector database. &lt;/p&gt; &lt;p&gt;This seems to work at 50-60% accuracy. So I tried reranker and badly failed. Reranker reduced accuracy to 10-20%. I assume reranker may not be able to understand legal laws while reranking ? &lt;/p&gt; &lt;p&gt;Here I want some guidance from you all. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Am I doing correct thing ? &lt;/li&gt; &lt;li&gt;Chunk size I tried from 160 tokens till 500 tokens and above 400 tokens is what giving good accuracy. &lt;/li&gt; &lt;li&gt;Will fine tuning llm is of any use here? I am not sure if I train llm it will hallucinate or not. &lt;/li&gt; &lt;li&gt;Embeddings is from e5-large-instruct and it's the best in my testing. &lt;/li&gt; &lt;li&gt;If I want to host my LLM say Gemma 3 27B, how much ram it will take and also will there be OOM errors ? And what if multiple people use it at the same time will I see ram issues ? &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks guys. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amitsingh80108"&gt; /u/amitsingh80108 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lafs6v/need_help_on_rag_based_project_in_legal_domain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lafs6v/need_help_on_rag_based_project_in_legal_domain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lafs6v/need_help_on_rag_based_project_in_legal_domain/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-13T13:21:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1laowwp</id>
    <title>Transfer docker volume (chat data) to another machine?</title>
    <updated>2025-06-13T19:32:59+00:00</updated>
    <author>
      <name>/u/redpandafire</name>
      <uri>https://old.reddit.com/user/redpandafire</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently setup with Docker, Ollama, and Open Web-Ui. It's running the container and things are peachy. I now want to move my chats to another machine. &lt;/p&gt; &lt;p&gt;I tried this method where I used &amp;quot;docker image save&amp;quot; to a .tar file, transferred the file to target machine, reinstalled docker/ollama/web-ui and ran &amp;quot;docker image load&amp;quot;. The file created new images in my docker but loading into ollama/web-ui showed the chats were complete empty.&lt;/p&gt; &lt;p&gt;Problem, I have no idea where to isolate and save the web-ui chats from machine A to machine B.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redpandafire"&gt; /u/redpandafire &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1laowwp/transfer_docker_volume_chat_data_to_another/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1laowwp/transfer_docker_volume_chat_data_to_another/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1laowwp/transfer_docker_volume_chat_data_to_another/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-13T19:32:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lai0jn</id>
    <title>Planning a 7‚Äì8B Model Benchmark on 8GB GPU ‚Äî What Should I Test &amp; Measure?</title>
    <updated>2025-06-13T14:55:35+00:00</updated>
    <author>
      <name>/u/kekePower</name>
      <uri>https://old.reddit.com/user/kekePower</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;Following up on my last deep-dive into the 24B &lt;a href="https://www.reddit.com/r/MistralAI/comments/1lagg0o/performance_cost_deep_dive_benchmarking_the/"&gt;magistral&lt;/a&gt; model, I‚Äôm now gearing up for a new round of benchmarks - this time focused entirely on &lt;strong&gt;7‚Äì8B models that actually run on consumer-grade GPUs&lt;/strong&gt; (I'm testing on an RTX 3070, 8GB VRAM).&lt;/p&gt; &lt;p&gt;To make this genuinely useful, I want your input on how to approach the testing. Here‚Äôs what I‚Äôm looking for:&lt;/p&gt; &lt;h1&gt;1. Model Suggestions&lt;/h1&gt; &lt;p&gt;Which 7‚Äì8B models &lt;em&gt;need&lt;/em&gt; to be on the list?&lt;br /&gt; I'm looking for daily drivers, hidden gems, or just models you're curious about ‚Äî instruct, chat, or code variants welcome.&lt;/p&gt; &lt;h1&gt;2. Challenging Prompts&lt;/h1&gt; &lt;p&gt;Got a small handful (1‚Äì3 max) of &lt;strong&gt;killer prompts&lt;/strong&gt; that stress test these models?&lt;br /&gt; Think:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;multi-step reasoning&lt;/li&gt; &lt;li&gt;instruction-following&lt;/li&gt; &lt;li&gt;short code gen&lt;/li&gt; &lt;li&gt;abstract or creative tasks&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. What Should I Measure?&lt;/h1&gt; &lt;p&gt;Beyond just ‚Äúdoes it work,‚Äù I want to dig into what actually &lt;em&gt;matters&lt;/em&gt;. Here‚Äôs what I‚Äôve got so far:&lt;/p&gt; &lt;h1&gt;Quantitative Metrics:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Inference speed (tokens/sec)&lt;/li&gt; &lt;li&gt;VRAM usage during inference&lt;/li&gt; &lt;li&gt;Total token count per response&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Qualitative Metrics (more subjective):&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Reasoning &amp;amp; logic&lt;/li&gt; &lt;li&gt;Instruction-following fidelity&lt;/li&gt; &lt;li&gt;Code quality / creativity&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Got thoughts on how I should compare quality? Any scoring frameworks or benchmarks you‚Äôve seen done &lt;em&gt;right&lt;/em&gt;?&lt;/p&gt; &lt;p&gt;I‚Äôll keep the testing &lt;strong&gt;fair&lt;/strong&gt;, &lt;strong&gt;replicable&lt;/strong&gt;, and free of cherry-picked results. Just a straight-up look at what these small models can ‚Äî and can‚Äôt ‚Äî do.&lt;/p&gt; &lt;p&gt;If your suggestions make it into the final write-up, you‚Äôll be credited in the article. Thanks in advance ‚Äî this subreddit has some of the sharpest minds in the local LLM scene, and I know the feedback will make the piece better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kekePower"&gt; /u/kekePower &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lai0jn/planning_a_78b_model_benchmark_on_8gb_gpu_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lai0jn/planning_a_78b_model_benchmark_on_8gb_gpu_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lai0jn/planning_a_78b_model_benchmark_on_8gb_gpu_what/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-13T14:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lapdwd</id>
    <title>Build a multi-agent AI researcher using Ollama, LangGraph, and Streamlit</title>
    <updated>2025-06-13T19:52:38+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lapdwd/build_a_multiagent_ai_researcher_using_ollama/"&gt; &lt;img alt="Build a multi-agent AI researcher using Ollama, LangGraph, and Streamlit" src="https://external-preview.redd.it/bhsF-Ra2bxWUF5Y7tWKegp2Q43758Wa3UyF7oQ90On8.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f882e60cc8407488df8214043bf5169a10103ec" title="Build a multi-agent AI researcher using Ollama, LangGraph, and Streamlit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/eV-zVWClcj0?si=Zv0Ib4XfZ8Q4TYeV"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lapdwd/build_a_multiagent_ai_researcher_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lapdwd/build_a_multiagent_ai_researcher_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-13T19:52:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lao9lh</id>
    <title>Trium Project</title>
    <updated>2025-06-13T19:06:03+00:00</updated>
    <author>
      <name>/u/xKage21x</name>
      <uri>https://old.reddit.com/user/xKage21x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://youtu.be/ITVPvvdom50"&gt;https://youtu.be/ITVPvvdom50&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Project i've been working on for close to a year now. Multi agent system with persistent individual memory, emotional processing, self goal creation, temporal processing, code analysis and much more.&lt;/p&gt; &lt;p&gt;All 3 identities are aware of and can interact with eachother.&lt;/p&gt; &lt;p&gt;Open to questions üòä&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xKage21x"&gt; /u/xKage21x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lao9lh/trium_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lao9lh/trium_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lao9lh/trium_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-13T19:06:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lavzaq</id>
    <title>Need recomendation on running models on my laptop</title>
    <updated>2025-06-14T00:50:48+00:00</updated>
    <author>
      <name>/u/jeremidelacruz</name>
      <uri>https://old.reddit.com/user/jeremidelacruz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I need some advice on which Ollama models I can run on my computer. I have a Galaxy Book 3 Ultra with 32GB of RAM, an i9 processor, and an RTX 4070. I tried running Gemma 3 once, but it was a bit slow. Basically, I want to use it to create an assistant.&lt;/p&gt; &lt;p&gt;What models do you recommend for my setup? Any tips for getting better performance would also be appreciated!&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jeremidelacruz"&gt; /u/jeremidelacruz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lavzaq/need_recomendation_on_running_models_on_my_laptop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lavzaq/need_recomendation_on_running_models_on_my_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lavzaq/need_recomendation_on_running_models_on_my_laptop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-14T00:50:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lal879</id>
    <title>[Update] Spy search: open source replacement of perplexity !</title>
    <updated>2025-06-13T17:04:19+00:00</updated>
    <author>
      <name>/u/jasonhon2013</name>
      <uri>https://old.reddit.com/user/jasonhon2013</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lal879/update_spy_search_open_source_replacement_of/"&gt; &lt;img alt="[Update] Spy search: open source replacement of perplexity !" src="https://external-preview.redd.it/8n6md9gNrOiQ5fxkk23mfnOJ0R8EgXliMzs4N0V2b_U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15a6d7e0785e7124d1a6c4b1766c2b088dafd90e" title="[Update] Spy search: open source replacement of perplexity !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama is really a great place. I start contribute and my open source journey with Ollama. I feel motivated with Ollama community. You guys always give me the courage and motivation I need. I am really happy to have you guys. This time spy search no longer just a replacement that can local host and play with Ollama like a toy. It is a product now. It search faster than perplexity. You can run with Ollama mistral or llama 3.3 to get quick response ! I am really happy without you guys it is not possible or feasible for me to make such an awesome project ! (I am posting the video here first this speed search version will be available tmr !hehe let me test a bit first haha) &lt;/p&gt; &lt;p&gt;url: &lt;a href="https://github.com/JasonHonKL/spy-search"&gt;https://github.com/JasonHonKL/spy-search&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1lal879/video/yh0fc5pi7q6f1/player"&gt;https://reddit.com/link/1lal879/video/yh0fc5pi7q6f1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jasonhon2013"&gt; /u/jasonhon2013 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lal879/update_spy_search_open_source_replacement_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lal879/update_spy_search_open_source_replacement_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lal879/update_spy_search_open_source_replacement_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-13T17:04:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1las4na</id>
    <title>I built an Ollama model release view for TRMNL e-ink device screens (including an Updated view)</title>
    <updated>2025-06-13T21:48:59+00:00</updated>
    <author>
      <name>/u/ucffool</name>
      <uri>https://old.reddit.com/user/ucffool</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1las4na/i_built_an_ollama_model_release_view_for_trmnl/"&gt; &lt;img alt="I built an Ollama model release view for TRMNL e-ink device screens (including an Updated view)" src="https://preview.redd.it/p136ges9mr6f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=babb95b767e8dbd067bb87d1265ac3a5a099fb23" title="I built an Ollama model release view for TRMNL e-ink device screens (including an Updated view)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ucffool"&gt; /u/ucffool &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p136ges9mr6f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1las4na/i_built_an_ollama_model_release_view_for_trmnl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1las4na/i_built_an_ollama_model_release_view_for_trmnl/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-13T21:48:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lb2fvy</id>
    <title>Text Extraction from Unstructured Data</title>
    <updated>2025-06-14T06:59:15+00:00</updated>
    <author>
      <name>/u/InstantNyte_026</name>
      <uri>https://old.reddit.com/user/InstantNyte_026</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a mini pc with i3 10th gen. The ocr data provided to me is completely messy and is unstructured. &lt;/p&gt; &lt;p&gt;Context: OCR text is from paddleocr v3 (Confidence of around 0.9 most of the time)&lt;/p&gt; &lt;p&gt;Please suggest me a model which can work in with this and provides me with a json format within 30 seconds. For now my safest bet is qwen2.5:3b but the problem is that it misreads and duplicates data.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InstantNyte_026"&gt; /u/InstantNyte_026 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lb2fvy/text_extraction_from_unstructured_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lb2fvy/text_extraction_from_unstructured_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lb2fvy/text_extraction_from_unstructured_data/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-14T06:59:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lb8w1g</id>
    <title>Building AI for Privacy: An asynchronous way to serve custom recommendations</title>
    <updated>2025-06-14T13:36:52+00:00</updated>
    <author>
      <name>/u/anttiOne</name>
      <uri>https://old.reddit.com/user/anttiOne</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lb8w1g/building_ai_for_privacy_an_asynchronous_way_to/"&gt; &lt;img alt="Building AI for Privacy: An asynchronous way to serve custom recommendations" src="https://external-preview.redd.it/LWiIDP9UXltu3NJNEqOfbRI5rwXe1dUkrctbZp7fyXA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09d7e07ca4bb5abfe2a14d1802f62d7b5c484d5c" title="Building AI for Privacy: An asynchronous way to serve custom recommendations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Privacy-first AI: built custom recommendations using Ollama + Django, or how to serve pre-generated recommendations in dynamic sessions&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anttiOne"&gt; /u/anttiOne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@vs3kulic/building-ai-for-privacy-custom-recommendations-with-ollama-django-4fb82f3da833"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lb8w1g/building_ai_for_privacy_an_asynchronous_way_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lb8w1g/building_ai_for_privacy_an_asynchronous_way_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-14T13:36:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lb8xz8</id>
    <title>AMD EPYC Venice 1.6TB/s single socket memory bandwidth with 8000Mt/s 16 channel memory</title>
    <updated>2025-06-14T13:39:32+00:00</updated>
    <author>
      <name>/u/sub_RedditTor</name>
      <uri>https://old.reddit.com/user/sub_RedditTor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lb8xz8/amd_epyc_venice_16tbs_single_socket_memory/"&gt; &lt;img alt="AMD EPYC Venice 1.6TB/s single socket memory bandwidth with 8000Mt/s 16 channel memory" src="https://preview.redd.it/muezlbq0cw6f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d8c405e4c7128b91662ad2d98982263def1a5848" title="AMD EPYC Venice 1.6TB/s single socket memory bandwidth with 8000Mt/s 16 channel memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Those are insane speeds but I believe that's only theoretical max bandwidth..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sub_RedditTor"&gt; /u/sub_RedditTor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/muezlbq0cw6f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lb8xz8/amd_epyc_venice_16tbs_single_socket_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lb8xz8/amd_epyc_venice_16tbs_single_socket_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-14T13:39:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lama7m</id>
    <title>Performance of ollama with mistral 7b on a macbook M1 air with only 8GB. quite impressive!</title>
    <updated>2025-06-13T17:45:56+00:00</updated>
    <author>
      <name>/u/irodov4030</name>
      <uri>https://old.reddit.com/user/irodov4030</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lama7m/performance_of_ollama_with_mistral_7b_on_a/"&gt; &lt;img alt="Performance of ollama with mistral 7b on a macbook M1 air with only 8GB. quite impressive!" src="https://external-preview.redd.it/bHFjNjIzaHdlcTZmMWNq72rO9xSWOuTtgX_-6Bxqcbvt6-TijH24W1mmf0dh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a89c545ef0fa0e36f1ec5c8857ed837cb39d29d" title="Performance of ollama with mistral 7b on a macbook M1 air with only 8GB. quite impressive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;plugged in and no other apps running&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/irodov4030"&gt; /u/irodov4030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/94raj4hweq6f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lama7m/performance_of_ollama_with_mistral_7b_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lama7m/performance_of_ollama_with_mistral_7b_on_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-13T17:45:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbgh6x</id>
    <title>üö™ Dungeo AI WebUI ‚Äì A Local Roleplay Frontend for LLM-based Dungeon Masters üßô‚Äç‚ôÇÔ∏è‚ú®</title>
    <updated>2025-06-14T19:09:11+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I‚Äôm the creator of &lt;a href="https://github.com/Laszlobeer/Dungeo_ai"&gt;Dungeo AI&lt;/a&gt;, and I‚Äôm excited to share the next evolution of the project: &lt;a href="https://github.com/Laszlobeer/Dungeo_ai_webui"&gt;&lt;strong&gt;Dungeo AI WebUI&lt;/strong&gt;&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;This is a major upgrade from the original terminal-based version ‚Äî now with a full web interfac. It's built for immersive, AI-powered solo roleplay in fantasy settings, kind of like having your own personal Dungeon Master on demand.&lt;/p&gt; &lt;p&gt;üîπ &lt;strong&gt;What‚Äôs New:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clean and responsive WebUi&lt;/li&gt; &lt;li&gt;Easy customise character : name, character &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üé≤ It‚Äôs built with simplicity and flexibility in mind. If you're into AI dungeon adventures or narrative roleplay, give it a try! Contributions, feedback, and forks are always welcome.&lt;/p&gt; &lt;p&gt;üì¶ GitHub: &lt;a href="https://github.com/Laszlobeer/Dungeo_ai_webui"&gt;https://github.com/Laszlobeer/Dungeo_ai_webui&lt;/a&gt;&lt;br /&gt; üß† Original Project: &lt;a href="https://github.com/Laszlobeer/Dungeo_ai"&gt;https://github.com/Laszlobeer/Dungeo_ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear what you think or see your own setups!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lbgh6x/dungeo_ai_webui_a_local_roleplay_frontend_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lbgh6x/dungeo_ai_webui_a_local_roleplay_frontend_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lbgh6x/dungeo_ai_webui_a_local_roleplay_frontend_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-14T19:09:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1lba7br</id>
    <title>ITRS - Make any ollama Model reason with the Iterative Transparent Reasoning System</title>
    <updated>2025-06-14T14:37:33+00:00</updated>
    <author>
      <name>/u/thomheinrich</name>
      <uri>https://old.reddit.com/user/thomheinrich</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there,&lt;/p&gt; &lt;p&gt;I am diving in the deep end of futurology, AI and Simulated Intelligence since many years - and although I am a MD at a Big4 in my working life (responsible for the AI transformation), my biggest private ambition is to a) drive AI research forward b) help to approach AGI c) support the progress towards the Singularity and d) be a part of the community that ultimately supports the emergence of an utopian society. &lt;/p&gt; &lt;p&gt;Currently I am looking for smart people wanting to work with or contribute to one of my side research projects, the ITRS‚Ä¶ more information here:&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://github.com/thom-heinrich/itrs/blob/main/ITRS.pdf"&gt;https://github.com/thom-heinrich/itrs/blob/main/ITRS.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/thom-heinrich/itrs"&gt;https://github.com/thom-heinrich/itrs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video: &lt;a href="https://youtu.be/ubwaZVtyiKA?si=BvKSMqFwHSzYLIhw"&gt;https://youtu.be/ubwaZVtyiKA?si=BvKSMqFwHSzYLIhw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Web: &lt;a href="https://www.chonkydb.com"&gt;https://www.chonkydb.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚úÖ TLDR: #ITRS is an innovative research solution to make any (local) #LLM more #trustworthy, #explainable and enforce #SOTA grade #reasoning. Links to the research #paper &amp;amp; #github are at the end of this posting.&lt;/p&gt; &lt;p&gt;Disclaimer: As I developed the solution entirely in my free-time and on weekends, there are a lot of areas to deepen research in (see the paper).&lt;/p&gt; &lt;p&gt;We present the Iterative Thought Refinement System (ITRS), a groundbreaking architecture that revolutionizes artificial intelligence reasoning through a purely large language model (LLM)-driven iterative refinement process integrated with dynamic knowledge graphs and semantic vector embeddings. Unlike traditional heuristic-based approaches, ITRS employs zero-heuristic decision, where all strategic choices emerge from LLM intelligence rather than hardcoded rules. The system introduces six distinct refinement strategies (TARGETED, EXPLORATORY, SYNTHESIS, VALIDATION, CREATIVE, and CRITICAL), a persistent thought document structure with semantic versioning, and real-time thinking step visualization. Through synergistic integration of knowledge graphs for relationship tracking, semantic vector engines for contradiction detection, and dynamic parameter optimization, ITRS achieves convergence to optimal reasoning solutions while maintaining complete transparency and auditability. We demonstrate the system's theoretical foundations, architectural components, and potential applications across explainable AI (XAI), trustworthy AI (TAI), and general LLM enhancement domains. The theoretical analysis demonstrates significant potential for improvements in reasoning quality, transparency, and reliability compared to single-pass approaches, while providing formal convergence guarantees and computational complexity bounds. The architecture advances the state-of-the-art by eliminating the brittleness of rule-based systems and enabling truly adaptive, context-aware reasoning that scales with problem complexity.&lt;/p&gt; &lt;p&gt;Best Thom &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thomheinrich"&gt; /u/thomheinrich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lba7br/itrs_make_any_ollama_model_reason_with_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lba7br/itrs_make_any_ollama_model_reason_with_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lba7br/itrs_make_any_ollama_model_reason_with_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-14T14:37:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lb70kk</id>
    <title>LLM with OCR capabilities</title>
    <updated>2025-06-14T11:59:58+00:00</updated>
    <author>
      <name>/u/depava</name>
      <uri>https://old.reddit.com/user/depava</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to create an app to OCR PDF documents. I need LLM model to understand context on how to map text to particular fields. Plain OCR things cannot do it. &lt;/p&gt; &lt;p&gt;It is for production, not a higload but 300 docs per day can be. &lt;/p&gt; &lt;p&gt;I use AWS, and thinking about using Bedrock and Claude. But I think, maybe it's cheaper to use some self-hosted models for this purpose? Or running in EC2 instance the model will cost more than just using API of paid models? Thank you very much in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/depava"&gt; /u/depava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lb70kk/llm_with_ocr_capabilities/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lb70kk/llm_with_ocr_capabilities/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lb70kk/llm_with_ocr_capabilities/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-14T11:59:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lb2vj9</id>
    <title>I made a free iOS app for people who run LLMs locally. It‚Äôs a chatbot that you can use away from home to interact with an LLM that runs locally on your desktop Mac.</title>
    <updated>2025-06-14T07:27:59+00:00</updated>
    <author>
      <name>/u/Valuable-Run2129</name>
      <uri>https://old.reddit.com/user/Valuable-Run2129</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is easy enough that anyone can use it. No tunnel or port forwarding needed.&lt;/p&gt; &lt;p&gt;The app is called LLM Pigeon and has a companion app called LLM Pigeon Server for Mac.&lt;br /&gt; It works like a carrier pigeon :). It uses iCloud to append each prompt and response to a file on iCloud.&lt;br /&gt; It‚Äôs not totally local because iCloud is involved, but I trust iCloud with all my files anyway (most people do) and I don‚Äôt trust AI companies. &lt;/p&gt; &lt;p&gt;The iOS app is a simple Chatbot app. The MacOS app is a simple bridge to LMStudio or Ollama. Just insert the model name you are running on LMStudio or Ollama and it‚Äôs ready to go.&lt;br /&gt; For Apple approval purposes I needed to provide it with an in-built model, but don‚Äôt use it, it‚Äôs a small Qwen3-0.6B model.&lt;/p&gt; &lt;p&gt;I find it super cool that I can chat anywhere with Qwen3-30B running on my Mac at home. &lt;/p&gt; &lt;p&gt;For now it‚Äôs just text based. It‚Äôs the very first version, so, be kind. I've tested it extensively with LMStudio and it works great. I haven't tested it with Ollama, but it should work. Let me know.&lt;/p&gt; &lt;p&gt;The apps are open source and these are the repos:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/permaevidence/LLM-Pigeon"&gt;https://github.com/permaevidence/LLM-Pigeon&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/permaevidence/LLM-Pigeon-Server"&gt;https://github.com/permaevidence/LLM-Pigeon-Server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;they have just been approved by Apple and are both on the App Store. Here are the links:&lt;/p&gt; &lt;p&gt;&lt;a href="https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB"&gt;https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12"&gt;https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PS. I hope this isn't viewed as self promotion because the app is free, collects no data and is open source.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Valuable-Run2129"&gt; /u/Valuable-Run2129 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lb2vj9/i_made_a_free_ios_app_for_people_who_run_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lb2vj9/i_made_a_free_ios_app_for_people_who_run_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lb2vj9/i_made_a_free_ios_app_for_people_who_run_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-14T07:27:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbfn7k</id>
    <title>an offline voice assistant</title>
    <updated>2025-06-14T18:33:21+00:00</updated>
    <author>
      <name>/u/ppzms</name>
      <uri>https://old.reddit.com/user/ppzms</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;&lt;em&gt;Jarvis&lt;/em&gt; is a voice assistant I made in C++ that operates entirely on your local computer with no internet required! This is the first time to push a project in Github, and I would really appreciate it if some of you could take a look at it.&lt;/p&gt; &lt;p&gt;I'm not a professional developer this is just a hobby project I‚Äôve been working on in my spare time ‚Äî so I‚Äôd really appreciate your feedback.&lt;/p&gt; &lt;p&gt;Jarvis is meant to be very light on resources and completely offline-capable (after downloading the models). It harnesses some wonderful open-source initiatives to do the heavy lifting.&lt;/p&gt; &lt;p&gt;To make the installation process as easy as possible, especially for the Linux community, I have created a setup.sh and run.sh scripts that can be used for a quick and easy installation.&lt;/p&gt; &lt;p&gt;The things that I would like to know:&lt;/p&gt; &lt;p&gt;Any unexpected faults such as crashes, error messages, or wrong behavior that should be reported.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance&lt;/strong&gt;: What is the speed on different hardware configurations (especially CPU vs. GPU for LLM)? &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Experience of Setting Up&lt;/strong&gt;: Did the README.md provide a clear message? &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code Feedback:&lt;/strong&gt; If you‚Äôre into C++, feel free to peek at the code and roast it nicely ‚Äî tips on cleaner structure, better practices, or just ‚Äúwhat were you thinking here?‚Äù moments are totally welcome!&lt;/p&gt; &lt;p&gt;Have a look at my &lt;a href="https://github.com/almimony75/jarvis"&gt;repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Remember to open the llama.cpp server in another terminal before you run Jarvis!&lt;/p&gt; &lt;p&gt;Thanks a lot for your contribution!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ppzms"&gt; /u/ppzms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lbfn7k/an_offline_voice_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lbfn7k/an_offline_voice_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lbfn7k/an_offline_voice_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-14T18:33:21+00:00</published>
  </entry>
</feed>
