<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-23T18:25:14+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mvs4qa</id>
    <title>Had some beginner questions regarding how to use Ollama?</title>
    <updated>2025-08-20T21:35:46+00:00</updated>
    <author>
      <name>/u/RandomHuman1002</name>
      <uri>https://old.reddit.com/user/RandomHuman1002</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I am a beginner in trying to run AI locally had some questions regarding it.&lt;br /&gt; I want to run the AI on my laptop (13th gen i7-13650HX, 32GB RAM, RTX 4060 Laptop GPU) &lt;/p&gt; &lt;p&gt;1) Which AI model should I use I can see many of them on the ollama website like the new (gpt-oss, deepseek-r1, gemma3, qwen3 and llama3.1). Has anyone compared the pros and cons of each model?&lt;br /&gt; I can see that llama3.1 does not have thinking capabilities and gemma3 is the only vision model how does that affect the model that is running?&lt;/p&gt; &lt;p&gt;2) I am on a Windows machine so should I just use windows ollama or try to use Linux ollama using wsl (was recommended to do this)&lt;/p&gt; &lt;p&gt;3) Should I install openweb-ui and install ollama through that or just install ollama first?&lt;/p&gt; &lt;p&gt;Any other things I should keep in mind?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomHuman1002"&gt; /u/RandomHuman1002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvs4qa/had_some_beginner_questions_regarding_how_to_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvs4qa/had_some_beginner_questions_regarding_how_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mvs4qa/had_some_beginner_questions_regarding_how_to_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-20T21:35:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvrxsz</id>
    <title>Best model for my use case?</title>
    <updated>2025-08-20T21:28:23+00:00</updated>
    <author>
      <name>/u/guacgang</name>
      <uri>https://old.reddit.com/user/guacgang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am building an application where the model needs to make recommendations on rock climbing routes, including details about weather, difficulty, suggested gear, etc.&lt;/p&gt; &lt;p&gt;It also needs to be able to review videos that users/climbers upload and make suggestions on technique.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/guacgang"&gt; /u/guacgang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvrxsz/best_model_for_my_use_case/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvrxsz/best_model_for_my_use_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mvrxsz/best_model_for_my_use_case/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-20T21:28:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvyc7i</id>
    <title>Anyone using Ollama on a Windows Snapdragon Machine?</title>
    <updated>2025-08-21T02:04:36+00:00</updated>
    <author>
      <name>/u/Clipbeam</name>
      <uri>https://old.reddit.com/user/Clipbeam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious to see how well it performs... What models can you run on say the Surface laptop 15?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Clipbeam"&gt; /u/Clipbeam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvyc7i/anyone_using_ollama_on_a_windows_snapdragon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvyc7i/anyone_using_ollama_on_a_windows_snapdragon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mvyc7i/anyone_using_ollama_on_a_windows_snapdragon/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T02:04:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwbjwl</id>
    <title>Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp; Streamlit</title>
    <updated>2025-08-21T13:52:46+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mwbjwl/build_a_local_ai_agent_with_mcp_tools_using/"&gt; &lt;img alt="Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp;amp; Streamlit" src="https://external-preview.redd.it/rq8k6bkBVDqS3EaB-6PmZwrrp9mjAeoX2Tt37ubIdpg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76f91a095d5d6782424d54183f94d9fb060dd411" title="Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp;amp; Streamlit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/Baa-z7cum1g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwbjwl/build_a_local_ai_agent_with_mcp_tools_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mwbjwl/build_a_local_ai_agent_with_mcp_tools_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T13:52:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwilxb</id>
    <title>Are there best practices on how to use vanna with large databases and suboptimal table and columnnames?</title>
    <updated>2025-08-21T18:10:29+00:00</updated>
    <author>
      <name>/u/Private_Tank</name>
      <uri>https://old.reddit.com/user/Private_Tank</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Private_Tank"&gt; /u/Private_Tank &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LLMFrameworks/comments/1mwilh4/are_there_best_practices_on_how_to_use_vanna_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwilxb/are_there_best_practices_on_how_to_use_vanna_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mwilxb/are_there_best_practices_on_how_to_use_vanna_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T18:10:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwkg27</id>
    <title>Best model for text summarization</title>
    <updated>2025-08-21T19:18:57+00:00</updated>
    <author>
      <name>/u/Paleone123</name>
      <uri>https://old.reddit.com/user/Paleone123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need to create a fair number of presentations in a short time. I'm wondering what models will do best at at summarizing text into a series of headings and bullet points for me. It would also be nice if the model could output in markdown without me having to include a description of how basic markdown works in the context window. I'm much less concerned about tokens per second and much more about accuracy. I have 12gig of vram on my GPU, so 8b or 12b Q4 models are probably the limit of what I can run. I also have a ridiculous amount of ram, but I'm afraid ollama will crash out if I try to run a huge model on the CPU. Any advice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Paleone123"&gt; /u/Paleone123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwkg27/best_model_for_text_summarization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwkg27/best_model_for_text_summarization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mwkg27/best_model_for_text_summarization/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T19:18:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwf8jv</id>
    <title>Andrej Karpathy Software 3.0</title>
    <updated>2025-08-21T16:06:59+00:00</updated>
    <author>
      <name>/u/Brad_159</name>
      <uri>https://old.reddit.com/user/Brad_159</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mwf8jv/andrej_karpathy_software_30/"&gt; &lt;img alt="Andrej Karpathy Software 3.0" src="https://external-preview.redd.it/ItVjsy1WeovauKf8qvgECqUafdKTXMoXeunnlcMT0Gg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f6c058a6b915a0d391d295c52ba1ae54086852c" title="Andrej Karpathy Software 3.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That is almost what you can envision for the next five years. All the the applications and systems are going to be equipped with features that allow llms to call and operate.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brad_159"&gt; /u/Brad_159 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/LCEmiRjPEtQ?si=1G5_xVoBm7w-crbM"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwf8jv/andrej_karpathy_software_30/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mwf8jv/andrej_karpathy_software_30/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T16:06:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwhwx8</id>
    <title>Can LLMs Explain Their Reasoning? - Lecture Clip</title>
    <updated>2025-08-21T17:44:32+00:00</updated>
    <author>
      <name>/u/kushalgoenka</name>
      <uri>https://old.reddit.com/user/kushalgoenka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mwhwx8/can_llms_explain_their_reasoning_lecture_clip/"&gt; &lt;img alt="Can LLMs Explain Their Reasoning? - Lecture Clip" src="https://external-preview.redd.it/TEUGnG3_498JAZ2_OVIe6SduCtLk60U1nCPoQNdAxiw.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea3c8a53ee2ddd06374d9ca9c87ab9d321a362aa" title="Can LLMs Explain Their Reasoning? - Lecture Clip" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kushalgoenka"&gt; /u/kushalgoenka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/u2uNPzzZ45k"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwhwx8/can_llms_explain_their_reasoning_lecture_clip/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mwhwx8/can_llms_explain_their_reasoning_lecture_clip/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T17:44:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwe5e2</id>
    <title>gpt-oss provides correct date, but is sure that it is a different day of week</title>
    <updated>2025-08-21T15:27:44+00:00</updated>
    <author>
      <name>/u/JNKO266</name>
      <uri>https://old.reddit.com/user/JNKO266</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mwe5e2/gptoss_provides_correct_date_but_is_sure_that_it/"&gt; &lt;img alt="gpt-oss provides correct date, but is sure that it is a different day of week" src="https://preview.redd.it/8kc7a68b5ekf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b32860da5af726cc3f32e587b14cf7a290a43ff8" title="gpt-oss provides correct date, but is sure that it is a different day of week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been playing around with the new gpt-oss model while other models are downloading on a new machine, came onto this, which I thought was quite funny&lt;/p&gt; &lt;p&gt;“User claims today is Thursday August 21, 2025. That is obviously wrong: August 21, 2025 falls on Saturday.”&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JNKO266"&gt; /u/JNKO266 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8kc7a68b5ekf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwe5e2/gptoss_provides_correct_date_but_is_sure_that_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mwe5e2/gptoss_provides_correct_date_but_is_sure_that_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T15:27:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mx4m48</id>
    <title>Having issues when running two instances of Ollama, not sure if it even could really work</title>
    <updated>2025-08-22T12:03:52+00:00</updated>
    <author>
      <name>/u/thexdroid</name>
      <uri>https://old.reddit.com/user/thexdroid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For a specific test I installed 2 instances of Ollama on my computer, one on top of Windows, normal installation and a second of with linux WSL. For the WSL I've set a parameter to force it use CPU only, the intention was running 2 models at the same &amp;quot;time&amp;quot;.&lt;/p&gt; &lt;p&gt;What happens is the Ollama seems now to be attached to the wsl layer, what means that once I boot my computer Windows Ollama's GUI won't popup properly unless I start wsl. One more thing: I am sharing the model folder for both installations so I can download a model and it will be visible for both.&lt;/p&gt; &lt;p&gt;Should I revert and try to isolate the wsl version? Thanks for any idea.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thexdroid"&gt; /u/thexdroid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mx4m48/having_issues_when_running_two_instances_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mx4m48/having_issues_when_running_two_instances_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mx4m48/having_issues_when_running_two_instances_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-22T12:03:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwm9si</id>
    <title>Ollama Discord Rich Presence</title>
    <updated>2025-08-21T20:28:14+00:00</updated>
    <author>
      <name>/u/r00tkit_</name>
      <uri>https://old.reddit.com/user/r00tkit_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mwm9si/ollama_discord_rich_presence/"&gt; &lt;img alt="Ollama Discord Rich Presence" src="https://preview.redd.it/sf9eke8xmfkf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=068033aebc62dfffb22c97f765735aa5e6080ad0" title="Ollama Discord Rich Presence" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Made a Discord Rich Presence for Ollama - shows your current model + system specs&lt;/p&gt; &lt;p&gt;One-click install, works immediately. Thought you guys might like it!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/teodorgross/ollama-discord-presence"&gt;https://github.com/teodorgross/ollama-discord-presence&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/r00tkit_"&gt; /u/r00tkit_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sf9eke8xmfkf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwm9si/ollama_discord_rich_presence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mwm9si/ollama_discord_rich_presence/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T20:28:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwhmgk</id>
    <title>GLM-4.5 Air now running on Ollama, thanks to this kind soul (MichelRosselli)</title>
    <updated>2025-08-21T17:33:34+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You, sir or ma’am, are a friggin’ LEGEND for posting working quants of GLM-4.5 Air on your Ollama repository &lt;a href="https://ollama.com/MichelRosselli/GLM-4.5-Air"&gt;https://ollama.com/MichelRosselli/GLM-4.5-Air&lt;/a&gt; even before any “official” Ollama quants have been posted. Hats off to you! Note: According to the notes, the chat template is “provisional”, so tool calling doesn’t seem to be working at the moment and disabling thinking may not be supported either until the finalized chat template is added, but otherwise this thing is WAY COOL! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwhmgk/glm45_air_now_running_on_ollama_thanks_to_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwhmgk/glm45_air_now_running_on_ollama_thanks_to_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mwhmgk/glm45_air_now_running_on_ollama_thanks_to_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T17:33:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwsckl</id>
    <title>Ollama GUI is Electron based?</title>
    <updated>2025-08-22T00:42:59+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mwsckl/ollama_gui_is_electron_based/"&gt; &lt;img alt="Ollama GUI is Electron based?" src="https://preview.redd.it/yqq6arbawgkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39dc439c44365f27ef240c603f43fce30fd18572" title="Ollama GUI is Electron based?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Copilot chat on the ollama repo seems to think so but im hearing conflicting information&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yqq6arbawgkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwsckl/ollama_gui_is_electron_based/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mwsckl/ollama_gui_is_electron_based/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-22T00:42:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxgk7p</id>
    <title>Local Ollama integration into VS plugin</title>
    <updated>2025-08-22T19:52:01+00:00</updated>
    <author>
      <name>/u/PacManFan123</name>
      <uri>https://old.reddit.com/user/PacManFan123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My work has tasked me to investigate how we can use a local AI server on our network running llama / Ollama and a model such as gpt-oss or deekseek-coder. The goal is to have 1 or more AI servers set up on the work network - and then have our software engineers using VS code with a plugin to do code reviews and generation. It's important that our code never leave our local network. &lt;/p&gt; &lt;p&gt;What VS code plugins would support this? Is there a guide to setting something like this up? I already have Ollama + Open WebUI configured and working with remote browser clients.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PacManFan123"&gt; /u/PacManFan123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxgk7p/local_ollama_integration_into_vs_plugin/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxgk7p/local_ollama_integration_into_vs_plugin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mxgk7p/local_ollama_integration_into_vs_plugin/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-22T19:52:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxit1d</id>
    <title>Which models are suitable for websearch?</title>
    <updated>2025-08-22T21:20:17+00:00</updated>
    <author>
      <name>/u/runsleeprepeat</name>
      <uri>https://old.reddit.com/user/runsleeprepeat</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/runsleeprepeat"&gt; /u/runsleeprepeat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1mucj1p/which_models_are_suitable_for_websearch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxit1d/which_models_are_suitable_for_websearch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mxit1d/which_models_are_suitable_for_websearch/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-22T21:20:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxj388</id>
    <title>Ollama using CPU when it shouldn't?</title>
    <updated>2025-08-22T21:31:34+00:00</updated>
    <author>
      <name>/u/OrganizationHot731</name>
      <uri>https://old.reddit.com/user/OrganizationHot731</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi&lt;/p&gt; &lt;p&gt;I was trying to run qwen3 the other day, unsloth Q5_K_M &lt;/p&gt; &lt;p&gt;When I run at default it runs in GPU But as soon as I increase the context it runs in CPU only even tho I have 4 GPU RTX a4000 16gb each&lt;/p&gt; &lt;p&gt;How can I get it to run in GPU only? I have tried many settings and nothing &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OrganizationHot731"&gt; /u/OrganizationHot731 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxj388/ollama_using_cpu_when_it_shouldnt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxj388/ollama_using_cpu_when_it_shouldnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mxj388/ollama_using_cpu_when_it_shouldnt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-22T21:31:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mx0lix</id>
    <title>Local AI for students</title>
    <updated>2025-08-22T08:09:40+00:00</updated>
    <author>
      <name>/u/just-rundeer</name>
      <uri>https://old.reddit.com/user/just-rundeer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I’d like to give ~20 students access to a local AI system in class.&lt;/p&gt; &lt;p&gt;The main idea: build a simple RAG (retrieval-augmented generation) so they can look up rules/answers on their own when they don’t want to ask me.&lt;/p&gt; &lt;p&gt;Would a Beelink mini PC with 32GB RAM be enough to host a small LLM (7B–13B, quantized) plus a RAG index for ~20 simultaneous users?&lt;/p&gt; &lt;p&gt;Any experiences with performance under classroom conditions? Would you recommend Beelink or a small tower PC with GPU for more scalability?&lt;/p&gt; &lt;p&gt;Perfect would be if I could create something like Study and Learn mode but that will probably need GPU power then I am willing to spend. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/just-rundeer"&gt; /u/just-rundeer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mx0lix/local_ai_for_students/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mx0lix/local_ai_for_students/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mx0lix/local_ai_for_students/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-22T08:09:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxada7</id>
    <title>How much video ram do I need to run 70b at full context?</title>
    <updated>2025-08-22T15:57:27+00:00</updated>
    <author>
      <name>/u/Suspicious-Half2593</name>
      <uri>https://old.reddit.com/user/Suspicious-Half2593</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been considering buying three 7600 xt’s so that I can use larger models, would this been enough for full context and does anyone have an estimate on on tokens per second? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suspicious-Half2593"&gt; /u/Suspicious-Half2593 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxada7/how_much_video_ram_do_i_need_to_run_70b_at_full/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxada7/how_much_video_ram_do_i_need_to_run_70b_at_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mxada7/how_much_video_ram_do_i_need_to_run_70b_at_full/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-22T15:57:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxwsr5</id>
    <title>Architecture for a Small-Scale Al Interface for MSSQL</title>
    <updated>2025-08-23T09:20:44+00:00</updated>
    <author>
      <name>/u/lokiiiiie</name>
      <uri>https://old.reddit.com/user/lokiiiiie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for some advice on the best way to add a simple AI feature to our internal application. Prompt like &amp;quot;What were our total sales last quarter?&amp;quot; All data can be get it from database, and get answers directly from our live Microsoft SQL Server database which holds financial data. &lt;/p&gt; &lt;p&gt;My plan:- ollama- openwebui -( postgres converted db)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lokiiiiie"&gt; /u/lokiiiiie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxwsr5/architecture_for_a_smallscale_al_interface_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxwsr5/architecture_for_a_smallscale_al_interface_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mxwsr5/architecture_for_a_smallscale_al_interface_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-23T09:20:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxzzs6</id>
    <title>Ollama Dashboard - Noob Question</title>
    <updated>2025-08-23T12:21:14+00:00</updated>
    <author>
      <name>/u/WalterKEKWh1te</name>
      <uri>https://old.reddit.com/user/WalterKEKWh1te</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So im kinda late to the party and been spending the past 2 weeks reading technical documentation and understand basics.&lt;/p&gt; &lt;p&gt;I managed to install ollama with an embed model, install postgres and pg vektor, obsidian, vs code with continue and connect all that shit. i also managed to setup open llm vtuber and whisper and make my llm more ayaya but thats besides the point. I decided to go with python as a framework and vs code and continue for coding.&lt;/p&gt; &lt;p&gt;Now thanks to Gaben the allmighty MCP got born. So i am looking for a gui frontend for my llm to implement mcp services. as far as i understand langchain and llamaindex used to be solid base. now there is crewai and many more.&lt;/p&gt; &lt;p&gt;I feel kinda lost and overwhelmed here because i dont know who supports just basic local ollama with some rag/sql and local preconfigured mcp servers. Its just for personal use.&lt;/p&gt; &lt;p&gt;And is there a thing that combines Open LLM Vtube with lets say Langchain to make an Ollama Dashboard? Control Input: Voice, Whisper, Llava, Prompt Tempering ... Control Agent: LLM, Tools via MCP or API Call ... Output Control: TTS, Avatar Control Is that a thing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WalterKEKWh1te"&gt; /u/WalterKEKWh1te &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxzzs6/ollama_dashboard_noob_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxzzs6/ollama_dashboard_noob_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mxzzs6/ollama_dashboard_noob_question/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-23T12:21:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxqthv</id>
    <title>Best model for my use case (updated)</title>
    <updated>2025-08-23T03:26:32+00:00</updated>
    <author>
      <name>/u/guacgang</name>
      <uri>https://old.reddit.com/user/guacgang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a post a few days ago but I should probably give more context (no pun intended).&lt;/p&gt; &lt;p&gt;I am building an application where the model needs to make recommendations on rock climbing routes, including details about weather, difficulty, suggested gear, etc.&lt;/p&gt; &lt;p&gt;It also needs to be able to review videos that users/climbers upload and make suggestions on technique.&lt;/p&gt; &lt;p&gt;I am a broke ass college student with a MacBook (M2 chip). Originally I was using 4o-mini but I want to switch to ollama because I don't want to keep paying for API credits and also because I think in the future most companies will be using local models for cost/security reasons and I want experience using them.&lt;/p&gt; &lt;p&gt;The plan is to scrape a variety of popular climbing websites for data and then build a RAG system for the LLM to use. Keeping the size of this model as low as possible is crucial for the testing phase because running ollama 3.2 8b makes my laptop shit its pants. How much does quality degrade as model size decreases?&lt;/p&gt; &lt;p&gt;Any help is super appreciated, especially resources on building RAG pipelines&lt;/p&gt; &lt;p&gt;So far the scraper is the most annoying part, for a couple reasons:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I often find that the scraper will work perfectly for one page on a site but is total garbage for others&lt;br /&gt;&lt;/li&gt; &lt;li&gt;I need to scrape through the html but the most important website I'm scraping also has JS and other lazy loading procedures which causes me to miss data (especially hard to get ALL of the photos for a climb, not just a couple if I get any at all). Same is true for the comments under climbs, which is arguably some of the most important data since that is where climbers actively discuss conditions and access for the route.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Having a single scraper seems unreasonable, what chunking strategies do you guys suggest? Has anyone dealt with this issue before?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/guacgang"&gt; /u/guacgang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxqthv/best_model_for_my_use_case_updated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxqthv/best_model_for_my_use_case_updated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mxqthv/best_model_for_my_use_case_updated/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-23T03:26:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1my3k3t</id>
    <title>AMD 395 128GB ram VS Apple Mac Air 10-core 32GB ram</title>
    <updated>2025-08-23T14:54:54+00:00</updated>
    <author>
      <name>/u/quantrpeter</name>
      <uri>https://old.reddit.com/user/quantrpeter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi&lt;br /&gt; If running local model such as codellama, AMD 395 128GB ram VS Apple Mac Air 10-core 32GB ram, AMD sure win, right? &lt;/p&gt; &lt;p&gt;My long duration of use is in library. Can AMD maintains 4-5 hours usage of vscode/netbeans after 2 years use?&lt;/p&gt; &lt;p&gt;thanks&lt;br /&gt; Peter&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantrpeter"&gt; /u/quantrpeter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1my3k3t/amd_395_128gb_ram_vs_apple_mac_air_10core_32gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1my3k3t/amd_395_128gb_ram_vs_apple_mac_air_10core_32gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1my3k3t/amd_395_128gb_ram_vs_apple_mac_air_10core_32gb_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-23T14:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxtpuz</id>
    <title>Mac Mini M4 32GB vs limited PC upgrade for local AI - tight budget</title>
    <updated>2025-08-23T06:08:18+00:00</updated>
    <author>
      <name>/u/Street_Trek_7754</name>
      <uri>https://old.reddit.com/user/Street_Trek_7754</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! I need your advice on a budget decision.&lt;/p&gt; &lt;p&gt;I currently have a desktop PC with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Intel i9 10th generstion&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;48 GB of RAM&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Radeon RX 7600 XT (16GB VRAM)&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm considering whether to buy a &lt;strong&gt;Mac Mini M4 with 32GB of RAM&lt;/strong&gt; or make small upgrades to my current setup. The primary use would be for &lt;strong&gt;local AI models&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The problem is that I have a &lt;strong&gt;limited budget&lt;/strong&gt; and my case is pretty much &lt;strong&gt;maxed out&lt;/strong&gt;: I can't do major hardware upgrades, at most increase the RAM.&lt;/p&gt; &lt;p&gt;My questions: 1. Can the 32GB Mac Mini M4 compete with my current setup for local AI? 2. Is it worth making the leap considering I would have less total RAM (32GB vs. 48GB)? 3. Does the Mac's unified architecture make up for the difference in RAM? 4. Has anyone made a similar switch and can share their experience?&lt;/p&gt; &lt;p&gt;Given budget and space constraints, should I stick with the PC and perhaps simply increase the RAM, or does the Mac Mini M4 offer a significant performance boost for the AI?&lt;/p&gt; &lt;p&gt;Thanks for any advice!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Street_Trek_7754"&gt; /u/Street_Trek_7754 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxtpuz/mac_mini_m4_32gb_vs_limited_pc_upgrade_for_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxtpuz/mac_mini_m4_32gb_vs_limited_pc_upgrade_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mxtpuz/mac_mini_m4_32gb_vs_limited_pc_upgrade_for_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-23T06:08:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1my1yfl</id>
    <title>One app to chat with multiple LLMs (Google, Ollama, Docker)</title>
    <updated>2025-08-23T13:48:51+00:00</updated>
    <author>
      <name>/u/Working-Magician-823</name>
      <uri>https://old.reddit.com/user/Working-Magician-823</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1my1yfl/one_app_to_chat_with_multiple_llms_google_ollama/"&gt; &lt;img alt="One app to chat with multiple LLMs (Google, Ollama, Docker)" src="https://b.thumbs.redditmedia.com/rcwy9JFCqxc-0whdvDCCdOBHn6swdWsMDKSgBZH0HrQ.jpg" title="One app to chat with multiple LLMs (Google, Ollama, Docker)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Working-Magician-823"&gt; /u/Working-Magician-823 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1my1oue/one_app_to_chat_with_multiple_llms_google_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1my1yfl/one_app_to_chat_with_multiple_llms_google_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1my1yfl/one_app_to_chat_with_multiple_llms_google_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-23T13:48:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1my7010</id>
    <title>ThinkPad for Local LLM Inference - Linux Compatibility Questions</title>
    <updated>2025-08-23T17:09:02+00:00</updated>
    <author>
      <name>/u/1guyonearth</name>
      <uri>https://old.reddit.com/user/1guyonearth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking to purchase a ThinkPad (or Legion if necessary) for running local LLMs and would love some real-world experiences from the community.&lt;/p&gt; &lt;h1&gt;My Requirements:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Running Linux (prefer Fedora/Arch/openSUSE - NOT Ubuntu)&lt;/li&gt; &lt;li&gt;Local LLM inference (7B-70B parameter models)&lt;/li&gt; &lt;li&gt;Professional build quality preferred&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;My Dilemma:&lt;/h1&gt; &lt;p&gt;I'm torn between NVIDIA and AMD graphics. Historically, I've had frustrating experiences with NVIDIA proprietary drivers on Linux (driver conflicts, kernel updates breaking things, etc.), but I also know CUDA ecosystem is still dominant for LLM frameworks like llama.cpp, Ollama, and others.&lt;/p&gt; &lt;h1&gt;Specific Questions:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;For NVIDIA users (RTX 4070/4080/4090 mobile):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How has your recent experience been with NVIDIA drivers on non-Ubuntu distros?&lt;/li&gt; &lt;li&gt;Any issues with driver stability during kernel updates?&lt;/li&gt; &lt;li&gt;Which distro handles NVIDIA best in your experience?&lt;/li&gt; &lt;li&gt;Performance with popular LLM tools (Ollama, llama.cpp, etc.)?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;For AMD users (RX 7900M or similar):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How mature is ROCm support now for LLM inference?&lt;/li&gt; &lt;li&gt;Any compatibility issues with popular LLM frameworks?&lt;/li&gt; &lt;li&gt;Performance comparison vs NVIDIA if you've used both?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ThinkPad-specific:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;P1 Gen 6/7 vs Legion Pro 7i for sustained workloads?&lt;/li&gt; &lt;li&gt;Thermal performance during extended inference sessions?&lt;/li&gt; &lt;li&gt;Linux compatibility issues with either line?&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Current Considerations:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;ThinkPad P1 Gen 7 (RTX 4090 mobile) - premium price but professional build&lt;/li&gt; &lt;li&gt;Legion Pro 7i (RTX 4090 mobile) - better price/performance, gaming design&lt;/li&gt; &lt;li&gt;Any AMD alternatives worth considering?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would really appreciate hearing from anyone running LLMs locally on modern ThinkPads or Legions with Linux. What's been your actual day-to-day experience?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1guyonearth"&gt; /u/1guyonearth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1my7010/thinkpad_for_local_llm_inference_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1my7010/thinkpad_for_local_llm_inference_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1my7010/thinkpad_for_local_llm_inference_linux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-23T17:09:02+00:00</published>
  </entry>
</feed>
