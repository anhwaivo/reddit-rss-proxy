<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-31T12:26:12+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ie4nbl</id>
    <title>Uninstalling deepseek LLM</title>
    <updated>2025-01-31T02:32:40+00:00</updated>
    <author>
      <name>/u/CraftyOwl21</name>
      <uri>https://old.reddit.com/user/CraftyOwl21</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi - Via Terminal, I installed locally at least two different versions of DeepSeek via Ollama. How do I now uninstall everything?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CraftyOwl21"&gt; /u/CraftyOwl21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie4nbl/uninstalling_deepseek_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie4nbl/uninstalling_deepseek_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie4nbl/uninstalling_deepseek_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T02:32:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1idvl47</id>
    <title>Model for language learning?</title>
    <updated>2025-01-30T19:44:33+00:00</updated>
    <author>
      <name>/u/KiRa937</name>
      <uri>https://old.reddit.com/user/KiRa937</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m currently learning Japanese and I want to use AI in my studies. The most obvious way is to ask “explain word A in context of sentence B”, but I also thought maybe it is possible to analyze the material in my target language. Something like “here’s a bunch of texts, order them by grammar difficulty”. Or maybe “I’m interested in vocabulary of topic X, so order texts by level of use of said vocabulary”. So what model should I use for language learning? And for future reference what models would work for different languages if there’s no “one fits all” model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KiRa937"&gt; /u/KiRa937 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idvl47/model_for_language_learning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idvl47/model_for_language_learning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idvl47/model_for_language_learning/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T19:44:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1idsvfd</id>
    <title>Open WebUI not working after 0.5.6, "Network Problem" 400 Bad Request (Docker)</title>
    <updated>2025-01-30T17:52:08+00:00</updated>
    <author>
      <name>/u/ucffool</name>
      <uri>https://old.reddit.com/user/ucffool</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Even since I pulled down 0.5.6 or 0.5.7, the container runs and seems fine, but talking to Ollama or with Together.ai both fail. It can retrieve the list of models just fine, but it seems to be an issue with completions.&lt;/p&gt; &lt;p&gt;Anyone else having this problem? Do I need to blow it away and do a completely clean install, loading settings for exports?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;2025-01-30 10:50:03 INFO: connection rejected (400 Bad Request) 2025-01-30 10:50:03 INFO: connection closed &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ucffool"&gt; /u/ucffool &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idsvfd/open_webui_not_working_after_056_network_problem/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idsvfd/open_webui_not_working_after_056_network_problem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idsvfd/open_webui_not_working_after_056_network_problem/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T17:52:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie1jzx</id>
    <title>Remove &lt;think&gt; tags?</title>
    <updated>2025-01-31T00:01:59+00:00</updated>
    <author>
      <name>/u/midlivecrisis</name>
      <uri>https://old.reddit.com/user/midlivecrisis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apologies in advance if this has already been answered. Is there a way to force the smaller Deepseek models (7b, 14b running on Ollama) to NOT return the &amp;lt;think&amp;gt; tags and thinking content - and to only return the end response? Is there a parameter I missed where you can disable that content? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/midlivecrisis"&gt; /u/midlivecrisis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1jzx/remove_think_tags/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1jzx/remove_think_tags/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie1jzx/remove_think_tags/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T00:01:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie1s1h</id>
    <title>Running DeepSeek-R1-GGUF from Unsloth with Ollama</title>
    <updated>2025-01-31T00:12:35+00:00</updated>
    <author>
      <name>/u/WitcherSanek</name>
      <uri>https://old.reddit.com/user/WitcherSanek</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How to run this model on Windows with ollama &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-Q2%5C_K%5C_XL"&gt;https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-Q2\_K\_XL&lt;/a&gt;?&lt;/p&gt; &lt;p&gt;Huggingface has instruction &amp;quot;ollama run hf.co/unsloth/DeepSeek-R1-GGUF:Q2_K_XL&amp;quot; which leads to error &amp;quot;Error: pull model manifest: 400: The specified repository contains sharded GGUF. Ollama does not support this yet. Follow this issue for more info:&amp;quot;.&lt;/p&gt; &lt;p&gt;Downloading this files locally and running &amp;quot;llama-gguf-split --merge&amp;quot; leads to instant creation of empty file without any error or info message.&lt;/p&gt; &lt;p&gt;Manual concatenation with copy -b allows model to be imported, but &amp;quot;ollama run R1:latest&amp;quot; leads to error &amp;quot;Error: llama runner process has terminated: error loading model: invalid split file: D:\Ollama\blobs\sha256-311b7e2b72da29daffbac5e5f5df9353b1b3be9879d22d1dc498ece99529cfe5&amp;quot;.&lt;/p&gt; &lt;p&gt;What is wrong with my attempts? Am i missing something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WitcherSanek"&gt; /u/WitcherSanek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1s1h/running_deepseekr1gguf_from_unsloth_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1s1h/running_deepseekr1gguf_from_unsloth_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie1s1h/running_deepseekr1gguf_from_unsloth_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T00:12:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie4s8g</id>
    <title>Running ollama locally on my phone, very weird responses</title>
    <updated>2025-01-31T02:39:55+00:00</updated>
    <author>
      <name>/u/NorthropChicken</name>
      <uri>https://old.reddit.com/user/NorthropChicken</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ie4s8g/running_ollama_locally_on_my_phone_very_weird/"&gt; &lt;img alt="Running ollama locally on my phone, very weird responses" src="https://preview.redd.it/sjp5jds6s8ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08630c765c18cf9a40aa0f10dcb2b7c621b68c1e" title="Running ollama locally on my phone, very weird responses" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tinyllama on Pixel 5&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NorthropChicken"&gt; /u/NorthropChicken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sjp5jds6s8ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie4s8g/running_ollama_locally_on_my_phone_very_weird/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie4s8g/running_ollama_locally_on_my_phone_very_weird/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T02:39:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1idv02o</id>
    <title>Has anyone been using the base M4 Mac Mini as an Ollama server and want to share their mileage?</title>
    <updated>2025-01-30T19:20:09+00:00</updated>
    <author>
      <name>/u/_AdamWTF</name>
      <uri>https://old.reddit.com/user/_AdamWTF</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently have a home lab setup that spans a few different machines but I don’t currently have anything capable of running AI at decent speeds, I was going to Frankenstein together a machine using spare parts I have laying around, but getting a decent GPU with acceptable vram in the UK is so expensive right now.&lt;/p&gt; &lt;p&gt;This brings me to the Mac Mini M4, looking at just the base model and getting it for around £550, it seems like a lot of power for a decent price. I’m just curious how good the performance really is, I’m expecting with something like a 3b model of say llama 3.2 the response for 5k context tokens to be &amp;lt; 3 seconds to be suitable for me. &lt;/p&gt; &lt;p&gt;If anyone is willing to share their experiences and some examples of the kind of speeds you’re getting I’d really appreciate it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_AdamWTF"&gt; /u/_AdamWTF &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idv02o/has_anyone_been_using_the_base_m4_mac_mini_as_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idv02o/has_anyone_been_using_the_base_m4_mac_mini_as_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idv02o/has_anyone_been_using_the_base_m4_mac_mini_as_an/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T19:20:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1idxa19</id>
    <title>Fine tuning for small models</title>
    <updated>2025-01-30T20:55:48+00:00</updated>
    <author>
      <name>/u/jcrowe</name>
      <uri>https://old.reddit.com/user/jcrowe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an app that pulls specific information from a website. Right now I scrape the page, convert it to text and upload it to chatgpt to get a json file of data. Works great…&lt;/p&gt; &lt;p&gt;What I would like to do is switch over to a small LLM model (llama3.2:1b for example). But this model doesn’t return the results I want.&lt;/p&gt; &lt;p&gt;I would like to fine tune this model to make it work for my use case.&lt;/p&gt; &lt;p&gt;Can anyone recommend a tutorial for this or tell me I’m an idiot if it’s not the right way to use the technology?&lt;/p&gt; &lt;p&gt;ETA: to clarify, I don’t want to fine tune for the model to have the new information, I want to fine tune so that it knows how to deal with this information. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jcrowe"&gt; /u/jcrowe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idxa19/fine_tuning_for_small_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idxa19/fine_tuning_for_small_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idxa19/fine_tuning_for_small_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T20:55:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie5k9y</id>
    <title>Facing errors while installing deepseek r1 1.5b through ollama in windows</title>
    <updated>2025-01-31T03:21:10+00:00</updated>
    <author>
      <name>/u/AndreoBee100</name>
      <uri>https://old.reddit.com/user/AndreoBee100</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ie5k9y/facing_errors_while_installing_deepseek_r1_15b/"&gt; &lt;img alt="Facing errors while installing deepseek r1 1.5b through ollama in windows" src="https://preview.redd.it/dmv3g0miz8ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f773d3a58be867c97bec65c82e0a6ef252cea3e6" title="Facing errors while installing deepseek r1 1.5b through ollama in windows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AndreoBee100"&gt; /u/AndreoBee100 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dmv3g0miz8ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie5k9y/facing_errors_while_installing_deepseek_r1_15b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie5k9y/facing_errors_while_installing_deepseek_r1_15b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T03:21:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1idpjtj</id>
    <title>What would be the simplest way to train deepseek model on self hosted server.</title>
    <updated>2025-01-30T15:31:52+00:00</updated>
    <author>
      <name>/u/shaxadhere</name>
      <uri>https://old.reddit.com/user/shaxadhere</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to train model on specific set of data, I have the data in raw text and I have questions and answers from that raw text.&lt;/p&gt; &lt;p&gt;Is there any way I can train my model on all of that data.&lt;/p&gt; &lt;p&gt;Documents in total are 400,000 pages, and questions and answers are around 1 million+&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shaxadhere"&gt; /u/shaxadhere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idpjtj/what_would_be_the_simplest_way_to_train_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idpjtj/what_would_be_the_simplest_way_to_train_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idpjtj/what_would_be_the_simplest_way_to_train_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T15:31:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1idvw46</id>
    <title>Recommended deepseek model for coding tasks for an average PC?</title>
    <updated>2025-01-30T19:57:31+00:00</updated>
    <author>
      <name>/u/Upset_Hippo_5304</name>
      <uri>https://old.reddit.com/user/Upset_Hippo_5304</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Want to join the gang and try this stuff for coding tasks.&lt;/p&gt; &lt;p&gt;Py, dart, js, c#&lt;/p&gt; &lt;p&gt;32GB ram RTX 3070 Ryzen 5 5600x&lt;/p&gt; &lt;p&gt;Cheers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Upset_Hippo_5304"&gt; /u/Upset_Hippo_5304 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idvw46/recommended_deepseek_model_for_coding_tasks_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idvw46/recommended_deepseek_model_for_coding_tasks_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idvw46/recommended_deepseek_model_for_coding_tasks_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T19:57:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie6fzy</id>
    <title>Empty response for Deepseek Models</title>
    <updated>2025-01-31T04:09:36+00:00</updated>
    <author>
      <name>/u/TheHarinator</name>
      <uri>https://old.reddit.com/user/TheHarinator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone have a problem when trying to interact with the Deepseek-r1 models? The response always is empty for me. I have tried removing and re-pulling these models too. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheHarinator"&gt; /u/TheHarinator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie6fzy/empty_response_for_deepseek_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie6fzy/empty_response_for_deepseek_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie6fzy/empty_response_for_deepseek_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T04:09:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1iealuo</id>
    <title>why Ai Model requirement always show Memory instead GPU VRAM size?</title>
    <updated>2025-01-31T08:51:06+00:00</updated>
    <author>
      <name>/u/TheLastAirbender2025</name>
      <uri>https://old.reddit.com/user/TheLastAirbender2025</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, &lt;/p&gt; &lt;p&gt;Apologies in advance i am so new to Ai and bit confused here about term RAM mean. Does it mean memory in the GPU or Memory physically on the pc? I am windows user and my pc is bit old meaning 13 years at lest if not longer. &lt;/p&gt; &lt;p&gt;Example &lt;/p&gt; &lt;ul&gt; &lt;li&gt;7b models generally require at least 8GB of RAM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So when i installed a model onto my pc i see in system resources Ram hitting 9 GB and CPU % is like 30 but GPU is not even hitting 1% and 3D part show no activity. &lt;/p&gt; &lt;p&gt;I have Sparkle Intel Arc A380 ELF, 6GB GDDR6, Single Fan, SA380E-6G &lt;/p&gt; &lt;p&gt;Can someone explain this to me please &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLastAirbender2025"&gt; /u/TheLastAirbender2025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iealuo/why_ai_model_requirement_always_show_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iealuo/why_ai_model_requirement_always_show_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iealuo/why_ai_model_requirement_always_show_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T08:51:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieam9t</id>
    <title>I can't see qwen2.5 max (the latest model) in ollama models list ? how can I run this locally ?</title>
    <updated>2025-01-31T08:52:01+00:00</updated>
    <author>
      <name>/u/AxelBlaze20850</name>
      <uri>https://old.reddit.com/user/AxelBlaze20850</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AxelBlaze20850"&gt; /u/AxelBlaze20850 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieam9t/i_cant_see_qwen25_max_the_latest_model_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieam9t/i_cant_see_qwen25_max_the_latest_model_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ieam9t/i_cant_see_qwen25_max_the_latest_model_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T08:52:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1idh8ft</id>
    <title>Deepseek r1 671b on my local PC</title>
    <updated>2025-01-30T07:03:27+00:00</updated>
    <author>
      <name>/u/Geschirrtuch</name>
      <uri>https://old.reddit.com/user/Geschirrtuch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;Two days ago, I turned night into day, and in the end, I managed to get R1 running on my local PC. Yesterday, I uploaded a video on YouTube showing how I did it: &lt;a href="https://www.youtube.com/watch?v=O3Lk3xSkAdk"&gt;https://www.youtube.com/watch?v=O3Lk3xSkAdk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I don't post here often, so I'm not sure if sharing the link is okay—I hope it is.&lt;/p&gt; &lt;p&gt;The video is in German, but with subtitles, everyone should be able to understand it.&lt;br /&gt; Be careful if you want to try this yourself! ;)&lt;/p&gt; &lt;p&gt;Update:&lt;/p&gt; &lt;p&gt;For those who don't feel like watching the video: The &amp;quot;trick&amp;quot; was using Windows' pagefile. I set up three of them on three different SSDs, which gave me around 750GB of virtual memory in total.&lt;/p&gt; &lt;p&gt;Loading the model and answering a question took my PC about 90 minutes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Geschirrtuch"&gt; /u/Geschirrtuch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T07:03:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie42ld</id>
    <title>Deepseek-r1:8b is 4 times slower than llama3.2:8b on Ollama running locally</title>
    <updated>2025-01-31T02:03:05+00:00</updated>
    <author>
      <name>/u/PawanAgarwal</name>
      <uri>https://old.reddit.com/user/PawanAgarwal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am running both llama3.2:8b and deepseek-r1:8b locally on my mac. I noticed than latency per token for deepseek-r1:8b model is 4x llama3.2:8b. Since both are 8b versions, I was hoping latency would be similar. Anyone else also seeing that? Any configs needed for ollama for deepseek serving?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PawanAgarwal"&gt; /u/PawanAgarwal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie42ld/deepseekr18b_is_4_times_slower_than_llama328b_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie42ld/deepseekr18b_is_4_times_slower_than_llama328b_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie42ld/deepseekr18b_is_4_times_slower_than_llama328b_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T02:03:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1iebe80</id>
    <title>Deepseek unsecure?</title>
    <updated>2025-01-31T09:53:57+00:00</updated>
    <author>
      <name>/u/Emergency-Radish-696</name>
      <uri>https://old.reddit.com/user/Emergency-Radish-696</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My deepseek r1 will not do creative writing when I give it inputs with people's names or business. ( Writing sci-fi ) Is there an open unsecure model that isn't so bashful?( A deepseek model: unsecure?)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emergency-Radish-696"&gt; /u/Emergency-Radish-696 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iebe80/deepseek_unsecure/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iebe80/deepseek_unsecure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iebe80/deepseek_unsecure/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T09:53:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieby8e</id>
    <title>Is there a way to limit the number of concurrent downloads when pulling a model?</title>
    <updated>2025-01-31T10:35:17+00:00</updated>
    <author>
      <name>/u/MarinatedPickachu</name>
      <uri>https://old.reddit.com/user/MarinatedPickachu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm on windows and I get a lot of problems when pulling models, often the progress bar reverts to a lower number and many times i end up getting a &amp;quot;max retries exceeded&amp;quot; error completely cancelling the pull. Sometimes it succeeds, but rarely. I think it's caused by ollama creating many parallel downloads at once and cancelling them as soon as they stall for 5 seconds. Is there a way to limit the number of simultaneous downoads or to increase the timeout?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarinatedPickachu"&gt; /u/MarinatedPickachu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieby8e/is_there_a_way_to_limit_the_number_of_concurrent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieby8e/is_there_a_way_to_limit_the_number_of_concurrent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ieby8e/is_there_a_way_to_limit_the_number_of_concurrent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T10:35:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iecg2d</id>
    <title>What is wrong with the fp16 llama3.2 model?</title>
    <updated>2025-01-31T11:11:12+00:00</updated>
    <author>
      <name>/u/Octopus0nFire</name>
      <uri>https://old.reddit.com/user/Octopus0nFire</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iecg2d/what_is_wrong_with_the_fp16_llama32_model/"&gt; &lt;img alt="What is wrong with the fp16 llama3.2 model?" src="https://a.thumbs.redditmedia.com/X9t_AQnuJ6IunxNin7I8EXVXmN37kil_4Wm0LgjbQo8.jpg" title="What is wrong with the fp16 llama3.2 model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4pyucoo0bbge1.png?width=639&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67afdd30cb3efb720edb25967e88677f6168f6e3"&gt;https://preview.redd.it/4pyucoo0bbge1.png?width=639&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67afdd30cb3efb720edb25967e88677f6168f6e3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model with the tag 'latest' work as expexted, but the fp16 (and also the q8_0 llama 3.1 model) seem to be absolutely insane. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Octopus0nFire"&gt; /u/Octopus0nFire &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iecg2d/what_is_wrong_with_the_fp16_llama32_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iecg2d/what_is_wrong_with_the_fp16_llama32_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iecg2d/what_is_wrong_with_the_fp16_llama32_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T11:11:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1iecgob</id>
    <title>LLama2 absurd response</title>
    <updated>2025-01-31T11:12:29+00:00</updated>
    <author>
      <name>/u/ppadiya</name>
      <uri>https://old.reddit.com/user/ppadiya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iecgob/llama2_absurd_response/"&gt; &lt;img alt="LLama2 absurd response" src="https://b.thumbs.redditmedia.com/4MKGfDk6ED6EOXxngZ1hATaXUNpkIBZRADr4clqZfpE.jpg" title="LLama2 absurd response" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/cwaw8c13bbge1.png?width=1731&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a56793cfeaefc285ee5bb39163047556a21c49b7"&gt;https://preview.redd.it/cwaw8c13bbge1.png?width=1731&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a56793cfeaefc285ee5bb39163047556a21c49b7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What is happening here? I just loaded it and asked 'Hi, How are you' and it went on and on about learning Chinese. I had to interrupt it as it went on and on for over 5 mins.&lt;br /&gt; I then loaded it on open-webui and asked the same question. it again started a very long response on 'aspiring writers and authors' &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ppadiya"&gt; /u/ppadiya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iecgob/llama2_absurd_response/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iecgob/llama2_absurd_response/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iecgob/llama2_absurd_response/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T11:12:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie1aui</id>
    <title>Does `ollama create` actually build a new model?</title>
    <updated>2025-01-30T23:50:04+00:00</updated>
    <author>
      <name>/u/homelab2946</name>
      <uri>https://old.reddit.com/user/homelab2946</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded a GGUF file and create a Modelfile to extend it. Then I run `ollama create model_name -f Modelfile`, a `model_name:latest` model is created and shows in `ollama list` 10 GB. The GGUF file is also around the same 10 GB. Does `ollama create` not just add instruction on a base model but actually acting more like `docker build`? Would it then be fine to remove the GGUF file after the build?&lt;/p&gt; &lt;p&gt;Another scenario is through a supported ollama model, like `llama3`. Does Ollama &amp;quot;build&amp;quot; a new image if I create a new Modelfile from `llama3`, so it takes double the storage?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/homelab2946"&gt; /u/homelab2946 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1aui/does_ollama_create_actually_build_a_new_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1aui/does_ollama_create_actually_build_a_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie1aui/does_ollama_create_actually_build_a_new_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T23:50:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iecleb</id>
    <title>Running DeepSeek R1 on my M4 Pro Mac mini with Ollama</title>
    <updated>2025-01-31T11:21:31+00:00</updated>
    <author>
      <name>/u/ope_poe</name>
      <uri>https://old.reddit.com/user/ope_poe</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ope_poe"&gt; /u/ope_poe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/macmini/comments/1idfuew/running_deepseek_r1_on_my_m4_pro_mac_mini_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iecleb/running_deepseek_r1_on_my_m4_pro_mac_mini_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iecleb/running_deepseek_r1_on_my_m4_pro_mac_mini_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T11:21:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1idqxto</id>
    <title>Why Are All Local AI Models So Bad? No One Talks About This!</title>
    <updated>2025-01-30T16:31:59+00:00</updated>
    <author>
      <name>/u/NikkEvan</name>
      <uri>https://old.reddit.com/user/NikkEvan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with local AI models, even &amp;quot;high-end ones&amp;quot; like the recent DeepSeek-R1 32B, using Open WebUI.&lt;br /&gt; I expected them to be weaker than online models, but the gap is just ridiculous.&lt;br /&gt; Even for the simplest questions, they either fail, give nonsense answers, or completely misunderstand the input.&lt;/p&gt; &lt;p&gt;I’ve set the parameters and all the settings at the best i could, tried different setups, system prompts, and still , even after parsing a basic document just a few pages long, is a struggle.&lt;br /&gt; If it already fails here, how am I supposed to use it for hundreds of internal company documents?&lt;/p&gt; &lt;p&gt;The crazy part? No one talks about this!&lt;br /&gt; Instead, i see every video in youtube saying :&lt;br /&gt; &amp;quot;How to run locally (modelname) much better than chat-gpt&amp;quot;&lt;br /&gt; &amp;quot;Local Deepseek beats Chat-gpt&amp;quot;&lt;br /&gt; Than the question they ask to those local models are : How many 'R' are in the word Strawberry and the model answer: 2 ... lol &lt;/p&gt; &lt;p&gt;Why is the performance so bad, even on 32B models?&lt;/p&gt; &lt;p&gt;Why are there no proper guides to get the best out of local AI?&lt;br /&gt; Having a big hardware such as the Nvidia project DIGITS will make a big model work close to the online Chat-gpt 3 or 4 ? I see those has 175b parameters. &lt;/p&gt; &lt;p&gt;What are we missing?&lt;/p&gt; &lt;p&gt;I really want to make local AI work as close as the online models, even buying bigger and stronger hardware, but, right now, it just feels like a waste of time.&lt;br /&gt; Has anyone actually succeeded in making these models work well? If so, how? And , what do you intend for Working Well for a local Model ? &lt;/p&gt; &lt;p&gt;Let’s discuss this! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NikkEvan"&gt; /u/NikkEvan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T16:31:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieb1za</id>
    <title>WARNING: Major Price Increase for Cursor’s Agentic Composer — 25x Hike</title>
    <updated>2025-01-31T09:26:56+00:00</updated>
    <author>
      <name>/u/pokemontra4321</name>
      <uri>https://old.reddit.com/user/pokemontra4321</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One of the best parts about cursor was agentic composer.&lt;/p&gt; &lt;p&gt;I’ve been using Cursor extensively for its Agentic Composer feature, where multiple tool calls (up to 25) within one composer message request used to count as a single “fast” request. Now, each tool call is billed separately, meaning quotas are used up at a much faster rate — effectively a 25x increase.&lt;/p&gt; &lt;p&gt;Now, you easily burn through your fast requests in a few days. Even the slow requests are not traffic based anymore, instead there are now hardcoded time limits. (Edit: I am using 0.45.7 - I had a timer countdown that seems to increase everytime. looked like exponential backoff mechanism kinda-thingy. But I could be wrong, it could be just traffic related.)&lt;/p&gt; &lt;p&gt;I loved how amazing cursor was. But 96% shrinkflation!!! What the actual fuck? This is bullshit.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;What are some good alternatives to cursor? I’ll compile a list here. (In no particular order)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/features/copilot"&gt;GithubCopilot&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.jetbrains.com/ai/"&gt;JetBrains with AI assistant&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://aws.amazon.com/q/developer/"&gt;Amazon Q Developer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://openai.com/index/introducing-canvas/?utm_source=chatgpt.com"&gt;ChatGPT Canvas&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.continue.dev/?utm_source=chatgpt.com"&gt;Continue with Ollama&lt;/a&gt; | &lt;a href="https://ollama.com/blog/continue-code-assistant"&gt;blog post from Ollama&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://codeium.com/windsurf"&gt;WindSurf&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.tabnine.com/"&gt;TabNine&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://codeium.com/"&gt;Codeium&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.trae.ai/"&gt;Trae&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://voideditor.com/"&gt;Void&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://sourcegraph.com/cody"&gt;Cody&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://zed.dev/"&gt;Zed&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://refact.ai"&gt;refact.ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.kite.com/blog/product/kite-is-saying-farewell/"&gt;Kite&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://aide.dev/"&gt;aide.dev&lt;/a&gt; | &lt;a href="https://aider.chat/"&gt;aider.chat&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://shelbula.dev/"&gt;Shelbula.dev&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;@Cursor - maybe add the US hosted deepseek R1 model in agent composer? I think that could help? but tbh, sonnet 3.5 v2 with agent composer was pretty darn solid for me!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pokemontra4321"&gt; /u/pokemontra4321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieb1za/warning_major_price_increase_for_cursors_agentic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieb1za/warning_major_price_increase_for_cursors_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ieb1za/warning_major_price_increase_for_cursors_agentic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T09:26:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie210z</id>
    <title>Got Deepseek R1 1.5b running locally on Pixel 8 pro</title>
    <updated>2025-01-31T00:24:19+00:00</updated>
    <author>
      <name>/u/Teradyyne</name>
      <uri>https://old.reddit.com/user/Teradyyne</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"&gt; &lt;img alt="Got Deepseek R1 1.5b running locally on Pixel 8 pro" src="https://external-preview.redd.it/cTk0emwwZ3ozOGdlMYbCJM1MQLfOpw8fF1FnxkqAh7visCMP7lFjyVXppg7i.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=263215164e10efcd203b94bae4672c47e11c63d8" title="Got Deepseek R1 1.5b running locally on Pixel 8 pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Teradyyne"&gt; /u/Teradyyne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cha0fplz38ge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T00:24:19+00:00</published>
  </entry>
</feed>
