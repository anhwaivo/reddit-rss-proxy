<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-12T13:24:56+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1imnu7l</id>
    <title>CPU only crashing</title>
    <updated>2025-02-11T02:18:49+00:00</updated>
    <author>
      <name>/u/justusiv</name>
      <uri>https://old.reddit.com/user/justusiv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From a pure curiosity aspect i was wondering how differently a Q4 vs a FP16 model would perform in a cpu only mode(Follow up question is how would this scale to models that is lets say 671B parameters). i started my testing with 7B and 14B models. I added CUDA_VISIBLE_DEVICES=-1 to my system variable and noticed ollama did not take any vram. However at this point ollama would continually crash and i could never actually get the verbose data as it would never complete a single prompt.&lt;/p&gt; &lt;p&gt;*EDIT*&lt;/p&gt; &lt;p&gt;I upgraded to 0.5.8 from 0.5.7 and i dont appear to have this problem anymore.&lt;/p&gt; &lt;p&gt;*EDIT2*&lt;/p&gt; &lt;p&gt;Spoke to soon still crashing&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/justusiv"&gt; /u/justusiv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imnu7l/cpu_only_crashing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imnu7l/cpu_only_crashing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1imnu7l/cpu_only_crashing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T02:18:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1img1gn</id>
    <title>Forgetting /bye and Deepseek reasoning that I might be upset.</title>
    <updated>2025-02-10T20:23:10+00:00</updated>
    <author>
      <name>/u/hawkedmd</name>
      <uri>https://old.reddit.com/user/hawkedmd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; .exit&lt;/p&gt; &lt;p&gt;&amp;lt;think&amp;gt;&lt;/p&gt; &lt;p&gt;Alright, the user typed &amp;quot;exit&amp;quot;. They might be trying to leave the conversation. Maybe they're upset or just finished what they needed help with.&lt;/p&gt; &lt;p&gt;I should acknowledge their choice and let them know I'm here if they change their mind. It's important not to push too hard but keep the door open for further assistance.&lt;/p&gt; &lt;p&gt;Expressing understanding and offering continued support in a friendly manner is probably the best approach.&lt;/p&gt; &lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;Understood! If you need anything else or just want to chat, feel free to come back anytimeâ€”I'm here for you! ðŸ˜Š&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/p&gt; &lt;p&gt;Use Ctrl + d or /bye to exit.&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hawkedmd"&gt; /u/hawkedmd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1img1gn/forgetting_bye_and_deepseek_reasoning_that_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1img1gn/forgetting_bye_and_deepseek_reasoning_that_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1img1gn/forgetting_bye_and_deepseek_reasoning_that_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T20:23:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1imqb05</id>
    <title>Newbie question about context sizes.</title>
    <updated>2025-02-11T04:24:47+00:00</updated>
    <author>
      <name>/u/Private-Citizen</name>
      <uri>https://old.reddit.com/user/Private-Citizen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im writing my own thing to use the API &lt;code&gt;/api/chat&lt;/code&gt; end point.&lt;/p&gt; &lt;p&gt;I am managing the context window by pruning older prompts as the tokens get near full.&lt;/p&gt; &lt;p&gt;The question is, when i prune the oldest prompt, which naturally the first time will be a &amp;quot;user&amp;quot; prompt, should i also automatically equally prune the corresponding &amp;quot;assistant&amp;quot; prompt? Will it trip up the model to see an &amp;quot;assistant&amp;quot; prompt after the &amp;quot;system&amp;quot; prompt? Or is it safe to go [&amp;quot;system&amp;quot;, &amp;quot;assistant&amp;quot;, &amp;quot;user&amp;quot;, &amp;quot;assistant&amp;quot;, &amp;quot;user&amp;quot;] so the model has that little extra context?&lt;/p&gt; &lt;h1&gt;Follow up questions...&lt;/h1&gt; &lt;p&gt;ollama (&lt;em&gt;or system under ollama&lt;/em&gt;) seems to be using some kind of caching in the GPU vram storing your last prompt/context window. The responses are smooth until you change the context history by pruning one. I image something in cached memory is being re-juggled because anytime i need to prune the history the model delays responding. Same as the waiting when you first load/run the model up. I can also see during this waiting the GPU is pegged out to the max which is why i assume its re-caching.&lt;/p&gt; &lt;p&gt;I assume no way around this? I couldn't find in settings using the cli or API to disable this caching feature for testing. Any performance tweaks around this issue?&lt;/p&gt; &lt;p&gt;Do i even need to do manual pruning? Can i just keep stuffing oversized context history into the API and let the API / model do what it does to ignore too much context? Or will that create other issues like response accuracy?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Private-Citizen"&gt; /u/Private-Citizen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imqb05/newbie_question_about_context_sizes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imqb05/newbie_question_about_context_sizes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1imqb05/newbie_question_about_context_sizes/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T04:24:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1imi0fi</id>
    <title>How to get started with AMD Zen 4 (Ryzen 7 8845HS)</title>
    <updated>2025-02-10T21:43:53+00:00</updated>
    <author>
      <name>/u/Morpheus90x</name>
      <uri>https://old.reddit.com/user/Morpheus90x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry folks, I am lost and do not know how to start best.&lt;/p&gt; &lt;p&gt;I searched the subreddit and web but I can't find a definitive answer.&lt;/p&gt; &lt;p&gt;I have a homeserver with an AMD Ryzen 7 8845HS (Zen 4).&lt;/p&gt; &lt;p&gt;I have 96 GB DDR5 RAM.&lt;/p&gt; &lt;p&gt;The CPU has a AMD 780M GPU built in - but no dedicated VRAM (?)&lt;/p&gt; &lt;p&gt;Also it has a dedicated AI NPU.&lt;/p&gt; &lt;p&gt;I was planning to also host a few smaller services on the machine and throw the ollama stack on top on proxmox, either as an LXC or via docker&lt;/p&gt; &lt;p&gt;Let's assume I have 64+ GB free only for ollama.&lt;/p&gt; &lt;p&gt;Questions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;lxc, docker or baremetal? Any significant differences?&lt;/li&gt; &lt;li&gt;Run ollama/models default on cpu or should I utilize gpu/npu capabilties? If so how?&lt;/li&gt; &lt;li&gt;Which model should I start with? I basically have two main use cases: &amp;quot;default chatbot&amp;quot; with file upload/analysis and text OCR. Maybe I should use two dedicated models? I was thinking of deepseek-r1 and minicpm-v.&lt;/li&gt; &lt;li&gt;Can I expect acceptable performance at all for my use cases? Or am I completely lost and should rather user chatgpt cloud?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks y'all - Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Morpheus90x"&gt; /u/Morpheus90x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imi0fi/how_to_get_started_with_amd_zen_4_ryzen_7_8845hs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imi0fi/how_to_get_started_with_amd_zen_4_ryzen_7_8845hs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1imi0fi/how_to_get_started_with_amd_zen_4_ryzen_7_8845hs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T21:43:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1im8t47</id>
    <title>I need your help with training an LMM</title>
    <updated>2025-02-10T15:33:20+00:00</updated>
    <author>
      <name>/u/karl27_</name>
      <uri>https://old.reddit.com/user/karl27_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Reddit,&lt;/p&gt; &lt;p&gt;(please let me know if this is the wrong sub for this kind of question, and I'll remove it.)&lt;br /&gt; (Also sorry for bad English)&lt;/p&gt; &lt;p&gt;So, my Boss just gave a New project to me: He wants me to Train an AI for my Institute, where we can Analyze Reaction Protocols (from Students) to find any Errors. Also he want it to be able to help us with Retrosynthesis (not as important as the first one).&lt;/p&gt; &lt;p&gt;The problem is: I'm a Lab assistant, i do know a Thing or two about PC's, but I'm not an expert in training an LLM. I'm struggling with quiet a lot of things. First i need to build a Machine to run the LLM, than i need to train it, to consistently analyze protocols. I've Googled quiet a lot the last Days, and learned some things, but I'm totally not comfortable in my knowledge. &lt;/p&gt; &lt;p&gt;I have a Budged of up to 4000â‚¬ for the Machine, which should be enough.&lt;br /&gt; I was thinking about 2 4060Ti with 16gb each, for the rest i would go with some decent but not over the top components, sins as far as i understood, only the VRAM and RAM is really important for this.&lt;/p&gt; &lt;p&gt;my Boss wants me to use DeepSeek, which I'm not sure if it would be the best choice for this kind of task, but i haven't really found something for what i need really. also I'm not really sure if i can even really train this kind of LLM or if i have to go from 0.&lt;/p&gt; &lt;p&gt;I would really appreciate any input. Thanks for your help.&lt;/p&gt; &lt;p&gt;TL;DR&lt;/p&gt; &lt;p&gt;I need help with building A Machine to train an LMM for some specific tasks and also with the Training itself&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/karl27_"&gt; /u/karl27_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1im8t47/i_need_your_help_with_training_an_lmm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1im8t47/i_need_your_help_with_training_an_lmm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1im8t47/i_need_your_help_with_training_an_lmm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T15:33:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1imwsd5</id>
    <title>Define literal output</title>
    <updated>2025-02-11T11:49:11+00:00</updated>
    <author>
      <name>/u/guuidx</name>
      <uri>https://old.reddit.com/user/guuidx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I like smol models, they're naive, less woke and listen well in general. But it's hard to get a literal value from it. I ask to grade how ethic the input is with a digit from 1 to 10 and ask to only respond with a number. Well, it responds with a number but with stuff around it. &lt;/p&gt; &lt;p&gt;Same for chatgpt even, it keeps responding sql queries in markdown. Dropt it! Now I strip the markdown but that's not very cool. &lt;/p&gt; &lt;p&gt;What a is the best way for a literal output? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/guuidx"&gt; /u/guuidx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imwsd5/define_literal_output/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imwsd5/define_literal_output/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1imwsd5/define_literal_output/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T11:49:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1in1xuc</id>
    <title>Load management questions</title>
    <updated>2025-02-11T16:01:17+00:00</updated>
    <author>
      <name>/u/Impossible_Art9151</name>
      <uri>https://old.reddit.com/user/Impossible_Art9151</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impossible_Art9151"&gt; /u/Impossible_Art9151 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/u_Impossible_Art9151/comments/1im9en3/load_management_questions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1in1xuc/load_management_questions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1in1xuc/load_management_questions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T16:01:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1imdf1h</id>
    <title>My experience with Mac Mini M4 and ollama models</title>
    <updated>2025-02-10T18:39:24+00:00</updated>
    <author>
      <name>/u/Fabulous_Can_2215</name>
      <uri>https://old.reddit.com/user/Fabulous_Can_2215</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1imdf1h/my_experience_with_mac_mini_m4_and_ollama_models/"&gt; &lt;img alt="My experience with Mac Mini M4 and ollama models" src="https://b.thumbs.redditmedia.com/3PpESgQA7u3Ba8-wSSN0S42xYYghfzrLBKOgNzih0OU.jpg" title="My experience with Mac Mini M4 and ollama models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;Yesterday I bought a MacMini M4 with 24 GB RAM.&lt;/p&gt; &lt;p&gt;I was worried that it wouldn't be enough to run 7b and 8b models but it even works fine with 14b models!&lt;/p&gt; &lt;p&gt;Memory goes yellow but nothing freezes in my system!&lt;/p&gt; &lt;p&gt;So, I'm more than impressed and absolutely in love with my new computer!&lt;/p&gt; &lt;p&gt;I didn't quite understand how to train models. So if you know any tutorials, I'd be happy!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wh5lz3mewcie1.jpg?width=3072&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d432f5eed816fe16a1bdf9f402597ab413857528"&gt;https://preview.redd.it/wh5lz3mewcie1.jpg?width=3072&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d432f5eed816fe16a1bdf9f402597ab413857528&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thank you all for your help and suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Can_2215"&gt; /u/Fabulous_Can_2215 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imdf1h/my_experience_with_mac_mini_m4_and_ollama_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imdf1h/my_experience_with_mac_mini_m4_and_ollama_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1imdf1h/my_experience_with_mac_mini_m4_and_ollama_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-10T18:39:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1in2z0h</id>
    <title>Compiling v0.5.8</title>
    <updated>2025-02-11T16:44:39+00:00</updated>
    <author>
      <name>/u/wahnsinnwanscene</name>
      <uri>https://old.reddit.com/user/wahnsinnwanscene</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to compile from source for v0.5.8 without avx2, avxnni , etc. I'm using gcc-14 but have gcc-9 as default. How do i disable avx2 etc. I tried cmake with gcc-14 but it hits the spot where it tries to compile for cpu flags that my processor doesn't have. Doesn't the build process detect the available flags? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wahnsinnwanscene"&gt; /u/wahnsinnwanscene &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1in2z0h/compiling_v058/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1in2z0h/compiling_v058/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1in2z0h/compiling_v058/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T16:44:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1imr1sd</id>
    <title>How to use Ollama and Open WebUI with Docker Compose [Part 4]</title>
    <updated>2025-02-11T05:06:54+00:00</updated>
    <author>
      <name>/u/geshan</name>
      <uri>https://old.reddit.com/user/geshan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1imr1sd/how_to_use_ollama_and_open_webui_with_docker/"&gt; &lt;img alt="How to use Ollama and Open WebUI with Docker Compose [Part 4]" src="https://external-preview.redd.it/Kjq3eLkn3NANokIJh38gy7x7eTf4VewbLYl79de-hAU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5bca5b0db8bb18176f2f6fcc5a037096d8d51af" title="How to use Ollama and Open WebUI with Docker Compose [Part 4]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geshan"&gt; /u/geshan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://geshan.com.np/blog/2025/02/ollama-docker-compose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imr1sd/how_to_use_ollama_and_open_webui_with_docker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1imr1sd/how_to_use_ollama_and_open_webui_with_docker/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T05:06:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1in2v80</id>
    <title>ollama WSL will not use GPU</title>
    <updated>2025-02-11T16:40:14+00:00</updated>
    <author>
      <name>/u/Beli_Mawrr</name>
      <uri>https://old.reddit.com/user/Beli_Mawrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I have ollama (llama_cpp_python) installed on my WSL. I am able to use nvidia-smi and nvcc, but for some reason all my layers are running on the CPU and take ages. Any idea what's going on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beli_Mawrr"&gt; /u/Beli_Mawrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1in2v80/ollama_wsl_will_not_use_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1in2v80/ollama_wsl_will_not_use_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1in2v80/ollama_wsl_will_not_use_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T16:40:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1imz722</id>
    <title>My app uses Mistral Small more than any other app on OpenRouter!</title>
    <updated>2025-02-11T14:00:30+00:00</updated>
    <author>
      <name>/u/No-Definition-2886</name>
      <uri>https://old.reddit.com/user/No-Definition-2886</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1imz722/my_app_uses_mistral_small_more_than_any_other_app/"&gt; &lt;img alt="My app uses Mistral Small more than any other app on OpenRouter!" src="https://preview.redd.it/jkxi43umniie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44e5277b1b2fd0b3a304702d6de7e37a77b94699" title="My app uses Mistral Small more than any other app on OpenRouter!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Definition-2886"&gt; /u/No-Definition-2886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jkxi43umniie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imz722/my_app_uses_mistral_small_more_than_any_other_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1imz722/my_app_uses_mistral_small_more_than_any_other_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T14:00:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1imqd0y</id>
    <title>Did ollama update and get faster?</title>
    <updated>2025-02-11T04:27:53+00:00</updated>
    <author>
      <name>/u/Logical-Egg</name>
      <uri>https://old.reddit.com/user/Logical-Egg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m running all the normal models and I swear theyâ€™re like 5 times faster. Even the bigger models are flying. Did I miss something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Logical-Egg"&gt; /u/Logical-Egg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imqd0y/did_ollama_update_and_get_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imqd0y/did_ollama_update_and_get_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1imqd0y/did_ollama_update_and_get_faster/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T04:27:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1indim5</id>
    <title>Help! RAGAS with Ollama â€“ Output Parser Failed &amp; Timeout Errors</title>
    <updated>2025-02-12T00:01:52+00:00</updated>
    <author>
      <name>/u/Repulsive-Diet-9322</name>
      <uri>https://old.reddit.com/user/Repulsive-Diet-9322</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to use &lt;strong&gt;RAGAS with Ollama&lt;/strong&gt; and keep running into frustrating errors.&lt;/p&gt; &lt;p&gt;I followed this tutorial: &lt;a href="https://www.youtube.com/watch?v=Ts2wDG6OEko&amp;amp;t=287s"&gt;https://www.youtube.com/watch?v=Ts2wDG6OEko&amp;amp;t=287s&lt;/a&gt;&lt;br /&gt; I also made sure my dataset is in the correct &lt;strong&gt;RAGAS format&lt;/strong&gt; and followed the documentation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Strangely, it works with the example dataset from the video and the one in the documentation, but not with my data.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;No matter what I try, I keep getting this error:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries. Prompt fix output format failed to parse output: The output parser failed to parse the output including retries. Prompt fix output format failed to parse output: The output parser failed to parse the output including retries. Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries. Exception raised in Job[8]: RagasOutputParserException(The output parser failed to parse the output including retries.)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And this happens &lt;strong&gt;for every metric&lt;/strong&gt;, not just one.&lt;/p&gt; &lt;p&gt;After a while, it just turns into:&lt;/p&gt; &lt;p&gt;&lt;code&gt;TimeoutError()&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I've spent &lt;strong&gt;3 days&lt;/strong&gt; trying to debug this, but I can't figure it out.&lt;br /&gt; Is anyone else facing this issue?&lt;br /&gt; Did you manage to fix it?&lt;br /&gt; I'd really appreciate any help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Repulsive-Diet-9322"&gt; /u/Repulsive-Diet-9322 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1indim5/help_ragas_with_ollama_output_parser_failed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1indim5/help_ragas_with_ollama_output_parser_failed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1indim5/help_ragas_with_ollama_output_parser_failed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T00:01:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ink671</id>
    <title>How can I run Ollama on windows (wsl2 ??) With openwebUi?</title>
    <updated>2025-02-12T05:52:29+00:00</updated>
    <author>
      <name>/u/Raners96</name>
      <uri>https://old.reddit.com/user/Raners96</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How can I run Ollama on windows (wsl2 ??) With openwebUi? Well i tried a few things but nothing worked. it did run but only on CPU. I have a 7900xtx. And I want to access OpenwebUi over the LAN,. Can someone help me?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Raners96"&gt; /u/Raners96 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ink671/how_can_i_run_ollama_on_windows_wsl2_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ink671/how_can_i_run_ollama_on_windows_wsl2_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ink671/how_can_i_run_ollama_on_windows_wsl2_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T05:52:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1inl37e</id>
    <title>Help in choosing right tool for help in academic writing.</title>
    <updated>2025-02-12T06:51:58+00:00</updated>
    <author>
      <name>/u/Fun_Repeat_3791</name>
      <uri>https://old.reddit.com/user/Fun_Repeat_3791</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I am very new to the world of large language models. I have recently joined as an assistant professor at a fairly renowned university. As part of my job, I have to do lots of writing such as grants, concept notes, conference and journal papers, class notes, etc. It is gradually becoming overwhelming. I was wondering if i can somehow utilise the large language models to help me. What I need. 1.Helper in writing my papers, grants in some parts which are common such as introduction, definitions, etc. 2. I have a fairly large corpus of my own writings such as my own papers, grants etc. sometimes it is just rehashing my old ideas into new. If I can get a tool. that can do this will be very helpful. &lt;/p&gt; &lt;p&gt;what I have 1. i can arrange large servers, large ram, gpu, etc for my work 2. i prefer open source tools but i can spend some initial amount around 200 USD. If it s recurring cost then it should not be more than 100 USD yearly. Can you please suggest me some tools that can be helpful for my issues? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun_Repeat_3791"&gt; /u/Fun_Repeat_3791 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inl37e/help_in_choosing_right_tool_for_help_in_academic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inl37e/help_in_choosing_right_tool_for_help_in_academic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inl37e/help_in_choosing_right_tool_for_help_in_academic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T06:51:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ina9uj</id>
    <title>Quickly deploy Ollama on the most affordable GPUs on the market</title>
    <updated>2025-02-11T21:41:08+00:00</updated>
    <author>
      <name>/u/Dylan-from-Shadeform</name>
      <uri>https://old.reddit.com/user/Dylan-from-Shadeform</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We made a template on our platform, Shadeform, to quickly deploy Ollama on the most affordable cloud GPUs on the market.&lt;/p&gt; &lt;p&gt;For context, Shadeform is a GPU marketplace for cloud providers like Lambda, Paperspace, Nebius, Datacrunch and more that lets you compare their on-demand pricing and spin up with one account.&lt;/p&gt; &lt;p&gt;This Ollama template lets you pre-load Ollama onto any of these instances, so it's ready to go as soon as the instance is active.&lt;/p&gt; &lt;p&gt;Takes &amp;lt; 5 min and works like butter.&lt;/p&gt; &lt;p&gt;Here's how it works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Follow &lt;a href="https://platform.shadeform.ai/templates/a1aaa5e1-d1ec-42ed-9261-ed69778cfa5a"&gt;this link&lt;/a&gt; to the Ollama template.&lt;/li&gt; &lt;li&gt;Click &amp;quot;Deploy Template&amp;quot;&lt;/li&gt; &lt;li&gt;Pick a GPU type&lt;/li&gt; &lt;li&gt;Pick the lowest priced listing&lt;/li&gt; &lt;li&gt;Click &amp;quot;Deploy&amp;quot;&lt;/li&gt; &lt;li&gt;Wait for the instance to become active&lt;/li&gt; &lt;li&gt;Download your private key and SSH&lt;/li&gt; &lt;li&gt;Run this command, and swap out the {model_name} with whatever you want&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker exec -it ollama ollama pull {model_name} &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Paste &lt;a href="http://localhost:8080"&gt;&lt;code&gt;http://localhost:8080&lt;/code&gt;&lt;/a&gt; into your browser&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dylan-from-Shadeform"&gt; /u/Dylan-from-Shadeform &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ina9uj/quickly_deploy_ollama_on_the_most_affordable_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ina9uj/quickly_deploy_ollama_on_the_most_affordable_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ina9uj/quickly_deploy_ollama_on_the_most_affordable_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T21:41:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1in88nw</id>
    <title>How many Ollama models can I have on my list.. but just running one at a time. That are 7b and I have 16 GB of RAM.. I run the Ollama via WSL. I have two models but wondering if I can fit several but just use one at a time..</title>
    <updated>2025-02-11T20:17:36+00:00</updated>
    <author>
      <name>/u/Emergency-Radish-696</name>
      <uri>https://old.reddit.com/user/Emergency-Radish-696</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emergency-Radish-696"&gt; /u/Emergency-Radish-696 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1in88nw/how_many_ollama_models_can_i_have_on_my_list_but/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1in88nw/how_many_ollama_models_can_i_have_on_my_list_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1in88nw/how_many_ollama_models_can_i_have_on_my_list_but/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T20:17:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1inb3pe</id>
    <title>Ollama spitting out gibberish on Windows 10 with RTX 3060. Only returning @ 'at' symbols to any and all prompts. How do I fix it?</title>
    <updated>2025-02-11T22:15:34+00:00</updated>
    <author>
      <name>/u/shittywhopper</name>
      <uri>https://old.reddit.com/user/shittywhopper</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shittywhopper"&gt; /u/shittywhopper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imgur.com/a/CErnNdv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inb3pe/ollama_spitting_out_gibberish_on_windows_10_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inb3pe/ollama_spitting_out_gibberish_on_windows_10_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T22:15:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1inofh0</id>
    <title>How to deploy deepseek-r1âˆ¶671b locally using Ollama?</title>
    <updated>2025-02-12T11:04:46+00:00</updated>
    <author>
      <name>/u/U2509</name>
      <uri>https://old.reddit.com/user/U2509</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 8 A100, each with 40GB video memory, and 1TB of RAM. How to deploy deepseek-r1âˆ¶671b locally? I cannot load the model using the video memory alone. Is there any parameter that Ollama can configure to load the model using my 1TB of RAM? thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/U2509"&gt; /u/U2509 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inofh0/how_to_deploy_deepseekr1671b_locally_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inofh0/how_to_deploy_deepseekr1671b_locally_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inofh0/how_to_deploy_deepseekr1671b_locally_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T11:04:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1inpm0l</id>
    <title>1-Click AI Tools in your browser - completely free to use with Ollama</title>
    <updated>2025-02-12T12:26:23+00:00</updated>
    <author>
      <name>/u/rajatrocks</name>
      <uri>https://old.reddit.com/user/rajatrocks</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1inpm0l/1click_ai_tools_in_your_browser_completely_free/"&gt; &lt;img alt="1-Click AI Tools in your browser - completely free to use with Ollama" src="https://external-preview.redd.it/Kc2hxFEu0Tcm7R53gZtdKq4h35EjhCxOfec1FTNUVPw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6b5d82b208e81c0db81ed49a653153e474089379" title="1-Click AI Tools in your browser - completely free to use with Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there - I built a Chrome/Edge extension called Ask Steve: &lt;a href="https://asksteve.to/"&gt;https://asksteve.to&lt;/a&gt; that gives you 1-Click AI Tools in your browser (along with Chat and several other integration points).&lt;/p&gt; &lt;p&gt;I recently added the ability to connect to local models for free and it works great with Ollama! Detailed instructions are here: &lt;a href="https://www.asksteve.to/docs/local-models"&gt;https://www.asksteve.to/docs/local-models&lt;/a&gt; - it does require a bit of additional config at startup to enable an extension to connect to Ollama's local server.&lt;/p&gt; &lt;p&gt;You can also assign specific models to Tools - so you can use a fast model like Phi for everyday Tools, and something like DeepSeek R1 for something that would benefit from a reasoning model.&lt;/p&gt; &lt;p&gt;If you get a chance to try it out, I'd welcome any feedback!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1inpm0l/video/li2i506cbpie1/player"&gt;Connect Ask Steve to Ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;0:00 - 1:18 Intro &amp;amp; Initial setup&lt;br /&gt; 2:26 - 3:10 Connect Ollama&lt;br /&gt; 4:00 - 5:56 Testing &amp;amp; assigning a specific model to a specific Tool&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rajatrocks"&gt; /u/rajatrocks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inpm0l/1click_ai_tools_in_your_browser_completely_free/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inpm0l/1click_ai_tools_in_your_browser_completely_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inpm0l/1click_ai_tools_in_your_browser_completely_free/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T12:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1inmmba</id>
    <title>Trying to setup Scourhead (an ai that can search the web) with Ollama but does not seem to work</title>
    <updated>2025-02-12T08:44:00+00:00</updated>
    <author>
      <name>/u/Medo1024</name>
      <uri>https://old.reddit.com/user/Medo1024</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to setup the app scourhead on my laptop (windows) and after download it says it needs Ollama and wants to download it, when i click on download it gives me a message that says 'scourhead was unable to download the model from Ollama, please insure Ollama is running, that the host and port are correct, and the model name is valid, then try again.' I checked the settings for the download and this is it 'Ollama Host: localhost OllamaPort: 11434 Model: llama3.2:3b. Pls help (ps: tried to download ollama and then restart the scourhead app but it still did not work)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Medo1024"&gt; /u/Medo1024 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inmmba/trying_to_setup_scourhead_an_ai_that_can_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inmmba/trying_to_setup_scourhead_an_ai_that_can_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inmmba/trying_to_setup_scourhead_an_ai_that_can_search/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T08:44:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1incg27</id>
    <title>One-liner RAG with Ollama</title>
    <updated>2025-02-11T23:13:25+00:00</updated>
    <author>
      <name>/u/yusufcanbayrak</name>
      <uri>https://old.reddit.com/user/yusufcanbayrak</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1incg27/oneliner_rag_with_ollama/"&gt; &lt;img alt="One-liner RAG with Ollama" src="https://external-preview.redd.it/aZg7arYZxu_ozI5IITwuT0FrG_0ip5ZXROM-WLJsfoU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe965c43d4c500c5565b2da8db64a4f480271700" title="One-liner RAG with Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've created tlm almost a year ago as an experimental project for CLI assistance. Now, introduce another feature that can be beneficial and more natural to use for RAG with open-source models using Ollama.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/yusufcanb/tlm/releases/tag/1.2"&gt;Release 1.2 Â· yusufcanb/tlm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/s3nufytdelie1.gif"&gt;tlm ask&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yusufcanbayrak"&gt; /u/yusufcanbayrak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1incg27/oneliner_rag_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1incg27/oneliner_rag_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1incg27/oneliner_rag_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T23:13:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1inp06h</id>
    <title>AMD 395 can run llama 70B without GPU</title>
    <updated>2025-02-12T11:51:44+00:00</updated>
    <author>
      <name>/u/grigio</name>
      <uri>https://old.reddit.com/user/grigio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Non enough but a good start&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/AMDGPU_/status/1889588690214637747"&gt;https://x.com/AMDGPU_/status/1889588690214637747&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grigio"&gt; /u/grigio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inp06h/amd_395_can_run_llama_70b_without_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inp06h/amd_395_can_run_llama_70b_without_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inp06h/amd_395_can_run_llama_70b_without_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T11:51:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ingiis</id>
    <title>GitHub Actions + Ollama = Free Compute</title>
    <updated>2025-02-12T02:27:09+00:00</updated>
    <author>
      <name>/u/Silent-Treat-6512</name>
      <uri>https://old.reddit.com/user/Silent-Treat-6512</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you guys do when you are bored? I created a simple AI bot which runs a full Ollama stack in Github Actions (free compute), pulls mistral model and ask for &amp;quot;some deep insight&amp;quot; this website now gets updated EVERY HOUR (Changed it to Daily) - Cost to run $0 &lt;/p&gt; &lt;p&gt;&lt;a href="https://ai.aww.sm/"&gt;https://ai.aww.sm/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Full code on GitHub, link on website. Let me know your thoughts.&lt;/p&gt; &lt;p&gt;Itâ€™s currently tasked to generate thoughts around Humans vs AI dominance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silent-Treat-6512"&gt; /u/Silent-Treat-6512 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ingiis/github_actions_ollama_free_compute/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ingiis/github_actions_ollama_free_compute/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ingiis/github_actions_ollama_free_compute/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T02:27:09+00:00</published>
  </entry>
</feed>
