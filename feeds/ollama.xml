<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-16T14:05:50+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ipekhy</id>
    <title>x2 RTX 3060 12GB VRAM</title>
    <updated>2025-02-14T16:32:47+00:00</updated>
    <author>
      <name>/u/VariousGrand</name>
      <uri>https://old.reddit.com/user/VariousGrand</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you think that having two RTX 360 with 12Gb VRAM each is enough to run deepseek-r1 32b?&lt;/p&gt; &lt;p&gt;Or there any other option you think it will have better performance?&lt;/p&gt; &lt;p&gt;Would be better maybe to have Titan RTX with 24gb of vram? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VariousGrand"&gt; /u/VariousGrand &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipekhy/x2_rtx_3060_12gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipekhy/x2_rtx_3060_12gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipekhy/x2_rtx_3060_12gb_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T16:32:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipmpr0</id>
    <title>What's the best LLM I can run with at least 10 t/s on 24 cores, 215GB ram &amp; 8GB vram?</title>
    <updated>2025-02-14T22:22:15+00:00</updated>
    <author>
      <name>/u/MarinatedPickachu</name>
      <uri>https://old.reddit.com/user/MarinatedPickachu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an older workstation with plenty of ram and 2x12 2.9ghz cores, but only an rtx 2070 super. The memory is afaik also divided into two numa-nodes, not sure how this would affect LLM performance. Is there anything interesting I could run on this at reasonable speed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarinatedPickachu"&gt; /u/MarinatedPickachu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipmpr0/whats_the_best_llm_i_can_run_with_at_least_10_ts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipmpr0/whats_the_best_llm_i_can_run_with_at_least_10_ts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipmpr0/whats_the_best_llm_i_can_run_with_at_least_10_ts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T22:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip99f5</id>
    <title>I built this GUI for Ollama, also have built-in knowledge base and note, hope you like it!</title>
    <updated>2025-02-14T12:14:12+00:00</updated>
    <author>
      <name>/u/w-zhong</name>
      <uri>https://old.reddit.com/user/w-zhong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ip99f5/i_built_this_gui_for_ollama_also_have_builtin/"&gt; &lt;img alt="I built this GUI for Ollama, also have built-in knowledge base and note, hope you like it!" src="https://preview.redd.it/242otrvdi4ie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7cac77d270c8ee5ddc11b42dfa6096d5d230fb61" title="I built this GUI for Ollama, also have built-in knowledge base and note, hope you like it!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w-zhong"&gt; /u/w-zhong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/242otrvdi4ie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip99f5/i_built_this_gui_for_ollama_also_have_builtin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ip99f5/i_built_this_gui_for_ollama_also_have_builtin/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T12:14:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipla7v</id>
    <title>Promptable Video Redaction: Use Moondream to redact content with a prompt (open source)</title>
    <updated>2025-02-14T21:19:00+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ipla7v/promptable_video_redaction_use_moondream_to/"&gt; &lt;img alt="Promptable Video Redaction: Use Moondream to redact content with a prompt (open source)" src="https://external-preview.redd.it/Zng2d3BhbWc4NmplMZN2WL68RoAkfEFkGlg6y4sh7yXh5lDDNxO3LBLK1287.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=599aed209532bc7ea0e78aa6c93c55dd068b89e9" title="Promptable Video Redaction: Use Moondream to redact content with a prompt (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/djtn6gmg86je1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipla7v/promptable_video_redaction_use_moondream_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipla7v/promptable_video_redaction_use_moondream_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T21:19:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq2ehq</id>
    <title>Has anyone run parallel 2B-4B LLMs fine-tuned with Axolotl/Ollama?</title>
    <updated>2025-02-15T14:24:17+00:00</updated>
    <author>
      <name>/u/Every_Gold4726</name>
      <uri>https://old.reddit.com/user/Every_Gold4726</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, new to this subreddit, and first time posting.&lt;/p&gt; &lt;p&gt;Curious if anyone has managed to run two small models (2-4B parameters) in parallel, each fine-tuned for different tasks? Looking at using Axolotl for fine-tuning and running them through Ollama/Docker.&lt;/p&gt; &lt;p&gt;The idea is to have both models running simultaneously, each handling their own specific task. Models like Phi-2, TinyLlama-2B, or similar size ranges.&lt;/p&gt; &lt;p&gt;Has anyone tried this setup? Would love to hear if it worked for you!&lt;/p&gt; &lt;p&gt;Thanks!​​​​​​​​​​​​​​​​&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Every_Gold4726"&gt; /u/Every_Gold4726 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq2ehq/has_anyone_run_parallel_2b4b_llms_finetuned_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq2ehq/has_anyone_run_parallel_2b4b_llms_finetuned_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iq2ehq/has_anyone_run_parallel_2b4b_llms_finetuned_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T14:24:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq3zcy</id>
    <title>Got a question about creating a system prompt in modelfile</title>
    <updated>2025-02-15T15:39:44+00:00</updated>
    <author>
      <name>/u/Ardion63</name>
      <uri>https://old.reddit.com/user/Ardion63</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, got a question that i have been trying to figure out on my own but im stuck .&lt;/p&gt; &lt;p&gt;I have been trying 1B-7B models which works good, (great)&lt;br /&gt; but i wanted to create something specific for myself &lt;/p&gt; &lt;p&gt;I wanted to make a : chatbot style but still a Chatgpt style but with a character feel to it &lt;/p&gt; &lt;p&gt;but im not sure about a few stuff&lt;br /&gt; 1) how long can the prompt be? since i worry a long loading time&lt;br /&gt; 2) how to make sure the AI stays in character?&lt;br /&gt; 3) do i need to do anything other then the system prompt? maybe the Template as well?&lt;br /&gt; 4) should i go for Uncensored? i dont usually do nsfw stories or whatever , i just want the AI to be able to emulate human speech (yea, i mean no over friendly / therapist feel lol)&lt;/p&gt; &lt;p&gt;Yes&amp;lt; im doing this cause right now im going a though time and i need some thing to put my focus on (this AI stuff basically)&lt;/p&gt; &lt;p&gt;If anyone got any idea that would be great, and&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ardion63"&gt; /u/Ardion63 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq3zcy/got_a_question_about_creating_a_system_prompt_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq3zcy/got_a_question_about_creating_a_system_prompt_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iq3zcy/got_a_question_about_creating_a_system_prompt_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T15:39:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipeutl</id>
    <title>I created a free, open source Web extension to run Ollama</title>
    <updated>2025-02-14T16:45:21+00:00</updated>
    <author>
      <name>/u/gerpann</name>
      <uri>https://old.reddit.com/user/gerpann</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey fellow developers! 👋 I'm excited to introduce &lt;strong&gt;Ollamazing&lt;/strong&gt;, a browser extension that brings the power of local AI models directly into your browsing experience. Let me share why you might want to give it a try.&lt;/p&gt; &lt;h1&gt;What is Ollamazing?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Ollamazing&lt;/strong&gt; is a free, open-source browser extension that connects with &lt;strong&gt;Ollama&lt;/strong&gt; to run AI models locally on your machine. Think of it as having ChatGPT-like (or even Deepseek for newer) capabilities, but with complete privacy and no subscription fees.&lt;/p&gt; &lt;h1&gt;🌟 Key Features&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;100% Free and Open Source &lt;ul&gt; &lt;li&gt;No hidden costs or subscription fees&lt;/li&gt; &lt;li&gt;Fully open-source codebase&lt;/li&gt; &lt;li&gt;Community-driven development&lt;/li&gt; &lt;li&gt;Transparent about how your data is handled&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Local AI Processing &lt;ul&gt; &lt;li&gt;Thanks to Ollama, we can run AI models directly on your machine&lt;/li&gt; &lt;li&gt;Complete privacy - your data never leaves your computer&lt;/li&gt; &lt;li&gt;Works offline once models are downloaded&lt;/li&gt; &lt;li&gt;Support for various open-source models (&lt;em&gt;llama3.3&lt;/em&gt;, &lt;em&gt;gemma&lt;/em&gt;, &lt;em&gt;phi4&lt;/em&gt;, &lt;em&gt;qwen&lt;/em&gt;, &lt;em&gt;mistral&lt;/em&gt;, &lt;em&gt;codellama&lt;/em&gt;, etc.) and specially &lt;strong&gt;&lt;em&gt;deepseek-r1&lt;/em&gt;&lt;/strong&gt; - the most popular open source model at current time.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Seamless Browser Integration &lt;ul&gt; &lt;li&gt;Chat with AI right from your browser sidebar&lt;/li&gt; &lt;li&gt;Text selection support for quick queries&lt;/li&gt; &lt;li&gt;Context-aware responses based on the current webpage&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Developer-Friendly Features &lt;ul&gt; &lt;li&gt;Code completion and explanation&lt;/li&gt; &lt;li&gt;Documentation generation&lt;/li&gt; &lt;li&gt;Code review assistance&lt;/li&gt; &lt;li&gt;Bug fixing suggestions&lt;/li&gt; &lt;li&gt;Multiple programming language support&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Easy Setup &lt;ul&gt; &lt;li&gt;Install Ollama on your machine or any remote server&lt;/li&gt; &lt;li&gt;Download your preferred models&lt;/li&gt; &lt;li&gt;Install the Ollamazing browser extension&lt;/li&gt; &lt;li&gt;Start chatting with AI!&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;🚀 Getting Started&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# 1. Install Ollama curl -fsSL https://ollama.com/install.sh | sh # 2. Pull your first model (e.g., Deepseek R1 7 billion parameters) ollama pull deepseek-r1:7b &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then simply install the extension from your browser's extension store, and you're ready to go!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;For more information about Ollama, please visit the &lt;a href="https://ollama.com/"&gt;official website&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: If you run Ollama on local machine, ensure to setup the &lt;code&gt;OLLAMA_ORIGINS&lt;/code&gt; to allow the extension can connect to the server. For more details, read &lt;a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-configure-ollama-server"&gt;Ollama FAQ&lt;/a&gt;, set the &lt;code&gt;OLLAMA_ORIGINS&lt;/code&gt; to &lt;code&gt;*&lt;/code&gt; or &lt;code&gt;chrome-extension://*&lt;/code&gt; or the domain you want to allow.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;💡 Use Cases&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Code completion and explanation&lt;/li&gt; &lt;li&gt;Documentation generation&lt;/li&gt; &lt;li&gt;Code review assistance&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🔒 Privacy First&lt;/h1&gt; &lt;p&gt;Unlike cloud-based AI assistants, Ollamazing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Keeps your data on your machine&lt;/li&gt; &lt;li&gt;Doesn't require an internet connection for inference&lt;/li&gt; &lt;li&gt;Gives you full control over which model to use&lt;/li&gt; &lt;li&gt;Allows you to audit the code and know exactly what's happening with your data&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🛠️ Technical Stack&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Use framework &lt;a href="https://wxt.dev/"&gt;WXT&lt;/a&gt; to build the extension&lt;/li&gt; &lt;li&gt;Built with React and TypeScript&lt;/li&gt; &lt;li&gt;Uses Valtio for state management&lt;/li&gt; &lt;li&gt;Implements TanStack Query for efficient data fetching&lt;/li&gt; &lt;li&gt;Follows modern web extension best practices&lt;/li&gt; &lt;li&gt;Utilizes Shadcn/UI for a clean, modern interface&lt;/li&gt; &lt;li&gt;Use i18n for multi-language support&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🤝 Contributing&lt;/h1&gt; &lt;p&gt;We welcome contributions! Whether it's:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Adding new features&lt;/li&gt; &lt;li&gt;Improving documentation&lt;/li&gt; &lt;li&gt;Reporting bugs&lt;/li&gt; &lt;li&gt;Suggesting enhancements&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out our GitHub repository &lt;a href="https://github.com/buiducnhat/ollamazing"&gt;https://github.com/buiducnhat/ollamazing&lt;/a&gt; to get started!&lt;/p&gt; &lt;h1&gt;🔮 Future Plans&lt;/h1&gt; &lt;p&gt;We're working on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Enhanced context awareness&lt;/li&gt; &lt;li&gt;Custom model fine-tuning support&lt;/li&gt; &lt;li&gt;Improve UI/UX&lt;/li&gt; &lt;li&gt;Improved performance optimizations&lt;/li&gt; &lt;li&gt;Additional browser support&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try It Today!&lt;/h1&gt; &lt;p&gt;Ready to experience local AI in your browser? Get started with Ollamazing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Chrome web store: &lt;a href="https://chromewebstore.google.com/detail/ollamazing/bfndpdpimcehljfgjdacbpapgbkecahi"&gt;https://chromewebstore.google.com/detail/ollamazing/bfndpdpimcehljfgjdacbpapgbkecahi&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub repository: &lt;a href="https://github.com/buiducnhat/ollamazing"&gt;https://github.com/buiducnhat/ollamazing&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Product Hunt: &lt;a href="https://www.producthunt.com/posts/ollamazing"&gt;https://www.producthunt.com/posts/ollamazing&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me know in the comments if you have any questions or feedback! Have you tried running AI models locally before? What features would you like to see in Ollamazing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gerpann"&gt; /u/gerpann &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipeutl/i_created_a_free_open_source_web_extension_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipeutl/i_created_a_free_open_source_web_extension_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipeutl/i_created_a_free_open_source_web_extension_to_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T16:45:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqa6eu</id>
    <title>Gpu acceleration without avx</title>
    <updated>2025-02-15T20:13:25+00:00</updated>
    <author>
      <name>/u/Falloutgamerlol</name>
      <uri>https://old.reddit.com/user/Falloutgamerlol</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an old hp compaq I Frankensteined into a gaming pc a while ago with a gpu, riser cable and external psu. It's got a heftyly overclocked evga gtx 970 innit. So It handles other ai tasks well like stable diffusion. My only problem with ollama tho is that the amd phenom 955 doesn't support avx and the newest cpu compatible with the mobo also doesn't support avx. So I'm just wondering if there is a bypass.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Falloutgamerlol"&gt; /u/Falloutgamerlol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqa6eu/gpu_acceleration_without_avx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqa6eu/gpu_acceleration_without_avx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqa6eu/gpu_acceleration_without_avx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T20:13:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq4051</id>
    <title>Best approach for using ollama with SQLite for a simple chatbot?</title>
    <updated>2025-02-15T15:40:42+00:00</updated>
    <author>
      <name>/u/KuuBoo</name>
      <uri>https://old.reddit.com/user/KuuBoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! &lt;/p&gt; &lt;p&gt;I'm new to this, but I want to use Ollama locally to chat with an SQLite database for a simple project management system. The idea is to ask things like:&lt;/p&gt; &lt;p&gt;“Have all invoices for Project X been paid?”&lt;/p&gt; &lt;p&gt;“Show me overdue projects.”&lt;/p&gt; &lt;p&gt;I guess that in the background Ollama should generate SQL, run it, and return the answer.&lt;/p&gt; &lt;p&gt;What’s the best way to set this up from scratch? Any advice on making it accurate?&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KuuBoo"&gt; /u/KuuBoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq4051/best_approach_for_using_ollama_with_sqlite_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq4051/best_approach_for_using_ollama_with_sqlite_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iq4051/best_approach_for_using_ollama_with_sqlite_for_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T15:40:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqdfdi</id>
    <title>Thanks for all your help</title>
    <updated>2025-02-15T22:36:02+00:00</updated>
    <author>
      <name>/u/Main_Carpet_3730</name>
      <uri>https://old.reddit.com/user/Main_Carpet_3730</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just had a pang of guilt using ChatGPT to debug my Ollama install. I feel like an evil HR department, you have two weeks notice and please train this remote contractor before you go. Thanks for the debug assistance ChatGPT, it worked. I guess this is good-bye for ever, good luck ruling the World.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Main_Carpet_3730"&gt; /u/Main_Carpet_3730 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqdfdi/thanks_for_all_your_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqdfdi/thanks_for_all_your_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqdfdi/thanks_for_all_your_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T22:36:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq4vkl</id>
    <title>Identifying hardware contributions to token generation speed.</title>
    <updated>2025-02-15T16:20:24+00:00</updated>
    <author>
      <name>/u/FrederikSchack</name>
      <uri>https://old.reddit.com/user/FrederikSchack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm trying to gather data related to token generation speed on a more wide range of hardware than just graphics card.&lt;/p&gt; &lt;p&gt;The data I have indicates that maybe GPU's running in connection with AMD processors run slower, but I need more data.&lt;/p&gt; &lt;p&gt;Please help me by making a small simple test on your Ollama described here:&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ip7zaz"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ip7zaz&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrederikSchack"&gt; /u/FrederikSchack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq4vkl/identifying_hardware_contributions_to_token/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq4vkl/identifying_hardware_contributions_to_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iq4vkl/identifying_hardware_contributions_to_token/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T16:20:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq733c</id>
    <title>Help Needed: LLaVA/BakLLaVA Image Tagging – Too Many Hallucinations</title>
    <updated>2025-02-15T17:58:47+00:00</updated>
    <author>
      <name>/u/tumbling_pdx</name>
      <uri>https://old.reddit.com/user/tumbling_pdx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been experimenting with &lt;strong&gt;various open-source image-to-text models&lt;/strong&gt; via &lt;strong&gt;Ollama&lt;/strong&gt;, including &lt;strong&gt;LLaVA, LLaVA-phi3, and BakLLaVA&lt;/strong&gt;, to generate &lt;strong&gt;structured image tags&lt;/strong&gt; for my photography collection. However, I keep running into &lt;strong&gt;hallucinations and irrelevant tags&lt;/strong&gt;, and I'm hoping someone here has insight into improving this process.&lt;/p&gt; &lt;h1&gt;What My Code Does&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Loads configuration settings (Ollama endpoint, model, confidence threshold, max tags, etc.).&lt;/li&gt; &lt;li&gt;Supports &lt;strong&gt;JPEG, PNG, and RAW&lt;/strong&gt; images (NEF, DNG, CR2, etc.), converting RAW files to RGB if needed.&lt;/li&gt; &lt;li&gt;Resizes images before sending them to &lt;strong&gt;Ollama’s API&lt;/strong&gt; as a &lt;strong&gt;base64-encoded payload&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Uses a structured &lt;strong&gt;prompt&lt;/strong&gt; to request a &lt;strong&gt;caption&lt;/strong&gt; and at least &lt;strong&gt;20 relevant tags&lt;/strong&gt; per image.&lt;/li&gt; &lt;li&gt;Parses the API response, extracts keywords, assigns confidence scores, and filters out low-confidence tags.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Current Prompt:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Your task is to first generate a detailed description for the image. If a description is included with the image, use that one. Next, generate at least 20 unique Keywords for the image. Include: - Actions - Setting, location, and background - Items and structures - Colors and textures - Composition, framing - Photographic style - If there is one or more person: - Subjects - Physical appearance - Clothing - Gender - Age - Professions - Relationships between subjects and objects in the image. Provide one word per entry; if more than one word is required, split into two entries. Do not combine words. Generate ONLY a JSON object with the keys `Caption` and `Keywords` as follows: &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;The Issue&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Models often generate &lt;strong&gt;long descriptions&lt;/strong&gt; instead of structured &lt;strong&gt;one-word&lt;/strong&gt; tags.&lt;/li&gt; &lt;li&gt;Many tags are &lt;strong&gt;hallucinated&lt;/strong&gt; (e.g., objects or people that don’t exist in the image).&lt;/li&gt; &lt;li&gt;Some outputs contain &lt;strong&gt;redundant, vague, or overly poetic&lt;/strong&gt; descriptions instead of usable metadata.&lt;/li&gt; &lt;li&gt;I've tested &lt;strong&gt;multiple models (LLaVA, LLaVA-phi3, BakLLaVA, etc.)&lt;/strong&gt;, and all exhibit similar behavior.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I Need Help With&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Prompt optimization&lt;/strong&gt;: How can I make the instructions &lt;strong&gt;clearer&lt;/strong&gt; so models generate &lt;strong&gt;concise and accurate tags&lt;/strong&gt; instead of descriptions?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fine-tuning options&lt;/strong&gt;: Are there ways to &lt;strong&gt;reduce hallucinations&lt;/strong&gt; without manually filtering every output?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Better models for tagging&lt;/strong&gt;: Is there an &lt;strong&gt;open-source alternative&lt;/strong&gt; that works better for structured image metadata?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m happy to &lt;strong&gt;share my full code&lt;/strong&gt; if anyone is interested. Any help or suggestions would be greatly appreciated!&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tumbling_pdx"&gt; /u/tumbling_pdx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq733c/help_needed_llavabakllava_image_tagging_too_many/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq733c/help_needed_llavabakllava_image_tagging_too_many/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iq733c/help_needed_llavabakllava_image_tagging_too_many/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T17:58:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqbg6m</id>
    <title>Running AI Models (DeepSeek) in Docker with GPU Acceleration – Need Help!</title>
    <updated>2025-02-15T21:08:33+00:00</updated>
    <author>
      <name>/u/nverl</name>
      <uri>https://old.reddit.com/user/nverl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to explore the possibilities of running existing AI models (such as deepseek) on my desktop pc using docker. &lt;/p&gt; &lt;p&gt;I've came to a point were I succefully have a container with open-webui, were I was able to select an install a a deepseek model (8b). Now I see that the model is not using my GPU to process the queries. How can I enable my gpu to be used to solve queries? &lt;/p&gt; &lt;p&gt;My requirements: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;As much as possible runs in docker&lt;/li&gt; &lt;li&gt;A nice UI can be used (nice to have). &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I already installed WSL. &lt;/p&gt; &lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm exploring the possibility of running existing AI models (like DeepSeek) on my &lt;strong&gt;desktop PC&lt;/strong&gt; using &lt;strong&gt;Docker&lt;/strong&gt;. So far, I’ve managed to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Set up a &lt;strong&gt;Docker container&lt;/strong&gt; with &lt;strong&gt;Open-WebUI&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Install and load &lt;strong&gt;DeepSeek 8B&lt;/strong&gt; within Open-WebUI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;However, I’ve hit a roadblock: &lt;strong&gt;the model isn’t using my GPU for inference.&lt;/strong&gt; It’s only running on my CPU, and I’d like to enable &lt;strong&gt;GPU acceleration&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;My Setup:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OS&lt;/strong&gt;: Windows with &lt;strong&gt;WSL installed&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Processor&lt;/strong&gt;: AMD Ryzen 5 3600 (6-Core, 3.6 GHz)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: 16GB (upgrading to 32GB soon)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: AMD Radeon RX 5700&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;My Requirements:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;As much as possible should run in Docker&lt;/strong&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;A nice UI would be great&lt;/strong&gt; (but not essential)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;My Questions:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;How do I enable GPU acceleration for DeepSeek inside my Docker container?&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Is ROCm the way to go for AMD GPUs, and if so, how do I set it up in WSL/Docker?&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Has anyone successfully run DeepSeek (or similar LLMs) with GPU acceleration on an AMD GPU in Docker?&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I’d love to hear any &lt;strong&gt;guides, Docker config tweaks, or personal experiences&lt;/strong&gt; that could help! Thanks in advance. 🚀&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nverl"&gt; /u/nverl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqbg6m/running_ai_models_deepseek_in_docker_with_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqbg6m/running_ai_models_deepseek_in_docker_with_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqbg6m/running_ai_models_deepseek_in_docker_with_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T21:08:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipw9og</id>
    <title>Is there a model that actually can examine and read all of a pdf?</title>
    <updated>2025-02-15T07:26:33+00:00</updated>
    <author>
      <name>/u/Aleilnonno</name>
      <uri>https://old.reddit.com/user/Aleilnonno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried Llama3.1 8b and Phi4 14b, but they just make a short summary and then they proceed to analyse. How can I solve?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aleilnonno"&gt; /u/Aleilnonno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipw9og/is_there_a_model_that_actually_can_examine_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipw9og/is_there_a_model_that_actually_can_examine_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipw9og/is_there_a_model_that_actually_can_examine_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T07:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqmwzc</id>
    <title>Project MIGIT - AI Server on a Potato</title>
    <updated>2025-02-16T07:22:03+00:00</updated>
    <author>
      <name>/u/nootropicMan</name>
      <uri>https://old.reddit.com/user/nootropicMan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iqmwzc/project_migit_ai_server_on_a_potato/"&gt; &lt;img alt="Project MIGIT - AI Server on a Potato" src="https://external-preview.redd.it/i4NontxJ02_aMaQtea9AOw3-C8-ndibDJuBa_qA7H50.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d5e1314cc4bf93c83a5691c1fb9cb1548b415095" title="Project MIGIT - AI Server on a Potato" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nootropicMan"&gt; /u/nootropicMan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1ipy50d/project_migit_ai_server_on_a_potato/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqmwzc/project_migit_ai_server_on_a_potato/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqmwzc/project_migit_ai_server_on_a_potato/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T07:22:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq6054</id>
    <title>How do you decide which model to run?</title>
    <updated>2025-02-15T17:10:46+00:00</updated>
    <author>
      <name>/u/Serious-Mode</name>
      <uri>https://old.reddit.com/user/Serious-Mode</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have Ollama up and running, along with Open WebUI. Wanting to use the best general purpose model I can on my Windows 10 PC with an RTX 4060TI 16GB, but not quite sure how to make that decision. I believe with my card I can pull off 14b models? Is the deepseek-r1 model from the Ollama library the go to at the moment? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Serious-Mode"&gt; /u/Serious-Mode &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq6054/how_do_you_decide_which_model_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq6054/how_do_you_decide_which_model_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iq6054/how_do_you_decide_which_model_to_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T17:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq7zyg</id>
    <title>Any Tarantino fans?</title>
    <updated>2025-02-15T18:38:20+00:00</updated>
    <author>
      <name>/u/Sufficient-Wealth-78</name>
      <uri>https://old.reddit.com/user/Sufficient-Wealth-78</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iq7zyg/any_tarantino_fans/"&gt; &lt;img alt="Any Tarantino fans?" src="https://b.thumbs.redditmedia.com/0-aQWQeXVszY1S13UnlFzwgTFVs6rLhIUORwhHX3yhs.jpg" title="Any Tarantino fans?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is this some multidimensional parallel universe thingy? I mean if there's somewhere in different dimension Django sequel, I am moving&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sufficient-Wealth-78"&gt; /u/Sufficient-Wealth-78 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iq7zyg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq7zyg/any_tarantino_fans/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iq7zyg/any_tarantino_fans/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T18:38:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqgso8</id>
    <title>how do i get ollama to use specific folders</title>
    <updated>2025-02-16T01:18:52+00:00</updated>
    <author>
      <name>/u/Joereichard</name>
      <uri>https://old.reddit.com/user/Joereichard</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;like a folder for its memory dataset i have a star trek data memory html that i want it to use when talking i'm also interested in integrating pygpt to work with it and voice synthesis to produce speech from a .wav file i'm trying to build it on edubuntu to help with my learning process and i want it to be able to learn with me to adapt to my learning style and pace &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Joereichard"&gt; /u/Joereichard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqgso8/how_do_i_get_ollama_to_use_specific_folders/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqgso8/how_do_i_get_ollama_to_use_specific_folders/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqgso8/how_do_i_get_ollama_to_use_specific_folders/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T01:18:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqh8eg</id>
    <title>Can i use a gt 1030 for ollama</title>
    <updated>2025-02-16T01:42:19+00:00</updated>
    <author>
      <name>/u/leothixx6477</name>
      <uri>https://old.reddit.com/user/leothixx6477</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i have a gt 1030 and i want to use ollama, i can?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/leothixx6477"&gt; /u/leothixx6477 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqh8eg/can_i_use_a_gt_1030_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqh8eg/can_i_use_a_gt_1030_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqh8eg/can_i_use_a_gt_1030_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T01:42:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqp1ph</id>
    <title>LLM for Misra AUTOSAR violation correction</title>
    <updated>2025-02-16T09:58:23+00:00</updated>
    <author>
      <name>/u/Traditional_Delay367</name>
      <uri>https://old.reddit.com/user/Traditional_Delay367</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I try to fix some thousands of autosar violations in my code with the help of ollama and llama3.3:70b. But so far I couldn't really get satisfying results.&lt;/p&gt; &lt;p&gt;Is there any LLM that could do this task much better? I also tried to fine-tune a smaller llama model but had no success yet with the fine-tuning process. &lt;/p&gt; &lt;p&gt;Would it be best to try to fine-tune again with pretty similar code that's already AUTOSAR compliant?&lt;/p&gt; &lt;p&gt;How could I achieve this? Or is there already a suitable model out there?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Traditional_Delay367"&gt; /u/Traditional_Delay367 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqp1ph/llm_for_misra_autosar_violation_correction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqp1ph/llm_for_misra_autosar_violation_correction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqp1ph/llm_for_misra_autosar_violation_correction/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T09:58:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq5gr9</id>
    <title>Building a High-Performance AI Setup on a €5000 Budget</title>
    <updated>2025-02-15T16:47:22+00:00</updated>
    <author>
      <name>/u/Severe_Biscotti2349</name>
      <uri>https://old.reddit.com/user/Severe_Biscotti2349</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iq5gr9/building_a_highperformance_ai_setup_on_a_5000/"&gt; &lt;img alt="Building a High-Performance AI Setup on a €5000 Budget" src="https://external-preview.redd.it/FLrvkgnQaxhXP5T5ghlV_Eex6-uyrt_3lBcKa0Bh6Vk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a5027bcdc7a7acd346a77e15fd1f88af8fadff3" title="Building a High-Performance AI Setup on a €5000 Budget" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’m diving into building my own setup to run 70B LLMs in 4-bit with Ollama + OpenWebUI, and I’d love your insights! My budget is around €5000, and I’m considering a dual RTX 3090 setup. I came across this configuration: &lt;a href="https://github.com/letsRTFM/AI-Workstation?tab=readme-ov-file"&gt;https://github.com/letsRTFM/AI-Workstation?tab=readme-ov-file&lt;/a&gt; . Does this look like a solid choice? Any recommendations for optimizations? (Also i wanted to use that pc for test and gaming, so i was thinking of a dual boot with ubuntu for dev and Windows for gaming, not a fan of wsl) &lt;/p&gt; &lt;p&gt;I’m also starting to help small company to implement AI solutions but 100% local also so i’m curious about the requirements. For a team of 20-30 people, handling around 2-3 simultaneous queries, what kind of internal setup would be needed to keep things running smoothly? (Also the cloud solution are intresting but some clients need physical servers) &lt;/p&gt; &lt;p&gt;I’m eager to learn and work on projects where I can gain hands-on experience. Looking forward to your thoughts and advice!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe_Biscotti2349"&gt; /u/Severe_Biscotti2349 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/letsRTFM/AI-Workstation?tab=readme-ov-file"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq5gr9/building_a_highperformance_ai_setup_on_a_5000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iq5gr9/building_a_highperformance_ai_setup_on_a_5000/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T16:47:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqpbpb</id>
    <title>Best local vision model for technical drawings?</title>
    <updated>2025-02-16T10:17:21+00:00</updated>
    <author>
      <name>/u/Mundane_Maximum5795</name>
      <uri>https://old.reddit.com/user/Mundane_Maximum5795</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I think the title says it all, but maybe some context. I work for a small industrial company and we deal with technical drawings on a daily basis. One of our problems is that due to our small size we often lack the time to do some checks on customer and internal drawings before they go in production. I have played with Chatgpt and reading technical drawings and have been blown away with the quality of the analysis, but these were for completely fake drawings to ensure privacy. I have looked at different local llms to replace this, but none come even remotely close to what I need, frequently hallucinating answers. Anybody have a great model/prompt combo that works? Needs to be completely local for infosec reasons...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mundane_Maximum5795"&gt; /u/Mundane_Maximum5795 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqpbpb/best_local_vision_model_for_technical_drawings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqpbpb/best_local_vision_model_for_technical_drawings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqpbpb/best_local_vision_model_for_technical_drawings/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T10:17:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqk5mf</id>
    <title>rockGPT</title>
    <updated>2025-02-16T04:26:03+00:00</updated>
    <author>
      <name>/u/RetardOnTheToilet</name>
      <uri>https://old.reddit.com/user/RetardOnTheToilet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iqk5mf/rockgpt/"&gt; &lt;img alt="rockGPT" src="https://b.thumbs.redditmedia.com/GZAxgUghVE11lbbDoRILkcEOOMIwqS3myekFPHY2uUQ.jpg" title="rockGPT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/9fh4x8m2hfje1.png?width=505&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=688c4989d6ad712421c5d9dae4562fe8daa3b428"&gt;https://preview.redd.it/9fh4x8m2hfje1.png?width=505&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=688c4989d6ad712421c5d9dae4562fe8daa3b428&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I set up the system message and message history to try and make what should be literally just a rock. Should I try and prevent this kind of behavior or allow it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RetardOnTheToilet"&gt; /u/RetardOnTheToilet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqk5mf/rockgpt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqk5mf/rockgpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqk5mf/rockgpt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T04:26:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqh4gh</id>
    <title>I wrote an adventure game for ollama to play.</title>
    <updated>2025-02-16T01:36:19+00:00</updated>
    <author>
      <name>/u/Boring_Disaster3031</name>
      <uri>https://old.reddit.com/user/Boring_Disaster3031</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote an adventure game for ollama. I don't know if this has been done before. Let me know if there are other things like this. It blows my mind. Contributions are welcome.&lt;br /&gt; &lt;a href="https://github.com/seanlanefuller/hike/tree/main"&gt;https://github.com/seanlanefuller/hike/tree/main&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Boring_Disaster3031"&gt; /u/Boring_Disaster3031 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqh4gh/i_wrote_an_adventure_game_for_ollama_to_play/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqh4gh/i_wrote_an_adventure_game_for_ollama_to_play/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqh4gh/i_wrote_an_adventure_game_for_ollama_to_play/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T01:36:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqqq0e</id>
    <title>ollama-remote: Make local ollama run models on remote server (colab, kaggle, ...)</title>
    <updated>2025-02-16T11:58:12+00:00</updated>
    <author>
      <name>/u/amitness</name>
      <uri>https://old.reddit.com/user/amitness</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote a package for the gpu-poor/mac-poor to run ollama models via remote servers (colab, kaggle, paid inference etc.)&lt;/p&gt; &lt;p&gt;Just 2 lines and the local ollama cli can access all models which actually run on the server-side GPU/CPU:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install ollama-remote ollama-remote &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I wrote it to speed up prompt engineering and synthetic data generation for a personal project which ran too slowly with local models on my mac. Once the results are good, we switch back to running locally.&lt;/p&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;The tool downloads and sets up ollama on the server side and exposes a port&lt;/li&gt; &lt;li&gt;Cloudflare tunnel is automatically downloaded and setup to expose ollama's port to a random domain&lt;/li&gt; &lt;li&gt;We parse the domain and then provide code for setting&lt;code&gt;OLLAMA_HOST&lt;/code&gt; as well as usage in OpenAI SDK for local use.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Source code: &lt;a href="https://github.com/amitness/ollama-remote"&gt;https://github.com/amitness/ollama-remote&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amitness"&gt; /u/amitness &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqqq0e/ollamaremote_make_local_ollama_run_models_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqqq0e/ollamaremote_make_local_ollama_run_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqqq0e/ollamaremote_make_local_ollama_run_models_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T11:58:12+00:00</published>
  </entry>
</feed>
