<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-27T02:57:24+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1iadcye</id>
    <title>Trying to get ui-tars-desktop running with local vlm and lm studios</title>
    <updated>2025-01-26T12:31:13+00:00</updated>
    <author>
      <name>/u/Designer_Award_2551</name>
      <uri>https://old.reddit.com/user/Designer_Award_2551</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried to get ui-tars desktop running with lm studios. But I can't get the program working.&lt;/p&gt; &lt;p&gt;Lm studio is up and running, loaded deepseek vlm. Got the local server running, but the settings of ui-tars do not allow me to connect to it? It only has a setting for hugging face (which is a paid service?) Do I mis anything? Or isn't it possible right now. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Designer_Award_2551"&gt; /u/Designer_Award_2551 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iadcye/trying_to_get_uitarsdesktop_running_with_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iadcye/trying_to_get_uitarsdesktop_running_with_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iadcye/trying_to_get_uitarsdesktop_running_with_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T12:31:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9smk3</id>
    <title>Running Deepseek-r1 7b distilled model locally in a PC with no GPU with Ollama.</title>
    <updated>2025-01-25T17:53:06+00:00</updated>
    <author>
      <name>/u/Secraciesmeet</name>
      <uri>https://old.reddit.com/user/Secraciesmeet</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secraciesmeet"&gt; /u/Secraciesmeet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9smk3/running_deepseekr1_7b_distilled_model_locally_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9smk3/running_deepseekr1_7b_distilled_model_locally_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9smk3/running_deepseekr1_7b_distilled_model_locally_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T17:53:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaermz</id>
    <title>LOCAL MODEL RECOMMENDATION</title>
    <updated>2025-01-26T13:39:22+00:00</updated>
    <author>
      <name>/u/potatopcuser4ever</name>
      <uri>https://old.reddit.com/user/potatopcuser4ever</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which model should i run locally with a laptop having i5 12th gen,8gb ram and only integrated intel gpu&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/potatopcuser4ever"&gt; /u/potatopcuser4ever &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaermz/local_model_recommendation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaermz/local_model_recommendation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaermz/local_model_recommendation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T13:39:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia71y4</id>
    <title>Deepseek_r1 considers “think” instead of “thiink”. Will the quantization and training parameters affect the performance? I am using 1.5b ; q4_k_m</title>
    <updated>2025-01-26T05:41:06+00:00</updated>
    <author>
      <name>/u/themoneylust</name>
      <uri>https://old.reddit.com/user/themoneylust</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ia71y4/deepseek_r1_considers_think_instead_of_thiink/"&gt; &lt;img alt="Deepseek_r1 considers “think” instead of “thiink”. Will the quantization and training parameters affect the performance? I am using 1.5b ; q4_k_m" src="https://preview.redd.it/heirhacyz9fe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef07b08bdb610da3b5499d2d699942ba0782c3eb" title="Deepseek_r1 considers “think” instead of “thiink”. Will the quantization and training parameters affect the performance? I am using 1.5b ; q4_k_m" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/themoneylust"&gt; /u/themoneylust &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/heirhacyz9fe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ia71y4/deepseek_r1_considers_think_instead_of_thiink/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ia71y4/deepseek_r1_considers_think_instead_of_thiink/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T05:41:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9zv0u</id>
    <title>Deepseek R1 + WebUI energy costs?</title>
    <updated>2025-01-25T23:13:58+00:00</updated>
    <author>
      <name>/u/DelPrive235</name>
      <uri>https://old.reddit.com/user/DelPrive235</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm pretty new to this. I am in the process of installing Browser Use Web UI. I'm considering using it with Deepseek R1 14b. Running locally on a MBP M1 Max 64GB ram. Will this be significantly more expensive in energy costs than using WebUI with GPT 4o? Am I likely to notice a spike in my energy costs using R1 14b? Would it be any less expensive to just use the Deepseek API running onto their servers? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DelPrive235"&gt; /u/DelPrive235 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9zv0u/deepseek_r1_webui_energy_costs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9zv0u/deepseek_r1_webui_energy_costs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9zv0u/deepseek_r1_webui_energy_costs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T23:13:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1iahxcs</id>
    <title>Difficulty Downloading DeepSeek Model from Ollama Due to DNS Resolution Error</title>
    <updated>2025-01-26T15:51:56+00:00</updated>
    <author>
      <name>/u/greyrabbit-21021420</name>
      <uri>https://old.reddit.com/user/greyrabbit-21021420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I'm encountering persistent issues while trying to download the latest DeepSeek model from Ollama. The download progresses to about 20-30% before resetting back to 0%, and eventually fails with the following error:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; ollama pull deepseek-r1:32b pulling manifest pulling 6150cb382311... 0% ▕ ▏ 0 B/ 19 GB Error: max retries exceeded: Get &amp;quot;https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/61/6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20250126%2Fauto%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20250126T143543Z&amp;amp;X-Amz-Expires=86400&amp;amp;X-Amz-SignedHeaders=host&amp;amp;X-Amz-Signature=28801b0c9d00c2c0cc383eac2333e77446286deb3005c483aafd2328513a7690&amp;quot;: dial tcp: lookup dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com on 127.0.0.53:53: read udp 127.0.0.1:34287-&amp;gt;127.0.0.53:53: i/o timeout &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Steps I've already taken include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Verifying that my internet connection is stable and other downloads work fine.&lt;/li&gt; &lt;li&gt;Ensuring that I'm using the latest version of Ollama.&lt;/li&gt; &lt;li&gt;Checking firewall and proxy settings to ensure they aren't interfering with the connection.&lt;/li&gt; &lt;li&gt;Restarted my computer (several times) and reset my Network settings .&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Despite these efforts, the issue persists. Has anyone else experienced similar problems or have additional suggestions on how to resolve this? I am using Ubuntu 20.04 .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/greyrabbit-21021420"&gt; /u/greyrabbit-21021420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iahxcs/difficulty_downloading_deepseek_model_from_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iahxcs/difficulty_downloading_deepseek_model_from_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iahxcs/difficulty_downloading_deepseek_model_from_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T15:51:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iadttf</id>
    <title>Total noob question (qwen-coder2.5 7b): Generated a clock with image, but image not loading</title>
    <updated>2025-01-26T12:55:10+00:00</updated>
    <author>
      <name>/u/Dull-Dragonfly-6851</name>
      <uri>https://old.reddit.com/user/Dull-Dragonfly-6851</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;i am a total beginner when it comes to AI and coding. I have tried to get &lt;a href="http://bolt.diy"&gt;bolt.diy&lt;/a&gt; and qwen-cloder2.5 7B to work locally and it does. Somehow &lt;a href="http://bolt.diy"&gt;bolt.diy&lt;/a&gt; with qwen-coder2.5 is running very slow, but qwen-coder2.5 in open-webui is running very fast - but thats only a side-note.&lt;/p&gt; &lt;p&gt;Please remember, a total noob is asking:&lt;/p&gt; &lt;p&gt;My main question is, if its possible to somehow see the image directly in the preview in qwen-coder2.5/bolt diy, because right now when i ask qwen-coder &amp;quot;Create a 1 page website with a clock that is updating every second&amp;quot; it does create the clock and shows a preview of it. When i then ask &amp;quot;add a ai generated donkey that blinks every time the clock gets updated&amp;quot; it does add a image, but this image is not loading, like its missing.&lt;/p&gt; &lt;p&gt;I understand that its missing because there is no path to that image, but i saw this video: &lt;a href="https://www.youtube.com/watch?v=197FcjcZ22A&amp;amp;t=1531s"&gt;https://www.youtube.com/watch?v=197FcjcZ22A&amp;amp;t=1531s&lt;/a&gt; and there the guy does almost the same and it show a ai generated donkey image. Due to the fact that he works pretty fast and only the code is overlayed displayed i cannot see what he exactly is doing.&lt;/p&gt; &lt;p&gt;So, is there a way to get it done? I hope you understand what i mean.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dull-Dragonfly-6851"&gt; /u/Dull-Dragonfly-6851 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iadttf/total_noob_question_qwencoder25_7b_generated_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iadttf/total_noob_question_qwencoder25_7b_generated_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iadttf/total_noob_question_qwencoder25_7b_generated_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T12:55:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaez8g</id>
    <title>Outdate document about python-llama-cpp</title>
    <updated>2025-01-26T13:49:12+00:00</updated>
    <author>
      <name>/u/wo-tatatatatata</name>
      <uri>https://old.reddit.com/user/wo-tatatatatata</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp/"&gt;https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the document in the link above is outdated and would not work, anyone knows how i can use local model from ollama instead in this example?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wo-tatatatatata"&gt; /u/wo-tatatatatata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaez8g/outdate_document_about_pythonllamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaez8g/outdate_document_about_pythonllamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaez8g/outdate_document_about_pythonllamacpp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T13:49:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaksh1</id>
    <title>need help choosing entry level gpu</title>
    <updated>2025-01-26T17:40:01+00:00</updated>
    <author>
      <name>/u/SnooBananas5215</name>
      <uri>https://old.reddit.com/user/SnooBananas5215</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am new to learning local llm. I have i5 -10400 &amp;amp; 550 PSU with 32 gigs of ram running on msi h410 vh pro - supporting pcie 3.0. I am not sure weather to go for rtx 4070 12 gb (undervolt) or rtx 4060 ti 16gb, which one would be better&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooBananas5215"&gt; /u/SnooBananas5215 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaksh1/need_help_choosing_entry_level_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaksh1/need_help_choosing_entry_level_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaksh1/need_help_choosing_entry_level_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T17:40:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1iao84u</id>
    <title>[Deepseek r1] REPLACETheseStringsWithCorrespondingValues()</title>
    <updated>2025-01-26T19:45:26+00:00</updated>
    <author>
      <name>/u/vsarunhah</name>
      <uri>https://old.reddit.com/user/vsarunhah</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I just tried running Deepseek r1 1.5b and was just messing around with it and got some weird output that I haven't seen before. Anyone what this is about/where this could've come from? Not sure if it's an ollama thing or an r1 thing.&lt;/p&gt; &lt;p&gt;``` ollama run deepseek-r1:1.5b&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;hello &amp;lt;think&amp;gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;Hello! How can I assist you today? 😊&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;What are you good at &amp;lt;think&amp;gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;Hello! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. For the best results, always do your own research before using AI tools.&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;What are you optimized for? `&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;Hi! I'm DeepSeek-R1, an AI assistant created to help with various tasks and provide information. I optimize my performance based on extensive training data and advanced algorithms to deliver accurate and helpful responses. For more details about our products or services, please visit our official website.&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;What is the most recent data you have been trained on `&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;REPLACETheseStringsWithCorrespondingValues()&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;What is the most recent data you have been trained on? &amp;lt;think&amp;gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;Hi! I'm DeepSeek-R1, an AI assistant created to help with various tasks and provide information. For the best results, always do your own research before using AI tools. If you have any questions or need assistance, feel free to ask!&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;What is the most recent data you have been trained on `&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;REPLACETheseStringsWithCorrespondingValues() ```&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Seems reproducible too, which is pretty weird.&lt;/p&gt; &lt;p&gt;ollama version is 0.5.7&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vsarunhah"&gt; /u/vsarunhah &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iao84u/deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iao84u/deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iao84u/deepseek_r1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T19:45:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9yv89</id>
    <title>Introducing Awesome Open Source AI: A list for tracking great open source models</title>
    <updated>2025-01-25T22:28:29+00:00</updated>
    <author>
      <name>/u/SuccessIsHardWork</name>
      <uri>https://old.reddit.com/user/SuccessIsHardWork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i9yv89/introducing_awesome_open_source_ai_a_list_for/"&gt; &lt;img alt="Introducing Awesome Open Source AI: A list for tracking great open source models" src="https://external-preview.redd.it/GXmJ82vxdZ1DvaPMBQ6TD4TQLzEhsWVdtuXgso5_c_Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce8f5a1c97427023bef612109fcd4409305118f3" title="Introducing Awesome Open Source AI: A list for tracking great open source models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuccessIsHardWork"&gt; /u/SuccessIsHardWork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/suncloudsmoon/awesome-open-source-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9yv89/introducing_awesome_open_source_ai_a_list_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9yv89/introducing_awesome_open_source_ai_a_list_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T22:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iatne4</id>
    <title>Ollama in a conda venv</title>
    <updated>2025-01-26T23:23:24+00:00</updated>
    <author>
      <name>/u/Big_Scholar_3358</name>
      <uri>https://old.reddit.com/user/Big_Scholar_3358</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to install Ollama latest inside a conda virtual env so that I can experiment and keeping my system clean. Any way to achieve this? The packages available in conda-forge and pypi are not the latest. &lt;/p&gt; &lt;p&gt;TIA &amp;amp; cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Big_Scholar_3358"&gt; /u/Big_Scholar_3358 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iatne4/ollama_in_a_conda_venv/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iatne4/ollama_in_a_conda_venv/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iatne4/ollama_in_a_conda_venv/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T23:23:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1iatzsq</id>
    <title>4x AMD Instinct Mi60 Server + vLLM + unsloth/DeepSeek-R1-Distill-Qwen-32B FP16</title>
    <updated>2025-01-26T23:39:12+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0u9az5iybffe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iatzsq/4x_amd_instinct_mi60_server_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iatzsq/4x_amd_instinct_mi60_server_vllm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T23:39:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1iap6ot</id>
    <title>running Ollama on Intet Arc iGPU</title>
    <updated>2025-01-26T20:21:33+00:00</updated>
    <author>
      <name>/u/JV_info</name>
      <uri>https://old.reddit.com/user/JV_info</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a Windows 11 on a mini PC (Geekom GT series G1 Mega) and it has the Intel Arc iGPU, + NPU&lt;br /&gt; I also have a local AI chat and my setup is this (Ollama + Docker + Openwebui)&lt;br /&gt; now I want to use my Intel ARC iGPU but no matter what I do, Ollama ends up running on CPU.&lt;/p&gt; &lt;p&gt;I tried the instruction from Intel, the PDF you shared as well but the problem is that the Intel oneAPI Base Toolkit they use in the instructions is version 2024.1.0 but the version available to download now is 2025.1 and it is not working, I mean when I run the &amp;quot;Ollama serve&amp;quot; it is missing some DLLs that I have but in a higher version... and it's a dead end.&lt;/p&gt; &lt;p&gt;So, I wanted to see if there is a way:&lt;/p&gt; &lt;p&gt;1- downloading the 2024.1.0 version.&lt;/p&gt; &lt;p&gt;2- or any other way to use for my case, Ollama + Docker + Openwebui, so that the intel Arc iGPU or NPU will be used when I use Ollama in OpenwebUi.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JV_info"&gt; /u/JV_info &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iap6ot/running_ollama_on_intet_arc_igpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iap6ot/running_ollama_on_intet_arc_igpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iap6ot/running_ollama_on_intet_arc_igpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T20:21:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1iajtdg</id>
    <title>SQL DB to RAG</title>
    <updated>2025-01-26T17:03:23+00:00</updated>
    <author>
      <name>/u/PaymentDramatic6154</name>
      <uri>https://old.reddit.com/user/PaymentDramatic6154</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any good tutorials on how to do this? Looking for some help, everything ive been seeing about RAG is with pdfs or documents. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaymentDramatic6154"&gt; /u/PaymentDramatic6154 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iajtdg/sql_db_to_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iajtdg/sql_db_to_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iajtdg/sql_db_to_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T17:03:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaodk3</id>
    <title>Issues running deepseek</title>
    <updated>2025-01-26T19:51:08+00:00</updated>
    <author>
      <name>/u/crazyT8nn1s</name>
      <uri>https://old.reddit.com/user/crazyT8nn1s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded deepseek-r1 (1.5b) version via ollama and tried running it on the terminal in MacOS.&lt;/p&gt; &lt;p&gt;I typed &amp;quot;Hi&amp;quot; at the prompt and I get nothing back. I just get another prompt. &amp;quot;&amp;gt;&amp;gt;&amp;gt;&amp;quot;&lt;/p&gt; &lt;p&gt;I don't see any &amp;quot;think&amp;quot; or anything of that sort. This is only isolated to deepseek. Other models I don't have this issue.&lt;/p&gt; &lt;p&gt;Any suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crazyT8nn1s"&gt; /u/crazyT8nn1s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaodk3/issues_running_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaodk3/issues_running_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaodk3/issues_running_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T19:51:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaqxrv</id>
    <title>Ollama on cpu never releases cpu?</title>
    <updated>2025-01-26T21:26:30+00:00</updated>
    <author>
      <name>/u/SweetumsTheMuppet</name>
      <uri>https://old.reddit.com/user/SweetumsTheMuppet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've got a server with 32 cpu cores and no real GPU on it. Performance is ok on 7-14b models so I'm playing around with it.&lt;/p&gt; &lt;p&gt;However, after a prompt (which reports back, for example, that it takes about 14 minutes to generate), the CPU will remain 100% of assigned cores indefinitely. Not all the time, but about 30% of the time. No idea what it's doing for tens of minutes or hours after releasing the prompt. I have to kill the process.&lt;/p&gt; &lt;p&gt;Any thoughts or tweaks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SweetumsTheMuppet"&gt; /u/SweetumsTheMuppet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaqxrv/ollama_on_cpu_never_releases_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaqxrv/ollama_on_cpu_never_releases_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaqxrv/ollama_on_cpu_never_releases_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T21:26:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaoigf</id>
    <title>Does deepseek r1 locally support reading images?</title>
    <updated>2025-01-26T19:56:28+00:00</updated>
    <author>
      <name>/u/PawanAgarwal</name>
      <uri>https://old.reddit.com/user/PawanAgarwal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iaoigf/does_deepseek_r1_locally_support_reading_images/"&gt; &lt;img alt="Does deepseek r1 locally support reading images?" src="https://preview.redd.it/ywwq5z6j8efe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc1f5bc8e418faa002c27dfce87f3f7daa503ccb" title="Does deepseek r1 locally support reading images?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PawanAgarwal"&gt; /u/PawanAgarwal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ywwq5z6j8efe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaoigf/does_deepseek_r1_locally_support_reading_images/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaoigf/does_deepseek_r1_locally_support_reading_images/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T19:56:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaqlk5</id>
    <title>GPU crashes after ~1 day of running phi4</title>
    <updated>2025-01-26T21:13:59+00:00</updated>
    <author>
      <name>/u/Admirral</name>
      <uri>https://old.reddit.com/user/Admirral</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;For starters, my system specs:&lt;br /&gt; i9 14900k&lt;br /&gt; AMD RX 6950XT&lt;br /&gt; 64gb DDR5 ram&lt;br /&gt; windows 11&lt;/p&gt; &lt;p&gt;I am running phi4 in conjunction with a data analytics bot I wrote. It computes batches of prompts roughly every 30 minutes or so, about 5 to 20 per batch (just a very rough ballpark). The issue I experience is that after about 24 hours of usage (give or take), my GPU driver starts to crash out, occasionally freezing the entire system. &lt;/p&gt; &lt;p&gt;What I have noticed is that when I first start the model, my GPU utilization stays around 0% until a prompt is sent in, at which point it spikes to 95% (along with power consumption going to 300 watts) and then drops back down to 0% and 40 watts after completing the prompt. However, after some time, I notice that the utilization trends up to 99% at idle (without any prompt processing) and power usage at 80%. This persists even when I /bye the model, and I need to go into task manager, locate &amp;quot;ollama&amp;quot; processes and end task before my card drops back down to its usual idle.&lt;/p&gt; &lt;p&gt;I am just wondering if there is anything I can do here to prevent this, or if this is just a symptom of using AMD. Am also wondering if it would be better to just use CPU processing instead, although I have this card and would hate to not have it be productive. &lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirral"&gt; /u/Admirral &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaqlk5/gpu_crashes_after_1_day_of_running_phi4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaqlk5/gpu_crashes_after_1_day_of_running_phi4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaqlk5/gpu_crashes_after_1_day_of_running_phi4/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T21:13:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaq51g</id>
    <title>UI-Tars vs Brower-Use Web-UI</title>
    <updated>2025-01-26T20:57:18+00:00</updated>
    <author>
      <name>/u/DelPrive235</name>
      <uri>https://old.reddit.com/user/DelPrive235</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone know which agent is performing better and more stable at the moment? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DelPrive235"&gt; /u/DelPrive235 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaq51g/uitars_vs_broweruse_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaq51g/uitars_vs_broweruse_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaq51g/uitars_vs_broweruse_webui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T20:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1iawxvm</id>
    <title>Deepseek-R1:8b</title>
    <updated>2025-01-27T02:02:52+00:00</updated>
    <author>
      <name>/u/Choice_Complaint9171</name>
      <uri>https://old.reddit.com/user/Choice_Complaint9171</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone please help me from my ignorance is it me or does deepseek R1 :8b seem to resemble a young adult with high IQ and a lot of self doubt &lt;/p&gt; &lt;p&gt;To confirm I’m not ignorant myself try asking “is buffalo a part of New York?” In your setup. &lt;/p&gt; &lt;h1&gt;just your Average AI Contributor&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Choice_Complaint9171"&gt; /u/Choice_Complaint9171 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iawxvm/deepseekr18b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iawxvm/deepseekr18b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iawxvm/deepseekr18b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T02:02:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iapk5s</id>
    <title>Can I restrict a model to only run on CPU?</title>
    <updated>2025-01-26T20:35:25+00:00</updated>
    <author>
      <name>/u/mindsetFPS</name>
      <uri>https://old.reddit.com/user/mindsetFPS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mindsetFPS"&gt; /u/mindsetFPS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iapk5s/can_i_restrict_a_model_to_only_run_on_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iapk5s/can_i_restrict_a_model_to_only_run_on_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iapk5s/can_i_restrict_a_model_to_only_run_on_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T20:35:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaht3c</id>
    <title>DeepSeek-R1's Bias</title>
    <updated>2025-01-26T15:47:14+00:00</updated>
    <author>
      <name>/u/cov_id19</name>
      <uri>https://old.reddit.com/user/cov_id19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iaht3c/deepseekr1s_bias/"&gt; &lt;img alt="DeepSeek-R1's Bias" src="https://preview.redd.it/41tjd3220dfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d3331ed008c8f650aba9895a60c4c649bd8ab47d" title="DeepSeek-R1's Bias" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cov_id19"&gt; /u/cov_id19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/41tjd3220dfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaht3c/deepseekr1s_bias/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaht3c/deepseekr1s_bias/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T15:47:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iavfc6</id>
    <title>8x AMD Instinct Mi60 Server + vLLM + unsloth/DeepSeek-R1-Distill-Qwen-32B FP16</title>
    <updated>2025-01-27T00:46:36+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vpaab64pkffe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iavfc6/8x_amd_instinct_mi60_server_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iavfc6/8x_amd_instinct_mi60_server_vllm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T00:46:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1iacoys</id>
    <title>Is there a pure, quantised version of DeekSeekr1 for Ollama?</title>
    <updated>2025-01-26T11:55:10+00:00</updated>
    <author>
      <name>/u/john_alan</name>
      <uri>https://old.reddit.com/user/john_alan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see this: &lt;a href="https://ollama.com/library/deepseek-r1"&gt;https://ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;but it appears to be Llama3.2/Qwen trained with DeepSeek r1.&lt;/p&gt; &lt;p&gt;I essentially want an 8bit 70bn version of pure r1?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/john_alan"&gt; /u/john_alan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iacoys/is_there_a_pure_quantised_version_of_deekseekr1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iacoys/is_there_a_pure_quantised_version_of_deekseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iacoys/is_there_a_pure_quantised_version_of_deekseekr1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T11:55:10+00:00</published>
  </entry>
</feed>
