<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-30T12:26:10+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jlqp06</id>
    <title>Ollama does not do well</title>
    <updated>2025-03-28T08:55:42+00:00</updated>
    <author>
      <name>/u/aadarsh_af</name>
      <uri>https://old.reddit.com/user/aadarsh_af</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;None of the ollama models or tags work well with structured output. I've tried it with 3B param models as i don't have large GPU resources, my GPU gets stuck even with llama3.2. I've tried prompt engineering and grammar, it does not generate valid JSON. Is there any way i could make smaller param models perform well with lesser compute power?? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aadarsh_af"&gt; /u/aadarsh_af &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlqp06/ollama_does_not_do_well/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlqp06/ollama_does_not_do_well/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlqp06/ollama_does_not_do_well/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T08:55:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlxfig</id>
    <title>@@@@ signs in model responses</title>
    <updated>2025-03-28T15:15:43+00:00</updated>
    <author>
      <name>/u/SergeiTvorogov</name>
      <uri>https://old.reddit.com/user/SergeiTvorogov</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone encountered the problem where the Qwen-coder model outputs @@@@ instead of text, and after restarting, everything normalizes for some time? I'm using it in the continue.dev plugin for code autocompletion&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SergeiTvorogov"&gt; /u/SergeiTvorogov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlxfig/signs_in_model_responses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlxfig/signs_in_model_responses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlxfig/signs_in_model_responses/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T15:15:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm2ee2</id>
    <title>Weird slowness after first query?</title>
    <updated>2025-03-28T18:47:13+00:00</updated>
    <author>
      <name>/u/john_alan</name>
      <uri>https://old.reddit.com/user/john_alan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, with all models I see weird behaviour that I googled around but can't see an explanation for...&lt;/p&gt; &lt;p&gt;On first run I get stats like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;total duration: 1.094507167s load duration: 8.850792ms prompt eval count: 33 token(s) prompt eval duration: 32.268125ms prompt eval rate: 1022.68 tokens/s eval count: 236 token(s) eval duration: 1.052533167s eval rate: 224.22 tokens/s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;then on second and further queries it slows:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;total duration: 1.041227416s load duration: 9.1175ms prompt eval count: 286 token(s) prompt eval duration: 29.909875ms prompt eval rate: 9562.06 tokens/s eval count: 212 token(s) eval duration: 1.001476792s eval rate: 211.69 tokens/ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Until about 155 tokens/ on eval rate. &lt;/p&gt; &lt;p&gt;Any idea why?&lt;/p&gt; &lt;p&gt;Closing the model and running again immediately returns to ~224.&lt;/p&gt; &lt;p&gt;I'm using Ollama &lt;code&gt;0.6.2&lt;/code&gt; - and Llama 3.&lt;/p&gt; &lt;p&gt;But it happens in other versions and with other models...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/john_alan"&gt; /u/john_alan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm2ee2/weird_slowness_after_first_query/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm2ee2/weird_slowness_after_first_query/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jm2ee2/weird_slowness_after_first_query/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T18:47:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlunnp</id>
    <title>Mac Studio M1 Ultra or a TrueNAS box w/ RTX 3070 Ti</title>
    <updated>2025-03-28T13:07:44+00:00</updated>
    <author>
      <name>/u/zog1300</name>
      <uri>https://old.reddit.com/user/zog1300</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone — I’m lucky enough to have both systems running, and I’m trying to decide which one to dedicate to running Ollama (mainly for local LLM stuff like LLaMA, Mistral, etc.).&lt;/p&gt; &lt;p&gt;Here are my two setups:&lt;/p&gt; &lt;p&gt;🔹 Mac Studio M1 Ultra&lt;/p&gt; &lt;p&gt;64 GB unified memory&lt;/p&gt; &lt;p&gt;Apple Silicon (Metal backend, no CUDA)&lt;/p&gt; &lt;p&gt;Runs Ollama natively on macOS&lt;/p&gt; &lt;p&gt;🔹 TrueNAS SCALE box&lt;/p&gt; &lt;p&gt;Intel Xeon Bronze 3204 @ 1.90GHz&lt;/p&gt; &lt;p&gt;31 GB ECC RAM&lt;/p&gt; &lt;p&gt;EVGA RTX 3070 Ti (CUDA support)&lt;/p&gt; &lt;p&gt;I can run a Linux VM or container for Ollama and pass through the GPU&lt;/p&gt; &lt;p&gt;I'm only planning to run Ollama and use Samba shares — no VMs, Plex, or anything else intensive.&lt;/p&gt; &lt;p&gt;My gut says the 3070 Ti with CUDA support will destroy the M1 Ultra in terms of inference speed, even with the lower RAM, but I’d love to hear from people who’ve tested both. Has anyone done direct comparisons?&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts — especially around performance with 7B and 13B models, startup time, and memory overhead.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zog1300"&gt; /u/zog1300 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlunnp/mac_studio_m1_ultra_or_a_truenas_box_w_rtx_3070_ti/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlunnp/mac_studio_m1_ultra_or_a_truenas_box_w_rtx_3070_ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlunnp/mac_studio_m1_ultra_or_a_truenas_box_w_rtx_3070_ti/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T13:07:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlkpsj</id>
    <title>Which is the smallest, fastest text generation model on ollama that can be used for chatbot?</title>
    <updated>2025-03-28T02:24:49+00:00</updated>
    <author>
      <name>/u/gilzonme</name>
      <uri>https://old.reddit.com/user/gilzonme</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gilzonme"&gt; /u/gilzonme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlkpsj/which_is_the_smallest_fastest_text_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlkpsj/which_is_the_smallest_fastest_text_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlkpsj/which_is_the_smallest_fastest_text_generation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T02:24:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm3obq</id>
    <title>Minimalist Note-Taking App with Integrated AI Assistant</title>
    <updated>2025-03-28T19:41:04+00:00</updated>
    <author>
      <name>/u/The_Money_Mindset</name>
      <uri>https://old.reddit.com/user/The_Money_Mindset</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I'm exploring an idea for a note-taking app inspired by Flatnotes—offering a simple, distraction-free interface for capturing ideas—enhanced with built-in AI functionalities. The envisioned features include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Summarization:&lt;/strong&gt; Automatically condensing long notes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Suggestions:&lt;/strong&gt; Offering context-aware recommendations to refine or expand ideas.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interactive Prompts:&lt;/strong&gt; Asking insightful questions to deepen understanding and clarity of the notes.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The goal is to blend a minimalist design with smart, targeted AI capabilities that truly add value.&lt;/p&gt; &lt;p&gt;How would you suggest approaching this project? Are there any existing solutions that combine straightforward note-taking with these AI elements?&lt;/p&gt; &lt;p&gt;Any insights or suggestions are greatly appreciated. Thanks for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The_Money_Mindset"&gt; /u/The_Money_Mindset &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm3obq/minimalist_notetaking_app_with_integrated_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm3obq/minimalist_notetaking_app_with_integrated_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jm3obq/minimalist_notetaking_app_with_integrated_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T19:41:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm49d9</id>
    <title>WSL + Ollama: Local LLMs Are (Kinda) Here — Full Guide + Use Case Thoughts</title>
    <updated>2025-03-28T20:05:45+00:00</updated>
    <author>
      <name>/u/Standard_Abrocoma539</name>
      <uri>https://old.reddit.com/user/Standard_Abrocoma539</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Standard_Abrocoma539"&gt; /u/Standard_Abrocoma539 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/wsl2/comments/1jkdro9/wsl_ollama_local_llms_are_kinda_here_full_guide/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm49d9/wsl_ollama_local_llms_are_kinda_here_full_guide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jm49d9/wsl_ollama_local_llms_are_kinda_here_full_guide/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T20:05:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm6k1u</id>
    <title>Edit this repo for streamed response?</title>
    <updated>2025-03-28T21:46:05+00:00</updated>
    <author>
      <name>/u/eriknau13</name>
      <uri>https://old.reddit.com/user/eriknau13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like &lt;a href="https://github.com/enricollen/rag-conversational-agent"&gt;this RAG project&lt;/a&gt; for its simplicity and customizability. The one thing I can't figure out how to customize is setting ollama streaming to true so it can post answers in chunks rather than all at once. If anyone is familiar with this project and can see how I might do that I would appreciate any suggestions. It seems like the place to insert that setting would be in &lt;a href="http://llm.py"&gt;llm.py&lt;/a&gt; but I can't get anything successful to happen.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eriknau13"&gt; /u/eriknau13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm6k1u/edit_this_repo_for_streamed_response/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm6k1u/edit_this_repo_for_streamed_response/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jm6k1u/edit_this_repo_for_streamed_response/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T21:46:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlhysn</id>
    <title>Building a front end that sits on ollama, is this pointless?</title>
    <updated>2025-03-28T00:08:08+00:00</updated>
    <author>
      <name>/u/Outside-Prune-5838</name>
      <uri>https://old.reddit.com/user/Outside-Prune-5838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jlhysn/building_a_front_end_that_sits_on_ollama_is_this/"&gt; &lt;img alt="Building a front end that sits on ollama, is this pointless?" src="https://b.thumbs.redditmedia.com/aDV48slsep_Y6zioWCu9y0T-8V33XWr0Au91yIcaZMA.jpg" title="Building a front end that sits on ollama, is this pointless?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I started using gpt but ran into limits, got the $20 plan and was still hitting limits (because ai is fun) so I asked gpt what I could do and it recommended chatting through the api. Another gpt and 30 versions later I had a front end that spoke to openai but had zero personality. They also tend to lose their minds when the conversations get long.&lt;/p&gt; &lt;p&gt;Back to gpt to complain and asked how to do it for free and it said go for local llm and landed on ollama. Naturally I chose models that were too big to run on my machine because I was clueless but I got it sorted.&lt;/p&gt; &lt;p&gt;Got a bit annoyed at the basic interface and lack of memory and personality so I went back to gpt (getting my moneys worth) and spent a week (so far) working on a frontend that can talk to either locally running ollama or openai through api, remembers everything you spoke about and your memory is stored locally. It can analyse files and store them in memory too. You can give it whole documents then ask for summaries or specific points. It also reads what llms are downloaded in ollama and can even autostart them from the interface. You can also load in custom personas over the llm.&lt;/p&gt; &lt;p&gt;Also supports either local embedding w/gpu or embedding from openai through their api. Im debating releasing it because it was just a niche thing I did for me which turned into a whole ass program. If you can run ollama comfortably, you can run this on top easily as theres almost zero overhead.&lt;/p&gt; &lt;p&gt;The goal is jarvis on a budget and the memory thing has evolved several times which resulted because I wanted it to remember my name and now it remembers &lt;em&gt;everything&lt;/em&gt;. It also has a voice journal mode (work in progress, think star trek captains log). Right now integrating more voice features and an even more niche feature - way to control sonar, sabnzbd and radarr through the llm. Its also going to have tool access to go online and whatnot.&lt;/p&gt; &lt;p&gt;Its basically a multi-LLM brain with a shared long-term memory that is saved on your pc. You can start a conversation with your local llm, switch to gpt for something more complicated THEN switch back and your local llm has access to everything. The chat window doesnt even clear.&lt;/p&gt; &lt;p&gt;Talking to gpt through api doesnt require a plus plan just requires a few bucks in your openai api account, although Im big on local everything.&lt;/p&gt; &lt;h1&gt;Here's what happens under the hood:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;You chat with Mistral (or whatever llm) → everything gets stored: &lt;ul&gt; &lt;li&gt;Chat history → SQLite&lt;/li&gt; &lt;li&gt;Embedded chunks → ChromaDB&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;You switch to GPT (OpenAI) → same memory system is accessed: &lt;ul&gt; &lt;li&gt;GPT pulls from the same vector memory&lt;/li&gt; &lt;li&gt;You may even embed with the same SentenceTransformer (if not OpenAI embeddings)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;You switch back to Mistral → &lt;strong&gt;nothing is lost&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Vector search still hits all past data&lt;/li&gt; &lt;li&gt;SQLite short-term history still intact (unless wiped)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Snippet below, shameless self plug, sorry:&lt;/p&gt; &lt;h1&gt;🚧 ATOM Status Update 3/30/25&lt;/h1&gt; &lt;h1&gt;- What’s Working + What’s Coming -&lt;/h1&gt; &lt;p&gt;I've been using Atom on my personal rig (13700k, 4080, 128gb ram). You'd be fine with 64gb of ram unless running a massive model but I make poor financial decisions and tried to run models my hardware couldnt handle, anywho now using the gemma3:12b model with latest ollama (4b model worked nice too). With just ollama direct to model in terminal I get instant answers (obviously generic ones but instant in .5 seconds), through Atom its maybe 3 seconds unless Im asking for info from a massive document. I've been uploading text documents and old scanned documents then having it summarize parts of the documents or expand on certain points. I've also been throwing spec sheets at it and asking for random product details, also hasnt missed.&lt;/p&gt; &lt;p&gt;Files tab now has individual summarize buttons that drops a nice 1-2 paragraph description right on the page if you dont want it in chat. Again, I'm just a nerd that wanted a fun little custom tool, just as surprised as anyone else that its gotten this deep so fast, that it works so far and that it works at all tbh. The gui could be better, but Im not a design guy, Im going for function and retro look although I tweaked it a bit since I posted originally and it will get tweaked a bit more before release. The code is sane, the file layout makes sense and its annotated 6 ways from Sunday. I'm also learning as I go and honestly just having fun.&lt;/p&gt; &lt;h1&gt;tldr ; to the update:&lt;/h1&gt; &lt;p&gt;ATOM is an offline-first, persona-driven LLM assistant with memory, file chunking, OCR, and real-time summarization.&lt;/p&gt; &lt;p&gt;It’s not released yet, hell it didn't exist a week ago. I’m not dropping it until it installs clean, works end-to-end, and doesn’t require a full-time sysadmin to maintain, so maybe a week or two til repo? The idea is if you are techy enough to know what an llm is, know ollama and got it running, you can easily throw Atom on top.&lt;/p&gt; &lt;p&gt;Also if it flops, I will just vanish into the night so reddit people don't get me. Havent really slept in a few days and been working on this even while at work so yeah, Im excited even if it flops at least I made a thing I think is cool but I've been talking to bots so much I that I forget they arent real sometimes.....&lt;/p&gt; &lt;p&gt;Here’s what’s already working, &lt;strong&gt;like actually working for hours on end error free in a gui on my desk running locally off my hardware right now&lt;/strong&gt; not some cloud nonsense and not some fantasy roadmap of hopeful bs:&lt;/p&gt; &lt;h1&gt;✅ CORE CHAT + PROMPTING&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;🧠 Chat API works (POST /api/chat)&lt;/li&gt; &lt;li&gt;⚙️ Ollama backend support - Gemma, Mistral, etc. ( use gemma for best experience, mistral is meh at best)&lt;/li&gt; &lt;li&gt;⚛️ Atom autostarts Ollama and loads last used model automatically if its not running already&lt;/li&gt; &lt;li&gt;🌐 Optional OpenAI fallback (for both embedding and model, both default to local)*&lt;/li&gt; &lt;li&gt;🧬 Persona-aware prompting with memory injection (4 basic ones included, easy to add with .yaml)&lt;/li&gt; &lt;li&gt;🎭 Proper prompt formatting (Gemma-style: system/user/assistant)&lt;/li&gt; &lt;li&gt;🔁 Auto-reflection every 10 messages&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;✅ MEMORY SYSTEM (This is where ATOM flexes, I just wanted it to know my name but that ship's sailed)&lt;/h1&gt; &lt;h1&gt;“I just wanted it to know my name…”&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;“Okay but it’s too generic…”&lt;/li&gt; &lt;li&gt;“Okay now it needs personality…”&lt;/li&gt; &lt;li&gt;“Okay now it needs memory…”&lt;/li&gt; &lt;li&gt;“Okay now it needs a face, a name, a UI, a summary tab&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Okay now it needs a lifelike body.... wait thats for v2&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;ATOM doesn’t just &amp;quot;save messages&amp;quot;. It has a real, structured memory architecture.&lt;/p&gt; &lt;h1&gt;🧠 Vector Memory via ChromaDB&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Stores embedded chunks of conversations, files, summaries, reflections&lt;/li&gt; &lt;li&gt;Uses sentence-merged chunking for high-quality embeddings&lt;/li&gt; &lt;li&gt;Every chunk has metadata: source, memory_type, chunk_index&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🏷️ Memory Types&lt;/h1&gt; &lt;p&gt;Each memory is tagged with a type:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;chat: general convo&lt;/li&gt; &lt;li&gt;identity: facts about the user (&amp;quot;my name is Kevin&amp;quot;)&lt;/li&gt; &lt;li&gt;task: goals or reminders&lt;/li&gt; &lt;li&gt;file: parsed content from uploads&lt;/li&gt; &lt;li&gt;summary: generated insights from reflection&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🧩 Context Injection on Chat&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Finds the most relevant chunks by meaning, not keywords&lt;/li&gt; &lt;li&gt;Filters memory by relevance + type based on input&lt;/li&gt; &lt;li&gt;Injects only what matters — compact and useful&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🔁 Reflection Engine&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Every 10 messages, ATOM: &lt;ul&gt; &lt;li&gt;Summarizes important memory types&lt;/li&gt; &lt;li&gt;Stores them back into memory as summary&lt;/li&gt; &lt;li&gt;Runs purge_expired_chunks() + agent_reprioritize_memory() to keep things lean&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🧠 Identity Memory&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Detects identity statements like “my name is…” or “I’m from…”&lt;/li&gt; &lt;li&gt;Saves them as long-term facts&lt;/li&gt; &lt;li&gt;Used to personalize future answers&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;✅ FILE HANDLING&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;📁 Upload .pdf, .txt, .docx, .csv, .json, .md&lt;/li&gt; &lt;li&gt;🧠 Auto-chunks and stores memory with file source tagging&lt;/li&gt; &lt;li&gt;📦 .zip upload: full unpack + ingestion&lt;/li&gt; &lt;li&gt;🧾 OCR fallback (Tesseract + Poppler) for scanned PDFs&lt;/li&gt; &lt;li&gt;📡 Upload status polling via /api/upload/status (this is kinda buggy, uploads work fine just not status bar)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;✅ FRONTEND UI&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🧠 Sidebar model + persona selector&lt;/li&gt; &lt;li&gt;🗣️ Avatar per persona&lt;/li&gt; &lt;li&gt;🖱️ Drag + drop uploads&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;✅ AGENT &amp;amp; TOOLCHAIN&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;⚒️ LLM tool calls via ::tool: format&lt;/li&gt; &lt;li&gt;🧠 Tool registry maps tool names to Python functions&lt;/li&gt; &lt;li&gt;🔄 Reflection tools: generate_memory_reflection, purge_expired_chunks, reprioritize_memory&lt;/li&gt; &lt;li&gt;🧾 Detects and stores identity info automatically&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;✅ INFRA &amp;amp; DEVOPS&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;🧹 wipe_all_memory.py wipes vector + SQLite clean (take it out back and shoot it why dont ya)&lt;/li&gt; &lt;li&gt;🛠 Logging middleware suppresses polling spam&lt;/li&gt; &lt;li&gt;🔐 Dual license: &lt;ul&gt; &lt;li&gt;MIT for personal/hobby use&lt;/li&gt; &lt;li&gt;Commercial license required for resale/deployment&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;📎 Inline annotations throughout codebase (mostly for me tbh)&lt;/li&gt; &lt;li&gt;🧭 Clean routing (/api/*)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🛠️ BEFORE PUBLIC RELEASE&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;📦 One-click install (install.bat or setup.sh) or docker package maybe?&lt;/li&gt; &lt;li&gt;🌱 .env.example and automatic sanity checks&lt;/li&gt; &lt;li&gt;📝 Journal tab (voice-to-text log entry w/ Whisper)&lt;/li&gt; &lt;li&gt;🔊 TTS playback toggle in chat (works through gTTS, with pyttsx3 fallback)&lt;/li&gt; &lt;li&gt;🧠 Memory dashboard in UI&lt;/li&gt; &lt;li&gt;🧾 Reflection summary viewing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;*if you switch between local embedding and openai embedding it will change the chunk size and you must nuke the memory with the included script. That being said, all my testing has been done with local embeddings and Im going to start testing with openai embedding.&lt;/p&gt; &lt;h1&gt;🤖 Why No Release Yet?&lt;/h1&gt; &lt;p&gt;Because Reddit doesn’t need another half-baked local LLM wrapper (so much jarvis crap)&lt;/p&gt; &lt;p&gt;and, well, I'm sensitive damn it.&lt;/p&gt; &lt;p&gt;I’m shipping this when:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The full GUI works&lt;/li&gt; &lt;li&gt;Memory/recall/cleanup flows run without babysitting&lt;/li&gt; &lt;li&gt;You can install it on a fresh machine and it Just Works™&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So maybe a week or two?&lt;/p&gt; &lt;h1&gt;🧠 Licensing?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;MIT for personal use&lt;/li&gt; &lt;li&gt;Commercial license for resale, SaaS, or commercial deployment&lt;/li&gt; &lt;li&gt;You bring your own models (Ollama required) — ATOM doesn't ship any weights&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's not ready — but it's close.&lt;/p&gt; &lt;p&gt;next post will talk about open ai cost for embeddings vs local and whatnot for those that want it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/odofppsvprre1.png?width=2555&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0b328dcf7245c35100db7fa12a24b5c624831479"&gt;Here's ATOM summarizing the CIA’s Gateway doc and breaking down biofeedback with a local Gemma model. All offline. All memory-aware. UI, file chunking, and persona logic fully wired.Still not public. Still baking.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outside-Prune-5838"&gt; /u/Outside-Prune-5838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlhysn/building_a_front_end_that_sits_on_ollama_is_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlhysn/building_a_front_end_that_sits_on_ollama_is_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlhysn/building_a_front_end_that_sits_on_ollama_is_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T00:08:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlvhgk</id>
    <title>Link model with DB for memory?</title>
    <updated>2025-03-28T13:48:11+00:00</updated>
    <author>
      <name>/u/Ben_Graf</name>
      <uri>https://old.reddit.com/user/Ben_Graf</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there, I was curious if its possible to link a model to a local database and use that as memory. The scenario: The goal is a proactively acting calender and planner as well as control media. My idea would be for that to create on the main pc promts and results and have the model on on a pie just play them dynamically. Also it should remember things from the calender and use those as trigger too. &lt;/p&gt; &lt;p&gt;Example: i plan a calender event to clean my home. It plays the reply and t2speech premade at the time i told it to start. Depending on my reaction it either plays a more cheerful or more sarcastic one to motivate me.&lt;/p&gt; &lt;p&gt;I managed to set all up but without a memory it was all gone. Also I'd need my main pc to run all day if it was the source. So i think running it on a pie be better&lt;/p&gt; &lt;p&gt;Is that possible?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ben_Graf"&gt; /u/Ben_Graf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlvhgk/link_model_with_db_for_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlvhgk/link_model_with_db_for_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlvhgk/link_model_with_db_for_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T13:48:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm1rqh</id>
    <title>Worth fine-tuning an embedding model specifically for file/folder naming?</title>
    <updated>2025-03-28T18:20:30+00:00</updated>
    <author>
      <name>/u/taxem_tbma</name>
      <uri>https://old.reddit.com/user/taxem_tbma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;br /&gt; I’m not very experienced in AI, but I’ve been experimenting with using embedding models to semantically organize files — basically comparing file names, clustering them, and generating folder names with a local LLM if needed.&lt;/p&gt; &lt;p&gt;Right now I’m using general-purpose embedding models &lt;em&gt;mxbai-embed-large&lt;/em&gt; , but they sometimes miss the mark when it comes to the &lt;em&gt;&amp;quot;folder naming intuition&amp;quot;&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;So my question is:&lt;br /&gt; &lt;strong&gt;Would it make sense to fine-tune a small embedding model specifically for file/folder naming semantics?&lt;/strong&gt;&lt;br /&gt; Or is that overkill for a local tool like this?&lt;/p&gt; &lt;p&gt;For context, I’ve been building a CLI tool called &lt;a href="https://github.com/PerminovEugene/messy-folder-reorganizer-ai"&gt;messy-folder-reorganizer-ai&lt;/a&gt; that does exactly this with Ollama and local vector search.&lt;/p&gt; &lt;p&gt;Would love to hear thoughts or similar experiences.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taxem_tbma"&gt; /u/taxem_tbma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm1rqh/worth_finetuning_an_embedding_model_specifically/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm1rqh/worth_finetuning_an_embedding_model_specifically/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jm1rqh/worth_finetuning_an_embedding_model_specifically/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T18:20:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jll087</id>
    <title>Great event tonight with Ollama and vLLM</title>
    <updated>2025-03-28T02:39:46+00:00</updated>
    <author>
      <name>/u/Rude-Bad-6579</name>
      <uri>https://old.reddit.com/user/Rude-Bad-6579</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jll087/great_event_tonight_with_ollama_and_vllm/"&gt; &lt;img alt="Great event tonight with Ollama and vLLM" src="https://preview.redd.it/6yvrist4fcre1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=90f67ffb2ffcb6b8fada40af2bd2ba6a22bfc94b" title="Great event tonight with Ollama and vLLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Packed house, lots of great attendees. Loved Gemma demo running off 1 Mac laptop live. Super impressive &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rude-Bad-6579"&gt; /u/Rude-Bad-6579 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6yvrist4fcre1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jll087/great_event_tonight_with_ollama_and_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jll087/great_event_tonight_with_ollama_and_vllm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T02:39:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm35s2</id>
    <title>Computer vision for reading</title>
    <updated>2025-03-28T19:19:07+00:00</updated>
    <author>
      <name>/u/gttcoelho</name>
      <uri>https://old.reddit.com/user/gttcoelho</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, guys! I am using the Google vision API for transcribing text from images, but it is too expensive... do you know some cheaper alternative for this? I have tried llava but it is petty bad for text transcribing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gttcoelho"&gt; /u/gttcoelho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm35s2/computer_vision_for_reading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm35s2/computer_vision_for_reading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jm35s2/computer_vision_for_reading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T19:19:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm5spn</id>
    <title>Ollama blobs</title>
    <updated>2025-03-28T21:11:53+00:00</updated>
    <author>
      <name>/u/techmago</name>
      <uri>https://old.reddit.com/user/techmago</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a ton of blobs...&lt;br /&gt; How do i figure out which model is the owner of each blob?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techmago"&gt; /u/techmago &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm5spn/ollama_blobs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm5spn/ollama_blobs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jm5spn/ollama_blobs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T21:11:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmuasb</id>
    <title>What is the best model i can run?</title>
    <updated>2025-03-29T19:36:37+00:00</updated>
    <author>
      <name>/u/xdvst8x</name>
      <uri>https://old.reddit.com/user/xdvst8x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the best model i can run on my machine? It is a ThreadRipper with 128GB RAM, 8TB SSD, 3x 3090 Nvidia cards with 24GB.&lt;/p&gt; &lt;p&gt;i have tried a lot of models, but I can seem to find anything that works as well as claude or GPT.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xdvst8x"&gt; /u/xdvst8x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmuasb/what_is_the_best_model_i_can_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmuasb/what_is_the_best_model_i_can_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmuasb/what_is_the_best_model_i_can_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T19:36:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmrr83</id>
    <title>Ollama connect to Microsoft o365 account mail, calendar, contact oneDrive SharePoint</title>
    <updated>2025-03-29T17:42:02+00:00</updated>
    <author>
      <name>/u/Awkward-Desk-8340</name>
      <uri>https://old.reddit.com/user/Awkward-Desk-8340</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How connect ollama to my Microsoft webmail to talk with im ?&lt;/p&gt; &lt;p&gt;I m looking how to connect ollama to my webmail Microsoft account&lt;/p&gt; &lt;p&gt;Calendar Mail One drive&lt;/p&gt; &lt;p&gt;To make it my agent and works with him&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Awkward-Desk-8340"&gt; /u/Awkward-Desk-8340 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmrr83/ollama_connect_to_microsoft_o365_account_mail/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmrr83/ollama_connect_to_microsoft_o365_account_mail/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmrr83/ollama_connect_to_microsoft_o365_account_mail/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T17:42:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm1a4g</id>
    <title>Mastering Text Chunking with Ollama: A Comprehensive Guide to Advanced Processing</title>
    <updated>2025-03-28T18:00:04+00:00</updated>
    <author>
      <name>/u/KonradFreeman</name>
      <uri>https://old.reddit.com/user/KonradFreeman</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KonradFreeman"&gt; /u/KonradFreeman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://danielkliewer.com/blog/2025-03-28-Ollama-Chunking"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm1a4g/mastering_text_chunking_with_ollama_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jm1a4g/mastering_text_chunking_with_ollama_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T18:00:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmnhbj</id>
    <title>Build a Voice RAG with Deepseek, LangChain and Streamlit</title>
    <updated>2025-03-29T14:28:46+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jmnhbj/build_a_voice_rag_with_deepseek_langchain_and/"&gt; &lt;img alt="Build a Voice RAG with Deepseek, LangChain and Streamlit" src="https://external-preview.redd.it/kTJZuoiqL6QSf3wgIhIvK5EAijtopYLgpBSTlRcJH9I.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cbdb9aeb80a0fda3ec7fff61aa3213901aac5065" title="Build a Voice RAG with Deepseek, LangChain and Streamlit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=HT4a6A_wXdA&amp;amp;list=PLp01ObP3udmq2quR-RfrX4zNut_t_kNot&amp;amp;index=14"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmnhbj/build_a_voice_rag_with_deepseek_langchain_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmnhbj/build_a_voice_rag_with_deepseek_langchain_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T14:28:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmt8y3</id>
    <title>Ollama python library "chat" method question</title>
    <updated>2025-03-29T18:48:22+00:00</updated>
    <author>
      <name>/u/Haghiri75</name>
      <uri>https://old.reddit.com/user/Haghiri75</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a python code which uses the chat method. I just need to know does this chat method come with any sort of logging? You know something like when you are generating with SD/FLUX on terminal and there is a progress bar. &lt;/p&gt; &lt;p&gt;I saw source codes but couldn't find anything showing the progress. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Haghiri75"&gt; /u/Haghiri75 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmt8y3/ollama_python_library_chat_method_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmt8y3/ollama_python_library_chat_method_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmt8y3/ollama_python_library_chat_method_question/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T18:48:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmnb8b</id>
    <title>Testability of LLMs: the elusive hunt for deterministic output with ollama (or any vendor actually)</title>
    <updated>2025-03-29T14:20:32+00:00</updated>
    <author>
      <name>/u/boxabirds</name>
      <uri>https://old.reddit.com/user/boxabirds</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a bit obsessed about testability and LLMs. I worked with pytorch in the past and found at least with diffusion models, &lt;strong&gt;passing a seed would give deterministic output&lt;/strong&gt; (on the same hardware / software config). This was very powerful because it meant I could test variations and factor out common parameters. &lt;/p&gt; &lt;p&gt;And in the open weight world I saw the seed parameter, I saw it exposed as a parameter with ollama and I saw it exposed in GPT-4+ API (though OpenAI has since augmented it with system fingerprint). &lt;/p&gt; &lt;p&gt;This brought joy to my heart, as an engineer who hates fuzziness. &amp;quot;The capital of France is Paris&amp;quot; is NOT THE SAME AS &amp;quot;The capital of France is Paris!&amp;quot;. &lt;/p&gt; &lt;p&gt;HOWEVER I've only found two specific configurations of language models anywhere that seems to produce deterministic results, and that is aws Bedrock nova lite and nano, when temperature = 0 they are &amp;quot;reasonably deterministic&amp;quot; which of course is an oxymoron. But better than others. &lt;/p&gt; &lt;p&gt;I also tried Gemini and OpenAI and had no luck. &lt;/p&gt; &lt;p&gt;Am I missing something here? Or are we really seeing what is effectively a global denial from vendors that deterministic output is basicaly a pipe dream. &lt;/p&gt; &lt;p&gt;Please if someone can correct me to provide example code that guarantees (for some reasonable definition of guarantee) deterministic output so I don't have to introduce another whole language model evaluation evaluation piece. &lt;/p&gt; &lt;p&gt;thanks in advance&lt;/p&gt; &lt;p&gt;🙏&lt;/p&gt; &lt;p&gt;Here's a super basic script that tries to find any deterministic models you have installed with ollama&lt;/p&gt; &lt;p&gt;&lt;a href="https://gist.github.com/boxabirds/6257440850d2a874dd467f891879c776"&gt;https://gist.github.com/boxabirds/6257440850d2a874dd467f891879c776&lt;/a&gt;&lt;/p&gt; &lt;p&gt;needs jq installed. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boxabirds"&gt; /u/boxabirds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmnb8b/testability_of_llms_the_elusive_hunt_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmnb8b/testability_of_llms_the_elusive_hunt_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmnb8b/testability_of_llms_the_elusive_hunt_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T14:20:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmxl2z</id>
    <title>Haproxy infront of multiple ollama servers</title>
    <updated>2025-03-29T22:08:32+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Does anyone have haproxy balancing load to multiple Ollama servers?&lt;br /&gt; Not able to get my app to see/use the models. &lt;/p&gt; &lt;p&gt;Seems that for example&lt;br /&gt; curl ollamaserver_IP:11434 returns &amp;quot;ollama is running&amp;quot;&lt;br /&gt; From haproxy and from application server, so at least that request goes to haproxy and then to ollama and back to appserver.&lt;/p&gt; &lt;p&gt;When I take the haproxy away from between application server and the AI server all works. But when I put the haproxy, for some reason the traffic wont flow from application server -&amp;gt; haproxy to AI server. At least my application says were unable to Failed to get models from Ollama: cURL error 7: Failed to connect to ai.server05.net port 11434 after 1 ms: Couldn't connect to server.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmxl2z/haproxy_infront_of_multiple_ollama_servers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmxl2z/haproxy_infront_of_multiple_ollama_servers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmxl2z/haproxy_infront_of_multiple_ollama_servers/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T22:08:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmypm0</id>
    <title>ollama docker api</title>
    <updated>2025-03-29T23:03:02+00:00</updated>
    <author>
      <name>/u/wbiggs205</name>
      <uri>https://old.reddit.com/user/wbiggs205</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a server off site running in docker desktop. On windows 11 pro . But It is open to everyone I would like to know how to local it down so I'm the only one that can access it ? I do have tailscale installed then I block the port for ollama in windows firewall but now I can not access it thought tailscale &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wbiggs205"&gt; /u/wbiggs205 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmypm0/ollama_docker_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmypm0/ollama_docker_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmypm0/ollama_docker_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T23:03:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmgz50</id>
    <title>MCP servers using Ollama</title>
    <updated>2025-03-29T07:23:01+00:00</updated>
    <author>
      <name>/u/mehul_gupta1997</name>
      <uri>https://old.reddit.com/user/mehul_gupta1997</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jmgz50/mcp_servers_using_ollama/"&gt; &lt;img alt="MCP servers using Ollama" src="https://external-preview.redd.it/BIpLRntRPIc0zEIJuZa-Z96CiFN4WBZP78bNcyh3oVU.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53645309938d8d9462dc3e9eefaa8383ea65ceb6" title="MCP servers using Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehul_gupta1997"&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=z0DScLrix48"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmgz50/mcp_servers_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmgz50/mcp_servers_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T07:23:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmvmhg</id>
    <title>Adding GPU to old desktop to run Ollama</title>
    <updated>2025-03-29T20:37:05+00:00</updated>
    <author>
      <name>/u/zair</name>
      <uri>https://old.reddit.com/user/zair</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a Lenovo V55t desktop with the following specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen 5 3400G Processor&lt;/li&gt; &lt;li&gt;24GB DDR4-2666Mhz RAM&lt;/li&gt; &lt;li&gt;256GB SSD M.2 PCIe NVMe Opal&lt;/li&gt; &lt;li&gt;Radeon Vega 11 Graphics &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If I added a suitable GPU, could this run a reasonably large model? Considering this is a relatively slow PC that may not be able to fully leverage the latest GPUs, can you suggest what GPU I could get?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zair"&gt; /u/zair &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmvmhg/adding_gpu_to_old_desktop_to_run_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmvmhg/adding_gpu_to_old_desktop_to_run_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmvmhg/adding_gpu_to_old_desktop_to_run_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T20:37:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmrw67</id>
    <title>ollama inference 25% faster on Linux than windows</title>
    <updated>2025-03-29T17:48:02+00:00</updated>
    <author>
      <name>/u/AdhesivenessLatter57</name>
      <uri>https://old.reddit.com/user/AdhesivenessLatter57</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;running latest version of ollama 0.6.2 on both systems, updated windows 11 and latest build of kali Linux with kernel 3.11. python 3.12.9, pytorch 2.6, cuda 12.6 on both pc.&lt;/p&gt; &lt;p&gt;I have tested major under 8b models(llama3.2, gemma2, gemma3, qwen2.5 and mistral) available in ollama that inference is 25% faster on Linux pc than windows pc.&lt;/p&gt; &lt;p&gt;nividia quadro rtx 4000 8gb vram, 32gb ram, intel i7&lt;/p&gt; &lt;p&gt;is this a known fact? any benchmarking data or article on this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdhesivenessLatter57"&gt; /u/AdhesivenessLatter57 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmrw67/ollama_inference_25_faster_on_linux_than_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmrw67/ollama_inference_25_faster_on_linux_than_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmrw67/ollama_inference_25_faster_on_linux_than_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T17:48:02+00:00</published>
  </entry>
</feed>
