<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-04-04T17:23:13+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jq2gj4</id>
    <title>How do I select installation directories?</title>
    <updated>2025-04-02T22:55:56+00:00</updated>
    <author>
      <name>/u/ImpossibleBritches</name>
      <uri>https://old.reddit.com/user/ImpossibleBritches</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Earlier this morning I began experimenting with llama-stack.&lt;/p&gt; &lt;p&gt;I discovered that the llama cli either offers no way for the user to select installation directories, or if it does then this feature is not documented.&lt;/p&gt; &lt;p&gt;I removed it and installed ollama.&lt;/p&gt; &lt;p&gt;However, I'm having trouble discovering how to tell ollama where to install models.&lt;/p&gt; &lt;p&gt;Most of my system is on a crowded ssd. But I've got a secondary ssd where I've installed image models. There is a lot of space on my secondary ssd. I'd like to install llm's there.&lt;/p&gt; &lt;p&gt;How can I direct ollama to install models in a specified directory?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ImpossibleBritches"&gt; /u/ImpossibleBritches &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jq2gj4/how_do_i_select_installation_directories/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jq2gj4/how_do_i_select_installation_directories/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jq2gj4/how_do_i_select_installation_directories/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T22:55:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpryie</id>
    <title>Best Model for json parser analyser.</title>
    <updated>2025-04-02T15:50:44+00:00</updated>
    <author>
      <name>/u/Proper-Acanthaceae39</name>
      <uri>https://old.reddit.com/user/Proper-Acanthaceae39</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, im new in the local LLM world, and im still learning.&lt;/p&gt; &lt;p&gt;Im running in my local a Ollama with gemma:2b, but im not sure if is the best one for what im doing.&lt;/p&gt; &lt;p&gt;Basically with python, in extracting a pdf with pdfplumber to a json.&lt;br /&gt; I want to send this json to the LLM, so it can understand the json and return me another parsed JSON.&lt;/p&gt; &lt;p&gt;However, I'm facing two main issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It seems like gemma only supports around 12k characters of context, which is hard to manage since the extracted JSON varies a lot depending on the PDF.&lt;/li&gt; &lt;li&gt;Its tooo slow, to process a small pdf, its taking too much time&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm also concerned about accuracy, I'm not sure if this is the most suitable model for structured data parsing. &lt;/p&gt; &lt;p&gt;Some one can help me with tips?&lt;/p&gt; &lt;p&gt;Also, here its the code&lt;/p&gt; &lt;pre&gt;&lt;code&gt;#aiProcessor.py import json import os import uuid import requests from typing import Optional def load_prompt(path: str) -&amp;gt; str: with open(path, &amp;quot;r&amp;quot;, encoding=&amp;quot;utf-8&amp;quot;) as f: return f.read().strip() def call_llm(pdf_json_data: list, filename: str, model: str = &amp;quot;gemma:2b&amp;quot;) -&amp;gt; str: client_prompt = load_prompt(&amp;quot;../json/client.prompt&amp;quot;) purchase_prompt = load_prompt(&amp;quot;../json/purchase.prompt&amp;quot;) full_prompt = f&amp;quot;&amp;quot;&amp;quot; You are an intelligent invoice parser. Based on the structured data extracted from a Brazilian invoice PDF (below), extract and return exactly TWO JSONs: First JSON: {client_prompt} Second JSON: {purchase_prompt} Only return valid JSON. Do not explain. Structured invoice data: {json.dumps(pdf_json_data, indent=2, ensure_ascii=False)[:12000]} Filename: {filename} &amp;quot;&amp;quot;&amp;quot; response = requests.post( &amp;quot;http://localhost:11434/api/generate&amp;quot;, json={&amp;quot;model&amp;quot;: model, &amp;quot;prompt&amp;quot;: full_prompt}, stream=True, timeout=300 ) result = &amp;quot;&amp;quot; for line in response.iter_lines(): if line: try: chunk = json.loads(line.decode(&amp;quot;utf-8&amp;quot;)) result += chunk.get(&amp;quot;response&amp;quot;, &amp;quot;&amp;quot;) except: continue return result.strip() def extract_two_jsons(text: str): import re candidates = re.findall(r'\{(?:[^{}]|\{[^{}]*\})*\}', text) if len(candidates) &amp;gt;= 2: return candidates[0], candidates[1] return None, None def process_with_ai( extracted_json: list, filename: str, save_to_disk: bool = False, output_dir: str = &amp;quot;output/ai&amp;quot; ) -&amp;gt; Optional[dict]: &amp;quot;&amp;quot;&amp;quot; Processa o JSON extraído do PDF com a IA e retorna dois JSONs: cliente e compra. &amp;quot;&amp;quot;&amp;quot; result_text = call_llm(extracted_json, filename) client_str, purchase_str = extract_two_jsons(result_text) if not client_str or not purchase_str: print(f&amp;quot;⚠️ Could not extract two JSONs from AI result for {filename}&amp;quot;) if save_to_disk: os.makedirs(f&amp;quot;{output_dir}/fallback&amp;quot;, exist_ok=True) with open(f&amp;quot;{output_dir}/fallback/{filename}.txt&amp;quot;, &amp;quot;w&amp;quot;, encoding=&amp;quot;utf-8&amp;quot;) as f: f.write(result_text) return None try: client_json = json.loads(client_str) purchase_json = json.loads(purchase_str) except json.JSONDecodeError as e: print(f&amp;quot;❌ JSON parse error for {filename}: {e}&amp;quot;) return None client_id = str(uuid.uuid4()) purchase_id = str(uuid.uuid4()) client_json[&amp;quot;id&amp;quot;] = client_id if &amp;quot;client&amp;quot; in purchase_json: purchase_json[&amp;quot;client&amp;quot;][&amp;quot;id&amp;quot;] = client_id purchase_json[&amp;quot;id&amp;quot;] = purchase_id if save_to_disk: os.makedirs(f&amp;quot;{output_dir}/clientes&amp;quot;, exist_ok=True) os.makedirs(f&amp;quot;{output_dir}/compras&amp;quot;, exist_ok=True) with open(f&amp;quot;{output_dir}/clientes/{client_id}.json&amp;quot;, &amp;quot;w&amp;quot;, encoding=&amp;quot;utf-8&amp;quot;) as f: json.dump(client_json, f, indent=2, ensure_ascii=False) with open(f&amp;quot;{output_dir}/compras/{purchase_id}.json&amp;quot;, &amp;quot;w&amp;quot;, encoding=&amp;quot;utf-8&amp;quot;) as f: json.dump(purchase_json, f, indent=2, ensure_ascii=False) return {&amp;quot;client&amp;quot;: client_json, &amp;quot;purchase&amp;quot;: purchase_json} # extractor.py import fitz # PyMuPDF import pdfplumber import json import os from typing import Union, Optional from io import BytesIO def extract_pdf_structure( file: Union[str, BytesIO], save_to_file: bool = False, output_path: Optional[str] = None ) -&amp;gt; Optional[list]: data = [] doc = fitz.open(stream=file.read(), filetype=&amp;quot;pdf&amp;quot;) if isinstance(file, BytesIO) else fitz.open(file) for page_num, page in enumerate(doc, start=1): page_data = { &amp;quot;page&amp;quot;: page_num, &amp;quot;text_blocks&amp;quot;: [], &amp;quot;tables&amp;quot;: [] } blocks = page.get_text(&amp;quot;dict&amp;quot;)[&amp;quot;blocks&amp;quot;] for block in blocks: if &amp;quot;lines&amp;quot; in block: text_content = &amp;quot;&amp;quot; for line in block[&amp;quot;lines&amp;quot;]: for span in line[&amp;quot;spans&amp;quot;]: text_content += span[&amp;quot;text&amp;quot;] + &amp;quot; &amp;quot; page_data[&amp;quot;text_blocks&amp;quot;].append({ &amp;quot;bbox&amp;quot;: block[&amp;quot;bbox&amp;quot;], &amp;quot;text&amp;quot;: text_content.strip() }) data.append(page_data) doc.close() plumber_doc = pdfplumber.open(file) if isinstance(file, str) else pdfplumber.open(BytesIO(file.getvalue())) for i, page in enumerate(plumber_doc.pages): try: tables = page.extract_tables() if tables: data[i][&amp;quot;tables&amp;quot;] = tables except: continue plumber_doc.close() if save_to_file and output_path: os.makedirs(os.path.dirname(output_path), exist_ok=True) with open(output_path, &amp;quot;w&amp;quot;, encoding=&amp;quot;utf-8&amp;quot;) as f: json.dump(data, f, indent=2, ensure_ascii=False) return data if not save_to_file else None &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proper-Acanthaceae39"&gt; /u/Proper-Acanthaceae39 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpryie/best_model_for_json_parser_analyser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpryie/best_model_for_json_parser_analyser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jpryie/best_model_for_json_parser_analyser/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T15:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpjntg</id>
    <title>I made an Ollama hub</title>
    <updated>2025-04-02T08:24:58+00:00</updated>
    <author>
      <name>/u/retoor42</name>
      <uri>https://old.reddit.com/user/retoor42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made an Ollama hub where you can share your Ollama resources with others to be used.&lt;/p&gt; &lt;p&gt;Long story, see &lt;a href="https://ollama.molodetz.nl"&gt;https://ollama.molodetz.nl&lt;/a&gt; for information.&lt;/p&gt; &lt;p&gt;Uses of the API can just use their default api clients! For security people can only call chat completions api on the shared resources. Content gets validated before forwarded to your Ollama instance if you're a host.&lt;/p&gt; &lt;p&gt;I hope you guys like this concept. Donate your server!&lt;/p&gt; &lt;p&gt;See model availability here: &lt;a href="https://ollama.molodetz.nl/models"&gt;https://ollama.molodetz.nl/models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/retoor42"&gt; /u/retoor42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpjntg/i_made_an_ollama_hub/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpjntg/i_made_an_ollama_hub/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jpjntg/i_made_an_ollama_hub/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T08:24:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jq8r5n</id>
    <title>Ram issue in ollama</title>
    <updated>2025-04-03T03:59:31+00:00</updated>
    <author>
      <name>/u/yakinrubaiyat</name>
      <uri>https://old.reddit.com/user/yakinrubaiyat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am facing an issue where using Ollama to make continuous calls (around 200+) to Gemma 3 uses up all my 32GB of RAM and then crashes. I can see the RAM usage increasing in Task Manager, and after some time, the system crashes. Does anyone have any suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yakinrubaiyat"&gt; /u/yakinrubaiyat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jq8r5n/ram_issue_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jq8r5n/ram_issue_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jq8r5n/ram_issue_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T03:59:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpnyx5</id>
    <title>Using LLM to work with documents?</title>
    <updated>2025-04-02T13:00:20+00:00</updated>
    <author>
      <name>/u/TheseMarionberry2902</name>
      <uri>https://old.reddit.com/user/TheseMarionberry2902</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ll jump in the use case: We have around 100 documents so far with an average of 50 pages each, and we are expanding this. We wanted to sort the information, search inside, map the information and their interlinks. The thing is that each document may or may not be directly linked to the other.&lt;/p&gt; &lt;p&gt;One idea was use make a gitlab wiki or a mindmap, and structure the documents and interlink them while having the documents on the wiki (for example a tree of information and their interlinks, and link to documents). Another thing is that the documents are on a MS sharepoint&lt;/p&gt; &lt;p&gt;I was suggesting to download a local LLM, and &amp;quot;upload&amp;quot; the documents and work directly and locally on a secure basis (no internet). Now imo that will help us easily to locate information within documents, analyse and work directly. It can help us even make the mindmap and visualizations.&lt;/p&gt; &lt;p&gt;Which is the right solution? Is my understanding correct? And what do I need to make it work? &lt;/p&gt; &lt;p&gt;Thank you. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheseMarionberry2902"&gt; /u/TheseMarionberry2902 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpnyx5/using_llm_to_work_with_documents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpnyx5/using_llm_to_work_with_documents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jpnyx5/using_llm_to_work_with_documents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T13:00:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jq296o</id>
    <title>I made an almost universal LLM Creator/Trainer</title>
    <updated>2025-04-02T22:46:48+00:00</updated>
    <author>
      <name>/u/KiloXii</name>
      <uri>https://old.reddit.com/user/KiloXii</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created my own LLM creator/trainer to simplify the creation and training of huggingface models for use with ollama.&lt;/p&gt; &lt;p&gt;Essentially, you choose your base model from huggingface. (I don't know if it works with gated models yet but it works with normal ones)&lt;/p&gt; &lt;p&gt;then you give it a specifically formatted dataset, a system prompt, and a name and it will train the base model on all that info, merge the trained data with the model permanently, then create a gguf of your new model for download which you can use to make a modelfile for ollama.&lt;/p&gt; &lt;p&gt;And it's built using gradio for a simplified interface as well so the user only needs to learn minimal code to just to set up and then they can just run it from their browser locally.&lt;/p&gt; &lt;p&gt;In theory, it should work with most different types of models such as LLama, GPT, Mistral, Falcon, however so far I have only tested it with DeepSeek-R1-Distill-Qwen-1.5B and dolphin-LLama and it works for both of those.&lt;/p&gt; &lt;p&gt;Right now it doesnt work with models that don't have a chat template built into their tokenizer though such as wizardlm-uncensored, so I have to fix that later.&lt;/p&gt; &lt;p&gt;Anyways, I feel like this program may help a few people make their own models so this is the link to the github for it if anyone is interested:&lt;br /&gt; &lt;a href="https://github.com/KiloXiix/Kilos_Custom_LLM_Creator_Universal"&gt;https://github.com/KiloXiix/Kilos_Custom_LLM_Creator_Universal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what y'all think and if you find any bugs please as I want to make it better overall&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KiloXii"&gt; /u/KiloXii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jq296o/i_made_an_almost_universal_llm_creatortrainer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jq296o/i_made_an_almost_universal_llm_creatortrainer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jq296o/i_made_an_almost_universal_llm_creatortrainer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T22:46:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqfy9m</id>
    <title>I made my own CLI vibe tool</title>
    <updated>2025-04-03T11:29:44+00:00</updated>
    <author>
      <name>/u/retoor42</name>
      <uri>https://old.reddit.com/user/retoor42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I made my own CLI vibe tool using C with support for:&lt;br /&gt; - ollama&lt;br /&gt; - anthropic claude&lt;br /&gt; - openai (default, my key with gpt3.5 limited is included, works out of the box).&lt;/p&gt; &lt;p&gt;You make something like this in minutes: &lt;a href="https://molodetz.nl/project/streamii/README.md.html"&gt;https://molodetz.nl/project/streamii/README.md.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm using it for over a week now and it's a blazing useful tool. What ever c compile you have to compile, if you execute it in the CLI and it sees errors, it will fix everything instant for you! 20% of this tool is vibed by himself. It could generate the tool calls at a certain moment. &lt;/p&gt; &lt;p&gt;It's for linux only. &lt;/p&gt; &lt;p&gt;This is the project page: &lt;a href="https://molodetz.nl/project/r/README.md.html"&gt;https://molodetz.nl/project/r/README.md.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have not much experience with the Ollama version, since I do not have a beefii machine. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/retoor42"&gt; /u/retoor42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqfy9m/i_made_my_own_cli_vibe_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqfy9m/i_made_my_own_cli_vibe_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jqfy9m/i_made_my_own_cli_vibe_tool/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T11:29:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jptd2v</id>
    <title>Fully Unified Model</title>
    <updated>2025-04-02T16:47:38+00:00</updated>
    <author>
      <name>/u/No-Mulberry6961</name>
      <uri>https://old.reddit.com/user/No-Mulberry6961</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From that one guy who brought you AMN&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Modern-Prometheus-AI/FullyUnifiedModel"&gt;https://github.com/Modern-Prometheus-AI/FullyUnifiedModel&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here is the repository to Fully Unified Model (FUM), an ambitious open-source AI project available on GitHub, developed by the creator of AMN. This repository explores the integration of diverse cognitive functions into a single framework. It features advanced concepts including a Self-Improvement Engine (SIE) driving learning through complex internal rewards (novelty, habituation) and an emergent Unified Knowledge Graph (UKG) built on neural activity and plasticity (STDP).&lt;/p&gt; &lt;p&gt;FUM is currently in active development (consider it alpha/beta stage). This project represents ongoing research into creating more holistic, potentially neuromorphic AI. Documentation is evolving. Feedback, questions, and potential contributions are highly encouraged via GitHub issues/discussions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Mulberry6961"&gt; /u/No-Mulberry6961 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jptd2v/fully_unified_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jptd2v/fully_unified_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jptd2v/fully_unified_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T16:47:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jq5l9v</id>
    <title>does anyone know why Gemma is doing this?(Gemma3:1b using through open-webui)</title>
    <updated>2025-04-03T01:21:32+00:00</updated>
    <author>
      <name>/u/mobheadfireball</name>
      <uri>https://old.reddit.com/user/mobheadfireball</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jq5l9v/does_anyone_know_why_gemma_is_doing_thisgemma31b/"&gt; &lt;img alt="does anyone know why Gemma is doing this?(Gemma3:1b using through open-webui)" src="https://preview.redd.it/b77n4zhpuise1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=728b201f97a590371b91ec2b5084713b03cd4534" title="does anyone know why Gemma is doing this?(Gemma3:1b using through open-webui)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mobheadfireball"&gt; /u/mobheadfireball &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b77n4zhpuise1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jq5l9v/does_anyone_know_why_gemma_is_doing_thisgemma31b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jq5l9v/does_anyone_know_why_gemma_is_doing_thisgemma31b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T01:21:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqa8xx</id>
    <title>Running Ollama model in a cloud service? It's murdering my Mac</title>
    <updated>2025-04-03T05:25:10+00:00</updated>
    <author>
      <name>/u/Wild_King_1035</name>
      <uri>https://old.reddit.com/user/Wild_King_1035</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm building a React Native app that sends user audio to llama 3.2, which is in a python backend that im running locally on my Macbook Pro.&lt;/p&gt; &lt;p&gt;I know its a terrible idea to run Ollama models on a Mac, and it is, even a single request eats up available CPU and threatens to crash my computer.&lt;/p&gt; &lt;p&gt;I realize I can't run it locally any longer, I need to host it somewhere but still have it available to continue working and testing it. &lt;/p&gt; &lt;p&gt;How can I host my backend for an affordable price? This is just a personal project, and I haven't hosted a backend this in-depth before. I'd prefer to host it now in a cloud service that I will eventually use if and when the app goes into production.&lt;/p&gt; &lt;p&gt;Thanks in advance all&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wild_King_1035"&gt; /u/Wild_King_1035 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqa8xx/running_ollama_model_in_a_cloud_service_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqa8xx/running_ollama_model_in_a_cloud_service_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jqa8xx/running_ollama_model_in_a_cloud_service_its/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T05:25:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqoh9i</id>
    <title>Downloading pytorch and tensorflow lowered the speed of my responses.</title>
    <updated>2025-04-03T17:25:42+00:00</updated>
    <author>
      <name>/u/Specialist-Damage102</name>
      <uri>https://old.reddit.com/user/Specialist-Damage102</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm very new to AI stuff and I don't think I am documented enough. Yesterday I managed to install privateGPT with ollama as an llm backend. When I ran it ,it showed this error: &amp;quot;None of PyTorch, TensorFlow &amp;gt;= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used&amp;quot; but I didn't think much of it since it would still run with 44% GPU usage and the responses were pretty fast. Today I got the bright idea to install pytorch and tensor flow because I tought I could get more perfomance... Well my GPU usage is now at 29% max and the AI responses are slower. The same model has been used in both cases: Llama3.1 8b and I tested it with qwen2.5-coder-7b-instruct and still have the same GPU usage and also lowered speed compared to llama3.1. Did I break something by installing pytorch and tensorflow? Can I make it go back or maybe be even better? Specs: gtx 1060 6gb,16gb ram, ryzen 5 5600x.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist-Damage102"&gt; /u/Specialist-Damage102 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqoh9i/downloading_pytorch_and_tensorflow_lowered_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqoh9i/downloading_pytorch_and_tensorflow_lowered_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jqoh9i/downloading_pytorch_and_tensorflow_lowered_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T17:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqmb1g</id>
    <title>Server Rack assembled.</title>
    <updated>2025-04-03T16:02:20+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jqmb1g/server_rack_assembled/"&gt; &lt;img alt="Server Rack assembled." src="https://preview.redd.it/k5d99gym3fse1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4f1664326930dfeecce826f57037f8ae9d75d34" title="Server Rack assembled." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k5d99gym3fse1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqmb1g/server_rack_assembled/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jqmb1g/server_rack_assembled/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T16:02:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqgzmw</id>
    <title>Need Advice on API Key Management with Ollama &amp; Terms of Service</title>
    <updated>2025-04-03T12:22:10+00:00</updated>
    <author>
      <name>/u/Lazy-Dragonfly7825</name>
      <uri>https://old.reddit.com/user/Lazy-Dragonfly7825</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm setting up an internal API service in my college to provide students with access to Ollama while ensuring proper resource utilization and fair access for everyone. The system will issue API keys to track usage. I have a couple of questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;After authentication, my backend currently interacts with Ollama using the Ollama SDK. Is this the right approach for an internal setup, or should I make direct API calls instead?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;For terms and conditions, should I follow a structure similar to Ollama's model-related terms, or do I need a more detailed agreement outlining usage policies?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Would love to hear your thoughts and best practices! Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lazy-Dragonfly7825"&gt; /u/Lazy-Dragonfly7825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqgzmw/need_advice_on_api_key_management_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqgzmw/need_advice_on_api_key_management_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jqgzmw/need_advice_on_api_key_management_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T12:22:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jq85ks</id>
    <title>Just for fun the Playstation 2 gets in on some NLP Olamma Hybrid chat action</title>
    <updated>2025-04-03T03:28:08+00:00</updated>
    <author>
      <name>/u/doscore</name>
      <uri>https://old.reddit.com/user/doscore</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jq85ks/just_for_fun_the_playstation_2_gets_in_on_some/"&gt; &lt;img alt="Just for fun the Playstation 2 gets in on some NLP Olamma Hybrid chat action" src="https://preview.redd.it/32rqi9wwgjse1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12af3a5cf385862372c5a7229da55d3a0ecbe107" title="Just for fun the Playstation 2 gets in on some NLP Olamma Hybrid chat action" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I trained a really really small model on a dictionary and NLP for telling stories, it can also access my ollama setup via the network and store and use the context to write new and better stories.&lt;/p&gt; &lt;p&gt;this ps2 is running debian 6, 300mhz 32bm ram with a 40 gig seagate hdd.&lt;/p&gt; &lt;p&gt;it takes around 5 mins for it to generate a story and much quicker if you just use ollama obviously&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doscore"&gt; /u/doscore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/32rqi9wwgjse1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jq85ks/just_for_fun_the_playstation_2_gets_in_on_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jq85ks/just_for_fun_the_playstation_2_gets_in_on_some/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T03:28:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqi8h7</id>
    <title>Reasoning with 3B Llama along with Long Prompt and Context Improvement</title>
    <updated>2025-04-03T13:19:06+00:00</updated>
    <author>
      <name>/u/adeelahmadch</name>
      <uri>https://old.reddit.com/user/adeelahmadch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey all, i just updated my RL based trained LLama that is not only reasoning but also is good at programming and long context/prompts: &lt;a href="https://huggingface.co/adeelahmad/ReasonableLlama3-3B-Jr"&gt;https://huggingface.co/adeelahmad/ReasonableLlama3-3B-Jr&lt;/a&gt;&lt;/p&gt; &lt;p&gt;let me know if a anyone have any feedback&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adeelahmadch"&gt; /u/adeelahmadch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqi8h7/reasoning_with_3b_llama_along_with_long_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqi8h7/reasoning_with_3b_llama_along_with_long_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jqi8h7/reasoning_with_3b_llama_along_with_long_prompt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T13:19:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqlkak</id>
    <title>Help with finding a good local LLM</title>
    <updated>2025-04-03T15:33:21+00:00</updated>
    <author>
      <name>/u/end69420</name>
      <uri>https://old.reddit.com/user/end69420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys I need to do some short videos analysis ~1 minute long. Mostly people talking. What is a good local multimodal LLM that is capable of doing this. Assume my PC can handle 70b models fairly well. Any suggestions would be appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/end69420"&gt; /u/end69420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqlkak/help_with_finding_a_good_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqlkak/help_with_finding_a_good_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jqlkak/help_with_finding_a_good_local_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T15:33:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqoeok</id>
    <title>LogSonic - A desktop log analysis tool powered by Ollama for English-to-Bleve search syntax query.</title>
    <updated>2025-04-03T17:22:59+00:00</updated>
    <author>
      <name>/u/aagosh</name>
      <uri>https://old.reddit.com/user/aagosh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jqoeok/logsonic_a_desktop_log_analysis_tool_powered_by/"&gt; &lt;img alt="LogSonic - A desktop log analysis tool powered by Ollama for English-to-Bleve search syntax query." src="https://external-preview.redd.it/djUxenZtMDRtbnNlMRR-PG9LIEToNYY7dRW63hJepYzoqwgIItzrxOLINo8I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=273a1f8e336b021bfcde08fdb10d7701ad8fdc40" title="LogSonic - A desktop log analysis tool powered by Ollama for English-to-Bleve search syntax query." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aagosh"&gt; /u/aagosh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/m7z8wm04mnse1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqoeok/logsonic_a_desktop_log_analysis_tool_powered_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jqoeok/logsonic_a_desktop_log_analysis_tool_powered_by/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T17:22:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqajhl</id>
    <title>DocuMind (RAG app using Ollama)</title>
    <updated>2025-04-03T05:42:50+00:00</updated>
    <author>
      <name>/u/harry0027</name>
      <uri>https://old.reddit.com/user/harry0027</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jqajhl/documind_rag_app_using_ollama/"&gt; &lt;img alt="DocuMind (RAG app using Ollama)" src="https://external-preview.redd.it/dh-_Wu3nlQIF6oJL5CtgqrVT_37A3bEVuNY-wVXwauM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75ddf988c05b0a6a647e7b1644b7a61cebb09aaf" title="DocuMind (RAG app using Ollama)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m excited to share &lt;a href="https://github.com/Harry-027/DocuMind"&gt;DocuMind&lt;/a&gt;, a RAG (Retrieval-Augmented Generation) desktop app I built to make document management smarter and more efficient. It uses Ollama at backend to connect with LLMs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Harry-027/DocuMind"&gt;Github: DocuMind&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With DocuMind, you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🔎 Quickly search and retrieve relevant information from large pdf files.&lt;/li&gt; &lt;li&gt;🔄 Generate insightful answers using AI based on the context.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Building this app was an incredible experience, and it deepened my understanding of retrieval-augmented generation and AI-powered solutions.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1jqajhl/video/iqv2xswxhkse1/player"&gt;Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;#AI #RAG #Ollama #Rust #Tauri #Axum #QdrantDB&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/harry0027"&gt; /u/harry0027 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqajhl/documind_rag_app_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqajhl/documind_rag_app_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jqajhl/documind_rag_app_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T05:42:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr1kjb</id>
    <title>Docker with Ollama Tool Calling</title>
    <updated>2025-04-04T02:45:54+00:00</updated>
    <author>
      <name>/u/vvbalboa98</name>
      <uri>https://old.reddit.com/user/vvbalboa98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For context, I am trying to build an application with its own UI, and other facilities, with the chatbot being just a small part of it.&lt;/p&gt; &lt;p&gt;I have been successfully locally running Llama3.2 with tool-calling using my own functions to query my own data for my specific use case. This has been good, if not quite slow. But I'm sure once i get a better computer/GPU it will much quicker. I have written the chatbot using python and i am exposing it as a FastAPI endpoint that my UI can call. It works well locally and I love the tool calling functionality&lt;/p&gt; &lt;p&gt;However, i need to dockerize this whole setup, with the UI, chatbot and other features of the app as different services and using a named volume to share data between the different part of the app and any data/models/things that need to be persisted to prevent downloading during every start. But I am unsure of how to go about the setup. All the tutorials I have seen online for docker with ollama seem to use the official ollama image and are using the models directly. If I do this, my tool calling functionality is gone, which will be my main purpose of doing this whole thing.&lt;/p&gt; &lt;p&gt;These are the things I need for my chatbot service container:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Ollama (the equivalent of the setup.exe)&lt;/li&gt; &lt;li&gt;the Llama3.2 model&lt;/li&gt; &lt;li&gt;the python script with the tool calling functionality.&lt;/li&gt; &lt;li&gt;exposing this whole thing as an endpoint with FastAPI.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;part 3 and 4 I have done, but when i call the endpoint, the part of the script where it is actually calling the LLM (response = ollama.chat(..)) is failing because it is not finding the model.&lt;/p&gt; &lt;p&gt;Has anyone faced this issue before? Any suggestions will help because I am out of my wits rn&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vvbalboa98"&gt; /u/vvbalboa98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jr1kjb/docker_with_ollama_tool_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jr1kjb/docker_with_ollama_tool_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jr1kjb/docker_with_ollama_tool_calling/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-04T02:45:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr0fzl</id>
    <title>4x AMD Instinct Mi210 QwQ-32B-FP16 - Effortless</title>
    <updated>2025-04-04T01:47:53+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gzc1k18v3qse1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jr0fzl/4x_amd_instinct_mi210_qwq32bfp16_effortless/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jr0fzl/4x_amd_instinct_mi210_qwq32bfp16_effortless/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-04T01:47:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr0hzj</id>
    <title>Question on OLLAMA_KV_CACHE_TYPE</title>
    <updated>2025-04-04T01:50:36+00:00</updated>
    <author>
      <name>/u/dookie168</name>
      <uri>https://old.reddit.com/user/dookie168</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I run a quantized model e.g. hf.co/bartowski/google_gemma-3-27b-it-GGUF:Q4_K_M&lt;/p&gt; &lt;p&gt;And I also have OLLAMA_KV_CACHE_TYPE set to q4_0. Does that mean the model is being quantized twice? How does that affect inference accuracy?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dookie168"&gt; /u/dookie168 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jr0hzj/question_on_ollama_kv_cache_type/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jr0hzj/question_on_ollama_kv_cache_type/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jr0hzj/question_on_ollama_kv_cache_type/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-04T01:50:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqps8y</id>
    <title>Build local AI Agents and RAGs over your docs/sites in minutes now.</title>
    <updated>2025-04-03T18:14:46+00:00</updated>
    <author>
      <name>/u/DonTizi</name>
      <uri>https://old.reddit.com/user/DonTizi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Following up on Rlama – many of you were interested in how quickly you can get a local RAG system running. The key now is the new &lt;strong&gt;Rlama Playground&lt;/strong&gt;, our web UI designed to take the guesswork out of configuration.&lt;/p&gt; &lt;p&gt;Building RAG systems often involves juggling models, data sources, chunking parameters, reranking settings, and more. It can get complex fast! The Playground simplifies this dramatically.&lt;/p&gt; &lt;p&gt;The Playground acts as a user-friendly interface to visually configure your entire Rlama RAG setup before you even touch the terminal.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here's how you build an AI solution in minutes using it:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Select Your Model:&lt;/strong&gt; Choose any model available via &lt;strong&gt;Ollama&lt;/strong&gt; (like llama3, gemma3, mistral) or &lt;strong&gt;Hugging Face&lt;/strong&gt; directly in the UI.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Choose Your Data Source:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local Folder:&lt;/strong&gt; Just provide the path to your documents (./my_project_docs).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Website:&lt;/strong&gt; Enter the URL (&lt;a href="https://rlama.dev"&gt;https://rlama.dev&lt;/a&gt;), set crawl depth, concurrency, and even specify paths to exclude (/blog, /archive). You can also leverage sitemaps.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;(Optional) Fine-Tune Settings:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Chunking:&lt;/strong&gt; While we offer sensible defaults (Hybrid or Auto), you can easily select different strategies (Semantic, Fixed, Hierarchical), adjust chunk size, and overlap if needed. Tooltips guide you.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reranking:&lt;/strong&gt; Enable/disable reranking (improves relevance), set a score threshold, or even specify a different reranker model – all visually.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generate Command:&lt;/strong&gt; This is the magic button! Based on all your visual selections, the Playground instantly generates the precise rlama CLI command needed to build this exact RAG system.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Copy &amp;amp; Run:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Click &amp;quot;Copy&amp;quot;.&lt;/li&gt; &lt;li&gt;Paste the generated command into your terminal.&lt;/li&gt; &lt;li&gt;Hit Enter. Rlama processes your data and builds the vector index.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Query Your Data:&lt;/strong&gt; Once complete (usually seconds to a couple of minutes depending on data size), run rlama run my_website_rag and start asking questions!&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;That's it!&lt;/strong&gt; The Playground turns potentially complex configuration into a simple point-and-click process, generating the exact command so you can launch your tailored, local AI solution in minutes. No need to memorize flags or manually craft long commands.&lt;/p&gt; &lt;p&gt;It abstracts the complexity while still giving you granular control if you want it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try the Playground yourself:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Playground/Website:&lt;/strong&gt; &lt;a href="https://rlama.dev/"&gt;https://rlama.dev/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/dontizi/rlama"&gt;https://github.com/dontizi/rlama&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me know if you have any questions about using the Playground!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DonTizi"&gt; /u/DonTizi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqps8y/build_local_ai_agents_and_rags_over_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqps8y/build_local_ai_agents_and_rags_over_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jqps8y/build_local_ai_agents_and_rags_over_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T18:14:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr3xgm</id>
    <title>Arch-Function-Chat (1B/3B/7B) - Device friendly, family of fast LLMs for function calling scenarios now trained to chat.</title>
    <updated>2025-04-04T04:57:26+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on feedback from users and the developer community that used Arch-Function (our previous gen) model, I am excited to share our latest work: &lt;a href="https://huggingface.co/katanemo/Arch-Function-Chat-3B"&gt;Arch-Function-Chat&lt;/a&gt; A collection of fast, device friendly LLMs that achieve performance on-par with GPT-4 on function calling, now trained to chat.&lt;/p&gt; &lt;p&gt;These LLMs have three additional training objectives.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Be able to refine and clarify the user request. This means to ask for required function parameters, clarify ambiguous input (e.g., &amp;quot;Transfer $500&amp;quot; without specifying accounts, can be “Transfer from” and “Transfer to”)&lt;/li&gt; &lt;li&gt;Accurately maintain context in two specific scenarios: &lt;ol&gt; &lt;li&gt;Progressive information disclosure such as in multi-turn conversations where information is revealed gradually (i.e., the model asks info of multiple parameters and the user only answers one or two instead of all the info)&lt;/li&gt; &lt;li&gt;Context switch where the model must infer missing parameters from context (e.g., &amp;quot;Check the weather&amp;quot; should prompt for location if not provided) and maintains context between turns (e.g., &amp;quot;What about tomorrow?&amp;quot; after a weather query but still in the middle of clarification)&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Respond to the user based on executed tools results. For common function calling scenarios where the response of the execution is all that's needed to complete the user request, Arch-Function-Chat can interpret and respond to the user via chat. Note, parallel and multiple function calling was already supported so if the model needs to respond based on multiple tools call it still can.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Of course the 3B model will now be the primary LLM used in &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;. Hope you all like the work 🙏. Happy building!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jr3xgm/archfunctionchat_1b3b7b_device_friendly_family_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jr3xgm/archfunctionchat_1b3b7b_device_friendly_family_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jr3xgm/archfunctionchat_1b3b7b_device_friendly_family_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-04T04:57:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jr4a4p</id>
    <title>When will we get Qwen 2.5 Omni - the most multi modal available in ollama ?</title>
    <updated>2025-04-04T05:18:57+00:00</updated>
    <author>
      <name>/u/AxelBlaze20850</name>
      <uri>https://old.reddit.com/user/AxelBlaze20850</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AxelBlaze20850"&gt; /u/AxelBlaze20850 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jr4a4p/when_will_we_get_qwen_25_omni_the_most_multi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jr4a4p/when_will_we_get_qwen_25_omni_the_most_multi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jr4a4p/when_will_we_get_qwen_25_omni_the_most_multi/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-04T05:18:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqzbvf</id>
    <title>I Created A Lightweight Voice Assistant for Ollama with Real-Time Interaction</title>
    <updated>2025-04-04T00:50:48+00:00</updated>
    <author>
      <name>/u/typhoon90</name>
      <uri>https://old.reddit.com/user/typhoon90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I just built OllamaGTTS, a lightweight voice assistant that brings AI-powered voice interactions to your local Ollama setup using Google TTS for natural speech synthesis. It’s fast, interruptible, and optimized for real-time conversations. I am aware that some people prefer to keep everything local so I am working on an update that will likely use Kokoro for local speech synthesis. I would love to hear your thoughts on it and how it can be improved.&lt;/p&gt; &lt;p&gt;Key Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time voice interaction (Silero VAD + Whisper transcription)&lt;/li&gt; &lt;li&gt;Interruptible speech playback (no more waiting for the AI to finish talking)&lt;/li&gt; &lt;li&gt;FFmpeg-accelerated audio processing (optional speed-up for faster * replies)&lt;/li&gt; &lt;li&gt;Persistent conversation history with configurable memory&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/ExoFi-Labs/OllamaGTTS"&gt;GitHub Repo: https://github.com/ExoFi-Labs/OllamaGTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Instructions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Clone Repo&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Install requirements&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Run ollama_gtts.py&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/typhoon90"&gt; /u/typhoon90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqzbvf/i_created_a_lightweight_voice_assistant_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqzbvf/i_created_a_lightweight_voice_assistant_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jqzbvf/i_created_a_lightweight_voice_assistant_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-04T00:50:48+00:00</published>
  </entry>
</feed>
