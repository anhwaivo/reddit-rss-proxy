<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-07-22T16:25:35+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1m3zbas</id>
    <title>introducing computron_9000</title>
    <updated>2025-07-19T15:41:59+00:00</updated>
    <author>
      <name>/u/larz01larz</name>
      <uri>https://old.reddit.com/user/larz01larz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on an AI personal assistant that runs on local hardware and currently uses Ollama as its inference backend. I've got plans to add a lot more capabilities beyond what it can do right now which is; search the web, search reddit, work on the filesystem, write and execute code (in containers), and do deep research on a topic. &lt;/p&gt; &lt;p&gt;It's still a WIP and the setup instructions aren't great. You'll have the best luck if you are running it on linux, at least for the code execution. Everything else should be OS agnostic.&lt;/p&gt; &lt;p&gt;Give it a try and let me know what features you'd like me to add. If you get stuck, let me know and I'll help you get setup.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lefoulkrod/computron_9000/"&gt;https://github.com/lefoulkrod/computron_9000/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/larz01larz"&gt; /u/larz01larz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3zbas/introducing_computron_9000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3zbas/introducing_computron_9000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m3zbas/introducing_computron_9000/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-19T15:41:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3xvg8</id>
    <title>Finetuning a model</title>
    <updated>2025-07-19T14:41:07+00:00</updated>
    <author>
      <name>/u/Private_Tank</name>
      <uri>https://old.reddit.com/user/Private_Tank</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;br /&gt; im kinda new to ollama and have a big project. I have a private cookbook which I populated with a lot of recipies. I mean there are over 1000 recipes in it, including personal ratings. Now I want to finetune the ai so I can talk to my cookbook if that makes sense.&lt;/p&gt; &lt;p&gt;&amp;quot;What is the best soup&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;I have ingedients x,y,z what can you recommend&amp;quot;&lt;/p&gt; &lt;p&gt;How would you tackle this task?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Private_Tank"&gt; /u/Private_Tank &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3xvg8/finetuning_a_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3xvg8/finetuning_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m3xvg8/finetuning_a_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-19T14:41:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4d1a1</id>
    <title>Ollama + ollama-mcp-bridge problem by Open Web UI</title>
    <updated>2025-07-20T01:49:38+00:00</updated>
    <author>
      <name>/u/carlosetabosa</name>
      <uri>https://old.reddit.com/user/carlosetabosa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m4d1a1/ollama_ollamamcpbridge_problem_by_open_web_ui/"&gt; &lt;img alt="Ollama + ollama-mcp-bridge problem by Open Web UI" src="https://a.thumbs.redditmedia.com/TCakJ4ogaz5-kdouKm5nMHoc_aMmjax2TaZEZ6AWhG8.jpg" title="Ollama + ollama-mcp-bridge problem by Open Web UI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ERROR | ollama_mcp_bridge.proxy_service:proxy_chat_with_tools:52 - Chat proxy failed: {&amp;quot;error&amp;quot;:&amp;quot;model is required&amp;quot;}&lt;br /&gt; ERROR | ollama_mcp_bridge.api:chat:49 - /api/chat failed: {&amp;quot;error&amp;quot;:&amp;quot;model is required&amp;quot;}&amp;quot;POST /api/chat HTTP/1.1&amp;quot; 400 Bad Request&lt;/p&gt; &lt;p&gt;I'm trying llama3.2 by Ollama with my Open WebUI.&lt;br /&gt; I have configured the tool in Manage Tool Servers:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a4w9jo2npxdf1.png?width=696&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f2ef1a61bc52382235f946819f1d1b025b870e67"&gt;https://preview.redd.it/a4w9jo2npxdf1.png?width=696&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f2ef1a61bc52382235f946819f1d1b025b870e67&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This phase is OK, because I can see my MCP in the chat screen, just like that:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cxshijivpxdf1.png?width=683&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9892901bb03f6e07aa880a092216ef358d1a3f4d"&gt;https://preview.redd.it/cxshijivpxdf1.png?width=683&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9892901bb03f6e07aa880a092216ef358d1a3f4d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However I'm asking somenthing that calls a MCP and the LLM calls the correct MCP but it does not put the model argument:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zuz24m05qxdf1.png?width=2056&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3afafca61e4301c0c86ac84a770ea6d5573449c7"&gt;https://preview.redd.it/zuz24m05qxdf1.png?width=2056&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3afafca61e4301c0c86ac84a770ea6d5573449c7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Someone?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/carlosetabosa"&gt; /u/carlosetabosa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4d1a1/ollama_ollamamcpbridge_problem_by_open_web_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4d1a1/ollama_ollamamcpbridge_problem_by_open_web_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4d1a1/ollama_ollamamcpbridge_problem_by_open_web_ui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T01:49:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3zfae</id>
    <title>RTX (RTX 3090/4090/5090) GPU vs Apple M4 Max/M3 Ultra. Is RTX worth it over when over MSRP?</title>
    <updated>2025-07-19T15:46:40+00:00</updated>
    <author>
      <name>/u/bikers301</name>
      <uri>https://old.reddit.com/user/bikers301</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I need a computer to run LLM jobs (likely qwen 2.5 32B Q4)&lt;/p&gt; &lt;p&gt;What I'm Doing:&lt;/p&gt; &lt;p&gt;I'm using a LLM hosted on a computer to run Celery Redis jobs. It pulls one report of ~20,000 characters to answer about 15 qualitative questions per job. I'd like to run minimum 6 of these jobs per hour. Preferably more. Plan is to run this 24/7 for months on end.&lt;/p&gt; &lt;p&gt;Question: Hardware - RTX 3090 vs 4090 vs 5090 vs M4 Max vs M3 Ultra&lt;/p&gt; &lt;p&gt;I know the GPUS will heavily out perform the M4 Max and M3 Ultra, but what makes more sense from a bang for your buck performance? I'm looking at grabbing a Mac Studio (M4 Max) with 48GB memory for ~$2,500. But would the performance be that terrible compared to a RTX 5090?&lt;/p&gt; &lt;p&gt;If I could find a RTX 5090 at MSRP that would be a different story, but I haven't see any drops since May for a FE.&lt;/p&gt; &lt;p&gt;Open to thoughts or suggestions? I'd like to make a system for sub $3k preferably.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bikers301"&gt; /u/bikers301 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3zfae/rtx_rtx_309040905090_gpu_vs_apple_m4_maxm3_ultra/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3zfae/rtx_rtx_309040905090_gpu_vs_apple_m4_maxm3_ultra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m3zfae/rtx_rtx_309040905090_gpu_vs_apple_m4_maxm3_ultra/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-19T15:46:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4izdn</id>
    <title>My Fine-Tuned Model Keeps Echoing Prompts or Giving Blank/Generic Responses</title>
    <updated>2025-07-20T07:32:43+00:00</updated>
    <author>
      <name>/u/Srmxz</name>
      <uri>https://old.reddit.com/user/Srmxz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I’ve been working on fine-tuning open-source LLMs like Phi-3 and LLaMA 3 using Unsloth in Google Colab, targeting a chatbot for customer support (around 500 prompt-response examples).&lt;/p&gt; &lt;p&gt;I’m facing the same recurring issues no matter what I do:&lt;/p&gt; &lt;p&gt;⸻&lt;/p&gt; &lt;p&gt;❗ The problems: 1. The model often responds with the exact same prompt I gave it, instead of the intended response. 2. Sometimes it returns blank output. 3. When it does respond, it gives very generic or off-topic answers, not the specific ones from my training data.&lt;/p&gt; &lt;p&gt;⸻&lt;/p&gt; &lt;p&gt;🛠️ My Setup: • Using Unsloth + FastLanguageModel • Trained on a .json or .jsonl dataset with format:&lt;/p&gt; &lt;p&gt;{ &amp;quot;prompt&amp;quot;: &amp;quot;How long does it take to get a refund?&amp;quot;, &amp;quot;response&amp;quot;: &amp;quot;Refunds typically take 5–7 business days.&amp;quot; }&lt;/p&gt; &lt;p&gt;Wrapped in training with:&lt;/p&gt; &lt;p&gt;f&amp;quot;### Input: {prompt}\n### Output: {response}&amp;lt;|endoftext|&amp;gt;&amp;quot;&lt;/p&gt; &lt;p&gt;Inference via:&lt;/p&gt; &lt;p&gt;messages = [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;How long does it take to get a refund?&amp;quot;}] tokenizer.apply_chat_template(...)&lt;/p&gt; &lt;p&gt;What I’ve tried: • Training with both 3 and 10 epochs • Training both Phi-3-mini and LLaMA 3 8B with LoRA (4-bit) • Testing with correct Modelfile templates in Ollama like:&lt;/p&gt; &lt;p&gt;TEMPLATE &amp;quot;&amp;quot;&amp;quot;### Input: {{ .Prompt }}\n### Output:&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;Why is the model not learning my input-output structure properly? • Is there a better way to format the prompts or structure the dataset? • Could the model size (like Phi-3) be a bottleneck? • Should I be adding system prompts or few-shot examples at inference?&lt;/p&gt; &lt;p&gt;Any advice, shared experiences, or working examples would help a lot. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Srmxz"&gt; /u/Srmxz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4izdn/my_finetuned_model_keeps_echoing_prompts_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4izdn/my_finetuned_model_keeps_echoing_prompts_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4izdn/my_finetuned_model_keeps_echoing_prompts_or/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T07:32:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4leia</id>
    <title>Re-ranking support using SQLite RAG with haiku.rag</title>
    <updated>2025-07-20T10:12:02+00:00</updated>
    <author>
      <name>/u/gogozad</name>
      <uri>https://old.reddit.com/user/gogozad</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gogozad"&gt; /u/gogozad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/Rag/comments/1m4kfzh/reranking_support_using_sqlite_rag_with_haikurag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4leia/reranking_support_using_sqlite_rag_with_haikurag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4leia/reranking_support_using_sqlite_rag_with_haikurag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T10:12:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m41wh3</id>
    <title>Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler</title>
    <updated>2025-07-19T17:29:07+00:00</updated>
    <author>
      <name>/u/PsychologicalTap1541</name>
      <uri>https://old.reddit.com/user/PsychologicalTap1541</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m41wh3/websitecrawler_extract_data_from_websites_in_llm/"&gt; &lt;img alt="Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler" src="https://external-preview.redd.it/nrxKAdkehTr03Y2fP2Hf-vrA-7QLtEcGdfnlKNEGSps.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1aec063dacb6b8a3700910c3ce49f2b57a32057" title="Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PsychologicalTap1541"&gt; /u/PsychologicalTap1541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/pc8544/Website-Crawler"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m41wh3/websitecrawler_extract_data_from_websites_in_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m41wh3/websitecrawler_extract_data_from_websites_in_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-19T17:29:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4igz7</id>
    <title>Gpu support</title>
    <updated>2025-07-20T07:00:21+00:00</updated>
    <author>
      <name>/u/Ok-Band6009</name>
      <uri>https://old.reddit.com/user/Ok-Band6009</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys how long do you think its gonna take for ollama to add support for the new AMD cards, my 10th gen i5 is kinda struggling, my 9060xt 16gb would perform a lot better&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Band6009"&gt; /u/Ok-Band6009 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4igz7/gpu_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4igz7/gpu_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4igz7/gpu_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T07:00:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4k2t1</id>
    <title>ollama models and Hugging Face models use case</title>
    <updated>2025-07-20T08:45:14+00:00</updated>
    <author>
      <name>/u/cipherninjabyte</name>
      <uri>https://old.reddit.com/user/cipherninjabyte</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just curious what would you use ollama models and hugging face models for ? writing articles locally or fine tuning or what else?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cipherninjabyte"&gt; /u/cipherninjabyte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4k2t1/ollama_models_and_hugging_face_models_use_case/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4k2t1/ollama_models_and_hugging_face_models_use_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4k2t1/ollama_models_and_hugging_face_models_use_case/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T08:45:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4j5qp</id>
    <title>When is SmolLM3 coming on Ollama?</title>
    <updated>2025-07-20T07:44:40+00:00</updated>
    <author>
      <name>/u/falconHigh13</name>
      <uri>https://old.reddit.com/user/falconHigh13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have tried the new Huggingface Model on different platforms and even hosting locally but its very slow and take a lot of compute. I even tried huggingface Inference API and its not working. So when is this model coming on Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/falconHigh13"&gt; /u/falconHigh13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4j5qp/when_is_smollm3_coming_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4j5qp/when_is_smollm3_coming_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4j5qp/when_is_smollm3_coming_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T07:44:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4624q</id>
    <title>i just managed to run tinyllama1.1b and n8n in a low-end android phone</title>
    <updated>2025-07-19T20:22:28+00:00</updated>
    <author>
      <name>/u/actuallytech</name>
      <uri>https://old.reddit.com/user/actuallytech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m4624q/i_just_managed_to_run_tinyllama11b_and_n8n_in_a/"&gt; &lt;img alt="i just managed to run tinyllama1.1b and n8n in a low-end android phone" src="https://b.thumbs.redditmedia.com/6dh4kc8nJaE5b1uWFo_bUTN5U0tOMrcyUHvZJ6zjJ-U.jpg" title="i just managed to run tinyllama1.1b and n8n in a low-end android phone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the phone i used is an samsung m32 6gb ram with a mediatek G80 &lt;/p&gt; &lt;p&gt;i runned in a Debian via proot-distro in Termux (no root) and i can access both locally, It’s working better than I expected &lt;/p&gt; &lt;p&gt;i dont know is there any way to use its gpu &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/actuallytech"&gt; /u/actuallytech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m4624q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4624q/i_just_managed_to_run_tinyllama11b_and_n8n_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4624q/i_just_managed_to_run_tinyllama11b_and_n8n_in_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-19T20:22:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4ploe</id>
    <title>mistral-small3.2:latest 15B takes 28GB VRAM?</title>
    <updated>2025-07-20T13:58:54+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;NAME ID SIZE PROCESSOR UNTIL mistral-small3.2:latest 5a408ab55df5 28 GB 38%/62% CPU/GPU 36 minutes from now 7900 XTX 24gb vram ryzen 7900 64GB RAM Question: Mistral size on disk is 15GB. Why it needs 28GB of VRAM and does not fit into 24GB GPU? ollama version is 0.9.6 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4ploe/mistralsmall32latest_15b_takes_28gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4ploe/mistralsmall32latest_15b_takes_28gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4ploe/mistralsmall32latest_15b_takes_28gb_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T13:58:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4rl2z</id>
    <title>Anyone else tracking their local LLMs’ performance? I built a tool to make it easier</title>
    <updated>2025-07-20T15:23:18+00:00</updated>
    <author>
      <name>/u/Hades_7658</name>
      <uri>https://old.reddit.com/user/Hades_7658</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I've been running some LLMs locally and was curious how others are keeping tabs on model performance, latency, and token usage. I didn’t find a lightweight tool that fit my needs, so I started working on one myself.&lt;/p&gt; &lt;p&gt;It’s a simple dashboard + API setup that helps me monitor and analyze what's going on under the hood mainly for performance tuning and observability. Still early days, but it’s been surprisingly useful for understanding how my models are behaving over time.&lt;/p&gt; &lt;p&gt;Curious how the rest of you handle observability. Do you use logs, custom scripts, or something else? I’ll drop a link in the comments in case anyone wants to check it out or build on top of it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hades_7658"&gt; /u/Hades_7658 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4rl2z/anyone_else_tracking_their_local_llms_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4rl2z/anyone_else_tracking_their_local_llms_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4rl2z/anyone_else_tracking_their_local_llms_performance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T15:23:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4nclb</id>
    <title>ChatGPT-like Voice LLM</title>
    <updated>2025-07-20T12:08:58+00:00</updated>
    <author>
      <name>/u/embracing_athena</name>
      <uri>https://old.reddit.com/user/embracing_athena</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like the ChaGPT voice mode where I was able to converse with the AI with voice but that is limited to 15 minutes or so daily.&lt;/p&gt; &lt;p&gt;My question is, is there an LLM that I can run with Ollama to achieve the same but with no limits? I feel like any LLM can be used but at the same time seems like I'm feeling I'm missing something. Any extra software must be used along with Ollama for this work?&lt;/p&gt; &lt;p&gt;Please excuse me for my bad English.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/embracing_athena"&gt; /u/embracing_athena &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4nclb/chatgptlike_voice_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4nclb/chatgptlike_voice_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4nclb/chatgptlike_voice_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T12:08:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4zl6c</id>
    <title>vision model that can "scape" webpages?</title>
    <updated>2025-07-20T20:44:39+00:00</updated>
    <author>
      <name>/u/larz01larz</name>
      <uri>https://old.reddit.com/user/larz01larz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is anyone aware of a vision model that would be able to take a screenshot of a webpage and create a playwright script to navigate the page based on the screen shot?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/larz01larz"&gt; /u/larz01larz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4zl6c/vision_model_that_can_scape_webpages/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m4zl6c/vision_model_that_can_scape_webpages/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m4zl6c/vision_model_that_can_scape_webpages/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-20T20:44:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5cqfp</id>
    <title>Can I run an embedding model on a dell wyse 3040? If so, How do I set it up for this single purpose?</title>
    <updated>2025-07-21T07:45:49+00:00</updated>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use obsidian+smart connections plugin to look up for semantical similarities between the texts of several research papers I have saved in markdown format, I have no clue about how to utilise RAG or LLMs in general for my usecase but what I do is just enough as of yet.&lt;/p&gt; &lt;p&gt;I want to unload some of the embeddings processing to a secondary device I have on me since both my devices are weak hardware wise, how to set up the thin client for this one purpose and what os+model to use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m5cqfp/can_i_run_an_embedding_model_on_a_dell_wyse_3040/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m5cqfp/can_i_run_an_embedding_model_on_a_dell_wyse_3040/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m5cqfp/can_i_run_an_embedding_model_on_a_dell_wyse_3040/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-21T07:45:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1m61ivl</id>
    <title>This started as a prompt snippet manager…</title>
    <updated>2025-07-22T01:49:01+00:00</updated>
    <author>
      <name>/u/TutorialDoctor</name>
      <uri>https://old.reddit.com/user/TutorialDoctor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m61ivl/this_started_as_a_prompt_snippet_manager/"&gt; &lt;img alt="This started as a prompt snippet manager…" src="https://preview.redd.it/1s4x9x9wzbef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cbe47b33c0d74023d7751a1e50657eb44cc148e0" title="This started as a prompt snippet manager…" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a snippet manager desktop app with ollama for myself and it quickly became a lot more than that… &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TutorialDoctor"&gt; /u/TutorialDoctor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1s4x9x9wzbef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m61ivl/this_started_as_a_prompt_snippet_manager/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m61ivl/this_started_as_a_prompt_snippet_manager/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T01:49:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6faor</id>
    <title>It make so much time to downlaod</title>
    <updated>2025-07-22T14:17:25+00:00</updated>
    <author>
      <name>/u/Maleficent_Floor_941</name>
      <uri>https://old.reddit.com/user/Maleficent_Floor_941</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m6faor/it_make_so_much_time_to_downlaod/"&gt; &lt;img alt="It make so much time to downlaod" src="https://preview.redd.it/le6tlowepfef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fda6831476840efa099cfc80c1af6ad4683bd96f" title="It make so much time to downlaod" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m just downloading the Ola model, but there is so many issues in the when my MacBook is suddenly when the screen is inactive, then it is off. After that time, there will be a half and complete, but not such more completed. Will start again from the 0 to downloading around the model, that’s called a three, that’s why I am using right now so for such a bug, so please fix it right now, okay&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent_Floor_941"&gt; /u/Maleficent_Floor_941 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/le6tlowepfef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6faor/it_make_so_much_time_to_downlaod/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m6faor/it_make_so_much_time_to_downlaod/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T14:17:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1m60gro</id>
    <title>How do I generate an entire book?</title>
    <updated>2025-07-22T00:58:22+00:00</updated>
    <author>
      <name>/u/-ProfitLogical-</name>
      <uri>https://old.reddit.com/user/-ProfitLogical-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I like to listen to something while doing things like painting and whatnot. Sometime I have an idea for a story that might be interesting to listen to but doesn't exist. What model and how can I get a book of approximately 80k-120k words to generate from an idea I put in. It seems like they can't generate it all in one window but can it just keep making new windows till its done? Maybe it can then go back and put all those windows in a doc? Most people seem to want an AI to help them write a story while I want it to do the whole thing. I know its not going to be awesome but it might be good enough to listen to while working on something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-ProfitLogical-"&gt; /u/-ProfitLogical- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m60gro/how_do_i_generate_an_entire_book/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m60gro/how_do_i_generate_an_entire_book/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m60gro/how_do_i_generate_an_entire_book/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T00:58:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1m67s3f</id>
    <title>Quali sono i passaggi per installare una GPU NVIDIA M40 24GB su un Dell Precision T5820?</title>
    <updated>2025-07-22T07:34:24+00:00</updated>
    <author>
      <name>/u/Jaded_Treat_230</name>
      <uri>https://old.reddit.com/user/Jaded_Treat_230</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sto cercando di installare seconda gpu m40 24gb sul dell t5820. Attualemnte monta una p4000. quando installo m40 pc non si avvia. &lt;/p&gt; &lt;p&gt;Sembra che ci sia problema di incompatibilità, ho provato queste soluzioni: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;aggiornamento bios, problrma persiste&lt;/li&gt; &lt;li&gt;Uso nvflash, e impostare m40 in modalita grafica ma come faccio non avendo gpu installata?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;qualcuno a soluzioni?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jaded_Treat_230"&gt; /u/Jaded_Treat_230 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m67s3f/quali_sono_i_passaggi_per_installare_una_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m67s3f/quali_sono_i_passaggi_per_installare_una_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m67s3f/quali_sono_i_passaggi_per_installare_una_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T07:34:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5z1x7</id>
    <title>Is there a simple way to "enhance" a model with the content of a book?</title>
    <updated>2025-07-21T23:52:30+00:00</updated>
    <author>
      <name>/u/RaticateLV99</name>
      <uri>https://old.reddit.com/user/RaticateLV99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run some DnD adventures and I want to teach local models with the content of a book.&lt;/p&gt; &lt;p&gt;But, I also want to add more details about &lt;strong&gt;my adventure from time to time&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Is there a simple way to enhance the model with the content of my adventures and the content of the books?&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RaticateLV99"&gt; /u/RaticateLV99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m5z1x7/is_there_a_simple_way_to_enhance_a_model_with_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m5z1x7/is_there_a_simple_way_to_enhance_a_model_with_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m5z1x7/is_there_a_simple_way_to_enhance_a_model_with_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-21T23:52:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1m638nd</id>
    <title>Realtime codebase indexing for coding agents with ~ 50 lines of Python (open source)</title>
    <updated>2025-07-22T03:11:53+00:00</updated>
    <author>
      <name>/u/Whole-Assignment6240</name>
      <uri>https://old.reddit.com/user/Whole-Assignment6240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Would love to share my open source project that buildings realtime indexing &amp;amp; context for coding agents ~ 50 lines of Python on the &lt;a href="https://github.com/cocoindex-io/cocoindex/blob/main/examples/code_embedding/main.py#L11-L84"&gt;indexing path&lt;/a&gt;. Full blog and explanation &lt;a href="https://cocoindex.io/blogs/index-code-base-for-rag"&gt;here&lt;/a&gt;. Would love your feedback and appreciate a star on the repo if it is helpful, thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Assignment6240"&gt; /u/Whole-Assignment6240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m638nd/realtime_codebase_indexing_for_coding_agents_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m638nd/realtime_codebase_indexing_for_coding_agents_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m638nd/realtime_codebase_indexing_for_coding_agents_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T03:11:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6dow3</id>
    <title>Disable ssl check</title>
    <updated>2025-07-22T13:10:46+00:00</updated>
    <author>
      <name>/u/qtm_music</name>
      <uri>https://old.reddit.com/user/qtm_music</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there a way to disable ssl check for ollama in docker? I work on windows, my corporate proxy replaces certificates, is there a way to disable the check?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/qtm_music"&gt; /u/qtm_music &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6dow3/disable_ssl_check/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6dow3/disable_ssl_check/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m6dow3/disable_ssl_check/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T13:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5jqbo</id>
    <title>Use llm to gather insights of market fluctuations</title>
    <updated>2025-07-21T14:02:07+00:00</updated>
    <author>
      <name>/u/m19990328</name>
      <uri>https://old.reddit.com/user/m19990328</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m5jqbo/use_llm_to_gather_insights_of_market_fluctuations/"&gt; &lt;img alt="Use llm to gather insights of market fluctuations" src="https://preview.redd.it/diudkgucf8ef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6f52d897baf99be2923a0d6ecfbc861119fda53" title="Use llm to gather insights of market fluctuations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I've recently built a project that explores stock price trends and gathers market insights. Last time I shared it here, some of you showed interest. Now, I've packaged it as a Windows app with a GUI. Feel free to check it out!&lt;/p&gt; &lt;p&gt;Project: &lt;a href="https://github.com/CyrusCKF/stock-gone-wrong"&gt;https://github.com/CyrusCKF/stock-gone-wrong&lt;/a&gt;&lt;br /&gt; Download: &lt;a href="https://github.com/CyrusCKF/stock-gone-wrong/releases/tag/v0.1.0-alpha"&gt;https://github.com/CyrusCKF/stock-gone-wrong/releases/tag/v0.1.0-alpha&lt;/a&gt; (Windows may display a warning)&lt;/p&gt; &lt;p&gt;To use this function, first navigate to the &lt;strong&gt;&amp;quot;Events&amp;quot;&lt;/strong&gt; tab. Enter your ticker, select a date range, and click the button. The stock trends will be split into several &amp;quot;&lt;em&gt;major events&amp;quot;&lt;/em&gt;. Use the slider to select an event you're interested in, then click &lt;strong&gt;&amp;quot;Find News&amp;quot;&lt;/strong&gt;. This will initialize an Ollama agent to scrape and summarize stock news around the timeframe. Note that this process may take several minutes, depending on your machine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DISCLAIMER&lt;/strong&gt; This tool is not intended to provide stock-picking recommendations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m19990328"&gt; /u/m19990328 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/diudkgucf8ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m5jqbo/use_llm_to_gather_insights_of_market_fluctuations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m5jqbo/use_llm_to_gather_insights_of_market_fluctuations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-21T14:02:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6gkzw</id>
    <title>Monitoring your repo 24/7 using Agents.</title>
    <updated>2025-07-22T15:07:12+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m6gkzw/monitoring_your_repo_247_using_agents/"&gt; &lt;img alt="Monitoring your repo 24/7 using Agents." src="https://external-preview.redd.it/MTZlOWQxZGF5ZmVmMXY1D7FxNTB0y5Y5bDWcUCwmhhELkEtUCpjgYWthIbZh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5504a3b62012fcba7cec25b8b88207b943a226a" title="Monitoring your repo 24/7 using Agents." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever wish you could have someone watching your Github repo 24/7? &lt;/p&gt; &lt;p&gt;We built an agent that monitors your repo, finds who most recently starred it, and autonomously reaches out via email!&lt;/p&gt; &lt;p&gt;Discord : &lt;a href="https://discord.com/invite/ZYN7f7KPjS"&gt;https://discord.com/invite/ZYN7f7KPjS&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wcf1sbjayfef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6gkzw/monitoring_your_repo_247_using_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m6gkzw/monitoring_your_repo_247_using_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T15:07:12+00:00</published>
  </entry>
</feed>
