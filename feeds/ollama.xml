<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-09T18:06:26+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ikyjsz</id>
    <title>LLM agent autonomous pentester</title>
    <updated>2025-02-08T21:45:11+00:00</updated>
    <author>
      <name>/u/FishermanEnough7091</name>
      <uri>https://old.reddit.com/user/FishermanEnough7091</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi ! I need some help: I want to build an autonomous LLM agent running locally (Ollama for example), which have access to a kali linux machine (in a docker running locally also on my MacBook). The agent have a target IP, and is able to run commands and to adapt his actions based on the output of the previous commands he gets (for example a Nmap scan, then he tries a msfconsole in order to exploit a CVE - really basic example here). &lt;/p&gt; &lt;p&gt;I need help to connect the LLM to docker and to have access to the output of each commands. Do you have any idea of how to do it ? Thanks a lot, and I am open to any suggestions ! :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FishermanEnough7091"&gt; /u/FishermanEnough7091 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikyjsz/llm_agent_autonomous_pentester/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikyjsz/llm_agent_autonomous_pentester/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ikyjsz/llm_agent_autonomous_pentester/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T21:45:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikmb2i</id>
    <title>Single GPU with more VRAM or split between two?</title>
    <updated>2025-02-08T12:34:39+00:00</updated>
    <author>
      <name>/u/Fluffy_Sun1498</name>
      <uri>https://old.reddit.com/user/Fluffy_Sun1498</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I am relatively new to LLMs and local AI. But it seems to me like a lot of fun and a potential to have a fully locally running assistant that would be enough for simple tasks (like assisting in smart home). I have read a lot of articles about it and ran a few models myself, so I am not a total newbie to the topic, but there is one thing I cannot find info about anywhere. I am putting myself a workstation for different tasks, kinda &amp;quot;home server&amp;quot; and I am planning on buying GPU (s) for LLM. Now after this rather long introduction, let me share the &lt;strong&gt;main problem&lt;/strong&gt;, I have two choices: Nvidia Quadro M6000 with 24GB of VRAM and Nvidia RTX Quadro A2000 with 12GB of VRAM. Why those two? Because when it comes to the price, it is roughly that one M6000 costs the same as two A2000 (Used) and overall they are relatively cheap (affordable by me). I know you can split the model between two GPUS, but I've read somewhere that this is not efficient if you don't have nvlink. The A2000 is much newer and has higher compute capability than the M6000, but to get &amp;quot;the same&amp;quot; amount of VRAM I should buy two A2000. Also, A2000 has much lower TDP. I planned on buying a single A2000 to run models no bigger than 10b, and later, if I'd ever love to run bigger models, just buy a second one. The question based on all previous text is, if I buy a single A2000 and later buy another one, will this be worse\simmilar\better experience than buying a single M6000?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluffy_Sun1498"&gt; /u/Fluffy_Sun1498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikmb2i/single_gpu_with_more_vram_or_split_between_two/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikmb2i/single_gpu_with_more_vram_or_split_between_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ikmb2i/single_gpu_with_more_vram_or_split_between_two/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T12:34:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1il2w8z</id>
    <title>Ollama gpu with alpine Linux</title>
    <updated>2025-02-09T01:07:57+00:00</updated>
    <author>
      <name>/u/greeneyestyle</name>
      <uri>https://old.reddit.com/user/greeneyestyle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m running an alpine Linux VM where the majority of my docker containers are. I want to pass through my nvidia rtx 3060. Will this work with my alpine Linux vm or is it going to be a painful process to try to get the gpu drivers working in this environment?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/greeneyestyle"&gt; /u/greeneyestyle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1il2w8z/ollama_gpu_with_alpine_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1il2w8z/ollama_gpu_with_alpine_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1il2w8z/ollama_gpu_with_alpine_linux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T01:07:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1il3vze</id>
    <title>Ollama not supporting Mac book pro with radeon pro 5500m 8gb</title>
    <updated>2025-02-09T01:59:03+00:00</updated>
    <author>
      <name>/u/SnooObjections989</name>
      <uri>https://old.reddit.com/user/SnooObjections989</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am using a 2019 MacBook Pro with radon pro 5500m 8gb.&lt;/p&gt; &lt;p&gt;When I try LLM that is 100% running on CPU. Does anyone know how can I use my laptop GPU to run LLM locally? &lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooObjections989"&gt; /u/SnooObjections989 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1il3vze/ollama_not_supporting_mac_book_pro_with_radeon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1il3vze/ollama_not_supporting_mac_book_pro_with_radeon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1il3vze/ollama_not_supporting_mac_book_pro_with_radeon/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T01:59:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikqjaj</id>
    <title>PlanExe: breakdown a description into a detailed plan, WBS, SWOT.</title>
    <updated>2025-02-08T16:03:42+00:00</updated>
    <author>
      <name>/u/neoneye2</name>
      <uri>https://old.reddit.com/user/neoneye2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ikqjaj/planexe_breakdown_a_description_into_a_detailed/"&gt; &lt;img alt="PlanExe: breakdown a description into a detailed plan, WBS, SWOT." src="https://preview.redd.it/ho8hbemkuxhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e47f8681c23fcdbbae6a0196536c121bbab3aa24" title="PlanExe: breakdown a description into a detailed plan, WBS, SWOT." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neoneye2"&gt; /u/neoneye2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ho8hbemkuxhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikqjaj/planexe_breakdown_a_description_into_a_detailed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ikqjaj/planexe_breakdown_a_description_into_a_detailed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T16:03:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1il3mpl</id>
    <title>70 Page PDF Refuses to Be Processed via Ollama CLI</title>
    <updated>2025-02-09T01:45:40+00:00</updated>
    <author>
      <name>/u/immediate_a982</name>
      <uri>https://old.reddit.com/user/immediate_a982</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cmd: Ollama run codestral “summarize: $(cat file1.txt)”&lt;/p&gt; &lt;p&gt;Error: arguments too long.&lt;/p&gt; &lt;p&gt;To fix I had to trim the file to 2000 lines from 3000 lines. &lt;/p&gt; &lt;p&gt;Anyone else have similar issues Note: the pdf2text (not noted) converted the PDF to text &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/immediate_a982"&gt; /u/immediate_a982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1il3mpl/70_page_pdf_refuses_to_be_processed_via_ollama_cli/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1il3mpl/70_page_pdf_refuses_to_be_processed_via_ollama_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1il3mpl/70_page_pdf_refuses_to_be_processed_via_ollama_cli/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T01:45:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilfvmk</id>
    <title>Run DeepSeek r1 distilled locally in Browser (Docker + Ollama + OpenWebUI)</title>
    <updated>2025-02-09T14:29:12+00:00</updated>
    <author>
      <name>/u/Kind-Industry-609</name>
      <uri>https://old.reddit.com/user/Kind-Industry-609</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ilfvmk/run_deepseek_r1_distilled_locally_in_browser/"&gt; &lt;img alt="Run DeepSeek r1 distilled locally in Browser (Docker + Ollama + OpenWebUI)" src="https://external-preview.redd.it/M1bzwemFNGCEHrSoJ4-2CAfycNK_REX_yKjsmC7WWEA.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a41b0c1acdb2dfa28a208791360a42932290ea2" title="Run DeepSeek r1 distilled locally in Browser (Docker + Ollama + OpenWebUI)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kind-Industry-609"&gt; /u/Kind-Industry-609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/7tkstNuCt8I?si=2Bs3Rx6thJDDO4af"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilfvmk/run_deepseek_r1_distilled_locally_in_browser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilfvmk/run_deepseek_r1_distilled_locally_in_browser/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T14:29:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikrucz</id>
    <title>npcsh: the agentic AI toolkit for the AI developer</title>
    <updated>2025-02-08T16:59:35+00:00</updated>
    <author>
      <name>/u/BidWestern1056</name>
      <uri>https://old.reddit.com/user/BidWestern1056</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ikrucz/npcsh_the_agentic_ai_toolkit_for_the_ai_developer/"&gt; &lt;img alt="npcsh: the agentic AI toolkit for the AI developer" src="https://external-preview.redd.it/kwNGmsvOXAXdFB9uHXepZLPzAAh-6zpds0LqUQTdBwA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=469f42b2005573f93976bc16d13a959330d1d998" title="npcsh: the agentic AI toolkit for the AI developer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidWestern1056"&gt; /u/BidWestern1056 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/cagostino/npcsh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikrucz/npcsh_the_agentic_ai_toolkit_for_the_ai_developer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ikrucz/npcsh_the_agentic_ai_toolkit_for_the_ai_developer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T16:59:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1il5d5q</id>
    <title>new 8 card AMD Instinct Mi50 Server Build incoming</title>
    <updated>2025-02-09T03:16:54+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalAIServers/comments/1il5cde/new_8_card_amd_instinct_mi50_server_build_incoming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1il5d5q/new_8_card_amd_instinct_mi50_server_build_incoming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1il5d5q/new_8_card_amd_instinct_mi50_server_build_incoming/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T03:16:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikwuf2</id>
    <title>deploy locally</title>
    <updated>2025-02-08T20:30:32+00:00</updated>
    <author>
      <name>/u/tech215</name>
      <uri>https://old.reddit.com/user/tech215</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ikwuf2/deploy_locally/"&gt; &lt;img alt="deploy locally" src="https://preview.redd.it/85gh6umh6zhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac7a45a49ec9fc3277e54e018226b22c4c95d375" title="deploy locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tech215"&gt; /u/tech215 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/85gh6umh6zhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikwuf2/deploy_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ikwuf2/deploy_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T20:30:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikjn89</id>
    <title>For those wondering about instruct models vs text models. The difference is huge!</title>
    <updated>2025-02-08T09:31:53+00:00</updated>
    <author>
      <name>/u/hn-mc</name>
      <uri>https://old.reddit.com/user/hn-mc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've heard some say it's not too important whether you use instruct or text model. I asked many questions about this and all answers were sort of confusing. So I decided to test it for myself. And, oh boy, the difference is huge. Basically instruct models behave like you'd expect an AI assistant to behave. Text models, on the other hand, literally try to continue what you were writing, literally predicting next word.&lt;/p&gt; &lt;p&gt;So in my case, it understood my prompt to be the beginning of a dialogue, and what it did was simulate the rest of the dialogue, in rather incoherent way. Anyway, here's how it went:&lt;/p&gt; &lt;p&gt;I used the same prompt which is: &lt;strong&gt;Hello! Do you know of some psychological experiments that I could do on myself?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I tested it on the same model, of the same size, with the same quantization, the only difference being instruct vs text.&lt;/p&gt; &lt;p&gt;So here's how &lt;strong&gt;llama3.2:3b-instruct-q8_0 replied&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;em&gt;&amp;quot;While it's essential to consult with a mental health professional before conducting any experiment, especially if&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;you have any underlying conditions or concerns. That being said, here are some simple and relatively safe&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;self-experiments you can try:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;1. \&lt;/em&gt;*The Forer Effect**: Have someone describe you in vague terms, saying things like &amp;quot;You're imaginative, sensitive, and enjoy exploring new ideas.&amp;quot; Then, ask yourself how many of those traits resonate with you. This experiment demonstrates the power of confirmation bias.*&lt;/p&gt; &lt;p&gt;&lt;em&gt;2. \&lt;/em&gt;*The Baader-Meinhof Phenomenon**: Start noticing a specific word or concept (e.g., &amp;quot;sustainable living&amp;quot;) in your daily life. Write it down whenever you see it, and track its frequency over time. This experiment explores how our brains become attuned to patterns and concepts that interest us.*&lt;/p&gt; &lt;p&gt;&lt;em&gt;3. \&lt;/em&gt;*The Stroop Task**: Look at a series of words on the screen, such as &amp;quot;red,&amp;quot; &amp;quot;blue,&amp;quot; or &amp;quot;green.&amp;quot; Then, quickly try to name the color each word is associated with (e.g., &amp;quot;red&amp;quot; would be answered with &amp;quot;red&amp;quot;). This experiment assesses how our brains process conflicting information.*&lt;/p&gt; &lt;p&gt;&lt;em&gt;4. \&lt;/em&gt;*The Zeigarnik Effect**: Write a story with an unresolved plot point (e.g., &amp;quot;As I walked into the room, I saw my best friend standing alone...&amp;quot;). Then, try to remember where you left off. Later, write down what happened next. This experiment shows how our brains tend to retain information that's incomplete or unfinished.*&lt;/p&gt; &lt;p&gt;&lt;em&gt;5. \&lt;/em&gt;*The Placebo Effect**: Take a pain reliever without knowing if it's actually effective (a placebo). Rate the level of discomfort or pain before and after taking the &amp;quot;medication.&amp;quot; If you experience a reduction in symptoms, this might demonstrate the power of our brains' expectations on our physical well-being.*&lt;/p&gt; &lt;p&gt;&lt;em&gt;Remember to:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;\&lt;/em&gt; Only conduct these experiments if you're feeling physically and mentally well.*&lt;/p&gt; &lt;p&gt;&lt;em&gt;\&lt;/em&gt; Be aware of any underlying conditions that may be affected by these self-experiments (e.g., anxiety, depression).*&lt;/p&gt; &lt;p&gt;&lt;em&gt;\&lt;/em&gt; Don't push yourself too hard or try to force specific outcomes.*&lt;/p&gt; &lt;p&gt;&lt;em&gt;\&lt;/em&gt; Consider discussing your results with a mental health professional for interpretation and support. Keep in mind that while self-experiments can be interesting and insightful, they shouldn't replace professional psychological assessments or treatment.&amp;quot;*&lt;/p&gt; &lt;p&gt;And this is how &lt;strong&gt;llama3.2:3b-text-q8_0&lt;/strong&gt; replied:&lt;/p&gt; &lt;p&gt;&lt;em&gt;I'm just curious, but i think it would be interesting to me. I don't want to hurt myself though.. so please give good suggestions. Thanks!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Have you ever been in a state of flow, where time stops and all of your senses are completely present? If not, try doing the following exercises.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;This is the first exercise that I recommend because this will allow you to discover more about yourself.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;1. Close your eyes and think of something that makes you happy. Try thinking of it as intensely as possible (don't worry if the thought isn't positive or doesn't make sense). Do this for 2-3 minutes.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;2. Now, try doing a free-form association exercise. Think of something that's bothering you. As soon as an image pops up in your head, write down what it is immediately, without thinking about it too much (like the image I wrote above).&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;What you just did was experience a flow state. A flow state is like a trance-like state where time seems to stop and all of your senses are present.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;I also recommend reading Flow: The Psychology of Optimal Experience by Mihaly Csikszentmihalyi. It's an easy read that will allow you to understand why people get into flow states and how to achieve them more often (I've already finished the book, and I'm sure you'll love it).&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Let me know if this helped! If not, please let me know in the comments.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Thank you for your post. I find it very interesting because I am currently doing a project on flow theory and have been researching into this area of psychology for some time now. I can tell that you are very interested in flow theories, as well as other areas of psychology. It's nice to see someone else sharing their knowledge and interests!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;I'm glad you liked the article! I really enjoy flow experiences. It makes me feel like nothing else matters for a little while.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;I am doing research on positive psychology too - my major is psychology and I was wondering if you could recommend some online resources that would be helpful to me as I prepare for an exam? Also, do you know of any famous psychologists who have contributed in the field?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Hi there! It's nice to meet you. First off, it's great that you're interested in positive psychology!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;I don't know much about positive psychology (apart from what is presented in my university's curriculum), but I will definitely try to find some online resources for you.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;You can check out this Wikipedia article on famous psychologists who contributed in the field. I'm sure it contains a lot of names that you may not have heard before, so give it a look!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Also, here are some more useful links (this one focuses on famous positive psychologist).&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;I hope these help! Let me know if they don't work for you. Thanks again.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;You're welcome! Feel free to leave any questions you might have about positive psychology in the comments section below. I'll do my best to answer them!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;P.S. - If anyone else is interested, please feel free to share this article with your friends and/or family members!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;I really like this page and the info it contains. It's very helpful! Thanks for posting this.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;You're welcome! Feel free to leave any other comments or questions you may have in the comment section below. I'll do my best to respond as quickly as possible!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;P.S. - If anyone else is interested, feel free to share this page with your friends and/or family members.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Wow, thank you for sharing this great article on flow psychology! This site has some interesting information about the effects of flow experiences on one's life. It's been a while since I've read anything from this perspective so it was nice to come across something like that again. Keep up the good work!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Hi there! I'm glad that you found my article helpful.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Thanks for sharing your thoughts as well!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;P.S. &lt;/p&gt; &lt;p&gt;Another thing that I noticed with text models is that they sometimes just start with prompt, but then they take their thoughts in completely unpredictable directions, and they can keep going on, and on, generating extremely long replies. So when I asked &lt;strong&gt;llama3.2:3b-text-q8_0&lt;/strong&gt; &amp;quot;What is the solution for the war in Ukraine&amp;quot;, &lt;strong&gt;it generated 2124 words long answer!&lt;/strong&gt; And only first paragraph had something to do with war. I'll copy just a part of it here so that you can see:&lt;/p&gt; &lt;p&gt;&lt;em&gt;For that, we need to ask ourselves a more fundamental question. Why do people hate each other so much? Many of our political leaders have been saying “It’s not the war; it’s the weapons” as if weapons are just something we use and get rid of when they’re no longer useful. We also see this with the United States’ response to the invasion of Ukraine by Russia – many Americans, including politicians, say that President Biden has done a great job in supporting our allies. But what is really going on here? Why do people hate each other so much?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Let’s look at how hatred works. If you think about it, all humans have emotions: anger, fear, love and happiness. But we also see these feelings expressed as being “good” or “bad.” This means that in order to be able to love someone else (or something), we need first of all to have a negative feeling towards them – either hate or fear.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Now let’s take another look at hatred – what it is and how it works. Hatred is actually more complex than anger or fear; indeed, there are many different types of hatred! For example:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;1) You may feel that someone has hurt you in some way (e.g., by not being fair to others), and then you want revenge.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;2) You may see yourself as better than another person because of how successful they’ve been at achieving something – e.g., making money or having a good career – and therefore think negatively about them when faced with challenges themselves; this kind of hatred can lead people into thinking things like: “I know I’m doing the right thing, but he’s getting away with it!”&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;3) You may be jealous because someone else seems to have more than you do (e.g., money or status), and so want them to suffer as a result – e.g., taking revenge on them by stealing their possessions!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;It’s interesting that there are actually two different ways in which people express hatred: through violence and through words. The former is what we call physical aggression, while the latter is verbal abuse.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;As an example of how this works in practice, imagine if someone had been mistreated by their parents as a child – e.g., being physically abused or not allowed to do anything they wanted because it was “wrong”...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I just copied the first page out of 7. It just kept on discussing different kinds of love and hate, for seven pages of text in MS Word.&lt;/p&gt; &lt;p&gt;In a way, text models are more like humans, they have their own will, they don't obey the prompt, they talk about what they want to talk about, and they are less politically correct.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hn-mc"&gt; /u/hn-mc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikjn89/for_those_wondering_about_instruct_models_vs_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikjn89/for_those_wondering_about_instruct_models_vs_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ikjn89/for_those_wondering_about_instruct_models_vs_text/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T09:31:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1il58sc</id>
    <title>mistral ai with memory</title>
    <updated>2025-02-09T03:10:14+00:00</updated>
    <author>
      <name>/u/Other-Maintenance764</name>
      <uri>https://old.reddit.com/user/Other-Maintenance764</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi how could i run mistral locally with ollama and make it have memory so it learns from what i say&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Other-Maintenance764"&gt; /u/Other-Maintenance764 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1il58sc/mistral_ai_with_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1il58sc/mistral_ai_with_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1il58sc/mistral_ai_with_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T03:10:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilclmy</id>
    <title>Using Ollama APIs to generate responses and much more [Part 3]</title>
    <updated>2025-02-09T11:17:18+00:00</updated>
    <author>
      <name>/u/geshan</name>
      <uri>https://old.reddit.com/user/geshan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ilclmy/using_ollama_apis_to_generate_responses_and_much/"&gt; &lt;img alt="Using Ollama APIs to generate responses and much more [Part 3]" src="https://external-preview.redd.it/J6e2dqdntRoo_I2LQX3M8W1CAgH3Wbhd3rvDGSB-IMo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e053b09089c4f2ceb880a653da9edb82cb3c99a" title="Using Ollama APIs to generate responses and much more [Part 3]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geshan"&gt; /u/geshan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://geshan.com.np/blog/2025/02/ollama-api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilclmy/using_ollama_apis_to_generate_responses_and_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilclmy/using_ollama_apis_to_generate_responses_and_much/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T11:17:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilakbj</id>
    <title>"Structured Output" with Ollama and LangChainJS [the return]</title>
    <updated>2025-02-09T08:49:48+00:00</updated>
    <author>
      <name>/u/Inevitable-Judge2642</name>
      <uri>https://old.reddit.com/user/Inevitable-Judge2642</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ilakbj/structured_output_with_ollama_and_langchainjs_the/"&gt; &lt;img alt="&amp;quot;Structured Output&amp;quot; with Ollama and LangChainJS [the return]" src="https://external-preview.redd.it/v8th63x1NCCLZjxABxP5OPU2v457BOEOTLqo9VEhnk4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac7bd7b01c348ab2f4a3654ac57684204869a258" title="&amp;quot;Structured Output&amp;quot; with Ollama and LangChainJS [the return]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Judge2642"&gt; /u/Inevitable-Judge2642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://k33g.hashnode.dev/structured-output-with-ollama-and-langchainjs-the-return"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilakbj/structured_output_with_ollama_and_langchainjs_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilakbj/structured_output_with_ollama_and_langchainjs_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T08:49:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ildwuu</id>
    <title>Ollama Integration Showcase: Local Model-Powered Writing Assistant - Feedback Welcome!</title>
    <updated>2025-02-09T12:43:33+00:00</updated>
    <author>
      <name>/u/Federal_Wrongdoer_44</name>
      <uri>https://old.reddit.com/user/Federal_Wrongdoer_44</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal_Wrongdoer_44"&gt; /u/Federal_Wrongdoer_44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/SideProject/comments/1ildqa3/what_my_break_built_now_with_ollamaopenrouter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ildwuu/ollama_integration_showcase_local_modelpowered/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ildwuu/ollama_integration_showcase_local_modelpowered/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T12:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikxer9</id>
    <title>Supercharge Your Document Processing: DataBridge Rules + DeepSeek = Magic!</title>
    <updated>2025-02-08T20:54:58+00:00</updated>
    <author>
      <name>/u/yes-no-maybe_idk</name>
      <uri>https://old.reddit.com/user/yes-no-maybe_idk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey r/ollama! I'm excited to present DataBridge's rules system - a powerful way to process documents &lt;strong&gt;exactly&lt;/strong&gt; how you want, completely locally!&lt;/p&gt; &lt;h1&gt;What's Cool About It?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;100% Local Processing&lt;/strong&gt;: Works beautifully with DeepSeek/Llama2 through Ollama&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Document Processing&lt;/strong&gt;: Extract metadata and transform content automatically&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Super Simple Setup&lt;/strong&gt;: Just modify &lt;code&gt;databridge.toml&lt;/code&gt; to use your preferred model:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[rules] provider = &amp;quot;ollama&amp;quot; model_name = &amp;quot;deepseek-coder&amp;quot; # or any other model you prefer &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Builtin Rules:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Metadata Rules&lt;/strong&gt;: Automatically extract structured data&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;metadata_rule = MetadataExtractionRule(schema={ &amp;quot;title&amp;quot;: str, &amp;quot;category&amp;quot;: str, &amp;quot;priority&amp;quot;: str }) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;2. &lt;strong&gt;Natural Language Rules&lt;/strong&gt;: Transform content using plain English&lt;/p&gt; &lt;pre&gt;&lt;code&gt;clean_rule = NaturalLanguageRule( prompt=&amp;quot;Remove PII and standardize formatting&amp;quot; ) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Totally Customizable!&lt;/h1&gt; &lt;p&gt;You can create your own rules! Here's a quick example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;class KeywordRule(BaseRule): &amp;quot;&amp;quot;&amp;quot;Extract keywords from documents&amp;quot;&amp;quot;&amp;quot; async def apply(self, content: str): # Your custom logic here return {&amp;quot;keywords&amp;quot;: extracted_keywords}, content &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Real-World Use Cases:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;PII removal&lt;/li&gt; &lt;li&gt;Content classification&lt;/li&gt; &lt;li&gt;Auto-summarization&lt;/li&gt; &lt;li&gt;Format standardization&lt;/li&gt; &lt;li&gt;Custom metadata extraction&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All this running on your hardware, your rules, your way. Works amazingly well with smaller models! 🎉&lt;/p&gt; &lt;p&gt;Let me know what custom rules you'd like to see implemented or if you have any questions!&lt;/p&gt; &lt;p&gt;Checkout &lt;a href="https://github.com/databridge-org/databridge-core"&gt;DatBridge&lt;/a&gt; and our &lt;a href="https://databridge.gitbook.io/databridge-docs"&gt;docs&lt;/a&gt;. Leave a ⭐ if you like it, feel free to submit a PR for your rules :).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yes-no-maybe_idk"&gt; /u/yes-no-maybe_idk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikxer9/supercharge_your_document_processing_databridge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ikxer9/supercharge_your_document_processing_databridge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ikxer9/supercharge_your_document_processing_databridge/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T20:54:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1iktf49</id>
    <title>IBM granite</title>
    <updated>2025-02-08T18:05:06+00:00</updated>
    <author>
      <name>/u/tech215</name>
      <uri>https://old.reddit.com/user/tech215</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iktf49/ibm_granite/"&gt; &lt;img alt="IBM granite" src="https://b.thumbs.redditmedia.com/fhgI8M53CsSJ8duYe-rKC09HGpOIDQY3UuCB43o1wZk.jpg" title="IBM granite" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For a 1b model it's pretty good 🌝(created by IBM)&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/granite3.1-moe:1b-instruct-fp16"&gt;https://ollama.com/library/granite3.1-moe:1b-instruct-fp16&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ibm info page &lt;a href="https://www.ibm.com/granite/docs/models/granite/"&gt;https://www.ibm.com/granite/docs/models/granite/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tech215"&gt; /u/tech215 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iktf49"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iktf49/ibm_granite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iktf49/ibm_granite/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T18:05:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1il7rwy</id>
    <title>ParLlama v0.3.15 released. Now supports Ollama, OpenAI, GoogleAI, Anthropic, Groq, xAI, Bedrock, OpenRouter</title>
    <updated>2025-02-09T05:36:51+00:00</updated>
    <author>
      <name>/u/probello</name>
      <uri>https://old.reddit.com/user/probello</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1il7rwy/parllama_v0315_released_now_supports_ollama/"&gt; &lt;img alt="ParLlama v0.3.15 released. Now supports Ollama, OpenAI, GoogleAI, Anthropic, Groq, xAI, Bedrock, OpenRouter" src="https://external-preview.redd.it/g5LJqgM5VjbIVUFBSAgJzLXbf1SuAHgtPCr33OTJjkY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6abaac865026b7320f41dbbdbdda11366edf04c" title="ParLlama v0.3.15 released. Now supports Ollama, OpenAI, GoogleAI, Anthropic, Groq, xAI, Bedrock, OpenRouter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/7n0k1sywv1ie1.png?width=1400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b7e8775111b28eb3b6af3cb35b15027ce485448"&gt;https://preview.redd.it/7n0k1sywv1ie1.png?width=1400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b7e8775111b28eb3b6af3cb35b15027ce485448&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What My project Does:&lt;/h1&gt; &lt;p&gt;PAR LLAMA is a powerful TUI (Text User Interface) written in Python and designed for easy management and use of Ollama and Large Language Models as well as interfacing with online Providers such as Ollama, OpenAI, GoogleAI, Anthropic, Bedrock, Groq, xAI, OpenRouter&lt;/p&gt; &lt;h1&gt;Whats New:&lt;/h1&gt; &lt;h1&gt;v0.3.15&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Added copy button to the fence blocks in chat markdown for easy code copy.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;v0.3.14&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Fix crash caused some models having some missing fields in model file&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;v0.3.13&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Handle clipboard errors&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;v0.3.12&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Fixed bug where changing providers that have custom urls would break other providers&lt;/li&gt; &lt;li&gt;Fixed bug where changing Ollama base url would cause connection timed out&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Easy-to-use interface for interacting with Ollama and cloud hosted LLMs&lt;/li&gt; &lt;li&gt;Dark and Light mode support, plus custom themes&lt;/li&gt; &lt;li&gt;Flexible installation options (uv, pipx, pip or dev mode)&lt;/li&gt; &lt;li&gt;Chat session management&lt;/li&gt; &lt;li&gt;Custom prompt library support&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;GitHub and PyPI&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;PAR LLAMA is under active development and getting new features all the time.&lt;/li&gt; &lt;li&gt;Check out the project on GitHub or for full documentation, installation instructions, and to contribute: &lt;a href="https://github.com/paulrobello/parllama"&gt;https://github.com/paulrobello/parllama&lt;/a&gt;&lt;/li&gt; &lt;li&gt;PyPI &lt;a href="https://pypi.org/project/parllama/"&gt;https://pypi.org/project/parllama/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Comparison:&lt;/h1&gt; &lt;p&gt;I have seem many command line and web applications for interacting with LLM's but have not found any TUI related applications&lt;/p&gt; &lt;h1&gt;Target Audience&lt;/h1&gt; &lt;p&gt;Anybody that loves or wants to love terminal interactions and LLM's&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/probello"&gt; /u/probello &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1il7rwy/parllama_v0315_released_now_supports_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1il7rwy/parllama_v0315_released_now_supports_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1il7rwy/parllama_v0315_released_now_supports_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T05:36:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilhdi0</id>
    <title>Pdf, images in Local Model</title>
    <updated>2025-02-09T15:37:54+00:00</updated>
    <author>
      <name>/u/Visual_Locksmith_997</name>
      <uri>https://old.reddit.com/user/Visual_Locksmith_997</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any way to upload pdf, images in deepseek r1 (8b ) local model. I run it using powershell /web-ui.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Visual_Locksmith_997"&gt; /u/Visual_Locksmith_997 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilhdi0/pdf_images_in_local_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilhdi0/pdf_images_in_local_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilhdi0/pdf_images_in_local_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T15:37:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilhwh2</id>
    <title>Is pulling models not working at the moment?</title>
    <updated>2025-02-09T16:01:06+00:00</updated>
    <author>
      <name>/u/Nabukadnezar</name>
      <uri>https://old.reddit.com/user/Nabukadnezar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I only get this: &lt;code&gt; ... pulling manifest pulling 2bada8a74506... 0% ▕ ▏ 0 B/4.7 GB Error: max retries exceeded: ... &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nabukadnezar"&gt; /u/Nabukadnezar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilhwh2/is_pulling_models_not_working_at_the_moment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilhwh2/is_pulling_models_not_working_at_the_moment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilhwh2/is_pulling_models_not_working_at_the_moment/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T16:01:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilg690</id>
    <title>Start chat with message from model.</title>
    <updated>2025-02-09T14:43:23+00:00</updated>
    <author>
      <name>/u/Velskadi</name>
      <uri>https://old.reddit.com/user/Velskadi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm having a hard time finding any info on this, so I am hoping someone here might have some guidance. I would like to start a chat with a model using ollama start &amp;lt;MODEL NAME&amp;gt;, and have the model start the conversation with a response before I give it a prompt.&lt;/p&gt; &lt;p&gt;Preferably I'd like this message to be static, something like &amp;quot;I am your workshop assistant. Please give me these pieces of information so I can assist. etc. etc&amp;quot;&lt;/p&gt; &lt;p&gt;Is this possible using Ollama? If so, would it be possible to do this in Openwebui as well? Any advice would be appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Velskadi"&gt; /u/Velskadi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilg690/start_chat_with_message_from_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilg690/start_chat_with_message_from_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilg690/start_chat_with_message_from_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T14:43:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilfj2d</id>
    <title>hardware question</title>
    <updated>2025-02-09T14:11:48+00:00</updated>
    <author>
      <name>/u/quantrpeter</name>
      <uri>https://old.reddit.com/user/quantrpeter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Jetson Orin Nano Super = 1024 CUDA&lt;br /&gt;&lt;/li&gt; &lt;li&gt;2070 = 2560 CUDA&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Telsa K80 24GB = 4992 CUDA&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For second hand price, K80 &amp;lt; 2070 &amp;lt; Jetson. For real ollama performance, isn't it more cuda core must win? If so, Jetson is not valuable.&lt;/p&gt; &lt;p&gt;thanks&lt;br /&gt; Peter&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantrpeter"&gt; /u/quantrpeter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilfj2d/hardware_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ilfj2d/hardware_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ilfj2d/hardware_question/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T14:11:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ildvgc</id>
    <title>Local TTS (text-to-speech) AI model with a human voice and file output?</title>
    <updated>2025-02-09T12:41:08+00:00</updated>
    <author>
      <name>/u/simo41993</name>
      <uri>https://old.reddit.com/user/simo41993</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don't know if this is the right place to ask, but... i was looking for a text to speech alternative to the quite expensive online ones i was looking for recently.&lt;/p&gt; &lt;p&gt;I'm partially blind and it would be of great help to have a recorded and narrated version of some technical e-books i own.&lt;/p&gt; &lt;p&gt;As i was saying, models like Elevenlabs and similar are really quite good but absolutely too expensive in terms of €/time for what i need to do (and the books are quite long too).&lt;/p&gt; &lt;p&gt;I was wondering, because of that, &lt;strong&gt;if there was a good (the normal TTS is quite abismal and distravting) alternative to run locally that can transpose the book in audio&lt;/strong&gt; and let me save a mp3 or similar file for later use.&lt;/p&gt; &lt;p&gt;I have to say, also, that i'm not a programmer whatsoever, so i should be able to follow simple instructions but, sadly, nothing more. so... a ready to use solution would be quite nice (or a detailed, like i'm a 3yo, set of instructions).&lt;/p&gt; &lt;p&gt;i'm using ollama + docker and free open web-ui for playing (literally) with some offline models and also thinking about using something compatible with this already running system... hopefully, possibly?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Another complication it's that i'm italian, so... the probably unexisting model should be capable to use italian language too...&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The following are my PC specs, if needed:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Processor: intel i7 13700k&lt;/li&gt; &lt;li&gt;MB: Asus ROG Z790-H&lt;/li&gt; &lt;li&gt;Ram: 64gb Corsair 5600 MT/S&lt;/li&gt; &lt;li&gt;Gpu: RTX 4070TI 12gb - MSI Ventus 3X&lt;/li&gt; &lt;li&gt;Storage: Samsung 970EVO NVME SSD + others&lt;/li&gt; &lt;li&gt;Windows 11 PRO 64bit&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Sorry for the long post and thank you for any help :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simo41993"&gt; /u/simo41993 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ildvgc/local_tts_texttospeech_ai_model_with_a_human/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ildvgc/local_tts_texttospeech_ai_model_with_a_human/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ildvgc/local_tts_texttospeech_ai_model_with_a_human/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T12:41:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1iljej9</id>
    <title>Model occasionally continues to use CPU despite having finished responding.</title>
    <updated>2025-02-09T17:04:21+00:00</updated>
    <author>
      <name>/u/Velskadi</name>
      <uri>https://old.reddit.com/user/Velskadi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty much the title. I am running the magnum-v4-9b model through Open-webui, using my CPU (Ryzen 9 5900X). The model runs well, but brings my CPU usage to about 80-90% while it is generating a response. After it finishes it will sometimes keep my CPU usage pegged at these levels.&lt;/p&gt; &lt;p&gt;The last time this happened I tried stopping it with &lt;code&gt;ollama stop &amp;lt;model name&amp;gt;&lt;/code&gt; but it was then stuck in the &amp;quot;Stopping&amp;quot; state, and my CPU useage stayed high. I had to restart the Ollama service to fix this issue.&lt;/p&gt; &lt;p&gt;I may have seen this issue with other models as well but not realized it, as it was only today that I started monitoring the CPU usage. Any advice is appreciated!&lt;/p&gt; &lt;p&gt;-SPECS-&lt;br /&gt; CPU: Ryzen 9 5900X&lt;br /&gt; GPU (Unused): AMD Radeon 6700 XT&lt;br /&gt; RAM: 33GB DDR4&lt;br /&gt; OS: Arch Linux&lt;/p&gt; &lt;p&gt;EDIT: I'd like to note that all I had prompted when this happened was &amp;quot;This is a test. Please respond with Hello&amp;quot;, which it did.&lt;/p&gt; &lt;p&gt;While it is stuck like this the model takes a long time to start responding to any new prompts, and it generates it much slower. The CPU stays almost maxed out even after these subsequent prompts as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Velskadi"&gt; /u/Velskadi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iljej9/model_occasionally_continues_to_use_cpu_despite/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iljej9/model_occasionally_continues_to_use_cpu_despite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iljej9/model_occasionally_continues_to_use_cpu_despite/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-09T17:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1il0zea</id>
    <title>Just released an open-source Mac client for Ollama built with Swift/SwiftUI</title>
    <updated>2025-02-08T23:34:21+00:00</updated>
    <author>
      <name>/u/billythepark</name>
      <uri>https://old.reddit.com/user/billythepark</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently created a new Mac app using Swift. Last year, I released an open-source iPhone client for Ollama (a program for running LLMs locally) called MyOllama using Flutter. I planned to make a Mac version too, but when I tried with Flutter, the design didn't feel very Mac-native, so I put it aside.&lt;/p&gt; &lt;p&gt;Early this year, I decided to rebuild it from scratch using Swift/SwiftUI. This app lets you install and chat with LLMs like Deepseek on your Mac using Ollama. Features include:&lt;/p&gt; &lt;p&gt;- Contextual conversations&lt;/p&gt; &lt;p&gt;- Save and search chat history&lt;/p&gt; &lt;p&gt;- Customize system prompts&lt;/p&gt; &lt;p&gt;- And more...&lt;/p&gt; &lt;p&gt;It's completely open-source! Check out the code here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/bipark/mac_ollama_client"&gt;https://github.com/bipark/mac_ollama_client&lt;/a&gt;&lt;/p&gt; &lt;p&gt;#Ollama #LLMHippo&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/billythepark"&gt; /u/billythepark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1il0zea/just_released_an_opensource_mac_client_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1il0zea/just_released_an_opensource_mac_client_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1il0zea/just_released_an_opensource_mac_client_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-08T23:34:21+00:00</published>
  </entry>
</feed>
