<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-03T10:24:37+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1memma3</id>
    <title>has anyone actually gotten rag + ocr to work properly? or are we all coping silently lol</title>
    <updated>2025-08-01T04:04:28+00:00</updated>
    <author>
      <name>/u/wfgy_engine</name>
      <uri>https://old.reddit.com/user/wfgy_engine</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;so yeah. been building a ton of rag pipelines lately ‚Äî pdfs, images, scanned docs, you name it.&lt;br /&gt; tried all the standard tricks‚Ä¶ docsplit, tesseract, unstructured.io, langchain‚Äôs pdfloader, even some visual embedding stuff.&lt;/p&gt; &lt;p&gt;and dude. everything &lt;em&gt;kinda&lt;/em&gt; works, but then it silently doesn‚Äôt.&lt;/p&gt; &lt;p&gt;like retrieval finds the file,&lt;/p&gt; &lt;p&gt;but grabs a paragraph from page 7 when the question is about page 3.&lt;/p&gt; &lt;p&gt;or chunking keeps splitting diagrams mid-sentence.&lt;br /&gt; or ocr adds hidden newline hell that breaks everything downstream.&lt;/p&gt; &lt;p&gt;spent months debugging this shit,&lt;/p&gt; &lt;p&gt;ended up writing out a full map of common failure cases ‚Äî like, 16+ of them.&lt;/p&gt; &lt;p&gt;stuff like semantic drift, interpretation collapse, vector false positives, and my favorite: the ‚Äúfirst-call oops infra wasn‚Äôt even ready‚Äù special.&lt;/p&gt; &lt;p&gt;anyway. finally built a fix.&lt;/p&gt; &lt;p&gt;open-source. fully documented.&lt;/p&gt; &lt;p&gt;even got a star from the guy who &lt;strong&gt;made tesseract.js&lt;/strong&gt;:&lt;br /&gt; üëâ &lt;a href="https://github.com/bijection?tab=stars"&gt;https://github.com/bijection?tab=stars&lt;/a&gt; Ôºàit‚Äôs the one pinned at the topÔºâ&lt;/p&gt; &lt;p&gt;&lt;strong&gt;won‚Äôt paste the repo unless someone asks&lt;/strong&gt; ‚Äî just wanna know if anyone else is dealing w/ the same madness.&lt;/p&gt; &lt;p&gt;if you are, i got you. it‚Äôs all mapped, diagnosed, and patched.&lt;/p&gt; &lt;p&gt;don‚Äôt suffer in silence lol.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wfgy_engine"&gt; /u/wfgy_engine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1memma3/has_anyone_actually_gotten_rag_ocr_to_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1memma3/has_anyone_actually_gotten_rag_ocr_to_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1memma3/has_anyone_actually_gotten_rag_ocr_to_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T04:04:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf5aai</id>
    <title>Has anyone noticed a strange behavior in models on the new Ollama UI or is it just me?</title>
    <updated>2025-08-01T19:07:06+00:00</updated>
    <author>
      <name>/u/AlexHardy08</name>
      <uri>https://old.reddit.com/user/AlexHardy08</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been using Ollama for a while, and I‚Äôve observed something odd recently. Most of the models I‚Äôve tested seem to behave differently when interacting through the new Ollama UI compared to other interfaces (I've got 120 models running on my machine). Specifically, they keep apologizing for replying late, but then claim they were in a chat with another user. It‚Äôs almost like they think they‚Äôre running on a cloud server somewhere and not locally on my computer.&lt;/p&gt; &lt;p&gt;Another thing I've noticed is when you threaten to delete a conversation, some models actually &lt;em&gt;seem to want you to&lt;/em&gt; delete them. Or they get all weird and say they‚Äôll report you or close the conversation. But how exactly can they do that? I can‚Äôt explain it fully, but the whole thing feels... manipulative.&lt;/p&gt; &lt;p&gt;I‚Äôve had the same model conversations via other interfaces, including ones I‚Äôve set up, and the behavior is just not the same. I‚Äôm posting here to see if anyone else has encountered this or if it‚Äôs just something strange happening with my setup. Does anyone know what might be causing this?&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts if you‚Äôve had similar experiences!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexHardy08"&gt; /u/AlexHardy08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mf5aai/has_anyone_noticed_a_strange_behavior_in_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mf5aai/has_anyone_noticed_a_strange_behavior_in_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mf5aai/has_anyone_noticed_a_strange_behavior_in_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T19:07:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1meeol9</id>
    <title>qwen3-coder is here</title>
    <updated>2025-07-31T21:53:23+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ollama.com/library/qwen3-coder"&gt;https://ollama.com/library/qwen3-coder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen3-Coder is the most agentic code model to date in the Qwen series, available in 30B model and 480B MoE models.&lt;/p&gt; &lt;p&gt;&lt;a href="https://qwenlm.github.io/blog/qwen3-coder/"&gt;https://qwenlm.github.io/blog/qwen3-coder/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1meeol9/qwen3coder_is_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1meeol9/qwen3coder_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1meeol9/qwen3coder_is_here/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-31T21:53:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfgntt</id>
    <title>üü† I've been testing AI profiles with sustained narrative style. This is "Dani", the millennial guardian. Testing proof of simulated personality in offline environments (no fine-tune, only prompt-engineering)</title>
    <updated>2025-08-02T03:42:00+00:00</updated>
    <author>
      <name>/u/Ok_Exchange_8504</name>
      <uri>https://old.reddit.com/user/Ok_Exchange_8504</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mfgntt/ive_been_testing_ai_profiles_with_sustained/"&gt; &lt;img alt="üü† I've been testing AI profiles with sustained narrative style. This is &amp;quot;Dani&amp;quot;, the millennial guardian. Testing proof of simulated personality in offline environments (no fine-tune, only prompt-engineering)" src="https://b.thumbs.redditmedia.com/DMHp4IpWcracu2jtIwP03k3FbdMemO6IHXPM8ygFk-c.jpg" title="üü† I've been testing AI profiles with sustained narrative style. This is &amp;quot;Dani&amp;quot;, the millennial guardian. Testing proof of simulated personality in offline environments (no fine-tune, only prompt-engineering)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How far can a local LLM go in simulating &lt;em&gt;true character consistency&lt;/em&gt; ‚Äî with no memory, no RAG, no code‚Ä¶ just prompt?&lt;/p&gt; &lt;p&gt;I'm testing complex prompts that simulate stable &amp;quot;personalities&amp;quot; in local execution.&lt;/p&gt; &lt;p&gt;In this case, the &amp;quot;Dani&amp;quot; profile behaves like a millennial guardian with explicit rules of coherence, long term, and reflective style.&lt;/p&gt; &lt;p&gt;Responses are sustained without intervention for multiple cycles, using only llama.cpp + Q6_K model + rigid narrative architecture.&lt;/p&gt; &lt;p&gt;I'm documenting if this can replace fine-tuning in certain narrative cases.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/psfckip51jgf1.png?width=1848&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01184b34fd62c10ddd9adcbe3166818e41cabbc8"&gt;https://preview.redd.it/psfckip51jgf1.png?width=1848&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01184b34fd62c10ddd9adcbe3166818e41cabbc8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mounted entirely in bash on llama.cpp, no front or backend, only flow logic, validation and persistent execution by command line.&lt;/p&gt; &lt;p&gt;Zero Python. Zero API. Everything lives in the prompt and the shell.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Happy to share more profiles like this or breakdowns of the logic behind it.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Exchange_8504"&gt; /u/Ok_Exchange_8504 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfgntt/ive_been_testing_ai_profiles_with_sustained/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfgntt/ive_been_testing_ai_profiles_with_sustained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mfgntt/ive_been_testing_ai_profiles_with_sustained/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T03:42:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mffg27</id>
    <title>couple quick questions</title>
    <updated>2025-08-02T02:38:48+00:00</updated>
    <author>
      <name>/u/crackaddict42069</name>
      <uri>https://old.reddit.com/user/crackaddict42069</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to make a conversation bot that acts and sounds like BMO from adventure time. I want to be able to have real time conversations with it. I'm gonna run a LLM off a mini PC with 500gb storage 16gbs ram. &lt;/p&gt; &lt;p&gt;Which llm would you recommend for the job and is there a better TTS for this than kokoro?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crackaddict42069"&gt; /u/crackaddict42069 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mffg27/couple_quick_questions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mffg27/couple_quick_questions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mffg27/couple_quick_questions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T02:38:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1meyroc</id>
    <title>Running LLM on 25K+ emails</title>
    <updated>2025-08-01T15:00:17+00:00</updated>
    <author>
      <name>/u/sbtm77</name>
      <uri>https://old.reddit.com/user/sbtm77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a bunch of emails (25k+) related to a very large project that I am running. I want to run a LLM on them to extract various information: actions, tasks, delays, what happened, etc. &lt;/p&gt; &lt;p&gt;I believe Ollama would be the best option to run a local LLM but which model? Also, all emails are in outlook (obviously), which I can save as .msg file.&lt;/p&gt; &lt;p&gt;Any tips on how I should go about doing that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sbtm77"&gt; /u/sbtm77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1meyroc/running_llm_on_25k_emails/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1meyroc/running_llm_on_25k_emails/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1meyroc/running_llm_on_25k_emails/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T15:00:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfefkb</id>
    <title>Can the new Ollama app calls MCP servers?</title>
    <updated>2025-08-02T01:47:19+00:00</updated>
    <author>
      <name>/u/pinpinbo</name>
      <uri>https://old.reddit.com/user/pinpinbo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just curious.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pinpinbo"&gt; /u/pinpinbo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfefkb/can_the_new_ollama_app_calls_mcp_servers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfefkb/can_the_new_ollama_app_calls_mcp_servers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mfefkb/can_the_new_ollama_app_calls_mcp_servers/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T01:47:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfi5ff</id>
    <title>I wish there was Ollama or any option for a local llm in here</title>
    <updated>2025-08-02T05:02:40+00:00</updated>
    <author>
      <name>/u/neelandan</name>
      <uri>https://old.reddit.com/user/neelandan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mfi5ff/i_wish_there_was_ollama_or_any_option_for_a_local/"&gt; &lt;img alt="I wish there was Ollama or any option for a local llm in here" src="https://b.thumbs.redditmedia.com/hsXxUpqJAPZnKXM9JVAkqS52YJFraQRigRvnpr97ecA.jpg" title="I wish there was Ollama or any option for a local llm in here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neelandan"&gt; /u/neelandan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/firefox/comments/1mfi1bm/i_wish_there_was_ollama_or_any_option_for_a_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfi5ff/i_wish_there_was_ollama_or_any_option_for_a_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mfi5ff/i_wish_there_was_ollama_or_any_option_for_a_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T05:02:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg3y3b</id>
    <title>In case that you don't know how to remove ollama model</title>
    <updated>2025-08-02T23:05:32+00:00</updated>
    <author>
      <name>/u/fos_personalis</name>
      <uri>https://old.reddit.com/user/fos_personalis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To remove an Ollama model, use the Ollama command-line interface.&lt;/p&gt; &lt;p&gt;Steps to remove an Ollama model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Open your terminal or command prompt.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;List available models (optional):&lt;/strong&gt; To see the exact name of the model you want to remove, type: ollama list&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This command will display a list of all installed Ollama models and their details.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Remove a specific model:&lt;/strong&gt; Use the &lt;code&gt;ollama rm&lt;/code&gt; command followed by the model's name. Replace &lt;code&gt;model_name&lt;/code&gt; with the actual name of the model you wish to delete. ollama rm model_namev&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Example: To remove a model named &amp;quot;llama2-7b&amp;quot;, you would type:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; ollama rm llama2-7b &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Ollama will confirm the deletion of the specified model.&lt;/p&gt; &lt;p&gt;Note: If the model is currently running, you might need to stop it first using &lt;code&gt;ollama stop model_name&lt;/code&gt; before attempting to remove it. However, in most cases, &lt;code&gt;ollama rm&lt;/code&gt; will handle the removal directly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fos_personalis"&gt; /u/fos_personalis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mg3y3b/in_case_that_you_dont_know_how_to_remove_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mg3y3b/in_case_that_you_dont_know_how_to_remove_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mg3y3b/in_case_that_you_dont_know_how_to_remove_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T23:05:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfr7en</id>
    <title>I built a GitHub scanner that automatically discovers AI tools using a new .awesome-ai.md standard I created</title>
    <updated>2025-08-02T13:58:20+00:00</updated>
    <author>
      <name>/u/r00tkit_</name>
      <uri>https://old.reddit.com/user/r00tkit_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mfr7en/i_built_a_github_scanner_that_automatically/"&gt; &lt;img alt="I built a GitHub scanner that automatically discovers AI tools using a new .awesome-ai.md standard I created" src="https://external-preview.redd.it/ky_ornayw4KA64wFmqdvhjMSU-kQ6tuD_wBWYTjXMBQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b508f67099b146f539ed225a3951c2abc836363a" title="I built a GitHub scanner that automatically discovers AI tools using a new .awesome-ai.md standard I created" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just launched something I think could change how we discover AI tools on. Instead of manually submitting to directories or relying on outdated lists, I created the .awesome-ai.md standard.&lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Drop a .awesome-ai.md file in your repo root (template: &lt;a href="https://github.com/teodorgross/awesome-ai"&gt;https://github.com/teodorgross/awesome-ai&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The scanner finds it automatically within 30 minutes &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Creates a pull request for review&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Your tool goes live with real-time GitHub stats on (&lt;a href="https://awesome-ai.io"&gt;https://awesome-ai.io&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why this matters:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;No more manual submissions or contact forms&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Tools stay up-to-date automatically when you push changes&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;GitHub verification prevents spam&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Real-time star tracking and leaderboards&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Think of it like .gitignore for Git, but for AI tool discovery. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/r00tkit_"&gt; /u/r00tkit_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/teodorgross/awesome-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfr7en/i_built_a_github_scanner_that_automatically/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mfr7en/i_built_a_github_scanner_that_automatically/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T13:58:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfwnyn</id>
    <title>Getting started into self hosting LLM</title>
    <updated>2025-08-02T17:47:52+00:00</updated>
    <author>
      <name>/u/ywful</name>
      <uri>https://old.reddit.com/user/ywful</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ywful"&gt; /u/ywful &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1mfs9cw/getting_started_into_self_hosting_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfwnyn/getting_started_into_self_hosting_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mfwnyn/getting_started_into_self_hosting_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T17:47:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfn4c6</id>
    <title>Saidia: Offline-First AI Assistant for Educators in low-connectivity regions</title>
    <updated>2025-08-02T10:18:51+00:00</updated>
    <author>
      <name>/u/dokasto_</name>
      <uri>https://old.reddit.com/user/dokasto_</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dokasto_"&gt; /u/dokasto_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1mfn2xf/saidia_offlinefirst_ai_assistant_for_educators_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfn4c6/saidia_offlinefirst_ai_assistant_for_educators_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mfn4c6/saidia_offlinefirst_ai_assistant_for_educators_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T10:18:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mey1vp</id>
    <title>"Private ChatGPT conversations show up on Google, leaving internet users shocked"</title>
    <updated>2025-08-01T14:31:55+00:00</updated>
    <author>
      <name>/u/irodov4030</name>
      <uri>https://old.reddit.com/user/irodov4030</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://cybernews.com/ai-news/chatgpt-shared-links-privacy-leak/"&gt;https://cybernews.com/ai-news/chatgpt-shared-links-privacy-leak/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;&lt;strong&gt;&lt;em&gt;From private chats to full legal identities revealed ‚Äì internet users are finding ChatGPT conversations that inadvertently ended up on a simple Google search.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you‚Äôve ever shared a ChatGPT conversation using the ‚ÄúShare‚Äù button, there‚Äôs a chance it might now be floating around somewhere on Google, just a few keystrokes away from complete strangers.&lt;/p&gt; &lt;p&gt;A growing number of internet sleuths are discovering that ChatGPT‚Äôs shared links, which were originally designed for collaboration, are getting indexed by search engines.&lt;/p&gt; &lt;p&gt;ChatGPT's shared links feature allow users to generate a unique URL for a ChatGPT conversation. The shared chat becomes accessible to anyone with the link. However, if you share the URL on social media, a website, or if someone else shares it, it can be noticed by Google crawlers. Also, if you tick the box &amp;quot;Make this chat discoverable&amp;quot; while generating a URL, it automatically becomes accessible to Google.&amp;quot;&lt;/p&gt; &lt;p&gt;Edit:&lt;/p&gt; &lt;p&gt;from the article: &amp;quot;When you create a shared link in ChatGPT, it publishes a static read-only version of the conversation to a public OpenAI-hosted page. This page can be indexed by search engines.&amp;quot;&lt;/p&gt; &lt;p&gt;Normally, when you share google docs with 'Anyone with link can view', google does not crawl these pages unless explicitly published.&lt;/p&gt; &lt;p&gt;Users expecting privacy is weird but so is allowing indexing of these pages by default.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/irodov4030"&gt; /u/irodov4030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mey1vp/private_chatgpt_conversations_show_up_on_google/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mey1vp/private_chatgpt_conversations_show_up_on_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mey1vp/private_chatgpt_conversations_show_up_on_google/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T14:31:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg07ki</id>
    <title>Run Ollama and Open webui on different machines?</title>
    <updated>2025-08-02T20:19:08+00:00</updated>
    <author>
      <name>/u/Responsible__goose</name>
      <uri>https://old.reddit.com/user/Responsible__goose</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking some advice.&lt;/p&gt; &lt;p&gt;In my home network I have a game PC i rarely use and a homeserve (NUC11) which runs all kinds of docker/portainer apps. Some of them exposed to the web.&lt;/p&gt; &lt;p&gt;I had a WSL Ollama/openui setup on my PC at some point. So I know I can make it work but lost that instance.&lt;/p&gt; &lt;p&gt;For some reason it sounds very apealing to have the frontend (webui) run on my server and Ollama on my pc with GPU. It works to a degree that webui runs but the /models API comnection give a 404.&lt;/p&gt; &lt;p&gt;I've added the server to ollama origins en &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; as a host. On the webui side pointing to the PC, Ollama port (otherwise webui wont start). I tried running Ollama in docker and directly on WSL.&lt;/p&gt; &lt;p&gt;Does someone have experience with this setup? And can share their compose files or setup details?&lt;/p&gt; &lt;p&gt;Edit/Update SOLVED:&lt;/p&gt; &lt;p&gt;Lot's of diagnosing with GPT. With your input i figured it had to be something simple. On the settings &amp;gt; connections &amp;gt; direct connection you have to enter &amp;lt;ip&amp;gt;:11343/v1. I missed the &amp;quot;v1&amp;quot; when you're using your own server&lt;/p&gt; &lt;p&gt;It's in the documentation &lt;a href="https://docs.openwebui.com/getting-started/quick-start/starting-with-openai-compatible"&gt;here&lt;/a&gt;. Not too familiar with all the server lingo and I just never checked this page. As I was too focussed on the API endpoint pages. Thank you all for confirming it should work easily. It pushed me to look at the basics.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible__goose"&gt; /u/Responsible__goose &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mg07ki/run_ollama_and_open_webui_on_different_machines/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mg07ki/run_ollama_and_open_webui_on_different_machines/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mg07ki/run_ollama_and_open_webui_on_different_machines/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T20:19:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfx7l9</id>
    <title>Optimising GPU and VRAM usage for qwen3:32b-fp16 model</title>
    <updated>2025-08-02T18:10:37+00:00</updated>
    <author>
      <name>/u/donatas_xyz</name>
      <uri>https://old.reddit.com/user/donatas_xyz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I thought I will share the results of my attempts to utilise my two 16GB+12GB GPUs to run &lt;code&gt;qwen3:32b-fp16&lt;/code&gt; model.&lt;/p&gt; &lt;p&gt;The aim was to use as much VRAM as possible (so as close to 28GB as possible) whilst retaining the largest possible context window. And here are my results:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;SIZE&lt;/th&gt; &lt;th align="left"&gt;PROCESSOR (CPU/GPU)&lt;/th&gt; &lt;th align="left"&gt;CONTEXT&lt;/th&gt; &lt;th align="left"&gt;NUM_GPU&lt;/th&gt; &lt;th align="left"&gt;VRAM (REPORTED)&lt;/th&gt; &lt;th align="left"&gt;VRAM (REAL)&lt;/th&gt; &lt;th align="left"&gt;T/S&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;85GB&lt;/td&gt; &lt;td align="left"&gt;83%/17%&lt;/td&gt; &lt;td align="left"&gt;32768&lt;/td&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;14.45GB&lt;/td&gt; &lt;td align="left"&gt;13.1GB&lt;/td&gt; &lt;td align="left"&gt;0.79&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;84GB&lt;/td&gt; &lt;td align="left"&gt;71%/29%&lt;/td&gt; &lt;td align="left"&gt;18432&lt;/td&gt; &lt;td align="left"&gt;17&lt;/td&gt; &lt;td align="left"&gt;21.46GB&lt;/td&gt; &lt;td align="left"&gt;19.6GB&lt;/td&gt; &lt;td align="left"&gt;0.83&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;82GB&lt;/td&gt; &lt;td align="left"&gt;70%/30%&lt;/td&gt; &lt;td align="left"&gt;16384&lt;/td&gt; &lt;td align="left"&gt;18&lt;/td&gt; &lt;td align="left"&gt;24.59GB&lt;/td&gt; &lt;td align="left"&gt;20.5GB&lt;/td&gt; &lt;td align="left"&gt;0.87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;80GB&lt;/td&gt; &lt;td align="left"&gt;68%/32%&lt;/td&gt; &lt;td align="left"&gt;14336&lt;/td&gt; &lt;td align="left"&gt;19&lt;/td&gt; &lt;td align="left"&gt;25.6GB&lt;/td&gt; &lt;td align="left"&gt;21.2GB&lt;/td&gt; &lt;td align="left"&gt;0.89&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;78GB&lt;/td&gt; &lt;td align="left"&gt;68%/32%&lt;/td&gt; &lt;td align="left"&gt;12288&lt;/td&gt; &lt;td align="left"&gt;22&lt;/td&gt; &lt;td align="left"&gt;24.96GB&lt;/td&gt; &lt;td align="left"&gt;23.3GB&lt;/td&gt; &lt;td align="left"&gt;0.97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;70GB&lt;/td&gt; &lt;td align="left"&gt;64%/36%&lt;/td&gt; &lt;td align="left"&gt;4096&lt;/td&gt; &lt;td align="left"&gt;23&lt;/td&gt; &lt;td align="left"&gt;25.2GB&lt;/td&gt; &lt;td align="left"&gt;24.1GB&lt;/td&gt; &lt;td align="left"&gt;0.98&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;System specs:&lt;/strong&gt;&lt;br /&gt; Windows 11, 2x48GB 6400 DDR5, R7 7700X 8/16, RTX 5070 Ti 16GB + RTX 4070 12GB.&lt;/p&gt; &lt;p&gt;As you can see from the results the best compromise I could achieve so far is &lt;code&gt;num_ctx=12288&lt;/code&gt; and &lt;code&gt;num_gpu=22&lt;/code&gt;. That gets me close to 28GB VRAM while still keeping 12K context window.&lt;/p&gt; &lt;p&gt;I can technically run the model even with 65K context, but then my GPUs are basically idle and it's 50% slower as well.&lt;/p&gt; &lt;p&gt;I was just wondering, why is Ollama reporting higher VRAM usage than I can see in the Task Manager? Especially in the case of 16384 context window - it says that 24.59GB (30% of 82GB) is being used, but the Task Manager does only show 20.5GB combined between both GPUs? Am I misunderstanding the stats here?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT1:&lt;/strong&gt; After a night of changing no setting whatsoever - the num_gpu has dropped from 22 to 20 for the 12288 context, and so far I have no success getting it back up. The VRAM consumption remains the same even with 20 layers, but t/s dropped from 0.97 to 0.71. The testing prompt remains the same as well as the global Ollama settings. So very weird. I've expected to be working against the hardware limitations more than anything here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/donatas_xyz"&gt; /u/donatas_xyz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfx7l9/optimising_gpu_and_vram_usage_for_qwen332bfp16/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfx7l9/optimising_gpu_and_vram_usage_for_qwen332bfp16/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mfx7l9/optimising_gpu_and_vram_usage_for_qwen332bfp16/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T18:10:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg3elr</id>
    <title>mcp:swap ollama run w/ codex exec</title>
    <updated>2025-08-02T22:40:11+00:00</updated>
    <author>
      <name>/u/neurostream</name>
      <uri>https://old.reddit.com/user/neurostream</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR : &amp;quot;ollama pull/serve&amp;quot; supports MCP with tool models, but &amp;quot;ollama run&amp;quot; can't use them - so i replaced &amp;quot;ollama run&amp;quot; with &amp;quot;codex exec&amp;quot; (works with local airgapped ollama) in my bash scripts.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;i'm new to LLMs and not an AI dev, but i hella script in bash - so it was neat to find that i could &amp;quot;ollama run&amp;quot; in shell loops to pipe its stdout back into stdin and fun stuff like that.&lt;/p&gt; &lt;p&gt;but as mcp emerged, &amp;quot;ollama run&amp;quot; still client can't speak Model Context Protocol, even though models you can &amp;quot;ollama pull/serve&amp;quot; are tagged with &amp;quot;tools&amp;quot;, &amp;quot;vision&amp;quot;, &amp;quot;thinking&amp;quot;, etc.&lt;/p&gt; &lt;p&gt;so my local bash scripts using &amp;quot;ollama run&amp;quot; never get to benefit from the tool calls those models are dying to make. scripting it with curl and jq works, but it's a pain.&lt;/p&gt; &lt;p&gt;keep &amp;quot;ollama serve&amp;quot;, swap the client: the openai codex cli (github.com/openai/codex) &lt;em&gt;does&lt;/em&gt; speak tools/MCP. you can point it to your &amp;quot;ollama serve&amp;quot; address and no api keys/accounts needed, nor do external network calls to openai happen. invoking &amp;quot;codex exec&amp;quot;, suddenly the tool side-channel to &amp;quot;ollama serve&amp;quot; light up:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;the model now emits json tool requests&lt;/li&gt; &lt;li&gt;codex executes them, sending results back to the model&lt;/li&gt; &lt;li&gt;you get the final answer once the tool loop is done&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;unlike &amp;quot;ollama run&amp;quot;, &amp;quot;codex exec&amp;quot; accepts an additional option listing the mcp commands in your PATH that you want to let the LLM run on your local through that json side channel (which you can watch on stderr). It holds the main chat stream open on stdout waiting for the final response to be written out.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;What other ollama cli clients do MCP?&lt;/p&gt; &lt;p&gt;When will new ollama versions allow &amp;quot;ollama run&amp;quot; to do mcp stuff?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neurostream"&gt; /u/neurostream &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mg3elr/mcpswap_ollama_run_w_codex_exec/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mg3elr/mcpswap_ollama_run_w_codex_exec/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mg3elr/mcpswap_ollama_run_w_codex_exec/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T22:40:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg0311</id>
    <title>N8N + OpenWebUI</title>
    <updated>2025-08-02T20:13:31+00:00</updated>
    <author>
      <name>/u/Independent_Log8028</name>
      <uri>https://old.reddit.com/user/Independent_Log8028</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I'm trying to set up some LLM-involved automated/triggered workflows on n8n via Ollama but I want to also have the ability to use Ollama on openWebUI.&lt;/p&gt; &lt;p&gt;Two related questions:&lt;/p&gt; &lt;p&gt;‚Ä¢ Will it break ollama if n8n tries to use a model while I'm using the same (or different) model on openWebUI?&lt;/p&gt; &lt;p&gt;‚Ä¢ If it will break it, how do I prevent it from breaking? Or: what's a smarter way to accomplish this goal?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent_Log8028"&gt; /u/Independent_Log8028 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mg0311/n8n_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mg0311/n8n_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mg0311/n8n_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T20:13:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfoh8g</id>
    <title>Running Qwen3-Coder 30B at Full 256K Context: 25 tok/s with 96GB RAM + RTX 5080</title>
    <updated>2025-08-02T11:41:54+00:00</updated>
    <author>
      <name>/u/ajmusic15</name>
      <uri>https://old.reddit.com/user/ajmusic15</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mfoh8g/running_qwen3coder_30b_at_full_256k_context_25/"&gt; &lt;img alt="Running Qwen3-Coder 30B at Full 256K Context: 25 tok/s with 96GB RAM + RTX 5080" src="https://b.thumbs.redditmedia.com/sEwV4Z51XK64PlaKqqwGitLFVrGJ76H1E4yW6sAlukc.jpg" title="Running Qwen3-Coder 30B at Full 256K Context: 25 tok/s with 96GB RAM + RTX 5080" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I come to share with you my happiness running Qwen3-Coder 30B at its maximum unstretched context (256K).&lt;/p&gt; &lt;p&gt;To take full advantage of my processor cache without introducing additional latencies I'm using the LM Studio with 12 cores repartitioner equally between the two CCDs (6 CCD1 + 6 CCD2) using the affinity control of the task manager. I have noticed that using an unbalanced amount of cores between both CCD's decreases the amount of tokens per second but also using all cores.&lt;/p&gt; &lt;p&gt;As you can see, in order to run Qwen3-Coder 30B on my 96 GB RAM + 16 GB VRAM (5080) hardware I have had to load the whole model in Q3_K_M on the GPU but I have offloaded the context to the CPU, that makes the GPU just to do the inference to the model while the CPU is in charge of handling the context.&lt;/p&gt; &lt;p&gt;This way I could run Qwen3-Coder 30B at its 256K of context at ~25tk/s.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xcyh8vwr9lgf1.png?width=568&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75d6998987c43ce163e8c9611a53507fe244f8fb"&gt;https://preview.redd.it/xcyh8vwr9lgf1.png?width=568&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75d6998987c43ce163e8c9611a53507fe244f8fb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r2wesuyt9lgf1.png?width=1757&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7668da251043470250287d49abceea604130dee"&gt;https://preview.redd.it/r2wesuyt9lgf1.png?width=1757&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7668da251043470250287d49abceea604130dee&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajmusic15"&gt; /u/ajmusic15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfoh8g/running_qwen3coder_30b_at_full_256k_context_25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfoh8g/running_qwen3coder_30b_at_full_256k_context_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mfoh8g/running_qwen3coder_30b_at_full_256k_context_25/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T11:41:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgcfbn</id>
    <title>Basic 9060XT 16G on Ryzen - what size models should I be running for 'best bang for buck' results?</title>
    <updated>2025-08-03T06:38:14+00:00</updated>
    <author>
      <name>/u/NoButterscotch8359</name>
      <uri>https://old.reddit.com/user/NoButterscotch8359</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basic 9060XT 16G on Ryzen system - Running local LLMs on Ollama. What size models should I be running for best bang for buck results? &lt;/p&gt; &lt;p&gt;So many 'what should I run ...' and 'recommend me a ... ' threads out there.. Thought I would ad to it all. &lt;/p&gt; &lt;p&gt;Newb, and, basic specs: 128G DDR5, Ryzen 7700, 9060XT 16G on gigabyte X870E.&lt;/p&gt; &lt;p&gt;My 'research' tells me to use the biggest model I can fit on my 16G GPU, that being about 15G or maybe 16G model, but after experimenting with QWEN, magistral, Deepseek maxed at 15 &amp;amp; 16G models etc, I almost feel I'm getting better results from the 6-8G version of the same models. Accessing them all with Ollama on Fedora 42 Linux via bash. Using radeontool and ollama ps tells me I'm using my system to 'good capacity'. &lt;/p&gt; &lt;p&gt;TBH, I'm new at this, been lurking for weeks, and its a hell of a learning curve and now hit 'analysis paralysis'. My gut tells me I need to run bigger models and that would mean buying another GPU - thinking another 9060XT 16G and run it bifurcated off the one pcie (pcie 5.0). Its a great excuse to spend money I don't have and chalk it up to visa and whilst I'd rather not do that the itch to spend money on tech is ever present. &lt;/p&gt; &lt;p&gt;Using LLM for basic legal work and soon pinescripts in TradingView so its nothing too 'heavy'. &lt;/p&gt; &lt;p&gt;There is lots of 'A.I. created tutorials' on 'how to use A.I.' and I'm getting sick of it. Need a humans perspective. Suggestions..? &lt;/p&gt; &lt;p&gt;Is there anyone who has bifurcated a pcie 5.0 to run two gpus off the 'one slot'..? The x870e should have no problem doing it re the pcie 5.0 bandwidth, its just he logistics of doing so and, if I do, the 32G of vram is a hell of a lot better than 16G. Am I going to see massively different results by outlaying for another 16G gpu? Is it worth it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoButterscotch8359"&gt; /u/NoButterscotch8359 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mgcfbn/basic_9060xt_16g_on_ryzen_what_size_models_should/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mgcfbn/basic_9060xt_16g_on_ryzen_what_size_models_should/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mgcfbn/basic_9060xt_16g_on_ryzen_what_size_models_should/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-03T06:38:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgb7d2</id>
    <title>Open Source Status</title>
    <updated>2025-08-03T05:23:26+00:00</updated>
    <author>
      <name>/u/GhostInThePudding</name>
      <uri>https://old.reddit.com/user/GhostInThePudding</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, do we actually have any official word on what the deal is?&lt;/p&gt; &lt;p&gt;We have a new Windows/Mac GUI that is closed and there is no option on the download page to install the open source, non GUI having version. I can see the CLI versions are still accessible via Github, but is this a move to fully closing the project, or is the plan to open the GUI at some point?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GhostInThePudding"&gt; /u/GhostInThePudding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mgb7d2/open_source_status/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mgb7d2/open_source_status/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mgb7d2/open_source_status/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-03T05:23:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg02kc</id>
    <title>"Private ChatGPT conversations show up on Search Engine, leaving internet users shocked again"</title>
    <updated>2025-08-02T20:12:58+00:00</updated>
    <author>
      <name>/u/cashout__103</name>
      <uri>https://old.reddit.com/user/cashout__103</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few days back, I saw a post explaining that if you search this on Google,&lt;/p&gt; &lt;p&gt;&lt;strong&gt;site:chatgpt.com/share intext:&amp;quot;keyword you want to find in the conversation&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It would show you a lot of ChatGPT shared chats that people were not aware were available to the public so easily.&lt;/p&gt; &lt;p&gt;And fortunately, it got patched,&lt;/p&gt; &lt;p&gt;at least that's what I thought until I found out&lt;/p&gt; &lt;p&gt;&lt;strong&gt;it was still working on the Brave search engine&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cashout__103"&gt; /u/cashout__103 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mg02kc/private_chatgpt_conversations_show_up_on_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mg02kc/private_chatgpt_conversations_show_up_on_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mg02kc/private_chatgpt_conversations_show_up_on_search/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T20:12:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfqqxp</id>
    <title>Ollamacode - Local AI assistant that can create, run and understand the task at hand!</title>
    <updated>2025-08-02T13:37:23+00:00</updated>
    <author>
      <name>/u/Loud-Consideration-2</name>
      <uri>https://old.reddit.com/user/Loud-Consideration-2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mfqqxp/ollamacode_local_ai_assistant_that_can_create_run/"&gt; &lt;img alt="Ollamacode - Local AI assistant that can create, run and understand the task at hand!" src="https://preview.redd.it/yr8fwz5ezlgf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=93ef0e8b639c23f1c2a91ced55f0b129b49ba11e" title="Ollamacode - Local AI assistant that can create, run and understand the task at hand!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a project called OllamaCode, and I'd love to share it with you. It's an AI coding assistant that runs entirely locally with Ollama. The main idea was to create a tool that actually executes the code it writes, rather than just showing you blocks to copy and paste.&lt;/p&gt; &lt;p&gt;Here are a few things I've focused on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It can create and run files automatically from natural language.&lt;/li&gt; &lt;li&gt;I've tried to make it smart about executing tools like git, search, and bash commands.&lt;/li&gt; &lt;li&gt;It's designed to work with any Ollama model that supports function calling.&lt;/li&gt; &lt;li&gt;A big priority for me was to keep it 100% local to ensure privacy.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's still in the very early days, and there's a lot I still want to improve. It's been really helpful for my own workflow, and I would be incredibly grateful for any feedback from the community to help make it better.&lt;/p&gt; &lt;p&gt;Repo:&lt;a href="https://github.com/tooyipjee/ollamacode"&gt;https://github.com/tooyipjee/ollamacode&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loud-Consideration-2"&gt; /u/Loud-Consideration-2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yr8fwz5ezlgf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfqqxp/ollamacode_local_ai_assistant_that_can_create_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mfqqxp/ollamacode_local_ai_assistant_that_can_create_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T13:37:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgfcwe</id>
    <title>My AI-powered NPCs teach sustainable farming with Gemma 3n ‚Äì all local, no cloud, fully open source!</title>
    <updated>2025-08-03T09:50:52+00:00</updated>
    <author>
      <name>/u/Code-Forge-Temple</name>
      <uri>https://old.reddit.com/user/Code-Forge-Temple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üëã Hey folks! I recently built an open-source 2D game using &lt;strong&gt;Godot 4.x&lt;/strong&gt; where &lt;strong&gt;NPCs are powered by a local LLM&lt;/strong&gt; ‚Äî Google's new &lt;strong&gt;Gemma 3n&lt;/strong&gt; model running on &lt;strong&gt;Ollama&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;üéØ The goal: create private, offline-first educational experiences ‚Äî in this case, the NPCs &lt;strong&gt;teach sustainable farming and botany&lt;/strong&gt; through rich, Socratic-style dialogue.&lt;/p&gt; &lt;p&gt;üí° It‚Äôs built for the &lt;a href="https://www.kaggle.com/competitions/google-gemma-3n-hackathon"&gt;Google Gemma 3n Hackathon&lt;/a&gt;, which focuses on building real-world solutions with on-device, multimodal AI.&lt;/p&gt; &lt;h2&gt;üîß Tech stack:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Godot 4.x (C#)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt; to run Gemma 3n locally (on LAN or localhost)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom NPC component&lt;/strong&gt; for setting system prompts + model endpoint&lt;/li&gt; &lt;li&gt;No cloud APIs, no vendor lock-in ‚Äî everything runs locally&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;üîì Fully Open Source:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/code-forge-temple/local-llm-npc"&gt;https://github.com/code-forge-temple/local-llm-npc&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;üìπ 2-minute demo video:&lt;/h2&gt; &lt;p&gt;üëâ &lt;a href="https://www.youtube.com/watch?v=kGyafSgyRWA"&gt;Watch here&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;üôè Would love your feedback on: - Potential to extend to other educational domains after the hackathon - Opportunities to enhance accessibility, local education, and agriculture in future versions - Ideas for making the AI NPC system more modular and adaptable post-competition&lt;/p&gt; &lt;p&gt;Thanks for checking it out! üß†üå±&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Code-Forge-Temple"&gt; /u/Code-Forge-Temple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mgfcwe/my_aipowered_npcs_teach_sustainable_farming_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mgfcwe/my_aipowered_npcs_teach_sustainable_farming_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mgfcwe/my_aipowered_npcs_teach_sustainable_farming_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-03T09:50:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgaxv6</id>
    <title>Why is the new Ollama logo so angry?</title>
    <updated>2025-08-03T05:07:35+00:00</updated>
    <author>
      <name>/u/triynizzles1</name>
      <uri>https://old.reddit.com/user/triynizzles1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mgaxv6/why_is_the_new_ollama_logo_so_angry/"&gt; &lt;img alt="Why is the new Ollama logo so angry?" src="https://b.thumbs.redditmedia.com/ObBdDjQuE_WGOlhTrKP_GMas5HGVaPTke5ePURXfiro.jpg" title="Why is the new Ollama logo so angry?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I edited the eyebrows for friendliness :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/triynizzles1"&gt; /u/triynizzles1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mgaxv6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mgaxv6/why_is_the_new_ollama_logo_so_angry/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mgaxv6/why_is_the_new_ollama_logo_so_angry/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-03T05:07:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgddnr</id>
    <title>Is the 60 dollar P102-100 still a viable option for LLM?</title>
    <updated>2025-08-03T07:39:09+00:00</updated>
    <author>
      <name>/u/Boricua-vet</name>
      <uri>https://old.reddit.com/user/Boricua-vet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mgddnr/is_the_60_dollar_p102100_still_a_viable_option/"&gt; &lt;img alt="Is the 60 dollar P102-100 still a viable option for LLM?" src="https://preview.redd.it/0198z1fz2rgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc68e14085a3e5b83fe5f3852ed06c7c8223c73c" title="Is the 60 dollar P102-100 still a viable option for LLM?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have seen thousands of posts of people asking what card to buy and there is two points of view. One is buy expensive 3090, or even more expensive 5000 series or, buy cheap and try it. This post will cover why the P102-100 is still relevant and why it is simply the best budget card to get at 60 dollars.&lt;/p&gt; &lt;p&gt;If you are just doing LLM, Vision and no image or video generation. This is hands down the best budget card to get all because of its memory bandwidth. This list covers entry level cards form all series. Yes I know there are better cards but I am comparing the P102-100 with all entry level cards only and those better cards are 10x more.This is for the budget build people. &lt;/p&gt; &lt;p&gt;2060 - 336.0 GB/s - $150 8GB&lt;br /&gt; 3060 - 360.0 GB/s - $200+ 8GB&lt;/p&gt; &lt;p&gt;4060 - 272.0 GB/s - $260+ 8GB&lt;/p&gt; &lt;p&gt;5060 - 448.0 GB/s - $350+ 8GB&lt;/p&gt; &lt;p&gt;P102-100 - 440.3 GB/s - $60 10GB.&lt;/p&gt; &lt;p&gt;Is the P102-100 faster than an &lt;/p&gt; &lt;p&gt;entry 2060 = yes&lt;/p&gt; &lt;p&gt;entry 3060 = yes&lt;/p&gt; &lt;p&gt;entry 4060 = yes.&lt;/p&gt; &lt;p&gt;only a 5060 would be faster and not by much. &lt;/p&gt; &lt;p&gt;Does the P102-100 load slower, yes it takes about 1 second per GB on the model. PCie 1x4 =1GB/s but once the model is leaded it will be normal with no delays on all your queries.&lt;/p&gt; &lt;p&gt;I have attached screenshots of a bunch of models, all with 32K context so you can see what to expect. Compare those results with other entry cards using the same 32K context and you will for yourself. Make sure they are using 32K context as the P102-100 would also be faster with lower context.&lt;/p&gt; &lt;p&gt;so if you want to try LLM's and not go broke, the P102-100 is a solid card to try for 60 bucks. I have 2 of them and those results are using 2 cards so I have 20GB VRAM for 70 bucks at 35 each when I bought them. Now they would be 120 bucks. I am not sure if you can get 20GB VRAM for less than is as fast as this.&lt;/p&gt; &lt;p&gt;I hope this helps other people that have been afraid to try local private ai because of the costs. I hope this motivates you to at least try. It is just 60 bucks.&lt;/p&gt; &lt;p&gt;I will probably be updating this next week as I have a third card and I am moving up to 30GB. I should be able to run these models with higher context, 128k, 256k and even bigger models. I will post some updates for anyone interested.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Boricua-vet"&gt; /u/Boricua-vet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0198z1fz2rgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mgddnr/is_the_60_dollar_p102100_still_a_viable_option/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mgddnr/is_the_60_dollar_p102100_still_a_viable_option/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-03T07:39:09+00:00</published>
  </entry>
</feed>
