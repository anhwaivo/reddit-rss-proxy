<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-19T05:35:30+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1isc3nh</id>
    <title>Need help with text extraction from images (and PDFs) for verification purposes ‚Äì which OpenAI model or alternative should I use?</title>
    <updated>2025-02-18T12:59:11+00:00</updated>
    <author>
      <name>/u/waeljlassii</name>
      <uri>https://old.reddit.com/user/waeljlassii</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm working on a project where I need to extract text from images, and in some cases, PDFs, for verification purposes. I know that OpenAI has some models capable of handling text extraction, but I'm wondering which one would be the best fit for this kind of task via API.&lt;/p&gt; &lt;p&gt;Has anyone here worked on something similar or can recommend a suitable model or API? Any alternatives to OpenAI models that might be better for this task? I'd appreciate any advice or pointers from those who have experience with image and PDF text extraction.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waeljlassii"&gt; /u/waeljlassii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isc3nh/need_help_with_text_extraction_from_images_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isc3nh/need_help_with_text_extraction_from_images_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isc3nh/need_help_with_text_extraction_from_images_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T12:59:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1isczhb</id>
    <title>I Cannot Uninstall Ollama on Windows 11</title>
    <updated>2025-02-18T13:44:56+00:00</updated>
    <author>
      <name>/u/petercli</name>
      <uri>https://old.reddit.com/user/petercli</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1isczhb/i_cannot_uninstall_ollama_on_windows_11/"&gt; &lt;img alt="I Cannot Uninstall Ollama on Windows 11" src="https://b.thumbs.redditmedia.com/1xDxlViYPCkuD2zsHQomVLt12ZdjQq0UpHMAY9iVxtw.jpg" title="I Cannot Uninstall Ollama on Windows 11" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vm2ueqv0iwje1.png?width=1022&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0565f10d8157917a109a15dfc425ad8698f7c5e"&gt;https://preview.redd.it/vm2ueqv0iwje1.png?width=1022&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0565f10d8157917a109a15dfc425ad8698f7c5e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9d4dq716iwje1.png?width=563&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5a099c2cb0dd78f53cbf59cc2f89f6ccd4cb6e8"&gt;https://preview.redd.it/9d4dq716iwje1.png?width=563&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5a099c2cb0dd78f53cbf59cc2f89f6ccd4cb6e8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Folder C:\Users\xx\AppData\Local\Programs\Ollama exists , but does not have the file listed above. &lt;/p&gt; &lt;p&gt;Can I just delete C:\Users\xx\.ollama folder ? &lt;/p&gt; &lt;p&gt;Task manager shows Ollama running. &lt;/p&gt; &lt;p&gt;PS&amp;gt; I've written Professional Windows installers - LMK if you want free help .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/petercli"&gt; /u/petercli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isczhb/i_cannot_uninstall_ollama_on_windows_11/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isczhb/i_cannot_uninstall_ollama_on_windows_11/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isczhb/i_cannot_uninstall_ollama_on_windows_11/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T13:44:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1irrxqs</id>
    <title>DataBridge Feature Dump: We'll implement anything you want!!</title>
    <updated>2025-02-17T19:09:59+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! &lt;/p&gt; &lt;p&gt;Thanks to the power of the &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; community, &lt;a href="https://github.com/databridge-org/databridge-core"&gt;DataBridge&lt;/a&gt; just hit 350 stars! As a token of our gratitude, we're committing to implementing the &lt;strong&gt;top 3 feature requests&lt;/strong&gt; from &lt;em&gt;you&lt;/em&gt; :) &lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to participate:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Leave your dream feature or improvement - RAG or otherwise - as a reply to this post! Upvote existing ideas you‚Äôd love to see. We‚Äôll tally the votes and build the &lt;strong&gt;top 3 most-requested features&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Let‚Äôs shape DataBridge‚Äôs future together‚Äîdrop your requests below! üöÄ&lt;/p&gt; &lt;p&gt;&lt;em&gt;(We'll start tallying at 2:00 pm ET on the 18th of Feb - happy to start working on stuff before that tho!)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Huge thanks again for being part of this journey! üôå ‚ù§Ô∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irrxqs/databridge_feature_dump_well_implement_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irrxqs/databridge_feature_dump_well_implement_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irrxqs/databridge_feature_dump_well_implement_anything/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T19:09:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1irvwt0</id>
    <title>CheerBot. Ollama based Cheer or Nudge through OCR of your screen.</title>
    <updated>2025-02-17T21:48:43+00:00</updated>
    <author>
      <name>/u/maranone5</name>
      <uri>https://old.reddit.com/user/maranone5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Just a quick and dirty proof of concept!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here's a little productivity monitor I threw together:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Take two screenshots&lt;/strong&gt; of your screen at a predefined time difference.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OCR with Tesseract&lt;/strong&gt; to extract text (sorry graphic designers, no design elements here!).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Analyze the text&lt;/strong&gt; through Ollama (basically, your personal productivity assistant).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Nudge or Cheer&lt;/strong&gt; accordingly using a pet personality for a bit of fun motivation.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;No APIs, no cloud ‚Äì all execution is &lt;strong&gt;local&lt;/strong&gt; and &lt;strong&gt;safe&lt;/strong&gt; on your machine. Think of it like a modern-day Clippy.&lt;/p&gt; &lt;p&gt;The current version is a bit of a &lt;strong&gt;CLI mess&lt;/strong&gt; ‚Äì if anyone‚Äôs up for a challenge, feel free to create a &lt;strong&gt;GUI or popup version&lt;/strong&gt; to make it more user-friendly and visually appealing!&lt;/p&gt; &lt;p&gt;Feel free to check it out and let me know what you think! üôå&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/maranone/CheerBot"&gt;https://github.com/maranone/CheerBot&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maranone5"&gt; /u/maranone5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irvwt0/cheerbot_ollama_based_cheer_or_nudge_through_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irvwt0/cheerbot_ollama_based_cheer_or_nudge_through_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irvwt0/cheerbot_ollama_based_cheer_or_nudge_through_ocr/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T21:48:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1isegnq</id>
    <title>I have a Ryzen 5 CPU with Radeon Graphics and 3050 Laptop. Is it possible to make these GPUs (iGPU and dGPU) to work together in Ollama? Or any other alternatives?</title>
    <updated>2025-02-18T14:54:33+00:00</updated>
    <author>
      <name>/u/Ambitious_Fee_5818</name>
      <uri>https://old.reddit.com/user/Ambitious_Fee_5818</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious_Fee_5818"&gt; /u/Ambitious_Fee_5818 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isegnq/i_have_a_ryzen_5_cpu_with_radeon_graphics_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isegnq/i_have_a_ryzen_5_cpu_with_radeon_graphics_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isegnq/i_have_a_ryzen_5_cpu_with_radeon_graphics_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T14:54:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1isbug5</id>
    <title>Model will not load back into GPU</title>
    <updated>2025-02-18T12:44:55+00:00</updated>
    <author>
      <name>/u/Board-Upbeat</name>
      <uri>https://old.reddit.com/user/Board-Upbeat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm having an issue where any model will not load back into GPU. My setup is ollama and a Jupyter notebook running in docker containers within a Linux system. I load up a model and it initially works as intended using the GPU, but after 5 mins of inactivity it unloads from the GPU. When trying to load the model back I get errors trying to find a CUDA enabled device and proceeds to use CPU. I have tried keep_alive=-1 which works, unless you want to switch models and then I get a similar error. Any ideas of what I could try to get this working as intended? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Board-Upbeat"&gt; /u/Board-Upbeat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isbug5/model_will_not_load_back_into_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isbug5/model_will_not_load_back_into_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isbug5/model_will_not_load_back_into_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T12:44:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1is9thv</id>
    <title>Is it possible to feed a model with a saas application and train the model with its codebase?</title>
    <updated>2025-02-18T10:37:35+00:00</updated>
    <author>
      <name>/u/devfullstack98</name>
      <uri>https://old.reddit.com/user/devfullstack98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The PHP application has approximately 20 million lines of code.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devfullstack98"&gt; /u/devfullstack98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is9thv/is_it_possible_to_feed_a_model_with_a_saas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is9thv/is_it_possible_to_feed_a_model_with_a_saas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is9thv/is_it_possible_to_feed_a_model_with_a_saas/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T10:37:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ishfpk</id>
    <title>BrowserUse with Deepseek Ollama dead slow</title>
    <updated>2025-02-18T16:59:14+00:00</updated>
    <author>
      <name>/u/sammopus</name>
      <uri>https://old.reddit.com/user/sammopus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried using Deepseek R1-14B on M1 PRO 16GB, it worked fine with Ollama, decent tokens/s but when I used it with BrowserUse, it is dead slow! Impractical to use, did anyone get it working on similar spec?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sammopus"&gt; /u/sammopus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ishfpk/browseruse_with_deepseek_ollama_dead_slow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ishfpk/browseruse_with_deepseek_ollama_dead_slow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ishfpk/browseruse_with_deepseek_ollama_dead_slow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T16:59:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iryvnt</id>
    <title>Deepscaler km q4 is insanly fast abd small even on CPU only</title>
    <updated>2025-02-17T23:56:07+00:00</updated>
    <author>
      <name>/u/tshawkins</name>
      <uri>https://old.reddit.com/user/tshawkins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running on NUC11PRO 11th gen i7 with 32GB - 4200 RAM, no GPU, consumes a little over 1.1GB of RAM, Ideal for running in the background on a dev machine. &lt;/p&gt; &lt;p&gt;The Model Is a 1.5B parameter distilation of deepseek-r1. &lt;/p&gt; &lt;p&gt;Im going to be exploring its code generation abilities, to see if it is usefull as a enclosed code generator, ie no requests to the internet. &lt;/p&gt; &lt;p&gt;It would interesting to start a leader board of size vs capabilities. To understand which models are best for edge AI use. &lt;/p&gt; &lt;p&gt;thawkins@TimServFed01:~$ ollama run deepscaler:1.5b-preview-q4_K_M --verbose&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;tell me a story about 10 ants who save thier anthill from drowning during a flood, how they earned the respect of thier queen and all the other ants in the anthill. &amp;lt;think&amp;gt; Okay, so I need to write a story about 10 ants who saved their anthill from drowning during a flood, and explain how they got respect from their queen and everyone else. Hmm, let's break this down.&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;First, setting the scene: a big flood hits an ancient, powerful anthill. The ants are probably scared because of the high water level. They need to escape or find shelter quickly. But maybe they don't know much about each other or how to help.&lt;/p&gt; &lt;p&gt;They decide to go together. Maybe they leave their usual positions and start searching for spots where the water is less. That could save some lives. But why are there only 10 ants? Maybe because it's a small group, so they can work together.&lt;/p&gt; &lt;p&gt;When the flood comes in, the water level rises, and the ants find an area with low water. They go in, maybe even get drenched. But then someone notices them, like maybe a bird or a duck that spots their tiny collection. The birds might offer help by giving information or clues on where to go.&lt;/p&gt; &lt;p&gt;The queen is really important because she's known for being the leader and enforcing rules. Maybe she gets called out if they don't behave well, but when they manage to escape or protect the anthill, her respect grows. Perhaps she tells everyone about their bravery.&lt;/p&gt; &lt;p&gt;Other ants might get curious and offer help too. They could even start a little community effort, maybe building shelters or finding other ways to help. Their teamwork is key here.&lt;/p&gt; &lt;p&gt;In the end, the group realizes that working together was crucial. Maybe they find a way to ensure their safety despite the flood. They probably learn from this experience how important it is to be part of something bigger than themselves.&lt;/p&gt; &lt;p&gt;I should make sure the story shows their bravery, how they rely on each other, and the respect they receive from their queen and others. Maybe include some dialogue or actions that show their cooperation and determination. &amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Title: The Evacuation Sequence&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Once upon a time, in a lush, ancient anthill with its powerful guardian queen, there lived ten tiny ants who were deeply rooted in their tiny world but had always been curious about the world around them. These ants knew they had to act quickly because the flood was coming, and the water level was rising.&lt;/p&gt; &lt;p&gt;The ants realized that the most effective way to escape was by working together. They decided to go to a hidden spot where the water would be lower. However, their knowledge was limited, so they started searching in different areas. Despite their small size, their collective effort ensured they found an unobstructed path through the flood's waters.&lt;/p&gt; &lt;p&gt;As the flood approached, many ants were caught by some birds that spotted them. These birds offered information, guiding the ants on their escape route. The queen noticed the group's bravery and was immediately informed about their efforts.&lt;/p&gt; &lt;p&gt;The queen, ever the leader of her colony, recognized their bravery and began to notice any discrepancies in their behavior. She taught everyone about their role as part of a larger community. As the group worked together, they realized that their collective strength was essential for survival.&lt;/p&gt; &lt;p&gt;In the end, the ants successfully escaped the flood, protecting their anthill from drowning. They learned that working together could yield greater results than individual efforts. Their bravery earned them respect not only among their queen but also by becoming a model community effort.&lt;/p&gt; &lt;p&gt;Thus, their journey became a symbol of unity and resilience, teaching all who heard about it to work together for shared survival.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;total duration: 34.749197247s. load duration: 10.291253ms. prompt eval count: 44 token(s). prompt eval duration: 40ms. prompt eval rate: 1100.00 tokens/s. eval count: 708 token(s). eval duration: 34.697s. eval rate: 20.41 tokens/s. &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;Send a message (/? for help).&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tshawkins"&gt; /u/tshawkins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iryvnt/deepscaler_km_q4_is_insanly_fast_abd_small_even/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iryvnt/deepscaler_km_q4_is_insanly_fast_abd_small_even/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iryvnt/deepscaler_km_q4_is_insanly_fast_abd_small_even/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T23:56:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1islc2c</id>
    <title>Local Llama3.2:3b - No conversational memory?</title>
    <updated>2025-02-18T19:31:50+00:00</updated>
    <author>
      <name>/u/DelosBoard2052</name>
      <uri>https://old.reddit.com/user/DelosBoard2052</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I read somewhere that these local models should have conversational memory. I'm running the Llama3.2:3b model, but it seems to have ZERO memory of ANY previous utterance - it seems to view every input as a new conversation. Here's an example of an interaction after approximately a dozen interchanges within the same session:&lt;/p&gt; &lt;p&gt;&lt;code&gt;****&amp;gt; ^Can you summarize our conversation so far today?&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;We haven't had a conversation yet, this is the first message. What would you like to talk about? I can summarize our conversation at the end if you'd like.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;****&amp;gt; ^Do you remember what you just stated to me in your immediate past utterance?&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;This is the beginning of our conversation, and I haven't made any previous statements. I'm a large language model, I don't have personal memories or recall previous conversations. Each time you interact with me, it's a new conversation. What would you like to talk about?&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I'm wondering if there's something I'm not doing that I should have when I installed Ollama, or if the Llama3.2:3b model doesn't support conversational memory. If it's simply the latter, what models DO support conversational memory?&lt;/p&gt; &lt;p&gt;EDIT: I should add that I'm running Ollama inside of a python script, so I capture Ollama's output to send it to my TTS system. Does it work differently through the CLI by any chance?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DelosBoard2052"&gt; /u/DelosBoard2052 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1islc2c/local_llama323b_no_conversational_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1islc2c/local_llama323b_no_conversational_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1islc2c/local_llama323b_no_conversational_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T19:31:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ismy41</id>
    <title>Where did Ollamahub go?!</title>
    <updated>2025-02-18T20:36:22+00:00</updated>
    <author>
      <name>/u/PaulLee420</name>
      <uri>https://old.reddit.com/user/PaulLee420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I get a 404 error when I try to go to ollamahub.com - did something happen to it? Does it hate my VPN? &lt;/p&gt; &lt;p&gt;Trying to learn using Modelfiles and...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaulLee420"&gt; /u/PaulLee420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ismy41/where_did_ollamahub_go/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ismy41/where_did_ollamahub_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ismy41/where_did_ollamahub_go/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T20:36:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1isn6zh</id>
    <title>LLM evaluations using Ollama</title>
    <updated>2025-02-18T20:46:19+00:00</updated>
    <author>
      <name>/u/FlimsyProperty8544</name>
      <uri>https://old.reddit.com/user/FlimsyProperty8544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I run an open-source project &lt;a href="https://github.com/confident-ai/deepeval"&gt;DeepEval&lt;/a&gt;, which contains 20+ LLM-as-a-judge metrics to help evaluate LLM applications (i.e. RAG metrics, conversational metrics, red-teaming metrics). We integrated with Ollama, so you can now run evaluations for free using Ollama models locally on your device. Here‚Äôs how you can get started: &lt;a href="https://docs.confident-ai.com/docs/metrics-introduction#using-ollama"&gt;https://docs.confident-ai.com/docs/metrics-introduction#using-ollama&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FlimsyProperty8544"&gt; /u/FlimsyProperty8544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isn6zh/llm_evaluations_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isn6zh/llm_evaluations_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isn6zh/llm_evaluations_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T20:46:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1is774t</id>
    <title>Would my 4060 be enough for 14b deepseek or should I go down to 8b?</title>
    <updated>2025-02-18T07:26:27+00:00</updated>
    <author>
      <name>/u/Squidlers17</name>
      <uri>https://old.reddit.com/user/Squidlers17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title suggests, I am not sure on how many parameters should I use for my 4060 laptop I am also using Ryzen 9 7945HX&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Squidlers17"&gt; /u/Squidlers17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is774t/would_my_4060_be_enough_for_14b_deepseek_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is774t/would_my_4060_be_enough_for_14b_deepseek_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is774t/would_my_4060_be_enough_for_14b_deepseek_or/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T07:26:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1isp73j</id>
    <title>multiple models</title>
    <updated>2025-02-18T22:32:33+00:00</updated>
    <author>
      <name>/u/w38122077</name>
      <uri>https://old.reddit.com/user/w38122077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it possible with ollama to have two models running and each be available on a different port? I can run two and interact with them via the command line, but I can't seem to figure out how to have them available concurrently to Visual Code for use with chat and tab autocomplete &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w38122077"&gt; /u/w38122077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isp73j/multiple_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isp73j/multiple_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isp73j/multiple_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T22:32:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1is8g74</id>
    <title>DeepSeek or MS Bing AI?</title>
    <updated>2025-02-18T08:57:50+00:00</updated>
    <author>
      <name>/u/Wizard_of_Awes</name>
      <uri>https://old.reddit.com/user/Wizard_of_Awes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1is8g74/deepseek_or_ms_bing_ai/"&gt; &lt;img alt="DeepSeek or MS Bing AI?" src="https://a.thumbs.redditmedia.com/XX5Qjey4O_bAKYgdR-2EUjRrM2XntcwNHsjW11xn4V8.jpg" title="DeepSeek or MS Bing AI?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wizard_of_Awes"&gt; /u/Wizard_of_Awes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1is8g74"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is8g74/deepseek_or_ms_bing_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is8g74/deepseek_or_ms_bing_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T08:57:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1issdkb</id>
    <title>Cheapest way to host local model</title>
    <updated>2025-02-19T00:45:44+00:00</updated>
    <author>
      <name>/u/pur_ling_leaf</name>
      <uri>https://old.reddit.com/user/pur_ling_leaf</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm running a small startup and now trying to find an option how to smoothly shift from OpenAI API models to self-hosted. I checked Google cloud offerings, but they only have an enterprise plans for gpu renting. My questions: 1. What is the cheapest way to host local model? 2. If I host 32b model that requires 20gb of VRAM on a server with 22gb of VRAM, will this server be able to process requests to the model? 3. For small web-service, is it cheaper to go with OpenAI API or self-hosted model? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pur_ling_leaf"&gt; /u/pur_ling_leaf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1issdkb/cheapest_way_to_host_local_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1issdkb/cheapest_way_to_host_local_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1issdkb/cheapest_way_to_host_local_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T00:45:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1issp7p</id>
    <title>Modelfile PARAMETER Min/Max Values?</title>
    <updated>2025-02-19T01:00:07+00:00</updated>
    <author>
      <name>/u/AwJa44</name>
      <uri>https://old.reddit.com/user/AwJa44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The docs/modelfile.md page is nice, but it does not list the possible ranges of values for each parameter (e.g., mirostat_eta). Does anyone know where to find those? They just say float or int and give some defaults for some of them. &lt;/p&gt; &lt;p&gt;I've pored through the code a bit and found that the floats are 32-bit, and that 0 disables many of these. But, even the sparse code comments give no clue about minimum/maximum values possible.&lt;/p&gt; &lt;p&gt;When I see a 0.8, is that just assumed I should know that means the value should be between 0 and 1.0? (Should it?)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AwJa44"&gt; /u/AwJa44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1issp7p/modelfile_parameter_minmax_values/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1issp7p/modelfile_parameter_minmax_values/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1issp7p/modelfile_parameter_minmax_values/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T01:00:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1is22ug</id>
    <title>Most cost effective way of hosting 70B/32B param model</title>
    <updated>2025-02-18T02:29:31+00:00</updated>
    <author>
      <name>/u/topsy_here</name>
      <uri>https://old.reddit.com/user/topsy_here</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious &lt;/p&gt; &lt;p&gt;It comes down to efficiency. I see it like crypto mining. It‚Äôs about getting the best token count for least cost.&lt;/p&gt; &lt;p&gt;There‚Äôs Mac minis I‚Äôve seen hosting the 72B param one. You gonna need about 8x of them which is about 3.5K usd each? &lt;/p&gt; &lt;p&gt;What about hosting on a VPS in Linus? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/topsy_here"&gt; /u/topsy_here &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is22ug/most_cost_effective_way_of_hosting_70b32b_param/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is22ug/most_cost_effective_way_of_hosting_70b32b_param/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is22ug/most_cost_effective_way_of_hosting_70b32b_param/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T02:29:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1isispb</id>
    <title>Can I create an LLM model that will behave strictly the way I want?</title>
    <updated>2025-02-18T17:52:46+00:00</updated>
    <author>
      <name>/u/Nuvola_Rossa</name>
      <uri>https://old.reddit.com/user/Nuvola_Rossa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to create LLM models that can be uploaded locally. Those models should speak in a certain way or should only consider certain knowledge. For example, I want to create a model that would answer like a medieval man.&lt;/p&gt; &lt;p&gt;The idea is to have a chatbot for which the user shouldn't have to prompt anything specific for it to behave that way (for example if the user asks &amp;quot;what happen during world war II&amp;quot;, the model should answer &amp;quot;I have no idea&amp;quot; or something like that). &lt;/p&gt; &lt;p&gt;I would like to have several AI model that could be loaded in order to compare the answers. And i will need a GGUF (this is not optional) of each model.&lt;/p&gt; &lt;p&gt;I've been looking around for a way to do this but I can't find any way out of it. Any ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nuvola_Rossa"&gt; /u/Nuvola_Rossa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isispb/can_i_create_an_llm_model_that_will_behave/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isispb/can_i_create_an_llm_model_that_will_behave/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isispb/can_i_create_an_llm_model_that_will_behave/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T17:52:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1isuyd4</id>
    <title>8x AMD Instinct Mi50 AI Server #1 is in Progress..</title>
    <updated>2025-02-19T02:45:22+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1isuyd4/8x_amd_instinct_mi50_ai_server_1_is_in_progress/"&gt; &lt;img alt="8x AMD Instinct Mi50 AI Server #1 is in Progress.." src="https://preview.redd.it/q7q2q1n970ke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=051ed274f0a297d2b44d83c02fd2cc80ce6d0b7f" title="8x AMD Instinct Mi50 AI Server #1 is in Progress.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q7q2q1n970ke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isuyd4/8x_amd_instinct_mi50_ai_server_1_is_in_progress/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isuyd4/8x_amd_instinct_mi50_ai_server_1_is_in_progress/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T02:45:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1isvaad</id>
    <title>wllama on crappy Chromebooks</title>
    <updated>2025-02-19T03:01:43+00:00</updated>
    <author>
      <name>/u/Over-Leader-9494</name>
      <uri>https://old.reddit.com/user/Over-Leader-9494</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;Apologies in advance as this relates to Wllama - &lt;a href="https://huggingface.co/spaces/ngxson/wllama"&gt;https://huggingface.co/spaces/ngxson/wllama&lt;/a&gt; A nifty way to use llama.cpp via WebAssembly to run in browser memory. &lt;/p&gt; &lt;p&gt;&amp;quot;Wllama is a project based on &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt;. It enables running LLM inference directly on browser by leveraging the power of &lt;strong&gt;WebAssembly&lt;/strong&gt;. It accepts GGUF as model format.&amp;quot;&lt;/p&gt; &lt;p&gt;In any case, I am trying to get this running on the crapiest hardware I could find and luckily a teacher is my friend and I have a 3.74 GB Celeron 3965Y @ 1.5 GHz... about $70 used these days &lt;a href="https://www.ebay.com/itm/176475517458"&gt;https://www.ebay.com/itm/176475517458&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It downloads, but freezes there.&lt;/p&gt; &lt;p&gt;Trying to set up a demo for kids at my daughter's school and prefer to use hardware they already have to help them understand SLM's.&lt;/p&gt; &lt;p&gt;Much appreciate if folks had any insights or experience here. This one may be a bridge to far, but I can hope.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Over-Leader-9494"&gt; /u/Over-Leader-9494 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isvaad/wllama_on_crappy_chromebooks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isvaad/wllama_on_crappy_chromebooks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isvaad/wllama_on_crappy_chromebooks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T03:01:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1isr8zl</id>
    <title>After great pains (learning curve), got llama.cpp running on my older AMD GPU (since Ollama isn‚Äôt compatible)‚Ä¶but the two things I want to use Ollama with don‚Äôt ‚Äúcommunicate‚Äù with it in the way they do Ollama. HomeAssistant and Frigate use Ollama at port 11434, llama.cpp doesn‚Äôt have that‚Ä¶help?</title>
    <updated>2025-02-18T23:55:58+00:00</updated>
    <author>
      <name>/u/FantasyMaster85</name>
      <uri>https://old.reddit.com/user/FantasyMaster85</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've got an older AMD GPU that is running llama.cpp (built with Vulcan and fully utilizing my GPU...an RX 570) along with the given sub 4gb models at a perfectly acceptable TPS for my two use cases (HomeAssistant and Frigate), as tested manually running llama-server and passing queries to it manually. &lt;/p&gt; &lt;p&gt;The issue is that while both HomeAssistant and Frigate have a means to work with Ollama at port 11434, I can't for the life of me figure out how to expose the same functionality using llama.cpp...is it even possible?&lt;/p&gt; &lt;p&gt;I've tried llama-server using llama.cpp and it doesn't work with HomeAssistant or Frigate, despite the web UI created by it working fine (seems that's an &amp;quot;openAI&amp;quot; API versus the &amp;quot;Ollama&amp;quot; style API exposed by Ollama. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FantasyMaster85"&gt; /u/FantasyMaster85 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isr8zl/after_great_pains_learning_curve_got_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isr8zl/after_great_pains_learning_curve_got_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isr8zl/after_great_pains_learning_curve_got_llamacpp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T23:55:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1isqhm1</id>
    <title>Ollama Shell -- improved Terminal app for using local models</title>
    <updated>2025-02-18T23:22:47+00:00</updated>
    <author>
      <name>/u/sunkencity999</name>
      <uri>https://old.reddit.com/user/sunkencity999</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1isqhm1/ollama_shell_improved_terminal_app_for_using/"&gt; &lt;img alt="Ollama Shell -- improved Terminal app for using local models" src="https://b.thumbs.redditmedia.com/i1OWSPjS_iJXLlQo0MDCNlUzL5p_Rm9K-O4rQsmb8NM.jpg" title="Ollama Shell -- improved Terminal app for using local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y'all,&lt;/p&gt; &lt;p&gt;I am personally a huge fan of working directly in the terminal; the existing terminal shell for Ollama, in my opinion, leaves much to be desired, functionality and aesthetics-wise. SO, I figured I would create a Shell application that allows you to work with Ollama and models in the terminal in a way that is practical and reasonably efficient. You can analyze documents by dragging-and-dropping them in the chat, manage models (pull and delete), have continuous chat history and save system prompts for use as necessary. If working in the terminal / shell is something you enjoy as well, please give it a shot. Free, and of course I welcome contributors.&lt;br /&gt; &lt;a href="https://github.com/sunkencity999/ollama_shell"&gt;Ollama Shell on Github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ym0w7cmtizje1.png?width=1071&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=427fbffce0d8ba45e5906e3342f5aa1028bcd959"&gt;Main Interface&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tpfh7k4vizje1.png?width=1059&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0fa8276a0e793475538033c655452323b68354bc"&gt;Prompt selection after Model selection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n2mhbjjxizje1.png?width=920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d99ceda3ceb10582f95be390b8bf8b12a3fc641e"&gt;Query answered by LLM and provided (deepseek-r1:14b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunkencity999"&gt; /u/sunkencity999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isqhm1/ollama_shell_improved_terminal_app_for_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isqhm1/ollama_shell_improved_terminal_app_for_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isqhm1/ollama_shell_improved_terminal_app_for_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T23:22:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1istkgr</id>
    <title>Does Ollama cache prompts?</title>
    <updated>2025-02-19T01:40:06+00:00</updated>
    <author>
      <name>/u/palaceofcesi</name>
      <uri>https://old.reddit.com/user/palaceofcesi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay I‚Äôm a little confused and freaked out right now but my first thought is that I didn‚Äôt read the documentation properly.&lt;/p&gt; &lt;p&gt;Does Ollama cache prompts? &lt;/p&gt; &lt;p&gt;I previously used the deepseek-r1:32B with Ollama to create a presentation about a business product, call it Product A.&lt;/p&gt; &lt;p&gt;Then I used deepseek to create a presentation about Product B. In my prompt ‚Äúollama run deepseek-r1:32b $prompt‚Äù I made no reference whatsoever to Product A. And yet, in its response, I received multiple references to Product A in my creating a presentation for Product B. &lt;/p&gt; &lt;p&gt;The model was praising how well these two products work together.&lt;/p&gt; &lt;p&gt;That‚Äôs great, but I was not aware of any prompt caching in Ollama. This has a huge security implication because I‚Äôm running Ollama on sensitive documents on internal networks of non-air-gapped systems so if Ollama is caching the prompts and/or outputs and potentially uploading them over the network that would be a huge security risk.&lt;/p&gt; &lt;p&gt;Can someone tell me what‚Äôs going on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/palaceofcesi"&gt; /u/palaceofcesi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1istkgr/does_ollama_cache_prompts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1istkgr/does_ollama_cache_prompts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1istkgr/does_ollama_cache_prompts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T01:40:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1isvd1h</id>
    <title>Ollama Deepseek-R1 AI writes my Obsidian notes by watching my screen (open source)</title>
    <updated>2025-02-19T03:05:26+00:00</updated>
    <author>
      <name>/u/louis3195</name>
      <uri>https://old.reddit.com/user/louis3195</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1isvd1h/ollama_deepseekr1_ai_writes_my_obsidian_notes_by/"&gt; &lt;img alt="Ollama Deepseek-R1 AI writes my Obsidian notes by watching my screen (open source)" src="https://external-preview.redd.it/NGI4NHl2aTBpMGtlMaEzGEUaUQonH8wQBp1lwNMA6DwRZxj8nxN2XpHfUUNy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6aa25cc908b1aaf94262c3fa735cebef775b5e9c" title="Ollama Deepseek-R1 AI writes my Obsidian notes by watching my screen (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/louis3195"&gt; /u/louis3195 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1eo4wui0i0ke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isvd1h/ollama_deepseekr1_ai_writes_my_obsidian_notes_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isvd1h/ollama_deepseekr1_ai_writes_my_obsidian_notes_by/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T03:05:26+00:00</published>
  </entry>
</feed>
