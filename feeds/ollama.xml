<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-04-22T10:06:59+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1k3798d</id>
    <title>Understanding ollama's comparative resource performance</title>
    <updated>2025-04-19T21:49:20+00:00</updated>
    <author>
      <name>/u/SocietyTomorrow</name>
      <uri>https://old.reddit.com/user/SocietyTomorrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been considering setting up a medium scale compute cluster for a private SaaS ollama (for context I run a [very]small rural ISP and also rent a little rack space to some of my business clients) as an add on for a chunk of my pro users (already got the green light that some would be happy to pay for it) but one interesting point of consideration has been raised. I am wondering whether it would be more efficient to make all the GPU resources clustered, or have individual machines that can be assigned to the client 1:1.&lt;/p&gt; &lt;p&gt;I think the biggest thing that boils down to me is how exactly tools utilize the available resources. I plan to ask around for other tools like torchchat for their version of this question, but basically...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If a model fits 100% into VRAM = 100% of expected performance, then does a model that exceeds VRAM and is loaded to system RAM result in performance based on the percentage of the model not in VRAM, or throttle 100% to the speed and bandwidth of the system RAM? Do models with MoE (like DeepSeek) perform better in this kind of situation where expert submodels loaded to VRAM still perform at full speed, or is that something that ollama would not directly know was happening if those conditions were met?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I appreciate any feedback on this subject, it's been a fascinating research subject and can't wait to hear if random people on the internet can help to justify buying excessive compute resources!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SocietyTomorrow"&gt; /u/SocietyTomorrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k3798d/understanding_ollamas_comparative_resource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k3798d/understanding_ollamas_comparative_resource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k3798d/understanding_ollamas_comparative_resource/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-19T21:49:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1k30t4d</id>
    <title>Best small ollama model for SQL code help</title>
    <updated>2025-04-19T16:56:05+00:00</updated>
    <author>
      <name>/u/VerbaGPT</name>
      <uri>https://old.reddit.com/user/VerbaGPT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've built an application that runs locally (in your browser) and allows the user to use LLMs to analyze databases like Microsoft SQL servers and MySQL, in addition to CSV etc.&lt;/p&gt; &lt;p&gt;I just added a method that allows for completely offline process using Ollama. I'm using llama3.2 currently, but on my average CPU laptop it is kind of slow. Wanted to ask here, do you recommend any small model Ollama model (&amp;lt;1gb) that has good coding performance? In particular python and/or SQL. TIA!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VerbaGPT"&gt; /u/VerbaGPT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k30t4d/best_small_ollama_model_for_sql_code_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k30t4d/best_small_ollama_model_for_sql_code_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k30t4d/best_small_ollama_model_for_sql_code_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-19T16:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2rb38</id>
    <title>I built a Local AI Voice Assistant with Ollama + gTTS with interruption</title>
    <updated>2025-04-19T08:04:21+00:00</updated>
    <author>
      <name>/u/typhoon90</name>
      <uri>https://old.reddit.com/user/typhoon90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I just built OllamaGTTS, a lightweight voice assistant that brings AI-powered voice interactions to your local Ollama setup using Google TTS for natural speech synthesis. It’s fast, interruptible, and optimized for real-time conversations. I am aware that some people prefer to keep everything local so I am working on an update that will likely use Kokoro for local speech synthesis. I would love to hear your thoughts on it and how it can be improved.&lt;/p&gt; &lt;p&gt;Key Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time voice interaction (Silero VAD + Whisper transcription)&lt;/li&gt; &lt;li&gt;Interruptible speech playback (no more waiting for the AI to finish talking)&lt;/li&gt; &lt;li&gt;FFmpeg-accelerated audio processing (optional speed-up for faster * replies)&lt;/li&gt; &lt;li&gt;Persistent conversation history with configurable memory&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/ExoFi-Labs/OllamaGTTS"&gt;GitHub Repo: https://github.com/ExoFi-Labs/OllamaGTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Instructions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Clone Repo&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Install requirements&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Run ollama_gtts.py&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;*I am working on integrating Kokoro STT at the moment, and perhaps Sesame in the coming days.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/typhoon90"&gt; /u/typhoon90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k2rb38/i_built_a_local_ai_voice_assistant_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k2rb38/i_built_a_local_ai_voice_assistant_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k2rb38/i_built_a_local_ai_voice_assistant_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-19T08:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1k39auq</id>
    <title>Automated metadata extraction and direct visual doc chats with Morphik (open-source, ollama support)</title>
    <updated>2025-04-19T23:30:45+00:00</updated>
    <author>
      <name>/u/yes-no-maybe_idk</name>
      <uri>https://old.reddit.com/user/yes-no-maybe_idk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k39auq/automated_metadata_extraction_and_direct_visual/"&gt; &lt;img alt="Automated metadata extraction and direct visual doc chats with Morphik (open-source, ollama support)" src="https://external-preview.redd.it/enozYndjcGNtdnZlMZ2wNVo-OE2bciGH4sJ2rG79auy1VwP-dEBcS0EBYyDD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2a91f2c95e900437f0b3e08eba69a41727996203" title="Automated metadata extraction and direct visual doc chats with Morphik (open-source, ollama support)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;We’ve been building &lt;a href="https://github.com/morphik-org/morphik-core"&gt;Morphik&lt;/a&gt;, an open-source platform for working with unstructured data—think PDFs, slides, medical reports, patents, etc. It’s designed to be &lt;strong&gt;modular, local-first, and LLM-agnostic&lt;/strong&gt; (works great with Ollama!).&lt;/p&gt; &lt;p&gt;Recent updates based on community feedback include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A much cleaner, more intuitive UI&lt;/li&gt; &lt;li&gt;Built-in workflows like metadata extraction and rule-based structuring&lt;/li&gt; &lt;li&gt;Knowledge graph + graph-RAG support&lt;/li&gt; &lt;li&gt;KV caching for fast lookups&lt;/li&gt; &lt;li&gt;Content transformation (e.g. PII redaction, page splitting)&lt;/li&gt; &lt;li&gt;Colpali-style embeddings — we send entire document pages as images to the LLM, which massively improves accuracy on diagrams and tables (vs just captioned OCR text)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It plugs nicely into local LLM setups, and we’d love for you to try it with your Ollama workflows. Feedback, feature requests, and PRs are very welcome!&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/morphik-org/morphik-core"&gt;github.com/morphik-org/morphik-core&lt;/a&gt;&lt;br /&gt; Discord: &lt;a href="https://discord.com/invite/BwMtv3Zaju"&gt;https://discord.com/invite/BwMtv3Zaju&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yes-no-maybe_idk"&gt; /u/yes-no-maybe_idk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ykllccpcmvve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k39auq/automated_metadata_extraction_and_direct_visual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k39auq/automated_metadata_extraction_and_direct_visual/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-19T23:30:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3a4p5</id>
    <title>I built a Local MCP Server to enable Computer-Use Agent to run through Claude Desktop, Cursor, and other MCP clients.</title>
    <updated>2025-04-20T00:13:51+00:00</updated>
    <author>
      <name>/u/sandropuppo</name>
      <uri>https://old.reddit.com/user/sandropuppo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k3a4p5/i_built_a_local_mcp_server_to_enable_computeruse/"&gt; &lt;img alt="I built a Local MCP Server to enable Computer-Use Agent to run through Claude Desktop, Cursor, and other MCP clients." src="https://external-preview.redd.it/ZG9reDkzZDN0dnZlMZppp7K11at-IaSBIx6ekIrLv_SO-25kJ8guE9xrX4Mu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7fbb282edc4f5bce6ea032ffe9805e991c32ec48" title="I built a Local MCP Server to enable Computer-Use Agent to run through Claude Desktop, Cursor, and other MCP clients." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Example using Claude Desktop and Tableau&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandropuppo"&gt; /u/sandropuppo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k41k73d3tvve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k3a4p5/i_built_a_local_mcp_server_to_enable_computeruse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k3a4p5/i_built_a_local_mcp_server_to_enable_computeruse/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-20T00:13:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3g1a5</id>
    <title>Ollama+AbletonMCP</title>
    <updated>2025-04-20T06:06:53+00:00</updated>
    <author>
      <name>/u/GokulSoundararajan</name>
      <uri>https://old.reddit.com/user/GokulSoundararajan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried Claude+AbletonMCP it's really amazing, I wonder how this could be done using ollama with good models, thoughts are welcome, can anybody guide me on the same&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GokulSoundararajan"&gt; /u/GokulSoundararajan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k3g1a5/ollamaabletonmcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k3g1a5/ollamaabletonmcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k3g1a5/ollamaabletonmcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-20T06:06:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1k41vpx</id>
    <title>How do I get the stats window?</title>
    <updated>2025-04-21T01:31:57+00:00</updated>
    <author>
      <name>/u/Final-Photograph656</name>
      <uri>https://old.reddit.com/user/Final-Photograph656</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k41vpx/how_do_i_get_the_stats_window/"&gt; &lt;img alt="How do I get the stats window?" src="https://external-preview.redd.it/p0GCiVqUr37S_BzI4wmk-WOaP1LCXXezSOj87ybrP4Q.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=286a627745cc1b24f2f9b19e13ba4115496fa0a3" title="How do I get the stats window?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How do I get the text at 2:11 mark where it shows token and stuff like that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Final-Photograph656"&gt; /u/Final-Photograph656 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=o1sN1lB76EA"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k41vpx/how_do_i_get_the_stats_window/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k41vpx/how_do_i_get_the_stats_window/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T01:31:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3up25</id>
    <title>Ollama on RHEL 7</title>
    <updated>2025-04-20T19:40:04+00:00</updated>
    <author>
      <name>/u/raghav-ai</name>
      <uri>https://old.reddit.com/user/raghav-ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not able to use ollama new version on RHEL 7 as glib version required is not installed. Upgrading glib is risky.. Is there any other solution ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/raghav-ai"&gt; /u/raghav-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k3up25/ollama_on_rhel_7/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k3up25/ollama_on_rhel_7/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k3up25/ollama_on_rhel_7/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-20T19:40:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3weim</id>
    <title>Load Models in RAM?</title>
    <updated>2025-04-20T21:00:24+00:00</updated>
    <author>
      <name>/u/Maple382</name>
      <uri>https://old.reddit.com/user/Maple382</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! Simple question, is it possible to load models into RAM rather than VRAM? There are some models (such as QwQ) which don't fit in my GPU memory, but would fit in my RAM just fine. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maple382"&gt; /u/Maple382 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k3weim/load_models_in_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k3weim/load_models_in_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k3weim/load_models_in_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-20T21:00:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4mpqt</id>
    <title>AI Helped Me Write Over A Quarter Million Lines of Code. The Internet Has No Idea What’s About to Happen.</title>
    <updated>2025-04-21T19:46:43+00:00</updated>
    <author>
      <name>/u/No-Definition-2886</name>
      <uri>https://old.reddit.com/user/No-Definition-2886</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k4mpqt/ai_helped_me_write_over_a_quarter_million_lines/"&gt; &lt;img alt="AI Helped Me Write Over A Quarter Million Lines of Code. The Internet Has No Idea What’s About to Happen." src="https://external-preview.redd.it/8tPuc2LiYbWj-nCC7gZItpWcqaXGApKn75446k3Vqvc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bab099a19a9ba1f29d4fa1c36a1f201032a37db" title="AI Helped Me Write Over A Quarter Million Lines of Code. The Internet Has No Idea What’s About to Happen." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Definition-2886"&gt; /u/No-Definition-2886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nexustrade.io/blog/ai-helped-me-write-over-a-quarter-million-lines-of-code-the-internet-has-no-idea-whats-about-to-happen-20250421"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4mpqt/ai_helped_me_write_over_a_quarter_million_lines/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k4mpqt/ai_helped_me_write_over_a_quarter_million_lines/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T19:46:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3898c</id>
    <title>Making a Live2D Character Chat Using Only Local AI</title>
    <updated>2025-04-19T22:38:35+00:00</updated>
    <author>
      <name>/u/fagenorn</name>
      <uri>https://old.reddit.com/user/fagenorn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k3898c/making_a_live2d_character_chat_using_only_local_ai/"&gt; &lt;img alt="Making a Live2D Character Chat Using Only Local AI" src="https://external-preview.redd.it/Y2lqdTJmNjJjdnZlMdxX11AfgZTEMF7oSAzFAlLSpvlezRf_S3o9RpaxpyHo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=32f94142be23c544b342cd5bb804efcf6547e735" title="Making a Live2D Character Chat Using Only Local AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share a personal project I've been working on in my freetime. I'm trying to build an interactive, voice-driven Live2D avatar.&lt;/p&gt; &lt;p&gt;The basic idea is: my voice goes in -&amp;gt; gets transcribed locally with Whisper -&amp;gt; that text gets sent to the Ollama api (along with history and a personality prompt) -&amp;gt; the response comes back -&amp;gt; gets turned into speech with a local TTS -&amp;gt; and finally animates the Live2D character (lipsync + emotions).&lt;/p&gt; &lt;p&gt;My main goal was to see if I could get this whole chain running smoothly &lt;em&gt;locally&lt;/em&gt; on my somewhat old GTX 1080 Ti. Since I also like being able to use latest and greatest models + ability to run bigger models on mac or whatever, I decided to make this work with ollama api so I can just plug and play that.&lt;/p&gt; &lt;p&gt;Getting the character (I included a demo model, Aria) to sound right definitely takes some fiddling with the prompt in the &lt;code&gt;personality.txt&lt;/code&gt; file. Any tips for keeping local LLMs consistently in character during conversations?&lt;/p&gt; &lt;p&gt;The whole thing's built in C#, which was a fun departure from the usual Python AI world for me, and the performance has been pretty decent.&lt;/p&gt; &lt;p&gt;Anyway, the code's here if you want to peek or try it: &lt;a href="https://github.com/fagenorn/handcrafted-persona-engine"&gt;https://github.com/fagenorn/handcrafted-persona-engine&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fagenorn"&gt; /u/fagenorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ltcrrg62cvve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k3898c/making_a_live2d_character_chat_using_only_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k3898c/making_a_live2d_character_chat_using_only_local_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-19T22:38:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1k469ef</id>
    <title>Quick question on GPU usage vs CPU for models</title>
    <updated>2025-04-21T05:45:28+00:00</updated>
    <author>
      <name>/u/Bored_Nerds</name>
      <uri>https://old.reddit.com/user/Bored_Nerds</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know almost nothing about LLM and Ollama but I have 1 question.&lt;/p&gt; &lt;p&gt;For some reason, when I am using llama3 my GPU is being used, however, when I use llama3.3 my CPU is being used. IS there a reason for that ?&lt;/p&gt; &lt;p&gt;I am using a Chrome extension UI for ollama called Page Assist. Also, that llama3 I guess got downloaded together with llama3.3 because I only pulled 3.3 and I see two models to choose from in the menu. Also, Gemma3 is also using GPU. I have only the extension + ollama for Windows installed, nothing else in terms of AI apps or something.&lt;/p&gt; &lt;p&gt;Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bored_Nerds"&gt; /u/Bored_Nerds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k469ef/quick_question_on_gpu_usage_vs_cpu_for_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k469ef/quick_question_on_gpu_usage_vs_cpu_for_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k469ef/quick_question_on_gpu_usage_vs_cpu_for_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T05:45:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1k49slw</id>
    <title>Hi, this is a question related to agentic workflows.</title>
    <updated>2025-04-21T09:58:10+00:00</updated>
    <author>
      <name>/u/Tough_Rooster_8164</name>
      <uri>https://old.reddit.com/user/Tough_Rooster_8164</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone. I recently became interested in Ai. I have a question.&lt;br /&gt; Is there currently a feature in olama that allows me to download different models and see the result values after cross-validation with each other?&lt;br /&gt; It might be a bit weird because I'm using a translator&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tough_Rooster_8164"&gt; /u/Tough_Rooster_8164 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k49slw/hi_this_is_a_question_related_to_agentic_workflows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k49slw/hi_this_is_a_question_related_to_agentic_workflows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k49slw/hi_this_is_a_question_related_to_agentic_workflows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T09:58:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4gvmo</id>
    <title>does anyone have any examples for Arduino as a client for Ollama?</title>
    <updated>2025-04-21T15:58:01+00:00</updated>
    <author>
      <name>/u/Geofrancis</name>
      <uri>https://old.reddit.com/user/Geofrancis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;does anyone have any esp32 examples for interacting with ollama ? I am using Google Gemini at the moment, but iI would like to use my own local server.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Geofrancis"&gt; /u/Geofrancis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4gvmo/does_anyone_have_any_examples_for_arduino_as_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4gvmo/does_anyone_have_any_examples_for_arduino_as_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k4gvmo/does_anyone_have_any_examples_for_arduino_as_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T15:58:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4fcwj</id>
    <title>built-in benchmark</title>
    <updated>2025-04-21T14:48:01+00:00</updated>
    <author>
      <name>/u/rorowhat</name>
      <uri>https://old.reddit.com/user/rorowhat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does Ollama have a benchmark tool similar to llama.cpp(llama-bench)? I looked at the docs, but nothing jumped out. Maybe I missed it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rorowhat"&gt; /u/rorowhat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4fcwj/builtin_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4fcwj/builtin_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k4fcwj/builtin_benchmark/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T14:48:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4orc5</id>
    <title>MHKetbi/ nvidia_Llama-3.3-Nemotron-Super-49B-v1</title>
    <updated>2025-04-21T21:10:09+00:00</updated>
    <author>
      <name>/u/Timziito</name>
      <uri>https://old.reddit.com/user/Timziito</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This Model keep crashing my Ollama docker.. what am i doing wrong i got 48gb vram..&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/MHKetbi"&gt;MHKetbi&lt;/a&gt;/&lt;a href="https://ollama.com/MHKetbi/nvidia_Llama-3.3-Nemotron-Super-49B-v1"&gt;nvidia_Llama-3.3-Nemotron-Super-49B-v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Timziito"&gt; /u/Timziito &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4orc5/mhketbi_nvidia_llama33nemotronsuper49bv1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4orc5/mhketbi_nvidia_llama33nemotronsuper49bv1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k4orc5/mhketbi_nvidia_llama33nemotronsuper49bv1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T21:10:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4f2bl</id>
    <title>Is there a good way to pass JSON input instead of raw text ?</title>
    <updated>2025-04-21T14:35:39+00:00</updated>
    <author>
      <name>/u/Unique-Algae-1145</name>
      <uri>https://old.reddit.com/user/Unique-Algae-1145</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want the input to be a JSON because I want to pass multiple paramaters (~5-10) but writing them into a sentence the model has some issues and often either ignores or sometimes replies in the format back (but not consistently enough to extract) or sees it as raw text. If possible I would like to pass a very similar format to the structured output.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unique-Algae-1145"&gt; /u/Unique-Algae-1145 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4f2bl/is_there_a_good_way_to_pass_json_input_instead_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4f2bl/is_there_a_good_way_to_pass_json_input_instead_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k4f2bl/is_there_a_good_way_to_pass_json_input_instead_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T14:35:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1k45btm</id>
    <title>Ollama vs Docker Model Runner - Which One Should You Use?</title>
    <updated>2025-04-21T04:44:00+00:00</updated>
    <author>
      <name>/u/Arindam_200</name>
      <uri>https://old.reddit.com/user/Arindam_200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been exploring local LLM runners lately and wanted to share a quick comparison of two popular options: &lt;strong&gt;Docker Model Runner&lt;/strong&gt; and &lt;strong&gt;Ollama&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;If you're deciding between them, here’s a no-fluff breakdown based on dev experience, API support, hardware compatibility, and more:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Dev Workflow Integration&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Docker Model Runner:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Feels native if you’re already living in Docker-land.&lt;/li&gt; &lt;li&gt;Models are packaged as OCI artifacts and distributed via Docker Hub.&lt;/li&gt; &lt;li&gt;Works seamlessly with Docker Desktop as part of a bigger dev environment.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ollama:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Super lightweight and easy to set up.&lt;/li&gt; &lt;li&gt;Works as a standalone tool, no Docker needed.&lt;/li&gt; &lt;li&gt;Great for folks who want to skip the container overhead.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;Model Availability &amp;amp; Customisation&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Docker Model Runner:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Offers pre-packaged models through a dedicated AI namespace on Docker Hub.&lt;/li&gt; &lt;li&gt;Customization isn’t a big focus (yet), more plug-and-play with trusted sources.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ollama:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tons of models are readily available.&lt;/li&gt; &lt;li&gt;Built for tinkering: Model files let you customize and fine-tune behavior.&lt;/li&gt; &lt;li&gt;Also supports importing &lt;code&gt;GGUF&lt;/code&gt; and &lt;code&gt;Safetensors&lt;/code&gt; formats.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;API &amp;amp; Integrations&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Docker Model Runner:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Offers OpenAI-compatible API (great if you’re porting from the cloud).&lt;/li&gt; &lt;li&gt;Access via Docker flow using a Unix socket or TCP endpoint.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ollama:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Super simple REST API for generation, chat, embeddings, etc.&lt;/li&gt; &lt;li&gt;Has OpenAI-compatible APIs.&lt;/li&gt; &lt;li&gt;Big ecosystem of language SDKs (Python, JS, Go… you name it).&lt;/li&gt; &lt;li&gt;Popular with LangChain, LlamaIndex, and community-built UIs.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;Performance &amp;amp; Platform Support&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Docker Model Runner:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Optimized for Apple Silicon (macOS).&lt;/li&gt; &lt;li&gt;GPU acceleration via Apple Metal.&lt;/li&gt; &lt;li&gt;Windows support (with NVIDIA GPU) is coming in April 2025.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ollama:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cross-platform: Works on macOS, Linux, and Windows.&lt;/li&gt; &lt;li&gt;Built on &lt;code&gt;llama.cpp&lt;/code&gt;, tuned for performance.&lt;/li&gt; &lt;li&gt;Well-documented hardware requirements.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;Community &amp;amp; Ecosystem&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Docker Model Runner:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Still new, but growing fast thanks to Docker’s enterprise backing.&lt;/li&gt; &lt;li&gt;Strong on standards (OCI), great for model versioning and portability.&lt;/li&gt; &lt;li&gt;Good choice for orgs already using Docker.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ollama:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Established open-source project with a huge community.&lt;/li&gt; &lt;li&gt;200+ third-party integrations.&lt;/li&gt; &lt;li&gt;Active Discord, GitHub, Reddit, and more.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;-&amp;gt; TL;DR – Which One Should You Pick?&lt;/p&gt; &lt;p&gt;Go with Docker Model Runner if:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You’re already deep into Docker.&lt;/li&gt; &lt;li&gt;You want OpenAI API compatibility.&lt;/li&gt; &lt;li&gt;You care about standardization and container-based workflows.&lt;/li&gt; &lt;li&gt;You’re on macOS (Apple Silicon).&lt;/li&gt; &lt;li&gt;You need a solution with enterprise vibes.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Go with Ollama if:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You want a standalone tool with minimal setup.&lt;/li&gt; &lt;li&gt;You love customizing models and tweaking behaviors.&lt;/li&gt; &lt;li&gt;You need community plugins or multimodal support.&lt;/li&gt; &lt;li&gt;You’re using LangChain or LlamaIndex.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;BTW, I made a video on how to use Docker Model Runner step-by-step,&lt;/strong&gt; might help if you’re just starting out or curious about trying it: &lt;a href="https://www.youtube.com/watch?v=RH_vdF2iGdo"&gt;Watch Now&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you’re using and why!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arindam_200"&gt; /u/Arindam_200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k45btm/ollama_vs_docker_model_runner_which_one_should/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k45btm/ollama_vs_docker_model_runner_which_one_should/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k45btm/ollama_vs_docker_model_runner_which_one_should/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T04:44:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4d2n4</id>
    <title>Which ollama model would you choose for chatbot ?</title>
    <updated>2025-04-21T13:07:14+00:00</updated>
    <author>
      <name>/u/Effective_Budget7594</name>
      <uri>https://old.reddit.com/user/Effective_Budget7594</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have to create a chatbot with ollama in Msty. I am using llama3.1:8b with mxbai-embed-large. I am giving to the model markdown files with the instructions and the answers that it should give to the questions and also the questions and how to solve problems. The chatbot has to solve customers questions like: how to vinculate the device with the phone or general questions like how much it's cost. Sometimes, the model invents the response even if I put in prompt to use only the files that I give. Could someone give some advices, models, parameters to improve it ? Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective_Budget7594"&gt; /u/Effective_Budget7594 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4d2n4/which_ollama_model_would_you_choose_for_chatbot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4d2n4/which_ollama_model_would_you_choose_for_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k4d2n4/which_ollama_model_would_you_choose_for_chatbot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T13:07:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4aibo</id>
    <title>Are there any good LLMs with 1B or fewer parameters for RAG models?</title>
    <updated>2025-04-21T10:45:49+00:00</updated>
    <author>
      <name>/u/armodrilo10</name>
      <uri>https://old.reddit.com/user/armodrilo10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;br /&gt; I'm working on building a RAG model and I'm aiming to keep it under 1B parameters. The context document I’ll be working with is fairly small, only about 100-200 lines so I don’t need a massive model (like a 4B or 7B parameter model).&lt;/p&gt; &lt;p&gt;Additionally, I’m looking to host the model for free, so keeping it under 1B is a must. Does anyone know of any good LLMs with 1B parameters or fewer that would work well for this kind of use case? If there’s a platform or space where I can compare smaller models, I’d appreciate that info as well!&lt;/p&gt; &lt;p&gt;Thanks in advance for any suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/armodrilo10"&gt; /u/armodrilo10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4aibo/are_there_any_good_llms_with_1b_or_fewer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4aibo/are_there_any_good_llms_with_1b_or_fewer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k4aibo/are_there_any_good_llms_with_1b_or_fewer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T10:45:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4apac</id>
    <title>Why Gemma3-4b QAT from ollama website uses twice a much memory versus GGUF</title>
    <updated>2025-04-21T10:58:15+00:00</updated>
    <author>
      <name>/u/ShineNo147</name>
      <uri>https://old.reddit.com/user/ShineNo147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k4apac/why_gemma34b_qat_from_ollama_website_uses_twice_a/"&gt; &lt;img alt="Why Gemma3-4b QAT from ollama website uses twice a much memory versus GGUF" src="https://b.thumbs.redditmedia.com/jS-ZHEQjDn6MrQ7gy5Yd2f_IGmgbzjtW6gZAr08LSAU.jpg" title="Why Gemma3-4b QAT from ollama website uses twice a much memory versus GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay let me rephrase my question Why Gemma3-4b QAT from ollama uses twice a much ram versus GGUF ?&lt;/p&gt; &lt;p&gt;I used ollama run gemma3:4b-it-qat and ollama run hf.co/lmstudio-community/gemma-3-4B-it-qat-GGUF:latest.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sdv0kj3v56we1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34bb0bb13d7fea900ae31c4f2ee9832f16c13a13"&gt;https://preview.redd.it/sdv0kj3v56we1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34bb0bb13d7fea900ae31c4f2ee9832f16c13a13&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dscp7iiw56we1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0bdadadab6290febbf2b1400f69de9b97206688d"&gt;https://preview.redd.it/dscp7iiw56we1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0bdadadab6290febbf2b1400f69de9b97206688d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShineNo147"&gt; /u/ShineNo147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4apac/why_gemma34b_qat_from_ollama_website_uses_twice_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4apac/why_gemma34b_qat_from_ollama_website_uses_twice_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k4apac/why_gemma34b_qat_from_ollama_website_uses_twice_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T10:58:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k49h4s</id>
    <title>Why ollama Gemma3:4b QAT uses almost 6GB Memory when LM studio google GGUF uses around 3GB</title>
    <updated>2025-04-21T09:35:47+00:00</updated>
    <author>
      <name>/u/ShineNo147</name>
      <uri>https://old.reddit.com/user/ShineNo147</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;As question above &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShineNo147"&gt; /u/ShineNo147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k49h4s/why_ollama_gemma34b_qat_uses_almost_6gb_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k49h4s/why_ollama_gemma34b_qat_uses_almost_6gb_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k49h4s/why_ollama_gemma34b_qat_uses_almost_6gb_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T09:35:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4wlry</id>
    <title>(openshift) - ollama model directory is empty in openshift but podman model directory is ok.</title>
    <updated>2025-04-22T03:23:15+00:00</updated>
    <author>
      <name>/u/Shot_Shallot7446</name>
      <uri>https://old.reddit.com/user/Shot_Shallot7446</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to deploy ollama on openshift in the closed network environment. &lt;/p&gt; &lt;p&gt;I created pulled model ollama for the usage. &lt;/p&gt; &lt;p&gt;podman works well but when I deploy the image to the openshift, model directory is emptry. Is this normal? &lt;/p&gt; &lt;p&gt;Here is my dockerfile: &lt;/p&gt; &lt;p&gt;FROM ollama/ollama&lt;/p&gt; &lt;p&gt;ENV OLLAMA_MODELS=/.ollama/models&lt;/p&gt; &lt;p&gt;RUN ollama serve &amp;amp; server=$! ; sleep 2 ; ollama pull llama3.2&lt;/p&gt; &lt;p&gt;ENTRYPOINT [ &amp;quot;/bin/bash&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;(sleep 2 ; ) &amp;amp; exec /bin/ollama $0&amp;quot; ]&lt;/p&gt; &lt;p&gt;CMD [ &amp;quot;serve&amp;quot; ]&lt;/p&gt; &lt;p&gt;~&lt;/p&gt; &lt;p&gt;podman works find with &amp;quot;ollama list &amp;quot;&lt;/p&gt; &lt;p&gt;However when this image is deployed to the openshift:&lt;/p&gt; &lt;p&gt;1000720000@ollamamodel-69945bd659-pkpgf:/.ollama/models/manifests$ exit&lt;/p&gt; &lt;p&gt;exit&lt;/p&gt; &lt;p&gt;[root@bastion doy]# oc exec -it ollamamodel-69945bd659-pkpgf -- bash&lt;/p&gt; &lt;p&gt;groups: cannot find name for group ID 1000720000&lt;/p&gt; &lt;p&gt;1000720000@ollamamodel-69945bd659-pkpgf:/$ ls -al /.ollama/models/manifests/*&lt;/p&gt; &lt;p&gt;ls: cannot access '/.ollama/models/manifests/*': No such file or directory&lt;/p&gt; &lt;p&gt;1000720000@ollamamodel-69945bd659-pkpgf:/$ ls -al /.ollama/models/manifests/&lt;/p&gt; &lt;p&gt;total 0&lt;/p&gt; &lt;p&gt;drwxr-sr-x. 2 1000720000 1000720000 0 Apr 22 03:00 .&lt;/p&gt; &lt;p&gt;drwxrwsr-x. 4 1000720000 1000720000 2 Apr 22 03:00 ..&lt;/p&gt; &lt;p&gt;1000720000@ollamamodel-69945bd659-pkpgf:/$&lt;/p&gt; &lt;p&gt;$ podman exec -it 1d2f43e64693 bash&lt;/p&gt; &lt;p&gt;1d2f43e64693 localhost/ollamamodel:latest serve 2 hours ago Up About an hour ollamamodel&lt;/p&gt; &lt;p&gt;[root@bastion doy]# podman exec -it 1d2f43e64693 bash&lt;/p&gt; &lt;p&gt;root@1d2f43e64693:/# ls /.ollama/models/manifests/&lt;/p&gt; &lt;p&gt;&lt;a href="http://registry.ollama.ai"&gt;registry.ollama.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;----&lt;/p&gt; &lt;p&gt;Is there anyone who was successful with pulled model ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shot_Shallot7446"&gt; /u/Shot_Shallot7446 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4wlry/openshift_ollama_model_directory_is_empty_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4wlry/openshift_ollama_model_directory_is_empty_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k4wlry/openshift_ollama_model_directory_is_empty_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-22T03:23:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4z2to</id>
    <title>MCP client for ollama</title>
    <updated>2025-04-22T05:50:16+00:00</updated>
    <author>
      <name>/u/slow-dash</name>
      <uri>https://old.reddit.com/user/slow-dash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/mihirrd/ollama-mcp-client"&gt;https://github.com/mihirrd/ollama-mcp-client&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slow-dash"&gt; /u/slow-dash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4z2to/mcp_client_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4z2to/mcp_client_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k4z2to/mcp_client_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-22T05:50:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4voqw</id>
    <title>I uploaded GLM-4-32B-0414 to ollama</title>
    <updated>2025-04-22T02:34:30+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.ollama.com/JollyLlama/GLM-4-32B-0414-Q4_K_M"&gt;https://www.ollama.com/JollyLlama/GLM-4-32B-0414-Q4_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama run JollyLlama/GLM-4-32B-0414-Q4_K_M&lt;/code&gt;&lt;/p&gt; &lt;h3&gt;&lt;strong&gt;&lt;em&gt;This model requires Ollama v0.6.6 or later.&lt;/em&gt;&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama/releases"&gt;https://github.com/ollama/ollama/releases&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Update:&lt;/p&gt; &lt;p&gt;Z1 reasoning model: &lt;/p&gt; &lt;p&gt;ollama run JollyLlama/GLM-Z1-32B-0414-Q4_K_M&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4voqw/i_uploaded_glm432b0414_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4voqw/i_uploaded_glm432b0414_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k4voqw/i_uploaded_glm432b0414_to_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-22T02:34:30+00:00</published>
  </entry>
</feed>
