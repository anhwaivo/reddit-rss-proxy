<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-12T21:05:36+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1in2z0h</id>
    <title>Compiling v0.5.8</title>
    <updated>2025-02-11T16:44:39+00:00</updated>
    <author>
      <name>/u/wahnsinnwanscene</name>
      <uri>https://old.reddit.com/user/wahnsinnwanscene</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to compile from source for v0.5.8 without avx2, avxnni , etc. I'm using gcc-14 but have gcc-9 as default. How do i disable avx2 etc. I tried cmake with gcc-14 but it hits the spot where it tries to compile for cpu flags that my processor doesn't have. Doesn't the build process detect the available flags? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wahnsinnwanscene"&gt; /u/wahnsinnwanscene &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1in2z0h/compiling_v058/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1in2z0h/compiling_v058/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1in2z0h/compiling_v058/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T16:44:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1imr1sd</id>
    <title>How to use Ollama and Open WebUI with Docker Compose [Part 4]</title>
    <updated>2025-02-11T05:06:54+00:00</updated>
    <author>
      <name>/u/geshan</name>
      <uri>https://old.reddit.com/user/geshan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1imr1sd/how_to_use_ollama_and_open_webui_with_docker/"&gt; &lt;img alt="How to use Ollama and Open WebUI with Docker Compose [Part 4]" src="https://external-preview.redd.it/Kjq3eLkn3NANokIJh38gy7x7eTf4VewbLYl79de-hAU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5bca5b0db8bb18176f2f6fcc5a037096d8d51af" title="How to use Ollama and Open WebUI with Docker Compose [Part 4]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geshan"&gt; /u/geshan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://geshan.com.np/blog/2025/02/ollama-docker-compose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imr1sd/how_to_use_ollama_and_open_webui_with_docker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1imr1sd/how_to_use_ollama_and_open_webui_with_docker/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T05:06:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1in2v80</id>
    <title>ollama WSL will not use GPU</title>
    <updated>2025-02-11T16:40:14+00:00</updated>
    <author>
      <name>/u/Beli_Mawrr</name>
      <uri>https://old.reddit.com/user/Beli_Mawrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I have ollama (llama_cpp_python) installed on my WSL. I am able to use nvidia-smi and nvcc, but for some reason all my layers are running on the CPU and take ages. Any idea what's going on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beli_Mawrr"&gt; /u/Beli_Mawrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1in2v80/ollama_wsl_will_not_use_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1in2v80/ollama_wsl_will_not_use_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1in2v80/ollama_wsl_will_not_use_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T16:40:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1imz722</id>
    <title>My app uses Mistral Small more than any other app on OpenRouter!</title>
    <updated>2025-02-11T14:00:30+00:00</updated>
    <author>
      <name>/u/No-Definition-2886</name>
      <uri>https://old.reddit.com/user/No-Definition-2886</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1imz722/my_app_uses_mistral_small_more_than_any_other_app/"&gt; &lt;img alt="My app uses Mistral Small more than any other app on OpenRouter!" src="https://preview.redd.it/jkxi43umniie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44e5277b1b2fd0b3a304702d6de7e37a77b94699" title="My app uses Mistral Small more than any other app on OpenRouter!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Definition-2886"&gt; /u/No-Definition-2886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jkxi43umniie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imz722/my_app_uses_mistral_small_more_than_any_other_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1imz722/my_app_uses_mistral_small_more_than_any_other_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T14:00:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1indim5</id>
    <title>Help! RAGAS with Ollama – Output Parser Failed &amp; Timeout Errors</title>
    <updated>2025-02-12T00:01:52+00:00</updated>
    <author>
      <name>/u/Repulsive-Diet-9322</name>
      <uri>https://old.reddit.com/user/Repulsive-Diet-9322</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to use &lt;strong&gt;RAGAS with Ollama&lt;/strong&gt; and keep running into frustrating errors.&lt;/p&gt; &lt;p&gt;I followed this tutorial: &lt;a href="https://www.youtube.com/watch?v=Ts2wDG6OEko&amp;amp;t=287s"&gt;https://www.youtube.com/watch?v=Ts2wDG6OEko&amp;amp;t=287s&lt;/a&gt;&lt;br /&gt; I also made sure my dataset is in the correct &lt;strong&gt;RAGAS format&lt;/strong&gt; and followed the documentation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Strangely, it works with the example dataset from the video and the one in the documentation, but not with my data.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;No matter what I try, I keep getting this error:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries. Prompt fix output format failed to parse output: The output parser failed to parse the output including retries. Prompt fix output format failed to parse output: The output parser failed to parse the output including retries. Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries. Exception raised in Job[8]: RagasOutputParserException(The output parser failed to parse the output including retries.)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And this happens &lt;strong&gt;for every metric&lt;/strong&gt;, not just one.&lt;/p&gt; &lt;p&gt;After a while, it just turns into:&lt;/p&gt; &lt;p&gt;&lt;code&gt;TimeoutError()&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I've spent &lt;strong&gt;3 days&lt;/strong&gt; trying to debug this, but I can't figure it out.&lt;br /&gt; Is anyone else facing this issue?&lt;br /&gt; Did you manage to fix it?&lt;br /&gt; I'd really appreciate any help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Repulsive-Diet-9322"&gt; /u/Repulsive-Diet-9322 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1indim5/help_ragas_with_ollama_output_parser_failed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1indim5/help_ragas_with_ollama_output_parser_failed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1indim5/help_ragas_with_ollama_output_parser_failed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T00:01:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1imqd0y</id>
    <title>Did ollama update and get faster?</title>
    <updated>2025-02-11T04:27:53+00:00</updated>
    <author>
      <name>/u/Logical-Egg</name>
      <uri>https://old.reddit.com/user/Logical-Egg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m running all the normal models and I swear they’re like 5 times faster. Even the bigger models are flying. Did I miss something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Logical-Egg"&gt; /u/Logical-Egg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imqd0y/did_ollama_update_and_get_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1imqd0y/did_ollama_update_and_get_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1imqd0y/did_ollama_update_and_get_faster/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T04:27:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ink671</id>
    <title>How can I run Ollama on windows (wsl2 ??) With openwebUi?</title>
    <updated>2025-02-12T05:52:29+00:00</updated>
    <author>
      <name>/u/Raners96</name>
      <uri>https://old.reddit.com/user/Raners96</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How can I run Ollama on windows (wsl2 ??) With openwebUi? Well i tried a few things but nothing worked. it did run but only on CPU. I have a 7900xtx. And I want to access OpenwebUi over the LAN,. Can someone help me?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Raners96"&gt; /u/Raners96 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ink671/how_can_i_run_ollama_on_windows_wsl2_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ink671/how_can_i_run_ollama_on_windows_wsl2_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ink671/how_can_i_run_ollama_on_windows_wsl2_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T05:52:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1inl37e</id>
    <title>Help in choosing right tool for help in academic writing.</title>
    <updated>2025-02-12T06:51:58+00:00</updated>
    <author>
      <name>/u/Fun_Repeat_3791</name>
      <uri>https://old.reddit.com/user/Fun_Repeat_3791</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I am very new to the world of large language models. I have recently joined as an assistant professor at a fairly renowned university. As part of my job, I have to do lots of writing such as grants, concept notes, conference and journal papers, class notes, etc. It is gradually becoming overwhelming. I was wondering if i can somehow utilise the large language models to help me. What I need. 1.Helper in writing my papers, grants in some parts which are common such as introduction, definitions, etc. 2. I have a fairly large corpus of my own writings such as my own papers, grants etc. sometimes it is just rehashing my old ideas into new. If I can get a tool. that can do this will be very helpful. &lt;/p&gt; &lt;p&gt;what I have 1. i can arrange large servers, large ram, gpu, etc for my work 2. i prefer open source tools but i can spend some initial amount around 200 USD. If it s recurring cost then it should not be more than 100 USD yearly. Can you please suggest me some tools that can be helpful for my issues? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun_Repeat_3791"&gt; /u/Fun_Repeat_3791 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inl37e/help_in_choosing_right_tool_for_help_in_academic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inl37e/help_in_choosing_right_tool_for_help_in_academic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inl37e/help_in_choosing_right_tool_for_help_in_academic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T06:51:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ina9uj</id>
    <title>Quickly deploy Ollama on the most affordable GPUs on the market</title>
    <updated>2025-02-11T21:41:08+00:00</updated>
    <author>
      <name>/u/Dylan-from-Shadeform</name>
      <uri>https://old.reddit.com/user/Dylan-from-Shadeform</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We made a template on our platform, Shadeform, to quickly deploy Ollama on the most affordable cloud GPUs on the market.&lt;/p&gt; &lt;p&gt;For context, Shadeform is a GPU marketplace for cloud providers like Lambda, Paperspace, Nebius, Datacrunch and more that lets you compare their on-demand pricing and spin up with one account.&lt;/p&gt; &lt;p&gt;This Ollama template lets you pre-load Ollama onto any of these instances, so it's ready to go as soon as the instance is active.&lt;/p&gt; &lt;p&gt;Takes &amp;lt; 5 min and works like butter.&lt;/p&gt; &lt;p&gt;Here's how it works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Follow &lt;a href="https://platform.shadeform.ai/templates/a1aaa5e1-d1ec-42ed-9261-ed69778cfa5a"&gt;this link&lt;/a&gt; to the Ollama template.&lt;/li&gt; &lt;li&gt;Click &amp;quot;Deploy Template&amp;quot;&lt;/li&gt; &lt;li&gt;Pick a GPU type&lt;/li&gt; &lt;li&gt;Pick the lowest priced listing&lt;/li&gt; &lt;li&gt;Click &amp;quot;Deploy&amp;quot;&lt;/li&gt; &lt;li&gt;Wait for the instance to become active&lt;/li&gt; &lt;li&gt;Download your private key and SSH&lt;/li&gt; &lt;li&gt;Run this command, and swap out the {model_name} with whatever you want&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker exec -it ollama ollama pull {model_name} &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Paste &lt;a href="http://localhost:8080"&gt;&lt;code&gt;http://localhost:8080&lt;/code&gt;&lt;/a&gt; into your browser&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dylan-from-Shadeform"&gt; /u/Dylan-from-Shadeform &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ina9uj/quickly_deploy_ollama_on_the_most_affordable_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ina9uj/quickly_deploy_ollama_on_the_most_affordable_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ina9uj/quickly_deploy_ollama_on_the_most_affordable_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T21:41:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1inv6m9</id>
    <title>Guys, I think my Deepseek is malfunctioning</title>
    <updated>2025-02-12T16:40:08+00:00</updated>
    <author>
      <name>/u/naqezis</name>
      <uri>https://old.reddit.com/user/naqezis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1inv6m9/guys_i_think_my_deepseek_is_malfunctioning/"&gt; &lt;img alt="Guys, I think my Deepseek is malfunctioning" src="https://preview.redd.it/kwv36exykqie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=158920e57ef8bb58edf955ab6490bff54a8c3174" title="Guys, I think my Deepseek is malfunctioning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/naqezis"&gt; /u/naqezis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kwv36exykqie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inv6m9/guys_i_think_my_deepseek_is_malfunctioning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inv6m9/guys_i_think_my_deepseek_is_malfunctioning/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T16:40:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1in88nw</id>
    <title>How many Ollama models can I have on my list.. but just running one at a time. That are 7b and I have 16 GB of RAM.. I run the Ollama via WSL. I have two models but wondering if I can fit several but just use one at a time..</title>
    <updated>2025-02-11T20:17:36+00:00</updated>
    <author>
      <name>/u/Emergency-Radish-696</name>
      <uri>https://old.reddit.com/user/Emergency-Radish-696</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emergency-Radish-696"&gt; /u/Emergency-Radish-696 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1in88nw/how_many_ollama_models_can_i_have_on_my_list_but/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1in88nw/how_many_ollama_models_can_i_have_on_my_list_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1in88nw/how_many_ollama_models_can_i_have_on_my_list_but/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T20:17:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1inb3pe</id>
    <title>Ollama spitting out gibberish on Windows 10 with RTX 3060. Only returning @ 'at' symbols to any and all prompts. How do I fix it?</title>
    <updated>2025-02-11T22:15:34+00:00</updated>
    <author>
      <name>/u/shittywhopper</name>
      <uri>https://old.reddit.com/user/shittywhopper</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shittywhopper"&gt; /u/shittywhopper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imgur.com/a/CErnNdv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inb3pe/ollama_spitting_out_gibberish_on_windows_10_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inb3pe/ollama_spitting_out_gibberish_on_windows_10_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T22:15:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1inmmba</id>
    <title>Trying to setup Scourhead (an ai that can search the web) with Ollama but does not seem to work</title>
    <updated>2025-02-12T08:44:00+00:00</updated>
    <author>
      <name>/u/Medo1024</name>
      <uri>https://old.reddit.com/user/Medo1024</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to setup the app scourhead on my laptop (windows) and after download it says it needs Ollama and wants to download it, when i click on download it gives me a message that says 'scourhead was unable to download the model from Ollama, please insure Ollama is running, that the host and port are correct, and the model name is valid, then try again.' I checked the settings for the download and this is it 'Ollama Host: localhost OllamaPort: 11434 Model: llama3.2:3b. Pls help (ps: tried to download ollama and then restart the scourhead app but it still did not work)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Medo1024"&gt; /u/Medo1024 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inmmba/trying_to_setup_scourhead_an_ai_that_can_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inmmba/trying_to_setup_scourhead_an_ai_that_can_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inmmba/trying_to_setup_scourhead_an_ai_that_can_search/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T08:44:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1inofh0</id>
    <title>How to deploy deepseek-r1∶671b locally using Ollama?</title>
    <updated>2025-02-12T11:04:46+00:00</updated>
    <author>
      <name>/u/U2509</name>
      <uri>https://old.reddit.com/user/U2509</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 8 A100, each with 40GB video memory, and 1TB of RAM. How to deploy deepseek-r1∶671b locally? I cannot load the model using the video memory alone. Is there any parameter that Ollama can configure to load the model using my 1TB of RAM? thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/U2509"&gt; /u/U2509 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inofh0/how_to_deploy_deepseekr1671b_locally_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inofh0/how_to_deploy_deepseekr1671b_locally_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inofh0/how_to_deploy_deepseekr1671b_locally_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T11:04:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1inw25m</id>
    <title>Ide.py</title>
    <updated>2025-02-12T17:15:01+00:00</updated>
    <author>
      <name>/u/GentReviews</name>
      <uri>https://old.reddit.com/user/GentReviews</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Made a cool community project with the goal of making an interesting ollama based agentic tool using cli -based on aider, and other similar tools &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/unaveragetech/IDE.OLLAMA"&gt;https://github.com/unaveragetech/IDE.OLLAMA&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GentReviews"&gt; /u/GentReviews &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inw25m/idepy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inw25m/idepy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inw25m/idepy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T17:15:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1inw5t1</id>
    <title>Actions to query an llm</title>
    <updated>2025-02-12T17:19:01+00:00</updated>
    <author>
      <name>/u/GentReviews</name>
      <uri>https://old.reddit.com/user/GentReviews</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/unaveragetech/Gitbot"&gt;https://github.com/unaveragetech/Gitbot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Made a fun lil tool to allow anyone to ask a one shot question to an llm using GitHub codespaces, actions and a lil creativity &lt;/p&gt; &lt;p&gt;The readme explains how to use it This was made in 24hr for a small project so I’m open to changes that can be made if you have ideas &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GentReviews"&gt; /u/GentReviews &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inw5t1/actions_to_query_an_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inw5t1/actions_to_query_an_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inw5t1/actions_to_query_an_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T17:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1inyagc</id>
    <title>Environment variables on Windows 11</title>
    <updated>2025-02-12T18:44:15+00:00</updated>
    <author>
      <name>/u/buddy1616</name>
      <uri>https://old.reddit.com/user/buddy1616</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm running into an issue getting Ollama to respect environment variables on Windows 11. Running it on my Windows 10 machine, everything works fine, but the same setup gets ignored on 11. Trying to set the OLLAMA_MODELS folder, the OLLAMA_KEEP_ALIVE and the OLLAMA_HOST value. On my laptop running windows 10 they get honored after you run Ollama from the start bar (but oddly enough running it from CMD just defaults everything). On the windows 11 machine, I can't get it to pull them in at all. Any ideas or known issues with Ollama on Windows 11?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/buddy1616"&gt; /u/buddy1616 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inyagc/environment_variables_on_windows_11/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inyagc/environment_variables_on_windows_11/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inyagc/environment_variables_on_windows_11/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T18:44:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1invo23</id>
    <title>Best Model for Text Understanding</title>
    <updated>2025-02-12T16:59:43+00:00</updated>
    <author>
      <name>/u/KhoteSikke</name>
      <uri>https://old.reddit.com/user/KhoteSikke</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! What are some good models in Ollama for Text understanding. Basically understading a text and generating a JSON. Preferably similar to llama3 instruct. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KhoteSikke"&gt; /u/KhoteSikke &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1invo23/best_model_for_text_understanding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1invo23/best_model_for_text_understanding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1invo23/best_model_for_text_understanding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T16:59:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1io1hjo</id>
    <title>URL to screenshots server for your self-hosted AI projects (MIT license)</title>
    <updated>2025-02-12T20:54:56+00:00</updated>
    <author>
      <name>/u/gkamer8</name>
      <uri>https://old.reddit.com/user/gkamer8</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1io1hjo/url_to_screenshots_server_for_your_selfhosted_ai/"&gt; &lt;img alt="URL to screenshots server for your self-hosted AI projects (MIT license)" src="https://external-preview.redd.it/k5m2RHvy9bfmZbcB7x2-uONoP8GvfJMHaFfCG-IoXcw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3ba3c756d03047423ca7edf67a7a613af44994a" title="URL to screenshots server for your self-hosted AI projects (MIT license)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gkamer8"&gt; /u/gkamer8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/goodreasonai/ScrapeServ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1io1hjo/url_to_screenshots_server_for_your_selfhosted_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1io1hjo/url_to_screenshots_server_for_your_selfhosted_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T20:54:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1incg27</id>
    <title>One-liner RAG with Ollama</title>
    <updated>2025-02-11T23:13:25+00:00</updated>
    <author>
      <name>/u/yusufcanbayrak</name>
      <uri>https://old.reddit.com/user/yusufcanbayrak</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1incg27/oneliner_rag_with_ollama/"&gt; &lt;img alt="One-liner RAG with Ollama" src="https://external-preview.redd.it/aZg7arYZxu_ozI5IITwuT0FrG_0ip5ZXROM-WLJsfoU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe965c43d4c500c5565b2da8db64a4f480271700" title="One-liner RAG with Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've created tlm almost a year ago as an experimental project for CLI assistance. Now, introduce another feature that can be beneficial and more natural to use for RAG with open-source models using Ollama.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/yusufcanb/tlm/releases/tag/1.2"&gt;Release 1.2 · yusufcanb/tlm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/s3nufytdelie1.gif"&gt;tlm ask&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yusufcanbayrak"&gt; /u/yusufcanbayrak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1incg27/oneliner_rag_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1incg27/oneliner_rag_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1incg27/oneliner_rag_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T23:13:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1inqxnl</id>
    <title>I want to introduce Telemarketers to Ollama - Anyone else do this yet?</title>
    <updated>2025-02-12T13:36:22+00:00</updated>
    <author>
      <name>/u/Comfortable_Ad_8117</name>
      <uri>https://old.reddit.com/user/Comfortable_Ad_8117</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I get upwards of 15 Telemarketing calls a day from people who want me to buy home solar, small business loans, sell my house, donate to the fraternal order of the police and more. &lt;/p&gt; &lt;p&gt;I would love to have the local LLM answer the phone - convert speech to text, generate a response then text back to speech in close to real time. I'm not sure if this is even possible, let alone is my hardware capable. &lt;/p&gt; &lt;p&gt;I have a decent Ryzen 7 64GB with a pair of RTX 3060's 12GB &lt;/p&gt; &lt;p&gt;Has anyone done this before? &lt;/p&gt; &lt;p&gt;How do you get the PC to answer the phone? I'm assuming you have to forward the calls to some kind of Google Voice number or some VOIP service with an API that can pickup the call?&lt;/p&gt; &lt;p&gt;If you can get the PC to answer the phone what would be used to handle the STT and TTS aspect and be fast enough? &lt;/p&gt; &lt;p&gt;I would love to hear from someone who has attempted this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable_Ad_8117"&gt; /u/Comfortable_Ad_8117 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inqxnl/i_want_to_introduce_telemarketers_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inqxnl/i_want_to_introduce_telemarketers_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inqxnl/i_want_to_introduce_telemarketers_to_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T13:36:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1inpm0l</id>
    <title>1-Click AI Tools in your browser - completely free to use with Ollama</title>
    <updated>2025-02-12T12:26:23+00:00</updated>
    <author>
      <name>/u/rajatrocks</name>
      <uri>https://old.reddit.com/user/rajatrocks</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1inpm0l/1click_ai_tools_in_your_browser_completely_free/"&gt; &lt;img alt="1-Click AI Tools in your browser - completely free to use with Ollama" src="https://external-preview.redd.it/Kc2hxFEu0Tcm7R53gZtdKq4h35EjhCxOfec1FTNUVPw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6b5d82b208e81c0db81ed49a653153e474089379" title="1-Click AI Tools in your browser - completely free to use with Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there - I built a Chrome/Edge extension called Ask Steve: &lt;a href="https://asksteve.to/"&gt;https://asksteve.to&lt;/a&gt; that gives you 1-Click AI Tools in your browser (along with Chat and several other integration points).&lt;/p&gt; &lt;p&gt;I recently added the ability to connect to local models for free and it works great with Ollama! Detailed instructions are here: &lt;a href="https://www.asksteve.to/docs/local-models"&gt;https://www.asksteve.to/docs/local-models&lt;/a&gt; - it does require a bit of additional config at startup to enable an extension to connect to Ollama's local server.&lt;/p&gt; &lt;p&gt;You can also assign specific models to Tools - so you can use a fast model like Phi for everyday Tools, and something like DeepSeek R1 for something that would benefit from a reasoning model.&lt;/p&gt; &lt;p&gt;If you get a chance to try it out, I'd welcome any feedback!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1inpm0l/video/li2i506cbpie1/player"&gt;Connect Ask Steve to Ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;0:00 - 1:18 Intro &amp;amp; Initial setup&lt;br /&gt; 2:26 - 3:10 Connect Ollama&lt;br /&gt; 4:00 - 5:56 Testing &amp;amp; assigning a specific model to a specific Tool&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rajatrocks"&gt; /u/rajatrocks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inpm0l/1click_ai_tools_in_your_browser_completely_free/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inpm0l/1click_ai_tools_in_your_browser_completely_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inpm0l/1click_ai_tools_in_your_browser_completely_free/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T12:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ingiis</id>
    <title>GitHub Actions + Ollama = Free Compute</title>
    <updated>2025-02-12T02:27:09+00:00</updated>
    <author>
      <name>/u/Silent-Treat-6512</name>
      <uri>https://old.reddit.com/user/Silent-Treat-6512</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you guys do when you are bored? I created a simple AI bot which runs a full Ollama stack in Github Actions (free compute), pulls mistral model and ask for &amp;quot;some deep insight&amp;quot; this website now gets updated EVERY HOUR (Changed it to Daily) - Cost to run $0 &lt;/p&gt; &lt;p&gt;&lt;a href="https://ai.aww.sm/"&gt;https://ai.aww.sm/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Full code on GitHub, link on website. Let me know your thoughts.&lt;/p&gt; &lt;p&gt;It’s currently tasked to generate thoughts around Humans vs AI dominance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silent-Treat-6512"&gt; /u/Silent-Treat-6512 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ingiis/github_actions_ollama_free_compute/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ingiis/github_actions_ollama_free_compute/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ingiis/github_actions_ollama_free_compute/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T02:27:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1inp06h</id>
    <title>AMD 395 can run llama 70B without GPU</title>
    <updated>2025-02-12T11:51:44+00:00</updated>
    <author>
      <name>/u/grigio</name>
      <uri>https://old.reddit.com/user/grigio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Non enough but a good start&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/AMDGPU_/status/1889588690214637747"&gt;https://x.com/AMDGPU_/status/1889588690214637747&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grigio"&gt; /u/grigio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inp06h/amd_395_can_run_llama_70b_without_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inp06h/amd_395_can_run_llama_70b_without_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inp06h/amd_395_can_run_llama_70b_without_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T11:51:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1inx1k8</id>
    <title>Ollama on mini PC Intel Ultra 5</title>
    <updated>2025-02-12T17:54:47+00:00</updated>
    <author>
      <name>/u/Parenormale</name>
      <uri>https://old.reddit.com/user/Parenormale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1inx1k8/ollama_on_mini_pc_intel_ultra_5/"&gt; &lt;img alt="Ollama on mini PC Intel Ultra 5" src="https://preview.redd.it/corfx7mcyqie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e5d130a0a99c468aaeed36474580d1582b3e45e" title="Ollama on mini PC Intel Ultra 5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;with arc and ipex-llm I feel like an alien in the AI ​​llm context I spent €600 it's mini it consumes 50w it flies and it's precise, here I published all my tests with the various language models &lt;/p&gt; &lt;p&gt;I think the performance is great for this little GPU accelerated PC.&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtube.com/@onlyrottami?si=MSYffeaGo0axCwh9"&gt;https://youtube.com/@onlyrottami?si=MSYffeaGo0axCwh9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Parenormale"&gt; /u/Parenormale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/corfx7mcyqie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inx1k8/ollama_on_mini_pc_intel_ultra_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inx1k8/ollama_on_mini_pc_intel_ultra_5/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T17:54:47+00:00</published>
  </entry>
</feed>
