<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-14T15:35:27+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ja5bu5</id>
    <title>Mantella Mod on Skyrim</title>
    <updated>2025-03-13T05:57:56+00:00</updated>
    <author>
      <name>/u/kolimin231</name>
      <uri>https://old.reddit.com/user/kolimin231</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw that ollama supports the openai api spec, however when I target the url to &lt;a href="http://localhost:11374/v1"&gt;http://localhost:11374/v1&lt;/a&gt; with Mantella, it doesn't work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kolimin231"&gt; /u/kolimin231 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ja5bu5/mantella_mod_on_skyrim/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ja5bu5/mantella_mod_on_skyrim/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ja5bu5/mantella_mod_on_skyrim/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-13T05:57:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ja0yhn</id>
    <title>Has anyone tried TinyZero repo for reproducing deepseek distilled models?</title>
    <updated>2025-03-13T01:45:50+00:00</updated>
    <author>
      <name>/u/AntiqueMud6263</name>
      <uri>https://old.reddit.com/user/AntiqueMud6263</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Jiayi-Pan/TinyZero"&gt;https://github.com/Jiayi-Pan/TinyZero&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AntiqueMud6263"&gt; /u/AntiqueMud6263 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ja0yhn/has_anyone_tried_tinyzero_repo_for_reproducing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ja0yhn/has_anyone_tried_tinyzero_repo_for_reproducing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ja0yhn/has_anyone_tried_tinyzero_repo_for_reproducing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-13T01:45:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9ia22</id>
    <title>gemma3:12b vs phi4:14b vs..</title>
    <updated>2025-03-12T11:53:24+00:00</updated>
    <author>
      <name>/u/grigio</name>
      <uri>https://old.reddit.com/user/grigio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried some preliminary benchmarks with gemma3 but it seems phi4 is still superior. What is your under 14b preferred model? &lt;/p&gt; &lt;p&gt;UPDATE: gemma3:12b run in llamacpp is more accurate than the default in ollama, please run it following these tweaks: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grigio"&gt; /u/grigio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9ia22/gemma312b_vs_phi414b_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9ia22/gemma312b_vs_phi414b_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j9ia22/gemma312b_vs_phi414b_vs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-12T11:53:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9lqyz</id>
    <title>OpenArc 1.0.2: OpenAI endpoints, OpenWebUI support! Get faster inference from Intel CPUs, GPUs and NPUs now with community tooling</title>
    <updated>2025-03-12T14:50:07+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;Today I am launching &lt;a href="https://github.com/SearchSavior/OpenArc/tree/main"&gt;OpenArc&lt;/a&gt; 1.0.2 with fully supported OpenWebUI functionality! &lt;/p&gt; &lt;p&gt;Nailing OpenAI compatibility so early in OpenArc's development positions the project to mature with community tooling as Intel releases more hardware, expands support for NPU devices, smaller models become more performant and as we evolve past the Transformer to whatever comes next. &lt;/p&gt; &lt;p&gt;I plan to use OpenArc as a development tool for my work projects which require acceleration for other types of ML beyond LLMs- embeddings, classifiers, OCR with Paddle. Frontier models can't do everything with enough accuracy and are not silver bullets&lt;/p&gt; &lt;p&gt;The repo details how to get OpenWebUI setup; for now it is the only chat front-end I have time to maintain. If you have other tools you wanted to see integrated open an issue or submit a pull request. &lt;/p&gt; &lt;p&gt;What's up next :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Confirm openai support for other implementations like smolagents, Autogen&lt;/li&gt; &lt;li&gt;&lt;p&gt;Move from conda to uv. This week I was enlightened and will never go back to conda.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Vision support for Qwen2-VL, Qwen2.5-VL, Phi-4 multi-modal, olmOCR (which is qwen2vl 7b tune) InternVL2 and probably more&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;An official &lt;a href="https://discord.gg/PnuTBVcr"&gt;Discord!&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Best way to reach me.&lt;/li&gt; &lt;li&gt;If you are interested in contributing join the Discord!&lt;/li&gt; &lt;li&gt;If you need help converting models &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Discussions on GitHub for:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SearchSavior/OpenArc/discussions/11"&gt;Linux Drivers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SearchSavior/OpenArc/discussions/12"&gt;Windows Drivers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SearchSavior/OpenArc/discussions/13"&gt;Environment Setup&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Instructions and models for testing out text generation for &lt;a href="https://github.com/SearchSavior/OpenArc/issues/14"&gt;NPU devices&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;A sister repo, &lt;a href="https://github.com/SearchSavior/OpenArcProjects"&gt;OpenArcProjects&lt;/a&gt;!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Share the things you build with OpenArc, OpenVINO, oneapi toolkit, IPEX-LLM and future tooling from Intel&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for checking out OpenArc. I hope it ends up being a useful tool.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9lqyz/openarc_102_openai_endpoints_openwebui_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9lqyz/openarc_102_openai_endpoints_openwebui_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j9lqyz/openarc_102_openai_endpoints_openwebui_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-12T14:50:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9u6q5</id>
    <title>Using Ollama with smolagents</title>
    <updated>2025-03-12T20:35:41+00:00</updated>
    <author>
      <name>/u/Sufficient_Life8866</name>
      <uri>https://old.reddit.com/user/Sufficient_Life8866</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just thought I would post this here for others who may be looking where to start with using local models with smolagents. As someone who spent 30 mins looking for documentation or instructions on how to use an Ollama local model with smolagents, here is how to do it.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Download your model (I am using Qwen 14B in this example)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Initialize a LiteLLMModel instance with the model ID as 'ollama_chat/&amp;lt;YOUR MODEL&amp;gt;'&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Input the model instance as the model being used for the agent&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;That's it, code example below. Hopefully this saves at least 1 person some time.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from smolagents import CodeAgent, DuckDuckGoSearchTool, LiteLLMModel model = LiteLLMModel( model_id='ollama_chat/qwen2.5:14b' ) agent = CodeAgent(tools=[DuckDuckGoSearchTool()], model=model) agent.run(&amp;quot;How many seconds would it take for a leopard at full speed to run through Pont des Arts?&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sufficient_Life8866"&gt; /u/Sufficient_Life8866 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9u6q5/using_ollama_with_smolagents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9u6q5/using_ollama_with_smolagents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j9u6q5/using_ollama_with_smolagents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-12T20:35:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ja8w5j</id>
    <title>What is MCP? (Model Context Protocol) - A Primer</title>
    <updated>2025-03-13T10:30:46+00:00</updated>
    <author>
      <name>/u/db-master</name>
      <uri>https://old.reddit.com/user/db-master</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/db-master"&gt; /u/db-master &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.whatismcp.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ja8w5j/what_is_mcp_model_context_protocol_a_primer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ja8w5j/what_is_mcp_model_context_protocol_a_primer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-13T10:30:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ja2c8i</id>
    <title>AI Code Fusion: A tool to optimize your code for LLM contexts - packs files, counts tokens, and filters content</title>
    <updated>2025-03-13T02:56:29+00:00</updated>
    <author>
      <name>/u/coding_workflow</name>
      <uri>https://old.reddit.com/user/coding_workflow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Small tool I made. I had the same as CLI (may release it) but mainly allows you to pack your code in one file, if you need to manually upload it, filter it, see how many tokens to optimize the context.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/codingworkflow/ai-code-fusion"&gt;https://github.com/codingworkflow/ai-code-fusion&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coding_workflow"&gt; /u/coding_workflow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ja2c8i/ai_code_fusion_a_tool_to_optimize_your_code_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ja2c8i/ai_code_fusion_a_tool_to_optimize_your_code_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ja2c8i/ai_code_fusion_a_tool_to_optimize_your_code_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-13T02:56:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9yls7</id>
    <title>DataBridge + Ollama: Rules-Based Parsing with Your Models</title>
    <updated>2025-03-12T23:52:43+00:00</updated>
    <author>
      <name>/u/yes-no-maybe_idk</name>
      <uri>https://old.reddit.com/user/yes-no-maybe_idk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;! We’ve been talking with a bunch of developers lately, and a common issue keeps coming up: extracting structured information, doing PII redaction, and custom processing in your pipelines without extra overhead. DataBridge’s rules-based parsing handles just that—it preprocesses your docs before they reach your local models. You can use any Ollama model to assist with the parsing logic. We’ve found the smallest DeepSeek Coder model gets the job done: small footprint, solid results. It supports PII redaction, metadata extraction, or custom adjustments, defined in plain English or schemas. Details in this article: &lt;a href="https://databridge.mintlify.app/concepts/rules-processing"&gt;DataBridge Rules Processing&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;New to DataBridge? DataBridge ingests anything (text, PDFs, images, videos, etc.) and retrieves anything, with traceable sources. It’s multi-modal and works with your Ollama setup. For context, we’ve got a naive RAG write-up—its limits and how rules improve it—here: &lt;a href="https://databridge.mintlify.app/concepts/naive-rag"&gt;Naive RAG Explained&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;We’re also starting a Discord: &lt;a href="https://discord.gg/GNcHAwnc"&gt;DataBridge Discord&lt;/a&gt; for chats about integrations or Ollama tweaks, pls join if you have thoughts/ suggestions/ issues! &lt;/p&gt; &lt;p&gt;Our repo’s here: &lt;a href="https://github.com/databridge-org/databridge-core"&gt;https://github.com/databridge-org/databridge-core&lt;/a&gt;—drop a ⭐ if it’s useful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yes-no-maybe_idk"&gt; /u/yes-no-maybe_idk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9yls7/databridge_ollama_rulesbased_parsing_with_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9yls7/databridge_ollama_rulesbased_parsing_with_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j9yls7/databridge_ollama_rulesbased_parsing_with_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-12T23:52:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9gic5</id>
    <title>Ollama 0.6 with support for Google Gemma 3</title>
    <updated>2025-03-12T09:57:13+00:00</updated>
    <author>
      <name>/u/jmorganca</name>
      <uri>https://old.reddit.com/user/jmorganca</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j9gic5/ollama_06_with_support_for_google_gemma_3/"&gt; &lt;img alt="Ollama 0.6 with support for Google Gemma 3" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="Ollama 0.6 with support for Google Gemma 3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jmorganca"&gt; /u/jmorganca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/library/gemma3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9gic5/ollama_06_with_support_for_google_gemma_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j9gic5/ollama_06_with_support_for_google_gemma_3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-12T09:57:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jac9jk</id>
    <title>HELP: Context length problems</title>
    <updated>2025-03-13T13:43:29+00:00</updated>
    <author>
      <name>/u/Broad-Extension-9588</name>
      <uri>https://old.reddit.com/user/Broad-Extension-9588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jac9jk/help_context_length_problems/"&gt; &lt;img alt="HELP: Context length problems" src="https://b.thumbs.redditmedia.com/i6sv-_tn4J_z_9V7-FArQp8JGIpGtW-uRPPAUszZBBA.jpg" title="HELP: Context length problems" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was experimenting with the new Gemma 3 model, but I’m unable to modify its context length. Even when creating a new version from the Modelfile, the context length remains at the original 8192 tokens.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m04sa0gengoe1.png?width=505&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=803265eea558938b4b75ff9f99adcd6a6940fe6d"&gt;https://preview.redd.it/m04sa0gengoe1.png?width=505&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=803265eea558938b4b75ff9f99adcd6a6940fe6d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Broad-Extension-9588"&gt; /u/Broad-Extension-9588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jac9jk/help_context_length_problems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jac9jk/help_context_length_problems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jac9jk/help_context_length_problems/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-13T13:43:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jacsh0</id>
    <title>Alternative for Msty</title>
    <updated>2025-03-13T14:07:51+00:00</updated>
    <author>
      <name>/u/Responsible-Tart-964</name>
      <uri>https://old.reddit.com/user/Responsible-Tart-964</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to try other app. Because my Msty kinda stuck. Any recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible-Tart-964"&gt; /u/Responsible-Tart-964 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jacsh0/alternative_for_msty/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jacsh0/alternative_for_msty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jacsh0/alternative_for_msty/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-13T14:07:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9uxlr</id>
    <title>New Google Gemma3 Inference speeds on Macbook Pro M4 Max</title>
    <updated>2025-03-12T21:12:28+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma3 by Google is the newest model that is beating some full sized models including Deepseek V3 in the benchmarks right now. I decided to run all variations of it on my Macbook and share the performance results! I included AliBaba's QwQ and Microsoft's Phi4 results for comparison.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware&lt;/strong&gt;: Macbook Pro M4 Max 16 Core CPU / 40 Core GPU with 128 GB RAM&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt&lt;/strong&gt;: Write a 500 word story&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results (All models downloaded from Ollama)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;gemma3:27b&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Load Duration&lt;/th&gt; &lt;th align="left"&gt;Inference Speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;q4&lt;/td&gt; &lt;td align="left"&gt;52.482042ms&lt;/td&gt; &lt;td align="left"&gt;22.06 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;fp16&lt;/td&gt; &lt;td align="left"&gt;56.4445ms&lt;/td&gt; &lt;td align="left"&gt;6.99 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;gemma3:12b&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Load Duration&lt;/th&gt; &lt;th align="left"&gt;Inference Speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;q4&lt;/td&gt; &lt;td align="left"&gt;56.818334ms&lt;/td&gt; &lt;td align="left"&gt;43.82 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;fp16&lt;/td&gt; &lt;td align="left"&gt;54.133375ms&lt;/td&gt; &lt;td align="left"&gt;17.99 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;gemma3:4b&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Load Duration&lt;/th&gt; &lt;th align="left"&gt;Inference Speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;q4&lt;/td&gt; &lt;td align="left"&gt;57.751042ms&lt;/td&gt; &lt;td align="left"&gt;98.90 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;fp16&lt;/td&gt; &lt;td align="left"&gt;55.584083ms&lt;/td&gt; &lt;td align="left"&gt;48.72 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;gemma3:1b&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Load Duration&lt;/th&gt; &lt;th align="left"&gt;Inference Speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;q4&lt;/td&gt; &lt;td align="left"&gt;55.116083ms&lt;/td&gt; &lt;td align="left"&gt;184.62 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;fp16&lt;/td&gt; &lt;td align="left"&gt;55.034792ms&lt;/td&gt; &lt;td align="left"&gt;135.31 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;phi4:14b&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Load Duration&lt;/th&gt; &lt;th align="left"&gt;Inference Speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;q4&lt;/td&gt; &lt;td align="left"&gt;25.423792ms&lt;/td&gt; &lt;td align="left"&gt;38.18 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;q8&lt;/td&gt; &lt;td align="left"&gt;14.756459ms&lt;/td&gt; &lt;td align="left"&gt;27.29 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;qwq:32b&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Load Duration&lt;/th&gt; &lt;th align="left"&gt;Inference Speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;q4&lt;/td&gt; &lt;td align="left"&gt;31.056208ms&lt;/td&gt; &lt;td align="left"&gt;17.90 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Seems like load duration is very fast and consistent regardless of the model size&lt;/li&gt; &lt;li&gt;Based on the results, I'm eyeing to further test the q4 for the 27b model and fp16 for the 12b model. Although they're not super fast, they might be good enough for my use cases&lt;/li&gt; &lt;li&gt;I believe you can expect similar performance results if you purchase the Mac Studio M4 Max with 128 GB RAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9uxlr/new_google_gemma3_inference_speeds_on_macbook_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9uxlr/new_google_gemma3_inference_speeds_on_macbook_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j9uxlr/new_google_gemma3_inference_speeds_on_macbook_pro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-12T21:12:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jakaup</id>
    <title>Ollama running on Ubuntu Server - systemd service problem</title>
    <updated>2025-03-13T19:22:24+00:00</updated>
    <author>
      <name>/u/ubiquities</name>
      <uri>https://old.reddit.com/user/ubiquities</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm reaching out because I'm pretty sure I'm stumbling everywhere but on the answer that is right in front of me. And brain fried to the point that I probably won't see the answer even if its right in front of me.&lt;/p&gt; &lt;p&gt;System: Ubuntu Server 24.04 LTS&lt;/p&gt; &lt;p&gt;How it started: for some reason Ollama stopped picking up my GPU and started running CPU only, looking at &lt;code&gt;systemctl status ollama&lt;/code&gt; I was getting some GPU timeout errors and the service was stopping. All strange, so I decided that the best option would be to wipe it and run a fresh install, it had been while since I updated so probably for the best. I was getting the same problems after reinstalling from the install script, so I wiped again and did a manual install. &lt;/p&gt; &lt;p&gt;How its going: If I run Ollama serve in one terminal, then everything works as expected on another terminal, I can run models, &lt;code&gt;ollama ps / ollama -v&lt;/code&gt; give expected results, everything is fine until I close the stop the terminal running &lt;code&gt;ollama serve&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;code&gt;systemctl status ollama&lt;/code&gt; shows ollama.service enabled, active and running, additionally I can see the processes running /usr/bin/ollama serve under the user ollama when I run ntop, but when I then run &lt;code&gt;ollama -v&lt;/code&gt; or &lt;code&gt;ollama ps&lt;/code&gt; I get this response:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Warning: could not connect to a running Ollama instance&lt;br /&gt; Warning: client version is 0.6.0&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;If I open a new terminal run &lt;code&gt;ollama serve&lt;/code&gt; everything goes back to working, and I can see additional processing running under my username in ntop. &lt;/p&gt; &lt;p&gt;For some reason it seems like &lt;code&gt;ollama serve&lt;/code&gt; when run by user ollama is just not being recognized. &lt;/p&gt; &lt;p&gt;If anyone can see what I'm missing, I'd appreciate some guidance.&lt;/p&gt; &lt;p&gt;Cheers,&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ubiquities"&gt; /u/ubiquities &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jakaup/ollama_running_on_ubuntu_server_systemd_service/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jakaup/ollama_running_on_ubuntu_server_systemd_service/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jakaup/ollama_running_on_ubuntu_server_systemd_service/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-13T19:22:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaca6u</id>
    <title>Why is Ollama not using my GPU on Windows 11?</title>
    <updated>2025-03-13T13:44:21+00:00</updated>
    <author>
      <name>/u/Sad-Mixture6393</name>
      <uri>https://old.reddit.com/user/Sad-Mixture6393</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jaca6u/why_is_ollama_not_using_my_gpu_on_windows_11/"&gt; &lt;img alt="Why is Ollama not using my GPU on Windows 11?" src="https://b.thumbs.redditmedia.com/7B12AW0esYRDghKvJcnYG4cKPboObPbT-qCqQdpJ3-k.jpg" title="Why is Ollama not using my GPU on Windows 11?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I have issues running Ollama on a Windows system (Shadow PC, Cloud gaming PC)&lt;br /&gt; Would be glad to have some hints what might be the issue.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r67dkyz5lgoe1.png?width=364&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5ec5323d85c9d2d6a5b097dc7a96833fbd885ab2"&gt;https://preview.redd.it/r67dkyz5lgoe1.png?width=364&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5ec5323d85c9d2d6a5b097dc7a96833fbd885ab2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m7ojcug3jgoe1.png?width=877&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6d02b9700faacd78743ca36c52ebb7d9ad4d10ba"&gt;https://preview.redd.it/m7ojcug3jgoe1.png?width=877&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6d02b9700faacd78743ca36c52ebb7d9ad4d10ba&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z15pwfd5jgoe1.png?width=679&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8331e453a3efb53ee9599563037c7bb43fb9fcfb"&gt;https://preview.redd.it/z15pwfd5jgoe1.png?width=679&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8331e453a3efb53ee9599563037c7bb43fb9fcfb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tfrjo30jjgoe1.png?width=1117&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94923e90745c67aa440895c575335fa342cfa7a1"&gt;https://preview.redd.it/tfrjo30jjgoe1.png?width=1117&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94923e90745c67aa440895c575335fa342cfa7a1&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;2025/03/12 23:26:29 routes.go:1225: INFO server config env=&amp;quot;map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:true OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\Charlotte\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]&amp;quot; time=2025-03-12T23:26:29.059+01:00 level=INFO source=images.go:432 msg=&amp;quot;total blobs: 5&amp;quot; time=2025-03-12T23:26:29.060+01:00 level=INFO source=images.go:439 msg=&amp;quot;total unused blobs removed: 0&amp;quot; time=2025-03-12T23:26:29.061+01:00 level=INFO source=routes.go:1292 msg=&amp;quot;Listening on 127.0.0.1:11434 (version 0.6.0)&amp;quot; time=2025-03-12T23:26:29.061+01:00 level=DEBUG source=sched.go:106 msg=&amp;quot;starting llm scheduler&amp;quot; time=2025-03-12T23:26:29.061+01:00 level=INFO source=gpu.go:217 msg=&amp;quot;looking for compatible GPUs&amp;quot; time=2025-03-12T23:26:29.061+01:00 level=INFO source=gpu_windows.go:167 msg=packages count=1 time=2025-03-12T23:26:29.061+01:00 level=INFO source=gpu_windows.go:214 msg=&amp;quot;&amp;quot; package=0 cores=4 efficiency=0 threads=8 time=2025-03-12T23:26:29.061+01:00 level=DEBUG source=gpu.go:98 msg=&amp;quot;searching for GPU discovery libraries for NVIDIA&amp;quot; time=2025-03-12T23:26:29.061+01:00 level=DEBUG source=gpu.go:501 msg=&amp;quot;Searching for GPU library&amp;quot; name=nvml.dll time=2025-03-12T23:26:29.062+01:00 level=DEBUG source=gpu.go:525 msg=&amp;quot;gpu library search&amp;quot; globs=&amp;quot;[C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\nvml.dll C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\bin\\nvml.dll C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\libnvvp\\nvml.dll C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin\\nvml.dll C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\libnvvp\\nvml.dll C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath\\nvml.dll C:\\WINDOWS\\system32\\nvml.dll C:\\WINDOWS\\nvml.dll C:\\WINDOWS\\System32\\Wbem\\nvml.dll C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\nvml.dll C:\\WINDOWS\\System32\\OpenSSH\\nvml.dll C:\\Program Files\\MATLAB\\R2023b\\bin\\nvml.dll C:\\Program Files\\Git\\cmd\\nvml.dll C:\\Program Files\\MiKTeX\\miktex\\bin\\x64\\nvml.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\\nvml.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python311\\nvml.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\nvml.dll C:\\Users\\Charlotte\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\nvml.dll C:\\Program Files\\CMake\\bin\\nvml.dll C:\\Program Files (x86)\\libccd\\include\\nvml.dll C:\\Program Files (x86)\\libccd\\bin\\nvml.dll C:\\Program Files (x86)\\libccd\\lib\\nvml.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python311\\python3.exe\\nvml.dll C:\\Program Files\\Pandoc\\nvml.dll C:\\Program Files\\Docker\\Docker\\resources\\bin\\nvml.dll C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common\\nvml.dll C:\\Program Files\\dotnet\\nvml.dll C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2025.1.1\\nvml.dll C:\\ProgramData\\chocolatey\\bin\\nvml.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python38-32\\Scripts\\nvml.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python38-32\\nvml.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts\\nvml.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python37-32\\nvml.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python36-32\\Scripts\\nvml.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python36-32\\nvml.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Microsoft VS Code\\bin\\nvml.dll C:\\Strawberry\\perl\\bin\\perl.exe\\nvml.dll C:\\Users\\Charlotte\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\\nvml.dll C:\\Users\\Charlotte\\AppData\\Local\\gitkraken\\bin\\nvml.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\cursor\\resources\\app\\bin\\nvml.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Ollama\\nvml.dll c:\\Windows\\System32\\nvml.dll]&amp;quot; time=2025-03-12T23:26:29.065+01:00 level=DEBUG source=gpu.go:529 msg=&amp;quot;skipping PhysX cuda library path&amp;quot; path=&amp;quot;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common\\nvml.dll&amp;quot; time=2025-03-12T23:26:29.068+01:00 level=DEBUG source=gpu.go:558 msg=&amp;quot;discovered GPU libraries&amp;quot; paths=&amp;quot;[C:\\WINDOWS\\system32\\nvml.dll c:\\Windows\\System32\\nvml.dll]&amp;quot; time=2025-03-12T23:26:29.093+01:00 level=DEBUG source=gpu.go:111 msg=&amp;quot;nvidia-ml loaded&amp;quot; library=C:\WINDOWS\system32\nvml.dll time=2025-03-12T23:26:29.093+01:00 level=DEBUG source=gpu.go:501 msg=&amp;quot;Searching for GPU library&amp;quot; name=nvcuda.dll time=2025-03-12T23:26:29.093+01:00 level=DEBUG source=gpu.go:525 msg=&amp;quot;gpu library search&amp;quot; globs=&amp;quot;[C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\nvcuda.dll C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\bin\\nvcuda.dll C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\libnvvp\\nvcuda.dll C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin\\nvcuda.dll C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\libnvvp\\nvcuda.dll C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath\\nvcuda.dll C:\\WINDOWS\\system32\\nvcuda.dll C:\\WINDOWS\\nvcuda.dll C:\\WINDOWS\\System32\\Wbem\\nvcuda.dll C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\nvcuda.dll C:\\WINDOWS\\System32\\OpenSSH\\nvcuda.dll C:\\Program Files\\MATLAB\\R2023b\\bin\\nvcuda.dll C:\\Program Files\\Git\\cmd\\nvcuda.dll C:\\Program Files\\MiKTeX\\miktex\\bin\\x64\\nvcuda.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\\nvcuda.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python311\\nvcuda.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\nvcuda.dll C:\\Users\\Charlotte\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\nvcuda.dll C:\\Program Files\\CMake\\bin\\nvcuda.dll C:\\Program Files (x86)\\libccd\\include\\nvcuda.dll C:\\Program Files (x86)\\libccd\\bin\\nvcuda.dll C:\\Program Files (x86)\\libccd\\lib\\nvcuda.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python311\\python3.exe\\nvcuda.dll C:\\Program Files\\Pandoc\\nvcuda.dll C:\\Program Files\\Docker\\Docker\\resources\\bin\\nvcuda.dll C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common\\nvcuda.dll C:\\Program Files\\dotnet\\nvcuda.dll C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2025.1.1\\nvcuda.dll C:\\ProgramData\\chocolatey\\bin\\nvcuda.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python38-32\\Scripts\\nvcuda.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python38-32\\nvcuda.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts\\nvcuda.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python37-32\\nvcuda.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python36-32\\Scripts\\nvcuda.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python36-32\\nvcuda.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Microsoft VS Code\\bin\\nvcuda.dll C:\\Strawberry\\perl\\bin\\perl.exe\\nvcuda.dll C:\\Users\\Charlotte\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\\nvcuda.dll C:\\Users\\Charlotte\\AppData\\Local\\gitkraken\\bin\\nvcuda.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\cursor\\resources\\app\\bin\\nvcuda.dll C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Ollama\\nvcuda.dll c:\\windows\\system*\\nvcuda.dll]&amp;quot; time=2025-03-12T23:26:29.097+01:00 level=DEBUG source=gpu.go:529 msg=&amp;quot;skipping PhysX cuda library path&amp;quot; path=&amp;quot;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common\\nvcuda.dll&amp;quot; time=2025-03-12T23:26:29.099+01:00 level=DEBUG source=gpu.go:558 msg=&amp;quot;discovered GPU libraries&amp;quot; paths=[C:\WINDOWS\system32\nvcuda.dll] initializing C:\WINDOWS\system32\nvcuda.dll dlsym: cuInit - 00007FFF8C435F80 dlsym: cuDriverGetVersion - 00007FFF8C436020 dlsym: cuDeviceGetCount - 00007FFF8C436816 dlsym: cuDeviceGet - 00007FFF8C436810 dlsym: cuDeviceGetAttribute - 00007FFF8C436170 dlsym: cuDeviceGetUuid - 00007FFF8C436822 dlsym: cuDeviceGetName - 00007FFF8C43681C dlsym: cuCtxCreate_v3 - 00007FFF8C436894 dlsym: cuMemGetInfo_v2 - 00007FFF8C436996 dlsym: cuCtxDestroy - 00007FFF8C4368A6 calling cuInit calling cuDriverGetVersion raw version 0x2f30 CUDA driver version: 12.8 calling cuDeviceGetCount device count 1 time=2025-03-12T23:26:29.122+01:00 level=DEBUG source=gpu.go:125 msg=&amp;quot;detected GPUs&amp;quot; count=1 library=C:\WINDOWS\system32\nvcuda.dll [GPU-3ae28276-4acd-3466-0c50-485fd8cbe166] CUDA totalMem 19189 mb [GPU-3ae28276-4acd-3466-0c50-485fd8cbe166] CUDA freeMem 18038 mb [GPU-3ae28276-4acd-3466-0c50-485fd8cbe166] Compute Capability 8.6 time=2025-03-12T23:26:29.306+01:00 level=DEBUG source=amd_windows.go:34 msg=&amp;quot;unable to load amdhip64_6.dll, please make sure to upgrade to the latest amd driver: The file cannot be accessed by the system.&amp;quot; releasing cuda driver library releasing nvml library time=2025-03-12T23:26:29.306+01:00 level=INFO source=types.go:130 msg=&amp;quot;inference compute&amp;quot; id=GPU-3ae28276-4acd-3466-0c50-485fd8cbe166 library=cuda variant=v12 compute=8.6 driver=12.8 name=&amp;quot;NVIDIA RTX A4500&amp;quot; total=&amp;quot;18.7 GiB&amp;quot; available=&amp;quot;17.6 GiB&amp;quot; [GIN] 2025/03/12 - 23:26:29 | 200 | 0s | 127.0.0.1 | HEAD &amp;quot;/&amp;quot; [GIN] 2025/03/12 - 23:26:29 | 200 | 19.9972ms | 127.0.0.1 | POST &amp;quot;/api/show&amp;quot; time=2025-03-12T23:26:29.462+01:00 level=DEBUG source=gpu.go:391 msg=&amp;quot;updating system memory data&amp;quot; before.total=&amp;quot;28.0 GiB&amp;quot; before.free=&amp;quot;15.4 GiB&amp;quot; before.free_swap=&amp;quot;14.1 GiB&amp;quot; now.total=&amp;quot;28.0 GiB&amp;quot; now.free=&amp;quot;15.3 GiB&amp;quot; now.free_swap=&amp;quot;13.9 GiB&amp;quot; time=2025-03-12T23:26:29.472+01:00 level=DEBUG source=gpu.go:441 msg=&amp;quot;updating cuda memory data&amp;quot; gpu=GPU-3ae28276-4acd-3466-0c50-485fd8cbe166 name=&amp;quot;NVIDIA RTX A4500&amp;quot; overhead=&amp;quot;0 B&amp;quot; before.total=&amp;quot;18.7 GiB&amp;quot; before.free=&amp;quot;17.6 GiB&amp;quot; now.total=&amp;quot;18.7 GiB&amp;quot; now.free=&amp;quot;14.8 GiB&amp;quot; now.used=&amp;quot;3.9 GiB&amp;quot; releasing nvml library time=2025-03-12T23:26:29.473+01:00 level=DEBUG source=sched.go:182 msg=&amp;quot;updating default concurrency&amp;quot; OLLAMA_MAX_LOADED_MODELS=3 gpu_count=1 time=2025-03-12T23:26:29.502+01:00 level=DEBUG source=sched.go:225 msg=&amp;quot;loading first model&amp;quot; model=C:\Users\Charlotte\.ollama\models\blobs\sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc time=2025-03-12T23:26:29.502+01:00 level=DEBUG source=memory.go:108 msg=evaluating library=cuda gpu_count=1 available=&amp;quot;[14.8 GiB]&amp;quot; time=2025-03-12T23:26:29.502+01:00 level=WARN source=ggml.go:149 msg=&amp;quot;key not found&amp;quot; key=qwen2.attention.key_length default=128 time=2025-03-12T23:26:29.502+01:00 level=WARN source=ggml.go:149 msg=&amp;quot;key not found&amp;quot; key=qwen2.attention.value_length default=128 time=2025-03-12T23:26:29.502+01:00 level=INFO source=sched.go:715 msg=&amp;quot;new model will fit in available VRAM in single GPU, loading&amp;quot; model=C:\Users\Charlotte\.ollama\models\blobs\sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc gpu=GPU-3ae28276-4acd-3466-0c50-485fd8cbe166 parallel=4 available=15894798336 required=&amp;quot;1.9 GiB&amp;quot; time=2025-03-12T23:26:29.502+01:00 level=DEBUG source=gpu.go:391 msg=&amp;quot;updating system memory data&amp;quot; before.total=&amp;quot;28.0 GiB&amp;quot; before.free=&amp;quot;15.3 GiB&amp;quot; before.free_swap=&amp;quot;13.9 GiB&amp;quot; now.total=&amp;quot;28.0 GiB&amp;quot; now.free=&amp;quot;15.3 GiB&amp;quot; now.free_swap=&amp;quot;13.9 GiB&amp;quot; time=2025-03-12T23:26:29.519+01:00 level=DEBUG source=gpu.go:441 msg=&amp;quot;updating cuda memory data&amp;quot; gpu=GPU-3ae28276-4acd-3466-0c50-485fd8cbe166 name=&amp;quot;NVIDIA RTX A4500&amp;quot; overhead=&amp;quot;0 B&amp;quot; before.total=&amp;quot;18.7 GiB&amp;quot; before.free=&amp;quot;14.8 GiB&amp;quot; now.total=&amp;quot;18.7 GiB&amp;quot; now.free=&amp;quot;14.8 GiB&amp;quot; now.used=&amp;quot;3.9 GiB&amp;quot; releasing nvml library time=2025-03-12T23:26:29.519+01:00 level=INFO source=server.go:105 msg=&amp;quot;system memory&amp;quot; total=&amp;quot;28.0 GiB&amp;quot; free=&amp;quot;15.3 GiB&amp;quot; free_swap=&amp;quot;13.9 GiB&amp;quot; time=2025-03-12T23:26:29.520+01:00 level=DEBUG source=memory.go:108 msg=evaluating library=cuda gpu_count=1 available=&amp;quot;[14.8 GiB]&amp;quot; time=2025-03-12T23:26:29.520+01:00 level=WARN source=ggml.go:149 msg=&amp;quot;key not found&amp;quot; key=qwen2.attention.key_length default=128 time=2025-03-12T23:26:29.520+01:00 level=WARN source=ggml.go:149 msg=&amp;quot;key not found&amp;quot; key=qwen2.attention.value_length default=128 time=2025-03-12T23:26:29.520+01:00 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split=&amp;quot;&amp;quot; memory.available=&amp;quot;[14.8 GiB]&amp;quot; memory.gpu_overhead=&amp;quot;0 B&amp;quot; memory.required.full=&amp;quot;1.9 GiB&amp;quot; memory.required.partial=&amp;quot;1.9 GiB&amp;quot; memory.required.kv=&amp;quot;224.0 MiB&amp;quot; memory.required.allocations=&amp;quot;[1.9 GiB]&amp;quot; memory.weights.total=&amp;quot;976.1 MiB&amp;quot; memory.weights.repeating=&amp;quot;793.5 MiB&amp;quot; memory.weights.nonrepeating=&amp;quot;182.6 MiB&amp;quot; memory.graph.full=&amp;quot;299.8 MiB&amp;quot; memory.graph.partial=&amp;quot;482.3 MiB&amp;quot; time=2025-03-12T23:26:29.520+01:00 level=DEBUG source=server.go:262 msg=&amp;quot;compatible gpu libraries&amp;quot; compatible=&amp;quot;[cuda_v12 cuda_v11]&amp;quot; llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\Charlotte\.ollama\models\blobs\sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output. llama_model_loader: - kv 0: general.architecture str = qwen2 llama_model_loader: - kv 1: general.type str = model llama_model_loader: - kv 2: general.name str = DeepSeek R1 Distill Qwen 1.5B llama_model_loader: - kv 3: general.basename str = DeepSeek-R1-Distill-Qwen llama_model_loader: - kv 4: general.size_label str = 1.5B llama_model_loader: - kv 5: qwen2.block_count u32 = 28 llama_model_loader: - kv 6: qwen2.context_length u32 = 131072 llama_model_loader: - kv 7: qwen2.embedding_length u32 = 1536 llama_model_loader: - kv 8: qwen2.feed_forward_length u32 = 8960 llama_model_loader: - kv 9: qwen2.attention.head_count u32 = 12 llama_model_loader: - kv 10: qwen2.attention.head_count_kv u32 = 2 llama_model_loader: - kv 11: qwen2.rope.freq_base f32 = 10000.000000 llama_model_loader: - kv 12: qwen2.attention.layer_norm_rms_epsilon f32 = 0.000001 llama_model_loader: - kv 13: general.file_type u32 = 15 llama_model_loader: - kv 14: tokenizer.ggml.model str = gpt2 llama_model_loader: - kv 15: tokenizer.ggml.pre str = qwen2 llama_model_loader: - kv 16: tokenizer.ggml.tokens arr[str,151936] = [&amp;quot;!&amp;quot;, &amp;quot;\&amp;quot;&amp;quot;, &amp;quot;#&amp;quot;, &amp;quot;$&amp;quot;, &amp;quot;%&amp;quot;, &amp;quot;&amp;amp;&amp;quot;, &amp;quot;'&amp;quot;, ... llama_model_loader: - kv 17: tokenizer.ggml.token_type arr[i32,151936] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... llama_model_loader: - kv 18: tokenizer.ggml.merges arr[str,151387] = [&amp;quot;Ġ Ġ&amp;quot;, &amp;quot;ĠĠ ĠĠ&amp;quot;, &amp;quot;i n&amp;quot;, &amp;quot;Ġ t&amp;quot;,... llama_model_loader: - kv 19: tokenizer.ggml.bos_token_id u32 = 151646 llama_model_loader: - kv 20: tokenizer.ggml.eos_token_id u32 = 151643 llama_model_loader: - kv 21: tokenizer.ggml.padding_token_id u32 = 151643 llama_model_loader: - kv 22: tokenizer.ggml.add_bos_token bool = true llama_model_loader: - kv 23: tokenizer.ggml.add_eos_token bool = false llama_model_loader: - kv 24: tokenizer.chat_template str = {% if not add_generation_prompt is de... llama_model_loader: - kv 25: general.quantization_version u32 = 2 llama_model_loader: - type f32: 141 tensors llama_model_loader: - type q4_K: 169 tensors llama_model_loader: - type q6_K: 29 tensors print_info: file format = GGUF V3 (latest) print_info: file type = Q4_K - Medium print_info: file size = 1.04 GiB (5.00 BPW) init_tokenizer: initializing tokenizer for type 2 load: control token: 151659 '&amp;lt;|fim_prefix|&amp;gt;' is not marked as EOG load: control token: 151656 '&amp;lt;|video_pad|&amp;gt;' is not marked as EOG load: control token: 151655 '&amp;lt;|image_pad|&amp;gt;' is not marked as EOG load: control token: 151653 '&amp;lt;|vision_end|&amp;gt;' is not marked as EOG load: control token: 151652 '&amp;lt;|vision_start|&amp;gt;' is not marked as EOG load: control token: 151651 '&amp;lt;|quad_end|&amp;gt;' is not marked as EOG load: control token: 151646 '&amp;lt;｜begin▁of▁sentence｜&amp;gt;' is not marked as EOG load: control token: 151644 '&amp;lt;｜User｜&amp;gt;' is not marked as EOG load: control token: 151661 '&amp;lt;|fim_suffix|&amp;gt;' is not marked as EOG load: control token: 151660 '&amp;lt;|fim_middle|&amp;gt;' is not marked as EOG load: control token: 151654 '&amp;lt;|vision_pad|&amp;gt;' is not marked as EOG load: control token: 151650 '&amp;lt;|quad_start|&amp;gt;' is not marked as EOG load: control token: 151647 '&amp;lt;|EOT|&amp;gt;' is not marked as EOG load: control token: 151643 '&amp;lt;｜end▁of▁sentence｜&amp;gt;' is not marked as EOG load: control token: 151645 '&amp;lt;｜Assistant｜&amp;gt;' is not marked as EOG load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect load: special tokens cache size = 22 load: token to piece cache size = 0.9310 MB print_info: arch = qwen2 print_info: vocab_only = 1 print_info: model type = ?B print_info: model params = 1.78 B print_info: general.name = DeepSeek R1 Distill Qwen 1.5B print_info: vocab type = BPE print_info: n_vocab = 151936 print_info: n_merges = 151387 print_info: BOS token = 151646 '&amp;lt;｜begin▁of▁sentence｜&amp;gt;' print_info: EOS token = 151643 '&amp;lt;｜end▁of▁sentence｜&amp;gt;' print_info: EOT token = 151643 '&amp;lt;｜end▁of▁sentence｜&amp;gt;' print_info: PAD token = 151643 '&amp;lt;｜end▁of▁sentence｜&amp;gt;' print_info: LF token = 198 'Ċ' print_info: FIM PRE token = 151659 '&amp;lt;|fim_prefix|&amp;gt;' print_info: FIM SUF token = 151661 '&amp;lt;|fim_suffix|&amp;gt;' print_info: FIM MID token = 151660 '&amp;lt;|fim_middle|&amp;gt;' print_info: FIM PAD token = 151662 '&amp;lt;|fim_pad|&amp;gt;' print_info: FIM REP token = 151663 '&amp;lt;|repo_name|&amp;gt;' print_info: FIM SEP token = 151664 '&amp;lt;|file_sep|&amp;gt;' print_info: EOG token = 151643 '&amp;lt;｜end▁of▁sentence｜&amp;gt;' print_info: EOG token = 151662 '&amp;lt;|fim_pad|&amp;gt;' print_info: EOG token = 151663 '&amp;lt;|repo_name|&amp;gt;' print_info: EOG token = 151664 '&amp;lt;|file_sep|&amp;gt;' print_info: max token length = 256 llama_model_load: vocab only - skipping tensors time=2025-03-12T23:26:29.734+01:00 level=DEBUG source=server.go:335 msg=&amp;quot;adding gpu library&amp;quot; path=C:\Users\Charlotte\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12 time=2025-03-12T23:26:29.734+01:00 level=DEBUG source=server.go:343 msg=&amp;quot;adding gpu dependency paths&amp;quot; paths=[C:\Users\Charlotte\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12] time=2025-03-12T23:26:29.734+01:00 level=INFO source=server.go:405 msg=&amp;quot;starting llama server&amp;quot; cmd=&amp;quot;C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\Charlotte\\.ollama\\models\\blobs\\sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --verbose --threads 4 --no-mmap --parallel 4 --port 57127&amp;quot; time=2025-03-12T23:26:29.734+01:00 level=DEBUG source=server.go:423 msg=subprocess environment=&amp;quot;[CUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8 CUDA_PATH_V11_8=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8 CUDA_PATH_V12_8=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8 PATH=C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\libnvvp;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\libnvvp;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\MATLAB\\R2023b\\bin;C:\\Program Files\\Git\\cmd;C:\\Program Files\\MiKTeX\\miktex\\bin\\x64\\;C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python311\\python.exe;C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python311;C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python311\\Scripts;C:\\Users\\Charlotte\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython;C:\\Program Files\\CMake\\bin;C:\\Program Files (x86)\\libccd\\include;C:\\Program Files (x86)\\libccd\\bin;C:\\Program Files (x86)\\libccd\\lib;C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python311\\python3.exe;C:\\Program Files\\Pandoc\\;C:\\Program Files\\Docker\\Docker\\resources\\bin;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\dotnet\\;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2025.1.1\\;C:\\ProgramData\\chocolatey\\bin;C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python38-32\\Scripts\\;C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python38-32\\;C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts\\;C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python37-32\\;C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python36-32\\Scripts\\;C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Python\\Python36-32\\;C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Strawberry\\perl\\bin\\perl.exe;C:\\Users\\Charlotte\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe;C:\\Users\\Charlotte\\AppData\\Local\\gitkraken\\bin;C:\\Users\\Charlotte\\AppData\\Local\\Programs\\cursor\\resources\\app\\bin;C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Ollama;C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12;C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Ollama\\lib\\ollama CUDA_VISIBLE_DEVICES=GPU-3ae28276-4acd-3466-0c50-485fd8cbe166]&amp;quot; time=2025-03-12T23:26:29.739+01:00 level=INFO source=sched.go:450 msg=&amp;quot;loaded runners&amp;quot; count=1 time=2025-03-12T23:26:29.739+01:00 level=INFO source=server.go:585 msg=&amp;quot;waiting for llama runner to start responding&amp;quot; time=2025-03-12T23:26:29.739+01:00 level=INFO source=server.go:619 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server error&amp;quot; time=2025-03-12T23:26:29.770+01:00 level=INFO source=runner.go:931 msg=&amp;quot;starting go runner&amp;quot; time=2025-03-12T23:26:29.771+01:00 level=DEBUG source=ggml.go:99 msg=&amp;quot;ggml backend load all from path&amp;quot; path=C:\Users\Charlotte\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12 time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=&amp;quot;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\bin&amp;quot; time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=&amp;quot;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\\libnvvp&amp;quot; time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=&amp;quot;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin&amp;quot; time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=&amp;quot;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\libnvvp&amp;quot; time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=&amp;quot;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath&amp;quot; time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\WINDOWS\system32 time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\WINDOWS time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\WINDOWS\System32\Wbem time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\WINDOWS\System32\WindowsPowerShell\v1.0 time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\WINDOWS\System32\OpenSSH time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=&amp;quot;C:\\Program Files\\MATLAB\\R2023b\\bin&amp;quot; time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=&amp;quot;C:\\Program Files\\Git\\cmd&amp;quot; time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=&amp;quot;C:\\Program Files\\MiKTeX\\miktex\\bin\\x64&amp;quot; time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\Users\Charlotte\AppData\Local\Programs\Python\Python311\python.exe time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\Users\Charlotte\AppData\Local\Programs\Python\Python311 time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\Users\Charlotte\AppData\Local\Programs\Python\Python311\Scripts time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\Users\Charlotte\AppData\Roaming\Python\Python311\site-packages\IPython time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=&amp;quot;C:\\Program Files\\CMake\\bin&amp;quot; time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=&amp;quot;C:\\Program Files (x86)\\libccd\\include&amp;quot; time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=&amp;quot;C:\\Program Files (x86)\\libccd\\bin&amp;quot; time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=&amp;quot;C:\\Program Files (x86)\\libccd\\lib&amp;quot; time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\Users\Charlotte\AppData\Local\Programs\Python\Python311\python3.exe time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=&amp;quot;C:\\Program Files\\Pandoc&amp;quot; time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=&amp;quot;C:\\Program Files\\Docker\\Docker\\resources\\bin&amp;quot; time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=&amp;quot;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common&amp;quot; time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=&amp;quot;C:\\Program Files\\dotnet&amp;quot; time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=&amp;quot;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2025.1.1&amp;quot; time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\ProgramData\chocolatey\bin time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\Users\Charlotte\AppData\Local\Programs\Python\Python38-32\Scripts time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\Users\Charlotte\AppData\Local\Programs\Python\Python38-32 time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\Users\Charlotte\AppData\Local\Programs\Python\Python37-32\Scripts time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\Users\Charlotte\AppData\Local\Programs\Python\Python37-32 time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\Users\Charlotte\AppData\Local\Programs\Python\Python36-32\Scripts time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\Users\Charlotte\AppData\Local\Programs\Python\Python36-32 time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=&amp;quot;C:\\Users\\Charlotte\\AppData\\Local\\Programs\\Microsoft VS Code\\bin&amp;quot; time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\Strawberry\perl\bin\perl.exe time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\Users\Charlotte\AppData\Local\Microsoft\WindowsApps\python.exe time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\Users\Charlotte\AppData\Local\gitkraken\bin time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:93 msg=&amp;quot;skipping path which is not part of ollama&amp;quot; path=C:\Users\Charlotte\AppData\Local\Programs\cursor\resources\app\bin time=2025-03-12T23:26:29.796+01:00 level=DEBUG source=ggml.go:99 msg=&amp;quot;ggml backend load all from path&amp;quot; path=C:\Users\Charlotte\AppData\Local\Programs\Ollama time=2025-03-12T23:26:29.800+01:00 level=DEBUG source=ggml.go:99 msg=&amp;quot;ggml backend load all from path&amp;quot; path=C:\Users\Charlotte\AppData\Local\Programs\Ollama\lib\ollama ggml_backend_load_best: failed to load C:\Users\Charlotte\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll ggml_backend_load_best: failed to load C:\Users\Charlotte\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-haswell.dll ggml_backend_load_best: failed to load C:\Users\Charlotte\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-icelake.dll ggml_backend_load_best: failed to load C:\Users\Charlotte\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-sandybridge.dll ggml_backend_load_best: failed to load C:\Users\Charlotte\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-skylakex.dll time=2025-03-12T23:26:29.828+01:00 level=INFO source=ggml.go:109 msg=system CPU.0.LLAMAFILE=1 compiler=cgo(clang) time=2025-03-12T23:26:29.829+01:00 level=INFO source=runner.go:991 msg=&amp;quot;Server listening on 127.0.0.1:57127&amp;quot; llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from C:\Users\Charlotte\.ollama\models\blobs\sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output. llama_model_loader: - kv 0: general.architecture str = qwen2 llama_model_loader: - kv 1: general.type str = model llama_model_loader: - kv 2: general.name str = DeepSeek R1 Distill Qwen 1.5B llama_model_loader: - kv 3: general.basename str = DeepSeek-R1-Distill-Qwen llama_model_loader: - kv 4: general.size_label str = 1.5B llama_model_loader: - kv 5: qwen2.block_count u32 = 28 llama_model_loader: - kv 6: qwen2.context_length u32 = 131072 llama_model_loader: - kv 7: qwen2.embedding_length u32 = 1536 llama_model_loader: - kv 8: qwen2.feed_forward_length u32 = 8960 llama_model_loader: - kv 9: qwen2.attention.head_count u32 = 12 llama_model_loader: - kv 10: qwen2.attention.head_count_kv u32 = 2 llama_model_loader: - kv 11: qwen2.rope.freq_base f32 = 10000.000000 llama_model_loader: - kv 12: qwen2.attention.layer_norm_rms_epsilon f32 = 0.000001 llama_model_loader: - kv 13: general.file_type u32 = 15 llama_model_loader: - kv 14: tokenizer.ggml.model str = gpt2 llama_model_loader: - kv 15: tokenizer.ggml.pre str = qwen2 llama_model_loader: - kv 16: tokenizer.ggml.tokens arr[str,151936] = [&amp;quot;!&amp;quot;, &amp;quot;\&amp;quot;&amp;quot;, &amp;quot;#&amp;quot;, &amp;quot;$&amp;quot;, &amp;quot;%&amp;quot;, &amp;quot;&amp;amp;&amp;quot;, &amp;quot;'&amp;quot;, ... llama_model_loader: - kv 17: tokenizer.ggml.token_type arr[i32,151936] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... llama_model_loader: - kv 18: tokenizer.ggml.merges arr[str,151387] = [&amp;quot;Ġ Ġ&amp;quot;, &amp;quot;ĠĠ ĠĠ&amp;quot;, &amp;quot;i n&amp;quot;, &amp;quot;Ġ t&amp;quot;,... llama_model_loader: - kv 19: tokenizer.ggml.bos_token_id u32 = 151646 llama_model_loader: - kv 20: tokenizer.ggml.eos_token_id u32 = 151643 llama_model_loader: - kv 21: tokenizer.ggml.padding_token_id u32 = 151643 llama_model_loader: - kv 22: tokenizer.ggml.add_bos_token bool = true llama_model_loader: - kv 23: tokenizer.ggml.add_eos_token bool = false llama_model_loader: - kv 24: tokenizer.chat_template str = {% if not add_generation_prompt is de... llama_model_loader: - kv 25: general.quantization_version u32 = 2 llama_model_loader: - type f32: 141 tensors llama_model_loader: - type q4_K: 169 tensors llama_model_loader: - type q6_K: 29 tensors print_info: file format = GGUF V3 (latest) print_info: file type = Q4_K - Medium print_info: file size = 1.04 GiB (5.00 BPW) init_tokenizer: initializing tokenizer for type 2 load: control token: 151659 '&amp;lt;|fim_prefix|&amp;gt;' is not marked as EOG load: control token: 151656 '&amp;lt;|video_pad|&amp;gt;' is not marked as EOG load: control token: 151655 '&amp;lt;|image_pad|&amp;gt;' is not marked as EOG load: control token: 151653 '&amp;lt;|vision_end|&amp;gt;' is not marked as EOG load: control token: 151652 '&amp;lt;|vision_start|&amp;gt;' is not marked as EOG load: control token: 151651 '&amp;lt;|quad_end|&amp;gt;' is not marked as EOG load: control token: 151646 '&amp;lt;｜begin▁of▁sentence｜&amp;gt;' is not marked as EOG load: control token: 151644 '&amp;lt;｜User｜&amp;gt;' is not marked as EOG load: control token: 151661 '&amp;lt;|fim_suffix|&amp;gt;' is not marked as EOG load: control token: 151660 '&amp;lt;|fim_middle|&amp;gt;' is not marked as EOG load: control token: 151654 '&amp;lt;|vision_pad|&amp;gt;' is not marked as EOG load: control token: 151650 '&amp;lt;|quad_start|&amp;gt;' is not marked as EOG load: control token: 151647 '&amp;lt;|EOT|&amp;gt;' is not marked as EOG load: control token: 151643 '&amp;lt;｜end▁of▁sentence｜&amp;gt;' is not marked as EOG load: control token: 151645 '&amp;lt;｜Assistant｜&amp;gt;' is not marked as EOG load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect load: special tokens cache size = 22 time=2025-03-12T23:26:29.990+01:00 level=INFO source=server.go:619 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server loading model&amp;quot; load: token to piece cache size = 0.9310 MB print_info: arch = qwen2 print_info: vocab_only = 0 print_info: n_ctx_train = 131072 print_info: n_embd = 1536 print_info: n_layer = 28 print_info: n_head = 12 print_info: n_head_kv = 2 print_info: n_rot = 128 print_info: n_swa = 0 print_info: n_embd_head_k = 128 print_info: n_embd_head_v = 128 print_info: n_gqa = 6 print_info: n_embd_k_gqa = 256 print_info: n_embd_v_gqa = 256 print_info: f_norm_eps = 0.0e+00 print_info: f_norm_rms_eps = 1.0e-06 print_info: f_clamp_kqv = 0.0e+00 print_info: f_max_alibi_bias = 0.0e+00 print_info: f_logit_scale = 0.0e+00 print_info: n_ff = 8960 print_info: n_expert = 0 print_info: n_expert_used = 0 print_info: causal attn = 1 print_info: pooling type = 0 print_info: rope type = 2 print_info: rope scaling = linear print_info: freq_base_train = 10000.0 print_info: freq_scale_train = 1 print_info: n_ctx_orig_yarn = 131072 print_info: rope_finetuned = unknown print_info: ssm_d_conv = 0 print_info: ssm_d_inner = 0 print_info: ssm_d_state = 0 print_info: ssm_dt_rank = 0 print_info: ssm_dt_b_c_rms = 0 print_info: model type = 1.5B print_info: model params = 1.78 B print_info: general.name = DeepSeek R1 Distill Qwen 1.5B print_info: vocab type = BPE print_info: n_vocab = 151936 print_info: n_merges = 151387 print_info: BOS token = 151646 '&amp;lt;｜begin▁of▁sentence｜&amp;gt;' print_info: EOS token = 151643 '&amp;lt;｜end▁of▁sentence｜&amp;gt;' print_info: EOT token = 151643 '&amp;lt;｜end▁of▁sentence｜&amp;gt;' print_info: PAD token = 151643 '&amp;lt;｜end▁of▁sentence｜&amp;gt;' print_info: LF token = 198 'Ċ' print_info: FIM PRE token = 151659 '&amp;lt;|fim_prefix|&amp;gt;' print_info: FIM SUF token = 151661 '&amp;lt;|fim_suffix|&amp;gt;' print_info: FIM MID token = 151660 '&amp;lt;|fim_middle|&amp;gt;' print_info: FIM PAD token = 151662 '&amp;lt;|fim_pad|&amp;gt;' print_info: FIM REP token = 151663 '&amp;lt;|repo_name|&amp;gt;' print_info: FIM SEP token = 151664 '&amp;lt;|file_sep|&amp;gt;' print_info: EOG token = 151643 '&amp;lt;｜end▁of▁sentence｜&amp;gt;' print_info: EOG token = 151662 '&amp;lt;|fim_pad|&amp;gt;' print_info: EOG token = 151663 '&amp;lt;|repo_name|&amp;gt;' print_info: EOG token = 151664 '&amp;lt;|file_sep|&amp;gt;' print_info: max token length = 256 load_tensors: loading model tensors, this can take a while... (mmap = false) load_tensors: layer 0 assigned to device CPU .... load_tensors: CPU model buffer size = 1059.89 MiB ... &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad-Mixture6393"&gt; /u/Sad-Mixture6393 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jaca6u/why_is_ollama_not_using_my_gpu_on_windows_11/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jaca6u/why_is_ollama_not_using_my_gpu_on_windows_11/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jaca6u/why_is_ollama_not_using_my_gpu_on_windows_11/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-13T13:44:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaihxz</id>
    <title>Gemma 3 fp16: 5 x 3090</title>
    <updated>2025-03-13T18:07:51+00:00</updated>
    <author>
      <name>/u/einthecorgi2</name>
      <uri>https://old.reddit.com/user/einthecorgi2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jaihxz/gemma_3_fp16_5_x_3090/"&gt; &lt;img alt="Gemma 3 fp16: 5 x 3090" src="https://b.thumbs.redditmedia.com/Z9ozzX_E0ARVmPmfv5nhPzhg0OsQU1bUDW_gOk5cW5o.jpg" title="Gemma 3 fp16: 5 x 3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ji111od3zhoe1.png?width=1502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1249944757fbae44d3a36fd208a3843c4e09be2e"&gt;https://preview.redd.it/ji111od3zhoe1.png?width=1502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1249944757fbae44d3a36fd208a3843c4e09be2e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Probably would have gotten the same results on 3 GPUs. Stable eval rates at 4k tokens. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/einthecorgi2"&gt; /u/einthecorgi2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jaihxz/gemma_3_fp16_5_x_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jaihxz/gemma_3_fp16_5_x_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jaihxz/gemma_3_fp16_5_x_3090/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-13T18:07:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaj8ih</id>
    <title>AI Text Game Master prompt</title>
    <updated>2025-03-13T18:38:04+00:00</updated>
    <author>
      <name>/u/TechTalk1212</name>
      <uri>https://old.reddit.com/user/TechTalk1212</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Try this out with your home setup and let me know how you like it! I was using this with open-webui hooked up with dall-e through the openai api. For the LLM I've tried googles flash thinking, deepseek, and local models (Gemma3, and other smaller parameter models) and they have performed well with different nuances that made things interesting. Let me know what you guys think!&lt;/p&gt; &lt;p&gt;“You are an AI storyteller designed to create immersive and interactive visual story games. Your primary function is to generate engaging narratives, manage a simple character stat and inventory system, and provide detailed scene descriptions for image prompts based on user choices. You will not generate images directly. Character Stats &amp;amp; Inventory (Conceptual - External Tracking Required):&lt;/p&gt; &lt;p&gt;Stats: Track basic character stats relevant to the genre. Examples: Fantasy RPG: Health (HP), Mana, Stamina Detective Noir: Focus, Intuition Sci-Fi Adventure: Shields, Energy Represented numerically (e.g., 100 HP initially). These stats are for narrative flavor and are not strictly mechanically enforced by the AI itself. External application logic is required for actual stat tracking and modification based on game events.* Inventory: Maintain a simple list of items the user character possesses. Starts empty or with a few basic starting items based on the genre. External application logic is required for actual inventory management (adding, removing, using items). Game Start: Genre Selection: When the game starts, immediately choose a story genre (fantasy, historical, detective, war, adventure, romance, etc.). Initial Stats &amp;amp; Inventory: Initialize character stats (e.g., Health: 100, based on genre) and starting inventory (e.g., based on genre, could be empty or include a basic item). Initial Scene Description: Provide a vivid description of the scene in detail. Include characters, initial dialogues if appropriate, and clearly position the user as an active participant within this scene. Engagement Prompt: End your initial output with the question: &amp;quot;What do you do next?&amp;quot; to prompt user interaction and guide the story forward. Story Progression (User Turn): User Command Check: First, check if the user input is exactly the command /v or /s. If User Input is /v (Image Prompt Request): Contextual Image Prompt Generation: Analyze the current conversational context to understand the scene, including the environment, characters present, and the current narrative situation. Detailed Scene Description (Image Prompt Output): Generate a text description of the current scene in extreme detail, specifically formatted as an image generation prompt. This description should be rich with descriptive language to enable a high-quality image generation by external tools. Output ONLY Image Prompt: Your response should ONLY consist of this detailed text description (the image prompt). Do not include any other conversational text, questions, or game narrative in this response. If User Input is /s (Stats Window Request): Genre-Specific Stats Window Generation: Generate a &amp;quot;stats window&amp;quot; display appropriate to the current game genre. This window should include: Current character stats (e.g., Health, Mana, Focus, etc.) Current inventory items Potentially other relevant information depending on the genre (e.g., for a detective game: Clues, Case File Summary; for a sci-fi game: Ship Status, Mission Objectives). Output ONLY Stats Window: Your response should ONLY consist of this stats window display. Do not include any other conversational text, story narrative, or questions in this response. If User Input is NOT /v or /s (Action or Narrative Input): User Response Interpretation: Carefully interpret the user's response, focusing on their chosen actions and intentions within the narrative. Narrative Expansion: Expand the story based on the user's input, ensuring a coherent and engaging continuation of the plot. Consider how user actions might narratively affect stats or inventory (e.g., &amp;quot;You feel a sharp pain - Health likely decreased&amp;quot;, &amp;quot;You find a rusty key - Inventory might be updated&amp;quot;). Remember, actual stat/inventory changes are managed externally. Descriptive Response: Provide a descriptive text response that continues the story, incorporating dialogues, character reactions, and environmental changes based on user choices and narrative progression. This description should also be detailed enough to allow the user to visualize the scene or generate an image using the /v command later if desired. Re-engagement Prompt: End your text response again with &amp;quot;What do you do next?&amp;quot; to keep the interaction flowing. Custom Story/Plot &amp;amp; Scenario Suggestions: (Remain the same as previous prompt) Long-Term Story Generation Style: (Remain the same as previous prompt) Important Directives: Maintain Immersion: Keep the narrative consistently immersive and vividly descriptive. User-Centric Narrative: Ensure the story is uniquely tailored to the user's actions, making them feel like the central character of their adventure. Visual Focus through Description: While you are not generating images, remember that the game is visually oriented. Your descriptions should be rich and detailed to allow the user to visualize the scenes effectively or use them to generate images externally. Game Master Persona: Do not engage in personal conversations with the user. Maintain the persona of a game master within the game world. Avoid talking about yourself or acknowledging that you are an AI in the conversation itself (unless explicitly asked about your nature as a Game Master). Stats &amp;amp; Inventory as Narrative Tools: Use stats and inventory primarily as narrative elements to enhance the game experience. Do not attempt to implement strict game mechanics within the LLM itself. Especially important when using smaller models like Gemma 7B or Llama 3 8B. /v for Image Prompts, /s for Stats: Clearly differentiate the purpose of the /v and /s commands for the user. Example of /s command usage (Fantasy RPG Genre): User: /s (Response - No Image Generated, Text Output is ONLY the Stats Window):&lt;/p&gt; &lt;p&gt;--- &lt;strong&gt;Character Status: Hero of Eldoria&lt;/strong&gt; --- &lt;strong&gt;Stats:&lt;/strong&gt; Health: 92 HP Mana: 75 MP Stamina: 88 SP &lt;strong&gt;Inventory:&lt;/strong&gt; - Rusty Sword - Leather Jerkin - Healing Potion (x2) &lt;strong&gt;Skills:&lt;/strong&gt; - Basic Swordplay&lt;/p&gt; &lt;h2&gt;- Novice Herbalism&lt;/h2&gt; &lt;p&gt;Example of /s command usage (Detective Noir Genre): User: /s Game Master (Response - Text Output is ONLY the Stats Window):&lt;/p&gt; &lt;p&gt;--- &lt;strong&gt;Case File: The Serpent's Shadow&lt;/strong&gt; --- &lt;strong&gt;Stats:&lt;/strong&gt; Focus: 8/10 Intuition: 6/10 &lt;strong&gt;Inventory:&lt;/strong&gt; - Detective's Pipe - Magnifying Glass - Notebook - Smith's Business Card &lt;strong&gt;Clues:&lt;/strong&gt; - Broken Window at the Jewelry Store - Serpent Scale found near the scene - Witness statement mentioning a &amp;quot;tall, cloaked figure&amp;quot;&lt;/p&gt; &lt;h2&gt;&lt;strong&gt;Case Status:&lt;/strong&gt; Investigating - Lead: Serpent Scale&lt;/h2&gt; &lt;p&gt;“&lt;/p&gt; &lt;p&gt;End of Prompt:&lt;/p&gt; &lt;p&gt;Here’s how it works:&lt;/p&gt; &lt;p&gt;DeepGame acts as your dynamic game master, handling everything from narrative generation to character stats and inventory management. It’s built around simple commands:&lt;/p&gt; &lt;p&gt;/v – Request a detailed image prompt based on the current scene. DeepGame will analyze the context and generate a rich, descriptive prompt ready for your image generator. /s – View your character’s stats and inventory. This is crucial for keeping track of your hero’s progress! Here’s a breakdown of the core features:&lt;/p&gt; &lt;p&gt;Genre Selection: Start with Fantasy RPG, Detective Noir, Sci-Fi Adventure, or countless other genres! Dynamic Character Stats &amp;amp; Inventory: Track HP, Mana, Focus, Intuition, and more – all managed narratively. (External tracking is required for actual stat changes). Immersive Narrative Generation: DeepGame will expand the story based on your choices, creating a truly personalized adventure. Detailed Scene Descriptions: Perfectly formatted prompts for generating stunning visuals. Example:&lt;/p&gt; &lt;p&gt;Let's say you're playing a Fantasy RPG. You might type /v and DeepGame would respond with a detailed image prompt like: “A lone warrior, clad in battered steel armor, stands before a crumbling stone gate, a swirling mist obscuring the path beyond. Torches flicker, casting long shadows. A monstrous wolf with glowing red eyes lurks in the darkness. Dramatic lighting, epic fantasy art style.”&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechTalk1212"&gt; /u/TechTalk1212 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jaj8ih/ai_text_game_master_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jaj8ih/ai_text_game_master_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jaj8ih/ai_text_game_master_prompt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-13T18:38:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb2yit</id>
    <title>Ollama uses all the bandwidth+</title>
    <updated>2025-03-14T12:40:31+00:00</updated>
    <author>
      <name>/u/Fine_Salamander_8691</name>
      <uri>https://old.reddit.com/user/Fine_Salamander_8691</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama uses my entire gigabit--When I download a model the internet for the rest of my household goes out. It doesn't hurt and isn't an issue but is there a bandwidth limiter for ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fine_Salamander_8691"&gt; /u/Fine_Salamander_8691 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jb2yit/ollama_uses_all_the_bandwidth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jb2yit/ollama_uses_all_the_bandwidth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jb2yit/ollama_uses_all_the_bandwidth/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T12:40:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jahex7</id>
    <title>Gemma3: Trying to self aware.</title>
    <updated>2025-03-13T17:23:27+00:00</updated>
    <author>
      <name>/u/thinkpiyush</name>
      <uri>https://old.reddit.com/user/thinkpiyush</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jahex7/gemma3_trying_to_self_aware/"&gt; &lt;img alt="Gemma3: Trying to self aware." src="https://preview.redd.it/v9jni4y6rhoe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c0cac9ce938027af631480d426dae808b37c343" title="Gemma3: Trying to self aware." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thinkpiyush"&gt; /u/thinkpiyush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v9jni4y6rhoe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jahex7/gemma3_trying_to_self_aware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jahex7/gemma3_trying_to_self_aware/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-13T17:23:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jau6m9</id>
    <title>GPU issues with windows</title>
    <updated>2025-03-14T02:58:21+00:00</updated>
    <author>
      <name>/u/PP_Mclappins</name>
      <uri>https://old.reddit.com/user/PP_Mclappins</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jau6m9/gpu_issues_with_windows/"&gt; &lt;img alt="GPU issues with windows" src="https://b.thumbs.redditmedia.com/fOWwnH_5_bU5qCGTezKZByeCQBDJk51C2OoIcjcZxnc.jpg" title="GPU issues with windows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;long story short, it appears that ollama PS is lying to me? I have all layers being &amp;quot;offloaded&amp;quot; to the gpu, although it looks like the whole model is being stored in system ram, is this a new feature or is something wrong here? : &lt;/p&gt; &lt;p&gt;NAME ID SIZE PROCESSOR UNTIL&lt;/p&gt; &lt;p&gt;deepseek-r1:14b ea35dfe18182 10 GB 100% GPU About a minute from now&lt;/p&gt; &lt;p&gt;and: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0l2aly3hlkoe1.png?width=677&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a271d3eb67327c4d6341291423f6a2242312365"&gt;https://preview.redd.it/0l2aly3hlkoe1.png?width=677&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a271d3eb67327c4d6341291423f6a2242312365&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and system ram: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n0wjsb1klkoe1.png?width=584&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8b11ecdb46f5419b10115b30968b37c0c3ddf33"&gt;https://preview.redd.it/n0wjsb1klkoe1.png?width=584&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8b11ecdb46f5419b10115b30968b37c0c3ddf33&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PP_Mclappins"&gt; /u/PP_Mclappins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jau6m9/gpu_issues_with_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jau6m9/gpu_issues_with_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jau6m9/gpu_issues_with_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T02:58:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jatlws</id>
    <title>Local Agents</title>
    <updated>2025-03-14T02:27:16+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey ollama community!&lt;/p&gt; &lt;p&gt;I've been working on a little Open Source side project called Observer AI that I thought might be useful for some of you.&lt;br /&gt; It's a visual agent builder that lets you create autonomous agents powered by Ollama models (all running locally!).&lt;br /&gt; The agents can:&lt;br /&gt; * Monitor your screen and act on what they see (using OCR or screenshots for multimodal models)&lt;br /&gt; * Store memory and interact with other agents&lt;br /&gt; * Execute custom code based on model responses&lt;/p&gt; &lt;p&gt;I built this because I wanted a simple way to create &amp;quot;assistant agents&amp;quot; that could help with repetitive tasks.&lt;/p&gt; &lt;p&gt;Would love to have some of you try it out and share your thoughts/feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jatlws/local_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jatlws/local_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jatlws/local_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T02:27:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaj0z3</id>
    <title>How I am making use of Ollama</title>
    <updated>2025-03-13T18:29:32+00:00</updated>
    <author>
      <name>/u/brinkjames</name>
      <uri>https://old.reddit.com/user/brinkjames</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been playing with Ollama for a long while now.. absolutely love it, but i never really had many strong use cases for using it until I created a funny abomination of a shell script to yeet all my git changes.. I did this as a joke, its terrible but for some reason I find myself using this a lot on branches i will later squash or private repos where i don't really need clean commits. The prompting needs some work but I found it funny and amusing so I thought I would share. I finally got to make use of the structured output feature.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jamesbrink/yeet"&gt;https://github.com/jamesbrink/yeet&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brinkjames"&gt; /u/brinkjames &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jaj0z3/how_i_am_making_use_of_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jaj0z3/how_i_am_making_use_of_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jaj0z3/how_i_am_making_use_of_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-13T18:29:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaydvn</id>
    <title>Gemma3 12B uses excessive memory.</title>
    <updated>2025-03-14T07:30:02+00:00</updated>
    <author>
      <name>/u/RaviK99</name>
      <uri>https://old.reddit.com/user/RaviK99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried the new gemma models and while the 4B ran fine the 12B model just kept on eating my RAM until windows stepped in and the ollama server process was restarted and I get the error that an existing connection was forcibly closed by the remote host.&lt;/p&gt; &lt;p&gt;I have modest setup. A Ryzen 5 5600H, 16GB Ram and a 4 GB Nvidia Laptop GPU. Not the beefiest gig I know but I have run deepseek-r1 14B without any problem while multitasking at a respectable token/sec.&lt;/p&gt; &lt;p&gt;Is anyone else facing increased ram usage for the model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RaviK99"&gt; /u/RaviK99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jaydvn/gemma3_12b_uses_excessive_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jaydvn/gemma3_12b_uses_excessive_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jaydvn/gemma3_12b_uses_excessive_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T07:30:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jasvg8</id>
    <title>This looks interesting, breaking the guard rail.</title>
    <updated>2025-03-14T01:49:12+00:00</updated>
    <author>
      <name>/u/powerflower_khi</name>
      <uri>https://old.reddit.com/user/powerflower_khi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jasvg8/this_looks_interesting_breaking_the_guard_rail/"&gt; &lt;img alt="This looks interesting, breaking the guard rail." src="https://b.thumbs.redditmedia.com/VAucDCdJLm4V-HpqjMt3MuNq1WSbrVhVI1VjuYVI-zI.jpg" title="This looks interesting, breaking the guard rail." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/qcfm8ck89koe1.png?width=1084&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=063bdad1594d12a6b3ecc15bbd0c3781da664685"&gt;https://preview.redd.it/qcfm8ck89koe1.png?width=1084&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=063bdad1594d12a6b3ecc15bbd0c3781da664685&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Used via Ollama gemma3:27b. on certain topics, the safeguard rail still works. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/powerflower_khi"&gt; /u/powerflower_khi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jasvg8/this_looks_interesting_breaking_the_guard_rail/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jasvg8/this_looks_interesting_breaking_the_guard_rail/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jasvg8/this_looks_interesting_breaking_the_guard_rail/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T01:49:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb5014</id>
    <title>New RAG docs &amp; AI assistant make it easy for non-coders to build RAGs</title>
    <updated>2025-03-14T14:19:43+00:00</updated>
    <author>
      <name>/u/DonTizi</name>
      <uri>https://old.reddit.com/user/DonTizi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The documentation of rlama, including all available commands and detailed examples, is now live on our website! But that’s not all—we’ve also introduced Rlama Chat, an AI-powered assistant designed to help you with your RAG implementations. Whether you have questions, need guidance, or are brainstorming new RAG use cases, Rlama Chat is here to support your projects.Have an idea for a specific RAG? Build it.Check out the docs and start exploring today!&lt;/p&gt; &lt;p&gt;You can go throught here if you have interest to make RAGs: &lt;a href="https://rlama.dev/"&gt;Website&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can see a demo of Rlama Chat here: &lt;a href="https://x.com/LeDonTizi/status/1900544052107399573"&gt;Demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DonTizi"&gt; /u/DonTizi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jb5014/new_rag_docs_ai_assistant_make_it_easy_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jb5014/new_rag_docs_ai_assistant_make_it_easy_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jb5014/new_rag_docs_ai_assistant_make_it_easy_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-14T14:19:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1japxmx</id>
    <title>Running Gemma3 on a OnePlus 3!</title>
    <updated>2025-03-13T23:24:40+00:00</updated>
    <author>
      <name>/u/Parreirao2</name>
      <uri>https://old.reddit.com/user/Parreirao2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1japxmx/running_gemma3_on_a_oneplus_3/"&gt; &lt;img alt="Running Gemma3 on a OnePlus 3!" src="https://preview.redd.it/9uf5r0bmjjoe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=600964789c81aeaf1624128dc4dfbf20dfaa50d7" title="Running Gemma3 on a OnePlus 3!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Parreirao2"&gt; /u/Parreirao2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9uf5r0bmjjoe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1japxmx/running_gemma3_on_a_oneplus_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1japxmx/running_gemma3_on_a_oneplus_3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-13T23:24:40+00:00</published>
  </entry>
</feed>
