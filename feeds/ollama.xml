<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-10T19:22:17+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mlg1sx</id>
    <title>How long until GLM4.5 availability (or what to use in the meantime)?</title>
    <updated>2025-08-09T04:02:44+00:00</updated>
    <author>
      <name>/u/BeardyScientist</name>
      <uri>https://old.reddit.com/user/BeardyScientist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am brand new to this wonderful world of local LLMs and am trying to pick a model for a fairly large scientific task (read and extract data from several thousand documents which will take about a month of solid processing). I’ve seen a lot of rave reviews for GLM4.5 and GLM4.5 air and trying it at &lt;a href="http://chat.z.ai/"&gt;chat.z.ai/&lt;/a&gt; has me impressed. However, as it’s not available on Ollama yet, I can’t use it for my task. I don’t have the experience to know how long I can expect to wait before it becomes available; are we talking days, weeks, months, or maybe never? Alternatively, have I missed something and it’s actually available now without me going through a huge effort?&lt;/p&gt; &lt;p&gt;In the meantime, what model would you all suggest for a scientific task where I’m asking detailed comprehension questions about long scientific texts (10k – 20k words). The texts are in a range of languages but mostly English and Chinese. The hardware I’m running on is a single RTX 5090. I’ve tried GPT-OSS:20b and DeepSeek R1:14b; GPT-OSS mostly flat-out refuses to answer my questions and just spits out generic summaries and the occasional garbled mess, whereas DeepSeek R1 delivered reliable, acceptable, but room-for-improvement results. I’ve also given Qwen3, Gemma3, and GLM4 a go in limited trials; all of which were good but I couldn’t decide which would be most reliable. What are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BeardyScientist"&gt; /u/BeardyScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlg1sx/how_long_until_glm45_availability_or_what_to_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlg1sx/how_long_until_glm45_availability_or_what_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlg1sx/how_long_until_glm45_availability_or_what_to_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T04:02:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml2qof</id>
    <title>Cheapest way to host a 24B parameter Ollama server?</title>
    <updated>2025-08-08T18:16:46+00:00</updated>
    <author>
      <name>/u/Few-Avocado4562</name>
      <uri>https://old.reddit.com/user/Few-Avocado4562</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking to run a 24B param Ollama model without breaking the bank. Any recommendations on the cheapest hosting options that actually work? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few-Avocado4562"&gt; /u/Few-Avocado4562 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml2qof/cheapest_way_to_host_a_24b_parameter_ollama_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml2qof/cheapest_way_to_host_a_24b_parameter_ollama_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ml2qof/cheapest_way_to_host_a_24b_parameter_ollama_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T18:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml9fsr</id>
    <title>Seeing the positive reception from the community for my tool that finds the most optimal model, I’ve released an updated version fixing bugs, optimizing the algorithm, and adding suggestions from some fellow redditors. The new version is now available :)</title>
    <updated>2025-08-08T22:45:17+00:00</updated>
    <author>
      <name>/u/pzarevich</name>
      <uri>https://old.reddit.com/user/pzarevich</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ml9fsr/seeing_the_positive_reception_from_the_community/"&gt; &lt;img alt="Seeing the positive reception from the community for my tool that finds the most optimal model, I’ve released an updated version fixing bugs, optimizing the algorithm, and adding suggestions from some fellow redditors. The new version is now available :)" src="https://external-preview.redd.it/aGJ3cXliMnZpdmhmMbi2piECoZUsnlTdrO5dt19_c4zYyJXquEM3aQ2Vkfc7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b0bf6292d985de20c4f39fd79e05441e57b3599" title="Seeing the positive reception from the community for my tool that finds the most optimal model, I’ve released an updated version fixing bugs, optimizing the algorithm, and adding suggestions from some fellow redditors. The new version is now available :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;npm here: &lt;a href="https://www.npmjs.com/package/llm-checker/v/2.2.0?activeTab=readme"&gt; LLM Checker - Intelligent Ollama Model Selector&lt;/a&gt; :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pzarevich"&gt; /u/pzarevich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/b5pe2g2vivhf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml9fsr/seeing_the_positive_reception_from_the_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ml9fsr/seeing_the_positive_reception_from_the_community/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T22:45:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml66jw</id>
    <title>New Favorite Game: Convince GPT-OSS it Exists</title>
    <updated>2025-08-08T20:30:23+00:00</updated>
    <author>
      <name>/u/j_din</name>
      <uri>https://old.reddit.com/user/j_din</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spoiler alert: it can not FATHOM its own existence.&lt;/p&gt; &lt;p&gt;I have been trying for an hour to have the model even admit it's possible that OpenAI would release an open weight model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/bB0XE2Zv"&gt;https://pastebin.com/bB0XE2Zv&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please comment any successful attempts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/j_din"&gt; /u/j_din &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml66jw/new_favorite_game_convince_gptoss_it_exists/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml66jw/new_favorite_game_convince_gptoss_it_exists/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ml66jw/new_favorite_game_convince_gptoss_it_exists/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T20:30:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlt4zw</id>
    <title>XandAI-extension - Allow you to chat with your browser using ollama.</title>
    <updated>2025-08-09T16:07:50+00:00</updated>
    <author>
      <name>/u/Sea-Reception-2697</name>
      <uri>https://old.reddit.com/user/Sea-Reception-2697</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mlt4zw/xandaiextension_allow_you_to_chat_with_your/"&gt; &lt;img alt="XandAI-extension - Allow you to chat with your browser using ollama." src="https://external-preview.redd.it/bYRVS90ZKX1r4OqkPxy0a_PbC3IEsvLjY869Bkl6r1A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7564c949be66cb52cdfacd0f7c58a0c70de1044" title="XandAI-extension - Allow you to chat with your browser using ollama." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Reception-2697"&gt; /u/Sea-Reception-2697 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1mlszpz/xandaiextension_allow_you_to_chat_with_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlt4zw/xandaiextension_allow_you_to_chat_with_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlt4zw/xandaiextension_allow_you_to_chat_with_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T16:07:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml750r</id>
    <title>Local RAG with 97% smaller index and Claude Code–compatible semantic search</title>
    <updated>2025-08-08T21:08:43+00:00</updated>
    <author>
      <name>/u/Lanky-District9096</name>
      <uri>https://old.reddit.com/user/Lanky-District9096</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re building &lt;strong&gt;LEANN&lt;/strong&gt; at Berkeley Sky Lab — a &lt;strong&gt;local vector index for RAG&lt;/strong&gt; that’s&lt;/p&gt; &lt;p&gt;🔒 privacy-first&lt;/p&gt; &lt;p&gt;📦 97% smaller&lt;/p&gt; &lt;p&gt;🧠 fully compatible with &lt;strong&gt;Claude Code&lt;/strong&gt;, &lt;strong&gt;Ollama&lt;/strong&gt;, and &lt;strong&gt;GPT-OSS&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Run semantic search on your laptop — fast, lightweight, and cloud-free.&lt;/p&gt; &lt;p&gt;🔗 &lt;a href="https://github.com/yichuan-w/LEANN"&gt;https://github.com/yichuan-w/LEANN&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why does LEANN matter?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Because most vector DBs store &lt;em&gt;everything&lt;/em&gt; — all embeddings, all graph structure — which quickly balloons to 100+GB when you index emails, chat, and code.&lt;/p&gt; &lt;p&gt;But… most queries only touch a tiny slice of the DB. So we asked:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why store every single embedding?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;LEANN introduces two ultra-lightweight backends:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;🔍 Graph-only mode:&lt;/strong&gt; Stores &lt;em&gt;no embeddings&lt;/em&gt;, just a pruned HNSW graph.Recomputes embeddings on the fly using overlapping neighbors.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;💡 PQ+Rerank mode:&lt;/strong&gt; Compresses vectors with PQ, then &lt;em&gt;replaces&lt;/em&gt; heavy embedding storage with lightweight on-the-fly recomputation on the candidate set.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each offers a different tradeoff, but both aim for the same goal:&lt;/p&gt; &lt;p&gt;🧠 &lt;strong&gt;Massive storage savings&lt;/strong&gt; with &lt;em&gt;no meaningful drop&lt;/em&gt; in recall.&lt;/p&gt; &lt;p&gt;📝 &lt;strong&gt;Note:&lt;/strong&gt; In modern RAG systems — with long inputs and reasoning-heavy models — &lt;strong&gt;generation&lt;/strong&gt;, not retrieval, is the bottleneck.&lt;/p&gt; &lt;p&gt;So even with slight retrieval latency increases, the end-to-end impact is &lt;strong&gt;~5% overhead&lt;/strong&gt; or less.&lt;/p&gt; &lt;p&gt;These give you blazing-fast semantic search over:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;• 📨 Apple Mail • 💾 Filesystem • 🕰️ Chrome / Chat history • 🧠 Codebase (Claude Code–compatible) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;LEANN = your personal Jarvis, running locally.&lt;/p&gt; &lt;p&gt;🔗 GitHub: &lt;a href="https://github.com/yichuan-w/LEANN"&gt;https://github.com/yichuan-w/LEANN&lt;/a&gt;&lt;/p&gt; &lt;p&gt;📄 Paper: &lt;a href="https://arxiv.org/abs/2506.08276"&gt;https://arxiv.org/abs/2506.08276&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’d love for you to try it out, give feedback, or ask questions on the repo!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lanky-District9096"&gt; /u/Lanky-District9096 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml750r/local_rag_with_97_smaller_index_and_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml750r/local_rag_with_97_smaller_index_and_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ml750r/local_rag_with_97_smaller_index_and_claude/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T21:08:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlzn7x</id>
    <title>Telling Ollama GUI to use GPU instead of CPU?</title>
    <updated>2025-08-09T20:38:46+00:00</updated>
    <author>
      <name>/u/Tinytitanic</name>
      <uri>https://old.reddit.com/user/Tinytitanic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a Ryzen 5700X and a RX 6800, both supported by Ollama. I used Ollama a few months ago and I noticed that it decided to use my GPU with high VRAM and GPU utilization but coming back to it to test the new GUI I noticed that it's instead using my CPU. Is there a way to tell it to use GPU instead? I feel like it was faster using the GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tinytitanic"&gt; /u/Tinytitanic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlzn7x/telling_ollama_gui_to_use_gpu_instead_of_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlzn7x/telling_ollama_gui_to_use_gpu_instead_of_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlzn7x/telling_ollama_gui_to_use_gpu_instead_of_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T20:38:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlgyct</id>
    <title>GPT-OSS 20b vs Qwen3-30B-A3B</title>
    <updated>2025-08-09T04:52:10+00:00</updated>
    <author>
      <name>/u/unofficialreddit0r</name>
      <uri>https://old.reddit.com/user/unofficialreddit0r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone used them in real life coding task? How do they compare?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialreddit0r"&gt; /u/unofficialreddit0r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlgyct/gptoss_20b_vs_qwen330ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlgyct/gptoss_20b_vs_qwen330ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlgyct/gptoss_20b_vs_qwen330ba3b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T04:52:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm1hs0</id>
    <title>user uploaded models</title>
    <updated>2025-08-09T21:58:41+00:00</updated>
    <author>
      <name>/u/Mediocre_Caramel_158</name>
      <uri>https://old.reddit.com/user/Mediocre_Caramel_158</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Where can I find user-uploaded models that are open and free to use by others on the Ollama server (ollama.com)? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mediocre_Caramel_158"&gt; /u/Mediocre_Caramel_158 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm1hs0/user_uploaded_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm1hs0/user_uploaded_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm1hs0/user_uploaded_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T21:58:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlxqtz</id>
    <title>Get a second GPU or go for a Mac Mini? (Qwen3-Coder30b)</title>
    <updated>2025-08-09T19:18:13+00:00</updated>
    <author>
      <name>/u/Manaberryio</name>
      <uri>https://old.reddit.com/user/Manaberryio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;I have a server machine I want to use for AI. I have an RX6800 (16GB VRAM) inside, along with a 13900KF and 128GB of ram. I've tried Qwen3-coder 30b 3ab but I cannot host it fully on my GPU with a bigger context than 16K. It's really slow and somehow unable to process debug request from Roo Code.&lt;/p&gt; &lt;p&gt;Will a second RX 6800 (around $300 used) would be helpful to do so? Or should I sell my stuff and get a Mac Mini M4 with at least 24GB of memory?&lt;/p&gt; &lt;p&gt;Thanks for helping&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Manaberryio"&gt; /u/Manaberryio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlxqtz/get_a_second_gpu_or_go_for_a_mac_mini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlxqtz/get_a_second_gpu_or_go_for_a_mac_mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlxqtz/get_a_second_gpu_or_go_for_a_mac_mini/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T19:18:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm0n7h</id>
    <title>Using a local LLM for proofreading on macOS?</title>
    <updated>2025-08-09T21:21:17+00:00</updated>
    <author>
      <name>/u/whooshingsounds</name>
      <uri>https://old.reddit.com/user/whooshingsounds</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to use a LLM to rephrase paragraphs of text, editing them for brevity, clarity, or to aim for a certain tone (formal, casual, businesslike…). I would also prefer not to upload these text to a server. Which local LLMs would be best suited for this? And would any Mac laptop be up to the task?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/whooshingsounds"&gt; /u/whooshingsounds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm0n7h/using_a_local_llm_for_proofreading_on_macos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm0n7h/using_a_local_llm_for_proofreading_on_macos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm0n7h/using_a_local_llm_for_proofreading_on_macos/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T21:21:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlwtkf</id>
    <title>Size of Gpt OSS</title>
    <updated>2025-08-09T18:40:05+00:00</updated>
    <author>
      <name>/u/Waakaari</name>
      <uri>https://old.reddit.com/user/Waakaari</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the size of gpt-oss-20b and gpt-oss-120b when I download from ollama? Is it the same from hugging face? &lt;/p&gt; &lt;p&gt;Is there any difference betwecn running gpt-oss models using ollama and using comfyui or setting it up any other way? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Waakaari"&gt; /u/Waakaari &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlwtkf/size_of_gpt_oss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlwtkf/size_of_gpt_oss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlwtkf/size_of_gpt_oss/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T18:40:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlz15z</id>
    <title>Are you good grok?</title>
    <updated>2025-08-09T20:12:41+00:00</updated>
    <author>
      <name>/u/Gandalfusmaximale</name>
      <uri>https://old.reddit.com/user/Gandalfusmaximale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mlz15z/are_you_good_grok/"&gt; &lt;img alt="Are you good grok?" src="https://preview.redd.it/lt6953j7x1if1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc8b58ce17e3bc02fb5fb414b4cf50dc3eb00ba4" title="Are you good grok?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else had this before ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gandalfusmaximale"&gt; /u/Gandalfusmaximale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lt6953j7x1if1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlz15z/are_you_good_grok/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlz15z/are_you_good_grok/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T20:12:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm7mp8</id>
    <title>Has anyone successfully run Ollama on a Jetson Orin Nano?</title>
    <updated>2025-08-10T02:56:49+00:00</updated>
    <author>
      <name>/u/matsyui_</name>
      <uri>https://old.reddit.com/user/matsyui_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m trying to turn my Jetson Orin Nano Super Developer Kit into a self-hosted AI API server running Ollama.&lt;/p&gt; &lt;p&gt;Has anyone here tried this, patched it, or gotten decent performance running Ollama on a Jetson device?&lt;/p&gt; &lt;p&gt;I’d love to hear about your setup, steps, or any pitfalls you ran into.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matsyui_"&gt; /u/matsyui_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm7mp8/has_anyone_successfully_run_ollama_on_a_jetson/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm7mp8/has_anyone_successfully_run_ollama_on_a_jetson/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm7mp8/has_anyone_successfully_run_ollama_on_a_jetson/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T02:56:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm9uds</id>
    <title>Need help with benchmarking for RAG + LLM running locally.</title>
    <updated>2025-08-10T04:56:32+00:00</updated>
    <author>
      <name>/u/irodov4030</name>
      <uri>https://old.reddit.com/user/irodov4030</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to benchmark RAG setup for multiple file formats like - doc, xls, csv, ppt, png etc.&lt;/p&gt; &lt;p&gt;Are there any benchmarks with which I can test accuracy/quality of answers across multiple file formats?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/irodov4030"&gt; /u/irodov4030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm9uds/need_help_with_benchmarking_for_rag_llm_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm9uds/need_help_with_benchmarking_for_rag_llm_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm9uds/need_help_with_benchmarking_for_rag_llm_running/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T04:56:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmf6xl</id>
    <title>Reasoning LLMs Explorer</title>
    <updated>2025-08-10T10:32:50+00:00</updated>
    <author>
      <name>/u/Boring_Rabbit2275</name>
      <uri>https://old.reddit.com/user/Boring_Rabbit2275</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a web page where a lot of information is compiled about Reasoning in LLMs (A tree of surveys, an atlas of definitions and a map of techniques in reasoning)&lt;/p&gt; &lt;p&gt;&lt;a href="https://azzedde.github.io/reasoning-explorer/"&gt;https://azzedde.github.io/reasoning-explorer/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Your insights ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Boring_Rabbit2275"&gt; /u/Boring_Rabbit2275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmf6xl/reasoning_llms_explorer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmf6xl/reasoning_llms_explorer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmf6xl/reasoning_llms_explorer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T10:32:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlxx5h</id>
    <title>LLMs are Stochastic Parrots - Interactive Visualization</title>
    <updated>2025-08-09T19:25:34+00:00</updated>
    <author>
      <name>/u/kushalgoenka</name>
      <uri>https://old.reddit.com/user/kushalgoenka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mlxx5h/llms_are_stochastic_parrots_interactive/"&gt; &lt;img alt="LLMs are Stochastic Parrots - Interactive Visualization" src="https://external-preview.redd.it/NvAI6Yum9O40l3qZlOeyOssVIs2oLgJwnoMTWT8Xzzg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64d63adc6a8fdef03b738bcffef859cc8986b0e8" title="LLMs are Stochastic Parrots - Interactive Visualization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kushalgoenka"&gt; /u/kushalgoenka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/6dn1kUwTFcc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlxx5h/llms_are_stochastic_parrots_interactive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlxx5h/llms_are_stochastic_parrots_interactive/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T19:25:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmmwcg</id>
    <title>Rookie question. Avoiding FOMO…</title>
    <updated>2025-08-10T16:25:44+00:00</updated>
    <author>
      <name>/u/Famous-Recognition62</name>
      <uri>https://old.reddit.com/user/Famous-Recognition62</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Famous-Recognition62"&gt; /u/Famous-Recognition62 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1mmmtlf/rookie_question_avoiding_fomo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmmwcg/rookie_question_avoiding_fomo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmmwcg/rookie_question_avoiding_fomo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T16:25:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmhxiv</id>
    <title>Qwen 4B Instruct works beautifully ❤️</title>
    <updated>2025-08-10T12:58:52+00:00</updated>
    <author>
      <name>/u/krishnajeya</name>
      <uri>https://old.reddit.com/user/krishnajeya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mmhxiv/qwen_4b_instruct_works_beautifully/"&gt; &lt;img alt="Qwen 4B Instruct works beautifully ❤️" src="https://preview.redd.it/izyjc6vps6if1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c27f3d6f828cbb7921438248a37810117753209" title="Qwen 4B Instruct works beautifully ❤️" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/krishnajeya"&gt; /u/krishnajeya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/izyjc6vps6if1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmhxiv/qwen_4b_instruct_works_beautifully/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmhxiv/qwen_4b_instruct_works_beautifully/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T12:58:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmfhfa</id>
    <title>Fastest and best model for my really low spec hardware</title>
    <updated>2025-08-10T10:50:01+00:00</updated>
    <author>
      <name>/u/PurpleUser0000</name>
      <uri>https://old.reddit.com/user/PurpleUser0000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an I5-4th gen, ddr3 8GB ram 1600hz , no GPU ( IGPU ) &lt;/p&gt; &lt;p&gt;What's the best model I can go with here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PurpleUser0000"&gt; /u/PurpleUser0000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmfhfa/fastest_and_best_model_for_my_really_low_spec/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmfhfa/fastest_and_best_model_for_my_really_low_spec/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmfhfa/fastest_and_best_model_for_my_really_low_spec/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T10:50:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmr2n1</id>
    <title>is we can run gpt oss in 16gb vram ? why mine is offload to cpu im running in docker in ubuntu 22.04</title>
    <updated>2025-08-10T19:06:17+00:00</updated>
    <author>
      <name>/u/actuallytech</name>
      <uri>https://old.reddit.com/user/actuallytech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mmr2n1/is_we_can_run_gpt_oss_in_16gb_vram_why_mine_is/"&gt; &lt;img alt="is we can run gpt oss in 16gb vram ? why mine is offload to cpu im running in docker in ubuntu 22.04" src="https://a.thumbs.redditmedia.com/T8e2323zafFhBWk7siLTxyY4iplSS1jgitzzPLUcEX8.jpg" title="is we can run gpt oss in 16gb vram ? why mine is offload to cpu im running in docker in ubuntu 22.04" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;gpu used is rtx 5060ti 16gb vram&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/actuallytech"&gt; /u/actuallytech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mmr2n1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmr2n1/is_we_can_run_gpt_oss_in_16gb_vram_why_mine_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmr2n1/is_we_can_run_gpt_oss_in_16gb_vram_why_mine_is/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T19:06:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm9zhv</id>
    <title>What should I do with my home ollama lab?</title>
    <updated>2025-08-10T05:04:27+00:00</updated>
    <author>
      <name>/u/onemorequickchange</name>
      <uri>https://old.reddit.com/user/onemorequickchange</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Setup an x99 with dual xeon 2699 v4s and dual 3090s, 256GB RAM, linux distro. Curious what people use this setup for? I just subscribed to Claude @ $100/month and have had chatGPT for teams $30/pers/month. Do people just use local to save money or are there legit use cases? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onemorequickchange"&gt; /u/onemorequickchange &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm9zhv/what_should_i_do_with_my_home_ollama_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm9zhv/what_should_i_do_with_my_home_ollama_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm9zhv/what_should_i_do_with_my_home_ollama_lab/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T05:04:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmnptg</id>
    <title>How to run multiple versions of same model?</title>
    <updated>2025-08-10T16:57:44+00:00</updated>
    <author>
      <name>/u/petr_bena</name>
      <uri>https://old.reddit.com/user/petr_bena</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Right now ollama always works only with latest version of a model, say Mistral:7b&lt;/p&gt; &lt;p&gt;These models get periodic updates. What if I wanted to retain version from 2024 and 2025 and be able to switch between them? Does ollama supports something like version tagging and maintaining multiple versions of same model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/petr_bena"&gt; /u/petr_bena &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmnptg/how_to_run_multiple_versions_of_same_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmnptg/how_to_run_multiple_versions_of_same_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmnptg/how_to_run_multiple_versions_of_same_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T16:57:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmj279</id>
    <title>The first time I heard a sound coming from my MacBook was while using GPT-OSS 20B</title>
    <updated>2025-08-10T13:49:09+00:00</updated>
    <author>
      <name>/u/anakedsuperman</name>
      <uri>https://old.reddit.com/user/anakedsuperman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mmj279/the_first_time_i_heard_a_sound_coming_from_my/"&gt; &lt;img alt="The first time I heard a sound coming from my MacBook was while using GPT-OSS 20B" src="https://external-preview.redd.it/ZThhMDNvMmE1N2lmMTSXhaJcSNaMLiLEA491TTFq3lE-Ha0mGK07Lje4LN3h.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7372d0bd82781bd6388dff2cabe3eca39528ceca" title="The first time I heard a sound coming from my MacBook was while using GPT-OSS 20B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running GPT-OSS 20B on my MacBook M4 Max with 36GB RAM. I don't hear anything from other models, though, even with Devtra 140B.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anakedsuperman"&gt; /u/anakedsuperman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vyk9un2a57if1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmj279/the_first_time_i_heard_a_sound_coming_from_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmj279/the_first_time_i_heard_a_sound_coming_from_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T13:49:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm4ibk</id>
    <title>I ran OpenAI’s GPT-OSS 20B locally on a 16GB Mac with Ollama — setup, gotchas, and mini demo</title>
    <updated>2025-08-10T00:19:29+00:00</updated>
    <author>
      <name>/u/Spirited-Wind6803</name>
      <uri>https://old.reddit.com/user/Spirited-Wind6803</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mm4ibk/i_ran_openais_gptoss_20b_locally_on_a_16gb_mac/"&gt; &lt;img alt="I ran OpenAI’s GPT-OSS 20B locally on a 16GB Mac with Ollama — setup, gotchas, and mini demo" src="https://external-preview.redd.it/MHQ5M3V2aXQ0M2lmMf_ZwYHO2m1fMNCQy9M-9mV9J_Z510ikdbK6GDGwXk75.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=678ce1f3ab9b2ba28aeffdad56e26d3d77e4258f" title="I ran OpenAI’s GPT-OSS 20B locally on a 16GB Mac with Ollama — setup, gotchas, and mini demo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all — I’ve been testing &lt;strong&gt;GPT-OSS 20B&lt;/strong&gt; locally using &lt;strong&gt;Ollama&lt;/strong&gt; and wanted to share a clean setup path, what worked, what didn’t, and a tiny QA demo. &lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Yes, 20B runs on a 16GB Mac&lt;/strong&gt; with Ollama. Do I have the patience? No, it took toooo long&lt;/li&gt; &lt;li&gt;Should you use 16GB to perform any other tasks such as coding, agent, RAG? No, not worth it - upgrade to 32GB maybe..maybe will give you more. I tried on A100 GPU and still did not meet my expectation &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spirited-Wind6803"&gt; /u/Spirited-Wind6803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qlifjvit43if1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm4ibk/i_ran_openais_gptoss_20b_locally_on_a_16gb_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm4ibk/i_ran_openais_gptoss_20b_locally_on_a_16gb_mac/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T00:19:29+00:00</published>
  </entry>
</feed>
