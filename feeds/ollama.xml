<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-05-31T10:37:47+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1kxhj4v</id>
    <title>Connecting Ollama and Open WebUI in container to the internet?</title>
    <updated>2025-05-28T13:47:12+00:00</updated>
    <author>
      <name>/u/OriginalDiddi</name>
      <uri>https://old.reddit.com/user/OriginalDiddi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, Iam running a Ollama on my PC and a docker container with open webui. Open WebUI and Ollama are connected, so Iam using LLMs from Ollama in Open WebUI. &lt;/p&gt; &lt;p&gt;Now I want to connect Open WebUI to a certain website thats hosted in my network. How Iam going to do that and is it possible for Open WebUI or Ollama to read informations from the website?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OriginalDiddi"&gt; /u/OriginalDiddi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxhj4v/connecting_ollama_and_open_webui_in_container_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxhj4v/connecting_ollama_and_open_webui_in_container_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kxhj4v/connecting_ollama_and_open_webui_in_container_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-28T13:47:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ky9hs3</id>
    <title>I want to create a chatgpt like online service using opensource models, where to get started?</title>
    <updated>2025-05-29T12:26:53+00:00</updated>
    <author>
      <name>/u/abdojapan</name>
      <uri>https://old.reddit.com/user/abdojapan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am a computer engineer. I did some web apps even though that wasn't my main speciality, but I know how to create web apps mainly using express or PHP laravel and how to dockerize it.&lt;/p&gt; &lt;p&gt;I recently got into AI and I am fascinated with the potential. Now I want to create an online service like chatgpt with a fine tuned model for specific niche.&lt;/p&gt; &lt;p&gt;I know I can just use ollama and expose it publicly but I am sure there're a lot of nitty gritty stuff that some of you might hint at.&lt;/p&gt; &lt;p&gt;I will appreciate it if you can throw any ideas where to get started what are the challenges. Especially the following&lt;/p&gt; &lt;p&gt;- Which model's license allow for such use case?&lt;/p&gt; &lt;p&gt;- How to manage credits for users and integrate that with some payment either though appstore or something like paypal.&lt;/p&gt; &lt;p&gt;- Anything that might be uesful.&lt;/p&gt; &lt;p&gt;Thank you for advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdojapan"&gt; /u/abdojapan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ky9hs3/i_want_to_create_a_chatgpt_like_online_service/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ky9hs3/i_want_to_create_a_chatgpt_like_online_service/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ky9hs3/i_want_to_create_a_chatgpt_like_online_service/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T12:26:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxyi4x</id>
    <title>Which model do you recommend for M1 Pro, 32 GB Memory?</title>
    <updated>2025-05-29T01:32:06+00:00</updated>
    <author>
      <name>/u/more_muscle_aim</name>
      <uri>https://old.reddit.com/user/more_muscle_aim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello friends,&lt;/p&gt; &lt;p&gt;I‚Äôm new to LLM selection and was curious which is the appropriate model I can run to get the best results. &lt;/p&gt; &lt;p&gt;I‚Äôll be mostly using the model for code generation / review, creating documentations, summarizing/generating MCQs/Indexing from PDF documents, etc.&lt;/p&gt; &lt;p&gt;I‚Äôm currently using gemma3:4b-iat-qat (randomly picked it. üòÖ). Not sure if it‚Äôs the best. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/more_muscle_aim"&gt; /u/more_muscle_aim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxyi4x/which_model_do_you_recommend_for_m1_pro_32_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxyi4x/which_model_do_you_recommend_for_m1_pro_32_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kxyi4x/which_model_do_you_recommend_for_m1_pro_32_gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T01:32:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kye015</id>
    <title>I need a model for adult SEO optimized content</title>
    <updated>2025-05-29T15:40:11+00:00</updated>
    <author>
      <name>/u/jimplementer</name>
      <uri>https://old.reddit.com/user/jimplementer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello.&lt;/p&gt; &lt;p&gt;I need a model who can write SEO-friendly descriptions for porn actors and categories for my adult video site.&lt;/p&gt; &lt;p&gt;Which model would you recommend?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jimplementer"&gt; /u/jimplementer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kye015/i_need_a_model_for_adult_seo_optimized_content/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kye015/i_need_a_model_for_adult_seo_optimized_content/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kye015/i_need_a_model_for_adult_seo_optimized_content/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T15:40:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxyav1</id>
    <title>Automate Your CSV Analysis with AI Agents ‚Äì CrewAI + Ollama</title>
    <updated>2025-05-29T01:22:03+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kxyav1/automate_your_csv_analysis_with_ai_agents_crewai/"&gt; &lt;img alt="Automate Your CSV Analysis with AI Agents ‚Äì CrewAI + Ollama" src="https://external-preview.redd.it/dmhwNXI2Y3RobTNmMV-krD7C_ZIFZ70xsLMOTqRPW4NjITnCLYEfMKf1vxnR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e66297828693b9b4a09c54595aa273083b9fabb" title="Automate Your CSV Analysis with AI Agents ‚Äì CrewAI + Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever spent hours wrestling with messy CSVs and Excel sheets to find that one elusive insight? I just wrapped up a side project that might save you a ton of time:&lt;/p&gt; &lt;h1&gt;üöÄ Automated Data Analysis with AI Agents&lt;/h1&gt; &lt;h1&gt;1Ô∏è‚É£ Effortless Data Ingestion&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Drop your customer-support ticket CSV into the pipeline&lt;/li&gt; &lt;li&gt;Agents spin up to parse, clean, and organize raw data&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2Ô∏è‚É£ Collaborative AI Agents at Work&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;üïµÔ∏è‚Äç‚ôÄÔ∏è Identify recurring issues &amp;amp; trending keywords&lt;/li&gt; &lt;li&gt;üìà Generate actionable insights on response times, ticket volumes, and more&lt;/li&gt; &lt;li&gt;üí° Propose concrete recommendations to boost customer satisfaction&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3Ô∏è‚É£ Polished, Shareable Reports&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Clean Markdown or PDF outputs&lt;/li&gt; &lt;li&gt;Charts, tables, and narrative summaries‚Äîready to share with stakeholders&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üîß Tech Stack Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mistral-Nemo&lt;/strong&gt; powering the NLP&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CrewAI&lt;/strong&gt; orchestrating parallel agents&lt;/li&gt; &lt;li&gt;&lt;strong&gt;100% open-source&lt;/strong&gt;, so you can fork and customize every step&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üëâ &lt;strong&gt;Check out the code &amp;amp; drop a&lt;/strong&gt; ‚≠ê&lt;br /&gt; &lt;a href="https://github.com/Pavankunchala/LLM-Learn-PK/blob/main/AIAgent-CrewAi/customer_support/customer_support.py"&gt;https://github.com/Pavankunchala/LLM-Learn-PK/blob/main/AIAgent-CrewAi/customer_support/customer_support.py&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üöÄ P.S. This project was a ton of fun, and I'm itching for my next AI challenge! If you or your team are doing innovative work in &lt;strong&gt;Computer Vision o&lt;/strong&gt;r LLMS and are looking for a passionate dev, I'd love to chat.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;My Email:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=mailto%3Apavankunchalaofficial%40gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My GitHub Profile (for more projects):&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My Resume:&lt;/strong&gt; &lt;a href="https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view"&gt;https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious to hear your thoughts, feedback, or feature ideas. What AI agent workflows do you wish existed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/q96916cthm3f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxyav1/automate_your_csv_analysis_with_ai_agents_crewai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kxyav1/automate_your_csv_analysis_with_ai_agents_crewai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T01:22:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyc8yg</id>
    <title>Online services that host ollama models?</title>
    <updated>2025-05-29T14:30:09+00:00</updated>
    <author>
      <name>/u/azimux</name>
      <uri>https://old.reddit.com/user/azimux</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey hey!&lt;/p&gt; &lt;p&gt;A recent upgrade of ollama results in my system rebooting if I use any models bigger than about 10GB in size. I'll probably try just rebuilding that whole machine to see if it alleviates the problem.&lt;/p&gt; &lt;p&gt;But made me realize... perhaps I should just pay for a service that hosts ollama models. This would allow me to access bigger models (I only have 24GB vram) and also save me time when upgrades go poorly.&lt;/p&gt; &lt;p&gt;Any recommendations for such a service?&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/azimux"&gt; /u/azimux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kyc8yg/online_services_that_host_ollama_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kyc8yg/online_services_that_host_ollama_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kyc8yg/online_services_that_host_ollama_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T14:30:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxdddd</id>
    <title>We believe the future of AI is local, private, and personalized.</title>
    <updated>2025-05-28T10:09:55+00:00</updated>
    <author>
      <name>/u/NightShade4275</name>
      <uri>https://old.reddit.com/user/NightShade4275</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That‚Äôs why we built &lt;strong&gt;Cobolt&lt;/strong&gt; ‚Äî a free cross-platform AI assistant that runs entirely on your device.&lt;/p&gt; &lt;p&gt;Cobolt represents our vision for the future of AI assistants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üîí &lt;strong&gt;Privacy-first by design&lt;/strong&gt; ‚Äî everything runs locally&lt;/li&gt; &lt;li&gt;üîß &lt;strong&gt;Extensible&lt;/strong&gt; with our open Model Context Protocol (MCP)&lt;/li&gt; &lt;li&gt;‚öôÔ∏è &lt;strong&gt;Powered by Ollama&lt;/strong&gt; for smooth performance&lt;/li&gt; &lt;li&gt;üß† &lt;strong&gt;Personalized&lt;/strong&gt; without sending your data to the cloud&lt;/li&gt; &lt;li&gt;ü§ù &lt;strong&gt;Built by the community&lt;/strong&gt;, for the community&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We're looking for contributors, testers, and fellow privacy advocates to join us in building the future of personal AI.&lt;/p&gt; &lt;p&gt;ü§ù Contributions Welcome! üåü Star us on &lt;a href="https://github.com/platinum-hill/cobolt"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üì• Try Cobolt on macOS or Windows or Linux. üéâ &lt;a href="https://github.com/platinum-hill/cobolt?tab=readme-ov-file#getting-started"&gt;Get started here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let's build AI that serves you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NightShade4275"&gt; /u/NightShade4275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxdddd/we_believe_the_future_of_ai_is_local_private_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxdddd/we_believe_the_future_of_ai_is_local_private_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kxdddd/we_believe_the_future_of_ai_is_local_private_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-28T10:09:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxh2tv</id>
    <title>i got tired of the errors, so automated debugging using Ollama</title>
    <updated>2025-05-28T13:27:30+00:00</updated>
    <author>
      <name>/u/AntelopeEntire9191</name>
      <uri>https://old.reddit.com/user/AntelopeEntire9191</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kxh2tv/i_got_tired_of_the_errors_so_automated_debugging/"&gt; &lt;img alt="i got tired of the errors, so automated debugging using Ollama" src="https://external-preview.redd.it/bHI2dDhoeGd4aTNmMezr6njSDoFLFdrxC2JtmiXopTq_OV4L1dh4D_3UKGf5.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=68ebbe326e350760cd92cb52a81c12c6ce093c4a" title="i got tired of the errors, so automated debugging using Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got tired of debugging the same Python errors over and over, so I built a CLI the past 2 months that auto-fixes them with local LLMs&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR: Terminal errors ‚Üí automatic fixes using your Ollama models + RAG across your entire codebase. 100% local&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You know when you see `&lt;code&gt;AttributeError\&lt;/code&gt;`for the 69th time? This catches those errors automatically and fixes them using:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Your local Ollama models (whatever you have downloaded)&lt;/li&gt; &lt;li&gt;RAG across your entire codebase for context&lt;/li&gt; &lt;li&gt;Everything stays on your machine&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Just integrated &lt;strong&gt;Claude 4&lt;/strong&gt; support aswell and it's genuinely scary good at debugging tbh&lt;/p&gt; &lt;p&gt;If you curious to see the implementation, its open source: &lt;a href="https://github.com/cloi-ai/cloi"&gt;https://github.com/cloi-ai/cloi&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AntelopeEntire9191"&gt; /u/AntelopeEntire9191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4x302fxgxi3f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxh2tv/i_got_tired_of_the_errors_so_automated_debugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kxh2tv/i_got_tired_of_the_errors_so_automated_debugging/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-28T13:27:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyctco</id>
    <title>Hackathon Idea : Build Your Own Internal Agent using C/ua</title>
    <updated>2025-05-29T14:52:52+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kyctco/hackathon_idea_build_your_own_internal_agent/"&gt; &lt;img alt="Hackathon Idea : Build Your Own Internal Agent using C/ua" src="https://external-preview.redd.it/eWVsN2g5MWlpcTNmMcuFUaVWr4ahtnKH-Dec5LgathfQnwmox07cDNY_yMlL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a69d0d0fcdbb7692436cea7eba2309f994bdd06a" title="Hackathon Idea : Build Your Own Internal Agent using C/ua" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Soon every employee will have their own AI agent handling the repetitive, mundane parts of their job, freeing them to focus on what they're uniquely good at. &lt;/p&gt; &lt;p&gt;Going through YC's recent Request for Startups, I am trying to build an internal agent builder for employees using c/ua.&lt;/p&gt; &lt;p&gt;C/ua provides a infrastructure to securely automate workflows using macOS and Linux containers on Apple Silicon.&lt;/p&gt; &lt;p&gt;We would try to make it work smoothly with everyday tools like your browser, IDE or Slack all while keeping permissions tight and handling sensitive data securely using the latest LLMs.&lt;/p&gt; &lt;p&gt;Github Link : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/of61wceiiq3f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kyctco/hackathon_idea_build_your_own_internal_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kyctco/hackathon_idea_build_your_own_internal_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T14:52:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyigpx</id>
    <title>Trying to read between the lines for Llama 4, how powerful of a machine is required?</title>
    <updated>2025-05-29T18:36:20+00:00</updated>
    <author>
      <name>/u/MrBlinko47</name>
      <uri>https://old.reddit.com/user/MrBlinko47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to understand if my computer can run Llama 4. I remember seeing a post about a rule of thumb for the amount of parameters to the amount of vram required.&lt;/p&gt; &lt;p&gt;Anyone have experience with Llama 4?&lt;/p&gt; &lt;p&gt;I have a 4080 Super so not sure if that is enough to power this model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrBlinko47"&gt; /u/MrBlinko47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kyigpx/trying_to_read_between_the_lines_for_llama_4_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kyigpx/trying_to_read_between_the_lines_for_llama_4_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kyigpx/trying_to_read_between_the_lines_for_llama_4_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T18:36:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ky965w</id>
    <title>GitHub - adeelahmad/mlx-grpo: üß† Train your own DeepSeek-R1 style reasoning model on Mac! First MLX implementation of GRPO - the breakthrough technique behind R1's o1-matching performance. Build mathematical reasoning AI without expensive RLHF. Apple Silicon optimized. üöÄ</title>
    <updated>2025-05-29T12:10:09+00:00</updated>
    <author>
      <name>/u/adeelahmadch</name>
      <uri>https://old.reddit.com/user/adeelahmadch</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ky965w/github_adeelahmadmlxgrpo_train_your_own/"&gt; &lt;img alt="GitHub - adeelahmad/mlx-grpo: üß† Train your own DeepSeek-R1 style reasoning model on Mac! First MLX implementation of GRPO - the breakthrough technique behind R1's o1-matching performance. Build mathematical reasoning AI without expensive RLHF. Apple Silicon optimized. üöÄ" src="https://external-preview.redd.it/4bujr2a9Rx-_7aFLAdHGHINXunCPwtFr2Yoq_Rriz8Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7b8787bb254f32fd8fbaf8bcac12c6686e69b828" title="GitHub - adeelahmad/mlx-grpo: üß† Train your own DeepSeek-R1 style reasoning model on Mac! First MLX implementation of GRPO - the breakthrough technique behind R1's o1-matching performance. Build mathematical reasoning AI without expensive RLHF. Apple Silicon optimized. üöÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adeelahmadch"&gt; /u/adeelahmadch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/adeelahmad/mlx-grpo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ky965w/github_adeelahmadmlxgrpo_train_your_own/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ky965w/github_adeelahmadmlxgrpo_train_your_own/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T12:10:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kysi2j</id>
    <title>Building anti-spyware agent</title>
    <updated>2025-05-30T01:56:59+00:00</updated>
    <author>
      <name>/u/PainCute5235</name>
      <uri>https://old.reddit.com/user/PainCute5235</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which model would you put in charge of your kernal?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PainCute5235"&gt; /u/PainCute5235 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kysi2j/building_antispyware_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kysi2j/building_antispyware_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kysi2j/building_antispyware_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-30T01:56:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kydwb5</id>
    <title>What cool ways can u use your local llm?</title>
    <updated>2025-05-29T15:36:06+00:00</updated>
    <author>
      <name>/u/DOK10101</name>
      <uri>https://old.reddit.com/user/DOK10101</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DOK10101"&gt; /u/DOK10101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kydwb5/what_cool_ways_can_u_use_your_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kydwb5/what_cool_ways_can_u_use_your_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kydwb5/what_cool_ways_can_u_use_your_local_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T15:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyg2bb</id>
    <title>Dual 3090 Build for Inference Questions</title>
    <updated>2025-05-29T17:01:52+00:00</updated>
    <author>
      <name>/u/rhh4x0r</name>
      <uri>https://old.reddit.com/user/rhh4x0r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been scouring the posts here to figure out what might be the best build for local llm inference / homelab server.&lt;/p&gt; &lt;p&gt;I'm picking up 2 RTX 3090s, but I've got the rest of my build to make.&lt;/p&gt; &lt;p&gt;Budget around $1500 for the remaining components. What would you use?&lt;/p&gt; &lt;p&gt;I'm looking at a Ryen 7950, and know I should probably get a 1500W PSU just to be safe. What thoughts you have on processor/mobo/RAM here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rhh4x0r"&gt; /u/rhh4x0r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kyg2bb/dual_3090_build_for_inference_questions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kyg2bb/dual_3090_build_for_inference_questions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kyg2bb/dual_3090_build_for_inference_questions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T17:01:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz9tag</id>
    <title>Ollamam, you'll skip this shit?</title>
    <updated>2025-05-30T17:14:40+00:00</updated>
    <author>
      <name>/u/Available-Mouse-8259</name>
      <uri>https://old.reddit.com/user/Available-Mouse-8259</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any way to bypass the censorship protections in ollama, or is there any other way with a different language model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available-Mouse-8259"&gt; /u/Available-Mouse-8259 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kz9tag/ollamam_youll_skip_this_shit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kz9tag/ollamam_youll_skip_this_shit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kz9tag/ollamam_youll_skip_this_shit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-30T17:14:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyos2x</id>
    <title>Using multiple files from the command line.</title>
    <updated>2025-05-29T22:56:49+00:00</updated>
    <author>
      <name>/u/digger27</name>
      <uri>https://old.reddit.com/user/digger27</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know how to use a prompt and a single file from the command line. I can do something like this: Ollama run gemma3 ‚Äúmy prompt here &amp;lt;File_To_Use.txt I‚Äôm wondering if there is a way to do this with multiple files? I tried something like ‚Äú&amp;lt; File1.txt &amp;amp; File2.txt‚Äù, but it didn‚Äôt work. I have resorted to combining the files into one, but I would rather be able to use them separately.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/digger27"&gt; /u/digger27 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kyos2x/using_multiple_files_from_the_command_line/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kyos2x/using_multiple_files_from_the_command_line/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kyos2x/using_multiple_files_from_the_command_line/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T22:56:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kymosf</id>
    <title>I built a local email summary dashboard</title>
    <updated>2025-05-29T21:25:25+00:00</updated>
    <author>
      <name>/u/vishruth555</name>
      <uri>https://old.reddit.com/user/vishruth555</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kymosf/i_built_a_local_email_summary_dashboard/"&gt; &lt;img alt="I built a local email summary dashboard" src="https://preview.redd.it/6p78z7ujgs3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=367bc299f4e2cf14d71a166dbe4e68f63942f8bf" title="I built a local email summary dashboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I often forget to check my emails, so I developed a tool that summarizes my inbox into a concise dashboard.&lt;/p&gt; &lt;p&gt;Features: ‚Ä¢ Runs locally using Ollama, Gemini api key can also be used for faster summaries at the cost of your privacy &lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ Summarizes Gmail inboxes into a clean, readable format ‚Ä¢ can be run in a container &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Check it out here: &lt;a href="https://github.com/vishruth555/mailBrief"&gt;https://github.com/vishruth555/mailBrief&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôd love to hear your feedback or suggestions for improvement!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vishruth555"&gt; /u/vishruth555 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6p78z7ujgs3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kymosf/i_built_a_local_email_summary_dashboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kymosf/i_built_a_local_email_summary_dashboard/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T21:25:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzc4i9</id>
    <title>Sorry for the NOOB question. :) - How to connect local OLLAMA instance with my MCP-Servers completely offline?</title>
    <updated>2025-05-30T18:47:44+00:00</updated>
    <author>
      <name>/u/Dorfmueller</name>
      <uri>https://old.reddit.com/user/Dorfmueller</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dorfmueller"&gt; /u/Dorfmueller &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/mcp/comments/1kxi0ik/sorry_for_the_noob_question_how_to_connect_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kzc4i9/sorry_for_the_noob_question_how_to_connect_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kzc4i9/sorry_for_the_noob_question_how_to_connect_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-30T18:47:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz7jdc</id>
    <title>Hosting Qwen 3 4B</title>
    <updated>2025-05-30T15:44:51+00:00</updated>
    <author>
      <name>/u/prahasanam-boi</name>
      <uri>https://old.reddit.com/user/prahasanam-boi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I vibe coded a telegram bot that uses Qwen 3 4B model (currently served via ollama). The bot works fine with my 16 gb laptop (No GPU) and can be currently accessed at a time by 3 people (didn't test further). Now I have two questions :&lt;/p&gt; &lt;p&gt;1) What are the ways to host this bot somewhere cheap and reliable. Is there any preference from experienced people here ? (At the most there will be 3/4 people user at a time)&lt;/p&gt; &lt;p&gt;2) Currently the maximum number of users gonna be 4/5, so ollama is fine. However, I am curious to know what is the reliable tool to scale this bot for many users, say in the order of 1000s of users. Any direction in this regard will be helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prahasanam-boi"&gt; /u/prahasanam-boi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kz7jdc/hosting_qwen_3_4b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kz7jdc/hosting_qwen_3_4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kz7jdc/hosting_qwen_3_4b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-30T15:44:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzf75z</id>
    <title>Is there any ollama frontend that can work like novelAI.</title>
    <updated>2025-05-30T20:54:04+00:00</updated>
    <author>
      <name>/u/blueandazure</name>
      <uri>https://old.reddit.com/user/blueandazure</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Where you can set cards for characters locations and themes ect for the ai to remember and you can work to write a story together, but using ollama as the backend.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blueandazure"&gt; /u/blueandazure &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kzf75z/is_there_any_ollama_frontend_that_can_work_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kzf75z/is_there_any_ollama_frontend_that_can_work_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kzf75z/is_there_any_ollama_frontend_that_can_work_like/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-30T20:54:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz4g01</id>
    <title>LLM for text to speech similar to Elevenlabs?</title>
    <updated>2025-05-30T13:38:02+00:00</updated>
    <author>
      <name>/u/sethshoultes</name>
      <uri>https://old.reddit.com/user/sethshoultes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for recommendations for a TTS LLM to create an audio book of my writings. I have over 1.1 million words written and don't want to burn up credits on Elevenlabs.&lt;/p&gt; &lt;p&gt;I'm currently using Ollama with Open WebUI as well as LM Studio on a Mac Studio M3 64gb.&lt;/p&gt; &lt;p&gt;Any recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sethshoultes"&gt; /u/sethshoultes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kz4g01/llm_for_text_to_speech_similar_to_elevenlabs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kz4g01/llm_for_text_to_speech_similar_to_elevenlabs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kz4g01/llm_for_text_to_speech_similar_to_elevenlabs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-30T13:38:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz1pk7</id>
    <title>[Release] Cognito AI Search v1.2.0 ‚Äì Fully Re-imagined, Lightning Fast, Now Prettier Than Ever</title>
    <updated>2025-05-30T11:21:31+00:00</updated>
    <author>
      <name>/u/kekePower</name>
      <uri>https://old.reddit.com/user/kekePower</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; üëã&lt;/p&gt; &lt;p&gt;Just dropped &lt;strong&gt;v1.2.0&lt;/strong&gt; of &lt;a href="https://github.com/kekePower/cognito-ai-search"&gt;Cognito AI Search&lt;/a&gt; ‚Äî and it‚Äôs the biggest update yet.&lt;/p&gt; &lt;p&gt;Over the last few days I‚Äôve completely reimagined the experience with a new UI, performance boosts, PDF export, and deep architectural cleanup. The goal remains the same: private AI + anonymous web search, in one fast and beautiful interface you can fully control.&lt;/p&gt; &lt;p&gt;Here‚Äôs what‚Äôs new:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Major UI/UX Overhaul&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Brand-new ‚ÄúHolographic Shard‚Äù design system (crystalline UI, glow effects, glass morphism)&lt;/li&gt; &lt;li&gt;Dark and light mode support with responsive layouts for all screen sizes&lt;/li&gt; &lt;li&gt;Updated typography, icons, gradients, and no-scroll landing experience&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Performance Improvements&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Build time cut from 5 seconds to 2 seconds (60% faster)&lt;/li&gt; &lt;li&gt;Removed 30,000+ lines of unused UI code and 28 unused dependencies&lt;/li&gt; &lt;li&gt;Reduced bundle size, faster initial page load, improved interactivity&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Enhanced Search &amp;amp; AI&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;200+ categorized search suggestions across 16 AI/tech domains&lt;/li&gt; &lt;li&gt;Export your searches and AI answers as beautifully formatted PDFs (supports LaTeX, Markdown, code blocks)&lt;/li&gt; &lt;li&gt;Modern Next.js 15 form system with client-side transitions and real-time loading feedback&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Improved Architecture&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Modular separation of the &lt;a href="https://ollama.com/"&gt;Ollama&lt;/a&gt; and &lt;a href="https://github.com/searxng/searxng"&gt;SearXNG&lt;/a&gt; integration layers&lt;/li&gt; &lt;li&gt;Reusable React components and hooks&lt;/li&gt; &lt;li&gt;Type-safe API and caching layer with automatic expiration and deduplication&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes &amp;amp; Compatibility&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hydration issues fixed (no more React warnings)&lt;/li&gt; &lt;li&gt;Fixed Firefox layout bugs and Zen browser quirks&lt;/li&gt; &lt;li&gt;Compatible with Ollama 0.9.0+ and self-hosted SearXNG setups&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Still fully local. No tracking. No telemetry. Just you, your machine, and clean search.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Try it now ‚Üí &lt;a href="https://github.com/kekePower/cognito-ai-search"&gt;https://github.com/kekePower/cognito-ai-search&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full release notes ‚Üí &lt;a href="https://github.com/kekePower/cognito-ai-search/blob/main/docs/RELEASE_NOTES_v1.2.0.md"&gt;https://github.com/kekePower/cognito-ai-search/blob/main/docs/RELEASE_NOTES_v1.2.0.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback, issues, or even a PR if you find something worth tweaking. Thanks for all the support so far ‚Äî this has been a blast to build.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kekePower"&gt; /u/kekePower &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kz1pk7/release_cognito_ai_search_v120_fully_reimagined/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kz1pk7/release_cognito_ai_search_v120_fully_reimagined/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kz1pk7/release_cognito_ai_search_v120_fully_reimagined/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-30T11:21:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kza8y8</id>
    <title>The "simplified" model version names are actually increasing confusion</title>
    <updated>2025-05-30T17:31:35+00:00</updated>
    <author>
      <name>/u/florinandrei</name>
      <uri>https://old.reddit.com/user/florinandrei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I understand what Ollama is trying to do - make it dead simple to run LLMs locally. That includes the way the models in the Ollama collection are named.&lt;/p&gt; &lt;p&gt;But I think the &amp;quot;simplification&amp;quot; has been taken too far. The updated DeepSeek-R1 has been released recently. Ollama already had a deepseek-r1 model name in its collection.&lt;/p&gt; &lt;p&gt;Instead of starting a new name, e.g. deepseek-r1-0528 or something, the updates are now overwriting the old name. But wait, not all the old name tags are updated! Only some. Wow.&lt;/p&gt; &lt;p&gt;It's even hard to tell now which tags are the old DeepSeek, and which are the new. It seems like deepseek-r1:8b is the new version. It seems like none of the others are the updated model, but that's a little unclear w.r.t. the biggest model.&lt;/p&gt; &lt;p&gt;Folks, I'm all for simplifying things. But please don't dumb it down to the point where you're increasing confusion. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/florinandrei"&gt; /u/florinandrei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kza8y8/the_simplified_model_version_names_are_actually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kza8y8/the_simplified_model_version_names_are_actually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kza8y8/the_simplified_model_version_names_are_actually/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-30T17:31:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzp7cz</id>
    <title>Thinking models</title>
    <updated>2025-05-31T05:11:46+00:00</updated>
    <author>
      <name>/u/HashMismatch</name>
      <uri>https://old.reddit.com/user/HashMismatch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama has just released 0.9 supporting showing the ‚Äúthought process‚Äù of thinking models (like DeepSeek-R1 and Qwen3) separate to the output. If a LLM is essentially text prediction based on a vector database and conceptual analytics, how is it ‚Äúthinking‚Äù at all? Is the ‚Äúthinking‚Äù output just text prediction as well?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HashMismatch"&gt; /u/HashMismatch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kzp7cz/thinking_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kzp7cz/thinking_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kzp7cz/thinking_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-31T05:11:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzni23</id>
    <title>Best uncensored model for writing stories</title>
    <updated>2025-05-31T03:33:52+00:00</updated>
    <author>
      <name>/u/MilaAmane</name>
      <uri>https://old.reddit.com/user/MilaAmane</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been playing around with ollama and I was wondering what the best uncensored, a I model for storytelling, is not for role play, but just for storytelling. Cause one thing i've noticed about a lot of the other models is that they all have the same. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MilaAmane"&gt; /u/MilaAmane &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kzni23/best_uncensored_model_for_writing_stories/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kzni23/best_uncensored_model_for_writing_stories/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kzni23/best_uncensored_model_for_writing_stories/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-31T03:33:52+00:00</published>
  </entry>
</feed>
