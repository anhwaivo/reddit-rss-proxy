<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-04T18:25:35+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1n6yybs</id>
    <title>[Project/Code] Fine-Tuning LLMs on Windows with GRPO + TRL</title>
    <updated>2025-09-02T23:22:27+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n6yybs/projectcode_finetuning_llms_on_windows_with_grpo/"&gt; &lt;img alt="[Project/Code] Fine-Tuning LLMs on Windows with GRPO + TRL" src="https://preview.redd.it/7ru5p8rw4umf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fce70b2c9fdaae6d869d15d2540623854f22557a" title="[Project/Code] Fine-Tuning LLMs on Windows with GRPO + TRL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a guide and script for fine-tuning open-source LLMs with &lt;strong&gt;GRPO&lt;/strong&gt; (Group-Relative PPO) directly on Windows. No Linux or Colab needed!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Runs natively on Windows.&lt;/li&gt; &lt;li&gt;Supports LoRA + 4-bit quantization.&lt;/li&gt; &lt;li&gt;Includes verifiable rewards for better-quality outputs.&lt;/li&gt; &lt;li&gt;Designed to work on consumer GPUs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;📖 &lt;strong&gt;Blog Post:&lt;/strong&gt; &lt;a href="https://pavankunchalapk.medium.com/windows-friendly-grpo-fine-tuning-with-trl-from-zero-to-verifiable-rewards-f28008c89323"&gt;https://pavankunchalapk.medium.com/windows-friendly-grpo-fine-tuning-with-trl-from-zero-to-verifiable-rewards-f28008c89323&lt;/a&gt;&lt;/p&gt; &lt;p&gt;💻 &lt;strong&gt;Code:&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/trl-ppo-fine-tuning"&gt;https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/trl-ppo-fine-tuning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I had a great time with this project and am currently looking for new opportunities in &lt;strong&gt;Computer Vision and LLMs&lt;/strong&gt;. If you or your team are hiring, I'd love to connect!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Contact Info:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Portolio: &lt;a href="https://pavan-portfolio-tawny.vercel.app/"&gt;https://pavan-portfolio-tawny.vercel.app/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Github: &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7ru5p8rw4umf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6yybs/projectcode_finetuning_llms_on_windows_with_grpo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6yybs/projectcode_finetuning_llms_on_windows_with_grpo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T23:22:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6ply3</id>
    <title>What does the "updated" date actually mean?</title>
    <updated>2025-09-02T17:20:32+00:00</updated>
    <author>
      <name>/u/XdtTransform</name>
      <uri>https://old.reddit.com/user/XdtTransform</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking through the models, I noticed that Gemma3 was &lt;a href="https://imgur.com/tLaswfx"&gt;updated&lt;/a&gt; 2 weeks ago. &lt;/p&gt; &lt;p&gt;I am pretty sure Gemma came out about 4-5 months ago. So what exactly was &amp;quot;updated&amp;quot;?&lt;/p&gt; &lt;p&gt;I downloaded one of the model variants - same one that I normally use and the files appear to be identical. &lt;/p&gt; &lt;p&gt;So what is this update referring to?&lt;/p&gt; &lt;p&gt;P.S. The &lt;a href="https://ollama.com/library/gemma3"&gt;readme&lt;/a&gt; on the model page doesn't provide any information.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XdtTransform"&gt; /u/XdtTransform &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6ply3/what_does_the_updated_date_actually_mean/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6ply3/what_does_the_updated_date_actually_mean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6ply3/what_does_the_updated_date_actually_mean/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T17:20:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6icod</id>
    <title>Running LLM Locally with Ollama + RAG</title>
    <updated>2025-09-02T12:39:32+00:00</updated>
    <author>
      <name>/u/Tough_Wrangler_6075</name>
      <uri>https://old.reddit.com/user/Tough_Wrangler_6075</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n6icod/running_llm_locally_with_ollama_rag/"&gt; &lt;img alt="Running LLM Locally with Ollama + RAG" src="https://external-preview.redd.it/BPsfK6tF48FEZfYsejUp1jtQVo-8HzMuWSqGwZUflzY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b27330825317d976070fbec281feea47b604b58a" title="Running LLM Locally with Ollama + RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tough_Wrangler_6075"&gt; /u/Tough_Wrangler_6075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@zackydzacky/running-llm-locally-with-ollama-rag-cb68ff31e838"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6icod/running_llm_locally_with_ollama_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6icod/running_llm_locally_with_ollama_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T12:39:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7mdy4</id>
    <title>Best current local NSFW TTS model?</title>
    <updated>2025-09-03T18:12:42+00:00</updated>
    <author>
      <name>/u/Stock-Fault5734</name>
      <uri>https://old.reddit.com/user/Stock-Fault5734</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stock-Fault5734"&gt; /u/Stock-Fault5734 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7mdy4/best_current_local_nsfw_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7mdy4/best_current_local_nsfw_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7mdy4/best_current_local_nsfw_tts_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T18:12:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7iizk</id>
    <title>How to use a Hugging Face embedding model in Ollama</title>
    <updated>2025-09-03T15:51:43+00:00</updated>
    <author>
      <name>/u/StringIntelligent763</name>
      <uri>https://old.reddit.com/user/StringIntelligent763</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StringIntelligent763"&gt; /u/StringIntelligent763 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7iizk/how_to_use_a_hugging_face_embedding_model_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7iizk/how_to_use_a_hugging_face_embedding_model_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7iizk/how_to_use_a_hugging_face_embedding_model_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T15:51:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7t0bu</id>
    <title>Microsoft with their sketchy data collection techniques as always</title>
    <updated>2025-09-03T22:27:14+00:00</updated>
    <author>
      <name>/u/Formal_Jeweler_488</name>
      <uri>https://old.reddit.com/user/Formal_Jeweler_488</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n7t0bu/microsoft_with_their_sketchy_data_collection/"&gt; &lt;img alt="Microsoft with their sketchy data collection techniques as always" src="https://external-preview.redd.it/eHQ1aGFraHl6MG5mMSKEsllT2BaBkbUqwvk0riQfqTI-3zznlfwJiR2mpLoX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ded832e0e00248c95a04eb99e4391e0262e136a6" title="Microsoft with their sketchy data collection techniques as always" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guy please pause and check my first chat where he reponds the exact same thing i called it out and, it started gaslighting me into thinking i left the memory on.&lt;/p&gt; &lt;p&gt;Things I discussed with Co Pilot (Mentions after deleting)&lt;/p&gt; &lt;ul&gt; &lt;li&gt; Kohya_ss (To train my face with Loras)&lt;/li&gt; &lt;li&gt;JuggernautXLv9 (Have recommended people on reddit previously)&lt;/li&gt; &lt;li&gt;Continue.dev for BYOK in VS code (you can read the first chat in video he mentions it then as well)&lt;/li&gt; &lt;li&gt;Mafia 3 (Was trying to find best cars and get some help in missions, too lazy to visit youtube.com) &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Irony is I am using Swift Keyboard, Gonna change&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Formal_Jeweler_488"&gt; /u/Formal_Jeweler_488 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x5rqqqcyz0nf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7t0bu/microsoft_with_their_sketchy_data_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7t0bu/microsoft_with_their_sketchy_data_collection/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T22:27:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7auub</id>
    <title>Unsloth gpt-oss gguf in Ollama</title>
    <updated>2025-09-03T10:11:45+00:00</updated>
    <author>
      <name>/u/Tema_Art_7777</name>
      <uri>https://old.reddit.com/user/Tema_Art_7777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama pull certainly works as advertized however when I download the huggingface unsloth gpt-oss-20b or 120b models, I get gibberish output (I am guessing due to template required?). Has anyone gotten it to work with ollama create -f Modelfile? Many thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tema_Art_7777"&gt; /u/Tema_Art_7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7auub/unsloth_gptoss_gguf_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7auub/unsloth_gptoss_gguf_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7auub/unsloth_gptoss_gguf_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T10:11:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n71bil</id>
    <title>Any actual downside to 4 x 3090 ($2400 total) vs RTX pro 6000 ($9000) other than power?</title>
    <updated>2025-09-03T01:09:30+00:00</updated>
    <author>
      <name>/u/devshore</name>
      <uri>https://old.reddit.com/user/devshore</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devshore"&gt; /u/devshore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1n71b95/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n71bil/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n71bil/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T01:09:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7kz78</id>
    <title>Conseils IA pour 3 use cases (email, briefing, chatbot) sur serveur local modeste</title>
    <updated>2025-09-03T17:20:56+00:00</updated>
    <author>
      <name>/u/Maleficent-Hotel8207</name>
      <uri>https://old.reddit.com/user/Maleficent-Hotel8207</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Salut, Je cherche des idées d’IA à faire tourner en local sur ma config : • GTX 1050 low profile (2 Go VRAM) • i3-3400 • 16 Go de RAM&lt;/p&gt; &lt;p&gt;J’ai 3 besoins : • IA pour générer des emails : environ 500 tokens en entrée, 30 tokens en sortie. Réponse en moins de 5 minutes. • IA pour faire un briefing du matin : environ 3000 tokens en entrée, 100 tokens en sortie. Résumé clair et rapide. • Chatbot ultra-rapide : environ 20 tokens en entrée, 20 tokens en sortie. Réponse en moins de 5 secondes.&lt;/p&gt; &lt;p&gt;Je cherche des modèles légers (quantifiés, optimisés, open-source si possible) pour que ça tourne sur cette config limitée. Si vous avez des idées de modèles, de frameworks ou de tips pour que ça passe, je suis preneur !&lt;/p&gt; &lt;p&gt;Merci d’avance !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent-Hotel8207"&gt; /u/Maleficent-Hotel8207 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7kz78/conseils_ia_pour_3_use_cases_email_briefing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7kz78/conseils_ia_pour_3_use_cases_email_briefing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7kz78/conseils_ia_pour_3_use_cases_email_briefing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T17:20:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7vnwy</id>
    <title>Hate AI frameworks? I may have something for you...</title>
    <updated>2025-09-04T00:22:55+00:00</updated>
    <author>
      <name>/u/BeautifulQuote6295</name>
      <uri>https://old.reddit.com/user/BeautifulQuote6295</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you're building with AI you may have found yourself grappling with one of the mainstream frameworks. Since I never really liked no having granular control over what's happening, last year I built a lib called `grafo` for easily AI workflows. It's rules are simple:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nodes contain coroutines to be run&lt;/li&gt; &lt;li&gt;A node only starts executing once all it's parent's have finished running&lt;/li&gt; &lt;li&gt;State is not passed around automatically, but you can do it manually&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These rules come together to make building AI-driven workflows generally easy. However, building around AI has more than DAGs: we need prompt building and mode calling - in comes `grafo ai tools`.&lt;/p&gt; &lt;p&gt;`Grafo AI Tools` is basically a wrapper lib where I've added some very simple prompt managing &amp;amp; model calling, coupled with `grafo`. It's built around the big guys, like `jinja2` and `instructor`.&lt;/p&gt; &lt;p&gt;My goal here is not to create a framework or any set of abstractions that take away from our control of the program as developers - I just wanted to bundle a toolkit which I found useful. In any case, here's the URL: &lt;a href="https://github.com/paulomtts/Grafo-AI-Tools"&gt;https://github.com/paulomtts/Grafo-AI-Tools&lt;/a&gt; . Let me know if you find this interesting at all. I'll be updating it going forward.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BeautifulQuote6295"&gt; /u/BeautifulQuote6295 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7vnwy/hate_ai_frameworks_i_may_have_something_for_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7vnwy/hate_ai_frameworks_i_may_have_something_for_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7vnwy/hate_ai_frameworks_i_may_have_something_for_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T00:22:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7m86q</id>
    <title>Build a Visual Document Index from multiple formats all at once - PDFs, Images, Slides - with ColPali without OCR</title>
    <updated>2025-09-03T18:06:40+00:00</updated>
    <author>
      <name>/u/Whole-Assignment6240</name>
      <uri>https://old.reddit.com/user/Whole-Assignment6240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Would love to share my latest project that builds visual document index from multiple formats in the same flow for PDFs, images using Colpali without OCR. Incremental processing out-of-box and can connect to google drive, s3, azure blob store.&lt;/p&gt; &lt;p&gt;- Detailed write up: &lt;a href="https://cocoindex.io/blogs/multi-format-indexing"&gt;https://cocoindex.io/blogs/multi-format-indexing&lt;/a&gt;&lt;br /&gt; - Fully open sourced: &lt;a href="https://github.com/cocoindex-io/cocoindex/tree/main/examples/multi_format_indexing"&gt;https://github.com/cocoindex-io/cocoindex/tree/main/examples/multi_format_indexing&lt;/a&gt;&lt;br /&gt; (70 lines python on index path)&lt;/p&gt; &lt;p&gt;Looking forward to your suggestions&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Assignment6240"&gt; /u/Whole-Assignment6240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7m86q/build_a_visual_document_index_from_multiple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7m86q/build_a_visual_document_index_from_multiple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7m86q/build_a_visual_document_index_from_multiple/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T18:06:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1n831ad</id>
    <title>my ranking and I am not sure whether it is your ranking</title>
    <updated>2025-09-04T06:48:32+00:00</updated>
    <author>
      <name>/u/Zestyclose-Duty3239</name>
      <uri>https://old.reddit.com/user/Zestyclose-Duty3239</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;best level: claude (for programming) &amp;gt; copilot research&lt;/p&gt; &lt;p&gt;good level: gpt o3 &amp;gt; gpt 4o (before $200/month plan) &amp;gt; copliot deeper thinking&lt;/p&gt; &lt;p&gt;normal level: gpt 4o (after $200/month plan) &amp;gt; supergrok fast and expert (they are working well for uncensored but both of them are almost shitty in ocr) &amp;gt; gemini colab (for python)&lt;/p&gt; &lt;p&gt;almost shitty level: deepseek r1 (tankman everywhere) &amp;gt; gemini &amp;gt; gpt 5 &amp;gt; copilot normal&lt;/p&gt; &lt;p&gt;shitty level: gpt o4-mini &amp;gt; gpt 4.5&lt;/p&gt; &lt;p&gt;I am using a mac pro 2019 with gv100. it is very difficult on running local ollama. I have to use online model.&lt;/p&gt; &lt;p&gt;I believe no company is actually earning money from their ai competition. $30-$300/month subscription is still much lower than the actual cost of the llm model and their gpu base. microsoft, google, meta, amazon, and openai are just wasting money for the market share. they will eventually let us use the weaker model in the next 2-3 years.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zestyclose-Duty3239"&gt; /u/Zestyclose-Duty3239 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n831ad/my_ranking_and_i_am_not_sure_whether_it_is_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n831ad/my_ranking_and_i_am_not_sure_whether_it_is_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n831ad/my_ranking_and_i_am_not_sure_whether_it_is_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T06:48:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n739d3</id>
    <title>ollama 0.11.9 Introducing A Nice CPU/GPU Performance Optimization</title>
    <updated>2025-09-03T02:40:16+00:00</updated>
    <author>
      <name>/u/cornucopea</name>
      <uri>https://old.reddit.com/user/cornucopea</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n739d3/ollama_0119_introducing_a_nice_cpugpu_performance/"&gt; &lt;img alt="ollama 0.11.9 Introducing A Nice CPU/GPU Performance Optimization" src="https://b.thumbs.redditmedia.com/xqOJbGWme_Lnii3nAdQLvJli58h2TtNMtqIsZum6xqs.jpg" title="ollama 0.11.9 Introducing A Nice CPU/GPU Performance Optimization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;This refactors the main run loop of the ollama runner to perform the main GPU intensive tasks (Compute+Floats) in a go routine so we can prepare the next batch in parallel to reduce the amount of time the GPU stalls waiting for the next batch of work.&lt;/p&gt; &lt;p&gt;On metal, I see a 2-3% speedup in token rate. On a single RTX 4090 I see a ~7% speedup.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cs98ja944vmf1.jpg?width=650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=01fd1804e5580b7cc7e85287b110a5cece68865d"&gt;https://preview.redd.it/cs98ja944vmf1.jpg?width=650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=01fd1804e5580b7cc7e85287b110a5cece68865d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.phoronix.com/news/ollama-0.11.9-More-Performance"&gt;https://www.phoronix.com/news/ollama-0.11.9-More-Performance&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cornucopea"&gt; /u/cornucopea &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n739d3/ollama_0119_introducing_a_nice_cpugpu_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n739d3/ollama_0119_introducing_a_nice_cpugpu_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n739d3/ollama_0119_introducing_a_nice_cpugpu_performance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T02:40:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7h8ox</id>
    <title>Anyone else frustrated with AI assistants forgetting context?</title>
    <updated>2025-09-03T15:03:43+00:00</updated>
    <author>
      <name>/u/PrestigiousBet9342</name>
      <uri>https://old.reddit.com/user/PrestigiousBet9342</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I keep bouncing between ChatGPT, Claude, and Perplexity depending on the task. The problem is every new session feels like starting over—I have to re-explain everything.&lt;/p&gt; &lt;p&gt;Just yesterday I wasted 10+ minutes walking perplexity through my project direction again just to get related search if not it is just useless. This morning, ChatGPT didn’t remember anything about my client’s requirements.&lt;/p&gt; &lt;p&gt;The result? I lose a couple of hours each week just re-establishing context. It also makes it hard to keep project discussions consistent across tools. Switching platforms means resetting, and there’s no way to keep a running history of decisions or knowledge.&lt;/p&gt; &lt;p&gt;I’ve tried copy-pasting old chats (messy and unreliable), keeping manual notes (which defeats the point of using AI), and sticking to just one tool (but each has its strengths).&lt;/p&gt; &lt;p&gt;Has anyone actually found a fix for this? I’m especially interested in something that works across different platforms, not just one. On my end, I’ve started tinkering with a solution and would love to hear what features people would find most useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PrestigiousBet9342"&gt; /u/PrestigiousBet9342 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7h8ox/anyone_else_frustrated_with_ai_assistants/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7h8ox/anyone_else_frustrated_with_ai_assistants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7h8ox/anyone_else_frustrated_with_ai_assistants/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T15:03:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n868ux</id>
    <title>Local Code Analyser</title>
    <updated>2025-09-04T10:15:26+00:00</updated>
    <author>
      <name>/u/r00tdr1v3</name>
      <uri>https://old.reddit.com/user/r00tdr1v3</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/r00tdr1v3"&gt; /u/r00tdr1v3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1n868dj/local_code_analyser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n868ux/local_code_analyser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n868ux/local_code_analyser/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T10:15:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7ypzr</id>
    <title>MoE models benchmarked on AMD iGPU</title>
    <updated>2025-09-04T02:47:16+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1n7ypio/moe_models_benchmarked_on_igpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7ypzr/moe_models_benchmarked_on_amd_igpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7ypzr/moe_models_benchmarked_on_amd_igpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T02:47:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7q8cx</id>
    <title>Ollama model most similar to GPT-4o?</title>
    <updated>2025-09-03T20:36:55+00:00</updated>
    <author>
      <name>/u/amstlicht</name>
      <uri>https://old.reddit.com/user/amstlicht</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been researching AI models and am looking for models similar to 4o in terms of personality, mostly. I remember 4o would often suggest interesting paths when I used it for research, it would remember the context and relate it to previous ideas. Does anyone have a recommendation of something similar for Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amstlicht"&gt; /u/amstlicht &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7q8cx/ollama_model_most_similar_to_gpt4o/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7q8cx/ollama_model_most_similar_to_gpt4o/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7q8cx/ollama_model_most_similar_to_gpt4o/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T20:36:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8bpyp</id>
    <title>RX570 compatibility issues</title>
    <updated>2025-09-04T14:28:38+00:00</updated>
    <author>
      <name>/u/Famous-Economics9054</name>
      <uri>https://old.reddit.com/user/Famous-Economics9054</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I’m completely new to LLMs and I tried out ollama on my homeserver which is running on an i5 4570 and RX570 8gb. As far as I understand ollama uses cuda cores on nvidia and rocm on amd. I’ve had issues making it use the rx570 as it is “gfx803” and doesn’t directly support rocm. Does anyone know a fix or workaround for this? &lt;/p&gt; &lt;p&gt;Also I’m sorry if I said something stupid, I’m new to this. &lt;/p&gt; &lt;p&gt;Thanks in advance guys!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Famous-Economics9054"&gt; /u/Famous-Economics9054 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8bpyp/rx570_compatibility_issues/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8bpyp/rx570_compatibility_issues/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8bpyp/rx570_compatibility_issues/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T14:28:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8c48y</id>
    <title>Clustering apple silicon and nvidia gpu based server.</title>
    <updated>2025-09-04T14:43:45+00:00</updated>
    <author>
      <name>/u/Pedroxns</name>
      <uri>https://old.reddit.com/user/Pedroxns</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all, &lt;/p&gt; &lt;p&gt;I was just scrolling through reddit and saw a post about the Apple silicon performance.&lt;/p&gt; &lt;p&gt;I already have 2 mac minis here, one M2 + 8gb and one M4+16gb, nothing too fancy, and i'm already running ollama on a PVE VM with a ryzen 5600g + 3060 12gb + 10gb ram( server has 32gb total), nothing fancy either but runs 4b and 7b models for my frigate and my home assistant instances.&lt;/p&gt; &lt;p&gt;My question is; whould I see any high gain by running ollama on the M4 rather than on the 3060? Could I/should I try clustering the 3 machines to run faster/bigger models?&lt;/p&gt; &lt;p&gt;Thanks in advance for the advices.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pedroxns"&gt; /u/Pedroxns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8c48y/clustering_apple_silicon_and_nvidia_gpu_based/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8c48y/clustering_apple_silicon_and_nvidia_gpu_based/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8c48y/clustering_apple_silicon_and_nvidia_gpu_based/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T14:43:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n87ngf</id>
    <title>Can I use Ollama + OpenWebUI through Docker Engine (In Terminal) or only through Desktop version?</title>
    <updated>2025-09-04T11:33:50+00:00</updated>
    <author>
      <name>/u/PracticalAd6966</name>
      <uri>https://old.reddit.com/user/PracticalAd6966</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am currently on Linux PC and I really need to use Docker Engine and as I understand they have conflicting files so I can use only one of them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PracticalAd6966"&gt; /u/PracticalAd6966 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n87ngf/can_i_use_ollama_openwebui_through_docker_engine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n87ngf/can_i_use_ollama_openwebui_through_docker_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n87ngf/can_i_use_ollama_openwebui_through_docker_engine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T11:33:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8e7uu</id>
    <title>System Crash while Running Local AI Models on MBA M1 – Need Help</title>
    <updated>2025-09-04T16:02:10+00:00</updated>
    <author>
      <name>/u/Separate-Road-3668</name>
      <uri>https://old.reddit.com/user/Separate-Road-3668</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hey Guys,&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I’m currently using a MacBook Air M1 to run some local AI models, but recently I’ve encountered an issue where my system crashes and restarts when I run a model. This has happened a few times, and I’m trying to figure out the exact cause.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Issue:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;When running the model, my system crashes and restarts.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I’ve tried:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;I’ve checked the system logs via the Console app, but there’s nothing helpful there—perhaps the logs got cleared, but I’m not sure.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Could this be related to swap usage, GPU, or CPU pressure? How can I pinpoint the exact cause of the crash? I’m looking for some evidence or debugging tips that can help confirm this.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Bonus Question:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Is there a way to control the resource usage dynamically while running AI models? For instance, can I tell a model to use only a certain percentage (like 40%) of the system’s resources, to prevent crashing while still running other tasks?&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Specs:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MacBook Air M1 (8GB RAM)&lt;br /&gt; Used MLX for the MPS support&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Separate-Road-3668"&gt; /u/Separate-Road-3668 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8e7uu/system_crash_while_running_local_ai_models_on_mba/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8e7uu/system_crash_while_running_local_ai_models_on_mba/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8e7uu/system_crash_while_running_local_ai_models_on_mba/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T16:02:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7uhkv</id>
    <title>Hows your experience running Ollama on Apple Sillicon M1, M2, M3 or M4</title>
    <updated>2025-09-03T23:29:58+00:00</updated>
    <author>
      <name>/u/Cultural-You-7096</name>
      <uri>https://old.reddit.com/user/Cultural-You-7096</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How's the experience, Does it run welll like web versions or is it slow. I'm concerned becuase I want to get a Macbook Pro just to run models .&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cultural-You-7096"&gt; /u/Cultural-You-7096 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7uhkv/hows_your_experience_running_ollama_on_apple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7uhkv/hows_your_experience_running_ollama_on_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7uhkv/hows_your_experience_running_ollama_on_apple/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T23:29:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8hfm7</id>
    <title>I made a simple C# agent that uses local Ollama models to manage my file system</title>
    <updated>2025-09-04T18:02:32+00:00</updated>
    <author>
      <name>/u/cride20</name>
      <uri>https://old.reddit.com/user/cride20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm a huge fan of running models locally and wanted to build something practical with them. So, I created &lt;strong&gt;AI Slop&lt;/strong&gt;: a C# console agent that lets you use natural language to create folders, write files, and manage a workspace.&lt;/p&gt; &lt;p&gt;It's all powered by Ollama and a model capable of tool use (I've had great success with qwen3-coder:30b-a3b-q4_K_M). The agent follows a strict &amp;quot;think-act-observe&amp;quot; loop.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Prompting Strategies:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Strict JSON Output:&lt;/strong&gt; The prompt demands that the only output is a single raw JSON object with two keys: &amp;quot;thought&amp;quot; and &amp;quot;tool_call&amp;quot;. No markdown, no preamble. This makes parsing super reliable.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;One Tool at a Time:&lt;/strong&gt; This is the most critical rule in the prompt. I explicitly forbid the model from trying to chain commands in one response. This forces it to wait for feedback from the environment after every single action, which prevents it from getting lost or making assumptions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Situational Awareness:&lt;/strong&gt; I encourage it to constantly use the GetWorkspaceEntries tool to check the contents of its current directory before acting, which dramatically reduces errors.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Defined Toolset:&lt;/strong&gt; The prompt includes a &amp;quot;manual&amp;quot; for all the available C# functions, including the tool name, description, and argument format (e.g., CreateFile, OpenFolder, TaskDone).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It's been fascinating to see how a well-structured prompt can turn a general-purpose LLM into a reliable tool-using agent.&lt;/p&gt; &lt;p&gt;The project is open source if you want to check out the full system prompt or run it yourself!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/cride9/AISlop"&gt;cride9/AISlop&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What other tools do you think would be useful for an agent like this?&lt;br /&gt; Inspired by the Manus project&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cride20"&gt; /u/cride20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8hfm7/i_made_a_simple_c_agent_that_uses_local_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8hfm7/i_made_a_simple_c_agent_that_uses_local_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8hfm7/i_made_a_simple_c_agent_that_uses_local_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T18:02:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n89sa4</id>
    <title>Most affordable AI computer with GPU (“GPUter”) you can build in 2025?</title>
    <updated>2025-09-04T13:13:36+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n89sa4/most_affordable_ai_computer_with_gpu_gputer_you/"&gt; &lt;img alt="Most affordable AI computer with GPU (“GPUter”) you can build in 2025?" src="https://preview.redd.it/bk6tf5l2e5nf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8da7afc16f4d8ff260c98ad24de5cc8adc50a222" title="Most affordable AI computer with GPU (“GPUter”) you can build in 2025?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bk6tf5l2e5nf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n89sa4/most_affordable_ai_computer_with_gpu_gputer_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n89sa4/most_affordable_ai_computer_with_gpu_gputer_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T13:13:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8cmcq</id>
    <title>Faster Ollama</title>
    <updated>2025-09-04T15:02:27+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One of my favorite Linux benchmark sites. &lt;/p&gt; &lt;p&gt;ollama 0.11.9 Introducing A Nice CPU/GPU Performance Optimization - Phoronix &lt;a href="https://share.google/2lCqH4Imkt2dmeS2G"&gt;https://share.google/2lCqH4Imkt2dmeS2G&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;On metal, I see a 2-3% speedup in token rate. On a single RTX 4090 I see a ~7% speedup.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8cmcq/faster_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n8cmcq/faster_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n8cmcq/faster_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-04T15:02:27+00:00</published>
  </entry>
</feed>
