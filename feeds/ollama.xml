<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-05T20:25:08+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mhpdjw</id>
    <title>Sufficient hardware for Home Assistant usage?</title>
    <updated>2025-08-04T20:51:09+00:00</updated>
    <author>
      <name>/u/jazzypants360</name>
      <uri>https://old.reddit.com/user/jazzypants360</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! I'm new to Ollama, and very intrigued with the idea of running something small in my homelab. The goal is to be able to serve up something capable of backing my Home Assistant installation. Basically, I'm wanting to give my existing Home Assistant (currently voiced by GLaDOS) a bit of a less scripted personality and some ability to make inferences. Before I get too far into the weeds, I'm trying to figure out if the spare hardware I have on hand is sufficient to support this use case... Can anyone comment on whether or not the following might be reasonable to run something like this?&lt;/p&gt; &lt;p&gt;- AMD Phenom II X4 @ 3.2 Ghz, 4 Cores&lt;br /&gt; - 24 GB DDR3 @ 1600 MHz&lt;br /&gt; - GeForce RTX 3060 w/ 12 GB VRAM&lt;/p&gt; &lt;p&gt;I understand that it makes a difference what model(s) I'd be looking to use and all that, but I don't have enough knowledge yet to know what a reasonably sized model would be for this use case.&lt;/p&gt; &lt;p&gt;Any advice would be appreciated! Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jazzypants360"&gt; /u/jazzypants360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhpdjw/sufficient_hardware_for_home_assistant_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhpdjw/sufficient_hardware_for_home_assistant_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mhpdjw/sufficient_hardware_for_home_assistant_usage/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-04T20:51:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhl650</id>
    <title>Run 0.6B LLM 100token/s locally on iPhone</title>
    <updated>2025-08-04T18:15:51+00:00</updated>
    <author>
      <name>/u/Glad-Speaker3006</name>
      <uri>https://old.reddit.com/user/Glad-Speaker3006</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mhl650/run_06b_llm_100tokens_locally_on_iphone/"&gt; &lt;img alt="Run 0.6B LLM 100token/s locally on iPhone" src="https://preview.redd.it/lls41nzqm1hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=26b6959bbf0837900160d9c038af01d00c3f46fb" title="Run 0.6B LLM 100token/s locally on iPhone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glad-Speaker3006"&gt; /u/Glad-Speaker3006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lls41nzqm1hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhl650/run_06b_llm_100tokens_locally_on_iphone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mhl650/run_06b_llm_100tokens_locally_on_iphone/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-04T18:15:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhyl1l</id>
    <title>A model for pure text continuation (not chirpy little Q&amp;A assistant)?</title>
    <updated>2025-08-05T03:31:54+00:00</updated>
    <author>
      <name>/u/ceoln</name>
      <uri>https://old.reddit.com/user/ceoln</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I notice that when using various of the models, even when using the /generate endpoint it will still often reply as a chirpy little first-person assistant, sometimes complete with various meta-stuff, rather than just producing plausible text continuations, which is what I want for my particular use-case.&lt;/p&gt; &lt;p&gt;Like with llama3.2 if I put in a random piece of text like &amp;quot;We came down out of the hills right at dawn&amp;quot;, the reply that comes back is likely to start with &amp;quot;It looks like you're starting a poem or song&amp;quot; or something like that, which doesn't seem like what a raw LLM would predict. gemma3 with start with something like '&amp;lt;think&amp;gt;Okay, the user wrote &amp;quot;We came down out of the hills right at dawn.&amp;quot; I need to figure out what they're asking for.' and qwen3 will say 'This is the opening line of Robert Frost's famous poem, &amp;quot;The Road Not Taken.&amp;quot;' (lol). &lt;/p&gt; &lt;p&gt;None of them I've tried (except llama3.2 once in awhile) will just complete the sentence.&lt;/p&gt; &lt;p&gt;I guess there's some fine-tuning / RLHF in the model weights that pushes them toward being helpful / annoying assistants?&lt;/p&gt; &lt;p&gt;Is there a particular model that's aimed at pure text continuation? Or is there some magic in the API or something else that I'm overlooking to get what I want?&lt;/p&gt; &lt;p&gt;Thanks for any ideas!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ceoln"&gt; /u/ceoln &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhyl1l/a_model_for_pure_text_continuation_not_chirpy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhyl1l/a_model_for_pure_text_continuation_not_chirpy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mhyl1l/a_model_for_pure_text_continuation_not_chirpy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T03:31:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhpvno</id>
    <title>V1PRE - local AI agent for pen testing.</title>
    <updated>2025-08-04T21:10:04+00:00</updated>
    <author>
      <name>/u/New_Pomegranate_1060</name>
      <uri>https://old.reddit.com/user/New_Pomegranate_1060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a local AI agent with a shell backend. It has a full command-line interface, can execute code and scripts, plan multi-step attacks, and do research on the fly.&lt;/p&gt; &lt;p&gt;Itâ€™s not just for suggestions, it can actually act. All local, no API.&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://www.tiktok.com/t/ZT6yYoXNq/"&gt;https://www.tiktok.com/t/ZT6yYoXNq/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Pomegranate_1060"&gt; /u/New_Pomegranate_1060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhpvno/v1pre_local_ai_agent_for_pen_testing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhpvno/v1pre_local_ai_agent_for_pen_testing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mhpvno/v1pre_local_ai_agent_for_pen_testing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-04T21:10:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhjpe3</id>
    <title>qwen3-30b-a3b thinking vs non-thinking</title>
    <updated>2025-08-04T17:23:47+00:00</updated>
    <author>
      <name>/u/randygeneric</name>
      <uri>https://old.reddit.com/user/randygeneric</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mhjpe3/qwen330ba3b_thinking_vs_nonthinking/"&gt; &lt;img alt="qwen3-30b-a3b thinking vs non-thinking" src="https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd872c4c3958b52ad860a6db5ba53994da65552e" title="qwen3-30b-a3b thinking vs non-thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some users have argued in other threads that qwen3-30b-a3b thinking and non-thinking models are nearly identical. So, I summarized the info from their huggingface pages. To me, the thinking model actually seems to have significant advantages in reasoning, coding, and agentic abilities. The only area where the non-thinking instruct model matches or slightly is better is alignment.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wjxg579wc1hf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6d7f1ff7705966ee3ea388b3256fc18f794caf41"&gt;https://preview.redd.it/wjxg579wc1hf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6d7f1ff7705966ee3ea388b3256fc18f794caf41&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Did I miss a point / misinterprete some data?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randygeneric"&gt; /u/randygeneric &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhjpe3/qwen330ba3b_thinking_vs_nonthinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhjpe3/qwen330ba3b_thinking_vs_nonthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mhjpe3/qwen330ba3b_thinking_vs_nonthinking/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-04T17:23:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhvfm0</id>
    <title>Expose port</title>
    <updated>2025-08-05T01:03:47+00:00</updated>
    <author>
      <name>/u/Far_Satisfaction6405</name>
      <uri>https://old.reddit.com/user/Far_Satisfaction6405</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hay I am new to ollama and I have a Ubuntu machine with it installed and I been trying to expose my ollama api but when I do it as localhost:11434 or 0.0.0.0:11434 it works but the moment I try my servers ip 1.1.1.1:11434 it refuses to connect error any ideas how to fix. I followed everything for install to a t and it is taking the port when I check&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far_Satisfaction6405"&gt; /u/Far_Satisfaction6405 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhvfm0/expose_port/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhvfm0/expose_port/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mhvfm0/expose_port/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T01:03:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mh9jac</id>
    <title>Is this the best value machine to run Local LLMs?</title>
    <updated>2025-08-04T10:14:19+00:00</updated>
    <author>
      <name>/u/optimism0007</name>
      <uri>https://old.reddit.com/user/optimism0007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mh9jac/is_this_the_best_value_machine_to_run_local_llms/"&gt; &lt;img alt="Is this the best value machine to run Local LLMs?" src="https://preview.redd.it/4m8omr1w9zgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc0a9517363a39bcaaf77f833193098a6e7ae1dd" title="Is this the best value machine to run Local LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/optimism0007"&gt; /u/optimism0007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4m8omr1w9zgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mh9jac/is_this_the_best_value_machine_to_run_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mh9jac/is_this_the_best_value_machine_to_run_local_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-04T10:14:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mif6k5</id>
    <title>A qestion.</title>
    <updated>2025-08-05T17:16:45+00:00</updated>
    <author>
      <name>/u/warmarduk</name>
      <uri>https://old.reddit.com/user/warmarduk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello... if this is not the right place to ask such question i apologize. I found in my garage my old &amp;quot;toaster&amp;quot;: i5 4570k 16gbram and a rx470-4gb. Can i run any local models on this old junk? Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/warmarduk"&gt; /u/warmarduk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mif6k5/a_qestion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mif6k5/a_qestion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mif6k5/a_qestion/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T17:16:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mifr38</id>
    <title>hallucinations - model specific ?</title>
    <updated>2025-08-05T17:37:16+00:00</updated>
    <author>
      <name>/u/rh4beakyd</name>
      <uri>https://old.reddit.com/user/rh4beakyd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;set gemma3 up and basically every answer has just been not only wildly incorrect but the model has stuck to it's guns and continued being wrong when challenged.&lt;/p&gt; &lt;p&gt;example - best books for RAG implementation using python. model listed three books, none of which exist. gave links to github project which didnt exist, apparently developed by either someone who doesnt exist or ( at a push ) a top coach of a US ladies basketball team. on multiple challenges it flipped from github to git lab, then back to git hub - this all continued a few times before I just gave up.&lt;/p&gt; &lt;p&gt;are they all needing medication or is Gemma3 just 'special' ?&lt;/p&gt; &lt;p&gt;ta&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rh4beakyd"&gt; /u/rh4beakyd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mifr38/hallucinations_model_specific/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mifr38/hallucinations_model_specific/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mifr38/hallucinations_model_specific/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T17:37:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mifs86</id>
    <title>Help! How to remove a model using MacOS app?</title>
    <updated>2025-08-05T17:38:23+00:00</updated>
    <author>
      <name>/u/lost_in_that_moment</name>
      <uri>https://old.reddit.com/user/lost_in_that_moment</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lost_in_that_moment"&gt; /u/lost_in_that_moment &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mifs86/help_how_to_remove_a_model_using_macos_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mifs86/help_how_to_remove_a_model_using_macos_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mifs86/help_how_to_remove_a_model_using_macos_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T17:38:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mifym9</id>
    <title>gpt-oss:20b ollama needs a newer version but i just updated to the latest one?</title>
    <updated>2025-08-05T17:44:54+00:00</updated>
    <author>
      <name>/u/veryhasselglad</name>
      <uri>https://old.reddit.com/user/veryhasselglad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;ollama pull gpt-oss:20b&lt;/p&gt; &lt;p&gt;pulling manifest &lt;/p&gt; &lt;p&gt;Error: pull model manifest: 412: &lt;/p&gt; &lt;p&gt;The model you are attempting to pull requires a newer version of Ollama.&lt;/p&gt; &lt;p&gt;Please download the latest version at:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;https://ollama.com/download&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/veryhasselglad"&gt; /u/veryhasselglad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mifym9/gptoss20b_ollama_needs_a_newer_version_but_i_just/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mifym9/gptoss20b_ollama_needs_a_newer_version_but_i_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mifym9/gptoss20b_ollama_needs_a_newer_version_but_i_just/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T17:44:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mibizg</id>
    <title>Cosine Similarity on Llama 3.2 model</title>
    <updated>2025-08-05T14:58:58+00:00</updated>
    <author>
      <name>/u/thewiirocks</name>
      <uri>https://old.reddit.com/user/thewiirocks</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm testing the embedding functionality to get a feel for working with it. But the results I'm getting aren't making much sense and I'm hoping someone can explain what's going on.&lt;/p&gt; &lt;p&gt;I have the following document for lookup:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;The sky is blue because of a magic spell cast by the space wizard Obi-Wan Kenobi&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;My expectation would be that this would be fairly close to the question:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Why is the sky blue?&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;(Yes, the results are hilarious when you convince Llama to roll with it. ðŸ˜‰)&lt;/p&gt; &lt;p&gt;I would expect to get a cosine distance relatively close to 1.0, such as 0.7 - 0.8. But what I actually get is 0.35399102976301283. Which seems pretty dang far away from the question!&lt;/p&gt; &lt;p&gt;Worse yet, the following document:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Under the sea, under the sea! Down where it's wetter, down where it's better, take it from meeee!!!&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;...computes as 0.45021770805463773. CLOSER to &lt;em&gt;&amp;quot;Why is the sky blue?&amp;quot;&lt;/em&gt; than the actual answer to why the sky is blue!&lt;/p&gt; &lt;p&gt;Digging further, I find that the cosine similarity between &lt;em&gt;&amp;quot;Why Is the sky blue?&amp;quot;&lt;/em&gt; and &lt;em&gt;&amp;quot;The sky is blue&amp;quot;&lt;/em&gt; is 0.418049006847794. Which makes no sense to me.&lt;/p&gt; &lt;p&gt;Am I misunderstanding something here or is this a bad example where I'm fighting the model's knowledge about why the sky is blue?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thewiirocks"&gt; /u/thewiirocks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mibizg/cosine_similarity_on_llama_32_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mibizg/cosine_similarity_on_llama_32_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mibizg/cosine_similarity_on_llama_32_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T14:58:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mihumw</id>
    <title>gpt-oss-20b WAY too slow on M1 MacBook Pro (2020)</title>
    <updated>2025-08-05T18:53:10+00:00</updated>
    <author>
      <name>/u/rafa3790543246789</name>
      <uri>https://old.reddit.com/user/rafa3790543246789</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I just saw the new open-weight models that OpenAI released, and I wanted to try them on my M1 MacBook Pro (from 2020, 16GB). OpenAI said the gpt-oss-20b model can run on most desktops and laptops, but I'm having trouble running it on my Mac.&lt;/p&gt; &lt;p&gt;When I try to run gpt-oss-20b (after closing every app, making room for the 13GB model), it just takes ages to generate single tokens. It's definitely not usable and cannot run on my Mac.&lt;/p&gt; &lt;p&gt;Curious to know if anyone had similar experiences.&lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rafa3790543246789"&gt; /u/rafa3790543246789 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mihumw/gptoss20b_way_too_slow_on_m1_macbook_pro_2020/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mihumw/gptoss20b_way_too_slow_on_m1_macbook_pro_2020/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mihumw/gptoss20b_way_too_slow_on_m1_macbook_pro_2020/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T18:53:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1miib62</id>
    <title>Up-to-date models. How can we know?</title>
    <updated>2025-08-05T19:10:01+00:00</updated>
    <author>
      <name>/u/Visible_Importance68</name>
      <uri>https://old.reddit.com/user/Visible_Importance68</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1miib62/uptodate_models_how_can_we_know/"&gt; &lt;img alt="Up-to-date models. How can we know?" src="https://b.thumbs.redditmedia.com/ejufC56qbiQjTOhJKcdBIuYyhGioAvoXUudQLRLjnpA.jpg" title="Up-to-date models. How can we know?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I would really like to know how to determine a model's most recent training date. For example, in the image I uploaded, the model responds by stating it was updated in 2023. If there were a way to download models where we could see the exact training date, I would prefer to download the most recently updated version instead of an older one. I really appreciate any help you can provide.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ns4enyeb29hf1.png?width=3902&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=164d9036b4f88dad2d7a8376d43a31c617177265"&gt;https://preview.redd.it/ns4enyeb29hf1.png?width=3902&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=164d9036b4f88dad2d7a8376d43a31c617177265&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Visible_Importance68"&gt; /u/Visible_Importance68 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miib62/uptodate_models_how_can_we_know/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miib62/uptodate_models_how_can_we_know/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1miib62/uptodate_models_how_can_we_know/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T19:10:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mij9gu</id>
    <title>Open AI GPT-OSS:20b is bullshit</title>
    <updated>2025-08-05T19:45:48+00:00</updated>
    <author>
      <name>/u/Embarrassed-Way-1350</name>
      <uri>https://old.reddit.com/user/Embarrassed-Way-1350</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have just tried GPT-OSS:20b on my machine. This is the stupidest COT MOE model I have ever interacted with. Open AI chose to shit on the open-source community by releasing this abomination of a model.&lt;/p&gt; &lt;p&gt;Cannot perform basic arithmetic reasoning tasks, Thinks too much, and thinking traits remind me of deepseek-distill:70b, Would have been a great model 3 generations ago. As of today there are a ton of better models out there GLM is a far better alternative. Do not even try this model, Pure shit spray dried into fine powder.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed-Way-1350"&gt; /u/Embarrassed-Way-1350 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mij9gu/open_ai_gptoss20b_is_bullshit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mij9gu/open_ai_gptoss20b_is_bullshit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mij9gu/open_ai_gptoss20b_is_bullshit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T19:45:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mijg4m</id>
    <title>Ollama removed the link to GitHub</title>
    <updated>2025-08-05T19:52:42+00:00</updated>
    <author>
      <name>/u/waescher</name>
      <uri>https://old.reddit.com/user/waescher</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mijg4m/ollama_removed_the_link_to_github/"&gt; &lt;img alt="Ollama removed the link to GitHub" src="https://preview.redd.it/bk6utn9v99hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cac4875cf871bdc5c59cca05a4063a0f9a11ae0b" title="Ollama removed the link to GitHub" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama added a link to their paid cloud &amp;quot;Turbo&amp;quot; subscription and removed the link to their GitHub repository. I don't like where this is going ...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waescher"&gt; /u/waescher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bk6utn9v99hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mijg4m/ollama_removed_the_link_to_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mijg4m/ollama_removed_the_link_to_github/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T19:52:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi63i6</id>
    <title>Any plans to support image generation models in ollama?</title>
    <updated>2025-08-05T11:01:45+00:00</updated>
    <author>
      <name>/u/sh_tomer</name>
      <uri>https://old.reddit.com/user/sh_tomer</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sh_tomer"&gt; /u/sh_tomer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mi63i6/any_plans_to_support_image_generation_models_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mi63i6/any_plans_to_support_image_generation_models_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mi63i6/any_plans_to_support_image_generation_models_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T11:01:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1miel5x</id>
    <title>Building a basic AI bot using Ollama, Angular and Node.js (Beginners )</title>
    <updated>2025-08-05T16:55:18+00:00</updated>
    <author>
      <name>/u/iamsausi</name>
      <uri>https://old.reddit.com/user/iamsausi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1miel5x/building_a_basic_ai_bot_using_ollama_angular_and/"&gt; &lt;img alt="Building a basic AI bot using Ollama, Angular and Node.js (Beginners )" src="https://external-preview.redd.it/0aUhnG-aPIcbdebztln4-6BC3XuZX3qsNFXppAhm1x8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e484a9e25e25eee60dd1ac867cdf5ed6fcbfc4a" title="Building a basic AI bot using Ollama, Angular and Node.js (Beginners )" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamsausi"&gt; /u/iamsausi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/p/629f25f52687"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miel5x/building_a_basic_ai_bot_using_ollama_angular_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1miel5x/building_a_basic_ai_bot_using_ollama_angular_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T16:55:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mifzoc</id>
    <title>gpt-oss OpenAIâ€™s open-weight models</title>
    <updated>2025-08-05T17:45:55+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ollama.com/library/gpt-oss"&gt;https://ollama.com/library/gpt-oss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;OpenAIâ€™s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases. &lt;/p&gt; &lt;p&gt;Edit: v0.11.0 required&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.11.0"&gt;https://github.com/ollama/ollama/releases/tag/v0.11.0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mifzoc/gptoss_openais_openweight_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mifzoc/gptoss_openais_openweight_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mifzoc/gptoss_openais_openweight_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T17:45:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi7f4l</id>
    <title>llama3.2-vision prompt for OCR</title>
    <updated>2025-08-05T12:09:24+00:00</updated>
    <author>
      <name>/u/vir_db</name>
      <uri>https://old.reddit.com/user/vir_db</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to get llama3.2-vision act like an OCR system, in order to transcribe the text inside an image.&lt;/p&gt; &lt;p&gt;The source image is like the page of a book, or a image-only PDF. The text is not handwritten, however I cannot find a working combination of system/user prompt that just report the full text in the image, without adding notes or information about what the image look like. Sometimes the model return the text, but with notes and explanation, sometimes the model return (with the same prompt, often) a lot of strange nonsense character sequences. I tried both simple prompts like&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Extract all text from the image and return it as markdown.\n Do not describe the image or add extra text.\n Only return the text found in the image. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and more complex ones like&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;You are a text extraction expert. Your task is to analyze the provided image and extract all visible text with maximum accuracy. Organize the extracted text into a structured Markdown format. Follow these rules:\n\n 1. Headers: If a section of the text appears larger, bold, or like a heading, format it as a Markdown header (#, ##, or ###).\n 2. Lists: Format bullets or numbered items using Markdown syntax.\n 3. Tables: Use Markdown table format.\n 4. Paragraphs: Keep normal text blocks as paragraphs.\n 5. Emphasis: Use _italics_ and **bold** where needed.\n 6. Links: Format links like [text](url).\n Ensure the extracted text mirrors the document\â€™s structure and formatting.\n Provide only the transcription without any additional comments.&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But none of them is working as expected. Somebody have ideas? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vir_db"&gt; /u/vir_db &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mi7f4l/llama32vision_prompt_for_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mi7f4l/llama32vision_prompt_for_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mi7f4l/llama32vision_prompt_for_ocr/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T12:09:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1miis70</id>
    <title>Why does web search require an ollama account? That's pretty lame</title>
    <updated>2025-08-05T19:27:47+00:00</updated>
    <author>
      <name>/u/Anxious-Bottle7468</name>
      <uri>https://old.reddit.com/user/Anxious-Bottle7468</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1miis70/why_does_web_search_require_an_ollama_account/"&gt; &lt;img alt="Why does web search require an ollama account? That's pretty lame" src="https://preview.redd.it/2x8cvu7i59hf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=514e04fbc3a672eba9c3d94cf3db31d0455e246d" title="Why does web search require an ollama account? That's pretty lame" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxious-Bottle7468"&gt; /u/Anxious-Bottle7468 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2x8cvu7i59hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miis70/why_does_web_search_require_an_ollama_account/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1miis70/why_does_web_search_require_an_ollama_account/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T19:27:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mig4bu</id>
    <title>OpenAI Open Source Models Released!</title>
    <updated>2025-08-05T17:50:35+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI has unleashed two new openâ€‘weight models:&lt;br /&gt; - &lt;strong&gt;GPTâ€‘OSSâ€‘120b (120B parameters)&lt;/strong&gt;&lt;br /&gt; - &lt;strong&gt;GPTâ€‘OSSâ€‘20b (20B parameters)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;It's their first to be actually downloadable and customizable models since GPTâ€‘2 in 2019. It has a &lt;strong&gt;GPLâ€‘friendly license&lt;/strong&gt; (Apache 2.0), allows free modification and commercial use. They're also Chainâ€‘ofâ€‘thought enabled, supports code generation, browsing, and agent use via OpenAI API&lt;/p&gt; &lt;p&gt;&lt;a href="https://openai.com/open-models/"&gt;https://openai.com/open-models/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mig4bu/openai_open_source_models_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mig4bu/openai_open_source_models_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mig4bu/openai_open_source_models_released/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T17:50:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi9zex</id>
    <title>Ollama's new app makes using local AI LLMs on your Windows 11 PC a breeze â€” no more need to chat in the terminal</title>
    <updated>2025-08-05T14:00:04+00:00</updated>
    <author>
      <name>/u/rkhunter_</name>
      <uri>https://old.reddit.com/user/rkhunter_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mi9zex/ollamas_new_app_makes_using_local_ai_llms_on_your/"&gt; &lt;img alt="Ollama's new app makes using local AI LLMs on your Windows 11 PC a breeze â€” no more need to chat in the terminal" src="https://external-preview.redd.it/BhemsEH58Hdd5wlj8RIXGA-DVEqtIWcM1c0-0glb5O8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2963f63bbb5ff18828ac8e0ab68dbd3e71338ac3" title="Ollama's new app makes using local AI LLMs on your Windows 11 PC a breeze â€” no more need to chat in the terminal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rkhunter_"&gt; /u/rkhunter_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.windowscentral.com/artificial-intelligence/ollamas-new-app-makes-using-local-ai-llms-on-your-windows-11-pc-a-breeze-no-more-need-to-chat-in-the-terminal"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mi9zex/ollamas_new_app_makes_using_local_ai_llms_on_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mi9zex/ollamas_new_app_makes_using_local_ai_llms_on_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T14:00:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mig251</id>
    <title>gpt-oss now available on Ollama</title>
    <updated>2025-08-05T17:48:20+00:00</updated>
    <author>
      <name>/u/john_rage</name>
      <uri>https://old.reddit.com/user/john_rage</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mig251/gptoss_now_available_on_ollama/"&gt; &lt;img alt="gpt-oss now available on Ollama" src="https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417" title="gpt-oss now available on Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI has published their opensource gpt model on Ollama.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/john_rage"&gt; /u/john_rage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/library/gpt-oss"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mig251/gptoss_now_available_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mig251/gptoss_now_available_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T17:48:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi6rqk</id>
    <title>Built a lightweight picker that finds the right Ollama model for your hardware (surprisingly useful!)</title>
    <updated>2025-08-05T11:37:10+00:00</updated>
    <author>
      <name>/u/pzarevich</name>
      <uri>https://old.reddit.com/user/pzarevich</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mi6rqk/built_a_lightweight_picker_that_finds_the_right/"&gt; &lt;img alt="Built a lightweight picker that finds the right Ollama model for your hardware (surprisingly useful!)" src="https://external-preview.redd.it/Ymhhd3RucXBzNmhmMUg6vjulbZIeLFSczOGwuN3tGRr8QNKtfAF_eyKX2Orf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfef104774b195d4b72123ead92016e96e4c61d5" title="Built a lightweight picker that finds the right Ollama model for your hardware (surprisingly useful!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pzarevich"&gt; /u/pzarevich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/st0gjoqps6hf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mi6rqk/built_a_lightweight_picker_that_finds_the_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mi6rqk/built_a_lightweight_picker_that_finds_the_right/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T11:37:10+00:00</published>
  </entry>
</feed>
