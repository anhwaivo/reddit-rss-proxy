<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-07-17T11:49:18+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1lywmxq</id>
    <title>Customization</title>
    <updated>2025-07-13T16:11:54+00:00</updated>
    <author>
      <name>/u/BikeDazzling8818</name>
      <uri>https://old.reddit.com/user/BikeDazzling8818</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BikeDazzling8818"&gt; /u/BikeDazzling8818 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/OpenWebUI/comments/1lywmex/customization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lywmxq/customization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lywmxq/customization/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T16:11:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyu3pn</id>
    <title>Is there a good model for generating working mechanical designs?</title>
    <updated>2025-07-13T14:26:25+00:00</updated>
    <author>
      <name>/u/spookyclever</name>
      <uri>https://old.reddit.com/user/spookyclever</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm trying to design a gear system and it would be helpful if I could get a model that could translate my basic ideas to working systems that I could improve on in blender or solid works. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spookyclever"&gt; /u/spookyclever &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyu3pn/is_there_a_good_model_for_generating_working/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyu3pn/is_there_a_good_model_for_generating_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyu3pn/is_there_a_good_model_for_generating_working/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T14:26:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyx1xt</id>
    <title>Trying to get my Ollama model to run faster, is my solution a good one?</title>
    <updated>2025-07-13T16:29:03+00:00</updated>
    <author>
      <name>/u/Convillious</name>
      <uri>https://old.reddit.com/user/Convillious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm a bit confused on how memory storage within the LLM works but from what I‚Äôve seen so far, it is common to pass in a system prompt with the user prompt for every chat that is sent to the LLM.&lt;/p&gt; &lt;p&gt;I have a slow computer and I need this to speed up so I had an idea. My project is a server hosting an LLM which a user can access with an API and receive a response.&lt;/p&gt; &lt;p&gt;Instead of sending a system prompt every time, would it speed things up if on server initialization, I send a system prompt that instructed the LLM on what it‚Äôs supposed to do. And then I stored this information using LangGraphs long term memory, and then whenever a user prompts my LLM it simply derives from its memory when answering?&lt;/p&gt; &lt;p&gt;Sorry if that sounds convoluted but I just figured cutting down on the total number of input tokens would speed things up. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Convillious"&gt; /u/Convillious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyx1xt/trying_to_get_my_ollama_model_to_run_faster_is_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyx1xt/trying_to_get_my_ollama_model_to_run_faster_is_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyx1xt/trying_to_get_my_ollama_model_to_run_faster_is_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T16:29:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyv7ge</id>
    <title>How I use Gemma 3 to help me reply my texts</title>
    <updated>2025-07-13T15:13:03+00:00</updated>
    <author>
      <name>/u/sean01-eth</name>
      <uri>https://old.reddit.com/user/sean01-eth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lyv7ge/how_i_use_gemma_3_to_help_me_reply_my_texts/"&gt; &lt;img alt="How I use Gemma 3 to help me reply my texts" src="https://external-preview.redd.it/rVejZdVoLwcawbSc5Q7BMTfnBvVftpV8Jx64l7lRtUY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c583a73ddcbd440fd51b76ffac1e785cc2ae281b" title="How I use Gemma 3 to help me reply my texts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sean01-eth"&gt; /u/sean01-eth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/48w6qb1mincf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyv7ge/how_i_use_gemma_3_to_help_me_reply_my_texts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyv7ge/how_i_use_gemma_3_to_help_me_reply_my_texts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T15:13:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyv9vm</id>
    <title>Ollama helping me study</title>
    <updated>2025-07-13T15:15:50+00:00</updated>
    <author>
      <name>/u/Economy_Cucumber_702</name>
      <uri>https://old.reddit.com/user/Economy_Cucumber_702</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lyv9vm/ollama_helping_me_study/"&gt; &lt;img alt="Ollama helping me study" src="https://b.thumbs.redditmedia.com/5mWC0Sa6yLocvhKCpyWdyHMzOonYz7tV1G0Gxwp49Bs.jpg" title="Ollama helping me study" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Economy_Cucumber_702"&gt; /u/Economy_Cucumber_702 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lyv9vm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyv9vm/ollama_helping_me_study/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyv9vm/ollama_helping_me_study/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T15:15:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lz2mos</id>
    <title>AMD GPU</title>
    <updated>2025-07-13T20:14:48+00:00</updated>
    <author>
      <name>/u/neofita_</name>
      <uri>https://old.reddit.com/user/neofita_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys I made a mistake and bought GPU based on AMD‚Ä¶is there a lot of work to make different framework than Ollama work with my GPU? Or is there any way to make it work with AMD? Or O should just sell and buy Nvidia? üôà&lt;/p&gt; &lt;p&gt;EDIT: you were all right. It took me 10minutes including downloading everything to make it work with AMD GPU&lt;/p&gt; &lt;p&gt;THANKS ALL! üí™üèøüí™üèø&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neofita_"&gt; /u/neofita_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lz2mos/amd_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lz2mos/amd_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lz2mos/amd_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T20:14:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyodnc</id>
    <title>Podcast generation app -- works with Ollama</title>
    <updated>2025-07-13T09:07:51+00:00</updated>
    <author>
      <name>/u/lfnovo</name>
      <uri>https://old.reddit.com/user/lfnovo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I've built a podcast generation app for people that use Notebook LM for this purpose and would lke some extra capabilities like Ollama support, 1-4 speakers, multiple generation profiles, other voice provider support, and enhanced control on the generation. It also handles extracting content from any file or URL to use in the casts.&lt;/p&gt; &lt;p&gt;It comes with all you need to run, plus a UI for you to create and manage your podcasts.&lt;/p&gt; &lt;p&gt;Community feedback is very welcome. I plan to maintain this actively as its used on another big project of ours.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lfnovo/podcast-creator"&gt;https://github.com/lfnovo/podcast-creator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are some examples of a [4 person debate](&lt;a href="https://soundcloud.com/lfnovo/situational-awareness-podcast"&gt;https://soundcloud.com/lfnovo/situational-awareness-podcast&lt;/a&gt;) and [single speaker lesson](&lt;a href="https://soundcloud.com/lfnovo/single-speaker-podcast-on-situational-awareness"&gt;https://soundcloud.com/lfnovo/single-speaker-podcast-on-situational-awareness&lt;/a&gt;) on the Situational Awareness paper. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lfnovo"&gt; /u/lfnovo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyodnc/podcast_generation_app_works_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyodnc/podcast_generation_app_works_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyodnc/podcast_generation_app_works_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T09:07:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzjle1</id>
    <title>Ollama retaining history?</title>
    <updated>2025-07-14T11:11:38+00:00</updated>
    <author>
      <name>/u/DimensionEnergy</name>
      <uri>https://old.reddit.com/user/DimensionEnergy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;so ive hosted ollama locally on my system on &lt;a href="http://localhost:11434/api/generate"&gt;http://localhost:11434/api/generate&lt;/a&gt; and was testing it out a bit and it seems that between separate fetch calls, ollama seems to be retaining some memory. &lt;/p&gt; &lt;p&gt;i don't understand why this would happen because as much as i have seen modern llms, they don't change their weights during inference. &lt;/p&gt; &lt;p&gt;Scenario:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;makes a query to ollama for topic 1 with a very specific keyword that i have created&lt;/li&gt; &lt;li&gt;makes another query to ollama for a topic that is similar to topic 1 but has a new keyword. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Turns out that the first keyword shows up in the second response aswell. Not always, but this shouldn't happen at all as much as i know&lt;/p&gt; &lt;p&gt;Is there something that i am missing?&lt;br /&gt; I checked the ollama/history file and it only contained prompts that i have made from the terminal using ollama run &amp;lt;model\_name&amp;gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DimensionEnergy"&gt; /u/DimensionEnergy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzjle1/ollama_retaining_history/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzjle1/ollama_retaining_history/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lzjle1/ollama_retaining_history/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-14T11:11:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzm1px</id>
    <title>Is it possible to generate images in open-webui about the generated text?</title>
    <updated>2025-07-14T13:12:45+00:00</updated>
    <author>
      <name>/u/assmaycsgoass</name>
      <uri>https://old.reddit.com/user/assmaycsgoass</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For ex. I ask the AI to write an intro for a story about a small village near a river, describing how it looks etc.&lt;/p&gt; &lt;p&gt;AI generates the text, and the image generation model uses that as a prompt and generates an image right below the paragraph in the window.&lt;/p&gt; &lt;p&gt;Is doing something like this possible? I use comfyui a lot but am a beginner here and was wondering if something like this can be done.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/assmaycsgoass"&gt; /u/assmaycsgoass &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzm1px/is_it_possible_to_generate_images_in_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzm1px/is_it_possible_to_generate_images_in_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lzm1px/is_it_possible_to_generate_images_in_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-14T13:12:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzmpnn</id>
    <title>With ROCm 7 expanding hardware compatibility and offering Windows support, will my 6700xt finally work natively on Windows?</title>
    <updated>2025-07-14T13:41:39+00:00</updated>
    <author>
      <name>/u/toast___ghost</name>
      <uri>https://old.reddit.com/user/toast___ghost</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Struggling to find a GPU compatibility list. Any one know or have a prediction?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/toast___ghost"&gt; /u/toast___ghost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzmpnn/with_rocm_7_expanding_hardware_compatibility_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzmpnn/with_rocm_7_expanding_hardware_compatibility_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lzmpnn/with_rocm_7_expanding_hardware_compatibility_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-14T13:41:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzxmfy</id>
    <title>How do I setup a research mode with ollama?</title>
    <updated>2025-07-14T20:29:56+00:00</updated>
    <author>
      <name>/u/MineDrumPE</name>
      <uri>https://old.reddit.com/user/MineDrumPE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want my local ai models to be able to search the web, is this possible locally? I've searched and haven't found any tutorials.&lt;/p&gt; &lt;p&gt;I want to be able to give ollama research access when I am accessing through webui and through n8n which will probably be 2 different setups I'm assuming?&lt;/p&gt; &lt;p&gt;Thanks for any help&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MineDrumPE"&gt; /u/MineDrumPE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzxmfy/how_do_i_setup_a_research_mode_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzxmfy/how_do_i_setup_a_research_mode_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lzxmfy/how_do_i_setup_a_research_mode_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-14T20:29:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1m18cwf</id>
    <title>Running Ollama with a smooth UI and no technical skills</title>
    <updated>2025-07-16T09:45:15+00:00</updated>
    <author>
      <name>/u/Constant-Post-122</name>
      <uri>https://old.reddit.com/user/Constant-Post-122</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've built a free Ollama client that might be useful for some of you. It lets you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Choose between different small models&lt;/li&gt; &lt;li&gt;Upload files for analysis or summaries&lt;/li&gt; &lt;li&gt;Do web searches&lt;/li&gt; &lt;li&gt;Create and organize custom prompts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Runs on Windows, Mac, and laptops. If you don't have a decent GPU, there's an option to connect to a remote Gemma 12B instance.&lt;/p&gt; &lt;p&gt;Everything stays on your machine - no cloud storage, works offline. Your data never leaves your device, so privacy is actually maintained.&lt;/p&gt; &lt;p&gt;Available at &lt;a href="http://skyllbox.com/"&gt;skyllbox.com&lt;/a&gt; if anyone wants to check it out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Constant-Post-122"&gt; /u/Constant-Post-122 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m18cwf/running_ollama_with_a_smooth_ui_and_no_technical/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m18cwf/running_ollama_with_a_smooth_ui_and_no_technical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m18cwf/running_ollama_with_a_smooth_ui_and_no_technical/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T09:45:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1m12wm4</id>
    <title>üö® Docker container stuck on ‚ÄúWaiting for application startup‚Äù ‚Äî Open WebUI won‚Äôt load in browser</title>
    <updated>2025-07-16T04:02:04+00:00</updated>
    <author>
      <name>/u/0nlyAxeman</name>
      <uri>https://old.reddit.com/user/0nlyAxeman</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/0nlyAxeman"&gt; /u/0nlyAxeman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1m12ij7/docker_container_stuck_on_waiting_for_application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m12wm4/docker_container_stuck_on_waiting_for_application/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m12wm4/docker_container_stuck_on_waiting_for_application/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T04:02:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0gzg2</id>
    <title>We built Explainable AI with pinpointed citations &amp; reasoning ‚Äî works across PDFs, Excel, CSV, Docs &amp; more</title>
    <updated>2025-07-15T12:53:56+00:00</updated>
    <author>
      <name>/u/Effective-Ad2060</name>
      <uri>https://old.reddit.com/user/Effective-Ad2060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just added explainability to our RAG pipeline ‚Äî the AI now shows &lt;strong&gt;pinpointed citations&lt;/strong&gt; down to the &lt;strong&gt;exact paragraph, table row, or cell&lt;/strong&gt; it used to generate its answer.&lt;/p&gt; &lt;p&gt;It doesn‚Äôt just name the source file but also &lt;strong&gt;highlights the exact text&lt;/strong&gt; and lets you &lt;strong&gt;jump directly to that part of the document&lt;/strong&gt;. This works across formats: PDFs, Excel, CSV, Word, PowerPoint, Markdown, and more.&lt;/p&gt; &lt;p&gt;It makes AI answers easy to &lt;strong&gt;trust and verify&lt;/strong&gt;, especially in messy or lengthy enterprise files. You also get insight into the &lt;strong&gt;reasoning&lt;/strong&gt; behind the answer.&lt;/p&gt; &lt;p&gt;It‚Äôs fully open-source: &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;br /&gt; Would love to hear your thoughts or feedback!&lt;/p&gt; &lt;p&gt;üìπ Demo: &lt;a href="https://youtu.be/QWY_jtjRcCM"&gt;https://youtu.be/QWY_jtjRcCM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective-Ad2060"&gt; /u/Effective-Ad2060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m0gzg2/we_built_explainable_ai_with_pinpointed_citations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m0gzg2/we_built_explainable_ai_with_pinpointed_citations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m0gzg2/we_built_explainable_ai_with_pinpointed_citations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-15T12:53:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1e4di</id>
    <title>HELP - How to get the llm to write and read to txt files on linux.</title>
    <updated>2025-07-16T14:31:43+00:00</updated>
    <author>
      <name>/u/TheStronkFemboy</name>
      <uri>https://old.reddit.com/user/TheStronkFemboy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have created a modified version of mistral-nemo:12b, to talk to my friends in my discord server. i managed to get her to send messages in the server, but id like for her to write and read from a text file for long term memory. Thanks in Advanced! :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheStronkFemboy"&gt; /u/TheStronkFemboy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1e4di/help_how_to_get_the_llm_to_write_and_read_to_txt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1e4di/help_how_to_get_the_llm_to_write_and_read_to_txt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m1e4di/help_how_to_get_the_llm_to_write_and_read_to_txt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T14:31:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1im63</id>
    <title>Dreaming Bard - lightweight self-hosted writing assistant for novels using external LLMs (R&amp;D project)</title>
    <updated>2025-07-16T17:19:51+00:00</updated>
    <author>
      <name>/u/leshiy-urban</name>
      <uri>https://old.reddit.com/user/leshiy-urban</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/leshiy-urban"&gt; /u/leshiy-urban &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/selfhosted/comments/1m1iljp/dreaming_bard_lightweight_selfhosted_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1im63/dreaming_bard_lightweight_selfhosted_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m1im63/dreaming_bard_lightweight_selfhosted_writing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T17:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1jj4l</id>
    <title>Recommend hardware for my use case?</title>
    <updated>2025-07-16T17:54:12+00:00</updated>
    <author>
      <name>/u/-how-about-69-</name>
      <uri>https://old.reddit.com/user/-how-about-69-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: My model right now is about 60gb. Uses a context window of 1million tokens. &lt;/p&gt; &lt;p&gt;I‚Äôm curious what kind of hardware should I look to upgrade to? I‚Äôd like something that is also future proofed a bit as I continue to tinker with the model and it gets more demanding. &lt;/p&gt; &lt;p&gt;I was thinking of either a Mac Studio with 512gb of ram or the Ryzen 395 max with 128gb but I‚Äôm open to other suggestions or recommendations. &lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;p&gt;Full context:&lt;/p&gt; &lt;p&gt;So my use case is a bit more extreme than most people. &lt;/p&gt; &lt;p&gt;I am a fan fic writer as a hobby. I have written 6 fan fiction books in my life. Each around 100-200k words. I have built a whole fictional universe for my characters. This is something I really enjoy but I actually hate the writing part of it. This is actually why I never publish anything for money and write under a fictional name as I have never been proud of my books. &lt;/p&gt; &lt;p&gt;Making fictional outlines is super fun for me but creative writing is my weak point and frankly just unenjoyable to me. &lt;/p&gt; &lt;p&gt;I‚Äôve been training an AI model from Ollama on my previous works and all my outlines. I want to use this model to help me refine my prior works to improve the writing and use it for turning my unwritten outlines into full novels. &lt;/p&gt; &lt;p&gt;I know there‚Äôs paid software out there to do this but having used them I felt they produced a product that was no better than my meager skills. I want to actually produce a product that I would be proud to put my name on. &lt;/p&gt; &lt;p&gt;I did test my model and was actually very happy with the result. It‚Äôs not perfect but It‚Äôs much better than the paid models online but it took about 4 weeks to produce a single response which consisted of 1 chapter or about 1500 tokens. &lt;/p&gt; &lt;p&gt;I‚Äôd like to reduce that response time into hours if possible. &lt;/p&gt; &lt;p&gt;My model right now is about 60gb. Uses a context window of 1million tokens. &lt;/p&gt; &lt;p&gt;My rig has 64gb of ram and a 1080ti w/11gb. I also have an old 4tb mechanical hdd as paging for windows otherwise ollama would complain I didn‚Äôt have enough memory. &lt;/p&gt; &lt;p&gt;I‚Äôm curious what kind of hardware should I look to upgrade to? &lt;/p&gt; &lt;p&gt;I was thinking of either a Mac Studio with 512gb of ram or the Ryzen 395 max with 128gb but I‚Äôm open to other suggestions or recommendations. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-how-about-69-"&gt; /u/-how-about-69- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1jj4l/recommend_hardware_for_my_use_case/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1jj4l/recommend_hardware_for_my_use_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m1jj4l/recommend_hardware_for_my_use_case/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T17:54:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1mjip</id>
    <title>Limit gpu usage on MacOs</title>
    <updated>2025-07-16T19:46:42+00:00</updated>
    <author>
      <name>/u/fossa04_</name>
      <uri>https://old.reddit.com/user/fossa04_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I just bought a M3 MacBook Air with 24GB of memory and I wanted to test Ollama.&lt;/p&gt; &lt;p&gt;The problem is that when I submit a prompt the gpu usage goes to 100% and the laptop really hot, there some setting to limit the usage of gpu on ollama? I don't mind if it will be slower, I just want to make it usable.&lt;/p&gt; &lt;p&gt;Bonus question: is it normal that deepseek r1 14B occupy only 1.6GB of memory from activity monitor, am I missing something?&lt;/p&gt; &lt;p&gt;Thank you all! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fossa04_"&gt; /u/fossa04_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1mjip/limit_gpu_usage_on_macos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1mjip/limit_gpu_usage_on_macos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m1mjip/limit_gpu_usage_on_macos/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T19:46:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1qxwf</id>
    <title>Which model would perform well for code auto-completion on my setup?</title>
    <updated>2025-07-16T22:42:24+00:00</updated>
    <author>
      <name>/u/SubstantialAdvisor37</name>
      <uri>https://old.reddit.com/user/SubstantialAdvisor37</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm using 3 x Quadro RTX 4000 GPUs (8GB each). I tested the Qwen2.5 Coder 14B, but it's a bit too slow. The 7B model runs fast, but I‚Äôm wondering if there‚Äôs a good middle ground‚Äîsomething faster than the 14B but potentially more capable than the 7B.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SubstantialAdvisor37"&gt; /u/SubstantialAdvisor37 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1qxwf/which_model_would_perform_well_for_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1qxwf/which_model_would_perform_well_for_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m1qxwf/which_model_would_perform_well_for_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T22:42:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1p2nk</id>
    <title>Ideal Ollama Setup Suggestions needed</title>
    <updated>2025-07-16T21:26:11+00:00</updated>
    <author>
      <name>/u/MUKE-13</name>
      <uri>https://old.reddit.com/user/MUKE-13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi. a novice local-LLM practiser here. i need help setting up ollama (again).&lt;/p&gt; &lt;p&gt;Some background for reference. I had installed it before and played around a bit with some LLM models (gemma3 mainly). I ran a WSL setup with Ollama and Open WEB-UI over a docker container inside WSL. I talked back and forth with gemma, which suggested i install the whole thing with python, as that would be more flexible in case i wanted to start using more advanced things like MCP and Databases (which i totally dont know how to do btw) but i thought, well ok, might give it a shot. I might learn the most by doing it wrong. soon enough, i must have did so, because my open Web-UI stopped working completely, i couldnt pull any new models and the ones installed wouldnt run anymore.&lt;br /&gt; Long story short, i tried uninstalling everything and installing it with docker desktop again but that only made things worse. I thought to myself alright happens and freshly installed windows from scratch because honestly i gave up on fixing the error/s.&lt;br /&gt; Now i would like to ask you guys, what would you suggest? Is it really that much of a difference, if i install it via python or wsl or docker desktop? what are the con's of the different setup-variations, apart from the rather difficult setup procedure for python (bear with me please, im not well versed in that area at all)&lt;br /&gt; I'm happy for any suggestions and help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MUKE-13"&gt; /u/MUKE-13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1p2nk/ideal_ollama_setup_suggestions_needed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1p2nk/ideal_ollama_setup_suggestions_needed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m1p2nk/ideal_ollama_setup_suggestions_needed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T21:26:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1eqlg</id>
    <title>5060TI 16GB or 5070 12GB which one is better to run ai model in ollama</title>
    <updated>2025-07-16T14:55:42+00:00</updated>
    <author>
      <name>/u/actuallytech</name>
      <uri>https://old.reddit.com/user/actuallytech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i just confused to buy 5060ti 16gb vram or 5070 12gb the diffrence is 4 gb in vram , 5070 have more cuda cores but if i cant load ai models there no point having good perfomance &lt;/p&gt; &lt;p&gt;i think i can run gemma3:27b and other models if i have 16gb vram &lt;/p&gt; &lt;p&gt;btw im new into running ai model i guess anyone can help me&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/actuallytech"&gt; /u/actuallytech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1eqlg/5060ti_16gb_or_5070_12gb_which_one_is_better_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1eqlg/5060ti_16gb_or_5070_12gb_which_one_is_better_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m1eqlg/5060ti_16gb_or_5070_12gb_which_one_is_better_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T14:55:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1munw</id>
    <title>How can I access open web ui from across the home network?</title>
    <updated>2025-07-16T19:59:00+00:00</updated>
    <author>
      <name>/u/Doge_gameing</name>
      <uri>https://old.reddit.com/user/Doge_gameing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've finished setting up Ollama and open webui on my home server, but I can't figure out how to use the open web ui from my other devices. I could not use Docker because the server is running Windows Server 2019, so I had to do a Python install of it. im just looking for any solution to use the open webui on my other devices&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Doge_gameing"&gt; /u/Doge_gameing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1munw/how_can_i_access_open_web_ui_from_across_the_home/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1munw/how_can_i_access_open_web_ui_from_across_the_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m1munw/how_can_i_access_open_web_ui_from_across_the_home/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T19:59:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1phub</id>
    <title>Shortcut to inject your desktop UI into AI context window with Ollama</title>
    <updated>2025-07-16T21:43:07+00:00</updated>
    <author>
      <name>/u/louis3195</name>
      <uri>https://old.reddit.com/user/louis3195</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m1phub/shortcut_to_inject_your_desktop_ui_into_ai/"&gt; &lt;img alt="Shortcut to inject your desktop UI into AI context window with Ollama" src="https://external-preview.redd.it/aXFhY3lpZzMxYmRmMRv1DRecgjO_4o9uwyVMt6D0M3tL8nnnWa1A7lkOWT7i.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e54fbf7745b45f915f917cd4df2b0f81718964ba" title="Shortcut to inject your desktop UI into AI context window with Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;git clone https://github.com/mediar-ai/terminator.git cd terminator/terminator-mcp-agent/examples/terminator-ai-summarizer cargo build --release --bin terminator-ai-summarizer # basic UI-dump mode (no AI summarization) ./target/release/terminator-ai-summarizer \--model ollama/gemma-1b \--system-prompt &amp;quot;Summarize this UI tree&amp;quot; \--hotkey &amp;quot;ctrl+alt+j&amp;quot; # AI summarization ./target/release/terminator-ai-summarizer \--model ollama/gemma-3b \--system-prompt &amp;quot;You are a UI assistant.&amp;quot; \--hotkey &amp;quot;ctrl+alt+j&amp;quot; \--ai-mode &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;How it works&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Use cases&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;- Copy paste your whole WhatsApp to clipboard and chat with the content&lt;br /&gt; - Same for Telegram&lt;br /&gt; - Other apps / website where cmd/ctrl A does not work or screenshot does not fit in viewport&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/louis3195"&gt; /u/louis3195 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ap37gjg31bdf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1phub/shortcut_to_inject_your_desktop_ui_into_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m1phub/shortcut_to_inject_your_desktop_ui_into_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T21:43:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2531i</id>
    <title>Struggling with structured data extraction from scanned receipts</title>
    <updated>2025-07-17T11:30:10+00:00</updated>
    <author>
      <name>/u/Easy_Letterhead5466</name>
      <uri>https://old.reddit.com/user/Easy_Letterhead5466</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I‚Äôm working on a project to extract structured data (like company name, date, total, address) from scanned receipts and forms using models like Donut. I‚Äôve prepared my dataset in a prompt format and trained Donut on it, but during evaluation I often get empty predictions. I‚Äôm wondering if this is due to tokenizer issues, formatting, or small dataset size. Has anyone faced similar problems with Donut or other imagetotext models? I‚Äôd also appreciate suggestions on better models or techniques for extracting data from scanned documents or noisy PDFs without using bounding boxes. Thanks! The dataset is SROIE one from kaggle&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Easy_Letterhead5466"&gt; /u/Easy_Letterhead5466 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2531i/struggling_with_structured_data_extraction_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2531i/struggling_with_structured_data_extraction_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m2531i/struggling_with_structured_data_extraction_from/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-17T11:30:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1n2pt</id>
    <title>recommend me an embedding model</title>
    <updated>2025-07-16T20:07:37+00:00</updated>
    <author>
      <name>/u/why_not_my_email</name>
      <uri>https://old.reddit.com/user/why_not_my_email</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm an academic, and over the years I've amassed a library of about 13,000 PDFs of journal articles and books. Over the past few days I put together a basic semantic search app where I can start with a sentence or paragraph (from something I'm writing) and find 10-15 items from my library (as potential sources/citations). &lt;/p&gt; &lt;p&gt;Since this is my first time working with document embeddings, I went with &lt;code&gt;snowflake-arctic-embed2&lt;/code&gt; primarily because it has a relatively long 8k context window. A typical journal article in my field is 8-10k words, and of course books are much longer. &lt;/p&gt; &lt;p&gt;I've found some recommendations to &amp;quot;choose an embedding model based on your use case,&amp;quot; but no actual discussion of &lt;em&gt;which&lt;/em&gt; models work well for different kinds of use cases. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/why_not_my_email"&gt; /u/why_not_my_email &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1n2pt/recommend_me_an_embedding_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1n2pt/recommend_me_an_embedding_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m1n2pt/recommend_me_an_embedding_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T20:07:37+00:00</published>
  </entry>
</feed>
