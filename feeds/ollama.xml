<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-07T14:09:21+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mig4bu</id>
    <title>OpenAI Open Source Models Released!</title>
    <updated>2025-08-05T17:50:35+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI has unleashed two new open‑weight models:&lt;br /&gt; - &lt;strong&gt;GPT‑OSS‑120b (120B parameters)&lt;/strong&gt;&lt;br /&gt; - &lt;strong&gt;GPT‑OSS‑20b (20B parameters)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;It's their first to be actually downloadable and customizable models since GPT‑2 in 2019. It has a &lt;strong&gt;GPL‑friendly license&lt;/strong&gt; (Apache 2.0), allows free modification and commercial use. They're also Chain‑of‑thought enabled, supports code generation, browsing, and agent use via OpenAI API&lt;/p&gt; &lt;p&gt;&lt;a href="https://openai.com/open-models/"&gt;https://openai.com/open-models/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mig4bu/openai_open_source_models_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mig4bu/openai_open_source_models_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mig4bu/openai_open_source_models_released/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T17:50:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjaq2f</id>
    <title>Does anyone use the ollama python library with gpt-oss? I can't get responses using the REST API methods like 'generate'</title>
    <updated>2025-08-06T17:19:20+00:00</updated>
    <author>
      <name>/u/most_crispy_owl</name>
      <uri>https://old.reddit.com/user/most_crispy_owl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can run the model fine and run prompts directly through the cli.&lt;/p&gt; &lt;p&gt;I have a tool I've created that can use models like phi4, qwen, Gemma, and others using the ollama rest API methods from the ollama python library, but I get no response for gpt-oss when I set that as the model in the generate method.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama-python?tab=readme-ov-file#generate"&gt;https://github.com/ollama/ollama-python?tab=readme-ov-file#generate &lt;/a&gt;&lt;/p&gt; &lt;p&gt;The gpt-oss docs say it's supported but I'm wondering if there's something I'm missing here.&lt;/p&gt; &lt;p&gt;Has anyone else had success?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/most_crispy_owl"&gt; /u/most_crispy_owl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjaq2f/does_anyone_use_the_ollama_python_library_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjaq2f/does_anyone_use_the_ollama_python_library_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjaq2f/does_anyone_use_the_ollama_python_library_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T17:19:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1miis70</id>
    <title>Why does web search require an ollama account? That's pretty lame</title>
    <updated>2025-08-05T19:27:47+00:00</updated>
    <author>
      <name>/u/Anxious-Bottle7468</name>
      <uri>https://old.reddit.com/user/Anxious-Bottle7468</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1miis70/why_does_web_search_require_an_ollama_account/"&gt; &lt;img alt="Why does web search require an ollama account? That's pretty lame" src="https://preview.redd.it/2x8cvu7i59hf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=514e04fbc3a672eba9c3d94cf3db31d0455e246d" title="Why does web search require an ollama account? That's pretty lame" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxious-Bottle7468"&gt; /u/Anxious-Bottle7468 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2x8cvu7i59hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miis70/why_does_web_search_require_an_ollama_account/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1miis70/why_does_web_search_require_an_ollama_account/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T19:27:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj57h9</id>
    <title>What American open source model to use?</title>
    <updated>2025-08-06T13:50:08+00:00</updated>
    <author>
      <name>/u/icerio</name>
      <uri>https://old.reddit.com/user/icerio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My boss is wanting to run an AI local. He specifically wants an American-made model. We were originally gonna use gemma3, but since GPT-OSS came out I'm not exactly sure which one to use. I've seen mixed reviews on it, would you use Gemma3 or GPT-OSS? Or is there another model that's better? I know Deepseek and QwQ is top notch, but boss spefically doesn't want to use them lol.&lt;/p&gt; &lt;p&gt;We would be mainly using it to rephrase stuff like emails and to summarize and analyze documents.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/icerio"&gt; /u/icerio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mj57h9/what_american_open_source_model_to_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mj57h9/what_american_open_source_model_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mj57h9/what_american_open_source_model_to_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T13:50:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjc9tv</id>
    <title>Ollama 2x mi50 32GB</title>
    <updated>2025-08-06T18:17:48+00:00</updated>
    <author>
      <name>/u/ExternalTransition17</name>
      <uri>https://old.reddit.com/user/ExternalTransition17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I have two 32GB Mi50s. They run well as long as the model is only running on one GPU. As soon as a model (70B) runs on both, it just outputs garbage.&lt;/p&gt; &lt;p&gt;Does anyone have a solution or idea for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExternalTransition17"&gt; /u/ExternalTransition17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjc9tv/ollama_2x_mi50_32gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjc9tv/ollama_2x_mi50_32gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjc9tv/ollama_2x_mi50_32gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T18:17:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj6d6a</id>
    <title>Hosted GPT-OSS 20B on Runpod for Anyone Facing GPU or Memory Issues</title>
    <updated>2025-08-06T14:36:05+00:00</updated>
    <author>
      <name>/u/Kitchen-Ad5791</name>
      <uri>https://old.reddit.com/user/Kitchen-Ad5791</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I have hosted GPT-OSS 20B on &lt;a href="http://runpod.io"&gt;runpod.io&lt;/a&gt; for anyone who can’t run it due to memory or GPU limitations.&lt;/p&gt; &lt;p&gt;Ollama endpoint: &lt;a href="https://wmpk6m19u6djuf-11434.proxy.runpod.net/"&gt;&lt;strong&gt;https://wmpk6m19u6djuf-11434.proxy.runpod.net/&lt;/strong&gt;&lt;/a&gt; (add this to your config and use)&lt;/p&gt; &lt;p&gt;Open WebUI: &lt;a href="https://wmpk6m19u6djuf-8888.proxy.runpod.net/"&gt;&lt;strong&gt;https://wmpk6m19u6djuf-8888.proxy.runpod.net/&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Login: [&lt;strong&gt;&lt;a href="mailto:admin@admin.com"&gt;admin@admin.com&lt;/a&gt;&lt;/strong&gt;](mailto:&lt;a href="mailto:admin@admin.com"&gt;admin@admin.com&lt;/a&gt;) / &lt;strong&gt;admin&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I don’t care what you do with it, just enjoy. It’ll be live for the next 24 hours or until my credits run out. Hope it helps!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kitchen-Ad5791"&gt; /u/Kitchen-Ad5791 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mj6d6a/hosted_gptoss_20b_on_runpod_for_anyone_facing_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mj6d6a/hosted_gptoss_20b_on_runpod_for_anyone_facing_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mj6d6a/hosted_gptoss_20b_on_runpod_for_anyone_facing_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T14:36:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjdqwh</id>
    <title>Ollama Client (automatic updates?)</title>
    <updated>2025-08-06T19:12:36+00:00</updated>
    <author>
      <name>/u/AreBee73</name>
      <uri>https://old.reddit.com/user/AreBee73</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi the new windows version of Ollama with interface, that you know, does it update automatically or do I need to completely redownload the client every time?&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AreBee73"&gt; /u/AreBee73 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjdqwh/ollama_client_automatic_updates/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjdqwh/ollama_client_automatic_updates/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjdqwh/ollama_client_automatic_updates/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T19:12:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjb3sc</id>
    <title>Setting GPT-OSS' reasoning level</title>
    <updated>2025-08-06T17:33:26+00:00</updated>
    <author>
      <name>/u/dougyitbos</name>
      <uri>https://old.reddit.com/user/dougyitbos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can supposedly set a parameter to make GPT-OSS think less, or more if you'd like.&lt;/p&gt; &lt;p&gt;I've seen how to do it in some other systems but I don't see anything about about how to configure that in ollama.&lt;/p&gt; &lt;p&gt;Can you not? Any tips would be appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dougyitbos"&gt; /u/dougyitbos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjb3sc/setting_gptoss_reasoning_level/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjb3sc/setting_gptoss_reasoning_level/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjb3sc/setting_gptoss_reasoning_level/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T17:33:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mig251</id>
    <title>gpt-oss now available on Ollama</title>
    <updated>2025-08-05T17:48:20+00:00</updated>
    <author>
      <name>/u/john_rage</name>
      <uri>https://old.reddit.com/user/john_rage</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mig251/gptoss_now_available_on_ollama/"&gt; &lt;img alt="gpt-oss now available on Ollama" src="https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417" title="gpt-oss now available on Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI has published their opensource gpt model on Ollama.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/john_rage"&gt; /u/john_rage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/library/gpt-oss"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mig251/gptoss_now_available_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mig251/gptoss_now_available_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T17:48:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj493k</id>
    <title>Ollama to analyze image on Apple M4 16go ?</title>
    <updated>2025-08-06T13:09:43+00:00</updated>
    <author>
      <name>/u/Zoic21</name>
      <uri>https://old.reddit.com/user/Zoic21</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I want to detect if my tarpaulin is on my swimming pool or not. It's a manual tarpaulin so not sensor, best simple solution it's to use a camera (2k but maybe in few month 4k) to detect it and full local solution.&lt;/p&gt; &lt;p&gt;Currently I use gemini with home assistant, it's work but I prefer a local system (and prevent send photo to google).&lt;/p&gt; &lt;p&gt;I wonder if I can do same things on Apple M4 with 16go ram and ollama (I don't known for now which model to use for that). &lt;/p&gt; &lt;p&gt;Analyze of image can take few minutes it's not a problem.&lt;/p&gt; &lt;p&gt;Is that possible ? Is Apple M4 powerful enough?&lt;/p&gt; &lt;p&gt;Thank in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zoic21"&gt; /u/Zoic21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mj493k/ollama_to_analyze_image_on_apple_m4_16go/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mj493k/ollama_to_analyze_image_on_apple_m4_16go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mj493k/ollama_to_analyze_image_on_apple_m4_16go/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T13:09:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mijg4m</id>
    <title>Ollama removed the link to GitHub</title>
    <updated>2025-08-05T19:52:42+00:00</updated>
    <author>
      <name>/u/waescher</name>
      <uri>https://old.reddit.com/user/waescher</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mijg4m/ollama_removed_the_link_to_github/"&gt; &lt;img alt="Ollama removed the link to GitHub" src="https://preview.redd.it/bk6utn9v99hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cac4875cf871bdc5c59cca05a4063a0f9a11ae0b" title="Ollama removed the link to GitHub" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama added a link to their paid cloud &amp;quot;Turbo&amp;quot; subscription and removed the link to their GitHub repository. I don't like where this is going ...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waescher"&gt; /u/waescher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bk6utn9v99hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mijg4m/ollama_removed_the_link_to_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mijg4m/ollama_removed_the_link_to_github/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T19:52:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjb9uw</id>
    <title>Can I use the ollama app with a ollama server on my home network?</title>
    <updated>2025-08-06T17:39:53+00:00</updated>
    <author>
      <name>/u/WorkerUpbeat4780</name>
      <uri>https://old.reddit.com/user/WorkerUpbeat4780</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got the ollama app on my Windows machine, can I somehow point it at a network address where my self hosted ollama instance resides? I'd like to use it as a convenient frontend with web search while using my own ollama server on my home network.&lt;br /&gt; I did not see such an option, but I'm not sure if I missed something.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WorkerUpbeat4780"&gt; /u/WorkerUpbeat4780 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjb9uw/can_i_use_the_ollama_app_with_a_ollama_server_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjb9uw/can_i_use_the_ollama_app_with_a_ollama_server_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjb9uw/can_i_use_the_ollama_app_with_a_ollama_server_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T17:39:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjtgbi</id>
    <title>Crush AI Coding Agent Setup with Ollama - for GPT-OSS model</title>
    <updated>2025-08-07T07:19:08+00:00</updated>
    <author>
      <name>/u/NoobMLDude</name>
      <uri>https://old.reddit.com/user/NoobMLDude</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mjtgbi/crush_ai_coding_agent_setup_with_ollama_for/"&gt; &lt;img alt="Crush AI Coding Agent Setup with Ollama - for GPT-OSS model" src="https://external-preview.redd.it/dVF9vgZbDZkrkGBjDZh1ca-6h-GTLwZaN8Ys-xeskAc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55709d3e7002abc6951207193982d7c17b9105c4" title="Crush AI Coding Agent Setup with Ollama - for GPT-OSS model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoobMLDude"&gt; /u/NoobMLDude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/mdpPUoxCo44"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjtgbi/crush_ai_coding_agent_setup_with_ollama_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjtgbi/crush_ai_coding_agent_setup_with_ollama_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T07:19:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1miyj5e</id>
    <title>Ollama uses internet by default?</title>
    <updated>2025-08-06T07:50:22+00:00</updated>
    <author>
      <name>/u/icecoffee888</name>
      <uri>https://old.reddit.com/user/icecoffee888</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;so after using LM Studio for a while I finally decided to try Ollama now that I'm doing more local LLM coding (for privacy reasons), I was shocked when I saw docs URLs in ollama's thinking log.&lt;/p&gt; &lt;p&gt;Then I figured I have to go to settings an turn airplane mode on to be offline, shouldn't that be the default? is this new? what else is ollama sending to the internet, I have deleted it now, just pretty shocked.&lt;/p&gt; &lt;p&gt;EDIT: Thanks for the answers, it looks like this is new and part of their efforts to sell ollama Turbo, I hope they fail, I wont be trying Ollama again.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/icecoffee888"&gt; /u/icecoffee888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miyj5e/ollama_uses_internet_by_default/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miyj5e/ollama_uses_internet_by_default/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1miyj5e/ollama_uses_internet_by_default/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T07:50:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mij9gu</id>
    <title>Open AI GPT-OSS:20b is bullshit</title>
    <updated>2025-08-05T19:45:48+00:00</updated>
    <author>
      <name>/u/Embarrassed-Way-1350</name>
      <uri>https://old.reddit.com/user/Embarrassed-Way-1350</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have just tried GPT-OSS:20b on my machine. This is the stupidest COT MOE model I have ever interacted with. Open AI chose to shit on the open-source community by releasing this abomination of a model.&lt;/p&gt; &lt;p&gt;Cannot perform basic arithmetic reasoning tasks, Thinks too much, and thinking traits remind me of deepseek-distill:70b, Would have been a great model 3 generations ago. As of today there are a ton of better models out there GLM is a far better alternative. Do not even try this model, Pure shit spray dried into fine powder.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed-Way-1350"&gt; /u/Embarrassed-Way-1350 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mij9gu/open_ai_gptoss20b_is_bullshit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mij9gu/open_ai_gptoss20b_is_bullshit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mij9gu/open_ai_gptoss20b_is_bullshit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T19:45:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjb2vv</id>
    <title>I built an interactive and customizable open-source meeting assistant (runs locally)</title>
    <updated>2025-08-06T17:32:29+00:00</updated>
    <author>
      <name>/u/Square-Test-515</name>
      <uri>https://old.reddit.com/user/Square-Test-515</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mjb2vv/i_built_an_interactive_and_customizable/"&gt; &lt;img alt="I built an interactive and customizable open-source meeting assistant (runs locally)" src="https://external-preview.redd.it/NHRtcTBsemJwZmhmMf7so8CSE-8PmjQuJPM-OgOW72CEju6_3gCE3GVMC0Pl.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c578bfb2644350d52ce8c0b015b669f1b01084b1" title="I built an interactive and customizable open-source meeting assistant (runs locally)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;two friends and I built an open-source meeting assistant. We’re now at the stage where we have an MVP on GitHub that developers can try out (with just 2 terminal commands), and we’d love your feedback on what to improve. 👉 &lt;a href="https://github.com/joinly-ai/joinly"&gt;https://github.com/joinly-ai/joinly&lt;/a&gt; &lt;/p&gt; &lt;p&gt;There are (at least) two very nice things about the assistant: First, it is interactive, so it speaks with you and can solve tasks in real time. Second, it is customizable. Customizable, meaning that you can add your favorite MCP servers so you canaccess their functionality during meetings. In addition, you can also easily change the agent’s system prompt. The meeting assistant also comes with real-time transcription.&lt;/p&gt; &lt;p&gt;A bit more on the technical side: We built a joinly MCP server that enables AI agents to interact in meetings, providing them tools like speak_text, write_chat_message, and leave_meeting and as a resource, the meeting transcript. We connected a sample joinly agent as the MCP client. But you can also connect your own agent to our joinly MCP server to make it meeting-ready.&lt;/p&gt; &lt;p&gt;You can run everything locally using Whisper (STT), Kokoro (TTS), and OLLaMA (LLM). But it is all provider-agnostic, meaning you can also use external APIs like Deepgram for STT, ElevenLabs for TTS, and OpenAI as LLM. &lt;/p&gt; &lt;p&gt;We’re currently using the slogan: “Agentic Meeting Assistant beyond note-taking.” But we’re wondering: Do you have better ideas for a slogan? And what do you think about the concept?&lt;/p&gt; &lt;p&gt;Btw, we’re reaching for the stars right now, so if you like it, consider giving us a star on GitHub :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Square-Test-515"&gt; /u/Square-Test-515 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y3p4plzbpfhf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjb2vv/i_built_an_interactive_and_customizable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjb2vv/i_built_an_interactive_and_customizable/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T17:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjvjyi</id>
    <title>SSL verification and URL change in ZScalar - model not downloading( wrong json error)</title>
    <updated>2025-08-07T09:33:56+00:00</updated>
    <author>
      <name>/u/Anxious-Resort1043</name>
      <uri>https://old.reddit.com/user/Anxious-Resort1043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The issue is I am unable to download the model using ollama pull and give me &amp;lt;“ error at the end of json. &lt;/p&gt; &lt;p&gt;I tried the same with gpt4all and was able to fix that in their source code where it downloads the json from a url. Since my firm uses ZScalar , it starts a thing called isolated environment which probably gives the bad json error. &lt;/p&gt; &lt;p&gt;Anyone been able to workaround ZScalar ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxious-Resort1043"&gt; /u/Anxious-Resort1043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjvjyi/ssl_verification_and_url_change_in_zscalar_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjvjyi/ssl_verification_and_url_change_in_zscalar_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjvjyi/ssl_verification_and_url_change_in_zscalar_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T09:33:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj5h97</id>
    <title>Is this good enough to run Ollama models on my laptop?</title>
    <updated>2025-08-06T14:01:01+00:00</updated>
    <author>
      <name>/u/summitsc</name>
      <uri>https://old.reddit.com/user/summitsc</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mj5h97/is_this_good_enough_to_run_ollama_models_on_my/"&gt; &lt;img alt="Is this good enough to run Ollama models on my laptop?" src="https://preview.redd.it/aqlc28i0oehf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57aba9789460eeabdc7f177180238768d4258ef3" title="Is this good enough to run Ollama models on my laptop?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/summitsc"&gt; /u/summitsc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aqlc28i0oehf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mj5h97/is_this_good_enough_to_run_ollama_models_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mj5h97/is_this_good_enough_to_run_ollama_models_on_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T14:01:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjwdhf</id>
    <title>Run OpenAI’s GPT-OSS on GPU Cloud</title>
    <updated>2025-08-07T10:24:01+00:00</updated>
    <author>
      <name>/u/Wide-Selection8708</name>
      <uri>https://old.reddit.com/user/Wide-Selection8708</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mjwdhf/run_openais_gptoss_on_gpu_cloud/"&gt; &lt;img alt="Run OpenAI’s GPT-OSS on GPU Cloud" src="https://preview.redd.it/8j8n3m9aqkhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6154b3582ca42d5a8b7706412e44c94a4429a69e" title="Run OpenAI’s GPT-OSS on GPU Cloud" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here’s a sample post you can use to promote running OpenAI models (like &lt;code&gt;gpt-oss&lt;/code&gt;) on your GPU cloud platform, such as RunC.AI:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Run OpenAI’s GPT-OSS on&lt;/strong&gt; &lt;a href="http://RunC.AI"&gt;&lt;strong&gt;RunC.AI&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;GPU Cloud&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;OpenAI has officially open-sourced its first large language model series – &lt;strong&gt;GPT-OSS&lt;/strong&gt;. Now you can &lt;strong&gt;run it directly on RunC.AI's powerful GPU cloud&lt;/strong&gt; with just a few clicks.&lt;/p&gt; &lt;h1&gt;Quick Start Guide&lt;/h1&gt; &lt;p&gt;Image link: &lt;a href="https://console.runc.ai/image-detail?image_id=image-hw3jxuvwnzef617q"&gt;https://console.runc.ai/image-detail?image_id=image-hw3jxuvwnzef617q&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Connect to pre-installed GPT-OSS environment:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Username: root@runc.ai Password: runc.ai &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wide-Selection8708"&gt; /u/Wide-Selection8708 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8j8n3m9aqkhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjwdhf/run_openais_gptoss_on_gpu_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjwdhf/run_openais_gptoss_on_gpu_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T10:24:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjsure</id>
    <title>RAG Problem Map 2.0 · debug local RAG stacks that run on Ollama (MIT, offline ready)</title>
    <updated>2025-08-07T06:41:32+00:00</updated>
    <author>
      <name>/u/wfgy_engine</name>
      <uri>https://old.reddit.com/user/wfgy_engine</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;### TL;DR &lt;/p&gt; &lt;p&gt;Running Llama 3 in Ollama is the easy part.&lt;br /&gt; Keeping retrieval sane is the hard part.&lt;br /&gt; Problem Map 2.0 puts the full RAG chain on one screen. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Docs → parse → chunk → embed → index → retrieve → prompt → reason.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Each hop lights up with a diagnostic signal so you see *where* it breaks and *why* (ΔS, E-resonance, coherence drift…). &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Everything is MIT licensed, works offline, no API keys.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;### Why Ollama users will care &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Works with local FAISS or Qdrant, no cloud callbacks&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Flags silent failures that only show up with small VRAM models&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Gives instant feedback when embeddings drift after a model swap&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Shows why multi-GB PDFs crash the context window and how to chunk them right&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If your current flow sometimes spits perfect answers and sometimes gibberish, this map tells you the exact hop that snapped.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;### What is new in 2.0 &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single-page flowchart with live semantic probes&lt;br /&gt;&lt;/li&gt; &lt;li&gt;ΔS heat map plus λ vectors for meaning shift in real time&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Three ready scripts: basic chat, hybrid search, ugly-JSON stress test&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Replayable edge cases: vector junk, wrong skips, context collapse&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All runnable on a laptop that can already load Ollama.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;### Inside the repo &lt;/p&gt; &lt;ul&gt; &lt;li&gt;RAG failure atlas with 16 reproduce-and-fix scripts&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Trace viewer backed by Tesseract so you see embeddings instead of guessing&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Adapters for FAISS, Chroma, Qdrant already wired for Ollama&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Weird-JSON corpus to blow up naive chunkers&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Metrics: ΔS, λ, E_r, σ_ctx, all pre-wired&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;### Links&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full Problem Map → &lt;a href="https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md"&gt;https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Tesseract.js author starred the repo → &lt;a href="https://github.com/bijection?tab=stars"&gt;https://github.com/bijection?tab=stars&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;MIT license. No telemetry.&lt;/strong&gt;&lt;br /&gt; Made possible with a star from the author of Tesseract.js.&lt;br /&gt; Break it, send a PR, it becomes 2.1.&lt;/p&gt; &lt;p&gt;Enjoy hacking, ship faster ^^&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wfgy_engine"&gt; /u/wfgy_engine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjsure/rag_problem_map_20_debug_local_rag_stacks_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjsure/rag_problem_map_20_debug_local_rag_stacks_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjsure/rag_problem_map_20_debug_local_rag_stacks_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T06:41:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjsi21</id>
    <title>Evolution Tree of Encoder / Decoder / Encoder-Decoder Models.</title>
    <updated>2025-08-07T06:19:32+00:00</updated>
    <author>
      <name>/u/theMonarch776</name>
      <uri>https://old.reddit.com/user/theMonarch776</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mjsi21/evolution_tree_of_encoder_decoder_encoderdecoder/"&gt; &lt;img alt="Evolution Tree of Encoder / Decoder / Encoder-Decoder Models." src="https://preview.redd.it/m087zu1gajhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f63bd45ac59bf4cce9cbdd69df3bb8da15faf3c6" title="Evolution Tree of Encoder / Decoder / Encoder-Decoder Models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theMonarch776"&gt; /u/theMonarch776 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m087zu1gajhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjsi21/evolution_tree_of_encoder_decoder_encoderdecoder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjsi21/evolution_tree_of_encoder_decoder_encoderdecoder/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T06:19:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjijvj</id>
    <title>Model providing correct date. How?</title>
    <updated>2025-08-06T22:18:24+00:00</updated>
    <author>
      <name>/u/rob_0</name>
      <uri>https://old.reddit.com/user/rob_0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mjijvj/model_providing_correct_date_how/"&gt; &lt;img alt="Model providing correct date. How?" src="https://preview.redd.it/qgm95fpi4hhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58652231a2353b4c6ada948a586c08a18baa0391" title="Model providing correct date. How?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sure there's a good reason, but as you can see I've not included any facts and functions in my system prompt. How is it able to output a correct date?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rob_0"&gt; /u/rob_0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qgm95fpi4hhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjijvj/model_providing_correct_date_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjijvj/model_providing_correct_date_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T22:18:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjxd87</id>
    <title>gpt-oss-20b running on i5 Iris Xe graphics with 16 gb ram.</title>
    <updated>2025-08-07T11:20:16+00:00</updated>
    <author>
      <name>/u/Western_Art_3308</name>
      <uri>https://old.reddit.com/user/Western_Art_3308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mjxd87/gptoss20b_running_on_i5_iris_xe_graphics_with_16/"&gt; &lt;img alt="gpt-oss-20b running on i5 Iris Xe graphics with 16 gb ram." src="https://preview.redd.it/zg28ifvd0lhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8921388569ea35dfe3d1ca4612738bd5072d2883" title="gpt-oss-20b running on i5 Iris Xe graphics with 16 gb ram." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Western_Art_3308"&gt; /u/Western_Art_3308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zg28ifvd0lhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjxd87/gptoss20b_running_on_i5_iris_xe_graphics_with_16/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjxd87/gptoss20b_running_on_i5_iris_xe_graphics_with_16/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T11:20:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjtd84</id>
    <title>gpt-oss thoughts</title>
    <updated>2025-08-07T07:13:46+00:00</updated>
    <author>
      <name>/u/cride20</name>
      <uri>https://old.reddit.com/user/cride20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share my experience with their open weight model..&lt;/p&gt; &lt;p&gt;I had some CMake issues (Not really familiar with it) and most LLMs like Gemini 2.5Pro, GPT free model with thinking couldn't really help me at all. They started hallucinating after 2-3error message.&lt;br /&gt; gpt-oss:20b just nailed the error in 1 prompt and fixed the whole building process..&lt;/p&gt; &lt;p&gt;So far it looks really promising while it's being really small (just 20b)&lt;/p&gt; &lt;p&gt;What are your thoughts so far?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cride20"&gt; /u/cride20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjtd84/gptoss_thoughts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjtd84/gptoss_thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjtd84/gptoss_thoughts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T07:13:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjo9ki</id>
    <title>Best models under 16GB</title>
    <updated>2025-08-07T02:31:33+00:00</updated>
    <author>
      <name>/u/Mr-Barack-Obama</name>
      <uri>https://old.reddit.com/user/Mr-Barack-Obama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a macbook m4 pro with 16gb ram so I've made a list of the best models that should be able to run on it. I will be using llama.cpp without GUI for max efficiency but even still some of these quants might be too large to have enough space for reasoning tokens and some context, idk I'm a noob.&lt;/p&gt; &lt;p&gt;Here are the best models and quants for under 16gb based on my research, but I'm a noob and I haven't tested these yet:&lt;/p&gt; &lt;p&gt;Best Reasoning: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Qwen3-32B (IQ3_XXS 12.8 GB)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Qwen3-30B-A3B-Thinking-2507 (IQ3_XS 12.7GB)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Qwen 14B (Q6_K_L 12.50GB)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;gpt-oss-20b (12GB)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Phi-4-reasoning-plus (Q6_K_L 12.3 GB)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Best non reasoning:&lt;br /&gt; 1. gemma-3-27b (IQ4_XS 14.77GB)&lt;br /&gt; 2. Mistral-Small-3.2-24B-Instruct-2506 (Q4_K_L 14.83GB)&lt;br /&gt; 3. gemma-3-12b (Q8_0 12.5 GB) &lt;/p&gt; &lt;p&gt;My use cases:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Accurately summarizing meeting transcripts.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Creating an anonymized/censored version of a a document by removing confidential info while keeping everything else the same.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Asking survival questions for scenarios without internet like camping. I think medgemma-27b-text would be cool for this scenario.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I prefer maximum accuracy and intelligence over speed. How's my list and quants for my use cases? Am I missing any model or have something wrong? Any advice for getting the best performance with llama.cpp on a macbook m4pro 16gb?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr-Barack-Obama"&gt; /u/Mr-Barack-Obama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjo9ki/best_models_under_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjo9ki/best_models_under_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjo9ki/best_models_under_16gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T02:31:33+00:00</published>
  </entry>
</feed>
