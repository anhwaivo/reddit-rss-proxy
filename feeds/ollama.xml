<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-05-03T00:27:01+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1kbexs2</id>
    <title>Ollama hangs after first successful response on Qwen3-30b-a3b MoE</title>
    <updated>2025-04-30T12:14:07+00:00</updated>
    <author>
      <name>/u/simracerman</name>
      <uri>https://old.reddit.com/user/simracerman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else experience this? I'm on the latest stable 0.6.6, and latest models from Ollama and Unsloth.&lt;/p&gt; &lt;p&gt;Confirmed this is Vulkan related. &lt;a href="https://github.com/ggml-org/llama.cpp/issues/13164"&gt;https://github.com/ggml-org/llama.cpp/issues/13164&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simracerman"&gt; /u/simracerman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kbexs2/ollama_hangs_after_first_successful_response_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kbexs2/ollama_hangs_after_first_successful_response_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kbexs2/ollama_hangs_after_first_successful_response_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-30T12:14:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbu9bq</id>
    <title>Qwen3-30B-A3B: Ollama vs LMStudio Speed Discrepancy (30tk/s vs 150tk/s) – Help?</title>
    <updated>2025-04-30T23:12:42+00:00</updated>
    <author>
      <name>/u/az-big-z</name>
      <uri>https://old.reddit.com/user/az-big-z</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/az-big-z"&gt; /u/az-big-z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1kbu7wf/qwen330ba3b_ollama_vs_lmstudio_speed_discrepancy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kbu9bq/qwen330ba3b_ollama_vs_lmstudio_speed_discrepancy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kbu9bq/qwen330ba3b_ollama_vs_lmstudio_speed_discrepancy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-30T23:12:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbpy1m</id>
    <title>Why is Ollama no longer using my GPU ?</title>
    <updated>2025-04-30T20:04:11+00:00</updated>
    <author>
      <name>/u/Unique-Algae-1145</name>
      <uri>https://old.reddit.com/user/Unique-Algae-1145</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I usually use big models since they give more accurate responses but the results I get recently are pretty bad (describing the conversation instead of actually replying, ignoring the system I tried avoiding naration through that as well but nothing (gemma3:27b btw) I am sending it some data in the form of a JSON object which might cause the issue but it worked pretty well at one point).&lt;br /&gt; ANYWAYS I wanted to go try 1b models mostly just to have a fast reply and suddenly I can't, Ollama only uses the CPU and takes a nice while. the logs says the GPU is not supported but it worked pretty recently too&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unique-Algae-1145"&gt; /u/Unique-Algae-1145 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kbpy1m/why_is_ollama_no_longer_using_my_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kbpy1m/why_is_ollama_no_longer_using_my_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kbpy1m/why_is_ollama_no_longer_using_my_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-30T20:04:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbwq8a</id>
    <title>Question about training ollama to determine if jobs on LinkedIn are real or not</title>
    <updated>2025-05-01T01:14:19+00:00</updated>
    <author>
      <name>/u/RobertTAS</name>
      <uri>https://old.reddit.com/user/RobertTAS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;System: m4 Mac Min 16 gig RAM&lt;br /&gt; Model: llama3&lt;/p&gt; &lt;p&gt;I have been building a chrome extension that will analyze jobs posted on LinkedIn and determine if they are real or not. I have the program all set up and its passing prompts to my ollama running on my mac and sending back a response. I now want to train the model to make it more fine tuned and return better results (like, if the company is a fortune 500 company, return true). I am new to LLM's and such and wanted to get some advice on the best way to go about training a model for usage. Any advice would be great! Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobertTAS"&gt; /u/RobertTAS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kbwq8a/question_about_training_ollama_to_determine_if/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kbwq8a/question_about_training_ollama_to_determine_if/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kbwq8a/question_about_training_ollama_to_determine_if/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-01T01:14:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1kc6w5r</id>
    <title>Seeking help for laptop setup</title>
    <updated>2025-05-01T11:56:45+00:00</updated>
    <author>
      <name>/u/Caputperson</name>
      <uri>https://old.reddit.com/user/Caputperson</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Caputperson"&gt; /u/Caputperson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1kc4k7o/seeking_help_for_laptop_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kc6w5r/seeking_help_for_laptop_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kc6w5r/seeking_help_for_laptop_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-01T11:56:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbzm2x</id>
    <title>Phi-4-Reasoning : Microsoft's new reasoning LLMs</title>
    <updated>2025-05-01T03:50:16+00:00</updated>
    <author>
      <name>/u/mehul_gupta1997</name>
      <uri>https://old.reddit.com/user/mehul_gupta1997</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kbzm2x/phi4reasoning_microsofts_new_reasoning_llms/"&gt; &lt;img alt="Phi-4-Reasoning : Microsoft's new reasoning LLMs" src="https://external-preview.redd.it/g5F1gVCpPlpMm9zBzdnXvnig0KKYcZHQYuBvdwYT5ec.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=340ff9b4c61fd1348a72f787c3066111783ab711" title="Phi-4-Reasoning : Microsoft's new reasoning LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehul_gupta1997"&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/1-7jlvL2zu8?si=aiYbB2dVeiqkjF-j"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kbzm2x/phi4reasoning_microsofts_new_reasoning_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kbzm2x/phi4reasoning_microsofts_new_reasoning_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-01T03:50:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kc6r3t</id>
    <title>"please respond as if you were &lt;x&gt;, here are texts you can copy their style from"</title>
    <updated>2025-05-01T11:48:44+00:00</updated>
    <author>
      <name>/u/prankousky</name>
      <uri>https://old.reddit.com/user/prankousky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everybody,&lt;/p&gt; &lt;p&gt;I am currently experimenting with ollama and Home Assistant. I would like my Voice Assistant to answer as if they were a specific person. However, this person is not famous (enough), my LLMs don't know the way this person speaks.&lt;/p&gt; &lt;p&gt;Can I somehow provide context? For example, ebooks, interviews, or similar?&lt;/p&gt; &lt;p&gt;Example: &lt;/p&gt; &lt;p&gt;&amp;quot;Which colors can dogs see?&amp;quot; &amp;gt; &amp;quot;Dogs have a unique visual system that is different from humans. While they can't see the world in all its vibrant colors like we do, their color vision is still quite impressive.&amp;quot; &lt;/p&gt; &lt;p&gt;VS &lt;/p&gt; &lt;p&gt;&amp;quot;Which colors can dogs see? Answer as if you were Donald Trump.&amp;quot; &amp;gt; &amp;quot;Folks, let me tell you, nobody knows more about dogs than I do. Believe me, I've made some of the greatest deals with dog owners, fantastic people, really top-notch folks. And one thing they always ask me is, &amp;quot;Mr. Trump, what colors can my dog see?&amp;quot;&amp;quot;.&lt;/p&gt; &lt;p&gt;In this specific case, I want my answers to sound as if they were given by German author / comic &amp;quot;Heinz Strunk&amp;quot;. If I tell, for example, llama3.1:8b to reply as if they were this person, it will answer, but the wording is nothing like this person would actually talk. However, there are tons of texts I could provide. &lt;/p&gt; &lt;p&gt;Is this possible with some additional tool or plugin? I am currently using open-webui and the linux command line to query ollama. &lt;/p&gt; &lt;p&gt;And if not: is anybody here aware of a project that might create (or modify an existing??) LLM to adapt to some particular person's speech style? &lt;/p&gt; &lt;p&gt;Sorry, I'm quite new to this and wasn't even sure what to search for in order to solve this. Perhaps you can point me in the right direction :) Thank you in advance for your ideas.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prankousky"&gt; /u/prankousky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kc6r3t/please_respond_as_if_you_were_x_here_are_texts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kc6r3t/please_respond_as_if_you_were_x_here_are_texts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kc6r3t/please_respond_as_if_you_were_x_here_are_texts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-01T11:48:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kc8k79</id>
    <title>Llama 4 News…?</title>
    <updated>2025-05-01T13:20:25+00:00</updated>
    <author>
      <name>/u/AdCompetitive6193</name>
      <uri>https://old.reddit.com/user/AdCompetitive6193</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone heard if/when Llama 4 Scout will be released on Ollama? &lt;/p&gt; &lt;p&gt;Also has anyone tried Llama 4? What do you think of it? What hardware are you running it on? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdCompetitive6193"&gt; /u/AdCompetitive6193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kc8k79/llama_4_news/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kc8k79/llama_4_news/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kc8k79/llama_4_news/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-01T13:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kcmlib</id>
    <title>How to include a timestamp directive in Ollama prompts?</title>
    <updated>2025-05-01T23:17:26+00:00</updated>
    <author>
      <name>/u/beedunc</name>
      <uri>https://old.reddit.com/user/beedunc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My prompts are for coding, and it would be excellent to just include a %DATE-TIME% directive for the model to include in its output for version control. &lt;/p&gt; &lt;p&gt;Possible?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beedunc"&gt; /u/beedunc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kcmlib/how_to_include_a_timestamp_directive_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kcmlib/how_to_include_a_timestamp_directive_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kcmlib/how_to_include_a_timestamp_directive_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-01T23:17:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kcdttm</id>
    <title>What front-end chat interface do yall use???</title>
    <updated>2025-05-01T17:02:31+00:00</updated>
    <author>
      <name>/u/PegThaStallion</name>
      <uri>https://old.reddit.com/user/PegThaStallion</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PegThaStallion"&gt; /u/PegThaStallion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kcdttm/what_frontend_chat_interface_do_yall_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kcdttm/what_frontend_chat_interface_do_yall_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kcdttm/what_frontend_chat_interface_do_yall_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-01T17:02:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kcuvkq</id>
    <title>Image classification</title>
    <updated>2025-05-02T07:09:57+00:00</updated>
    <author>
      <name>/u/LobsterInYakuze-2113</name>
      <uri>https://old.reddit.com/user/LobsterInYakuze-2113</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am using ollama/gemma3 to sort a folder with images into predefined categories. It works but falls behind with more nuanced differentiations. Would I be better off using a different strategy? Another model from huggingface? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LobsterInYakuze-2113"&gt; /u/LobsterInYakuze-2113 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kcuvkq/image_classification/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kcuvkq/image_classification/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kcuvkq/image_classification/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T07:09:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kcsr77</id>
    <title>llama 4 system requirements</title>
    <updated>2025-05-02T04:48:04+00:00</updated>
    <author>
      <name>/u/Ok_Cartographer8945</name>
      <uri>https://old.reddit.com/user/Ok_Cartographer8945</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am noob in this space and want to use this model is an OCR what is the system requirements for it.&lt;/p&gt; &lt;p&gt;And can I run it on 20 to 24 GB VRAM gpu&lt;/p&gt; &lt;p&gt;And what should be required CPU, RAM etc&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/llama4"&gt;https://ollama.com/library/llama4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Can you tell me required specs for each model variant.&lt;/p&gt; &lt;p&gt;SCOUT, MAVERICK&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Cartographer8945"&gt; /u/Ok_Cartographer8945 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kcsr77/llama_4_system_requirements/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kcsr77/llama_4_system_requirements/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kcsr77/llama_4_system_requirements/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T04:48:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kd0swo</id>
    <title>How do i make ollama use my Radeon 6750xt?</title>
    <updated>2025-05-02T13:21:08+00:00</updated>
    <author>
      <name>/u/_TheTrickster_</name>
      <uri>https://old.reddit.com/user/_TheTrickster_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title says most of it, i just can't get it to work, it keeps just using my CPU and system memory, doesn't even touch my GPU, i want to use it because it does have 12GB of vram so it might come in handy, certainly more handy than using like 40% of my processor and ram to run a base model. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_TheTrickster_"&gt; /u/_TheTrickster_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kd0swo/how_do_i_make_ollama_use_my_radeon_6750xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kd0swo/how_do_i_make_ollama_use_my_radeon_6750xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kd0swo/how_do_i_make_ollama_use_my_radeon_6750xt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T13:21:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kdawcx</id>
    <title>Hey All, Im a bit noob looking for some pointers.</title>
    <updated>2025-05-02T20:25:14+00:00</updated>
    <author>
      <name>/u/Painter_Turbulent</name>
      <uri>https://old.reddit.com/user/Painter_Turbulent</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, Im a bit new to ai despite having a couple of decades as a techie, built my own pcs, run supprted windows run some game servers on linux. a lot of dabbling really. &lt;/p&gt; &lt;p&gt;Ive now installed docker, wls, python, openwebui, and im trying to get ollama with rocm (rocm amd drivers is installed on the linux install. installed to use my 9070xt that i have. (5950x, 64gig ddr4). to start testing. &lt;/p&gt; &lt;p&gt;I think i might have installed some things in the wrong place. and im a lil confused. as to how to get my openwebui to actually see the ollama i installed at all. ive been reading posts for a few days trying to understand and i feel as if the rabbit hole just goes deeper and depper. every day a new level i have to try to understand. &lt;/p&gt; &lt;p&gt;is there a guide specifically for rocm support with ollama running through docker/openwebui and a 9070xt. or should i start with somethign simpler? to get my old brain working along with this. there are so many opiniong on what is best its just overwhelming atm. how did you guys start? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Painter_Turbulent"&gt; /u/Painter_Turbulent &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdawcx/hey_all_im_a_bit_noob_looking_for_some_pointers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdawcx/hey_all_im_a_bit_noob_looking_for_some_pointers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kdawcx/hey_all_im_a_bit_noob_looking_for_some_pointers/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T20:25:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kclr4s</id>
    <title>Llama4 with vison</title>
    <updated>2025-05-01T22:38:28+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ollama.com/library/llama4"&gt;https://ollama.com/library/llama4&lt;/a&gt; &lt;a href="https://github.com/ollama/ollama/releases/tag/v0.6.7"&gt;https://github.com/ollama/ollama/releases/tag/v0.6.7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kclr4s/llama4_with_vison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kclr4s/llama4_with_vison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kclr4s/llama4_with_vison/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-01T22:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kd5q7x</id>
    <title>If you have adequate GPU, does the CPU matter?</title>
    <updated>2025-05-02T16:47:55+00:00</updated>
    <author>
      <name>/u/Havanatha_banana</name>
      <uri>https://old.reddit.com/user/Havanatha_banana</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an old xeon server and it has multiple pcie lanes, so I'm planning to get a few cheaper GPUs with high vrams to meet the 50gb vram requirement from 70b.&lt;/p&gt; &lt;p&gt;Context: For work, I want to train an AI to be able to format documents into a specific style, to fill it gaps of our documentations with transcriptions from videos. We have way too many meetings that are actually important but no minutes have been taken.&lt;/p&gt; &lt;p&gt;As such, I wanna start self hosting. I'm not sure if it's appropriate, but 70b seems to be default for my application?&lt;/p&gt; &lt;p&gt;As such, I need to run multiple GPUs to get it to work. I have an old xeon server with multiple pcie lanes. So hopefully that will work? Or should I settle for a smaller model, like 8b? Accuracy is more important here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Havanatha_banana"&gt; /u/Havanatha_banana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kd5q7x/if_you_have_adequate_gpu_does_the_cpu_matter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kd5q7x/if_you_have_adequate_gpu_does_the_cpu_matter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kd5q7x/if_you_have_adequate_gpu_does_the_cpu_matter/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T16:47:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kcybew</id>
    <title>Qwen3 disable thinking in Ollama?</title>
    <updated>2025-05-02T11:12:24+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, How to get instant answer and disable thinking in qwen3 with Ollama?&lt;/p&gt; &lt;p&gt;Qwen3 pages states this is possible: &amp;quot;This flexibility allows users to control how much “thinking” the model performs based on the task at hand. For example, harder problems can be tackled with extended reasoning, while easier ones can be answered directly without delay.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kcybew/qwen3_disable_thinking_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kcybew/qwen3_disable_thinking_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kcybew/qwen3_disable_thinking_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T11:12:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1kdfx4z</id>
    <title>Should it be possible to download Mistral Small 3.1 from ollama, use llama.cpp to split/shard it reassemble it, then use it in ollama?</title>
    <updated>2025-05-03T00:12:42+00:00</updated>
    <author>
      <name>/u/aaronr_90</name>
      <uri>https://old.reddit.com/user/aaronr_90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need to move the model from one network to another network via DVD’s. Inconvenient, I know. I downloaded the gguf using the ids in the manifest, went through the process of splitting, burning, moving, merging, and when created a new model with a modfile everything went fine. When I tried to run it, ollama tried to phone home to get the manifest file, with obviously no avail. None of my other models I moved gave me this error.&lt;/p&gt; &lt;p&gt;Maybe I missed the mmproj file.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aaronr_90"&gt; /u/aaronr_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdfx4z/should_it_be_possible_to_download_mistral_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdfx4z/should_it_be_possible_to_download_mistral_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kdfx4z/should_it_be_possible_to_download_mistral_small/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-03T00:12:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kd4r8l</id>
    <title>How to use bigger models</title>
    <updated>2025-05-02T16:07:10+00:00</updated>
    <author>
      <name>/u/newz2000</name>
      <uri>https://old.reddit.com/user/newz2000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have found many posts asking a similar question, but the answers don't make sense to me. I do not know what quantization and some of these other terms mean when it comes to the different model formats, and when I get AI tools to explain it to me, they're either too simple or too complex.&lt;/p&gt; &lt;p&gt;I have an older workstation with an 8gb GTX 1070 GPU. I'm having a lot of fun using it with 9b and smaller models (thanks to the suggestion for Gemma 3 4b - it packs quite a bunch). Specifically, I like Qwen 2.5, Gemma 3 and Qwen 3. Most of what I do is process, summarize, and reorganize info, but I have used Qwen 2.5 coder to write some shell scripts and automations.&lt;/p&gt; &lt;p&gt;I have bumped into a project that just fails with the smaller models. By failing, I mean it tries, and thinks its doing a good job, but the output is not nearly the quality of what a human would do. It works in ChatGPT and Gemini and I suspect it would work with bigger models.&lt;/p&gt; &lt;p&gt;I am due for a computer upgrade. My desktop is a 2019 i9 iMac with 64gb of RAM. I think I will replace it with a maxed out Mac mini or a mid-range Mac Studio. Or I could upgrade the graphics card in the workstation that has the 1070 gpu. (or I could do both)&lt;/p&gt; &lt;p&gt;My goal is to simply take legal and technical information and allow a human or an AI to ask questions about the information and generate useful reports on that info. The task that currently fails is having the AI generate follow-up questions of the human to clarify the goals without hallucinating.&lt;/p&gt; &lt;p&gt;What do I need to do to use bigger models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newz2000"&gt; /u/newz2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kd4r8l/how_to_use_bigger_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kd4r8l/how_to_use_bigger_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kd4r8l/how_to_use_bigger_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T16:07:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kdc1kj</id>
    <title>Localhost request MUCH slower than cmd</title>
    <updated>2025-05-02T21:13:56+00:00</updated>
    <author>
      <name>/u/Unique-Algae-1145</name>
      <uri>https://old.reddit.com/user/Unique-Algae-1145</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not talking a bit slower I am talking a LOT slower about 10-20x times.&lt;br /&gt; Using 1B model I receive the full message in about a second but when calling it through localhost it takes about 20 seconds to receive the response.&lt;br /&gt; This is not an additive delay either using bigger model increases the time delay.&lt;br /&gt; 27b might take several seconds to be done but receiving a response after sending POST request on localhost it takes minutes.&lt;br /&gt; I don't see anything on system to go ever past 60% usage so I don't think it's a bottleneck.&lt;br /&gt; Ollama appears to immidiately allocate the memory and CPU to the task as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unique-Algae-1145"&gt; /u/Unique-Algae-1145 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdc1kj/localhost_request_much_slower_than_cmd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdc1kj/localhost_request_much_slower_than_cmd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kdc1kj/localhost_request_much_slower_than_cmd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T21:13:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kd95or</id>
    <title>Train Better Computer-Use AI by Creating Human Demonstration Datasets</title>
    <updated>2025-05-02T19:10:53+00:00</updated>
    <author>
      <name>/u/Original-Thanks-8118</name>
      <uri>https://old.reddit.com/user/Original-Thanks-8118</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The C/ua team just released a new tutorial that shows how anyone with macOS can contribute to training better computer-use AI models by recording their own human demonstrations.&lt;/p&gt; &lt;p&gt;Why this matters:&lt;/p&gt; &lt;p&gt;One of the biggest challenges in developing AI that can use computers effectively is the lack of high-quality human demonstration data. Current computer-use models often fail to capture the nuanced ways humans navigate interfaces, recover from errors, and adapt to changing contexts.&lt;/p&gt; &lt;p&gt;This tutorial walks through using C/ua's Computer-Use Interface (CUI) with a Gradio UI to:&lt;/p&gt; &lt;p&gt;- Record your natural computer interactions in a sandbox macOS environment&lt;/p&gt; &lt;p&gt;- Organize and tag your demonstrations for maximum research value&lt;/p&gt; &lt;p&gt;- Share your datasets on Hugging Face to advance computer-use AI research&lt;/p&gt; &lt;p&gt;What makes human demonstrations particularly valuable is that they capture aspects of computer use that synthetic data misses:&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Natural pacing&lt;/strong&gt; - the rhythm of real human computer use&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Error recovery&lt;/strong&gt; - how humans detect and fix mistakes&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Context-sensitive actions&lt;/strong&gt; - adjusting behavior based on changing UI states&lt;/p&gt; &lt;p&gt;You can find the blog-post here: &lt;a href="https://trycua.com/blog/training-computer-use-models-trajectories-1"&gt;https://trycua.com/blog/training-computer-use-models-trajectories-1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The only requirements are Python 3.10+ and macOS Sequoia.&lt;/p&gt; &lt;p&gt;Would love to hear if anyone else has been working on computer-use AI and your thoughts on this approach to building better training datasets!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Original-Thanks-8118"&gt; /u/Original-Thanks-8118 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.trycua.com/blog/training-computer-use-models-trajectories-1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kd95or/train_better_computeruse_ai_by_creating_human/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kd95or/train_better_computeruse_ai_by_creating_human/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T19:10:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1kdaogc</id>
    <title>Ollama Show model gpu/cpu layer</title>
    <updated>2025-05-02T20:15:55+00:00</updated>
    <author>
      <name>/u/lavoie005</name>
      <uri>https://old.reddit.com/user/lavoie005</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I searched a way to Find out many GPU offload layers a model have.&lt;/p&gt; &lt;p&gt;I also want to set the parameter for execute all layer in my gpu.&lt;/p&gt; &lt;p&gt;You can do it with lm studio But I ain't find any way to get how many layers the model have in Ollama&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lavoie005"&gt; /u/lavoie005 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdaogc/ollama_show_model_gpucpu_layer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdaogc/ollama_show_model_gpucpu_layer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kdaogc/ollama_show_model_gpucpu_layer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T20:15:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kdfc3k</id>
    <title>I was confused at first about what model types mean, but this clarified it, I found 5-bit works the best on my system without sacrificing speed or accuracy. 16 bit works, but sluggish. If you're new to this...explanations of terminology in post.</title>
    <updated>2025-05-02T23:44:04+00:00</updated>
    <author>
      <name>/u/Sandalwoodincencebur</name>
      <uri>https://old.reddit.com/user/Sandalwoodincencebur</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kdfc3k/i_was_confused_at_first_about_what_model_types/"&gt; &lt;img alt="I was confused at first about what model types mean, but this clarified it, I found 5-bit works the best on my system without sacrificing speed or accuracy. 16 bit works, but sluggish. If you're new to this...explanations of terminology in post." src="https://preview.redd.it/fbv2s5a0fgye1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2149c48778b4bb213734368aa65fe75a9c1a5747" title="I was confused at first about what model types mean, but this clarified it, I found 5-bit works the best on my system without sacrificing speed or accuracy. 16 bit works, but sluggish. If you're new to this...explanations of terminology in post." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These are different versions (tags) of the &lt;strong&gt;Llama3.2&lt;/strong&gt; model, each optimized for specific use cases, sizes, and quantization levels. Here's a breakdown of what each part of the naming convention means:&lt;/p&gt; &lt;h1&gt;1. Model Size (1b, 3b)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;1b&lt;/code&gt;: A 1-billion-parameter version of the model (smaller, faster, less resource-intensive).&lt;/li&gt; &lt;li&gt;&lt;code&gt;3b&lt;/code&gt;: A 3-billion-parameter version (larger, more capable, but requires more RAM/VRAM).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. Model Type (text, instruct)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;text&lt;/code&gt;: A base model trained for general text generation (like autocompletion or story writing).&lt;/li&gt; &lt;li&gt;&lt;code&gt;instruct&lt;/code&gt;: Fine-tuned for instruction-following (better at following prompts like chatbots or assistants).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. Precision &amp;amp; Quantization (fp16, q2_K, q4_K_M, etc.)&lt;/h1&gt; &lt;p&gt;Quantization reduces model size by lowering numerical precision, trading off some accuracy for efficiency.&lt;/p&gt; &lt;h1&gt;Full Precision (No Quantization)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;fp16&lt;/code&gt;: Full 16-bit floating-point precision (highest quality, largest file size).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What q5_K_M What q5_K_M Specifically Means&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;code&gt;q5&lt;/code&gt; → 5-bit quantization &lt;ul&gt; &lt;li&gt;Weights stored in 5 bits (vs. 32 bits in &lt;code&gt;fp32&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Balances size and accuracy (better than &lt;code&gt;q4&lt;/code&gt;, smaller than &lt;code&gt;q6&lt;/code&gt;).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;_K&lt;/code&gt; → &amp;quot;K-means&amp;quot; clustering &lt;ul&gt; &lt;li&gt;Groups similar weights together to minimize precision loss.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;_M&lt;/code&gt; → &amp;quot;Middle&amp;quot; precision tier &lt;ul&gt; &lt;li&gt;Optimized for &lt;strong&gt;balanced&lt;/strong&gt; performance (other options: &lt;code&gt;_S&lt;/code&gt; for small, &lt;code&gt;_L&lt;/code&gt; for large).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sandalwoodincencebur"&gt; /u/Sandalwoodincencebur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fbv2s5a0fgye1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdfc3k/i_was_confused_at_first_about_what_model_types/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kdfc3k/i_was_confused_at_first_about_what_model_types/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T23:44:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kdc3z8</id>
    <title>I'm amazed by ollama</title>
    <updated>2025-05-02T21:16:50+00:00</updated>
    <author>
      <name>/u/Sandalwoodincencebur</name>
      <uri>https://old.reddit.com/user/Sandalwoodincencebur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here in my city home I have an old computer from 2008 (i7 920 and DX58so 16GB ddr3, RTX 3050) and LM studio, GPT4All and koboldccp didn't work, I managed to get it kind of working but it was painfully slow (kobold). &lt;/p&gt; &lt;p&gt;Then I tried Ollama, and oh boy is this amazing, installed docker to run open webui and everything is dandy. I run couple of models locally, hermes3b:8, deepseek-r1:7b, llama3.2:1b, samantha-mistral:latest, still trying out different stuff, so I was wondering if you have any recommendations for lightweight models specialized in psychology, philosophy, arts and mythology, religions, metaphysics and poetry? &lt;/p&gt; &lt;p&gt;And I was also wondering if there's any FREE API for image generation I can outsource? I tried dalle3 but it doesn't work without subscription, is there API I could use for free? I wouldn't abuse it only an image here and there, as I'm not really a heavy user. Gemini also didn't work, something wrong with base url. So any recommendations what to try next, I really love tinkering with this stuff, and seeing it work so flawlessly on my old pc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sandalwoodincencebur"&gt; /u/Sandalwoodincencebur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdc3z8/im_amazed_by_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kdc3z8/im_amazed_by_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kdc3z8/im_amazed_by_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T21:16:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kd4woq</id>
    <title>zero dolars vibe debugging menace</title>
    <updated>2025-05-02T16:13:34+00:00</updated>
    <author>
      <name>/u/AntelopeEntire9191</name>
      <uri>https://old.reddit.com/user/AntelopeEntire9191</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kd4woq/zero_dolars_vibe_debugging_menace/"&gt; &lt;img alt="zero dolars vibe debugging menace" src="https://preview.redd.it/yg8mgjgt7eye1.gif?width=640&amp;amp;crop=smart&amp;amp;s=ee84534512b5deabf250bb21516f287f281c3f40" title="zero dolars vibe debugging menace" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been tweaking on building &lt;strong&gt;Cloi&lt;/strong&gt; its local debugging agent that runs in your terminal &lt;/p&gt; &lt;p&gt;cursor's o3 got me down astronomical ($0.30 per request??) and claude 3.7 still taking my lunch money ($0.05 a pop) so made something that's zero dollar sign vibes, just pure on-device cooking.&lt;/p&gt; &lt;p&gt;The technical breakdown is pretty straightforward: cloi deadass catches your error tracebacks, spins up a local LLM (zero api key nonsense, no cloud tax) and only with your permission (we respectin boundaries) drops some clean af patches directly to ur files.&lt;/p&gt; &lt;p&gt;Been working on this during my research downtime. If anyone's interested in exploring the implementation or wants to issue feedback, cloi its open source: &lt;a href="https://github.com/cloi-ai/cloi"&gt;https://github.com/cloi-ai/cloi&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AntelopeEntire9191"&gt; /u/AntelopeEntire9191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yg8mgjgt7eye1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kd4woq/zero_dolars_vibe_debugging_menace/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kd4woq/zero_dolars_vibe_debugging_menace/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-02T16:13:34+00:00</published>
  </entry>
</feed>
