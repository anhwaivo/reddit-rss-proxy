<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-29T16:07:10+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1icn70u</id>
    <title>I am trying to download llama3.2-vision on my local host but it is giving me this error basically not able to download, pls help.</title>
    <updated>2025-01-29T05:57:30+00:00</updated>
    <author>
      <name>/u/botkeshav</name>
      <uri>https://old.reddit.com/user/botkeshav</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1icn70u/i_am_trying_to_download_llama32vision_on_my_local/"&gt; &lt;img alt="I am trying to download llama3.2-vision on my local host but it is giving me this error basically not able to download, pls help." src="https://b.thumbs.redditmedia.com/dkO-Csv8WqN-rwVtpxUJHNk9xnO3nr2bGdFQsIxdWsk.jpg" title="I am trying to download llama3.2-vision on my local host but it is giving me this error basically not able to download, pls help." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/botkeshav"&gt; /u/botkeshav &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1icn70u"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icn70u/i_am_trying_to_download_llama32vision_on_my_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icn70u/i_am_trying_to_download_llama32vision_on_my_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T05:57:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1icnc96</id>
    <title>Getting stuck at this point</title>
    <updated>2025-01-29T06:06:51+00:00</updated>
    <author>
      <name>/u/Own-Perception-1574</name>
      <uri>https://old.reddit.com/user/Own-Perception-1574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1icnc96/getting_stuck_at_this_point/"&gt; &lt;img alt="Getting stuck at this point" src="https://b.thumbs.redditmedia.com/OPTeKQoA9soYc_OCd4vsAk8qWOqbHyK00t1l9BPCycg.jpg" title="Getting stuck at this point" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have updated chrome and chromedriver and also my no chrome tab is open and running still Im getting this error message.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/iyx7vyhcjvfe1.jpg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6083a9cff617f56fb43e41d7e96d91d152030f8b"&gt;https://preview.redd.it/iyx7vyhcjvfe1.jpg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6083a9cff617f56fb43e41d7e96d91d152030f8b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Perception-1574"&gt; /u/Own-Perception-1574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icnc96/getting_stuck_at_this_point/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icnc96/getting_stuck_at_this_point/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icnc96/getting_stuck_at_this_point/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T06:06:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibwdvx</id>
    <title>Ollama enjoying the Chinese New Year! Open Source FTW ðŸš€</title>
    <updated>2025-01-28T07:34:16+00:00</updated>
    <author>
      <name>/u/eternviking</name>
      <uri>https://old.reddit.com/user/eternviking</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ibwdvx/ollama_enjoying_the_chinese_new_year_open_source/"&gt; &lt;img alt="Ollama enjoying the Chinese New Year! Open Source FTW ðŸš€" src="https://preview.redd.it/nv34uyjqtofe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9be62292f2e57c4882e28f231b0a82d1126bdabd" title="Ollama enjoying the Chinese New Year! Open Source FTW ðŸš€" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eternviking"&gt; /u/eternviking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nv34uyjqtofe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibwdvx/ollama_enjoying_the_chinese_new_year_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibwdvx/ollama_enjoying_the_chinese_new_year_open_source/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T07:34:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1icqsqp</id>
    <title>Ollama Deletes Automatically When Internet Is Down</title>
    <updated>2025-01-29T10:25:27+00:00</updated>
    <author>
      <name>/u/AlperParlak2009</name>
      <uri>https://old.reddit.com/user/AlperParlak2009</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to download DeepSeek with ollama. I have a decent internet speed but its not stable. When internet goes down for 10 to 15 seconds ollama deletes the already downloaded part. Because of that i cant download it. Is there a solution for this?&lt;/p&gt; &lt;p&gt;Edit: This is bullshit. I'm trying to download it for 7 fucking hours and it's still not downloaded. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlperParlak2009"&gt; /u/AlperParlak2009 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icqsqp/ollama_deletes_automatically_when_internet_is_down/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icqsqp/ollama_deletes_automatically_when_internet_is_down/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icqsqp/ollama_deletes_automatically_when_internet_is_down/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T10:25:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibulvz</id>
    <title>These random accounts have been showing up ever since I started using ollama. Should I be worried?</title>
    <updated>2025-01-28T05:31:49+00:00</updated>
    <author>
      <name>/u/Liquidmesh</name>
      <uri>https://old.reddit.com/user/Liquidmesh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ibulvz/these_random_accounts_have_been_showing_up_ever/"&gt; &lt;img alt="These random accounts have been showing up ever since I started using ollama. Should I be worried?" src="https://preview.redd.it/lqatzv6y7ofe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=21464609696b2ac648efbca375cb46f4d41f5c57" title="These random accounts have been showing up ever since I started using ollama. Should I be worried?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Liquidmesh"&gt; /u/Liquidmesh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lqatzv6y7ofe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibulvz/these_random_accounts_have_been_showing_up_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibulvz/these_random_accounts_have_been_showing_up_ever/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T05:31:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1icod50</id>
    <title>What's the recommended for my setup</title>
    <updated>2025-01-29T07:18:06+00:00</updated>
    <author>
      <name>/u/m9nasr</name>
      <uri>https://old.reddit.com/user/m9nasr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My Setup is: &lt;/p&gt; &lt;p&gt;32 GB 1070 TI 4GB&lt;/p&gt; &lt;p&gt;Which is best model work fine. My main purpose is Coding &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m9nasr"&gt; /u/m9nasr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icod50/whats_the_recommended_for_my_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icod50/whats_the_recommended_for_my_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icod50/whats_the_recommended_for_my_setup/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T07:18:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1icohvi</id>
    <title>Looking for Open-Source LLM for Text Summarization and Elaboration (Task-Specific)</title>
    <updated>2025-01-29T07:28:10+00:00</updated>
    <author>
      <name>/u/zujutsu</name>
      <uri>https://old.reddit.com/user/zujutsu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Iâ€™m looking for an open-source large language model (LLM) that specializes in text summarization and elaboration. I want a model that I can fine-tune on my own data and customize for my specific needs. The key is that I donâ€™t need it to solve mathematical problems or coding questions â€“ I only need it for summarization and elaboration tasks.&lt;/p&gt; &lt;p&gt;Ideally, Iâ€™m looking for something lightweight and fast, so a smaller model size would be great. Does anyone know of any open-source models that would fit this purpose? Iâ€™d really appreciate any suggestions or guidance on how to approach this!&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zujutsu"&gt; /u/zujutsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icohvi/looking_for_opensource_llm_for_text_summarization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icohvi/looking_for_opensource_llm_for_text_summarization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icohvi/looking_for_opensource_llm_for_text_summarization/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T07:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1icq076</id>
    <title>Help with model selection</title>
    <updated>2025-01-29T09:25:19+00:00</updated>
    <author>
      <name>/u/Ginger_Leopard</name>
      <uri>https://old.reddit.com/user/Ginger_Leopard</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to select a module that has a response with little or no bloat. I have test a few (llama3.1, deepseek-r1, granite3.1-dense). I have given several instructions to reduce the bloat but it still struggles. Ideally I would like to keep it under 14B for simplicity. &lt;/p&gt; &lt;p&gt;For more context I am trying to extract information on differnt plant species from many abstracts on that plant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ginger_Leopard"&gt; /u/Ginger_Leopard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icq076/help_with_model_selection/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icq076/help_with_model_selection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icq076/help_with_model_selection/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T09:25:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1icn3pu</id>
    <title>How to use ollama with Models from huggingface?</title>
    <updated>2025-01-29T05:51:25+00:00</updated>
    <author>
      <name>/u/countjj</name>
      <uri>https://old.reddit.com/user/countjj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a folder full of LLM models from huggingface I want to use with Ollama, but when I tell it to run from the path &lt;code&gt;ollama run path/to/qwencoder2.5-7b/&lt;/code&gt; it tells me I have an invalid model path. these models work in Ooba Booga, what am I doing wrong?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/countjj"&gt; /u/countjj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icn3pu/how_to_use_ollama_with_models_from_huggingface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icn3pu/how_to_use_ollama_with_models_from_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icn3pu/how_to_use_ollama_with_models_from_huggingface/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T05:51:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1icich0</id>
    <title>I need a good and light grammar check model.</title>
    <updated>2025-01-29T01:32:40+00:00</updated>
    <author>
      <name>/u/tonitz4493</name>
      <uri>https://old.reddit.com/user/tonitz4493</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title. Im running ollama in a minipc with 16gb ram and intel iGPU (no external GPU)&lt;/p&gt; &lt;p&gt;I tried deepseek but booooi it takes forerver to answer a simple grammar check.&lt;/p&gt; &lt;p&gt;With the specs above, what's the best grammar check model that I can use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tonitz4493"&gt; /u/tonitz4493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icich0/i_need_a_good_and_light_grammar_check_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icich0/i_need_a_good_and_light_grammar_check_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icich0/i_need_a_good_and_light_grammar_check_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T01:32:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic7lt5</id>
    <title>Which Ollama local UI for Windows is the lightest and fastest?</title>
    <updated>2025-01-28T17:55:06+00:00</updated>
    <author>
      <name>/u/mazapo101</name>
      <uri>https://old.reddit.com/user/mazapo101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Through command line I can run ollama with deepseek-r1:32b and it works, it types the response a bit slow, but it works fine.&lt;/p&gt; &lt;p&gt;I tried installing Open WebUI through Docker, but it takes almost 3 minutes to start typing the thinking. I also tried AnythingLLM, but the same thing happens.&lt;/p&gt; &lt;p&gt;I just want an UI to have a more comfortable chat and to be able to keep my chat history. What options are there?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mazapo101"&gt; /u/mazapo101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic7lt5/which_ollama_local_ui_for_windows_is_the_lightest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic7lt5/which_ollama_local_ui_for_windows_is_the_lightest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic7lt5/which_ollama_local_ui_for_windows_is_the_lightest/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T17:55:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1icr8ln</id>
    <title>If you are having trouble with downloads when pulling models in ollama, try this</title>
    <updated>2025-01-29T10:57:47+00:00</updated>
    <author>
      <name>/u/zzozozoz</name>
      <uri>https://old.reddit.com/user/zzozozoz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When trying to run deepseek-r1 offline I was having some trouble with the download reaching a midpoint then looping. It seemed others were as well. &lt;/p&gt; &lt;p&gt;If you have another device, try pulling the model with ollama on that device. Try a few different devices if the first one doesn't work. &lt;/p&gt; &lt;p&gt;Then copy the blob folder contents to the same folder on the device that was giving you trouble. &lt;/p&gt; &lt;p&gt;Pull the model again on the problem device and it should finalize successfully. &lt;/p&gt; &lt;p&gt;I also downloaded the versions I wanted and stored them by isolating the independent model files. &lt;/p&gt; &lt;p&gt;Hope it helps! &lt;/p&gt; &lt;p&gt;There is one other option too where you use the file name to get the download link through cloudflare directly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zzozozoz"&gt; /u/zzozozoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icr8ln/if_you_are_having_trouble_with_downloads_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icr8ln/if_you_are_having_trouble_with_downloads_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icr8ln/if_you_are_having_trouble_with_downloads_when/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T10:57:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1icjthh</id>
    <title>8x-AMD-Instinct-Mi60-Server-DeepSeek-R1-Distill-Llama-70B-Q8-vLLM</title>
    <updated>2025-01-29T02:46:41+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/e44y1oh0jufe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icjthh/8xamdinstinctmi60serverdeepseekr1distillllama70bq8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icjthh/8xamdinstinctmi60serverdeepseekr1distillllama70bq8/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T02:46:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1icsasp</id>
    <title>Need help to instal local deepseek with ollama</title>
    <updated>2025-01-29T12:07:32+00:00</updated>
    <author>
      <name>/u/Ok-Half-3728</name>
      <uri>https://old.reddit.com/user/Ok-Half-3728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1icsasp/need_help_to_instal_local_deepseek_with_ollama/"&gt; &lt;img alt="Need help to instal local deepseek with ollama" src="https://a.thumbs.redditmedia.com/cWM0rnccu6lOMOHjkfmYIyGgx5i4yUSgft-OnmO5q20.jpg" title="Need help to instal local deepseek with ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;br /&gt; Can you help me?&lt;br /&gt; I try to run deepseek on macbook pro 48 gig memory.&lt;br /&gt; I am trying chatbox and directly with terminal but does not work properly. Not any answers from chatbox and quite random from terminal after waiting 5 minutes. &lt;/p&gt; &lt;p&gt;How should I install and use it that it works well?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6z8tlzgmbxfe1.jpg?width=648&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a6423b553422c941a9d6b9ad41a18dfa99b34217"&gt;https://preview.redd.it/6z8tlzgmbxfe1.jpg?width=648&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a6423b553422c941a9d6b9ad41a18dfa99b34217&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Half-3728"&gt; /u/Ok-Half-3728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icsasp/need_help_to_instal_local_deepseek_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icsasp/need_help_to_instal_local_deepseek_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icsasp/need_help_to_instal_local_deepseek_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T12:07:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1icseux</id>
    <title>Proxmox + Ollama</title>
    <updated>2025-01-29T12:14:22+00:00</updated>
    <author>
      <name>/u/samuelpaluba</name>
      <uri>https://old.reddit.com/user/samuelpaluba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Post Title:&lt;/strong&gt; Mini LXC Proxmox Setup with Tesla P4 and Lenovo ThinkCentre M920q - Is It Possible?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Post Content:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Iâ€™m planning to build a mini LXC Proxmox setup using a Tesla P4 GPU and a Lenovo ThinkCentre M920q. Iâ€™m curious if this configuration would be sufficient to run a DeepSeek R1 model.&lt;/p&gt; &lt;p&gt;Here are some details:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: Tesla P4 (8 GB VRAM)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: Lenovo ThinkCentre M920q (with a suitable processor)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: I want to experiment with AI models, specifically DeepSeek R1 (and few light containers for mail and webhosting)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Do you think this combination would be enough for efficient model performance? What are your experiences with similar setups?&lt;/p&gt; &lt;p&gt;Additionally, Iâ€™d like to know if there are any other low-profile GPUs that would fit into the M920q and offer better performance than the Tesla P4. &lt;/p&gt; &lt;p&gt;Thanks for your insights and advice!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samuelpaluba"&gt; /u/samuelpaluba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icseux/proxmox_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icseux/proxmox_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icseux/proxmox_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T12:14:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1icm0er</id>
    <title>I want to generate images locally with my 4070 Ti</title>
    <updated>2025-01-29T04:45:27+00:00</updated>
    <author>
      <name>/u/AxelBlaze20850</name>
      <uri>https://old.reddit.com/user/AxelBlaze20850</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This might not be right subreddit but still asking for guidance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AxelBlaze20850"&gt; /u/AxelBlaze20850 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icm0er/i_want_to_generate_images_locally_with_my_4070_ti/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icm0er/i_want_to_generate_images_locally_with_my_4070_ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icm0er/i_want_to_generate_images_locally_with_my_4070_ti/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T04:45:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1icsli1</id>
    <title>ollama best model for coding etc (help)</title>
    <updated>2025-01-29T12:25:23+00:00</updated>
    <author>
      <name>/u/Aryangupt556</name>
      <uri>https://old.reddit.com/user/Aryangupt556</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1icsli1/ollama_best_model_for_coding_etc_help/"&gt; &lt;img alt="ollama best model for coding etc (help)" src="https://b.thumbs.redditmedia.com/Leo1Bavg0LVE7_6ojga8FsZKCa5kEALvOvQRu6Kb0ZI.jpg" title="ollama best model for coding etc (help)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hi, Iâ€™m new to Ollama and AI in general. I recently created my own model and have Llama 3.2 installed as the default. I just wanted to know which is the best model I can download that is safe and runs smoothly on my Mac. I was considering Dolphin-Mixtral, but at 26GB, it seems quite large. Any recommendations?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ti5rkonjexfe1.png?width=878&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d092469ab131e0532aa65574023667197b6f410b"&gt;https://preview.redd.it/ti5rkonjexfe1.png?width=878&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d092469ab131e0532aa65574023667197b6f410b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aryangupt556"&gt; /u/Aryangupt556 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icsli1/ollama_best_model_for_coding_etc_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icsli1/ollama_best_model_for_coding_etc_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icsli1/ollama_best_model_for_coding_etc_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T12:25:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ictf5d</id>
    <title>best model using 4080 super for general tasks?</title>
    <updated>2025-01-29T13:11:44+00:00</updated>
    <author>
      <name>/u/ButterscotchOk1476</name>
      <uri>https://old.reddit.com/user/ButterscotchOk1476</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im going to get the 5080 when it releases which is just a few percent faster than the 4080 super, what models can I run locally? I mainly use ai for creative writing and programming. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ButterscotchOk1476"&gt; /u/ButterscotchOk1476 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ictf5d/best_model_using_4080_super_for_general_tasks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ictf5d/best_model_using_4080_super_for_general_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ictf5d/best_model_using_4080_super_for_general_tasks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T13:11:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1icq63a</id>
    <title>Which Deepseek model should I use in ollama if I want to get fast response?</title>
    <updated>2025-01-29T09:38:00+00:00</updated>
    <author>
      <name>/u/Tasty-Mint-4945</name>
      <uri>https://old.reddit.com/user/Tasty-Mint-4945</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My GPU is RTX 3050 and I'm using Deepseek model 8b, and the response is good but kinda slow, which one should I use.? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tasty-Mint-4945"&gt; /u/Tasty-Mint-4945 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icq63a/which_deepseek_model_should_i_use_in_ollama_if_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icq63a/which_deepseek_model_should_i_use_in_ollama_if_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icq63a/which_deepseek_model_should_i_use_in_ollama_if_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T09:38:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1icu15w</id>
    <title>How to set system prompt in ollama</title>
    <updated>2025-01-29T13:43:12+00:00</updated>
    <author>
      <name>/u/mans-987</name>
      <uri>https://old.reddit.com/user/mans-987</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can the system prompt be set on Ollama to work with all LLMs? I know that I can create a new model with the specific system prompt, but I want to set it via client and not create a new model for every different task that I am doing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mans-987"&gt; /u/mans-987 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icu15w/how_to_set_system_prompt_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icu15w/how_to_set_system_prompt_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icu15w/how_to_set_system_prompt_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T13:43:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1icexf6</id>
    <title>Would a 4090 mixed with a 3090 be enough to speed up R1 70b? (48 gb VRAM total)</title>
    <updated>2025-01-28T22:55:51+00:00</updated>
    <author>
      <name>/u/magicomiralles</name>
      <uri>https://old.reddit.com/user/magicomiralles</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I currently have a 4090 build. I am able to run the 70b version but it is extremelly slow. So I want to add a new GPU. I recently found out that it is possible to mix GPUs, so I can buy a 3090, which is much more affordable than a 4090.&lt;/p&gt; &lt;p&gt;For this I would have to get a new PSU, case, and an RTX 3090. Would this be the best approach?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/magicomiralles"&gt; /u/magicomiralles &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icexf6/would_a_4090_mixed_with_a_3090_be_enough_to_speed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icexf6/would_a_4090_mixed_with_a_3090_be_enough_to_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icexf6/would_a_4090_mixed_with_a_3090_be_enough_to_speed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T22:55:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1icncir</id>
    <title>How to access deepseek-ai/Janus-Pro-1B on Ollama?</title>
    <updated>2025-01-29T06:07:19+00:00</updated>
    <author>
      <name>/u/Current_Mountain_100</name>
      <uri>https://old.reddit.com/user/Current_Mountain_100</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, please help me on how to access deepseek-ai/Janus-Pro-1B on ollama. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Current_Mountain_100"&gt; /u/Current_Mountain_100 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icncir/how_to_access_deepseekaijanuspro1b_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icncir/how_to_access_deepseekaijanuspro1b_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icncir/how_to_access_deepseekaijanuspro1b_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T06:07:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1icv7wv</id>
    <title>Hardware requirements for running the full size deepseek R1 with ollama?</title>
    <updated>2025-01-29T14:39:12+00:00</updated>
    <author>
      <name>/u/BC547</name>
      <uri>https://old.reddit.com/user/BC547</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My machine runs the Deepseek R1-14B model fine, but the 34B and 70B are too slow for practical use. I am looking at building a machine capable of running the full 671B model fast enough that it's not too annoying as a coding assistant. What kind of hardware do i need?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BC547"&gt; /u/BC547 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icv7wv/hardware_requirements_for_running_the_full_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icv7wv/hardware_requirements_for_running_the_full_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icv7wv/hardware_requirements_for_running_the_full_size/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T14:39:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1icivtq</id>
    <title>Why do local models still censor stuff?</title>
    <updated>2025-01-29T01:59:16+00:00</updated>
    <author>
      <name>/u/Sure-Year2141</name>
      <uri>https://old.reddit.com/user/Sure-Year2141</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I asked it a very grotesque question and it straight up refused to answer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sure-Year2141"&gt; /u/Sure-Year2141 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icivtq/why_do_local_models_still_censor_stuff/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icivtq/why_do_local_models_still_censor_stuff/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icivtq/why_do_local_models_still_censor_stuff/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T01:59:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1icrcdx</id>
    <title>Why don't we use NVMe instead of VRAM</title>
    <updated>2025-01-29T11:04:25+00:00</updated>
    <author>
      <name>/u/infinity6570</name>
      <uri>https://old.reddit.com/user/infinity6570</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why don't we use NMVe storage drives on PCIe lanes to directly serve the GPU instead of loading huge models to VRAM?? Yes, it will be slower and will have more latency, but being able to run something vs nothing is better right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/infinity6570"&gt; /u/infinity6570 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icrcdx/why_dont_we_use_nvme_instead_of_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icrcdx/why_dont_we_use_nvme_instead_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icrcdx/why_dont_we_use_nvme_instead_of_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T11:04:25+00:00</published>
  </entry>
</feed>
