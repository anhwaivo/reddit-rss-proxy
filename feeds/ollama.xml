<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-01T09:53:56+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mcypg1</id>
    <title>Clia - Bash tool to get Linux help without switching context</title>
    <updated>2025-07-30T05:55:45+00:00</updated>
    <author>
      <name>/u/Comfortable-Okra753</name>
      <uri>https://old.reddit.com/user/Comfortable-Okra753</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mcypg1/clia_bash_tool_to_get_linux_help_without/"&gt; &lt;img alt="Clia - Bash tool to get Linux help without switching context" src="https://external-preview.redd.it/bjQ2emFkcTdheWZmMeLG9aICInxYkj3deyjqciGNhLwvAlriDKsYg5qLih4_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6680eec3c9d87a5560d5706fd3e55c7be2f4df67" title="Clia - Bash tool to get Linux help without switching context" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by &lt;a href="/u/LoganPederson"&gt;u/LoganPederson&lt;/a&gt;'s zsh plugin but not wanting to install zsh, I wrote a similar script but in Bash, so it can just be installed and run on any default Linux installation (in my case Ubuntu).&lt;/p&gt; &lt;p&gt;Meet Clia, a minimalist Bash tool that lets you ask Linux-related command-line questions directly from your terminal and get expert, copy-paste-ready answers powered by your local Ollama server.&lt;/p&gt; &lt;p&gt;I made it to avoid context-switching, having to move away from the terminal to search for a command help query. Feel free to propose suggestions and improvements.&lt;/p&gt; &lt;p&gt;Code is here: &lt;a href="https://github.com/Mircea-S/clia"&gt;https://github.com/Mircea-S/clia&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Okra753"&gt; /u/Comfortable-Okra753 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5ycr98q7ayff1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mcypg1/clia_bash_tool_to_get_linux_help_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mcypg1/clia_bash_tool_to_get_linux_help_without/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-30T05:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1md0xg4</id>
    <title>Using Ollama for Coding Agents in marimo notebooks</title>
    <updated>2025-07-30T08:17:55+00:00</updated>
    <author>
      <name>/u/cantdutchthis</name>
      <uri>https://old.reddit.com/user/cantdutchthis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1md0xg4/using_ollama_for_coding_agents_in_marimo_notebooks/"&gt; &lt;img alt="Using Ollama for Coding Agents in marimo notebooks" src="https://external-preview.redd.it/EphkokuYv3JI_Qz3CBCCvQlyGdfvWxPixjasg8Txwu0.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5811336c66ad948395c5f7ec9cb74e7e09fac3a5" title="Using Ollama for Coding Agents in marimo notebooks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Figured folks might be interested in using Ollama for their Python notebook work. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cantdutchthis"&gt; /u/cantdutchthis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=NIBprn5cEZA&amp;amp;t=9s&amp;amp;ab_channel=marimo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1md0xg4/using_ollama_for_coding_agents_in_marimo_notebooks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1md0xg4/using_ollama_for_coding_agents_in_marimo_notebooks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-30T08:17:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mddx6n</id>
    <title>Pwn2Own Contestants hold on to Ollama exploits due to its rapid update cycle</title>
    <updated>2025-07-30T18:09:59+00:00</updated>
    <author>
      <name>/u/audibleBLiNK</name>
      <uri>https://old.reddit.com/user/audibleBLiNK</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mddx6n/pwn2own_contestants_hold_on_to_ollama_exploits/"&gt; &lt;img alt="Pwn2Own Contestants hold on to Ollama exploits due to its rapid update cycle" src="https://external-preview.redd.it/7AXrzv4L6ZD8vnqWbxd8siZWy-FKDEVUwzLu-jnXkE8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=745b6dda7f47983453b1d752c6b74ebae226babe" title="Pwn2Own Contestants hold on to Ollama exploits due to its rapid update cycle" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over 10k open servers on the internet&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/audibleBLiNK"&gt; /u/audibleBLiNK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.trendmicro.com/vinfo/us/security/news/threat-landscape/trend-micro-state-of-ai-security-report-1h-2025#:~:text=However%2C%20when%20asked%2C%20we%20found,a%20high%2Dstakes%20competition%20like%20Pwn2Own"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mddx6n/pwn2own_contestants_hold_on_to_ollama_exploits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mddx6n/pwn2own_contestants_hold_on_to_ollama_exploits/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-30T18:09:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdecur</id>
    <title>Should I buy a QuietBox or just build my own station?</title>
    <updated>2025-07-30T18:26:28+00:00</updated>
    <author>
      <name>/u/asumaria95</name>
      <uri>https://old.reddit.com/user/asumaria95</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone. I am trying to play around with more opensource models because I am really worried about privacy. I recently thought about having my own server to do inference, and now considering to buy a QuietBox. But at the same time, as I look through this sub, it seems like building my own station seems to be better too. Was wondering what would be better. Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asumaria95"&gt; /u/asumaria95 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mdecur/should_i_buy_a_quietbox_or_just_build_my_own/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mdecur/should_i_buy_a_quietbox_or_just_build_my_own/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mdecur/should_i_buy_a_quietbox_or_just_build_my_own/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-30T18:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1md0o5i</id>
    <title>Chat Box: An Open-Source Browser Extension for AI Chat</title>
    <updated>2025-07-30T08:00:47+00:00</updated>
    <author>
      <name>/u/MinhxThanh</name>
      <uri>https://old.reddit.com/user/MinhxThanh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I wanted to share this open-source project I've come across called Chat Box. It's a browser extension that brings AI chat, advanced web search, document interaction, and other handy tools right into a sidebar in your browser. It's designed to make your online workflow smoother without needing to switch tabs or apps constantly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What It Does&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;At its core, Chat Box gives you a persistent AI-powered chat interface that you can access with a quick shortcut (Ctrl+E or Cmd+E). It supports a bunch of AI providers like OpenAI, DeepSeek, Claude, Groq, and even local LLMs via Ollama. You just configure your API keys in the settings, and you're good to go.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-AI Support:&lt;/strong&gt; Switch between different providers and models easily.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sidebar Chat:&lt;/strong&gt; Chat with AI while browsing, and it stays there across tabs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Conversation Management:&lt;/strong&gt; Start new chats, view history, and delete old ones.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Document Interaction:&lt;/strong&gt; Upload docs like DOCX, TXT, MD, etc., and chat about their content. It handles large files with semantic chunking.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Web Search and Scraping&lt;/strong&gt;: Integrates with tools like Firecrawl or Jina for better searches (or defaults to DuckDuckGo). You can scrape URLs, summarize content, and use it in chats.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;YouTube Integration:&lt;/strong&gt; Detects videos and lets you summarize or ask questions about them.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom Prompts:&lt;/strong&gt; Save and reuse your own prompts for repetitive tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text Selection:&lt;/strong&gt; Highlight text on any page, and it auto-uses it as context in the chat.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Secure Storage&lt;/strong&gt;: Everything's stored locally in your browser—no cloud worries.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dark Mode UI:&lt;/strong&gt; Built with modern tools like React, Tailwind, and Shadcn for a clean look.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's all open-source under GPL-3.0, so you can tweak it if you want.&lt;/p&gt; &lt;p&gt;If you run into any errors, issues, or want to suggest a new feature, please create a new Issue on GitHub and describe it in detail – I'll respond ASAP!&lt;/p&gt; &lt;p&gt;Chrome Web Store: &lt;a href="https://chromewebstore.google.com/detail/chat-box-chat-with-all-ai/hhaaoibkigonnoedcocnkehipecgdodm"&gt;https://chromewebstore.google.com/detail/chat-box-chat-with-all-ai/hhaaoibkigonnoedcocnkehipecgdodm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MinhxThanh/Chat-Box"&gt;https://github.com/MinhxThanh/Chat-Box&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MinhxThanh"&gt; /u/MinhxThanh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1md0o5i/chat_box_an_opensource_browser_extension_for_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1md0o5i/chat_box_an_opensource_browser_extension_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1md0o5i/chat_box_an_opensource_browser_extension_for_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-30T08:00:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdnzt4</id>
    <title>How do I run Ollama (the whole thing, not just the models) from a location that does not require to access to my appdata/local/programs on Windows?</title>
    <updated>2025-07-31T01:02:11+00:00</updated>
    <author>
      <name>/u/fttklr</name>
      <uri>https://old.reddit.com/user/fttklr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I installed Ollama which works fine, but it is installing data on the computer appdata folder in my user folder (windows 11). I would like to have a portable version on an external NVME, and while I can set where the models are, I cannot run Llama from the external drive if I uninstall LLama from my C drive.&lt;/p&gt; &lt;p&gt;Is there a way to change this, so I can just run it from the drive and it won't bother to look into Appdata folder anymore?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fttklr"&gt; /u/fttklr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mdnzt4/how_do_i_run_ollama_the_whole_thing_not_just_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mdnzt4/how_do_i_run_ollama_the_whole_thing_not_just_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mdnzt4/how_do_i_run_ollama_the_whole_thing_not_just_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-31T01:02:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdwrv3</id>
    <title>need help</title>
    <updated>2025-07-31T09:15:21+00:00</updated>
    <author>
      <name>/u/Live-Budget-5493</name>
      <uri>https://old.reddit.com/user/Live-Budget-5493</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mdwrv3/need_help/"&gt; &lt;img alt="need help" src="https://b.thumbs.redditmedia.com/o4uauDOp3VXPp4-wDwdHYLCfBHt14ei_C6qfu4sP59E.jpg" title="need help" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;why is it not working&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qza31p2qf6gf1.png?width=1469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09a7776e131d2acf3cfd7e5733f32d8d9680ceb0"&gt;https://preview.redd.it/qza31p2qf6gf1.png?width=1469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09a7776e131d2acf3cfd7e5733f32d8d9680ceb0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Live-Budget-5493"&gt; /u/Live-Budget-5493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mdwrv3/need_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mdwrv3/need_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mdwrv3/need_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-31T09:15:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1md2lg3</id>
    <title>qwen3:30b 2507 is out</title>
    <updated>2025-07-30T10:04:38+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 30b and 235b 2507 on ollama&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/qwen3:30b-a3b-instruct-2507-q4_K_M"&gt;https://ollama.com/library/qwen3:30b-a3b-instruct-2507-q4_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/qwen3:235b-a22b-instruct-2507-q4_K_M"&gt;https://ollama.com/library/qwen3:235b-a22b-instruct-2507-q4_K_M&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1md2lg3/qwen330b_2507_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1md2lg3/qwen330b_2507_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1md2lg3/qwen330b_2507_is_out/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-30T10:04:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mduay2</id>
    <title>Ollama on Intel Arc A770 without Resizable BAR Getting SIGSEGV on model load</title>
    <updated>2025-07-31T06:35:05+00:00</updated>
    <author>
      <name>/u/sleepinfinit</name>
      <uri>https://old.reddit.com/user/sleepinfinit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’ve been trying to run Ollama on my Intel Arc A770 GPU, which is installed in my Proxmox server. I set up an Ubuntu 24.04 VM and followed the official Intel driver installation guide: &lt;a href="https://dgpu-docs.intel.com/driver/client/overview.html"&gt;https://dgpu-docs.intel.com/driver/client/overview.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Everything installed fine, but when I ran clinfo, I got this warning:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;WARNING: Small BAR detected for device 0000:01:00.0&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I’m assuming this is because my system is based on an older Intel Gen 3 (Ivy Bridge) platform, and my motherboard doesn’t support Resizable BAR.&lt;/p&gt; &lt;p&gt;Despite the warning, I went ahead and installed the Ollama Docker container from this repo: &lt;a href="https://github.com/eleiton/ollama-intel-arc"&gt;https://github.com/eleiton/ollama-intel-arc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;First, I tested the Whisper container — it worked and used the GPU (confirmed with intel_gpu_top), but it was very slow.&lt;/p&gt; &lt;p&gt;Then I tried the Ollama container — the GPU is detected, and the model starts to load into VRAM, but I consistently get a SIGSEGV (segmentation fault) during model load.&lt;/p&gt; &lt;p&gt;Here's part of the log:&lt;/p&gt; &lt;p&gt;load_backend: loaded SYCL backend from /usr/local/lib/python3.11/dist-packages/bigdl/cpp/libs/ollama/libggml-sycl.so&lt;br /&gt; llama_model_load_from_file_impl: using device SYCL0 (Intel(R) Arc(TM) A770 Graphics)&lt;br /&gt; ...&lt;br /&gt; SIGSEGV&lt;/p&gt; &lt;p&gt;I suspect the issue might be caused by the lack of Resizable BAR support. I'm considering trying this tool to enable it: &lt;a href="https://github.com/xCuri0/ReBarUEFI"&gt;https://github.com/xCuri0/ReBarUEFI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone else here run into similar issues?&lt;/p&gt; &lt;p&gt;Are you using Ollama with Arc GPUs successfully?&lt;/p&gt; &lt;p&gt;Did Resizable BAR make a difference for you?&lt;/p&gt; &lt;p&gt;Would love to hear from others in the same boat. Thanks!&lt;/p&gt; &lt;p&gt;EDIT : i tried &lt;a href="https://github.com/whyvl/ollama-vulkan"&gt;ollama-vulkan&lt;/a&gt; from this &lt;a href="https://kovasky.me/blogs/ollama_vulkan_intel/"&gt;guide&lt;/a&gt; and it worked even without resizable bar, i was getting about 25 token/s in llama3:8b&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sleepinfinit"&gt; /u/sleepinfinit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mduay2/ollama_on_intel_arc_a770_without_resizable_bar/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mduay2/ollama_on_intel_arc_a770_without_resizable_bar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mduay2/ollama_on_intel_arc_a770_without_resizable_bar/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-31T06:35:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdiaps</id>
    <title>Project Update : OllamaCode | Refactored the whole thing and yeah just sharing it here cause I had some comments asking for link to it. Well it's back! :)</title>
    <updated>2025-07-30T20:57:55+00:00</updated>
    <author>
      <name>/u/Loud-Consideration-2</name>
      <uri>https://old.reddit.com/user/Loud-Consideration-2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mdiaps/project_update_ollamacode_refactored_the_whole/"&gt; &lt;img alt="Project Update : OllamaCode | Refactored the whole thing and yeah just sharing it here cause I had some comments asking for link to it. Well it's back! :)" src="https://external-preview.redd.it/jnGzQof1ojdvwEI2nQCT8OBXxcT_7kXi1PpQb2uXzf8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7892e20646a4fe7c052a9f34127cbb0970bdefa2" title="Project Update : OllamaCode | Refactored the whole thing and yeah just sharing it here cause I had some comments asking for link to it. Well it's back! :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Still needs a lot of work so really gonna have to lean on you lot to make this a reality! :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loud-Consideration-2"&gt; /u/Loud-Consideration-2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/tooyipjee/ollamacode"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mdiaps/project_update_ollamacode_refactored_the_whole/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mdiaps/project_update_ollamacode_refactored_the_whole/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-30T20:57:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1me13go</id>
    <title>Help for the beginner in AI creation.</title>
    <updated>2025-07-31T13:07:36+00:00</updated>
    <author>
      <name>/u/Nikion-TV</name>
      <uri>https://old.reddit.com/user/Nikion-TV</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm just a 21 year old medical college student now. I've tons of ideas that I want to implement. But I have to first learn a lot of stuff to actually begin my journey, and to do that I need your help. I want to create AI that can redraw SFW and NSFW images into specific style. I have up to 3000 jpg pictures in my desired style. And since I do not have proper hardware, I made runpod account. The problem is I am still green in programming, and I need your help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nikion-TV"&gt; /u/Nikion-TV &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1me13go/help_for_the_beginner_in_ai_creation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1me13go/help_for_the_beginner_in_ai_creation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1me13go/help_for_the_beginner_in_ai_creation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-31T13:07:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1me1jx2</id>
    <title>num_thread doesn't work?</title>
    <updated>2025-07-31T13:27:19+00:00</updated>
    <author>
      <name>/u/Juggernaut_Tight</name>
      <uri>https://old.reddit.com/user/Juggernaut_Tight</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! &lt;/p&gt; &lt;p&gt;I used &lt;a href="https://community-scripts.github.io/ProxmoxVE/scripts?id=openwebui"&gt;this&lt;/a&gt; script on my proxmox server to create an lxc (container, sort of), whit as hardware got assigned 8 cores (cpu is 8c/16t, xenon d-1540@2GHz), 16G ram (Ihave 128GB installed) and full access to a Tesla P4, that runs both Open WebUI and Ollama.&lt;/p&gt; &lt;p&gt;saying &amp;quot;hi&amp;quot; to deepseek-r1:8b results in &lt;/p&gt; &lt;ul&gt; &lt;li&gt;response_token/s 17.67&lt;/li&gt; &lt;li&gt;prompt_token/s 317.28&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;now my question regards cpu utilization. while running, the gpu shows 6.5GB of VRAM used and 61W over 75W budget, so I guess it's working at nearly 100%. On the CPU I see just one core at 100% and 950MB of RAM used. &lt;/p&gt; &lt;p&gt;I tryed setting num_thread = 8 for the model, reloading it and even rebooting the machine, nothing changed&lt;/p&gt; &lt;p&gt;why doesn't the model load on cpu memory, as it does if I use LM studio for example? and why does it only use a single core?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juggernaut_Tight"&gt; /u/Juggernaut_Tight &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1me1jx2/num_thread_doesnt_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1me1jx2/num_thread_doesnt_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1me1jx2/num_thread_doesnt_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-31T13:27:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1me5jf5</id>
    <title>Welk model van Ollama</title>
    <updated>2025-07-31T16:02:50+00:00</updated>
    <author>
      <name>/u/Original-Chapter-112</name>
      <uri>https://old.reddit.com/user/Original-Chapter-112</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ik ben op zoek naar een model van Ollama dat bij mijn snapshot’s van de camera goed kan vertellen of er een bezorger voor de deur staat. Ik draai op een NUC8i5 met 32gB RAM. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Original-Chapter-112"&gt; /u/Original-Chapter-112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1me5jf5/welk_model_van_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1me5jf5/welk_model_van_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1me5jf5/welk_model_van_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-31T16:02:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdxeon</id>
    <title>Is it possible to run MLX model through Ollama?</title>
    <updated>2025-07-31T09:57:00+00:00</updated>
    <author>
      <name>/u/Bokoblob</name>
      <uri>https://old.reddit.com/user/Bokoblob</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perhaps a noob question, as I'm not very familiar with all that LLM Stuff. I’ve got an M1 Pro Mac with 32GB RAM, and I’m loving how smoothly the Qwen3-30B-A3B-Instruct-2507 (MLX version) runs in LM Studio and Open Web UI.&lt;/p&gt; &lt;p&gt;Now I'd like to run it through Ollama instead (if I understand correctly, LM Studio isn't open source and I'd like to stay with FOSS software) but it seems like Ollama only works with GGUF, despite some post I found saying that Ollama now supports MLX. &lt;/p&gt; &lt;p&gt;Is there any way to import the MLX model to Ollama? &lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bokoblob"&gt; /u/Bokoblob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mdxeon/is_it_possible_to_run_mlx_model_through_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mdxeon/is_it_possible_to_run_mlx_model_through_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mdxeon/is_it_possible_to_run_mlx_model_through_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-31T09:57:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1medk3x</id>
    <title>Thanks for the Qwen 3 coder!!</title>
    <updated>2025-07-31T21:08:08+00:00</updated>
    <author>
      <name>/u/beedunc</name>
      <uri>https://old.reddit.com/user/beedunc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Will you be posting the 408B variants as well? I know the quants are still huge, but I'm ready for the 220GB models. Fingers crossed. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beedunc"&gt; /u/beedunc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1medk3x/thanks_for_the_qwen_3_coder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1medk3x/thanks_for_the_qwen_3_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1medk3x/thanks_for_the_qwen_3_coder/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-31T21:08:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mechyo</id>
    <title>deepseek-r1:70b just got a bit sassy with me</title>
    <updated>2025-07-31T20:27:14+00:00</updated>
    <author>
      <name>/u/Solid-Coast3358</name>
      <uri>https://old.reddit.com/user/Solid-Coast3358</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I asked it to create a swagger definition based off of some api routes. It gave me the definition for the first endpoint, then told me to do the rest and refused network connections for subsequent requests, lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid-Coast3358"&gt; /u/Solid-Coast3358 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mechyo/deepseekr170b_just_got_a_bit_sassy_with_me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mechyo/deepseekr170b_just_got_a_bit_sassy_with_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mechyo/deepseekr170b_just_got_a_bit_sassy_with_me/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-31T20:27:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1me2bik</id>
    <title>Introducing new RAGLight Library feature : chat CLI powered by LangChain! 💬</title>
    <updated>2025-07-31T13:58:14+00:00</updated>
    <author>
      <name>/u/Labess40</name>
      <uri>https://old.reddit.com/user/Labess40</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1me2bik/introducing_new_raglight_library_feature_chat_cli/"&gt; &lt;img alt="Introducing new RAGLight Library feature : chat CLI powered by LangChain! 💬" src="https://external-preview.redd.it/L7sqQQlSqrhjdBT1rj6pembBI4r_Xita328G-VTptM0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53dc5b0bd5cbb773a5e786a5980d8cc9a35d0291" title="Introducing new RAGLight Library feature : chat CLI powered by LangChain! 💬" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm excited to announce a major &lt;strong&gt;new feature&lt;/strong&gt; in &lt;strong&gt;RAGLight v2.0.0&lt;/strong&gt; : the new &lt;code&gt;raglight chat&lt;/code&gt; &lt;strong&gt;CLI&lt;/strong&gt;, built with &lt;strong&gt;Typer&lt;/strong&gt; and backed by &lt;strong&gt;LangChain&lt;/strong&gt;. Now, you can launch an interactive Retrieval-Augmented Generation session directly from your terminal, no Python scripting required !&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5o5hzv2fu7gf1.jpg?width=2880&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fa2bc876a0c0ebbe8c8e0a4edcd126a7f30bb173"&gt;https://preview.redd.it/5o5hzv2fu7gf1.jpg?width=2880&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fa2bc876a0c0ebbe8c8e0a4edcd126a7f30bb173&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most RAG tools assume you're ready to write Python. With this CLI:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Users can launch a RAG chat in &lt;strong&gt;seconds&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;No code needed, just install RAGLight library and type &lt;code&gt;raglight chat&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;It’s perfect for demos, quick prototyping, or non-developers.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Interactive setup wizard&lt;/strong&gt;: guides you through choosing your document directory, vector store location, embeddings model, LLM provider (Ollama, LMStudio, Mistral, OpenAI), and retrieval settings.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart indexing&lt;/strong&gt;: detects existing databases and optionally re-indexes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Beautiful CLI UX&lt;/strong&gt;: uses &lt;strong&gt;Rich&lt;/strong&gt; to colorize the interface; prompts are intuitive and clean.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Powered by LangChain&lt;/strong&gt; under the hood, but hidden behind the CLI for simplicity.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt;&lt;br /&gt; 👉 &lt;a href="https://github.com/Bessouat40/RAGLight"&gt;https://github.com/Bessouat40/RAGLight&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Labess40"&gt; /u/Labess40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1me2bik/introducing_new_raglight_library_feature_chat_cli/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1me2bik/introducing_new_raglight_library_feature_chat_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1me2bik/introducing_new_raglight_library_feature_chat_cli/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-31T13:58:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mej8og</id>
    <title>An Ollama wrapper for IRC/Slack/Discord, you want to run your own AI for chat? Here ya go.</title>
    <updated>2025-08-01T01:15:58+00:00</updated>
    <author>
      <name>/u/jjasghar</name>
      <uri>https://old.reddit.com/user/jjasghar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mej8og/an_ollama_wrapper_for_ircslackdiscord_you_want_to/"&gt; &lt;img alt="An Ollama wrapper for IRC/Slack/Discord, you want to run your own AI for chat? Here ya go." src="https://external-preview.redd.it/g6vxT0esOljml79fXZH6UeNZ4VDz-9uDcCn3t81Ue44.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76d1057ca7bd51891610bbd01e1b4e895cebeb60" title="An Ollama wrapper for IRC/Slack/Discord, you want to run your own AI for chat? Here ya go." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jjasghar"&gt; /u/jjasghar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jjasghar/ai-irc-slack-discord-ollama-bot"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mej8og/an_ollama_wrapper_for_ircslackdiscord_you_want_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mej8og/an_ollama_wrapper_for_ircslackdiscord_you_want_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T01:15:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdjtv7</id>
    <title>Ollama’s new app — Ollama 0.10 is here for macOS and Windows!</title>
    <updated>2025-07-30T21:59:22+00:00</updated>
    <author>
      <name>/u/bllshrfv</name>
      <uri>https://old.reddit.com/user/bllshrfv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mdjtv7/ollamas_new_app_ollama_010_is_here_for_macos_and/"&gt; &lt;img alt="Ollama’s new app — Ollama 0.10 is here for macOS and Windows!" src="https://preview.redd.it/u06kk6m433gf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=41739ef75f658e3fc5a92523b4481bb7cb36b537" title="Ollama’s new app — Ollama 0.10 is here for macOS and Windows!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Download on ollama.com/download &lt;/p&gt; &lt;p&gt;or GitHub releases&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.10.0"&gt;https://github.com/ollama/ollama/releases/tag/v0.10.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://ollama.com/blog/new-app"&gt;Ollama's new app&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bllshrfv"&gt; /u/bllshrfv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u06kk6m433gf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mdjtv7/ollamas_new_app_ollama_010_is_here_for_macos_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mdjtv7/ollamas_new_app_ollama_010_is_here_for_macos_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-30T21:59:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1melxlt</id>
    <title>Waiting on direct MCP integration—dev team, got a roadmap update?</title>
    <updated>2025-08-01T03:28:39+00:00</updated>
    <author>
      <name>/u/myusuf3</name>
      <uri>https://old.reddit.com/user/myusuf3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Do we know if (or when) MCP is slated for the Ollama desktop app?&lt;/p&gt; &lt;p&gt;I’ve seen references to MCP servers out in the wild, but haven’t spotted anything concrete on the official roadmap. If a timeline exists—rough estimate, next release branch, “sometime after X feature,” whatever—would love to hear it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/myusuf3"&gt; /u/myusuf3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1melxlt/waiting_on_direct_mcp_integrationdev_team_got_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1melxlt/waiting_on_direct_mcp_integrationdev_team_got_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1melxlt/waiting_on_direct_mcp_integrationdev_team_got_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T03:28:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1merzq7</id>
    <title>How to Make AI Agents Collaborate with ACP (Agent Communication Protocol)</title>
    <updated>2025-08-01T09:33:39+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1merzq7/how_to_make_ai_agents_collaborate_with_acp_agent/"&gt; &lt;img alt="How to Make AI Agents Collaborate with ACP (Agent Communication Protocol)" src="https://external-preview.redd.it/2Bn6jjb_5hT8ZY-5bxvozopGwfkkZPqgZVGqgsVrrM8.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b28b5593b154a43ba80e7815e81564650675ab7" title="How to Make AI Agents Collaborate with ACP (Agent Communication Protocol)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/fABcNHKVqYM"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1merzq7/how_to_make_ai_agents_collaborate_with_acp_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1merzq7/how_to_make_ai_agents_collaborate_with_acp_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T09:33:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1meox99</id>
    <title>New Qwen3 Coder 30B does not support tools?</title>
    <updated>2025-08-01T06:16:44+00:00</updated>
    <author>
      <name>/u/iChrist</name>
      <uri>https://old.reddit.com/user/iChrist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems like the ollama library lists the new Qwen3 coder as not supported by tool callings (native/default) It sure does support them, surely a config issue&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iChrist"&gt; /u/iChrist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1meox99/new_qwen3_coder_30b_does_not_support_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1meox99/new_qwen3_coder_30b_does_not_support_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1meox99/new_qwen3_coder_30b_does_not_support_tools/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T06:16:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1medmuz</id>
    <title>New Ollama App Tutorial</title>
    <updated>2025-07-31T21:11:10+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1medmuz/new_ollama_app_tutorial/"&gt; &lt;img alt="New Ollama App Tutorial" src="https://external-preview.redd.it/uceaGE0btrjND6BUkirKo6Z6KNAPzXKSLsJYDyaPW9k.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e83c90aca5bc9d70b56463f3cd401eb1cf9e6978" title="New Ollama App Tutorial" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/e1Ey4tMxD34"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1medmuz/new_ollama_app_tutorial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1medmuz/new_ollama_app_tutorial/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-31T21:11:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1memma3</id>
    <title>has anyone actually gotten rag + ocr to work properly? or are we all coping silently lol</title>
    <updated>2025-08-01T04:04:28+00:00</updated>
    <author>
      <name>/u/wfgy_engine</name>
      <uri>https://old.reddit.com/user/wfgy_engine</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;so yeah. been building a ton of rag pipelines lately — pdfs, images, scanned docs, you name it.&lt;br /&gt; tried all the standard tricks… docsplit, tesseract, &lt;a href="http://unstructured.io"&gt;unstructured.io&lt;/a&gt;, langchain’s pdfloader, even some visual embedding stuff. &lt;/p&gt; &lt;p&gt;and dude. everything &lt;em&gt;kinda&lt;/em&gt; works, but then it silently doesn’t.&lt;/p&gt; &lt;p&gt;like retrieval finds the file, &lt;/p&gt; &lt;p&gt;but grabs a paragraph from page 7 when the question is about page 3. &lt;/p&gt; &lt;p&gt;or chunking keeps splitting diagrams mid-sentence.&lt;br /&gt; or ocr adds hidden newline hell that breaks everything downstream.&lt;/p&gt; &lt;p&gt;spent months debugging this shit, &lt;/p&gt; &lt;p&gt;ended up writing out a full map of common failure cases — like, 16+ of them. &lt;/p&gt; &lt;p&gt;stuff like semantic drift, interpretation collapse, vector false positives, and my favorite: the “first-call oops infra wasn’t even ready” special.&lt;/p&gt; &lt;p&gt;anyway. finally built a fix. &lt;/p&gt; &lt;p&gt;open-source. fully documented. &lt;/p&gt; &lt;p&gt;even got a star from the guy who &lt;strong&gt;made tesseract.js&lt;/strong&gt;:&lt;br /&gt; 👉 &lt;a href="https://github.com/bijection?tab=stars"&gt;https://github.com/bijection?tab=stars&lt;/a&gt; （it’s the one pinned at the top）&lt;/p&gt; &lt;p&gt;&lt;strong&gt;won’t paste the repo unless someone asks&lt;/strong&gt; — just wanna know if anyone else is dealing w/ the same madness. &lt;/p&gt; &lt;p&gt;if you are, i got you. it’s all mapped, diagnosed, and patched.&lt;/p&gt; &lt;p&gt;don’t suffer in silence lol.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wfgy_engine"&gt; /u/wfgy_engine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1memma3/has_anyone_actually_gotten_rag_ocr_to_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1memma3/has_anyone_actually_gotten_rag_ocr_to_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1memma3/has_anyone_actually_gotten_rag_ocr_to_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T04:04:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1meeol9</id>
    <title>qwen3-coder is here</title>
    <updated>2025-07-31T21:53:23+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ollama.com/library/qwen3-coder"&gt;https://ollama.com/library/qwen3-coder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen3-Coder is the most agentic code model to date in the Qwen series, available in 30B model and 480B MoE models.&lt;/p&gt; &lt;p&gt;&lt;a href="https://qwenlm.github.io/blog/qwen3-coder/"&gt;https://qwenlm.github.io/blog/qwen3-coder/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1meeol9/qwen3coder_is_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1meeol9/qwen3coder_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1meeol9/qwen3coder_is_here/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-31T21:53:23+00:00</published>
  </entry>
</feed>
