<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-06-26T20:07:10+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1lit6oy</id>
    <title>üß†üí¨ Introducing AI Dialogue Duo ‚Äì A Two-AI Conversational Roleplay System (Open Source)</title>
    <updated>2025-06-23T21:34:50+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! üëã&lt;/p&gt; &lt;p&gt;I‚Äôve just released &lt;strong&gt;AI-Dialogue-Duo&lt;/strong&gt; ‚Äì a lightweight, open-source tool that lets you run &lt;strong&gt;two local LLMs&lt;/strong&gt; side-by-side in a real-time, back-and-forth dialogue.&lt;/p&gt; &lt;p&gt;&lt;a href="https://imgur.com/a/YXAnngw"&gt;https://imgur.com/a/YXAnngw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üîß &lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Spins up two separate models using &lt;a href="https://ollama.com/"&gt;Ollama&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Lets them &amp;quot;talk&amp;quot; to each other in turns&lt;/li&gt; &lt;li&gt;Great for testing prompt strategies, comparing models, or just watching two AIs debate anything you throw at them&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üí° &lt;strong&gt;Use Cases:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt engineering &amp;amp; testing&lt;/li&gt; &lt;li&gt;Simulated debates, interviews, or storytelling&lt;/li&gt; &lt;li&gt;LLM evaluation and comparison&lt;/li&gt; &lt;li&gt;Or just for fun!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üñ•Ô∏è &lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Python 3.11+&lt;/li&gt; &lt;li&gt;Ollama with your favorite models (e.g., LLaMA3, Mistral, Gemma, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üì¶ GitHub: &lt;a href="https://github.com/Laszlobeer/AI-Dialogue-Duo"&gt;https://github.com/Laszlobeer/AI-Dialogue-Duo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I built this because I wanted an easy way to watch different models interact‚Äîand it turns out, the results can be both hilarious and surprisingly insightful.&lt;/p&gt; &lt;p&gt;Would love feedback, ideas, and pull requests. If you try it out, feel free to share your favorite AI convos in the thread! ü§ñü§ñ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lit6oy/introducing_ai_dialogue_duo_a_twoai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lit6oy/introducing_ai_dialogue_duo_a_twoai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lit6oy/introducing_ai_dialogue_duo_a_twoai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-23T21:34:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1liq85g</id>
    <title>Can some AI models be illegal ?</title>
    <updated>2025-06-23T19:39:39+00:00</updated>
    <author>
      <name>/u/matdefays</name>
      <uri>https://old.reddit.com/user/matdefays</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was searching for uncensored models and then I came across this model : &lt;a href="https://ollama.com/gdisney/mistral-uncensored"&gt;https://ollama.com/gdisney/mistral-uncensored&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I downloaded it but then I asked myself, can AI models be illegal ?&lt;/p&gt; &lt;p&gt;Or it just depends on how you use them ?&lt;/p&gt; &lt;p&gt;I mean, it really looks too uncensored.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matdefays"&gt; /u/matdefays &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1liq85g/can_some_ai_models_be_illegal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1liq85g/can_some_ai_models_be_illegal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1liq85g/can_some_ai_models_be_illegal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-23T19:39:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lifhbg</id>
    <title>Llama on iPhone's Neural Engine - 0.05s to first token</title>
    <updated>2025-06-23T12:37:35+00:00</updated>
    <author>
      <name>/u/Glad-Speaker3006</name>
      <uri>https://old.reddit.com/user/Glad-Speaker3006</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lifhbg/llama_on_iphones_neural_engine_005s_to_first_token/"&gt; &lt;img alt="Llama on iPhone's Neural Engine - 0.05s to first token" src="https://preview.redd.it/om1dws269o8f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e13f8161d5ef3f38be856535c310d98ff277e99" title="Llama on iPhone's Neural Engine - 0.05s to first token" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just pushed a significant update to Vector Space, the app that runs LLMs directly on your iPhone's Apple Neural Engine. If you've been wanting to run AI models locally without destroying your battery, this might be exactly what you're looking for.&lt;/p&gt; &lt;p&gt;What makes Vector Space different&lt;/p&gt; &lt;p&gt;‚Ä¢ 4x more power efficient - Uses Apple's Neural Engine instead of GPU, so your phone stays cool and your battery actually lasts&lt;/p&gt; &lt;p&gt;‚Ä¢ Blazing fast inference - 0.05s to first token, sustaining 35 tokens/sec (iPhone 14 Pro Max, Llama 3.2 1b)&lt;/p&gt; &lt;p&gt;‚Ä¢ Proper context window - Full 8K context length for real conversations&lt;/p&gt; &lt;p&gt;‚Ä¢ Smart quantization - Maintains accuracy where it matters (tool calling still works perfectly)&lt;/p&gt; &lt;p&gt;‚Ä¢ Zero setup hassle - Literally download ‚Üí run. No configuration needed.&lt;/p&gt; &lt;p&gt;Note: First model load takes ~5 minutes (one-time setup), then subsequent loads are 1-2 seconds.&lt;/p&gt; &lt;p&gt;TestFlight link: &lt;a href="https://testflight.apple.com/join/HXyt2bjU"&gt;https://testflight.apple.com/join/HXyt2bjU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For current testers:Delete the old version before updating - there were some breaking changes under the hood.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glad-Speaker3006"&gt; /u/Glad-Speaker3006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/om1dws269o8f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lifhbg/llama_on_iphones_neural_engine_005s_to_first_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lifhbg/llama_on_iphones_neural_engine_005s_to_first_token/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-23T12:37:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljifna</id>
    <title>Image generator that can accept images?</title>
    <updated>2025-06-24T18:09:54+00:00</updated>
    <author>
      <name>/u/Key_Appointment_7582</name>
      <uri>https://old.reddit.com/user/Key_Appointment_7582</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are there any image generators that can accept my own images. For example, if I want to make memes based on my or my friends' likeliness is there a model that I can upload context images and then make it alter those images. All the image generators I see only accept text and then spit out an image. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Key_Appointment_7582"&gt; /u/Key_Appointment_7582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ljifna/image_generator_that_can_accept_images/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ljifna/image_generator_that_can_accept_images/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ljifna/image_generator_that_can_accept_images/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-24T18:09:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lj75sb</id>
    <title>why do we have to tokenize our input in huggingface but not in ollama?</title>
    <updated>2025-06-24T10:04:56+00:00</updated>
    <author>
      <name>/u/Beyond_Birthday_13</name>
      <uri>https://old.reddit.com/user/Beyond_Birthday_13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;when you use ollama you are able to use the models right away unlike huggingface where you need to tokenized and maybe quantize and so on&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beyond_Birthday_13"&gt; /u/Beyond_Birthday_13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lj75sb/why_do_we_have_to_tokenize_our_input_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lj75sb/why_do_we_have_to_tokenize_our_input_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lj75sb/why_do_we_have_to_tokenize_our_input_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-24T10:04:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljjpjq</id>
    <title>TinyTavern - Ollama and Openrouter client for Character Chat via mobile app</title>
    <updated>2025-06-24T18:58:09+00:00</updated>
    <author>
      <name>/u/Ill_Marketing_5245</name>
      <uri>https://old.reddit.com/user/Ill_Marketing_5245</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ljjpjq/tinytavern_ollama_and_openrouter_client_for/"&gt; &lt;img alt="TinyTavern - Ollama and Openrouter client for Character Chat via mobile app" src="https://external-preview.redd.it/I4ZwGn49f0Fz8dspKD3UYF9Hq1zJGXwnOJqZm4H1e7E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cbbe9fc0be112d7d20644d0b5b8c74169cabaa83" title="TinyTavern - Ollama and Openrouter client for Character Chat via mobile app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I love SillyTavern so much, I'm using my hosted Ollama on my other machine and tunnelling via ngrok so I can chat &amp;quot;locally&amp;quot; with my characters. &lt;/p&gt; &lt;p&gt;I wonder if I still can chat with my characters on the go using mobile app. I'm looking for existing solution where I can chat using hosted Ollama like enchanted app, but can't find any.&lt;/p&gt; &lt;p&gt;So I vibe code my way, and within 5 hours, I have this: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tiny Tavern.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;You can connect to ollama or openrouter. &lt;/p&gt; &lt;p&gt;If you don't know already, you can completely use Openrouter for free because they have up to 60 free model you can use. &lt;/p&gt; &lt;p&gt;I test all free model to see if any of them can be used for ERP. I can share my finding if you want. &lt;/p&gt; &lt;p&gt;Using this app you can import any Character card with chara_card_v2 or chara_card_v3 specs.&lt;br /&gt; Export from your silly tavern, or download character PNG from various website such as character-tavern.com. &lt;/p&gt; &lt;p&gt;Setup instruction and everything is on this github link:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/virkillz/tinytavern"&gt;https://github.com/virkillz/tinytavern&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Give me star if you like it. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zeh06x4z9x8f1.png?width=1170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d13dd437f47e0c7997c845b25416bc74b4395533"&gt;https://preview.redd.it/zeh06x4z9x8f1.png?width=1170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d13dd437f47e0c7997c845b25416bc74b4395533&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c3div05z9x8f1.png?width=1170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0affed45acb63c0008c9276b56ab014748ea2413"&gt;https://preview.redd.it/c3div05z9x8f1.png?width=1170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0affed45acb63c0008c9276b56ab014748ea2413&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/53r36y4z9x8f1.png?width=1170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6d303fdcee9a1461348386df2b0fdf380e95cf53"&gt;https://preview.redd.it/53r36y4z9x8f1.png?width=1170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6d303fdcee9a1461348386df2b0fdf380e95cf53&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ddd3kw4z9x8f1.png?width=1170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0efeab093e11f86a5ffb0bb3ecfb00b4898d7b7"&gt;https://preview.redd.it/ddd3kw4z9x8f1.png?width=1170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0efeab093e11f86a5ffb0bb3ecfb00b4898d7b7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill_Marketing_5245"&gt; /u/Ill_Marketing_5245 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ljjpjq/tinytavern_ollama_and_openrouter_client_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ljjpjq/tinytavern_ollama_and_openrouter_client_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ljjpjq/tinytavern_ollama_and_openrouter_client_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-24T18:58:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljbxgt</id>
    <title>how would you approach about making a book summerizer using rag?</title>
    <updated>2025-06-24T14:02:27+00:00</updated>
    <author>
      <name>/u/Beyond_Birthday_13</name>
      <uri>https://old.reddit.com/user/Beyond_Birthday_13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the best approach i can think of is to chunk the book using langchain, then each chunk would go to a for loop that vectorized them and feed them to the llm, maybe vectorizing isn't neccissery and feeding the text raw would be enough, but that's just a suggestion, is there a better way to make it?, I was thinking about transforming the entire book to vector and then make the llm do the summery, but I don't think the model I can have, which has like 100k tokens can output enough words to summarize the whole book, my idea is to turn like 500 pages to 30 or 50 pages, would passing like one or some chunks at a time in a for loop be a good idea?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beyond_Birthday_13"&gt; /u/Beyond_Birthday_13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ljbxgt/how_would_you_approach_about_making_a_book/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ljbxgt/how_would_you_approach_about_making_a_book/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ljbxgt/how_would_you_approach_about_making_a_book/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-24T14:02:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljg3nw</id>
    <title>WebBench: A real-world benchmark for Browser Agents</title>
    <updated>2025-06-24T16:42:50+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ljg3nw/webbench_a_realworld_benchmark_for_browser_agents/"&gt; &lt;img alt="WebBench: A real-world benchmark for Browser Agents" src="https://preview.redd.it/vbvyc82ulw8f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea27bff4ad70e9eddc65ab924ee110f0dfdd116b" title="WebBench: A real-world benchmark for Browser Agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;WebBench is an open, task-oriented benchmark designed to measure how effectively browser agents handle complex, realistic web workflows. It includes 2,454 tasks across 452 live websites selected from the global top-1000 by traffic.&lt;/p&gt; &lt;p&gt;GitHub : &lt;a href="https://github.com/Halluminate/WebBench"&gt;https://github.com/Halluminate/WebBench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vbvyc82ulw8f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ljg3nw/webbench_a_realworld_benchmark_for_browser_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ljg3nw/webbench_a_realworld_benchmark_for_browser_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-24T16:42:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk3lol</id>
    <title>GPU for deepseek-r1:8b</title>
    <updated>2025-06-25T11:52:08+00:00</updated>
    <author>
      <name>/u/davidetakotako</name>
      <uri>https://old.reddit.com/user/davidetakotako</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hello everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm planning to run Deepseek-R1-8B and wanted to get a sense of real-world performance on a mid-range GPU. Here‚Äôs my setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; RTX 5070 (12 GB VRAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Ryzen 5 5600X&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 64 GB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context length:&lt;/strong&gt; realistically ~15 K tokens (I‚Äôve capped it at 20 K to be safe)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;On my laptop (RTX 3060 6 GB), generating the TXT file I need takes about 12 minutes, which isn‚Äôt terrible. though it‚Äôs a bit slow for production.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My question:&lt;/strong&gt; Would an RTX 5070 be ‚Äúfast enough‚Äù for a reliable production environment with this model and workload?&lt;/p&gt; &lt;p&gt;thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davidetakotako"&gt; /u/davidetakotako &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lk3lol/gpu_for_deepseekr18b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lk3lol/gpu_for_deepseekr18b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lk3lol/gpu_for_deepseekr18b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-25T11:52:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljr0iq</id>
    <title>Roleplaying for real?</title>
    <updated>2025-06-24T23:55:29+00:00</updated>
    <author>
      <name>/u/No_Vegetable6570</name>
      <uri>https://old.reddit.com/user/No_Vegetable6570</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been spending a lot of time in LLM communities lately, and I've noticed ppl are focused on finding the best models for Roleplaying and uncensored models for this purpose seems alot.&lt;/p&gt; &lt;p&gt;This has me genuinely curious, because in my offline life, I don't really know anyone who's into RP. It's made me wonder , is it really just for RP? or is it a proxy for something else?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1&lt;/strong&gt;: text-based Roleplaying is a far larger and more passionate hobby than many of us realize? &lt;/p&gt; &lt;p&gt;&lt;strong&gt;2:&lt;/strong&gt; Or, is RP less about the hobby itself and more of a proxy for a model's overall quality? A good RP session requires an LLM to excel at multiple difficult tasks simultaneously... maybe?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Vegetable6570"&gt; /u/No_Vegetable6570 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ljr0iq/roleplaying_for_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ljr0iq/roleplaying_for_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ljr0iq/roleplaying_for_real/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-24T23:55:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk5zum</id>
    <title>How do I setup Ollama to run on my GPU?</title>
    <updated>2025-06-25T13:41:32+00:00</updated>
    <author>
      <name>/u/Vashe00</name>
      <uri>https://old.reddit.com/user/Vashe00</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have downloaded ollama from the website and also through pip (as I mainly use it through python scripts) and I‚Äôm also using gemma3:27b.&lt;/p&gt; &lt;p&gt;My scripts are running flawlessly, but I can see that it is purely using my CPU.&lt;/p&gt; &lt;p&gt;Windows 11&lt;/p&gt; &lt;p&gt;My CPU is a 13th gen intel(R) core(tm) i9-13950HX&lt;/p&gt; &lt;p&gt;GPU0 - Intel(R) UHD Graphics&lt;/p&gt; &lt;p&gt;GPU1 - NVIDA RTX5000 Ada Generation Laptop GPU&lt;/p&gt; &lt;p&gt;128 GB RAM&lt;/p&gt; &lt;p&gt;I just haven‚Äôt seen anything online on how to reliably setup my model and ollama to utilize the GPU instead of the CPU.&lt;/p&gt; &lt;p&gt;Can anyone point me to a step by step tutorial?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vashe00"&gt; /u/Vashe00 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lk5zum/how_do_i_setup_ollama_to_run_on_my_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lk5zum/how_do_i_setup_ollama_to_run_on_my_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lk5zum/how_do_i_setup_ollama_to_run_on_my_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-25T13:41:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk8f1g</id>
    <title>Ollama serve logs say new model will fit in gpu vram but nvidia smi shows no usage ?</title>
    <updated>2025-06-25T15:16:59+00:00</updated>
    <author>
      <name>/u/Feeling_Ad6553</name>
      <uri>https://old.reddit.com/user/Feeling_Ad6553</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to run openhermes 2.5 7b parameter model on nvidia tesla t4 on Linux. The initial logs say model offload to cuda and model will fit into gpu. But the inference is slow and nvidia smi shows no processes found&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Feeling_Ad6553"&gt; /u/Feeling_Ad6553 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lk8f1g/ollama_serve_logs_say_new_model_will_fit_in_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lk8f1g/ollama_serve_logs_say_new_model_will_fit_in_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lk8f1g/ollama_serve_logs_say_new_model_will_fit_in_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-25T15:16:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkn5a8</id>
    <title>What‚Äôs the best user interface for AGI like?</title>
    <updated>2025-06-26T01:12:14+00:00</updated>
    <author>
      <name>/u/suvsuvsuv</name>
      <uri>https://old.reddit.com/user/suvsuvsuv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let's say we will achieve AGI tomorrow, can we feel it with the current shape of AI applications with chat UI? If not, what should it be like?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/suvsuvsuv"&gt; /u/suvsuvsuv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lkn5a8/whats_the_best_user_interface_for_agi_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lkn5a8/whats_the_best_user_interface_for_agi_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lkn5a8/whats_the_best_user_interface_for_agi_like/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T01:12:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkepfp</id>
    <title>Looking for Metrics, Reports, or Case Studies on Ollama in Enterprise Environments</title>
    <updated>2025-06-25T19:14:27+00:00</updated>
    <author>
      <name>/u/patitopower</name>
      <uri>https://old.reddit.com/user/patitopower</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi, does anyone know of any reliable reports or metrics on Ollama adoption in businesses? thanks for any insights or resources!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/patitopower"&gt; /u/patitopower &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lkepfp/looking_for_metrics_reports_or_case_studies_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lkepfp/looking_for_metrics_reports_or_case_studies_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lkepfp/looking_for_metrics_reports_or_case_studies_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-25T19:14:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkhizo</id>
    <title>Ollama won't listen to connections outside of localhost machine.</title>
    <updated>2025-06-25T21:05:03+00:00</updated>
    <author>
      <name>/u/Gamervote</name>
      <uri>https://old.reddit.com/user/Gamervote</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried editing the sudo systemctl edit ollama command to change the port that it listens on, to no avail. I'm running ollama on a ubuntu server. Pls help lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gamervote"&gt; /u/Gamervote &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lkhizo/ollama_wont_listen_to_connections_outside_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lkhizo/ollama_wont_listen_to_connections_outside_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lkhizo/ollama_wont_listen_to_connections_outside_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-25T21:05:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkaqlt</id>
    <title>üöÄ Revamped My Dungeon AI GUI Project ‚Äì Now with a Clean Interface &amp; Better Usability!</title>
    <updated>2025-06-25T16:44:38+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lkaqlt/revamped_my_dungeon_ai_gui_project_now_with_a/"&gt; &lt;img alt="üöÄ Revamped My Dungeon AI GUI Project ‚Äì Now with a Clean Interface &amp;amp; Better Usability!" src="https://external-preview.redd.it/NpzXevyc8hccQld5KSdg19C9gOjRKnxTSGT0NaYuRD8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=429f3f424c02aa8038222f2c0d7e58d883b099aa" title="üöÄ Revamped My Dungeon AI GUI Project ‚Äì Now with a Clean Interface &amp;amp; Better Usability!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/m7ca9bd1r39f1.gif"&gt;https://i.redd.it/m7ca9bd1r39f1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey folks!&lt;br /&gt; I just gave my old project &lt;a href="https://github.com/Laszlobeer/Dungeo_ai"&gt;Dungeo_ai&lt;/a&gt; a serious upgrade and wanted to share the improved version:&lt;br /&gt; üîó &lt;a href="https://github.com/Laszlobeer/Dungeo_ai_GUI"&gt;&lt;strong&gt;Dungeo_ai_GUI on GitHub&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a &lt;strong&gt;local, GUI-based Dungeon Master AI&lt;/strong&gt; designed to let you roleplay solo DnD-style adventures using your own LLM (like a local LLaMA model via Ollama). The original project was CLI-based and clunky, but now it‚Äôs been reworked with:&lt;/p&gt; &lt;p&gt;üß† &lt;strong&gt;Improvements:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üñ•Ô∏è &lt;strong&gt;User-friendly GUI&lt;/strong&gt; using &lt;code&gt;tkinter&lt;/code&gt;&lt;/li&gt; &lt;li&gt;üéÆ More immersive roleplay support&lt;/li&gt; &lt;li&gt;üíæ Easy save/load system for sessions&lt;/li&gt; &lt;li&gt;üõ†Ô∏è Cleaner codebase and better modularity for community mods&lt;/li&gt; &lt;li&gt;üß© Simple integration with local LLM APIs (e.g. Ollama, LM Studio)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üß™ Currently testing with local models like &lt;strong&gt;LLaMA 3 8B/13B&lt;/strong&gt;, and performance is smooth even on mid-range hardware.&lt;/p&gt; &lt;p&gt;If you‚Äôre into solo RPGs, interactive storytelling, or just want to tinker with AI-powered DMs, I‚Äôd love your feedback or contributions!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try it, break it, or fork it:&lt;/strong&gt;&lt;br /&gt; üëâ &lt;a href="https://github.com/Laszlobeer/Dungeo_ai_GUI"&gt;https://github.com/Laszlobeer/Dungeo_ai_GUI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy dungeon delving! üêâ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lkaqlt/revamped_my_dungeon_ai_gui_project_now_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lkaqlt/revamped_my_dungeon_ai_gui_project_now_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lkaqlt/revamped_my_dungeon_ai_gui_project_now_with_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-25T16:44:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lks25n</id>
    <title>Bring your own LLM server</title>
    <updated>2025-06-26T05:30:48+00:00</updated>
    <author>
      <name>/u/numinouslymusing</name>
      <uri>https://old.reddit.com/user/numinouslymusing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So if you‚Äôre a hobby developer making an app you want to release for free to the internet, chances are you can‚Äôt just pay for the inference costs for users, so logic kind of dictates you make the app bring-your-own-key.&lt;/p&gt; &lt;p&gt;So while ideating along the lines of ‚Äúhow can I have users have free LLMs?‚Äù I thought of webllm, which is a very cool project, but a couple of drawbacks that made me want to find an alternate solution was the lack of support for the OpenAI ask, and lack of multimodal support.&lt;/p&gt; &lt;p&gt;Then I arrived at the idea of a ‚Äúbring your own LLM server‚Äù model, where people can still use hosted, book providers, but people can also spin up local servers with ollama or llama cpp, expose the port over ngrok, and use that.&lt;/p&gt; &lt;p&gt;Idk this may sound redundant to some but I kinda just wanted to hear some other ideas/thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/numinouslymusing"&gt; /u/numinouslymusing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lks25n/bring_your_own_llm_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lks25n/bring_your_own_llm_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lks25n/bring_your_own_llm_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T05:30:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lku8ip</id>
    <title>Is there a 'ready-to-use' Linux distribution for running LLMs locally (like Ollama)?</title>
    <updated>2025-06-26T07:49:39+00:00</updated>
    <author>
      <name>/u/AreBee73</name>
      <uri>https://old.reddit.com/user/AreBee73</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, do you know of a Linux distribution specifically prepared to use ollama or other LMMs locally, therefore preconfigured and specific for this purpose?&lt;/p&gt; &lt;p&gt;In practice, provided already &amp;quot;ready to use&amp;quot; with only minimal settings to change.&lt;/p&gt; &lt;p&gt;A bit like there are specific distributions for privacy or other sectoral tasks.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AreBee73"&gt; /u/AreBee73 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lku8ip/is_there_a_readytouse_linux_distribution_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lku8ip/is_there_a_readytouse_linux_distribution_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lku8ip/is_there_a_readytouse_linux_distribution_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T07:49:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll5kes</id>
    <title>I built an AI Compound Analyzer with a custom multi-agent backend (Agno/Python) and a TypeScript/React frontend.</title>
    <updated>2025-06-26T17:06:06+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ll5kes/i_built_an_ai_compound_analyzer_with_a_custom/"&gt; &lt;img alt="I built an AI Compound Analyzer with a custom multi-agent backend (Agno/Python) and a TypeScript/React frontend." src="https://external-preview.redd.it/NnNqYmV6aW96YTlmMeQY9vO3ByPnz_trCSlC0d709aWnAv-iGUW2bEz5TRNb.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f05559fd916dbbe8194251bf5b7a3f45d453330" title="I built an AI Compound Analyzer with a custom multi-agent backend (Agno/Python) and a TypeScript/React frontend." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been deep in a personal project building a larger &amp;quot;BioAI Platform,&amp;quot; and I'm excited to share the first major module. It's an AI Compound Analyzer that takes a chemical name, pulls its structure, and runs a full analysis for things like molecular properties and ADMET predictions (basically, how a drug might behave in the body).&lt;/p&gt; &lt;p&gt;The goal was to build a highly responsive, modern tool.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; TypeScript, React, Next.js, and framer-motion for the smooth animations.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; This is where it gets fun. I used &lt;strong&gt;Agno&lt;/strong&gt;, a lightweight Python framework, to build a multi-agent system that orchestrates the analysis. It's a faster, leaner alternative to some of the bigger agentic frameworks out there.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Communication:&lt;/strong&gt; I'm using Server-Sent Events (SSE) to stream the analysis results from the backend to the frontend in real-time, which is what makes the UI update live as it works.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's been a challenging but super rewarding project, especially getting the backend agents to communicate efficiently with the reactive frontend.&lt;/p&gt; &lt;p&gt;Would love to hear any thoughts on the architecture or if you have suggestions for other cool open-source tools to integrate!&lt;/p&gt; &lt;p&gt;üöÄ P.S. I am looking for new roles , If you like my work and have any Opportunites in Computer Vision or LLM Domain do contact me&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;My Email:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=mailto%3Apavankunchalaofficial%40gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My GitHub Profile (for more projects):&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My Resume:&lt;/strong&gt; &lt;a href="https://drive.google.com/file/d/1LVMVgAPKGUJbnrfE09OLJ0MrEZlBccOT/view"&gt;https://drive.google.com/file/d/1LVMVgAPKGUJbnrfE09OLJ0MrEZlBccOT/view&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gvkxb6joza9f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ll5kes/i_built_an_ai_compound_analyzer_with_a_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ll5kes/i_built_an_ai_compound_analyzer_with_a_custom/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T17:06:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lla5bn</id>
    <title>Homebrew install of Ollama 0.9.3 still has binary that reports as 0.9.0</title>
    <updated>2025-06-26T20:04:43+00:00</updated>
    <author>
      <name>/u/illkeepthatinmind</name>
      <uri>https://old.reddit.com/user/illkeepthatinmind</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else seeing this? Can't run the new Gemma model due to this. Already tried reinstalling and with cleared brew cache.&lt;/p&gt; &lt;p&gt;&lt;code&gt;brew install ollama&lt;/code&gt;&lt;br /&gt; &lt;code&gt;Warning: Treating ollama as a formula. For the cask, use homebrew/cask/ollama-app or specify the \--cask\&lt;/code&gt; flag. To silence this message, use the `--formula` flag.&lt;code&gt; &lt;/code&gt;==&amp;gt; Downloading&lt;a href="https://ghcr.io/v2/homebrew/core/ollama/manifests/0.9.3"&gt;https://ghcr.io/v2/homebrew/core/ollama/manifests/0.9.3&lt;/a&gt;&lt;code&gt; &lt;/code&gt;...&lt;code&gt; &lt;/code&gt;...&lt;code&gt; &lt;/code&gt;ollama -v&lt;code&gt; &lt;/code&gt;ollama version is 0.9.0&lt;code&gt; &lt;/code&gt;Warning: client version is 0.9.3``&lt;/p&gt; &lt;p&gt;&lt;code&gt;...&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/illkeepthatinmind"&gt; /u/illkeepthatinmind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lla5bn/homebrew_install_of_ollama_093_still_has_binary/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lla5bn/homebrew_install_of_ollama_093_still_has_binary/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lla5bn/homebrew_install_of_ollama_093_still_has_binary/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T20:04:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll4us5</id>
    <title>Beautify Ollama</title>
    <updated>2025-06-26T16:38:25+00:00</updated>
    <author>
      <name>/u/falkon2112</name>
      <uri>https://old.reddit.com/user/falkon2112</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1ll4us5/video/5zt9ljutua9f1/player"&gt;https://reddit.com/link/1ll4us5/video/5zt9ljutua9f1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I got tired of the basic Ollama interfaces out there and decided to build something that looks like it belongs in 2025. Meet &lt;strong&gt;BeautifyOllama&lt;/strong&gt; - a modern web interface that makes chatting with your local AI models actually enjoyable.&lt;/p&gt; &lt;h1&gt;What it does:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Animated shine borders&lt;/strong&gt; that cycle through colors (because why not make AI conversations pretty?)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time streaming&lt;/strong&gt; responses that feel snappy&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dark/light themes&lt;/strong&gt; that follow your system preferences&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mobile-responsive&lt;/strong&gt; so you can chat with AI on the toilet (we've all been there)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Glassmorphism effects&lt;/strong&gt; and smooth animations everywhere&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Tech stack (for the nerds):&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Next.js 15 + React 19 (bleeding edge stuff)&lt;/li&gt; &lt;li&gt;TypeScript (because I like my code to not break)&lt;/li&gt; &lt;li&gt;TailwindCSS 4 (utility classes go brrr)&lt;/li&gt; &lt;li&gt;Framer Motion (for those buttery smooth animations)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Demo &amp;amp; Code:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Live demo:&lt;/strong&gt; &lt;a href="https://beautifyollama.vercel.app/"&gt;https://beautifyollama.vercel.app/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/falkon2/BeautifyOllama"&gt;https://github.com/falkon2/BeautifyOllama&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What's coming next:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;File uploads (drag &amp;amp; drop your docs)&lt;/li&gt; &lt;li&gt;Conversation history that doesn't disappear&lt;/li&gt; &lt;li&gt;Plugin system for extending functionality&lt;/li&gt; &lt;li&gt;Maybe a mobile app if people actually use this thing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup is stupid simple:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Have Ollama running (&lt;code&gt;ollama serve&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Clone the repo&lt;/li&gt; &lt;li&gt;&lt;code&gt;npm install &amp;amp;&amp;amp; npm run dev&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Profit&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I would appreciate any and all feedback as well as criticism.&lt;/p&gt; &lt;p&gt;The project is early-stage but functional. I'm actively working on it and would love feedback, contributions, or just general roasting of my code.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question for the community:&lt;/strong&gt; What features would you actually want in a local AI interface? I'm building this for real use,.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/falkon2112"&gt; /u/falkon2112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ll4us5/beautify_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ll4us5/beautify_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ll4us5/beautify_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T16:38:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll4wfy</id>
    <title>Troll My First SaaS app</title>
    <updated>2025-06-26T16:40:12+00:00</updated>
    <author>
      <name>/u/Significant_Abroad36</name>
      <uri>https://old.reddit.com/user/Significant_Abroad36</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ll4wfy/troll_my_first_saas_app/"&gt; &lt;img alt="Troll My First SaaS app" src="https://external-preview.redd.it/MWFyMGU1cDV2YTlmMWde8gwunT_bYnWBdtlOsaKnzitVEHx3CN-s6EvsSOjA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7cea005975486b63a632859ece4b535ed73d4379" title="Troll My First SaaS app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys - I have built an app which creates a roadmap of chapters that you need to read to learn a given topic.&lt;/p&gt; &lt;p&gt;It is personalized, so chapters are created in runtime based on user's learning curve.&lt;/p&gt; &lt;p&gt;User has to pass each quiz to unlock the next chapter.&lt;/p&gt; &lt;p&gt;below is the video , check this out and tell me what you think and share some cool product recommendations.&lt;/p&gt; &lt;p&gt;Best reccomendations will get free access to the beta app ( + some GPU credits!!) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant_Abroad36"&gt; /u/Significant_Abroad36 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rj9b70p5va9f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ll4wfy/troll_my_first_saas_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ll4wfy/troll_my_first_saas_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T16:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkp8bu</id>
    <title>Anyone using Ollama with browser plugins? We built something interesting.</title>
    <updated>2025-06-26T02:55:26+00:00</updated>
    <author>
      <name>/u/InfiniteJX</name>
      <uri>https://old.reddit.com/user/InfiniteJX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks ‚Äî I‚Äôve been working a lot with &lt;strong&gt;Ollama&lt;/strong&gt; lately and really love how smooth it runs locally.&lt;/p&gt; &lt;p&gt;As part of exploring real-world uses, we recently built a Chrome extension called &lt;a href="https://nativemind.app/"&gt;&lt;strong&gt;NativeMind&lt;/strong&gt;&lt;/a&gt;. It connects to your local Ollama instance and lets you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Summarize any webpage directly in a sidebar&lt;/li&gt; &lt;li&gt;Ask questions about the current page content&lt;/li&gt; &lt;li&gt;Do &lt;em&gt;local&lt;/em&gt; search across open tabs ‚Äî no cloud needed, which I think is super cool&lt;/li&gt; &lt;li&gt;Plug-and-play with any model you‚Äôve started in Ollama&lt;/li&gt; &lt;li&gt;Run fully on-device (no external calls, ever)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It‚Äôs open-source and works out of the box ‚Äî just install and start chatting with the web like it‚Äôs a doc. I‚Äôve been using it for reading research papers, articles, and documentation, and it‚Äôs honestly made browsing a lot more productive.&lt;/p&gt; &lt;p&gt;üëâ GitHub: &lt;a href="https://github.com/NativeMindBrowser/NativeMindExtension"&gt;https://github.com/NativeMindBrowser/NativeMindExtension&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="https://chromewebstore.google.com/detail/nativemind-your-fully-pri/mgchaojnijgpemdfhpnbeejnppigfllj"&gt;Chrome Web Store&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear if anyone else here is exploring similar Ollama + browser workflows ‚Äî or if you try this one out, happy to take feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InfiniteJX"&gt; /u/InfiniteJX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lkp8bu/anyone_using_ollama_with_browser_plugins_we_built/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lkp8bu/anyone_using_ollama_with_browser_plugins_we_built/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lkp8bu/anyone_using_ollama_with_browser_plugins_we_built/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T02:55:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lktb12</id>
    <title>I tested 10 LLMs locally on my MacBook Air M1 (8GB RAM!) ‚Äì Here's what actually works-</title>
    <updated>2025-06-26T06:48:24+00:00</updated>
    <author>
      <name>/u/irodov4030</name>
      <uri>https://old.reddit.com/user/irodov4030</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lktb12/i_tested_10_llms_locally_on_my_macbook_air_m1_8gb/"&gt; &lt;img alt="I tested 10 LLMs locally on my MacBook Air M1 (8GB RAM!) ‚Äì Here's what actually works-" src="https://external-preview.redd.it/te4YfuD8PP6HnzEPXIgrUZWitrs0nRz7rrJC6dhgl-g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6851992f1a218c5b31b036d8faf07e13da48567" title="I tested 10 LLMs locally on my MacBook Air M1 (8GB RAM!) ‚Äì Here's what actually works-" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I went down the LLM rabbit hole trying to find the &lt;strong&gt;best local model&lt;/strong&gt; that runs &lt;em&gt;well&lt;/em&gt; on a humble MacBook Air M1 with just 8GB RAM.&lt;/p&gt; &lt;p&gt;My goal? &lt;strong&gt;Compare 10 models&lt;/strong&gt; across question generation, answering, and self-evaluation.&lt;/p&gt; &lt;p&gt;TL;DR: Some models were brilliant, others‚Ä¶ not so much. One even took &lt;strong&gt;8 minutes&lt;/strong&gt; to write a question.&lt;/p&gt; &lt;p&gt;Here's the breakdown &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Models Tested&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mistral 7B&lt;/li&gt; &lt;li&gt;DeepSeek-R1 1.5B&lt;/li&gt; &lt;li&gt;Gemma3:1b&lt;/li&gt; &lt;li&gt;Gemma3:latest&lt;/li&gt; &lt;li&gt;Qwen3 1.7B&lt;/li&gt; &lt;li&gt;Qwen2.5-VL 3B&lt;/li&gt; &lt;li&gt;Qwen3 4B&lt;/li&gt; &lt;li&gt;LLaMA 3.2 1B&lt;/li&gt; &lt;li&gt;LLaMA 3.2 3B&lt;/li&gt; &lt;li&gt;LLaMA 3.1 8B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;(All models run with quantized versions, via: os.environ[&amp;quot;OLLAMA_CONTEXT_LENGTH&amp;quot;] = &amp;quot;4096&amp;quot; and os.environ[&amp;quot;OLLAMA_KV_CACHE_TYPE&amp;quot;] = &amp;quot;q4_0&amp;quot;)&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Each model:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Generated 1 question on 5 topics: &lt;em&gt;Math, Writing, Coding, Psychology, History&lt;/em&gt;&lt;/li&gt; &lt;li&gt;Answered all 50 questions (5 x 10)&lt;/li&gt; &lt;li&gt;Evaluated every answer (including their own)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So in total:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;50 questions&lt;/li&gt; &lt;li&gt;500 answers&lt;/li&gt; &lt;li&gt;4830 evaluations (Should be 5000; I evaluated less answers with qwen3:1.7b and qwen3:4b as they do not generate scores and take a lot of time&lt;strong&gt;)&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And I tracked:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;token generation speed (tokens/sec)&lt;/li&gt; &lt;li&gt;tokens created&lt;/li&gt; &lt;li&gt;time taken&lt;/li&gt; &lt;li&gt;scored all answers for quality&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question Generation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fastest: &lt;strong&gt;LLaMA 3.2 1B&lt;/strong&gt;, &lt;strong&gt;Gemma3:1b&lt;/strong&gt;, &lt;strong&gt;Qwen3 1.7B&lt;/strong&gt; (LLaMA 3.2 1B hit 82 tokens/sec, avg is ~40 tokens/sec (for english topic question it reached &lt;strong&gt;146 tokens/sec)&lt;/strong&gt; &lt;/li&gt; &lt;li&gt;Slowest: &lt;strong&gt;LLaMA 3.1 8B&lt;/strong&gt;, &lt;strong&gt;Qwen3 4B&lt;/strong&gt;, &lt;strong&gt;Mistral 7B&lt;/strong&gt; Qwen3 4B took &lt;strong&gt;486s&lt;/strong&gt; (8+ mins) to generate a single Math question! &lt;/li&gt; &lt;li&gt;Fun fact: deepseek-r1:1.5b, qwen3:4b and Qwen3:1.7B output &amp;lt;think&amp;gt; tags in questions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Answer Generation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fastest: &lt;strong&gt;Gemma3:1b&lt;/strong&gt;, &lt;strong&gt;LLaMA 3.2 1B&lt;/strong&gt; and &lt;strong&gt;DeepSeek-R1 1.5B&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;DeepSeek got faster answering &lt;em&gt;its own&lt;/em&gt; questions (80 tokens/s vs. avg 40 tokens/s)&lt;/li&gt; &lt;li&gt;Qwen3 4B generates &lt;strong&gt;2‚Äì3x more tokens&lt;/strong&gt; per answer&lt;/li&gt; &lt;li&gt;Slowest: llama3.1:8b, qwen3:4b and mistral:7b&lt;/li&gt; &lt;/ul&gt; &lt;p&gt; &lt;strong&gt;Evaluation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Best scorer: Gemma3:latest ‚Äì consistent, numerical, no bias&lt;/li&gt; &lt;li&gt;Worst scorer: &lt;strong&gt;DeepSeek-R1 1.5B&lt;/strong&gt; ‚Äì often skipped scores entirely&lt;/li&gt; &lt;li&gt;Bias detected: Many models &lt;strong&gt;rate their own answers higher&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;DeepSeek even evaluated some answers &lt;strong&gt;in Chinese&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Fun Observations&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Some models create &amp;lt;think&amp;gt; tags for questions, answers and even while evaluation as output&lt;/li&gt; &lt;li&gt;Score inflation is real: Mistral, Qwen3, and LLaMA 3.1 8B overrate themselves&lt;/li&gt; &lt;li&gt;Score formats vary wildly (text explanations vs. plain numbers)&lt;/li&gt; &lt;li&gt;Speed isn‚Äôt everything ‚Äì some slower models gave much higher quality answers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Best Performers (My Picks)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;|| || |&lt;strong&gt;Task&lt;/strong&gt;|&lt;strong&gt;Best Model&lt;/strong&gt;|&lt;strong&gt;Why&lt;/strong&gt;| |Question Gen|LLaMA 3.2 1B|Fast &amp;amp; relevant| |Answer Gen|Gemma3:1b |Fast, accurate| |Evaluation|llama3.2:3b|Generates numerical scores and evaluations closest to the model average|&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Worst Surprises&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;|| || |&lt;strong&gt;Task&lt;/strong&gt;|&lt;strong&gt;Model&lt;/strong&gt;|&lt;strong&gt;Problem&lt;/strong&gt;| |Question Gen|Qwen3 4B|Took &lt;strong&gt;486s&lt;/strong&gt; to generate 1 question| |Answer Gen|LLaMA 3.1 8B|Slow | |Evaluation|DeepSeek-R1 1.5B|Inconsistent, skipped scores|&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Screenshots Galore&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm adding screenshots of:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Questions generation&lt;/li&gt; &lt;li&gt;Answer comparisons&lt;/li&gt; &lt;li&gt;Evaluation outputs&lt;/li&gt; &lt;li&gt;Token/sec charts (So stay tuned or ask if you want raw data!)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Takeaways&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You &lt;strong&gt;can&lt;/strong&gt; run decent LLMs locally on M1 Air (8GB) ‚Äì if you pick the right ones&lt;/li&gt; &lt;li&gt;Model size ‚â† performance. Bigger isn't always better.&lt;/li&gt; &lt;li&gt;Bias in self-evaluation is &lt;strong&gt;real&lt;/strong&gt; ‚Äì and model behavior varies wildly&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Post questions if you have any, I will try to answer&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/irodov4030"&gt; /u/irodov4030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lktb12"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lktb12/i_tested_10_llms_locally_on_my_macbook_air_m1_8gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lktb12/i_tested_10_llms_locally_on_my_macbook_air_m1_8gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T06:48:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll5h5p</id>
    <title>gemma3n is out</title>
    <updated>2025-06-26T17:02:40+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma 3n models are designed for efficient execution on everyday devices such as laptops, tablets or phones.&lt;/p&gt; &lt;p&gt;Gemma 3n models are designed for efficient execution on everyday devices such as laptops, tablets or phones. These models were trained with data in over 140 spoken languages.&lt;/p&gt; &lt;p&gt;Gemma 3n models use selective parameter activation technology to reduce resource requirements. This technique allows the models to operate at an effective size of 2B and 4B parameters, which is lower than the total number of parameters they contain.&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/gemma3n"&gt;https://ollama.com/library/gemma3n&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Upd: ollama 0.9.3 required&lt;/p&gt; &lt;p&gt;Upd2: official post &lt;a href="https://www.reddit.com/r/LocalLLaMA/s/0nLcE3wzA1"&gt;https://www.reddit.com/r/LocalLLaMA/s/0nLcE3wzA1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ll5h5p/gemma3n_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ll5h5p/gemma3n_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ll5h5p/gemma3n_is_out/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T17:02:40+00:00</published>
  </entry>
</feed>
