<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-01T07:05:18+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1iecleb</id>
    <title>Running DeepSeek R1 on my M4 Pro Mac mini with Ollama</title>
    <updated>2025-01-31T11:21:31+00:00</updated>
    <author>
      <name>/u/ope_poe</name>
      <uri>https://old.reddit.com/user/ope_poe</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ope_poe"&gt; /u/ope_poe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/macmini/comments/1idfuew/running_deepseek_r1_on_my_m4_pro_mac_mini_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iecleb/running_deepseek_r1_on_my_m4_pro_mac_mini_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iecleb/running_deepseek_r1_on_my_m4_pro_mac_mini_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T11:21:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1iedn22</id>
    <title>a 5B model? need some help ( cause of some words, not very nsfw but eh)</title>
    <updated>2025-01-31T12:26:52+00:00</updated>
    <author>
      <name>/u/Ardion63</name>
      <uri>https://old.reddit.com/user/Ardion63</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's ups guys, im first timer here in this reddit page but i need some help&lt;/p&gt; &lt;p&gt;i got a rtx 3060 6 gb vrm , i can use 7B models but the speed is a bit slow, especially longer sentences&lt;br /&gt; do you guys know any good 5B models?&lt;/p&gt; &lt;p&gt;Prefer Uncensored or abliterated ones cause i want to see how much &amp;quot;personality&amp;quot; an AI can get with its text / make it be my AI for the rest of my life, so a little good buddy AI just wont cut but oh well&lt;/p&gt; &lt;p&gt;i tried a bunch 7B models, they are alright but you know vram limited ( the rest of my laptop specs are much better yea) i am also trying the prithivMLmods/Triangulum-5B&lt;/p&gt; &lt;p&gt;but other then that i haven't seen any 5B ( 3B will make the AI impossible to get what i wanted sooo 5B is the sweet spot)&lt;/p&gt; &lt;p&gt;thanks!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ardion63"&gt; /u/Ardion63 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iedn22/a_5b_model_need_some_help_cause_of_some_words_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iedn22/a_5b_model_need_some_help_cause_of_some_words_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iedn22/a_5b_model_need_some_help_cause_of_some_words_not/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T12:26:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie1aui</id>
    <title>Does `ollama create` actually build a new model?</title>
    <updated>2025-01-30T23:50:04+00:00</updated>
    <author>
      <name>/u/homelab2946</name>
      <uri>https://old.reddit.com/user/homelab2946</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded a GGUF file and create a Modelfile to extend it. Then I run `ollama create model_name -f Modelfile`, a `model_name:latest` model is created and shows in `ollama list` 10 GB. The GGUF file is also around the same 10 GB. Does `ollama create` not just add instruction on a base model but actually acting more like `docker build`? Would it then be fine to remove the GGUF file after the build?&lt;/p&gt; &lt;p&gt;Another scenario is through a supported ollama model, like `llama3`. Does Ollama &amp;quot;build&amp;quot; a new image if I create a new Modelfile from `llama3`, so it takes double the storage?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/homelab2946"&gt; /u/homelab2946 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1aui/does_ollama_create_actually_build_a_new_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1aui/does_ollama_create_actually_build_a_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie1aui/does_ollama_create_actually_build_a_new_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T23:50:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iegqef</id>
    <title>AI models basics for newcomers</title>
    <updated>2025-01-31T15:03:14+00:00</updated>
    <author>
      <name>/u/Level_Fennel8071</name>
      <uri>https://old.reddit.com/user/Level_Fennel8071</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there any good place to underatand basic terms and specs for different models,and how it gonna affect the model usecases and hw requirements, terms like parameter size, quantization, model type, model format, context window....etc.&lt;/p&gt; &lt;p&gt;my goal is to run model that helps me studying, mostly summarizing books and docs, being able to chat with the model about some materials is plus&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Level_Fennel8071"&gt; /u/Level_Fennel8071 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegqef/ai_models_basics_for_newcomers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegqef/ai_models_basics_for_newcomers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iegqef/ai_models_basics_for_newcomers/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T15:03:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iejgwp</id>
    <title>Hugging face TAG for models supporting "tools" mode?</title>
    <updated>2025-01-31T17:01:19+00:00</updated>
    <author>
      <name>/u/HeadGr</name>
      <uri>https://old.reddit.com/user/HeadGr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm testing models for AI mysql agent, and can't find appropriate tag on HuggingFace models lisl to pick ones capable. Please, point me to correct one(s)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HeadGr"&gt; /u/HeadGr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iejgwp/hugging_face_tag_for_models_supporting_tools_mode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iejgwp/hugging_face_tag_for_models_supporting_tools_mode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iejgwp/hugging_face_tag_for_models_supporting_tools_mode/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T17:01:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1iemeqp</id>
    <title>Which deepseek model to install locally?</title>
    <updated>2025-01-31T19:02:39+00:00</updated>
    <author>
      <name>/u/_weshall</name>
      <uri>https://old.reddit.com/user/_weshall</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using an M1 Macbook Air with 16GB of memory. I want to run a distilled deepseek model locally. What is the best model I can download that will be able to run on this machine without completely choking up the system?&lt;/p&gt; &lt;p&gt;From the limited information I was able to get on the internet the 14b model &lt;em&gt;could&lt;/em&gt; be the one I want but it seems like it performs worse than the 32b model on coding tasks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_weshall"&gt; /u/_weshall &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iemeqp/which_deepseek_model_to_install_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iemeqp/which_deepseek_model_to_install_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iemeqp/which_deepseek_model_to_install_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T19:02:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1iedpso</id>
    <title>Is anyone else's Deepseek a bit chatty?</title>
    <updated>2025-01-31T12:31:06+00:00</updated>
    <author>
      <name>/u/Jimmy_drumstix</name>
      <uri>https://old.reddit.com/user/Jimmy_drumstix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've installed deepseek-r1:8b on my computer using ollama. I am asking it to provide shorter answers but it has verbal diarrhoea. Has anyone else experienced this?&lt;/p&gt; &lt;p&gt;Here's from the last two prompts I gave it:&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jimmy_drumstix"&gt; /u/Jimmy_drumstix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iedpso/is_anyone_elses_deepseek_a_bit_chatty/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iedpso/is_anyone_elses_deepseek_a_bit_chatty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iedpso/is_anyone_elses_deepseek_a_bit_chatty/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T12:31:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieeqcf</id>
    <title>What's going on with my download? keeps going backwards after it makes some progress?</title>
    <updated>2025-01-31T13:26:43+00:00</updated>
    <author>
      <name>/u/dusty_whale</name>
      <uri>https://old.reddit.com/user/dusty_whale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ieeqcf/whats_going_on_with_my_download_keeps_going/"&gt; &lt;img alt="What's going on with my download? keeps going backwards after it makes some progress?" src="https://external-preview.redd.it/MTljMHJtN2h6YmdlMfBoFUu8tfPjIjApOHTrjSrxAuMuJVPtuHQoaGzrhN1h.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5315b7f329164fa7d7289dc1353237595c527220" title="What's going on with my download? keeps going backwards after it makes some progress?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dusty_whale"&gt; /u/dusty_whale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rfoy8p7hzbge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieeqcf/whats_going_on_with_my_download_keeps_going/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ieeqcf/whats_going_on_with_my_download_keeps_going/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T13:26:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieqn97</id>
    <title>Which model to run petroleum engineering calculation</title>
    <updated>2025-01-31T22:02:02+00:00</updated>
    <author>
      <name>/u/Papema3</name>
      <uri>https://old.reddit.com/user/Papema3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I'm trying to solve petroleum engineering simple calculations using some models like mistral, llama and deepseek.&lt;/p&gt; &lt;p&gt;Mistral performs terrible, llama seems to heavy on my rtx quadro 5000, taking up to 2 minutes per answer and deepseek r1 I can only run the 1.5b version.&lt;/p&gt; &lt;p&gt;Is there a model that u recommend focus on petroleum engineering calculation or physics calculation in general?&lt;/p&gt; &lt;p&gt;I have tried one named wizard something, didn't work pretty well&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Papema3"&gt; /u/Papema3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieqn97/which_model_to_run_petroleum_engineering/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieqn97/which_model_to_run_petroleum_engineering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ieqn97/which_model_to_run_petroleum_engineering/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T22:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1idqxto</id>
    <title>Why Are All Local AI Models So Bad? No One Talks About This!</title>
    <updated>2025-01-30T16:31:59+00:00</updated>
    <author>
      <name>/u/NikkEvan</name>
      <uri>https://old.reddit.com/user/NikkEvan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with local AI models, even &amp;quot;high-end ones&amp;quot; like the recent DeepSeek-R1 32B, using Open WebUI.&lt;br /&gt; I expected them to be weaker than online models, but the gap is just ridiculous.&lt;br /&gt; Even for the simplest questions, they either fail, give nonsense answers, or completely misunderstand the input.&lt;/p&gt; &lt;p&gt;I’ve set the parameters and all the settings at the best i could, tried different setups, system prompts, and still , even after parsing a basic document just a few pages long, is a struggle.&lt;br /&gt; If it already fails here, how am I supposed to use it for hundreds of internal company documents?&lt;/p&gt; &lt;p&gt;The crazy part? No one talks about this!&lt;br /&gt; Instead, i see every video in youtube saying :&lt;br /&gt; &amp;quot;How to run locally (modelname) much better than chat-gpt&amp;quot;&lt;br /&gt; &amp;quot;Local Deepseek beats Chat-gpt&amp;quot;&lt;br /&gt; Than the question they ask to those local models are : How many 'R' are in the word Strawberry and the model answer: 2 ... lol &lt;/p&gt; &lt;p&gt;Why is the performance so bad, even on 32B models?&lt;/p&gt; &lt;p&gt;Why are there no proper guides to get the best out of local AI?&lt;br /&gt; Having a big hardware such as the Nvidia project DIGITS will make a big model work close to the online Chat-gpt 3 or 4 ? I see those has 175b parameters. &lt;/p&gt; &lt;p&gt;What are we missing?&lt;/p&gt; &lt;p&gt;I really want to make local AI work as close as the online models, even buying bigger and stronger hardware, but, right now, it just feels like a waste of time.&lt;br /&gt; Has anyone actually succeeded in making these models work well? If so, how? And , what do you intend for Working Well for a local Model ? &lt;/p&gt; &lt;p&gt;Let’s discuss this! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NikkEvan"&gt; /u/NikkEvan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T16:31:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iem982</id>
    <title>RAG and Perplexity like Search</title>
    <updated>2025-01-31T18:56:29+00:00</updated>
    <author>
      <name>/u/atomique90</name>
      <uri>https://old.reddit.com/user/atomique90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wonder which stack you guys use to solve the following tasks: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;RAG (Pipeline): How do you guys get your data chunked and stored into a vectorized database? Especially what do you do to upsert or update the data? I am interested in solutions that are local only and dont rely on a cloud service. &lt;/li&gt; &lt;li&gt;How do you realize perplexity like chat search with ollama? I tried perplexica but especially with k8s I had many problems to kickstart it. I would love to have it in an UI like open webui. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I already tried to play a bit with ollama, perplexica, open webui, flowise, n8n, qdrant and psql (chat memory). &lt;/p&gt; &lt;p&gt;I am just curious what would be the best way to solve this and improve my journey! &lt;/p&gt; &lt;p&gt;Thanks a lot&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atomique90"&gt; /u/atomique90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iem982/rag_and_perplexity_like_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iem982/rag_and_perplexity_like_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iem982/rag_and_perplexity_like_search/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T18:56:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ievb8g</id>
    <title>Running ollama with Llama3.2:3b on a Raspberry Pi 5 8 Gig - runs, but "unimportable"?</title>
    <updated>2025-02-01T01:38:56+00:00</updated>
    <author>
      <name>/u/DelosBoard2052</name>
      <uri>https://old.reddit.com/user/DelosBoard2052</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've got Llama3.2:3b running well on a Raspberry Pi, and I'm running Python version 3.11.2. I have a small script to capture the output from Llama, and the first line of the script is &amp;quot;import ollama&amp;quot;. I get the old ModuleNotFoundError.&lt;/p&gt; &lt;p&gt;I installed ollama via curl.&lt;/p&gt; &lt;p&gt;I am running a virtual environment, and installed ollama in that environment.&lt;/p&gt; &lt;p&gt;When I try, in either the VE or the Non-VE windows, to find where ollama is installed, via either &amp;quot;pip show ollama&amp;quot; or the string &amp;quot;python -c 'import ollama; print(ollama.__file__)'&amp;quot;, it says &amp;quot;no module named ollama&amp;quot;.&lt;/p&gt; &lt;p&gt;Where does ollama install? It's not in my /usr/lib/python3.11/site-packages directory in either VE or Non-VE windows. And yet it runs perfectly. &lt;/p&gt; &lt;p&gt;I feel dumb. Guidance greatly appreciated here, thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DelosBoard2052"&gt; /u/DelosBoard2052 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ievb8g/running_ollama_with_llama323b_on_a_raspberry_pi_5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ievb8g/running_ollama_with_llama323b_on_a_raspberry_pi_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ievb8g/running_ollama_with_llama323b_on_a_raspberry_pi_5/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T01:38:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iegbea</id>
    <title>Why is open-webui this size?</title>
    <updated>2025-01-31T14:43:53+00:00</updated>
    <author>
      <name>/u/DevuDixit</name>
      <uri>https://old.reddit.com/user/DevuDixit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iegbea/why_is_openwebui_this_size/"&gt; &lt;img alt="Why is open-webui this size?" src="https://external-preview.redd.it/Erly_C0iblWMXf_1Jomyy7GuVEMgel-rmrf80xwRk9I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa064b742fd7f951e2c00ee5d40558b26fb7d7d2" title="Why is open-webui this size?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/87a620piccge1.png?width=1102&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=824a6c45bcd69a837b8fe314f55546e743df3eee"&gt;https://preview.redd.it/87a620piccge1.png?width=1102&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=824a6c45bcd69a837b8fe314f55546e743df3eee&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/id1h4wgudcge1.png?width=1089&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f88f10529c414c7eaa7e3b463f062079af32aeb1"&gt;https://preview.redd.it/id1h4wgudcge1.png?width=1089&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f88f10529c414c7eaa7e3b463f062079af32aeb1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I was just trying to install and use the &lt;a href="https://github.com/open-webui/open-webui"&gt;open-webui&lt;/a&gt; for using the gui and I was curious about these files (marked by red) it is pulling. I have used some local web based guis in the past and all of them were under 100 MBs of size. So why is this comparatively bulky ? ( just curious ; )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DevuDixit"&gt; /u/DevuDixit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegbea/why_is_openwebui_this_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegbea/why_is_openwebui_this_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iegbea/why_is_openwebui_this_size/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T14:43:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1iegog2</id>
    <title>Would x2 RTX 3060 12GB suffice advanced models like DeepSeek R1 14B or higher?</title>
    <updated>2025-01-31T15:00:54+00:00</updated>
    <author>
      <name>/u/GamerGuy95953</name>
      <uri>https://old.reddit.com/user/GamerGuy95953</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am determining if I should buy 2 RTX 3060 12GB (around $270 US each.) wondering if it’s even worth it over other options. Based on my research AI models perform better with lots of VRAM over clock speeds. &lt;/p&gt; &lt;p&gt;My server PC setup currently is a GTX 1050 and a Ryzen 7 2700X. I am planning to also upgrade the CPU to R9 something. It depending on the price for the R9 models. &lt;/p&gt; &lt;p&gt;Any suggestions are appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GamerGuy95953"&gt; /u/GamerGuy95953 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegog2/would_x2_rtx_3060_12gb_suffice_advanced_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegog2/would_x2_rtx_3060_12gb_suffice_advanced_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iegog2/would_x2_rtx_3060_12gb_suffice_advanced_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T15:00:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1iet8lx</id>
    <title>Place Olama model on external drive</title>
    <updated>2025-01-31T23:58:07+00:00</updated>
    <author>
      <name>/u/jaserjsk</name>
      <uri>https://old.reddit.com/user/jaserjsk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it possible to download a model from Olama and place it on external harddrive?&lt;br /&gt; I'm searhing for a way, but I cannot figure out how!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jaserjsk"&gt; /u/jaserjsk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iet8lx/place_olama_model_on_external_drive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iet8lx/place_olama_model_on_external_drive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iet8lx/place_olama_model_on_external_drive/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T23:58:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1iewe66</id>
    <title>Ollama for Knowledge base</title>
    <updated>2025-02-01T02:35:52+00:00</updated>
    <author>
      <name>/u/Videodad</name>
      <uri>https://old.reddit.com/user/Videodad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spun up Ollama on a docker and was goofing with it, having it read some mark down files to see what it could do. Got me thinking, maybe I could use it for a Knowledge Base for the helpdesk, but if I do this, do I need it to do anything but use my files to give output, and can I present images as part of this. &lt;/p&gt; &lt;p&gt;So, question is, has anyone else spun up Ollama with just the data that would be needed for something like this. If so, what did you do, what did you learn, and what pitfalls can you share to avoid?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Videodad"&gt; /u/Videodad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iewe66/ollama_for_knowledge_base/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iewe66/ollama_for_knowledge_base/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iewe66/ollama_for_knowledge_base/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T02:35:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie210z</id>
    <title>Got Deepseek R1 1.5b running locally on Pixel 8 pro</title>
    <updated>2025-01-31T00:24:19+00:00</updated>
    <author>
      <name>/u/Teradyyne</name>
      <uri>https://old.reddit.com/user/Teradyyne</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"&gt; &lt;img alt="Got Deepseek R1 1.5b running locally on Pixel 8 pro" src="https://external-preview.redd.it/cTk0emwwZ3ozOGdlMYbCJM1MQLfOpw8fF1FnxkqAh7visCMP7lFjyVXppg7i.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=263215164e10efcd203b94bae4672c47e11c63d8" title="Got Deepseek R1 1.5b running locally on Pixel 8 pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Teradyyne"&gt; /u/Teradyyne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cha0fplz38ge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T00:24:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1iewzve</id>
    <title>I'm having trouble getting my local Chrome extension to work with the Ollama local on ubuntu. Need assistance</title>
    <updated>2025-02-01T03:08:25+00:00</updated>
    <author>
      <name>/u/hasan_py</name>
      <uri>https://old.reddit.com/user/hasan_py</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using ubuntu 24. In my chrome extension I'm trying to call this two api. But always giving me CORS error. Tried out to add OLLAMA_HOST=&amp;quot;0.0.0.0&amp;quot; OLLAMA_ORIGINS=&amp;quot;chrome-extension://*&amp;quot; in my bashrc and also zshrc but not worked. Anyone can help me?? &lt;/p&gt; &lt;pre&gt;&lt;code&gt;http://127.0.0.1:11434/api/chat http://127.0.0.1:11434/api/generate &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hasan_py"&gt; /u/hasan_py &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iewzve/im_having_trouble_getting_my_local_chrome/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iewzve/im_having_trouble_getting_my_local_chrome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iewzve/im_having_trouble_getting_my_local_chrome/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T03:08:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ietdh4</id>
    <title>How to train your dragon? (dragon = deepseek R1 on Ollama)</title>
    <updated>2025-02-01T00:04:04+00:00</updated>
    <author>
      <name>/u/Flying_Motorbike</name>
      <uri>https://old.reddit.com/user/Flying_Motorbike</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m a complete beginner here. I have a large collection of PDFs (around 400) that I want the AI to read. Once the ‘learning’ phase is complete, I want it to be able to answer questions and provide references to the PDFs where it found the information. I’d like this process to be repeated without the need for ‘relearning’. I’ve searched through the existing posts, but I could only find information on RAG, which I’m not sure can be used for the read-once-use-later approach that I’m looking for. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flying_Motorbike"&gt; /u/Flying_Motorbike &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ietdh4/how_to_train_your_dragon_dragon_deepseek_r1_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ietdh4/how_to_train_your_dragon_dragon_deepseek_r1_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ietdh4/how_to_train_your_dragon_dragon_deepseek_r1_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T00:04:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iey7g7</id>
    <title>WSL2 says not enough memory but Nivida control panel says plenty.</title>
    <updated>2025-02-01T04:15:43+00:00</updated>
    <author>
      <name>/u/azimuth79b</name>
      <uri>https://old.reddit.com/user/azimuth79b</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iey7g7/wsl2_says_not_enough_memory_but_nivida_control/"&gt; &lt;img alt="WSL2 says not enough memory but Nivida control panel says plenty." src="https://preview.redd.it/3gazoiw6egge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba94ac15ddf35d2b7dd393095dc8ec04ff82ed62" title="WSL2 says not enough memory but Nivida control panel says plenty." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I should be able to use llama3.3 given I have 81.5 GB VRAM&lt;/p&gt; &lt;p&gt;How to fix please?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/azimuth79b"&gt; /u/azimuth79b &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3gazoiw6egge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iey7g7/wsl2_says_not_enough_memory_but_nivida_control/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iey7g7/wsl2_says_not_enough_memory_but_nivida_control/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T04:15:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1iewdmc</id>
    <title>Why is the ollama file size smaller than the models on hugging face?</title>
    <updated>2025-02-01T02:35:04+00:00</updated>
    <author>
      <name>/u/Any_Dot769</name>
      <uri>https://old.reddit.com/user/Any_Dot769</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I'm downloading the R1 32GB model from Ollama and it looks like the total size in GB is much smaller (20GB) than the one on hugging face (~65GB). How is this possible? Am I misunderstanding the GB figure on ollama? Is it a guide on how much VRAM is needed for the model? I'm new to Ollama so not sure how it works, any advice is much appreciated! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Dot769"&gt; /u/Any_Dot769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iewdmc/why_is_the_ollama_file_size_smaller_than_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iewdmc/why_is_the_ollama_file_size_smaller_than_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iewdmc/why_is_the_ollama_file_size_smaller_than_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T02:35:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iem5it</id>
    <title>AI Tools as a Hiring Requirement? Just Saw a Job Post That Blew My Mind</title>
    <updated>2025-01-31T18:52:03+00:00</updated>
    <author>
      <name>/u/Far_Flamingo5333</name>
      <uri>https://old.reddit.com/user/Far_Flamingo5333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just came across a job listing that explicitly requires experience with Cursor and Windsurf as part of the stack. Not “nice to have” it’s actually listed as a preference for hiring.&lt;/p&gt; &lt;p&gt;The post reads:&lt;/p&gt; &lt;p&gt;“We’re hiring our first engineer(s)!&lt;/p&gt; &lt;p&gt;👾 Prefer AI-native (you use Cursor, Windsurf, or built your own setup)&lt;/p&gt; &lt;p&gt;👾 Can showcase past projects/work&lt;/p&gt; &lt;p&gt;👾 Based in the Bay Area &amp;amp; down for 3 days/week in-person&lt;/p&gt; &lt;p&gt;…we don’t technically have an office yet, so you can help us decide where to go”&lt;/p&gt; &lt;p&gt;I’m honestly amazed and astonished. This is the first time I’ve seen AI coding tools being treated as a must-have skill rather than just a productivity boost. It makes me wonder:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Are we at the point where AI-assisted coding is a hard requirement for top tech jobs&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Will future engineers be judged not just on their raw coding ability but how well they integrate AI into their workflow&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;How long until AI-native workflows become the default expectation everywhere?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Would love to hear thoughts from others, are you seeing this trend in hiring, or is this just an early sign of what’s to come?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far_Flamingo5333"&gt; /u/Far_Flamingo5333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iem5it/ai_tools_as_a_hiring_requirement_just_saw_a_job/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iem5it/ai_tools_as_a_hiring_requirement_just_saw_a_job/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iem5it/ai_tools_as_a_hiring_requirement_just_saw_a_job/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T18:52:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iezx7o</id>
    <title>Ollama Model Benchmark Tool for Development</title>
    <updated>2025-02-01T06:01:18+00:00</updated>
    <author>
      <name>/u/binoy_manoj</name>
      <uri>https://old.reddit.com/user/binoy_manoj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iezx7o/ollama_model_benchmark_tool_for_development/"&gt; &lt;img alt="Ollama Model Benchmark Tool for Development" src="https://external-preview.redd.it/oBe7rH-Bc5AYJcp-6yFnxIyDF9OJYJwLuLN_Ozvb8uM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=295cac930833f6c08e012f4e6a8d0bbe5b53c6c0" title="Ollama Model Benchmark Tool for Development" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey community! 👋&lt;/p&gt; &lt;p&gt;I just released an open-source tool that helps developers benchmark different Ollama models specifically for development tasks. If you're using Ollama for coding assistance, this might be useful for finding the best model for your needs.&lt;/p&gt; &lt;h1&gt;What it does:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Runs your installed models through real-world coding scenarios&lt;/li&gt; &lt;li&gt;Measures response times and generates detailed performance reports&lt;/li&gt; &lt;li&gt;Tests things like component creation, API routes, and data fetching patterns&lt;/li&gt; &lt;li&gt;Runs in an isolated environment (so it's easy to test and remove)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Sample output:&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/azcz96flwgge1.png?width=1921&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30ff786ca58617260d7c65f2518a27ce01c66045"&gt;https://preview.redd.it/azcz96flwgge1.png?width=1921&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30ff786ca58617260d7c65f2518a27ce01c66045&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Why I built this:&lt;/h1&gt; &lt;p&gt;I was running multiple models locally and wanted a systematic way to compare their:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Speed (response times)&lt;/li&gt; &lt;li&gt;Code quality&lt;/li&gt; &lt;li&gt;Understanding of modern development framework&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;github: &lt;a href="https://github.com/binoymanoj/ollama-benchmark/"&gt;https://github.com/binoymanoj/ollama-benchmark/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Post your results here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/binoy_manoj"&gt; /u/binoy_manoj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iezx7o/ollama_model_benchmark_tool_for_development/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iezx7o/ollama_model_benchmark_tool_for_development/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iezx7o/ollama_model_benchmark_tool_for_development/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T06:01:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1iey40e</id>
    <title>What is the best way to setup a local LLM to use a local PDF</title>
    <updated>2025-02-01T04:10:09+00:00</updated>
    <author>
      <name>/u/Outrageous-Win-3244</name>
      <uri>https://old.reddit.com/user/Outrageous-Win-3244</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How would you create a local AI chatbot that uses one or more local PDFs as information source? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous-Win-3244"&gt; /u/Outrageous-Win-3244 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iey40e/what_is_the_best_way_to_setup_a_local_llm_to_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iey40e/what_is_the_best_way_to_setup_a_local_llm_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iey40e/what_is_the_best_way_to_setup_a_local_llm_to_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T04:10:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ietwx3</id>
    <title>Built my own discord bot using ollama and uh...</title>
    <updated>2025-02-01T00:29:41+00:00</updated>
    <author>
      <name>/u/GlitchPhoenix98</name>
      <uri>https://old.reddit.com/user/GlitchPhoenix98</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ietwx3/built_my_own_discord_bot_using_ollama_and_uh/"&gt; &lt;img alt="Built my own discord bot using ollama and uh..." src="https://preview.redd.it/xokxk8pu9fge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=902c2a44c4ab57ad1a35490d4766d3c979bf1edf" title="Built my own discord bot using ollama and uh..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GlitchPhoenix98"&gt; /u/GlitchPhoenix98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xokxk8pu9fge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ietwx3/built_my_own_discord_bot_using_ollama_and_uh/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ietwx3/built_my_own_discord_bot_using_ollama_and_uh/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T00:29:41+00:00</published>
  </entry>
</feed>
