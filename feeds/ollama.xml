<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-26T15:22:46+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1i8qav7</id>
    <title>A list of all the top Open Source Chat UI for ollama/any LLM in general. (community edition)</title>
    <updated>2025-01-24T07:43:58+00:00</updated>
    <author>
      <name>/u/VisibleLawfulness246</name>
      <uri>https://old.reddit.com/user/VisibleLawfulness246</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i8qav7/a_list_of_all_the_top_open_source_chat_ui_for/"&gt; &lt;img alt="A list of all the top Open Source Chat UI for ollama/any LLM in general. (community edition)" src="https://b.thumbs.redditmedia.com/2mlU8Ofw8DVHHHmMnu33kSfWqc1LdBv2Whegj1zwhkY.jpg" title="A list of all the top Open Source Chat UI for ollama/any LLM in general. (community edition)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's my list right now&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open Web UI&lt;/li&gt; &lt;li&gt;LibreChat&lt;/li&gt; &lt;li&gt;anythingLLM&lt;/li&gt; &lt;li&gt;GPT4all&lt;/li&gt; &lt;li&gt;oobabooga&lt;/li&gt; &lt;li&gt;verba&lt;/li&gt; &lt;li&gt;dify&lt;/li&gt; &lt;li&gt;SillyTavern&lt;/li&gt; &lt;li&gt;Danswer&lt;/li&gt; &lt;li&gt;Lobe Ui&lt;/li&gt; &lt;li&gt;hugging face chat-Ui&lt;/li&gt; &lt;li&gt;kobold Cpp/ for from llama cpp&lt;/li&gt; &lt;li&gt;private gpt&lt;/li&gt; &lt;li&gt;serge chat&lt;/li&gt; &lt;li&gt;JanHQ&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What am I missing from this list?&lt;/p&gt; &lt;p&gt;adding this image from my research&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VisibleLawfulness246"&gt; /u/VisibleLawfulness246 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8qav7/a_list_of_all_the_top_open_source_chat_ui_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8qav7/a_list_of_all_the_top_open_source_chat_ui_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8qav7/a_list_of_all_the_top_open_source_chat_ui_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T07:43:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9hs4z</id>
    <title>Book Translation using ollama</title>
    <updated>2025-01-25T07:27:10+00:00</updated>
    <author>
      <name>/u/Hefty_Cup_8160</name>
      <uri>https://old.reddit.com/user/Hefty_Cup_8160</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have ollama and openwebui installed on my PC and I'm trying to figure out how to use ollama model to translate a book of epub or pdf. I know that I can use some ocr or pandoc to convert those books into markdown first, but what should i do next? it doesn't seem like there's a feature in webui that allows me to translate a very long paragraph (the whole book). Is there a tool like Open WebUI that allows me to do that? Thanks in advance for any tips&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hefty_Cup_8160"&gt; /u/Hefty_Cup_8160 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9hs4z/book_translation_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9hs4z/book_translation_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9hs4z/book_translation_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T07:27:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9zyir</id>
    <title>How to remove deepseek models</title>
    <updated>2025-01-25T23:18:37+00:00</updated>
    <author>
      <name>/u/mallorcastro</name>
      <uri>https://old.reddit.com/user/mallorcastro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The title is quite self-explanatory. How can I remove models of deepseek? Does the rm command work to also retrieve disk space?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mallorcastro"&gt; /u/mallorcastro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9zyir/how_to_remove_deepseek_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9zyir/how_to_remove_deepseek_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9zyir/how_to_remove_deepseek_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T23:18:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9hh2l</id>
    <title>I run ollama on window cmds Is there any easy to use webui?</title>
    <updated>2025-01-25T07:04:31+00:00</updated>
    <author>
      <name>/u/labdogeth</name>
      <uri>https://old.reddit.com/user/labdogeth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hate cmd. Is there any easy to use webui?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/labdogeth"&gt; /u/labdogeth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9hh2l/i_run_ollama_on_window_cmds_is_there_any_easy_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9hh2l/i_run_ollama_on_window_cmds_is_there_any_easy_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9hh2l/i_run_ollama_on_window_cmds_is_there_any_easy_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T07:04:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i97odo</id>
    <title>DataBridge: Local, Modular, fully open source RAG System (Now easier than ever to get started!)</title>
    <updated>2025-01-24T22:18:31+00:00</updated>
    <author>
      <name>/u/yes-no-maybe_idk</name>
      <uri>https://old.reddit.com/user/yes-no-maybe_idk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I'm back with an exciting update for &lt;strong&gt;DataBridge&lt;/strong&gt;, the open-source, fully local, multimodal RAG system you've been supporting so generously. Thanks to your amazing feedback, we've made significant improvements, especially around &lt;strong&gt;Docker support&lt;/strong&gt; to make getting started easier than ever!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What‚Äôs New?&lt;/strong&gt;&lt;br /&gt; üì¶ &lt;strong&gt;Docker Support&lt;/strong&gt; ‚Äì The most requested feature is here! Now, you can spin up DataBridge effortlessly.&lt;br /&gt; ‚ö° &lt;strong&gt;CAG (Cache Augmented Generation)&lt;/strong&gt; ‚Äì Coming very, very soon to boost efficiency (you can explore the CAG branch to try it out today).&lt;br /&gt; üåê &lt;strong&gt;Graph RAG&lt;/strong&gt; ‚Äì On the way, stay tuned for exciting updates!&lt;br /&gt; üìä &lt;strong&gt;Evaluations and Comparisons&lt;/strong&gt; ‚Äì We‚Äôre working on adding benchmarking to help you compare various setups.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;New Video:&lt;/strong&gt;&lt;br /&gt; I‚Äôve put together a detailed walkthrough that covers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Installation &amp;amp; Setup&lt;/strong&gt; ‚Äì Whether you're using Docker or manual installation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Basic Ingestion &amp;amp; Querying&lt;/strong&gt; ‚Äì Learn how to quickly bring your data into DataBridge.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Shell &amp;amp; UI Demo&lt;/strong&gt; ‚Äì See DataBridge in action with both CLI and UI components.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Component Swapping&lt;/strong&gt; ‚Äì Easily switch completion models (e.g., from LLaMA to OpenAI).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üëâ &lt;a href="https://www.youtube.com/watch?v=__Kpt7tVQ6k&amp;amp;t=7s"&gt;&lt;strong&gt;Watch the video here&lt;/strong&gt;&lt;/a&gt; üëà&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Looking for:&lt;/strong&gt;&lt;br /&gt; - Your thoughts and feedback&lt;br /&gt; - Feature requests and use cases&lt;br /&gt; - Bug reports&lt;br /&gt; - Contributors to join the journey&lt;/p&gt; &lt;p&gt;A huge thanks to the community for your continued support and enthusiasm. Your feedback has been invaluable in shaping DataBridge.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;br /&gt; üîó GitHub: &lt;a href="https://github.com/databridge-org/databridge-core"&gt;https://github.com/databridge-org/databridge-core&lt;/a&gt;&lt;br /&gt; üìñ Docs: &lt;a href="https://databridge.gitbook.io/databridge-docs"&gt;https://databridge.gitbook.io/databridge-docs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PS: I used DataBridge and gpt4 to help me structure this post.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yes-no-maybe_idk"&gt; /u/yes-no-maybe_idk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i97odo/databridge_local_modular_fully_open_source_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i97odo/databridge_local_modular_fully_open_source_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i97odo/databridge_local_modular_fully_open_source_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T22:18:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9q1xv</id>
    <title>Best local ai setup for writing?</title>
    <updated>2025-01-25T16:01:24+00:00</updated>
    <author>
      <name>/u/Game-Lover44</name>
      <uri>https://old.reddit.com/user/Game-Lover44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to try creating things with ai and ideas and writing would be helpful but what tools and interfaces can i use locally with ease? It would also be nice to automatic the process or include stable diffusion too.&lt;/p&gt; &lt;p&gt;are there any ai tools that might help with writing? Im not even sure if writing and brainstorming with ai is a good idea.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Game-Lover44"&gt; /u/Game-Lover44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9q1xv/best_local_ai_setup_for_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9q1xv/best_local_ai_setup_for_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9q1xv/best_local_ai_setup_for_writing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T16:01:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9mxui</id>
    <title>AnythingLLM or LM Studio for a beginner?</title>
    <updated>2025-01-25T13:28:57+00:00</updated>
    <author>
      <name>/u/321headbang</name>
      <uri>https://old.reddit.com/user/321headbang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve tried both but couldn‚Äôt get A-LLM to work. It asks me to ‚Äúsearch providers‚Äù and lists Ollama but when I click on it, it just says something about not discovering the provider endpoint. &lt;/p&gt; &lt;p&gt;In LM studio, I was able to download a couple that ran on my notebook, but when I tried to download a small image generator to run, it said it wasn‚Äôt a valid file extension (??)&lt;/p&gt; &lt;p&gt;Before I spend too much time getting used to one or the other, which do you recommend for beginners who just want to run whatever models work on my machine and play around with them for basic experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/321headbang"&gt; /u/321headbang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9mxui/anythingllm_or_lm_studio_for_a_beginner/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9mxui/anythingllm_or_lm_studio_for_a_beginner/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9mxui/anythingllm_or_lm_studio_for_a_beginner/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T13:28:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i94pn4</id>
    <title>How I fixed R1 from being a whiney bitch</title>
    <updated>2025-01-24T20:10:37+00:00</updated>
    <author>
      <name>/u/redonculous</name>
      <uri>https://old.reddit.com/user/redonculous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you find R1's thoughts are whiney and lacking self confidence?&lt;br /&gt; Do you find it wasting tokens second guessing itself? &lt;/p&gt; &lt;p&gt;Simply add this to the end of your prompt for much more concise and confident output.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;You are very knowledgeable. An expert. Think and respond with confidence. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In my testing it really works! I'd be happy to hear how it responds for you guys too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redonculous"&gt; /u/redonculous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i94pn4/how_i_fixed_r1_from_being_a_whiney_bitch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i94pn4/how_i_fixed_r1_from_being_a_whiney_bitch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i94pn4/how_i_fixed_r1_from_being_a_whiney_bitch/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T20:10:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9qnvn</id>
    <title>How to use deepseek r1 alongside a game engine?</title>
    <updated>2025-01-25T16:28:10+00:00</updated>
    <author>
      <name>/u/Game-Lover44</name>
      <uri>https://old.reddit.com/user/Game-Lover44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How might one use r1 too makes games with a game engine. i tried godot with the 7b model but it dosent work and makes up its own programming lauguage. so what game engine can i use with deepseek r1? im looking for a free solution that has a interface.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Game-Lover44"&gt; /u/Game-Lover44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9qnvn/how_to_use_deepseek_r1_alongside_a_game_engine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9qnvn/how_to_use_deepseek_r1_alongside_a_game_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9qnvn/how_to_use_deepseek_r1_alongside_a_game_engine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T16:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9ypx9</id>
    <title>403 Forbidden Error when trying to trigger localhost API endpoint in Chrome extension</title>
    <updated>2025-01-25T22:21:34+00:00</updated>
    <author>
      <name>/u/24Gameplay_</name>
      <uri>https://old.reddit.com/user/24Gameplay_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey fellow devs,&lt;/p&gt; &lt;p&gt;I'm building a Chrome extension for work and I'm running into an issue. I'm trying to trigger a local API endpoint at http://localhost:11434/api/generate, but I'm getting a 403 Forbidden Error. I've checked that the model is running locally and I have the latest version of LLaMA installed.&lt;/p&gt; &lt;p&gt;Checked: http://localhost:11434 Message: Ollama is running &lt;/p&gt; &lt;p&gt;I've also tried accessing the endpoint through a web browser, but I'm still getting the same error. Has anyone else encountered this issue? Any ideas on how to fix it?&lt;/p&gt; &lt;p&gt;Thanks in advance for any help!&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/24Gameplay_"&gt; /u/24Gameplay_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9ypx9/403_forbidden_error_when_trying_to_trigger/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9ypx9/403_forbidden_error_when_trying_to_trigger/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9ypx9/403_forbidden_error_when_trying_to_trigger/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T22:21:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9zz8u</id>
    <title>8x AMD Instinct Mi60 Server + vLLM + DeepSeek-R1-Qwen-14B-FP16</title>
    <updated>2025-01-25T23:19:35+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jxwbmm4l38fe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9zz8u/8x_amd_instinct_mi60_server_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9zz8u/8x_amd_instinct_mi60_server_vllm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T23:19:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia71y4</id>
    <title>Deepseek_r1 considers ‚Äúthink‚Äù instead of ‚Äúthiink‚Äù. Will the quantization and training parameters affect the performance? I am using 1.5b ; q4_k_m</title>
    <updated>2025-01-26T05:41:06+00:00</updated>
    <author>
      <name>/u/themoneylust</name>
      <uri>https://old.reddit.com/user/themoneylust</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ia71y4/deepseek_r1_considers_think_instead_of_thiink/"&gt; &lt;img alt="Deepseek_r1 considers ‚Äúthink‚Äù instead of ‚Äúthiink‚Äù. Will the quantization and training parameters affect the performance? I am using 1.5b ; q4_k_m" src="https://preview.redd.it/heirhacyz9fe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef07b08bdb610da3b5499d2d699942ba0782c3eb" title="Deepseek_r1 considers ‚Äúthink‚Äù instead of ‚Äúthiink‚Äù. Will the quantization and training parameters affect the performance? I am using 1.5b ; q4_k_m" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/themoneylust"&gt; /u/themoneylust &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/heirhacyz9fe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ia71y4/deepseek_r1_considers_think_instead_of_thiink/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ia71y4/deepseek_r1_considers_think_instead_of_thiink/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T05:41:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia5sl7</id>
    <title>How to put image as input to deepseep r1?</title>
    <updated>2025-01-26T04:24:21+00:00</updated>
    <author>
      <name>/u/Spark0411</name>
      <uri>https://old.reddit.com/user/Spark0411</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have PC which does not have internet but I need to extract texts from the images. Is it possible to give image as input any llm model (deepseep r1 or etc) ? If you know fully offline OCR application, then that may also work but I will prefer llm model as it can be used for multi purpose &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spark0411"&gt; /u/Spark0411 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ia5sl7/how_to_put_image_as_input_to_deepseep_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ia5sl7/how_to_put_image_as_input_to_deepseep_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ia5sl7/how_to_put_image_as_input_to_deepseep_r1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T04:24:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia6tkw</id>
    <title>I connected llama3.2 to an IRC server. It was hilarious. Tell me what conversation I should make it have with itself. I have gotten 3 clients plus myself working at once.</title>
    <updated>2025-01-26T05:26:23+00:00</updated>
    <author>
      <name>/u/malformed-packet</name>
      <uri>https://old.reddit.com/user/malformed-packet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ia6tkw/i_connected_llama32_to_an_irc_server_it_was/"&gt; &lt;img alt="I connected llama3.2 to an IRC server. It was hilarious. Tell me what conversation I should make it have with itself. I have gotten 3 clients plus myself working at once." src="https://external-preview.redd.it/H4Bh2eY4U0J1DjNkHHXDisrlI59Jd9-BDZ41Xgi_oSU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6f88627b5a91ac681f815ef765b2409f9671098" title="I connected llama3.2 to an IRC server. It was hilarious. Tell me what conversation I should make it have with itself. I have gotten 3 clients plus myself working at once." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/malformed-packet"&gt; /u/malformed-packet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/longjoel/llama-bot-framework"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ia6tkw/i_connected_llama32_to_an_irc_server_it_was/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ia6tkw/i_connected_llama32_to_an_irc_server_it_was/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T05:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9vzey</id>
    <title>RTX 4060 8GB vs RX 7600 XT 16GB for running 14B parameter models (Phi4, DeepSeek R1 Q4 quantized)</title>
    <updated>2025-01-25T20:19:08+00:00</updated>
    <author>
      <name>/u/fugxto</name>
      <uri>https://old.reddit.com/user/fugxto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm currently looking into upgrading my GPU and I'm wondering which one would be better suited for running 14B parameter models like Phi4 or DeepSeek R1, specifically the Q4 quantized versions. I'm deciding between the NVIDIA RTX 4060 (8GB VRAM) and the AMD Radeon RX 7600 XT (16GB VRAM).&lt;/p&gt; &lt;p&gt;Given that these models can be pretty demanding in terms of both VRAM and processing power, which one do you think would offer better performance for these tasks?&lt;/p&gt; &lt;p&gt;One concern I have is the state of ROCm for AMD GPUs, as I‚Äôm not familiar with its current support and compatibility with models like these.&lt;/p&gt; &lt;p&gt;Thanks in advance for your insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fugxto"&gt; /u/fugxto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9vzey/rtx_4060_8gb_vs_rx_7600_xt_16gb_for_running_14b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9vzey/rtx_4060_8gb_vs_rx_7600_xt_16gb_for_running_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9vzey/rtx_4060_8gb_vs_rx_7600_xt_16gb_for_running_14b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T20:19:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1iabnph</id>
    <title>How to Generate Random RPG Character Names with an LLM</title>
    <updated>2025-01-26T10:56:48+00:00</updated>
    <author>
      <name>/u/Inevitable-Judge2642</name>
      <uri>https://old.reddit.com/user/Inevitable-Judge2642</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iabnph/how_to_generate_random_rpg_character_names_with/"&gt; &lt;img alt="How to Generate Random RPG Character Names with an LLM" src="https://external-preview.redd.it/iKnEKLG7VTAX2ecjFNkIHoIan9TbV0PXQvNHWAl_Kwk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=17c0af3fb105bc100fc7c148671e7ebed2fca4c2" title="How to Generate Random RPG Character Names with an LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Judge2642"&gt; /u/Inevitable-Judge2642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://k33g.hashnode.dev/how-to-generate-random-rpg-character-names-with-an-llm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iabnph/how_to_generate_random_rpg_character_names_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iabnph/how_to_generate_random_rpg_character_names_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T10:56:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1iac061</id>
    <title>ollama-grid-search for Vision Model ?! Any alternative ?</title>
    <updated>2025-01-26T11:16:15+00:00</updated>
    <author>
      <name>/u/elgeekphoenix</name>
      <uri>https://old.reddit.com/user/elgeekphoenix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I really love the ollama-grid-search (&lt;a href="https://github.com/dezoito/ollama-grid-search"&gt;https://github.com/dezoito/ollama-grid-search&lt;/a&gt;) to compare side by side Text queries.&lt;/p&gt; &lt;p&gt;I wanted to know if there is any fork or alternative to compare vision (image) models ?&lt;/p&gt; &lt;p&gt;I know Msty doing it for text, and Open Webui doing it as an arena , but didn't see anything close for image input .&lt;/p&gt; &lt;p&gt;Thanks a lot&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elgeekphoenix"&gt; /u/elgeekphoenix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iac061/ollamagridsearch_for_vision_model_any_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iac061/ollamagridsearch_for_vision_model_any_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iac061/ollamagridsearch_for_vision_model_any_alternative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T11:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9smk3</id>
    <title>Running Deepseek-r1 7b distilled model locally in a PC with no GPU with Ollama.</title>
    <updated>2025-01-25T17:53:06+00:00</updated>
    <author>
      <name>/u/Secraciesmeet</name>
      <uri>https://old.reddit.com/user/Secraciesmeet</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secraciesmeet"&gt; /u/Secraciesmeet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9smk3/running_deepseekr1_7b_distilled_model_locally_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9smk3/running_deepseekr1_7b_distilled_model_locally_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9smk3/running_deepseekr1_7b_distilled_model_locally_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T17:53:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1iadcye</id>
    <title>Trying to get ui-tars-desktop running with local vlm and lm studios</title>
    <updated>2025-01-26T12:31:13+00:00</updated>
    <author>
      <name>/u/Designer_Award_2551</name>
      <uri>https://old.reddit.com/user/Designer_Award_2551</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried to get ui-tars desktop running with lm studios. But I can't get the program working.&lt;/p&gt; &lt;p&gt;Lm studio is up and running, loaded deepseek vlm. Got the local server running, but the settings of ui-tars do not allow me to connect to it? It only has a setting for hugging face (which is a paid service?) Do I mis anything? Or isn't it possible right now. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Designer_Award_2551"&gt; /u/Designer_Award_2551 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iadcye/trying_to_get_uitarsdesktop_running_with_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iadcye/trying_to_get_uitarsdesktop_running_with_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iadcye/trying_to_get_uitarsdesktop_running_with_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T12:31:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaermz</id>
    <title>LOCAL MODEL RECOMMENDATION</title>
    <updated>2025-01-26T13:39:22+00:00</updated>
    <author>
      <name>/u/potatopcuser4ever</name>
      <uri>https://old.reddit.com/user/potatopcuser4ever</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which model should i run locally with a laptop having i5 12th gen,8gb ram and only integrated intel gpu&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/potatopcuser4ever"&gt; /u/potatopcuser4ever &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaermz/local_model_recommendation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaermz/local_model_recommendation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaermz/local_model_recommendation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T13:39:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaez8g</id>
    <title>Outdate document about python-llama-cpp</title>
    <updated>2025-01-26T13:49:12+00:00</updated>
    <author>
      <name>/u/wo-tatatatatata</name>
      <uri>https://old.reddit.com/user/wo-tatatatatata</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp/"&gt;https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the document in the link above is outdated and would not work, anyone knows how i can use local model from ollama instead in this example?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wo-tatatatatata"&gt; /u/wo-tatatatatata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaez8g/outdate_document_about_pythonllamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaez8g/outdate_document_about_pythonllamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaez8g/outdate_document_about_pythonllamacpp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T13:49:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9zv0u</id>
    <title>Deepseek R1 + WebUI energy costs?</title>
    <updated>2025-01-25T23:13:58+00:00</updated>
    <author>
      <name>/u/DelPrive235</name>
      <uri>https://old.reddit.com/user/DelPrive235</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm pretty new to this. I am in the process of installing Browser Use Web UI. I'm considering using it with Deepseek R1 14b. Running locally on a MBP M1 Max 64GB ram. Will this be significantly more expensive in energy costs than using WebUI with GPT 4o? Am I likely to notice a spike in my energy costs using R1 14b? Would it be any less expensive to just use the Deepseek API running onto their servers? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DelPrive235"&gt; /u/DelPrive235 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9zv0u/deepseek_r1_webui_energy_costs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9zv0u/deepseek_r1_webui_energy_costs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9zv0u/deepseek_r1_webui_energy_costs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T23:13:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1iadttf</id>
    <title>Total noob question (qwen-coder2.5 7b): Generated a clock with image, but image not loading</title>
    <updated>2025-01-26T12:55:10+00:00</updated>
    <author>
      <name>/u/Dull-Dragonfly-6851</name>
      <uri>https://old.reddit.com/user/Dull-Dragonfly-6851</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;i am a total beginner when it comes to AI and coding. I have tried to get &lt;a href="http://bolt.diy"&gt;bolt.diy&lt;/a&gt; and qwen-cloder2.5 7B to work locally and it does. Somehow &lt;a href="http://bolt.diy"&gt;bolt.diy&lt;/a&gt; with qwen-coder2.5 is running very slow, but qwen-coder2.5 in open-webui is running very fast - but thats only a side-note.&lt;/p&gt; &lt;p&gt;Please remember, a total noob is asking:&lt;/p&gt; &lt;p&gt;My main question is, if its possible to somehow see the image directly in the preview in qwen-coder2.5/bolt diy, because right now when i ask qwen-coder &amp;quot;Create a 1 page website with a clock that is updating every second&amp;quot; it does create the clock and shows a preview of it. When i then ask &amp;quot;add a ai generated donkey that blinks every time the clock gets updated&amp;quot; it does add a image, but this image is not loading, like its missing.&lt;/p&gt; &lt;p&gt;I understand that its missing because there is no path to that image, but i saw this video: &lt;a href="https://www.youtube.com/watch?v=197FcjcZ22A&amp;amp;t=1531s"&gt;https://www.youtube.com/watch?v=197FcjcZ22A&amp;amp;t=1531s&lt;/a&gt; and there the guy does almost the same and it show a ai generated donkey image. Due to the fact that he works pretty fast and only the code is overlayed displayed i cannot see what he exactly is doing.&lt;/p&gt; &lt;p&gt;So, is there a way to get it done? I hope you understand what i mean.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dull-Dragonfly-6851"&gt; /u/Dull-Dragonfly-6851 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iadttf/total_noob_question_qwencoder25_7b_generated_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iadttf/total_noob_question_qwencoder25_7b_generated_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iadttf/total_noob_question_qwencoder25_7b_generated_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T12:55:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9yv89</id>
    <title>Introducing Awesome Open Source AI: A list for tracking great open source models</title>
    <updated>2025-01-25T22:28:29+00:00</updated>
    <author>
      <name>/u/SuccessIsHardWork</name>
      <uri>https://old.reddit.com/user/SuccessIsHardWork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i9yv89/introducing_awesome_open_source_ai_a_list_for/"&gt; &lt;img alt="Introducing Awesome Open Source AI: A list for tracking great open source models" src="https://external-preview.redd.it/GXmJ82vxdZ1DvaPMBQ6TD4TQLzEhsWVdtuXgso5_c_Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce8f5a1c97427023bef612109fcd4409305118f3" title="Introducing Awesome Open Source AI: A list for tracking great open source models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuccessIsHardWork"&gt; /u/SuccessIsHardWork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/suncloudsmoon/awesome-open-source-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9yv89/introducing_awesome_open_source_ai_a_list_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9yv89/introducing_awesome_open_source_ai_a_list_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T22:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iacoys</id>
    <title>Is there a pure, quantised version of DeekSeekr1 for Ollama?</title>
    <updated>2025-01-26T11:55:10+00:00</updated>
    <author>
      <name>/u/john_alan</name>
      <uri>https://old.reddit.com/user/john_alan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see this: &lt;a href="https://ollama.com/library/deepseek-r1"&gt;https://ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;but it appears to be Llama3.2/Qwen trained with DeepSeek r1.&lt;/p&gt; &lt;p&gt;I essentially want an 8bit 70bn version of pure r1?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/john_alan"&gt; /u/john_alan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iacoys/is_there_a_pure_quantised_version_of_deekseekr1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iacoys/is_there_a_pure_quantised_version_of_deekseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iacoys/is_there_a_pure_quantised_version_of_deekseekr1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T11:55:10+00:00</published>
  </entry>
</feed>
