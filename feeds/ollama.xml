<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-29T22:23:23+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1icseux</id>
    <title>Proxmox + Ollama</title>
    <updated>2025-01-29T12:14:22+00:00</updated>
    <author>
      <name>/u/samuelpaluba</name>
      <uri>https://old.reddit.com/user/samuelpaluba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Post Title:&lt;/strong&gt; Mini LXC Proxmox Setup with Tesla P4 and Lenovo ThinkCentre M920q - Is It Possible?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Post Content:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m planning to build a mini LXC Proxmox setup using a Tesla P4 GPU and a Lenovo ThinkCentre M920q. I’m curious if this configuration would be sufficient to run a DeepSeek R1 model.&lt;/p&gt; &lt;p&gt;Here are some details:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: Tesla P4 (8 GB VRAM)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: Lenovo ThinkCentre M920q (with a suitable processor)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: I want to experiment with AI models, specifically DeepSeek R1 (and few light containers for mail and webhosting)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Do you think this combination would be enough for efficient model performance? What are your experiences with similar setups?&lt;/p&gt; &lt;p&gt;Additionally, I’d like to know if there are any other low-profile GPUs that would fit into the M920q and offer better performance than the Tesla P4. &lt;/p&gt; &lt;p&gt;Thanks for your insights and advice!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samuelpaluba"&gt; /u/samuelpaluba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icseux/proxmox_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icseux/proxmox_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icseux/proxmox_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T12:14:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1icm0er</id>
    <title>I want to generate images locally with my 4070 Ti</title>
    <updated>2025-01-29T04:45:27+00:00</updated>
    <author>
      <name>/u/AxelBlaze20850</name>
      <uri>https://old.reddit.com/user/AxelBlaze20850</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This might not be right subreddit but still asking for guidance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AxelBlaze20850"&gt; /u/AxelBlaze20850 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icm0er/i_want_to_generate_images_locally_with_my_4070_ti/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icm0er/i_want_to_generate_images_locally_with_my_4070_ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icm0er/i_want_to_generate_images_locally_with_my_4070_ti/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T04:45:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1icsli1</id>
    <title>ollama best model for coding etc (help)</title>
    <updated>2025-01-29T12:25:23+00:00</updated>
    <author>
      <name>/u/Aryangupt556</name>
      <uri>https://old.reddit.com/user/Aryangupt556</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1icsli1/ollama_best_model_for_coding_etc_help/"&gt; &lt;img alt="ollama best model for coding etc (help)" src="https://b.thumbs.redditmedia.com/Leo1Bavg0LVE7_6ojga8FsZKCa5kEALvOvQRu6Kb0ZI.jpg" title="ollama best model for coding etc (help)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hi, I’m new to Ollama and AI in general. I recently created my own model and have Llama 3.2 installed as the default. I just wanted to know which is the best model I can download that is safe and runs smoothly on my Mac. I was considering Dolphin-Mixtral, but at 26GB, it seems quite large. Any recommendations?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ti5rkonjexfe1.png?width=878&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d092469ab131e0532aa65574023667197b6f410b"&gt;https://preview.redd.it/ti5rkonjexfe1.png?width=878&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d092469ab131e0532aa65574023667197b6f410b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aryangupt556"&gt; /u/Aryangupt556 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icsli1/ollama_best_model_for_coding_etc_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icsli1/ollama_best_model_for_coding_etc_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icsli1/ollama_best_model_for_coding_etc_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T12:25:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1icjthh</id>
    <title>8x-AMD-Instinct-Mi60-Server-DeepSeek-R1-Distill-Llama-70B-Q8-vLLM</title>
    <updated>2025-01-29T02:46:41+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/e44y1oh0jufe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icjthh/8xamdinstinctmi60serverdeepseekr1distillllama70bq8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icjthh/8xamdinstinctmi60serverdeepseekr1distillllama70bq8/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T02:46:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1icxy97</id>
    <title>deepseek llm made my year.</title>
    <updated>2025-01-29T16:35:19+00:00</updated>
    <author>
      <name>/u/Forsaken-Diamond2145</name>
      <uri>https://old.reddit.com/user/Forsaken-Diamond2145</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm literally jumping for joy, deepseek r1 being already available on ollama is absolutely nuts, I'm making a school project about ollama (and it basically decides if I pass the 10th grade or not so uh... it's kinda important) and DEEPSEEK IS HERE ALREADY?!?!! MY PROJECT IS SAVED! I CAN TURN THIS INTO A WEBSITE WITH SOME WORK OR JUST ENJOY IT THROUGH MY TERMINAL NORMALLY. MY LIFE HAS BEEN BLESSED WITH DEEPSEEK (and ollama for that matter)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Forsaken-Diamond2145"&gt; /u/Forsaken-Diamond2145 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icxy97/deepseek_llm_made_my_year/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icxy97/deepseek_llm_made_my_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icxy97/deepseek_llm_made_my_year/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T16:35:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1icu15w</id>
    <title>How to set system prompt in ollama</title>
    <updated>2025-01-29T13:43:12+00:00</updated>
    <author>
      <name>/u/mans-987</name>
      <uri>https://old.reddit.com/user/mans-987</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can the system prompt be set on Ollama to work with all LLMs? I know that I can create a new model with the specific system prompt, but I want to set it via client and not create a new model for every different task that I am doing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mans-987"&gt; /u/mans-987 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icu15w/how_to_set_system_prompt_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icu15w/how_to_set_system_prompt_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icu15w/how_to_set_system_prompt_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T13:43:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1icexf6</id>
    <title>Would a 4090 mixed with a 3090 be enough to speed up R1 70b? (48 gb VRAM total)</title>
    <updated>2025-01-28T22:55:51+00:00</updated>
    <author>
      <name>/u/magicomiralles</name>
      <uri>https://old.reddit.com/user/magicomiralles</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I currently have a 4090 build. I am able to run the 70b version but it is extremelly slow. So I want to add a new GPU. I recently found out that it is possible to mix GPUs, so I can buy a 3090, which is much more affordable than a 4090.&lt;/p&gt; &lt;p&gt;For this I would have to get a new PSU, case, and an RTX 3090. Would this be the best approach?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/magicomiralles"&gt; /u/magicomiralles &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icexf6/would_a_4090_mixed_with_a_3090_be_enough_to_speed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icexf6/would_a_4090_mixed_with_a_3090_be_enough_to_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icexf6/would_a_4090_mixed_with_a_3090_be_enough_to_speed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T22:55:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1icxmza</id>
    <title>Which DeepSeek Model is Best for Extracting Text from Images Available on Ollama</title>
    <updated>2025-01-29T16:22:36+00:00</updated>
    <author>
      <name>/u/Consistent-Mind8434</name>
      <uri>https://old.reddit.com/user/Consistent-Mind8434</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm exploring ways to extract text from images available on Ollama and came across DeepSeek. However, there are multiple DeepSeek models, and I'm unsure which one would be the best for this task.&lt;/p&gt; &lt;p&gt;I’d appreciate any advice or experiences you can share! Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Consistent-Mind8434"&gt; /u/Consistent-Mind8434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icxmza/which_deepseek_model_is_best_for_extracting_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icxmza/which_deepseek_model_is_best_for_extracting_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icxmza/which_deepseek_model_is_best_for_extracting_text/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T16:22:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1icqsqp</id>
    <title>Ollama Deletes Automatically When Internet Is Down</title>
    <updated>2025-01-29T10:25:27+00:00</updated>
    <author>
      <name>/u/AlperParlak2009</name>
      <uri>https://old.reddit.com/user/AlperParlak2009</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to download DeepSeek with ollama. I have a decent internet speed but its not stable. When internet goes down for 10 to 15 seconds ollama deletes the already downloaded part. Because of that i cant download it. Is there a solution for this?&lt;/p&gt; &lt;p&gt;Edit: This is bullshit. I'm trying to download it for 7 fucking hours and it's still not downloaded. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlperParlak2009"&gt; /u/AlperParlak2009 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icqsqp/ollama_deletes_automatically_when_internet_is_down/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icqsqp/ollama_deletes_automatically_when_internet_is_down/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icqsqp/ollama_deletes_automatically_when_internet_is_down/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T10:25:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1icncir</id>
    <title>How to access deepseek-ai/Janus-Pro-1B on Ollama?</title>
    <updated>2025-01-29T06:07:19+00:00</updated>
    <author>
      <name>/u/Current_Mountain_100</name>
      <uri>https://old.reddit.com/user/Current_Mountain_100</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, please help me on how to access deepseek-ai/Janus-Pro-1B on ollama. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Current_Mountain_100"&gt; /u/Current_Mountain_100 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icncir/how_to_access_deepseekaijanuspro1b_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icncir/how_to_access_deepseekaijanuspro1b_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icncir/how_to_access_deepseekaijanuspro1b_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T06:07:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ictf5d</id>
    <title>best model using 4080 super for general tasks?</title>
    <updated>2025-01-29T13:11:44+00:00</updated>
    <author>
      <name>/u/ButterscotchOk1476</name>
      <uri>https://old.reddit.com/user/ButterscotchOk1476</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im going to get the 5080 when it releases which is just a few percent faster than the 4080 super, what models can I run locally? I mainly use ai for creative writing and programming. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ButterscotchOk1476"&gt; /u/ButterscotchOk1476 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ictf5d/best_model_using_4080_super_for_general_tasks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ictf5d/best_model_using_4080_super_for_general_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ictf5d/best_model_using_4080_super_for_general_tasks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T13:11:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1icz4jj</id>
    <title>help understanding RAM usage of LLM's with ollama</title>
    <updated>2025-01-29T17:22:28+00:00</updated>
    <author>
      <name>/u/vinaypundith</name>
      <uri>https://old.reddit.com/user/vinaypundith</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apologies if this is a dumb question. mainly: Can I run a large model (such as the full deepseek one) on a system with lots of system RAM but limited GPU VRAM, and still have the GPU do the computing? Can it transfer data from system RAM to the GPU to process and then return to storing the model in system RAM?&lt;/p&gt; &lt;p&gt;I have an older server computer with 640GB system RAM but only one GPU (an 11GB GTX 1080ti) and want to try to run DeepSeek R1-671b on it. There's plenty enough RAM to hold the model, but I want to know if that means it will run all the processing on the CPUs instead of utilizing the GPU.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vinaypundith"&gt; /u/vinaypundith &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icz4jj/help_understanding_ram_usage_of_llms_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icz4jj/help_understanding_ram_usage_of_llms_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icz4jj/help_understanding_ram_usage_of_llms_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T17:22:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1iczj8c</id>
    <title>What's the "minimum" parameters needed to have a useful model for everyday q&amp;a?</title>
    <updated>2025-01-29T17:38:52+00:00</updated>
    <author>
      <name>/u/abrandis</name>
      <uri>https://old.reddit.com/user/abrandis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So what is your feeling about what's the smalles model size in terms of parameters that yields USEFUL and mostly ACCURATE results most of the time. &lt;/p&gt; &lt;p&gt;I understand this is subjective, but for I find any models under 32b parameters just aren't very accurate...what are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abrandis"&gt; /u/abrandis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iczj8c/whats_the_minimum_parameters_needed_to_have_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iczj8c/whats_the_minimum_parameters_needed_to_have_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iczj8c/whats_the_minimum_parameters_needed_to_have_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T17:38:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1id0mur</id>
    <title>Ollama API as proxy for remote LLM?</title>
    <updated>2025-01-29T18:22:57+00:00</updated>
    <author>
      <name>/u/oRlrg5_XY4</name>
      <uri>https://old.reddit.com/user/oRlrg5_XY4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been playing around with Jetbrains AI Assistant - which lets me connect to a handful of preapproved remote models and ollama. I want to be able to connect to any model I want, whether it is remote or local. Is there a tool that could emulate the local ollama API but use any AI to serve the request? Whether forwarding to ollama or to OpenAI or Anthropic or some other future provider.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oRlrg5_XY4"&gt; /u/oRlrg5_XY4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id0mur/ollama_api_as_proxy_for_remote_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id0mur/ollama_api_as_proxy_for_remote_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1id0mur/ollama_api_as_proxy_for_remote_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T18:22:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1id17lf</id>
    <title>Deepseek-r1:8b broken english and refers to itself in the second person?</title>
    <updated>2025-01-29T18:46:23+00:00</updated>
    <author>
      <name>/u/AJaxx92</name>
      <uri>https://old.reddit.com/user/AJaxx92</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1id17lf/deepseekr18b_broken_english_and_refers_to_itself/"&gt; &lt;img alt="Deepseek-r1:8b broken english and refers to itself in the second person?" src="https://b.thumbs.redditmedia.com/S5hijmY0eFS2f7fSehoLDBZX7qVtW8eH3eltx5fEX4A.jpg" title="Deepseek-r1:8b broken english and refers to itself in the second person?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/atnwc2ks9zfe1.png?width=1064&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=216007c0f818ae007cbdc609e33c32ca20b9c85d"&gt;https://preview.redd.it/atnwc2ks9zfe1.png?width=1064&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=216007c0f818ae007cbdc609e33c32ca20b9c85d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Not the first odd response I've had. It's used Chinese for 'woof woof' when I told it to think like a dog before, but I wasn't expecting broken english. Is it to do with my hardware (12600k, 3080 ti, 32gb 3600mhz), or is it the model?&lt;/p&gt; &lt;p&gt;Edit: just tried asking the same question with the 14b model, and it definitely gave me a better response but still included Chinese with no explanation.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ec947p37ezfe1.png?width=1059&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=faccf6ccdf65e228bb362f9c7c4d97db35eab549"&gt;https://preview.redd.it/ec947p37ezfe1.png?width=1059&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=faccf6ccdf65e228bb362f9c7c4d97db35eab549&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit 2: is it translating on the fly or seeing something different to me? There wasn't &lt;em&gt;that&lt;/em&gt; much Chinese in the last response.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s9wcjenwezfe1.png?width=1054&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=03b91ad575246eb45a13bf51b2c1927993a77464"&gt;https://preview.redd.it/s9wcjenwezfe1.png?width=1054&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=03b91ad575246eb45a13bf51b2c1927993a77464&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AJaxx92"&gt; /u/AJaxx92 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id17lf/deepseekr18b_broken_english_and_refers_to_itself/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id17lf/deepseekr18b_broken_english_and_refers_to_itself/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1id17lf/deepseekr18b_broken_english_and_refers_to_itself/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T18:46:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1id1uzk</id>
    <title>Which model of Ai can i use with old pc ?</title>
    <updated>2025-01-29T19:12:18+00:00</updated>
    <author>
      <name>/u/TheLastAirbender2025</name>
      <uri>https://old.reddit.com/user/TheLastAirbender2025</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello All,&lt;/p&gt; &lt;p&gt;Very new to Ai, just trying to learn using simple model and nothing to fancy until i get to advance level &lt;/p&gt; &lt;p&gt;My System:&lt;/p&gt; &lt;p&gt;Operating System Windows 11 Pro 64-bit&lt;/p&gt; &lt;p&gt;CPU Intel Core i7 4790 @ 3.60GHz Haswell 22nm Technology&lt;/p&gt; &lt;p&gt;RAM 16.0GB Dual-Channel DDR3 @ 798MHz (11-11-11-28)&lt;/p&gt; &lt;p&gt;Motherboard LENOVO SHARKBAY (SOCKET 0)&lt;/p&gt; &lt;p&gt;Graphics 8176MB ATI AMD Radeon RX 6600 (Unknown)&lt;/p&gt; &lt;p&gt;Storage 465GB Samsung SSD 860 EVO 500GB (SATA (SSD))&lt;/p&gt; &lt;p&gt;Which model of Ai can i use so i can learn about ai and ai model and all other good skills that comes with it please advise &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLastAirbender2025"&gt; /u/TheLastAirbender2025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id1uzk/which_model_of_ai_can_i_use_with_old_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id1uzk/which_model_of_ai_can_i_use_with_old_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1id1uzk/which_model_of_ai_can_i_use_with_old_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T19:12:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1icivtq</id>
    <title>Why do local models still censor stuff?</title>
    <updated>2025-01-29T01:59:16+00:00</updated>
    <author>
      <name>/u/Sure-Year2141</name>
      <uri>https://old.reddit.com/user/Sure-Year2141</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I asked it a very grotesque question and it straight up refused to answer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sure-Year2141"&gt; /u/Sure-Year2141 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icivtq/why_do_local_models_still_censor_stuff/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icivtq/why_do_local_models_still_censor_stuff/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icivtq/why_do_local_models_still_censor_stuff/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T01:59:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1id36qa</id>
    <title>`ollama pull` keeps failing and restarting?</title>
    <updated>2025-01-29T20:05:51+00:00</updated>
    <author>
      <name>/u/XiPingTing</name>
      <uri>https://old.reddit.com/user/XiPingTing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried running &lt;code&gt;ollama pull deepseek-r1:14b&lt;/code&gt; overnight. The logs show it gets to about 30% done then gives up and restarts. I think I've downloaded the model from Hugging Face:&lt;/p&gt; &lt;p&gt;huggingface-cli download bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF --include &amp;quot;DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf&amp;quot; --local-dir ./&lt;/p&gt; &lt;p&gt;Where do I put this file and what do I name it to so I can run &lt;code&gt;ollama pull deepseek-r1:14b&lt;/code&gt;? I've got the &lt;code&gt;ollama pull deepseek-r1:7b&lt;/code&gt; model working just fine&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XiPingTing"&gt; /u/XiPingTing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id36qa/ollama_pull_keeps_failing_and_restarting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id36qa/ollama_pull_keeps_failing_and_restarting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1id36qa/ollama_pull_keeps_failing_and_restarting/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T20:05:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1id5z7c</id>
    <title>Multimodal ollama</title>
    <updated>2025-01-29T22:02:35+00:00</updated>
    <author>
      <name>/u/DifficultTomatillo29</name>
      <uri>https://old.reddit.com/user/DifficultTomatillo29</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using ollama constantly on my mac - and love it - have python programs that talk to ollama via rest api using various models - but now I have some tasks I want to move into multimodal - particularly in terms of getting an image back - is that possible using ollama? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DifficultTomatillo29"&gt; /u/DifficultTomatillo29 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id5z7c/multimodal_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id5z7c/multimodal_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1id5z7c/multimodal_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T22:02:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1icxpnt</id>
    <title>modelfile for DeepSeek-R1</title>
    <updated>2025-01-29T16:25:37+00:00</updated>
    <author>
      <name>/u/Fine-Degree431</name>
      <uri>https://old.reddit.com/user/Fine-Degree431</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone created a modelfile for deepseek, to set parameters such as temperature, context in ollama, if so can you share how it was done, and is used when using deepseek.&lt;/p&gt; &lt;p&gt;Thank you for your help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fine-Degree431"&gt; /u/Fine-Degree431 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icxpnt/modelfile_for_deepseekr1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icxpnt/modelfile_for_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icxpnt/modelfile_for_deepseekr1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T16:25:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1icxuzx</id>
    <title>KIOXIA Releases AiSAQ as Open-Source Software to Reduce DRAM Needs in AI Systems</title>
    <updated>2025-01-29T16:31:35+00:00</updated>
    <author>
      <name>/u/Kqyxzoj</name>
      <uri>https://old.reddit.com/user/Kqyxzoj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.techpowerup.com/331706/kioxia-releases-aisaq-as-open-source-software-to-reduce-dram-needs-in-ai-systems"&gt;TechPowerUp:&lt;/a&gt; &lt;em&gt;&amp;quot;Kioxia Corporation, a world leader in memory solutions, today announced the open-source release of its new All-in-Storage ANNS with Product Quantization (AiSAQ) technology. A novel &amp;quot;approximate nearest neighbor&amp;quot; search (ANNS) algorithm optimized for SSDs, KIOXIA AiSAQ software delivers scalable performance for retrieval-augmented generation (RAG) without placing index data in DRAM - and instead searching directly on SSDs.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Generative AI systems demand significant compute, memory and storage resources. While they have the potential to drive transformative breakthroughs across various industries, their deployment often comes with high costs. RAG is a critical phase of AI that refines large language models (LLMs) with data specific to the company or application.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Now in general using SSDs instead of VRAM or DRAM is going to be pretty damn slow. But for some use cases such as RAG, SSDs can be part of the solution:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.techpowerup.com/331706/kioxia-releases-aisaq-as-open-source-software-to-reduce-dram-needs-in-ai-systems"&gt;TechPowerUp: KIOXIA Releases AiSAQ as Open-Source Software to Reduce DRAM Needs in AI Systems&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/kioxiaamerica/aisaq-diskann"&gt;KIOXIA AiSAQ-DiskANN repo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2404.06004"&gt;arXiv:2404.06004: Writes Hurt: Lessons in Cache Design for Optane NVRAM&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Incidentally, that Kioxia repo is a fork of &lt;a href="https://github.com/microsoft/DiskANN"&gt;Microsoft's DiskANN repo&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It would be cool if something like this could be integrated in an Ollama RAG pipeline to lower the DRAM requirements. Running a &lt;a href="https://www.reddit.com/r/selfhosted/comments/1ic8zil/yes_you_can_run_deepseekr1_locally_on_your_device/"&gt;heavily quantized version of DeepSeek-R1 locally&lt;/a&gt; combined with RAG, where most of the RAG storage requirements are pushed to SSD sounds like an interesting idea.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kqyxzoj"&gt; /u/Kqyxzoj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icxuzx/kioxia_releases_aisaq_as_opensource_software_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icxuzx/kioxia_releases_aisaq_as_opensource_software_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icxuzx/kioxia_releases_aisaq_as_opensource_software_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T16:31:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1icrcdx</id>
    <title>Why don't we use NVMe instead of VRAM</title>
    <updated>2025-01-29T11:04:25+00:00</updated>
    <author>
      <name>/u/infinity6570</name>
      <uri>https://old.reddit.com/user/infinity6570</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why don't we use NMVe storage drives on PCIe lanes to directly serve the GPU instead of loading huge models to VRAM?? Yes, it will be slower and will have more latency, but being able to run something vs nothing is better right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/infinity6570"&gt; /u/infinity6570 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icrcdx/why_dont_we_use_nvme_instead_of_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icrcdx/why_dont_we_use_nvme_instead_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icrcdx/why_dont_we_use_nvme_instead_of_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T11:04:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1id5kkt</id>
    <title>Multi GPU</title>
    <updated>2025-01-29T21:45:21+00:00</updated>
    <author>
      <name>/u/666devi</name>
      <uri>https://old.reddit.com/user/666devi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Can I mix a 4080super and a 1080ti to handle bigger models? Is it worth it or should i just sell the 1080?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/666devi"&gt; /u/666devi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id5kkt/multi_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id5kkt/multi_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1id5kkt/multi_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T21:45:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1icv7wv</id>
    <title>Hardware requirements for running the full size deepseek R1 with ollama?</title>
    <updated>2025-01-29T14:39:12+00:00</updated>
    <author>
      <name>/u/BC547</name>
      <uri>https://old.reddit.com/user/BC547</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My machine runs the Deepseek R1-14B model fine, but the 34B and 70B are too slow for practical use. I am looking at building a machine capable of running the full 671B model fast enough that it's not too annoying as a coding assistant. What kind of hardware do i need?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BC547"&gt; /u/BC547 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icv7wv/hardware_requirements_for_running_the_full_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icv7wv/hardware_requirements_for_running_the_full_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icv7wv/hardware_requirements_for_running_the_full_size/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T14:39:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1icyl2l</id>
    <title>Will there ever be uncensored self hosted AI?</title>
    <updated>2025-01-29T17:00:55+00:00</updated>
    <author>
      <name>/u/mshriver2</name>
      <uri>https://old.reddit.com/user/mshriver2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tried out Ollama today for the first time as I was excited at the possibility of having a fully uncensored and unrestricted AI that could answer any question. Unfortunately even self hosted it is just as censored as Chat-GPT or any other large AI model. Do you think we will ever have a fully open source completely unrestricted AI? I don't understand how a company gets to decide what code runs or doesn't run on my own hardware.&lt;/p&gt; &lt;p&gt;Apologies for the rant in advance.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I should be the one deciding what is &amp;quot;legal&amp;quot; or &amp;quot;ethical&amp;quot; not my computer.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model used for testing: DeepSeek-R1-Distill-Qwen-32B&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mshriver2"&gt; /u/mshriver2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icyl2l/will_there_ever_be_uncensored_self_hosted_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icyl2l/will_there_ever_be_uncensored_self_hosted_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icyl2l/will_there_ever_be_uncensored_self_hosted_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T17:00:55+00:00</published>
  </entry>
</feed>
