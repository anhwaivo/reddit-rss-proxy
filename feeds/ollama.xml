<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-24T21:05:40+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1iwgsen</id>
    <title>Problem connecting remotely [Windows]</title>
    <updated>2025-02-23T18:27:49+00:00</updated>
    <author>
      <name>/u/sraasch</name>
      <uri>https://old.reddit.com/user/sraasch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I've checked all the obvious stuff already. And... It worked a month or so ago, when I last tried it!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I'm running Ollama native on windows (not WSL)&lt;/li&gt; &lt;li&gt;The server indicates that it is listening: &amp;quot;Listening on [::]:11434 (version 0.5.11)&amp;quot;&lt;/li&gt; &lt;li&gt;I have IPV6 disabled&lt;/li&gt; &lt;li&gt;I have windows firewall turned off&lt;/li&gt; &lt;li&gt;The OLLAMA_HOST variable is set to &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;I've rebooted several times and made sure I'm updated&lt;/li&gt; &lt;li&gt;I can access the localhost:11434 from the local machine&lt;/li&gt; &lt;li&gt;Running wireshark, I can see that the remote machine's attempt to open the connection *does* arrive at the windows machine. I see the original SYN and 3 retries.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I need to get smarter on TCP/IP to better understand the connection attempt as that *may* provide a clue, but I'm not optomistic.&lt;/p&gt; &lt;p&gt;If anyone has seen something like this, or has a thought on how to debug this, I'd be very grateful.&lt;/p&gt; &lt;p&gt;Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sraasch"&gt; /u/sraasch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwgsen/problem_connecting_remotely_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwgsen/problem_connecting_remotely_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwgsen/problem_connecting_remotely_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T18:27:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw51wo</id>
    <title>Self hosted LLM cpu non-gpu AVX512 importance ?</title>
    <updated>2025-02-23T07:51:38+00:00</updated>
    <author>
      <name>/u/centminmod</name>
      <uri>https://old.reddit.com/user/centminmod</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fairly new to self hosted LLM side. I use LM Studio on my 14&amp;quot; MacBook Pro M4 Pro with 48GB and 1TB drive and save LLM models to JEYI 2464 Pro fan edition USB4 NVMe external enclosure with 2TB Kingston KC3000.&lt;/p&gt; &lt;p&gt;However just started self hosted journey on my existing dedicated web servers developing my or-cli.py python client script that supports Openrouter.ai API + local Ollama &lt;a href="https://github.com/centminmod/or-cli"&gt;https://github.com/centminmod/or-cli&lt;/a&gt; and plan on adding vLLM support.&lt;/p&gt; &lt;p&gt;But the dedicated servers are fairly old and ram limited and lack AVX512 support. AMD Ryzen 5950X and Intel Xeon E-2276G with 64GB and 32GB memory respectively.&lt;/p&gt; &lt;p&gt;Short of GPU hosted servers, how much difference in performance would cpu only based usage for Ollama and vLLM and the like would there be if server supported AVX512 instructions for x86_64 based servers? Anyone got any past performance benchmark/results?&lt;/p&gt; &lt;p&gt;Even for GPU hosted, any noticeable difference pairing with/without cpu support for AVX512?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/centminmod"&gt; /u/centminmod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iw51wo/self_hosted_llm_cpu_nongpu_avx512_importance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iw51wo/self_hosted_llm_cpu_nongpu_avx512_importance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iw51wo/self_hosted_llm_cpu_nongpu_avx512_importance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T07:51:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1iweyjw</id>
    <title>Help: SyntaxError: Unexpected token '&lt;', "&lt;!DOCTYPE "... is not valid JSON</title>
    <updated>2025-02-23T17:10:35+00:00</updated>
    <author>
      <name>/u/flamingreaper1</name>
      <uri>https://old.reddit.com/user/flamingreaper1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, has anyone ever gotten this error when using Ollama through Openwebui?&lt;/p&gt; &lt;p&gt;I recently got a 7900xt, but after a few uses of it working fine, I run into this error and can only get past it by rebooting Unraid.&lt;/p&gt; &lt;p&gt;If this isn't the right community, please point me in the right direction! Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/flamingreaper1"&gt; /u/flamingreaper1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iweyjw/help_syntaxerror_unexpected_token_doctype_is_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iweyjw/help_syntaxerror_unexpected_token_doctype_is_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iweyjw/help_syntaxerror_unexpected_token_doctype_is_not/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T17:10:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwp77w</id>
    <title>Exporting an Ollama Model to Hugging Face Format</title>
    <updated>2025-02-24T00:36:31+00:00</updated>
    <author>
      <name>/u/naza01</name>
      <uri>https://old.reddit.com/user/naza01</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;The first disclaimer of this post is that I'm super new into this world so forgive me in advance if my question is silly.&lt;br /&gt; I looked a lot over the internet but haven't found anything useful so far.&lt;br /&gt; I was looking to fine-tune a model locally from my laptop.&lt;br /&gt; I'm using &lt;code&gt;qwen2.5-coder:1.5b&lt;/code&gt; model and I have already preprocessed the data I want to add to that model and have it in a JSONL format, which I read, is needed in order to successfully fine tune the LLM.&lt;br /&gt; Nevertheless, I'm having an error when trying to train the LLM with this data because apparently my model is not compatible with hugging face.&lt;br /&gt; I was hoping to have some built-in command from ollama to accomplish this, something like: &lt;code&gt;ollama fine-tune --model model_name --data data_to_finetune.jsonl&lt;/code&gt; but there's no native solution, therefore I read I can do this with hugging face, but then I'm having these incompatibilities.&lt;/p&gt; &lt;p&gt;Could someone care to explain what am I'm missing or what can I do differently to fine-tune my ollama model locally please?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/naza01"&gt; /u/naza01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwp77w/exporting_an_ollama_model_to_hugging_face_format/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwp77w/exporting_an_ollama_model_to_hugging_face_format/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwp77w/exporting_an_ollama_model_to_hugging_face_format/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T00:36:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwpbbt</id>
    <title>Quick &amp; Clean Web Data for Your Local LLMs? ðŸ‘‹ Introducing LexiCrawler (Binaries Inside!)</title>
    <updated>2025-02-24T00:42:10+00:00</updated>
    <author>
      <name>/u/Embarrassed-Way-1350</name>
      <uri>https://old.reddit.com/user/Embarrassed-Way-1350</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed-Way-1350"&gt; /u/Embarrassed-Way-1350 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1iwowsz/quick_clean_web_data_for_your_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwpbbt/quick_clean_web_data_for_your_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwpbbt/quick_clean_web_data_for_your_local_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T00:42:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwc049</id>
    <title>Moe for LLMs</title>
    <updated>2025-02-23T15:01:10+00:00</updated>
    <author>
      <name>/u/wahnsinnwanscene</name>
      <uri>https://old.reddit.com/user/wahnsinnwanscene</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What does it mean to have a mixture of experts in llama.cpp? Does it mean parts of weights are loaded when the mixture router decides on the expert, or is the entire model loaded and is partitioned programmatically ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wahnsinnwanscene"&gt; /u/wahnsinnwanscene &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwc049/moe_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwc049/moe_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwc049/moe_for_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T15:01:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwpjfp</id>
    <title>2nd GPU: VRAM overhead and available</title>
    <updated>2025-02-24T00:53:14+00:00</updated>
    <author>
      <name>/u/jujubre</name>
      <uri>https://old.reddit.com/user/jujubre</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all!&lt;br /&gt; Does someone could explain me why Ollama says that VRAM available is 11GB instead of 12GB?&lt;/p&gt; &lt;p&gt;Is there a way to have the 12GB available?&lt;/p&gt; &lt;p&gt;I have search quite a lot about this and I still do not understand why. Here are the facts: &lt;/p&gt; &lt;ul&gt; &lt;li&gt; I run ollama in win 11, both up to date.&lt;/li&gt; &lt;li&gt; Win 11 display: integrated GPU (AMD 7700X).&lt;/li&gt; &lt;li&gt; RTX 3060 12GB VRAM, as 2nd graphic card, no display attached.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ollama starting log: &lt;code&gt; time=2025-02-23T19:42:19.412-05:00 level=INFO source=images.go:432 msg=&amp;quot;total blobs: 64&amp;quot; time=2025-02-23T19:42:19.414-05:00 level=INFO source=images.go:439 msg=&amp;quot;total unused blobs removed: 0&amp;quot; time=2025-02-23T19:42:19.416-05:00 level=INFO source=routes.go:1237 msg=&amp;quot;Listening on [::]:11434 (version 0.5.11)&amp;quot; time=2025-02-23T19:42:19.416-05:00 level=INFO source=gpu.go:217 msg=&amp;quot;looking for compatible GPUs&amp;quot; time=2025-02-23T19:42:19.416-05:00 level=INFO source=gpu_windows.go:167 msg=packages count=1 time=2025-02-23T19:42:19.416-05:00 level=INFO source=gpu_windows.go:214 msg=&amp;quot;&amp;quot; package=0 cores=8 efficiency=0 threads=16 time=2025-02-23T19:42:19.539-05:00 level=INFO source=gpu.go:319 msg=&amp;quot;detected OS VRAM overhead&amp;quot; id=GPU-25c2f227-db2e-9f0b-b32a-ecff37fac3d0 library=cuda compute=8.6 driver=12.8 name=&amp;quot;NVIDIA GeForce RTX 3060&amp;quot; overhead=&amp;quot;867.3 MiB&amp;quot; time=2025-02-23T19:42:19.952-05:00 level=INFO source=amd_windows.go:127 msg=&amp;quot;unsupported Radeon iGPU detected skipping&amp;quot; id=0 total=&amp;quot;24.0 GiB&amp;quot; time=2025-02-23T19:42:19.954-05:00 level=INFO source=types.go:130 msg=&amp;quot;inference compute&amp;quot; id=GPU-25c2f227-db2e-9f0b-b32a-ecff37fac3d0 library=cuda variant=v12 compute=8.6 driver=12.8 name=&amp;quot;NVIDIA GeForce RTX 3060&amp;quot; total=&amp;quot;12.0 GiB&amp;quot; available=&amp;quot;11.0 GiB&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jujubre"&gt; /u/jujubre &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwpjfp/2nd_gpu_vram_overhead_and_available/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwpjfp/2nd_gpu_vram_overhead_and_available/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwpjfp/2nd_gpu_vram_overhead_and_available/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T00:53:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwmxf6</id>
    <title>External Ollama API Support has been added in Notate. RAG web &amp; vector store search, data ingestion pipeline and more!</title>
    <updated>2025-02-23T22:49:37+00:00</updated>
    <author>
      <name>/u/Hairetsu</name>
      <uri>https://old.reddit.com/user/Hairetsu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iwmxf6/external_ollama_api_support_has_been_added_in/"&gt; &lt;img alt="External Ollama API Support has been added in Notate. RAG web &amp;amp; vector store search, data ingestion pipeline and more!" src="https://external-preview.redd.it/uJ03FCBmwin1Bs6kmUTNhxfLIQ4gRfplVkP8UGh6jTs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb4e5c1bc213426afae9759155975f1cbdd10bc5" title="External Ollama API Support has been added in Notate. RAG web &amp;amp; vector store search, data ingestion pipeline and more!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hairetsu"&gt; /u/Hairetsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/CNTRLAI/Notate"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwmxf6/external_ollama_api_support_has_been_added_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwmxf6/external_ollama_api_support_has_been_added_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T22:49:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwyigw</id>
    <title>I need help to boost the results</title>
    <updated>2025-02-24T09:54:08+00:00</updated>
    <author>
      <name>/u/Eliahhigh787</name>
      <uri>https://old.reddit.com/user/Eliahhigh787</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using ollama with different models such as llama3, phi and mistra but the results take so long to show up. I use this model on a laptop.. should i upload it some where for better performance?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eliahhigh787"&gt; /u/Eliahhigh787 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwyigw/i_need_help_to_boost_the_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwyigw/i_need_help_to_boost_the_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwyigw/i_need_help_to_boost_the_results/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T09:54:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwh3iu</id>
    <title>Back at it again..</title>
    <updated>2025-02-23T18:40:56+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iwh3iu/back_at_it_again/"&gt; &lt;img alt="Back at it again.." src="https://preview.redd.it/5er15txqkxke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ba76a55522110f381f14f58cc97ad34e09bd0c9" title="Back at it again.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5er15txqkxke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwh3iu/back_at_it_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwh3iu/back_at_it_again/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T18:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwu3ap</id>
    <title>Just Released v1 of My AI-Powered VS Code Extension â€“ Looking for Feedback!</title>
    <updated>2025-02-24T04:53:59+00:00</updated>
    <author>
      <name>/u/Matrix_030</name>
      <uri>https://old.reddit.com/user/Matrix_030</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Matrix_030"&gt; /u/Matrix_030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/opensource/comments/1iuz8jk/just_released_v1_of_my_aipowered_vs_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwu3ap/just_released_v1_of_my_aipowered_vs_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwu3ap/just_released_v1_of_my_aipowered_vs_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T04:53:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwpy32</id>
    <title>Look Closely - 8x Mi50 (left) + 8x Mi60 (right) - Llama-3.3-70B - Do the Mi50s use less power ?!?!</title>
    <updated>2025-02-24T01:13:24+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6wvh9swlsxke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwpy32/look_closely_8x_mi50_left_8x_mi60_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwpy32/look_closely_8x_mi50_left_8x_mi60_right/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T01:13:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix2ec9</id>
    <title>Ollama API connection</title>
    <updated>2025-02-24T13:47:00+00:00</updated>
    <author>
      <name>/u/Low_Cherry_3357</name>
      <uri>https://old.reddit.com/user/Low_Cherry_3357</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I just installed ollama to run the AI â€‹â€‹model named &amp;quot;Mistral&amp;quot; locally.&lt;/p&gt; &lt;p&gt;Everything works perfectly when I talk to it through Windows 11 PowerShell with the following code &amp;quot;ollama run mistral&amp;quot;.&lt;/p&gt; &lt;p&gt;Now I would like the model to be able to use a certain number of PDF documents contained in a folder on my computer.&lt;/p&gt; &lt;p&gt;I used the &amp;quot;all-MiniLM-L6-v2&amp;quot; model to vectorize my text data. This seems to work well and create a &amp;quot;my_folder_chroma&amp;quot; folder with files inside.&lt;/p&gt; &lt;p&gt;I would now like to be able to query the Mistral model locally so that it can answer me by fetching the answers in my folder containing my PDFs.&lt;/p&gt; &lt;p&gt;Only I have the impression that it is asking me for an API connection with Ollama and I don't understand why? and on the other hand, I don't know how to activate this connection if it is necessary?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Low_Cherry_3357"&gt; /u/Low_Cherry_3357 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix2ec9/ollama_api_connection/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix2ec9/ollama_api_connection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ix2ec9/ollama_api_connection/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T13:47:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix43qn</id>
    <title>command-line options for LLMs</title>
    <updated>2025-02-24T15:04:22+00:00</updated>
    <author>
      <name>/u/beedunc</name>
      <uri>https://old.reddit.com/user/beedunc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a list of command-line options when running local LLMs? How is everyone getting statistics like TPS, etc? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beedunc"&gt; /u/beedunc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix43qn/commandline_options_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix43qn/commandline_options_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ix43qn/commandline_options_for_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T15:04:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix4ipy</id>
    <title>Practise usecases</title>
    <updated>2025-02-24T15:22:37+00:00</updated>
    <author>
      <name>/u/Turbulent-Cupcake-66</name>
      <uri>https://old.reddit.com/user/Turbulent-Cupcake-66</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, Ollama and others are powerful and easy to start tools. But what we can built with it in practise to help in our lifes. - home assistant - lokal chat gpt (why not use the paid one from openai)&lt;/p&gt; &lt;p&gt;I am asking about your ideas more for private life that for business cases.&lt;/p&gt; &lt;p&gt;I am also programmer. What can I do more than using just chat gpt? Can I for example show my local LLM my whole private code (thousends of lines) and then he will my new Junior developer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent-Cupcake-66"&gt; /u/Turbulent-Cupcake-66 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix4ipy/practise_usecases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix4ipy/practise_usecases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ix4ipy/practise_usecases/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T15:22:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix5er7</id>
    <title>AD/LDAP for agents</title>
    <updated>2025-02-24T16:00:13+00:00</updated>
    <author>
      <name>/u/productboy</name>
      <uri>https://old.reddit.com/user/productboy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My team is conducting R&amp;amp;D on authentication for AI agents. Ollama is a good test case because itâ€™s an abstraction layer for LLM I/O [similar to OpenRouter, etc.; but not direct API access to OpenAI, Anthropicâ€¦ which weâ€™ll test in the future]. &lt;/p&gt; &lt;p&gt;We believe AI agents need to be provisioned and onboarded like human staff in an enterprise. Thus they must be accounted for in an AD or LDAP like system. HR accounting is also an eventuality [Workday, ADPâ€¦]&lt;/p&gt; &lt;p&gt;The primitive requirements weâ€™re testing now are below. Question for this community: how do you currently authenticate AI agents in your enterprise?&lt;/p&gt; &lt;p&gt;Requirements: - Centralized management - Centralized authorization - RBAC - Multi tenant - Zero trust - Continuous verification &lt;/p&gt; &lt;p&gt;Social incentives: - Rewards for compliance - Confirms hierarchy direction&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/productboy"&gt; /u/productboy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix5er7/adldap_for_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix5er7/adldap_for_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ix5er7/adldap_for_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T16:00:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix0q07</id>
    <title>Please help with an error in Langchain's Ollama Deep Researcher</title>
    <updated>2025-02-24T12:19:09+00:00</updated>
    <author>
      <name>/u/_harsh_</name>
      <uri>https://old.reddit.com/user/_harsh_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Preface: I don't know much about python or programing. Have been able to run local LLMS and Ollama just by explicitly following instructions on github and such.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/langchain-ai/ollama-deep-researcher"&gt;https://github.com/langchain-ai/ollama-deep-researcher&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Installation went fine without any errors.&lt;/p&gt; &lt;p&gt;On inputting a string for research topic and clicking submit, the error &amp;quot;UnsupportedProtocol(&amp;quot;Request URL is missing an 'http://' or 'https://' protocol.&amp;quot;)&amp;quot; shows up.&lt;/p&gt; &lt;p&gt;I searched online for the issue and 3 people had a similar issue and it was resolved by removing quotation marks (&amp;quot; &amp;quot;) from the URl/API. (&lt;a href="https://learn.microsoft.com/en-us/answers/questions/2046429/azureopenai-apiconnectionerror-lacks-protocol-http"&gt;Link 1&lt;/a&gt;, &lt;a href="https://github.com/OpenInterpreter/open-interpreter/issues/994#issuecomment-2205417885"&gt;Link 2&lt;/a&gt;, &lt;a href="https://github.com/langchain-ai/langchain/issues/22951"&gt;Link 3&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;I cannot figure out where to edit this in the files. The env and config files do not have any URL line (Using DuckDuckGo by default which does not have any APIs). I also tried Tavily and put the API without quotes and still got the same error.&lt;/p&gt; &lt;p&gt;Other files that reference the DuckDuckGo URL are deep in .venv\Lib\site-packages directory and I am scared of touching them.&lt;/p&gt; &lt;p&gt;Posting here because a similar issue is open on the github page without any reply.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/langchain-ai/ollama-deep-researcher/pull/26/commits/08ff24f966bedf5b0eb0be03dcee8ad63d9021d6"&gt;Pull request when they added DuckDuckGo as default search.&lt;/a&gt; I dont think the error is search engine specific as I am getting it with Tavily as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_harsh_"&gt; /u/_harsh_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix0q07/please_help_with_an_error_in_langchains_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix0q07/please_help_with_an_error_in_langchains_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ix0q07/please_help_with_an_error_in_langchains_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T12:19:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwn9hk</id>
    <title>I created an open-source planning assistant that works with Ollama models that supports structured output</title>
    <updated>2025-02-23T23:04:23+00:00</updated>
    <author>
      <name>/u/neoneye2</name>
      <uri>https://old.reddit.com/user/neoneye2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iwn9hk/i_created_an_opensource_planning_assistant_that/"&gt; &lt;img alt="I created an open-source planning assistant that works with Ollama models that supports structured output" src="https://external-preview.redd.it/oy_g42SiRHj8efbYTgaxUdlRuNoUk8FyHzjh0Dar9K4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f87689dfecd43f7f8b9c7d55d48522c70d744e9e" title="I created an open-source planning assistant that works with Ollama models that supports structured output" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neoneye2"&gt; /u/neoneye2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/neoneye/PlanExe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwn9hk/i_created_an_opensource_planning_assistant_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwn9hk/i_created_an_opensource_planning_assistant_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T23:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwtdr7</id>
    <title>How good is a 7-14b model finetuned for a super specific use case (i.e. a spdcific sql dialect, or transforming data with pandas or pyspark)?</title>
    <updated>2025-02-24T04:12:39+00:00</updated>
    <author>
      <name>/u/juan_berger</name>
      <uri>https://old.reddit.com/user/juan_berger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like would it make sense to have a bunch of smaller models running locally, fined tuned to the specific task you are currently working on, and switching between them?&lt;/p&gt; &lt;p&gt;Would this even be that useful (or too much hastle switching between models and only working for that specific use case...)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juan_berger"&gt; /u/juan_berger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwtdr7/how_good_is_a_714b_model_finetuned_for_a_super/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwtdr7/how_good_is_a_714b_model_finetuned_for_a_super/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwtdr7/how_good_is_a_714b_model_finetuned_for_a_super/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T04:12:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixbla3</id>
    <title>Understanding System Prompt Behavior.</title>
    <updated>2025-02-24T20:09:49+00:00</updated>
    <author>
      <name>/u/Private-Citizen</name>
      <uri>https://old.reddit.com/user/Private-Citizen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On the ollama website, model pages show what's in the mod file, template, system, license.&lt;/p&gt; &lt;p&gt;My question is about the instructions in the system prompt, what you would see if you did &lt;code&gt;ollama show &amp;lt;model&amp;gt; --modelfile&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Does that system prompt get overwritten when you send a system prompt to the chat API &lt;code&gt;messages&lt;/code&gt; parameter or the generate API &lt;code&gt;prompt&lt;/code&gt; parameter? Or does it get appended to by your new system prompt? Or does it depend on the model, and if so then how do you know which behavior will be used?&lt;/p&gt; &lt;p&gt;For example; The openthinker model has a system prompt in the mod file which tells it how to process prompts using chain of thought. If im sending a system prompt in the API am i destroying those instructions? Would i need to manually include those instructions in my new system prompt?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Private-Citizen"&gt; /u/Private-Citizen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixbla3/understanding_system_prompt_behavior/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixbla3/understanding_system_prompt_behavior/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ixbla3/understanding_system_prompt_behavior/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T20:09:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixc05k</id>
    <title>Looking fot ollama installer on windows for an almost 80 years old uncle</title>
    <updated>2025-02-24T20:25:59+00:00</updated>
    <author>
      <name>/u/TotalRico</name>
      <uri>https://old.reddit.com/user/TotalRico</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I discussed ollama with an almost 80 years old uncle and showed him how to install and run it on a computer. He was fascinated and noted everything, even the opening of PowerShell which he had never run. Of course I also showed him chatgpt but he has personal health questions that he didn't want to ask online and I think it's great to keep that sparkle in his eyes at his age. Is there an installer for an ollama UI or an equivalent?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TotalRico"&gt; /u/TotalRico &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixc05k/looking_fot_ollama_installer_on_windows_for_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixc05k/looking_fot_ollama_installer_on_windows_for_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ixc05k/looking_fot_ollama_installer_on_windows_for_an/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T20:25:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwvj07</id>
    <title>Llama with no gpu and 120 gb RAM</title>
    <updated>2025-02-24T06:23:17+00:00</updated>
    <author>
      <name>/u/rock_db_saanu</name>
      <uri>https://old.reddit.com/user/rock_db_saanu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can Llama work efficiently with 120 GB RAM and no GPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rock_db_saanu"&gt; /u/rock_db_saanu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwvj07/llama_with_no_gpu_and_120_gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwvj07/llama_with_no_gpu_and_120_gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwvj07/llama_with_no_gpu_and_120_gb_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T06:23:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix7rso</id>
    <title>Getting familiar with llama</title>
    <updated>2025-02-24T17:35:44+00:00</updated>
    <author>
      <name>/u/Busy_Needleworker114</name>
      <uri>https://old.reddit.com/user/Busy_Needleworker114</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys! I am quite new to the ide of running LLM models locally. I am considering to use it because of privacy concers. Using it for work stuffs maybe more optimal than for example chatgpt. As far as I got in the maze of LLMs only smaller models can be run on laptops. I want to use it on a laptop which has a RTX4050 and 32Gb ddr5 rams. Can I run llama3.3? Should I try deepseek? Also is it even fully private?&lt;/p&gt; &lt;p&gt;I started using linux and i am thinking about installing it in docker, but I didnâ€™t found any usefull guide yet so if you know about some please share it with me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Busy_Needleworker114"&gt; /u/Busy_Needleworker114 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix7rso/getting_familiar_with_llama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix7rso/getting_familiar_with_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ix7rso/getting_familiar_with_llama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T17:35:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwm09j</id>
    <title>I created an open-source tool for using ANY Ollama model for real-time financial analysis</title>
    <updated>2025-02-23T22:08:44+00:00</updated>
    <author>
      <name>/u/No-Definition-2886</name>
      <uri>https://old.reddit.com/user/No-Definition-2886</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iwm09j/i_created_an_opensource_tool_for_using_any_ollama/"&gt; &lt;img alt="I created an open-source tool for using ANY Ollama model for real-time financial analysis" src="https://external-preview.redd.it/Ila7NpPu5TjU1zt5yIdrVeYFtcNrMgsyMgU6l0x4FVc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76ff6c15337ec12e3f3fb9b6b0962ffd1d2a31b6" title="I created an open-source tool for using ANY Ollama model for real-time financial analysis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Definition-2886"&gt; /u/No-Definition-2886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/austin-starks/FinAnGPT-Pro"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwm09j/i_created_an_opensource_tool_for_using_any_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwm09j/i_created_an_opensource_tool_for_using_any_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T22:08:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix5m9o</id>
    <title>I created an Ollama GUI in Next.js What do you think?</title>
    <updated>2025-02-24T16:08:40+00:00</updated>
    <author>
      <name>/u/Itsaliensbro453</name>
      <uri>https://old.reddit.com/user/Itsaliensbro453</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ix5m9o/i_created_an_ollama_gui_in_nextjs_what_do_you/"&gt; &lt;img alt="I created an Ollama GUI in Next.js What do you think?" src="https://preview.redd.it/woqefjsc24le1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eabc04436e27f4911b01aa209e8c1c9a65e91a90" title="I created an Ollama GUI in Next.js What do you think?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hellou guys im a developer trying to land my first job so im creating projects for my portfolio!&lt;/p&gt; &lt;p&gt;I have built this OLLAMA GUI with Next.js and Typescrypt!ðŸ˜€&lt;/p&gt; &lt;p&gt;How do you like it? Feel free to use the app and contribute its 100% free and open source! &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Ablasko32/Project-Shard---GUI-for-local-LLM-s"&gt;https://github.com/Ablasko32/Project-Shard---GUI-for-local-LLM-s&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Itsaliensbro453"&gt; /u/Itsaliensbro453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/woqefjsc24le1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix5m9o/i_created_an_ollama_gui_in_nextjs_what_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ix5m9o/i_created_an_ollama_gui_in_nextjs_what_do_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T16:08:40+00:00</published>
  </entry>
</feed>
