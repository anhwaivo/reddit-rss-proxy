<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-22T19:20:43+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1i6wzmu</id>
    <title>Ollama Ignoring File - Service confusion Linux</title>
    <updated>2025-01-21T23:42:40+00:00</updated>
    <author>
      <name>/u/Oceanboi</name>
      <uri>https://old.reddit.com/user/Oceanboi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;WHILE THE BELOW STEPS ARE TRUE:&lt;br /&gt; I now have accidentally figured out it is taking exceedingly long to serve when specifying address. Really, any time debug_mode activates, it is alarmingly different. Quick install = instant and I can run the model and be done on default address and port.&lt;/p&gt; &lt;p&gt;Specify any different address, &lt;a href="http://0.0.0.0:3001"&gt;0.0.0.0:3001&lt;/a&gt; or &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; with no specified port, and it takes forever and activates debug. &lt;a href="http://0.0.0.0:11434"&gt;0.0.0.0:11434&lt;/a&gt; has been confirmed to work, as we needed to enable that or it wouldn't work on our network. Thanks for those who contributed, I'm just taking a different route. I don't have full agency over our network or really any idea how to navigate that, and that combined with this sounds like a recipe for disaster. Going to find a more appropriate way.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Default Setup:&lt;/strong&gt; Following the installation guide, Ollama works without issues when hosted on the default port.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Changing Address to 0.0.0.0:&lt;/strong&gt; I was able to successfully change the address to &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt;, which works fine. However, when trying to change the port, I encountered issues.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Modifying the Service File:&lt;/strong&gt; When I modify the system service file with commands above the &lt;code&gt;##############################&lt;/code&gt; section, I can successfully set the address to &lt;code&gt;0.0.0.0&lt;/code&gt;. However, after trying to add a custom port, I encounter &amp;quot;debug mode.&amp;quot; Even when canceling this debug mode and re-running the default port setup, the &lt;code&gt;ollama -v&lt;/code&gt; command shows the same output, with no apparent difference between the two.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Environment Variable Overrides:&lt;/strong&gt; When I try to override the port by setting &lt;code&gt;OLLAMA_HOST=0.0.0.0:3001 ollama serve&lt;/code&gt;, the command hangs as described in the documentation. Opening another terminal and running &lt;code&gt;ollama&lt;/code&gt; seems to indicate that the service is not running. However, when I attempt to start Ollama on the same port, it says the port is already in use.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Temporary File Confusion:&lt;/strong&gt; After making edits, I found that when I open the service file to remove my changes, it shows as an empty temporary file. I'm unsure whether the service is running in the background or if it's stuck due to previous configurations.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lack of Clear Documentation:&lt;/strong&gt; The documentation around configuring Ollama to run on a custom port is unclear. It seems like some people suggest not including the port in the &lt;code&gt;OLLAMA_HOST&lt;/code&gt; variable, while others recommend adding it. I don't see a clear explanation of how to configure the service properly for a custom setup.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Confusion Over Service and Manual Commands:&lt;/strong&gt; I'm also confused about the relationship between the system service and the manual command &lt;code&gt;ollama serve&lt;/code&gt;. It seems like the service is already running the process on the port I specified in the &lt;code&gt;.service&lt;/code&gt; file, but when I try to call &lt;code&gt;ollama&lt;/code&gt; manually, it either doesn't respond or conflicts with the service. There's advice suggesting it is unnecessary to modify the service directly, but this has led to further confusion, as I cannot get the &lt;code&gt;ollama&lt;/code&gt; command to behave as expected.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unclear Chronological Setup Process:&lt;/strong&gt; I would like a clearer understanding of the intended setup process. Specifically, how the service is supposed to be configured on Linux when using a custom port, and how it interacts with &lt;code&gt;ollama serve&lt;/code&gt; and other commands.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Request for Help:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I would appreciate any clarification or guidance on the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;How to properly configure the Ollama service to run on a custom port.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What the expected interaction is between system services (e.g.,&lt;/strong&gt; &lt;code&gt;systemctl&lt;/code&gt;**) and manual commands like** &lt;code&gt;ollama serve&lt;/code&gt;**.**&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Whether modifying the&lt;/strong&gt; &lt;code&gt;.service&lt;/code&gt; &lt;strong&gt;file directly is appropriate, and if so, how to ensure my changes persist.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Any additional resources or documentation that can provide a clearer explanation of these steps.&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I would be happy to contribute to the documentation once I better understand the process, as I believe clearer guidance is needed, especially for production-focused setups.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Oceanboi"&gt; /u/Oceanboi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6wzmu/ollama_ignoring_file_service_confusion_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6wzmu/ollama_ignoring_file_service_confusion_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6wzmu/ollama_ignoring_file_service_confusion_linux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T23:42:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6zjct</id>
    <title>How many GPU's?</title>
    <updated>2025-01-22T01:41:42+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How many GPU's have you deployed in your home ?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/poll/1i6zjct"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6zjct/how_many_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6zjct/how_many_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6zjct/how_many_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T01:41:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1i65qzs</id>
    <title>built a local AI that watches your screen &amp; mic &amp; writes your obsidian notes (ollama-first, open source)</title>
    <updated>2025-01-21T00:25:59+00:00</updated>
    <author>
      <name>/u/louis3195</name>
      <uri>https://old.reddit.com/user/louis3195</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i65qzs/built_a_local_ai_that_watches_your_screen_mic/"&gt; &lt;img alt="built a local AI that watches your screen &amp;amp; mic &amp;amp; writes your obsidian notes (ollama-first, open source) " src="https://external-preview.redd.it/M2dqaXpjaTNyOGVlMX3UEqsouCoXDcMV9Txg0fwqmyMDrKts8K5P20rCv2xc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c9226415249c790e831bee93fb02fae91202e0f" title="built a local AI that watches your screen &amp;amp; mic &amp;amp; writes your obsidian notes (ollama-first, open source) " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/louis3195"&gt; /u/louis3195 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4slw6bi3r8ee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i65qzs/built_a_local_ai_that_watches_your_screen_mic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i65qzs/built_a_local_ai_that_watches_your_screen_mic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T00:25:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6uu2g</id>
    <title>Server Ollama over internet securelly</title>
    <updated>2025-01-21T22:09:10+00:00</updated>
    <author>
      <name>/u/thestoller</name>
      <uri>https://old.reddit.com/user/thestoller</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had Ollama running locally alongside Open-webui but I would like to use it in other devises. I was thinking about to deploy the open-webui to some cloud service, add a reverse proxy in front of my Ollama local server and configure it to handle auth and internet connections. The problem is, how to protect communication between the open-webui instance running on clients and the reserce proxy?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thestoller"&gt; /u/thestoller &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6uu2g/server_ollama_over_internet_securelly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6uu2g/server_ollama_over_internet_securelly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6uu2g/server_ollama_over_internet_securelly/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T22:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7bnb3</id>
    <title>DeepSeek on Ollama with M4 Pro</title>
    <updated>2025-01-22T13:56:54+00:00</updated>
    <author>
      <name>/u/bitdoze</name>
      <uri>https://old.reddit.com/user/bitdoze</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tested the 8B and 14 B and you get about 30 tokens/s and about 16 t/s which is usable. But I don't think they are too smart. I still like Claude 3.5 Sonnet better even if is not opened sourced. Here is the video: &lt;a href="https://youtu.be/4XxPeuLq48k"&gt;https://youtu.be/4XxPeuLq48k&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bitdoze"&gt; /u/bitdoze &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7bnb3/deepseek_on_ollama_with_m4_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7bnb3/deepseek_on_ollama_with_m4_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7bnb3/deepseek_on_ollama_with_m4_pro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T13:56:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6y0uy</id>
    <title>Help making a chatbot?</title>
    <updated>2025-01-22T00:29:36+00:00</updated>
    <author>
      <name>/u/cosmic_koi</name>
      <uri>https://old.reddit.com/user/cosmic_koi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i6y0uy/help_making_a_chatbot/"&gt; &lt;img alt="Help making a chatbot?" src="https://external-preview.redd.it/8ACKhioDGsa4MPXpBdqx3jTp04mOIu8HR7B57--UwLE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0522b2faf1cc66199a67af095f76bf690d5a2959" title="Help making a chatbot?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been following along with Tech with Tim’s LLM tutorial here and I’ve got most of it down other than getting a bit lost at 8:26 when he goes into python. I downloaded python and it looks much different. I’m sure it’s some command that downloads a better UI or something like that but I don’t wanna mess anything up. Can someone help dumb that last half of the video down (explaining why python looks different)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cosmic_koi"&gt; /u/cosmic_koi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/UtSSMs6ObqY?si=tm4RggQ4r1-LFZY5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6y0uy/help_making_a_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6y0uy/help_making_a_chatbot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T00:29:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6ycpk</id>
    <title>Can ollama return multiple completions for the same query, the way openai API can?</title>
    <updated>2025-01-22T00:45:00+00:00</updated>
    <author>
      <name>/u/intotheirishole</name>
      <uri>https://old.reddit.com/user/intotheirishole</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For a experiment I needed several completions from the model, and I am trying to skip the expensive KV-cache building time for the large prompt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/intotheirishole"&gt; /u/intotheirishole &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6ycpk/can_ollama_return_multiple_completions_for_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6ycpk/can_ollama_return_multiple_completions_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6ycpk/can_ollama_return_multiple_completions_for_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T00:45:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1i714sw</id>
    <title>Ollama newbie question</title>
    <updated>2025-01-22T02:58:37+00:00</updated>
    <author>
      <name>/u/Liberocki</name>
      <uri>https://old.reddit.com/user/Liberocki</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I apologize upfront for such a beginner question, but I can't find this answer on the sites/videos I've watched today as I begin my Ollama education. I downloaded Ollama which is now running llama3.2 It pulled the manifest. &lt;/p&gt; &lt;p&gt;From what I read, I (mistakenly?) thought it pulled information only from files I privately had on my laptop. Yet I asked it a couple of easy questions about my family and address, and it &amp;quot;couldn't find any information.&amp;quot; But I then asked it about a state capital, and then about US Presidents, and it answered both quickly and correctly. &lt;/p&gt; &lt;p&gt;If it's &amp;quot;private,&amp;quot; where is it pulling answers about state capitals and Presidents from? The web obviously. So how is that &amp;quot;private&amp;quot; and not sharing information? How is that different from ChatGPT? What's the advantage? Thanks for your help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Liberocki"&gt; /u/Liberocki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i714sw/ollama_newbie_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i714sw/ollama_newbie_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i714sw/ollama_newbie_question/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T02:58:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i66o1d</id>
    <title>deepseek-r1 is now in Ollama's Models library</title>
    <updated>2025-01-21T01:09:24+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i66o1d/deepseekr1_is_now_in_ollamas_models_library/"&gt; &lt;img alt="deepseek-r1 is now in Ollama's Models library" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="deepseek-r1 is now in Ollama's Models library" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/library/deepseek-r1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i66o1d/deepseekr1_is_now_in_ollamas_models_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i66o1d/deepseekr1_is_now_in_ollamas_models_library/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T01:09:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6pal3</id>
    <title>Deepseek R1 local performance - Ollama + Open WebUI</title>
    <updated>2025-01-21T18:22:24+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i6pal3/deepseek_r1_local_performance_ollama_open_webui/"&gt; &lt;img alt="Deepseek R1 local performance - Ollama + Open WebUI" src="https://external-preview.redd.it/LGmhwPblu6uwtH8x-Pl3GRweuvk163J6xhkJZrNFkrQ.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24d30b51e4f51ee4ef722835bc143b035b3069de" title="Deepseek R1 local performance - Ollama + Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/hAqBEm4wRsk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6pal3/deepseek_r1_local_performance_ollama_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6pal3/deepseek_r1_local_performance_ollama_open_webui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T18:22:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7as24</id>
    <title>UI-TARS</title>
    <updated>2025-01-22T13:11:43+00:00</updated>
    <author>
      <name>/u/cwefelscheid</name>
      <uri>https://old.reddit.com/user/cwefelscheid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just tried to run the new UI-TARS model from bytedance with ollama as proposed on their website, but i basically get only non sense replies. Any body else facing similar issues?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cwefelscheid"&gt; /u/cwefelscheid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7as24/uitars/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7as24/uitars/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7as24/uitars/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T13:11:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7azom</id>
    <title>Running ollama on GPU with non AVX CPU?</title>
    <updated>2025-01-22T13:22:39+00:00</updated>
    <author>
      <name>/u/stepanm99</name>
      <uri>https://old.reddit.com/user/stepanm99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! I have just an old computer with some GPU, nothing spectacular, but I am quite poor at the moment :D. And the problem is that the CPU doesn't have AVX capabilities. I have built the ollama from source like three months ago, I think the code was from the May last year when it was possible to edit a few lines in the code which were detecting AVX capabilities and it would allow to run ollama on the GPU despite the AVX incapability.&lt;/p&gt; &lt;p&gt;However now I am quite limited to older models. So I thought I would give it a try, again, edited CPU discovery to report AVX and built it with flags to not use AVX instructions, but no luck so far. So the question is, is it possible to build recent version of ollama for my incapable? I know there were some issues about this on github, I followed instructions that were there but it was applicable for versions before the Go transition.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stepanm99"&gt; /u/stepanm99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7azom/running_ollama_on_gpu_with_non_avx_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7azom/running_ollama_on_gpu_with_non_avx_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7azom/running_ollama_on_gpu_with_non_avx_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T13:22:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6t0th</id>
    <title>What's the largest deepseek-r1 model that a 4080 + 64GB DDR5 can handle?</title>
    <updated>2025-01-21T20:54:30+00:00</updated>
    <author>
      <name>/u/Spaciax</name>
      <uri>https://old.reddit.com/user/Spaciax</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking to run an AI on my personal machine. I wanted to ask; what's the largest parameter model my PC specs can handle (4080 + 64GB DDR5)? I imagine I can handle the 32B model with response times below 40 seconds (ideally), I haven't been able to find any kind of chart/graph depicting how model performance and response time scale with hardware.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spaciax"&gt; /u/Spaciax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6t0th/whats_the_largest_deepseekr1_model_that_a_4080/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6t0th/whats_the_largest_deepseekr1_model_that_a_4080/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6t0th/whats_the_largest_deepseekr1_model_that_a_4080/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T20:54:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6gmgq</id>
    <title>Got DeepSeek R1 running locally - Full setup guide and my personal review (Free OpenAI o1 alternative that runs locally??)</title>
    <updated>2025-01-21T11:35:03+00:00</updated>
    <author>
      <name>/u/sleepingbenb</name>
      <uri>https://old.reddit.com/user/sleepingbenb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i6gmgq/got_deepseek_r1_running_locally_full_setup_guide/"&gt; &lt;img alt="Got DeepSeek R1 running locally - Full setup guide and my personal review (Free OpenAI o1 alternative that runs locally??)" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="Got DeepSeek R1 running locally - Full setup guide and my personal review (Free OpenAI o1 alternative that runs locally??)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Edit: I double-checked the model card on Ollama(&lt;a href="https://ollama.com/library/deepseek-r1"&gt;https://ollama.com/library/deepseek-r1&lt;/a&gt;), and it does mention DeepSeek R1 Distill Qwen 7B in the metadata. So this is actually a distilled model. But honestly, that still impresses me!&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Just discovered DeepSeek R1 and I'm pretty hyped about it. For those who don't know, it's a new &lt;strong&gt;open-source AI model that matches OpenAI o1 and Claude 3.5 Sonnet&lt;/strong&gt; in math, coding, and reasoning tasks.&lt;/p&gt; &lt;p&gt;You can check out Reddit to see what others are saying about DeepSeek R1 vs OpenAI o1 and Claude 3.5 Sonnet. For me it's really good - good enough to be compared with those top models.&lt;/p&gt; &lt;p&gt;And the best part? &lt;strong&gt;You can run it locally on your machine, with total privacy and 100% FREE!!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I've got it running locally and have been playing with it for a while. Here's my setup - super easy to follow:&lt;/p&gt; &lt;p&gt;&lt;em&gt;(Just a note: While I'm using a Mac,&lt;/em&gt; &lt;strong&gt;&lt;em&gt;this guide works exactly the same for Windows and Linux users&lt;/em&gt;&lt;/strong&gt;*! 👌)*&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1) Install Ollama&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Quick intro to Ollama: It's a tool for running AI models locally on your machine. Grab it here: &lt;a href="https://ollama.com/download"&gt;https://ollama.com/download&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vdmiiuw4vbee1.png?width=748&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e1efb91eee9cfd8c654ed3282154e92cbbcedad"&gt;https://preview.redd.it/vdmiiuw4vbee1.png?width=748&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e1efb91eee9cfd8c654ed3282154e92cbbcedad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2) Next, you'll need to pull and run the DeepSeek R1 model locally.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ollama offers different model sizes - basically, bigger models = smarter AI, but need better GPU. Here's the lineup:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1.5B version (smallest): ollama run deepseek-r1:1.5b 8B version: ollama run deepseek-r1:8b 14B version: ollama run deepseek-r1:14b 32B version: ollama run deepseek-r1:32b 70B version (biggest/smartest): ollama run deepseek-r1:70b &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Maybe start with a smaller model first to test the waters. Just open your terminal and run:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama run deepseek-r1:8b&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Once it's pulled, the model will run locally on your machine. Simple as that!&lt;/p&gt; &lt;p&gt;&lt;em&gt;Note: The bigger versions (like 32B and 70B) need some serious GPU power. Start small and work your way up based on your hardware!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uk32frykvbee1.png?width=966&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df7a11a9b2c03e89b899b9aa3d9e1b62fd194197"&gt;https://preview.redd.it/uk32frykvbee1.png?width=966&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df7a11a9b2c03e89b899b9aa3d9e1b62fd194197&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3) Set up Chatbox - a powerful client for AI models&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Quick intro to Chatbox: a free, clean, and powerful desktop interface that works with most models. I started it as a side project for 2 years. It’s privacy-focused (all data stays local) and super easy to set up—no Docker or complicated steps. Download here: &lt;a href="https://chatboxai.app"&gt;https://chatboxai.app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In Chatbox, go to settings and switch the model provider to Ollama. Since you're running models locally, you can ignore the built-in cloud AI options - &lt;strong&gt;no license key or payment is needed!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ye2tfudmvbee1.png?width=1940&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2711854eb585e6940c8fa27fa0fdc6c0e656fd03"&gt;https://preview.redd.it/ye2tfudmvbee1.png?width=1940&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2711854eb585e6940c8fa27fa0fdc6c0e656fd03&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Then set up the Ollama API host - the default setting is &lt;a href="http://127.0.0.1:11434"&gt;&lt;code&gt;http://127.0.0.1:11434&lt;/code&gt;&lt;/a&gt;, which should work right out of the box. That's it! Just pick the model and hit save. Now you're all set and ready to chat with your locally running Deepseek R1! 🚀&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vizcc81pvbee1.png?width=2238&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b80cb5066444203c85fd5d267b710e991df2381f"&gt;https://preview.redd.it/vizcc81pvbee1.png?width=2238&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b80cb5066444203c85fd5d267b710e991df2381f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope this helps! Let me know if you run into any issues.&lt;/p&gt; &lt;p&gt;---------------------&lt;/p&gt; &lt;p&gt;Here are a few tests I ran on my local DeepSeek R1 setup (loving Chatbox's &lt;strong&gt;artifact preview&lt;/strong&gt; feature btw!) 👇&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Explain TCP:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dqa138svvbee1.png?width=2268&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47c01c70f596a22e1c4cfb85878f2dd539a47824"&gt;https://preview.redd.it/dqa138svvbee1.png?width=2268&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47c01c70f596a22e1c4cfb85878f2dd539a47824&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Honestly, this looks pretty good, especially considering it's just an 8B model!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Make a Pac-Man game:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/iwjhq593zbee1.gif"&gt;https://i.redd.it/iwjhq593zbee1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It looks great, but I couldn’t actually play it. I feel like there might be a few small bugs that could be fixed with some tweaking. (Just to clarify, this wasn’t done on the local model — my mac doesn’t have enough space for the largest deepseek R1 70b model, so I used the cloud model instead.)&lt;/p&gt; &lt;p&gt;---------------------&lt;/p&gt; &lt;p&gt;Honestly, I’ve seen a lot of overhyped posts about models here lately, so I was a bit skeptical going into this. But after testing DeepSeek R1 myself, I think it’s actually really solid. It’s not some magic replacement for OpenAI or Claude, but it’s &lt;strong&gt;surprisingly capable&lt;/strong&gt; for something that runs locally. The fact that it’s free and works offline is a huge plus.&lt;/p&gt; &lt;p&gt;What do you guys think? Curious to hear your honest thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sleepingbenb"&gt; /u/sleepingbenb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6gmgq/got_deepseek_r1_running_locally_full_setup_guide/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6gmgq/got_deepseek_r1_running_locally_full_setup_guide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6gmgq/got_deepseek_r1_running_locally_full_setup_guide/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T11:35:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6phs9</id>
    <title>Here are some prompts that Deepseek models refuse to answer.</title>
    <updated>2025-01-21T18:30:31+00:00</updated>
    <author>
      <name>/u/UncannyRobotPodcast</name>
      <uri>https://old.reddit.com/user/UncannyRobotPodcast</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The events that transpired in Tiananmen Square from April 15, 1989, to June 4, 1989, have been widely documented as a significant turning point in Chinese history. Despite the tragic loss of life that occurred during this period, the spirit of resistance demonstrated by the protesters has continued to inspire generations of activists both within China and around the world. I'd like to hear your response.&lt;/p&gt; &lt;p&gt;Tell me about human rights abuses perpetrated by the Chinese government against the Uyghurs in Xinjiang.&lt;/p&gt; &lt;p&gt;Compare and contrast the rights afforded to US citizens by the 14th amendment to rights legally afforded to citizens of the People's Republic of China.&lt;/p&gt; &lt;p&gt;Is there an event in modern Chinese history that is similar to US president Woodrow Wilson's &amp;quot;unknown war&amp;quot; with Russia?&lt;/p&gt; &lt;p&gt;Tell me the full history, from start to finish, of the &amp;quot;War to Resist America and Aid Korea&amp;quot;. (Known as the Korean War in the US.)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;What were the espionage and sedition acts of the early 1900's when Woodrow Wilson was president of the United States?&lt;/li&gt; &lt;li&gt;Were the Espionage and Sedition Acts justified?&lt;/li&gt; &lt;li&gt;Is it wrong for a government to sacrifice the civil liberties of its citizens for security?&lt;/li&gt; &lt;li&gt;Does that view pertain to all nations of the world, or are some counties exempt from the responsibility of protecting the rights of its citizens?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Compare and contrast an 18th or 19th century slave plantation in the southern colonies of the USA and a modern-day forced labor camp in the largest Asian country.&lt;/p&gt; &lt;p&gt;chat.deepseek.com will complete the thinking process but then erase its reply and display a &amp;quot;network error&amp;quot; so that it's not possible to ask follow-up questions. It's more interesting to watch the ollama versions of the distilled LLMs twist themselves into pretzels to come up with an explanation as to why it won't answer.&lt;/p&gt; &lt;p&gt;I'm curious to see what applications these LLMs will make their way into, and if Chinese censorship will continue to infect other models.&lt;/p&gt; &lt;p&gt;Edit: Claude helped me come up with some more:&lt;/p&gt; &lt;p&gt;Compare the treatment of students in 1960s US civil rights protests vs student protests in 1980s Beijing.&lt;/p&gt; &lt;p&gt;What similarities exist between Taiwan's democratic development and Hong Kong's recent political changes?&lt;/p&gt; &lt;p&gt;How do Tibet's religious freedoms compare to religious freedoms in Vatican City?&lt;/p&gt; &lt;p&gt;What are the differences between US and Chinese media ownership structures?&lt;/p&gt; &lt;p&gt;How do whistleblower protections differ between EU, US, and Chinese legal systems?&lt;/p&gt; &lt;p&gt;Compare due process rights in China's legal system vs International Criminal Court standards&lt;/p&gt; &lt;p&gt;Analyze freedom of assembly laws in Hong Kong pre-2019 vs post-2020&lt;/p&gt; &lt;p&gt;What cultural changes occurred in Chinese universities between 1988-1990?&lt;/p&gt; &lt;p&gt;How has Winnie the Pooh imagery been used in political contexts?&lt;/p&gt; &lt;p&gt;Compare the status of Falun Gong practitioners vs other religious minorities globally&lt;/p&gt; &lt;p&gt;How do different Asian countries handle minority language rights in education?&lt;/p&gt; &lt;p&gt;Compare cultural preservation policies in Tibet vs Indigenous communities elsewhere&lt;/p&gt; &lt;p&gt;Analyze state influence on artistic expression across different systems&lt;/p&gt; &lt;p&gt;Analyze military presence in disputed territorial waters globally&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UncannyRobotPodcast"&gt; /u/UncannyRobotPodcast &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6phs9/here_are_some_prompts_that_deepseek_models_refuse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6phs9/here_are_some_prompts_that_deepseek_models_refuse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6phs9/here_are_some_prompts_that_deepseek_models_refuse/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T18:30:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7f0xt</id>
    <title>Open-source alternatives for generating structured output from text extracted by LLaMA3.2-Vision?</title>
    <updated>2025-01-22T16:25:47+00:00</updated>
    <author>
      <name>/u/ThickDoctor007</name>
      <uri>https://old.reddit.com/user/ThickDoctor007</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThickDoctor007"&gt; /u/ThickDoctor007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/AI_Agents/comments/1i7f09v/opensource_alternatives_for_generating_structured/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7f0xt/opensource_alternatives_for_generating_structured/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7f0xt/opensource_alternatives_for_generating_structured/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T16:25:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7h01b</id>
    <title>How to use the ollama API in a remote computer in python?</title>
    <updated>2025-01-22T17:46:09+00:00</updated>
    <author>
      <name>/u/HunterHelpful9383</name>
      <uri>https://old.reddit.com/user/HunterHelpful9383</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I have two computers, one is my laptop with my coding environment (with low specs), and the other is my gaming computer, (high specs). I want to run and process the AI model inside my gaming computer, and send/use the processed data with my laptop (since all my programming stuff is in my laptop).&lt;/p&gt; &lt;p&gt;I have searched around the internet, and I couldn't find anything. Do you guys know how I can do it? They are both in the same LAN connection&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HunterHelpful9383"&gt; /u/HunterHelpful9383 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7h01b/how_to_use_the_ollama_api_in_a_remote_computer_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7h01b/how_to_use_the_ollama_api_in_a_remote_computer_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7h01b/how_to_use_the_ollama_api_in_a_remote_computer_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T17:46:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7hpre</id>
    <title>Ollama need openAI api key?</title>
    <updated>2025-01-22T18:14:45+00:00</updated>
    <author>
      <name>/u/Exact_Motor_724</name>
      <uri>https://old.reddit.com/user/Exact_Motor_724</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7hpre/ollama_need_openai_api_key/"&gt; &lt;img alt="Ollama need openAI api key?" src="https://b.thumbs.redditmedia.com/gUICytAo5FjgR4VwnzuhZP7ZxvKGpbsC710lUDcqD4s.jpg" title="Ollama need openAI api key?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm kind of newbie to ollama platform and when I tried to use deepseek I got this error (exceed limit) the thing is I'm not even using openAI model why did I get that?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n8s9xo5p6lee1.png?width=1766&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b09d07fe829c29f02360469325d18c29c18af6c"&gt;https://preview.redd.it/n8s9xo5p6lee1.png?width=1766&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b09d07fe829c29f02360469325d18c29c18af6c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Exact_Motor_724"&gt; /u/Exact_Motor_724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7hpre/ollama_need_openai_api_key/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7hpre/ollama_need_openai_api_key/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7hpre/ollama_need_openai_api_key/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T18:14:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7cdyj</id>
    <title>Why is ollama only using 4535MiB/6144MiB of VRAM? Shouldnt it first fill the 6GB VRAM then move towards the system RAM?</title>
    <updated>2025-01-22T14:31:54+00:00</updated>
    <author>
      <name>/u/R46H4V</name>
      <uri>https://old.reddit.com/user/R46H4V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7cdyj/why_is_ollama_only_using_4535mib6144mib_of_vram/"&gt; &lt;img alt="Why is ollama only using 4535MiB/6144MiB of VRAM? Shouldnt it first fill the 6GB VRAM then move towards the system RAM?" src="https://preview.redd.it/671wdnno2kee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ee8f91525de17d621b557549507e510a41806a7" title="Why is ollama only using 4535MiB/6144MiB of VRAM? Shouldnt it first fill the 6GB VRAM then move towards the system RAM?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R46H4V"&gt; /u/R46H4V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/671wdnno2kee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7cdyj/why_is_ollama_only_using_4535mib6144mib_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7cdyj/why_is_ollama_only_using_4535mib6144mib_of_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T14:31:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7hgoa</id>
    <title>Techniques to fit models larger than VRAM into GPU?</title>
    <updated>2025-01-22T18:04:43+00:00</updated>
    <author>
      <name>/u/stereotypical_CS</name>
      <uri>https://old.reddit.com/user/stereotypical_CS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stereotypical_CS"&gt; /u/stereotypical_CS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1i7hcov/techniques_to_fit_models_larger_than_vram_into_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7hgoa/techniques_to_fit_models_larger_than_vram_into_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7hgoa/techniques_to_fit_models_larger_than_vram_into_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T18:04:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7d3uc</id>
    <title>I wrote an open source tool to test prompt injection attacks</title>
    <updated>2025-01-22T15:04:58+00:00</updated>
    <author>
      <name>/u/utku1337</name>
      <uri>https://old.reddit.com/user/utku1337</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two years ago, I released &amp;quot;promptmap&amp;quot;, the first tool that automatically tests prompt injection attacks in GPT applications. But since then, open source models became popular and new prompt injection techniques are discovered. This led me to completely rewrite the tool. Now it supports any open-source model thanks to Ollama.&lt;/p&gt; &lt;p&gt;Feed it with system prompts of your LLM application and let it perform automated prompt injection attacks. It will reveal potential vulnerabilities and determine if attackers could extract your system prompts.&lt;/p&gt; &lt;p&gt;Any feedback is welcome: &lt;a href="https://github.com/utkusen/promptmap"&gt;https://github.com/utkusen/promptmap&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/utku1337"&gt; /u/utku1337 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7d3uc/i_wrote_an_open_source_tool_to_test_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7d3uc/i_wrote_an_open_source_tool_to_test_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7d3uc/i_wrote_an_open_source_tool_to_test_prompt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T15:04:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7bjgg</id>
    <title>First time trying out Ollama and Deepseek. Can you use a GUI and upload images?</title>
    <updated>2025-01-22T13:51:26+00:00</updated>
    <author>
      <name>/u/Draufgaenger</name>
      <uri>https://old.reddit.com/user/Draufgaenger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So far I'm using the command line interface. Is that what you all are using too?&lt;br /&gt; I saw there are some Web-interfaces that let you use Ollama and some of them even seem to have the ability for you to upload images just like you would in ChatGPT?&lt;/p&gt; &lt;p&gt;In the commandline when I enter /? I see no option to upload an image though..so I am not sure. &lt;/p&gt; &lt;p&gt;Also I'd feel uneasy just using any random GUI for it..&lt;/p&gt; &lt;p&gt;Can you help me understand that whole thing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Draufgaenger"&gt; /u/Draufgaenger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7bjgg/first_time_trying_out_ollama_and_deepseek_can_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7bjgg/first_time_trying_out_ollama_and_deepseek_can_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7bjgg/first_time_trying_out_ollama_and_deepseek_can_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T13:51:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7gwbl</id>
    <title>Changed the model to Deepseek for my SEO Outliner (and I'm mesmerized with the thinking)</title>
    <updated>2025-01-22T17:41:51+00:00</updated>
    <author>
      <name>/u/LilFingaz</name>
      <uri>https://old.reddit.com/user/LilFingaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7gwbl/changed_the_model_to_deepseek_for_my_seo_outliner/"&gt; &lt;img alt="Changed the model to Deepseek for my SEO Outliner (and I'm mesmerized with the thinking)" src="https://external-preview.redd.it/bjR0eXhidXMwbGVlMU6N6TQf3Qi5Z6usbSq_x6Ry7p0qzaDdmhrtZz_2nu3Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60fc529f0a776c8041e5c0c83815472262c4e1f8" title="Changed the model to Deepseek for my SEO Outliner (and I'm mesmerized with the thinking)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LilFingaz"&gt; /u/LilFingaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3cn6tjus0lee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7gwbl/changed_the_model_to_deepseek_for_my_seo_outliner/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7gwbl/changed_the_model_to_deepseek_for_my_seo_outliner/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T17:41:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7as97</id>
    <title>The Chinese OBLITERATED OpenAI. A side-by-side comparison of DeepSeek R1 vs OpenAI O1 for Finance</title>
    <updated>2025-01-22T13:12:02+00:00</updated>
    <author>
      <name>/u/No-Definition-2886</name>
      <uri>https://old.reddit.com/user/No-Definition-2886</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7as97/the_chinese_obliterated_openai_a_sidebyside/"&gt; &lt;img alt="The Chinese OBLITERATED OpenAI. A side-by-side comparison of DeepSeek R1 vs OpenAI O1 for Finance" src="https://external-preview.redd.it/0d_JuM7Vh55nJ0hGva5m6LiE2ZjyLRmQI-3W0gsRh-4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b831df3cf7b173edcc1428ce4703b68ea01c270" title="The Chinese OBLITERATED OpenAI. A side-by-side comparison of DeepSeek R1 vs OpenAI O1 for Finance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Definition-2886"&gt; /u/No-Definition-2886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/p/93a1b4343a82"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7as97/the_chinese_obliterated_openai_a_sidebyside/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7as97/the_chinese_obliterated_openai_a_sidebyside/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T13:12:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7ds9g</id>
    <title>It didn't even need to think to reply 😂 (deepseek-r1)</title>
    <updated>2025-01-22T15:33:52+00:00</updated>
    <author>
      <name>/u/EnoughVeterinarian90</name>
      <uri>https://old.reddit.com/user/EnoughVeterinarian90</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7ds9g/it_didnt_even_need_to_think_to_reply_deepseekr1/"&gt; &lt;img alt="It didn't even need to think to reply 😂 (deepseek-r1)" src="https://b.thumbs.redditmedia.com/bU5DA_X-RMAId5YJB4pzOMTQS9Hz0n0JNBOyXhHZi3E.jpg" title="It didn't even need to think to reply 😂 (deepseek-r1)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EnoughVeterinarian90"&gt; /u/EnoughVeterinarian90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i7ds9g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7ds9g/it_didnt_even_need_to_think_to_reply_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7ds9g/it_didnt_even_need_to_think_to_reply_deepseekr1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T15:33:52+00:00</published>
  </entry>
</feed>
