<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-05T00:49:37+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ih6w6i</id>
    <title>Question on the Berkley Model</title>
    <updated>2025-02-04T02:10:00+00:00</updated>
    <author>
      <name>/u/wulfendark</name>
      <uri>https://old.reddit.com/user/wulfendark</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has this Berkley model been added to ollama yet, and if so what is it called? I just started learning how to use ollama with the deepseek coder model and wanted another one to test. &lt;/p&gt; &lt;p&gt;Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wulfendark"&gt; /u/wulfendark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih6w6i/question_on_the_berkley_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih6w6i/question_on_the_berkley_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ih6w6i/question_on_the_berkley_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T02:10:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1igv6lp</id>
    <title>Is there any LLM model that can play Chess at all?</title>
    <updated>2025-02-03T17:53:30+00:00</updated>
    <author>
      <name>/u/pcbeard</name>
      <uri>https://old.reddit.com/user/pcbeard</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every test I've done with various models, including deepseek-r1, llama3.3 (both run with ollama), ChatGPT, etc. all fail to follow the rules of the game, and seem to be unable to even create a coherent representation of the game state from move to move. Here's an example poorly represented board produced by llama3.3 after just two moves (e2-e4, e5-e7):&lt;/p&gt; &lt;p&gt; &lt;code&gt;A B C D E F G H&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;8 ♔ ♗ ♘ ♕ ♚ ♘ ♗ ♔&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;7 ♙ ♙ ♙ ♙ . ♙ ♙ ♙&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;6 . . . . . . . .&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;5 . . . . ♙ . . .&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;4 . . . . ♟ . . .&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;3 . . . . . . . .&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;2 ♟ ♟ ♟ ♟ ♟ ♟ ♟ ♟&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;1 ♜ ♞ ♝ ♛ ♚ ♝ ♞ ♜&lt;/code&gt;&lt;/p&gt; &lt;p&gt;As you can see, the white side has an extra pawn showing, and the black side is completely scrambled with respect to the initial positions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pcbeard"&gt; /u/pcbeard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igv6lp/is_there_any_llm_model_that_can_play_chess_at_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igv6lp/is_there_any_llm_model_that_can_play_chess_at_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igv6lp/is_there_any_llm_model_that_can_play_chess_at_all/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T17:53:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1igxodm</id>
    <title>Has anyone ever tried analyzing their knowledge base before feeding it to a RAG?</title>
    <updated>2025-02-03T19:32:30+00:00</updated>
    <author>
      <name>/u/noduslabs</name>
      <uri>https://old.reddit.com/user/noduslabs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm curious because most of the tools out there just let you preview the chunks but you don't have a way of knowing whether your RAG is hallucinating or not. So is there anyone who actually tried to analyze their knowledge base before to know more or less what's inside and be able to verify how good RAG and AI responses are? If so, what are the tools you've used?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noduslabs"&gt; /u/noduslabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igxodm/has_anyone_ever_tried_analyzing_their_knowledge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igxodm/has_anyone_ever_tried_analyzing_their_knowledge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igxodm/has_anyone_ever_tried_analyzing_their_knowledge/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T19:32:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1igoppc</id>
    <title>Good iOS App for OLLAMA?</title>
    <updated>2025-02-03T13:10:44+00:00</updated>
    <author>
      <name>/u/lordtazou</name>
      <uri>https://old.reddit.com/user/lordtazou</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good morning!&lt;/p&gt; &lt;p&gt;Wanted to see what other users are currently checking out in the form of Front-End Apps for their own personal hosted LLMs. Currently borrowing an iPhone at the time, so would need to be an iOS App.&lt;/p&gt; &lt;p&gt;(Was not my first choice in phone / ecosystem but not going to complain... It was free. lol)&lt;/p&gt; &lt;p&gt;Currently, I am using Open WebUI and have it saved to my phone. But figured I would screw around with other Front-Ends to see what's out there.&lt;/p&gt; &lt;p&gt;Edit 02/03/2025:&lt;br /&gt; Checking out Enchanted so far as several users have mentioned it.&lt;/p&gt; &lt;p&gt;Edit 02/04/2025:&lt;br /&gt; Thanks for the suggestions! I think a few of the apps I missed just based on how I was phrasing / looking for some. Checking out the ones you guys all suggested, and a few others I discovered as well over the last day or two. Still up for more suggestions though!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lordtazou"&gt; /u/lordtazou &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igoppc/good_ios_app_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igoppc/good_ios_app_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igoppc/good_ios_app_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T13:10:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihbg3w</id>
    <title>Ollama times out / hangs</title>
    <updated>2025-02-04T06:23:45+00:00</updated>
    <author>
      <name>/u/oxnvyss</name>
      <uri>https://old.reddit.com/user/oxnvyss</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;i have tried running Lllama 8b, and 3b, and 1b, on a 3070 TI with 128gb ram and a i7 12700K, it keeps timing out / stops working after a few reqeusts. like the openwebui just keeps the circling symbol, i refresh and type another prompt and nothing happens just circles unless i restart the docker server and same thing with AnythingLLM, i have to close it and reopen it for it to work again.&lt;/p&gt; &lt;p&gt;Its like max 4-5 prompts before it does this.. my GPU is not maxed neither is my RAM or CPU. So not sure whats happening? how can i keep it alive?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oxnvyss"&gt; /u/oxnvyss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihbg3w/ollama_times_out_hangs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihbg3w/ollama_times_out_hangs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihbg3w/ollama_times_out_hangs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T06:23:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih5y85</id>
    <title>Ollama running on iGPU instead of dGPU</title>
    <updated>2025-02-04T01:24:19+00:00</updated>
    <author>
      <name>/u/unfiltereddz</name>
      <uri>https://old.reddit.com/user/unfiltereddz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I get more usage on CPU, but when I disable my iGPU in my laptop, now dGPU has 100% utilization. I already set it to high performance in both Nvidia Control Panel and Windows and downloaded Cuda Tool Kit. I still dont work. I have to disable my iGPU in order for it to work. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unfiltereddz"&gt; /u/unfiltereddz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih5y85/ollama_running_on_igpu_instead_of_dgpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih5y85/ollama_running_on_igpu_instead_of_dgpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ih5y85/ollama_running_on_igpu_instead_of_dgpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T01:24:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihm12m</id>
    <title>DeepSeek-R1: Normal vs Qwen vs Llama vs q8 q4 vs fp16 vs KM ?</title>
    <updated>2025-02-04T16:44:19+00:00</updated>
    <author>
      <name>/u/RedditNoobie777</name>
      <uri>https://old.reddit.com/user/RedditNoobie777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;|| || |&lt;a href="https://ollama.com/library/deepseek-r1:1.5b"&gt;1.5b&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:7b"&gt;7b&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:8b"&gt;8b&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:14b"&gt;14b&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:32b"&gt;32b&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:70b"&gt;70b&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:671b"&gt;671b&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:1.5b-qwen-distill-fp16"&gt;1.5b-qwen-distill-fp16&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:1.5b-qwen-distill-q4_K_M"&gt;1.5b-qwen-distill-q4_K_M&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:1.5b-qwen-distill-q8_0"&gt;1.5b-qwen-distill-q8_0&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:14b-qwen-distill-fp16"&gt;14b-qwen-distill-fp16&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:14b-qwen-distill-q4_K_M"&gt;14b-qwen-distill-q4_K_M&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:14b-qwen-distill-q8_0"&gt;14b-qwen-distill-q8_0&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:32b-qwen-distill-fp16"&gt;32b-qwen-distill-fp16&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:32b-qwen-distill-q4_K_M"&gt;32b-qwen-distill-q4_K_M&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:32b-qwen-distill-q8_0"&gt;32b-qwen-distill-q8_0&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:671b-fp16"&gt;671b-fp16&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:671b-q8_0"&gt;671b-q8_0&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:70b-llama-distill-fp16"&gt;70b-llama-distill-fp16&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:70b-llama-distill-q4_K_M"&gt;70b-llama-distill-q4_K_M&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:70b-llama-distill-q8_0"&gt;70b-llama-distill-q8_0&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:7b-qwen-distill-fp16"&gt;7b-qwen-distill-fp16&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:7b-qwen-distill-q4_K_M"&gt;7b-qwen-distill-q4_K_M&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:7b-qwen-distill-q8_0"&gt;7b-qwen-distill-q8_0&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:8b-llama-distill-fp16"&gt;8b-llama-distill-fp16&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:8b-llama-distill-q4_K_M"&gt;8b-llama-distill-q4_K_M&lt;/a&gt;| |&lt;a href="https://ollama.com/library/deepseek-r1:8b-llama-distill-q8_0"&gt;8b-llama-distill-q8_0&lt;/a&gt;|&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RedditNoobie777"&gt; /u/RedditNoobie777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihm12m/deepseekr1_normal_vs_qwen_vs_llama_vs_q8_q4_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihm12m/deepseekr1_normal_vs_qwen_vs_llama_vs_q8_q4_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihm12m/deepseekr1_normal_vs_qwen_vs_llama_vs_q8_q4_vs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T16:44:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih4bjv</id>
    <title>Local vs. Cloud? A Simple Diagram to Help You Choose an LLM</title>
    <updated>2025-02-04T00:08:25+00:00</updated>
    <author>
      <name>/u/Fun-Assignment4054</name>
      <uri>https://old.reddit.com/user/Fun-Assignment4054</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ih4bjv/local_vs_cloud_a_simple_diagram_to_help_you/"&gt; &lt;img alt="Local vs. Cloud? A Simple Diagram to Help You Choose an LLM" src="https://b.thumbs.redditmedia.com/SdQHBpXNjLeoDqok6HP4ruKNgH2Pa31TiMYMgGCnKts.jpg" title="Local vs. Cloud? A Simple Diagram to Help You Choose an LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/91w79sw7k0he1.png?width=2862&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d4f9b28206c2d51d503c9a0a6a340c8f3a181962"&gt;Diagram&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLM/comments/1ih46wf/local_vs_cloud_a_simple_diagram_to_help_you/"&gt;Originally posted&lt;/a&gt; in on &lt;a href="/r/LocalLLM"&gt;r/LocalLLM&lt;/a&gt;.&lt;br /&gt; ---- &lt;/p&gt; &lt;p&gt;Hi, I’m new to local LLMs and have been learning through Reddit and YouTube. I made a diagram to show when to use on‑device models vs. cloud‑based models. While building it, I added a branch labeled “on‑device AI determines,” thinking about an ideal setup. &lt;strong&gt;Is it possible to create a programmatic way to handle that?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In this diagram, I assume two things: (1) the user knows how to set up local models, and (2) they have already paid for (a) cloud‑based model(s). I hope this visual helps others out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Assignment4054"&gt; /u/Fun-Assignment4054 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih4bjv/local_vs_cloud_a_simple_diagram_to_help_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih4bjv/local_vs_cloud_a_simple_diagram_to_help_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ih4bjv/local_vs_cloud_a_simple_diagram_to_help_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T00:08:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ighr82</id>
    <title>Customizable GUI for ollama (less than 1MB)</title>
    <updated>2025-02-03T05:17:25+00:00</updated>
    <author>
      <name>/u/A8LR</name>
      <uri>https://old.reddit.com/user/A8LR</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ighr82/customizable_gui_for_ollama_less_than_1mb/"&gt; &lt;img alt="Customizable GUI for ollama (less than 1MB)" src="https://preview.redd.it/vyq6efv0zuge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fdbf99e396bc2b32ada4b4be7b12700a7f25056" title="Customizable GUI for ollama (less than 1MB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A barebones chat interface for Ollama in 4 files; HTML, CSS, JS and Python.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/qusaismael/localllm"&gt;https://github.com/qusaismael/localllm&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Why post: seeing people struggle with over-engineered examples. MIT licensed = modify freely. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A8LR"&gt; /u/A8LR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vyq6efv0zuge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ighr82/customizable_gui_for_ollama_less_than_1mb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ighr82/customizable_gui_for_ollama_less_than_1mb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T05:17:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihahfr</id>
    <title>How to save the llm state after binding it with tools?</title>
    <updated>2025-02-04T05:23:30+00:00</updated>
    <author>
      <name>/u/Lower-Substance3655</name>
      <uri>https://old.reddit.com/user/Lower-Substance3655</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’m working with an LLM (specifically using Ollama), and I’ve successfully customized it by binding some tools to the model using the .bindtools() function. Now I want to save the state of this model along with the tool bindings so that I can run it later using the command:&lt;/p&gt; &lt;p&gt;ollama run modelname&lt;/p&gt; &lt;p&gt;The idea is to avoid re-binding the tools every time I need to use the model and just save the whole setup once, but I haven’t been able to figure out how to persist the LLM with the tools attached.&lt;/p&gt; &lt;p&gt;Does anyone know how to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Save the model and the tools as one entity in Ollama (or another environment)?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Run it later with the command ollama run modelname without needing to reconfigure everything?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any help or pointers in the right direction would be appreciated!&lt;/p&gt; &lt;p&gt;Thanks in advance!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lower-Substance3655"&gt; /u/Lower-Substance3655 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihahfr/how_to_save_the_llm_state_after_binding_it_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihahfr/how_to_save_the_llm_state_after_binding_it_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihahfr/how_to_save_the_llm_state_after_binding_it_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T05:23:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihfsji</id>
    <title>Error parsing Excel file with PyMuPDFPro</title>
    <updated>2025-02-04T11:43:33+00:00</updated>
    <author>
      <name>/u/Single_Teacher_5926</name>
      <uri>https://old.reddit.com/user/Single_Teacher_5926</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone. I have been using Pymupdf4llm for Japanese resume pdfs extraction to push it to llm along with prompt. In order to get structured resume schema&lt;/p&gt; &lt;p&gt;CODE - import pymupdf4llm def extract_text_and_tables(pdf_path): md_text = pymupdf4llm.to_markdown(pdf_path) return md_text&lt;/p&gt; &lt;p&gt;But now i want to process excel files using PyMuPDFPro. &lt;/p&gt; &lt;p&gt;CODE - import pymupdf.pro pymupdf.pro.unlock(api_key) #i am putting my key here&lt;/p&gt; &lt;p&gt;doc = pymupdf.open(&amp;quot;/data/JP resume format 011.xlsx&amp;quot;)&lt;/p&gt; &lt;p&gt;for page in doc: text = page.get_text(&amp;quot;text&amp;quot;) print(text)&lt;/p&gt; &lt;p&gt;I am getting the error mentioned below - &lt;/p&gt; &lt;p&gt;File &amp;quot;/Bluparrot/test1/knowledge/test1.py&amp;quot;, line 1, in &amp;lt;module&amp;gt; import pymupdf.pro File &amp;quot;/Bluparrot/test1/myenv/lib/python3.12/site-packages/pymupdf/pro.py&amp;quot;, line 10, in &amp;lt;module&amp;gt; from . import _pro ImportError: libmupdf.so.25.1: cannot open shared object file: No such file or directory&lt;/p&gt; &lt;p&gt;What am i doing wrong ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Single_Teacher_5926"&gt; /u/Single_Teacher_5926 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihfsji/error_parsing_excel_file_with_pymupdfpro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihfsji/error_parsing_excel_file_with_pymupdfpro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihfsji/error_parsing_excel_file_with_pymupdfpro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T11:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih9w5x</id>
    <title>Threadripper CPU Testing</title>
    <updated>2025-02-04T04:49:36+00:00</updated>
    <author>
      <name>/u/BuffMcBigHuge</name>
      <uri>https://old.reddit.com/user/BuffMcBigHuge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ih9w5x/threadripper_cpu_testing/"&gt; &lt;img alt="Threadripper CPU Testing" src="https://b.thumbs.redditmedia.com/9l5wy5MbnsTyEe3u6Sfvh4UBwqXoF42_7TFF9jZuFro.jpg" title="Threadripper CPU Testing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ojb33b52y1he1.png?width=888&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b60337a4431b2394c7882236fae787bb7e806d31"&gt;https://preview.redd.it/ojb33b52y1he1.png?width=888&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b60337a4431b2394c7882236fae787bb7e806d31&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been testing with a Threadripper 3960x and 256gb of RAM. The issue I'm experiencing is that when the inference is completed, half of my CPU cores go in overdrive doing nothing. I feel like it's a bug with Ollama. I will test further.&lt;/p&gt; &lt;p&gt;Here are some results:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9pwodtxny1he1.png?width=778&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=363f6fede2a1cbbd9dfd5c78efda3e32a925be0d"&gt;https://preview.redd.it/9pwodtxny1he1.png?width=778&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=363f6fede2a1cbbd9dfd5c78efda3e32a925be0d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Getting around 1.25 tokens per second with 2.22bit 671b, RAM at 3200mhz. My system is unstable at 3600mhz 256gb.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BuffMcBigHuge"&gt; /u/BuffMcBigHuge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih9w5x/threadripper_cpu_testing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih9w5x/threadripper_cpu_testing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ih9w5x/threadripper_cpu_testing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T04:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1igrtd7</id>
    <title>LLM Powered Map</title>
    <updated>2025-02-03T15:36:26+00:00</updated>
    <author>
      <name>/u/ranoutofusernames__</name>
      <uri>https://old.reddit.com/user/ranoutofusernames__</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1igrtd7/llm_powered_map/"&gt; &lt;img alt="LLM Powered Map" src="https://preview.redd.it/imcksipg1yge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ffd22c99c31c7a7c90376fee3c2d82e64d1c2451" title="LLM Powered Map" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open source, LLM powered discovery/exploration map I made a month ago. Runs locally or using cloud models. With a big enough model, it’s pretty much like having an offline, global map. Cheers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/space0blaster/godview"&gt;Repo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ranoutofusernames__"&gt; /u/ranoutofusernames__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/imcksipg1yge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igrtd7/llm_powered_map/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igrtd7/llm_powered_map/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T15:36:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ignq9z</id>
    <title>Is there a way to "train" an open-source LLM to do one type of task really well?</title>
    <updated>2025-02-03T12:15:45+00:00</updated>
    <author>
      <name>/u/ArtPerToken</name>
      <uri>https://old.reddit.com/user/ArtPerToken</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, forgive me if its a silly question, but is there a way to train or modify an existing LLM (i guess an open source one) to do one type of tasks really well?&lt;/p&gt; &lt;p&gt;For example if I have 50 poems I wrote in my own unique style, how can I &amp;quot;feed&amp;quot; it to the LLM and then ask it to generate a new poem about a new subject in the same style?&lt;/p&gt; &lt;p&gt;Would appreciate any thoughts on the best way to go about this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArtPerToken"&gt; /u/ArtPerToken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ignq9z/is_there_a_way_to_train_an_opensource_llm_to_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ignq9z/is_there_a_way_to_train_an_opensource_llm_to_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ignq9z/is_there_a_way_to_train_an_opensource_llm_to_do/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T12:15:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihq1pr</id>
    <title>Is human consciousness an illusion reproducible by AI?</title>
    <updated>2025-02-04T19:26:22+00:00</updated>
    <author>
      <name>/u/MoreIndependent5967</name>
      <uri>https://old.reddit.com/user/MoreIndependent5967</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If human consciousness is just an emergent product of the complex interactions between our neurons, then it is logical to think that one day, AI could develop a form of consciousness. Consciousness could be seen as a functional illusion, created by memory, reflection, and constant adjustments between neural signals, initially developed for survival. If AI reaches a point where it can adjust its own weights, learn from its experiences and set its own goals, it could naturally pass a critical threshold.&lt;/p&gt; &lt;p&gt;This does not necessarily mean that she will feel emotions or qualia (subjective feeling), but she could have a functional consciousness: an ability to represent herself, to adapt to her environment, and to anticipate consequences. of his actions. It would be a form of pragmatic consciousness, useful for one's own autonomous development.&lt;/p&gt; &lt;p&gt;But when would we consider that an AI is truly conscious? Is it when she simulates complex human behaviors or when she begins to set her own priorities without supervision? The line between simulation and reality could become blurred much faster than we think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoreIndependent5967"&gt; /u/MoreIndependent5967 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihq1pr/is_human_consciousness_an_illusion_reproducible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihq1pr/is_human_consciousness_an_illusion_reproducible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihq1pr/is_human_consciousness_an_illusion_reproducible/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T19:26:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihll6s</id>
    <title>Enhanced Privacy with Ollama and others</title>
    <updated>2025-02-04T16:26:08+00:00</updated>
    <author>
      <name>/u/Key_Opening_3243</name>
      <uri>https://old.reddit.com/user/Key_Opening_3243</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’m excited to announce my Open Source tool focused on privacy during inference with AI models locally via Ollama or generic obfuscation for any case.&lt;/p&gt; &lt;p&gt;&lt;a href="https://maltese.johan.chat/"&gt;https://maltese.johan.chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I invite you all to contribute to this idea, which, although quite simple, can be highly effective in certain cases.&lt;br /&gt; Feel free to reach out to discuss the idea and how to evolve it.&lt;/p&gt; &lt;p&gt;Best regards, Johan.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Key_Opening_3243"&gt; /u/Key_Opening_3243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihll6s/enhanced_privacy_with_ollama_and_others/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihll6s/enhanced_privacy_with_ollama_and_others/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihll6s/enhanced_privacy_with_ollama_and_others/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T16:26:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihf1ou</id>
    <title>Ollama Flashcard creation</title>
    <updated>2025-02-04T10:52:15+00:00</updated>
    <author>
      <name>/u/Key_King_1216</name>
      <uri>https://old.reddit.com/user/Key_King_1216</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not a programmer. I need help creating flashcards in a csv file format and exporting it to anki using a reliable language model. Can anyone explain to me how I would do this. After downloading ollama and downloading the model whether it be deepseek-r1, or llama3, what do I do after?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Key_King_1216"&gt; /u/Key_King_1216 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihf1ou/ollama_flashcard_creation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihf1ou/ollama_flashcard_creation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihf1ou/ollama_flashcard_creation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T10:52:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihu2by</id>
    <title>Titan XP vs 2 Tesla M40 12GB Cards</title>
    <updated>2025-02-04T22:10:32+00:00</updated>
    <author>
      <name>/u/MajorJakePennington</name>
      <uri>https://old.reddit.com/user/MajorJakePennington</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking at building a box for running local DeepSeek models, but am having difficulty finding performance metrics for the Titan XP and the Tesla M40. For 1/2 the price of a Titan XP I can buy 2 Tesla M40 cards and have 24GB of VRAM, but is the performance there for 8b+ models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MajorJakePennington"&gt; /u/MajorJakePennington &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihu2by/titan_xp_vs_2_tesla_m40_12gb_cards/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihu2by/titan_xp_vs_2_tesla_m40_12gb_cards/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihu2by/titan_xp_vs_2_tesla_m40_12gb_cards/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T22:10:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihjfy9</id>
    <title>Slow performance on K8S</title>
    <updated>2025-02-04T14:54:29+00:00</updated>
    <author>
      <name>/u/geeky217</name>
      <uri>https://old.reddit.com/user/geeky217</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m trying to run ollama with deepseek-r1 7b on CPU only inside K8S. I’m using the official helm chart inside RedHat Openshift 4.12.2 with cephRBD storage. This is all on a vm (it’s a single node openshift dev box) with 24 cores of 6148 gold Xeon cpu and 96GB. The ollama deployment has 16 cores and 16GB set as reserved. Now the issue is that it runs like a dog compared to ollama on a basic Ubuntu vm on the same esx host which has half the resources (8&amp;amp;8). The only difference is one is containerised the other just a vm. I’m at a loss why there is such a performance difference. Both ollama instances run off nvme local storage so have plenty of bandwidth and low latency. Anyone got any insights here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geeky217"&gt; /u/geeky217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihjfy9/slow_performance_on_k8s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihjfy9/slow_performance_on_k8s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihjfy9/slow_performance_on_k8s/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T14:54:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihqub9</id>
    <title>I want to get into Local LLMs for coding, home assistant, and maybe a little conversation. Low token/s is fine to see if I even like it. Which hardware that I have listed could do it? Or do I need a GPU solely for this, even to start off?</title>
    <updated>2025-02-04T19:59:03+00:00</updated>
    <author>
      <name>/u/bigrjsuto</name>
      <uri>https://old.reddit.com/user/bigrjsuto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have the following hardware:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Personal PC (LInux Mint 21) &lt;ul&gt; &lt;li&gt;Ryzen 5800X, 64GB DDR4, 3060 12GB, 1TB NVMe + 2TB NVMe&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Server PC (Proxmox) &lt;ul&gt; &lt;li&gt;Intel 12500T, 128GB DDR4, A4000 16GB (passthrough to Windows 11 VM for Solidworks), 128GB NVMe (boot) + 1TB NVMe (VMs/LXCs), 2x 18TB HDDs&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Gaming/HTPC (Windows) &lt;ul&gt; &lt;li&gt;Intel 10600K, 32GB DDR4, RX 590 8GB, 128GB + 1TB SSD&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;MiniPCs &lt;ul&gt; &lt;li&gt;20x Datto ALTO 3 V2 &lt;ul&gt; &lt;li&gt;Celeron 3865U, 2-16GB DDR4 (Can configure as needed)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;2x Datto S3X2 Dual-NIC &lt;ul&gt; &lt;li&gt;Intel i3-7100U, 2-16GB DDR4&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;4x Optiplex 3040 &lt;ul&gt; &lt;li&gt;Intel i3-6100T, 2-8GB DDR3&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Intel NUC &lt;ul&gt; &lt;li&gt;Intel i7-7567U, 2-16GB DDR4&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Other Hardware &lt;ul&gt; &lt;li&gt;RX 560 4GB LP&lt;/li&gt; &lt;li&gt;Various extra HDDs 160GB - 8TB&lt;/li&gt; &lt;li&gt;Various extra NVMe 32GB - 500GB&lt;/li&gt; &lt;li&gt;A few extra network switches (if needed for clustering)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wanted to add the RX 560 to the server, but after some research, everything I've seen says that 4GB is too little VRAM for even slow output. That's the case, right?&lt;/p&gt; &lt;p&gt;How about a Coral TPU? Or multiple? Each of those Datto ALTO MiniPCs have a A+E keyed m.2 slot, where I could place them and cluster them together.&lt;/p&gt; &lt;p&gt;Could I just run it on my PC? Would the 3060 be good enough to get some output?&lt;/p&gt; &lt;p&gt;I know there's the A4000, but I need it for CAD work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigrjsuto"&gt; /u/bigrjsuto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihqub9/i_want_to_get_into_local_llms_for_coding_home/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihqub9/i_want_to_get_into_local_llms_for_coding_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihqub9/i_want_to_get_into_local_llms_for_coding_home/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T19:59:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihu6s2</id>
    <title>Need help with training</title>
    <updated>2025-02-04T22:15:47+00:00</updated>
    <author>
      <name>/u/Kind_Ad_2866</name>
      <uri>https://old.reddit.com/user/Kind_Ad_2866</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just entered the AI race not too long ago and I have some concepts to wrap my head around &lt;/p&gt; &lt;p&gt;1- what’s the difference between training the model via chat and training the model via unsloth&lt;/p&gt; &lt;p&gt;2- what’s the difference between datasets fed to unsloth and standard RAG where you upload bunch of files then you can ask the model about them? I’m asking because I have pdf text files (books, novels, etc) and I want to chat with the models about it or ask the AI to give me a decision based on data in these files. &lt;/p&gt; &lt;p&gt;3- if unsloth is the way to go, how would I go about creating a dataset for a novel? I have seen datasets where they mention the characters, but I don’t understand how the model would piece the story just buy using the character description!&lt;/p&gt; &lt;p&gt;here is an example on the dataset I mean: &lt;a href="https://huggingface.co/datasets/xywang1/OpenCharacter?row=1"&gt;https://huggingface.co/datasets/xywang1/OpenCharacter?row=1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kind_Ad_2866"&gt; /u/Kind_Ad_2866 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihu6s2/need_help_with_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihu6s2/need_help_with_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihu6s2/need_help_with_training/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T22:15:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihnhpl</id>
    <title>Whats the best open source model for video generation?</title>
    <updated>2025-02-04T17:43:10+00:00</updated>
    <author>
      <name>/u/gl2101</name>
      <uri>https://old.reddit.com/user/gl2101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im currently running a 3060 setup but planning to upgrade to a more powerful GPU. &lt;/p&gt; &lt;p&gt;My main goal is to build ai videos but I don’t know where to start. &lt;/p&gt; &lt;p&gt;Any recommendations are greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gl2101"&gt; /u/gl2101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihnhpl/whats_the_best_open_source_model_for_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihnhpl/whats_the_best_open_source_model_for_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihnhpl/whats_the_best_open_source_model_for_video/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T17:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihibp9</id>
    <title>Is Wikipedia RAG possible entirely locally with a gaming machine?</title>
    <updated>2025-02-04T14:02:46+00:00</updated>
    <author>
      <name>/u/trichofobia</name>
      <uri>https://old.reddit.com/user/trichofobia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y'all, I'm super super new, so I'm sorry if this is a stupid question, but I just heard what RAG is, I'd like to improve a local model (I'm only really familiar with deepseek, but I understand that ollama is great with RAG) with RAG.&lt;/p&gt; &lt;p&gt;I'd like to download Wikipedia locally, and use that for RAG. I've got a passable gaming laptop I don't use which has 32gb RAM, an RTX 3070 and an i7, along with an SSD.&lt;/p&gt; &lt;p&gt;I know I can download Wikipedia without images and it's something like 12-17gb. Would a local LLM be capable of searching through it automatically and choosing the best 2-3 articles based on my question? Or am I opening a can of worms?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/trichofobia"&gt; /u/trichofobia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihibp9/is_wikipedia_rag_possible_entirely_locally_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihibp9/is_wikipedia_rag_possible_entirely_locally_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihibp9/is_wikipedia_rag_possible_entirely_locally_with_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T14:02:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1iht2tf</id>
    <title>How To Setup DeepSeek-R1 LLM AI ChatBot Using Ollama On An Ubuntu Linux GPU Cloud Server (VPS)</title>
    <updated>2025-02-04T21:29:42+00:00</updated>
    <author>
      <name>/u/websplaining</name>
      <uri>https://old.reddit.com/user/websplaining</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iht2tf/how_to_setup_deepseekr1_llm_ai_chatbot_using/"&gt; &lt;img alt="How To Setup DeepSeek-R1 LLM AI ChatBot Using Ollama On An Ubuntu Linux GPU Cloud Server (VPS)" src="https://external-preview.redd.it/uNOHV2Maw2LzcoSfFMG6sju1JNogiBy71eR2QYEqjOQ.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbe5a0f4cb1ca54f001e5f84b38b48e405eb752e" title="How To Setup DeepSeek-R1 LLM AI ChatBot Using Ollama On An Ubuntu Linux GPU Cloud Server (VPS)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/websplaining"&gt; /u/websplaining &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/S_JEkuE9EyU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iht2tf/how_to_setup_deepseekr1_llm_ai_chatbot_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iht2tf/how_to_setup_deepseekr1_llm_ai_chatbot_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T21:29:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihfyi2</id>
    <title>Ollama + DeepSeek + Obsidian = The Ultimate Offline AI Assistant!</title>
    <updated>2025-02-04T11:54:17+00:00</updated>
    <author>
      <name>/u/Kind-Industry-609</name>
      <uri>https://old.reddit.com/user/Kind-Industry-609</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ihfyi2/ollama_deepseek_obsidian_the_ultimate_offline_ai/"&gt; &lt;img alt="Ollama + DeepSeek + Obsidian = The Ultimate Offline AI Assistant!" src="https://external-preview.redd.it/32JgoJVP2Vxa0PebR1pmCtaV_33XwoDfHhsNkStqIjE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49a27f19a9e63b070eb41d43f95929897b33eb6f" title="Ollama + DeepSeek + Obsidian = The Ultimate Offline AI Assistant!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kind-Industry-609"&gt; /u/Kind-Industry-609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/qAsGO5N7OCk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihfyi2/ollama_deepseek_obsidian_the_ultimate_offline_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihfyi2/ollama_deepseek_obsidian_the_ultimate_offline_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T11:54:17+00:00</published>
  </entry>
</feed>
