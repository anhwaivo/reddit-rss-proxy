<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-04-23T21:06:10+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1k4fcwj</id>
    <title>built-in benchmark</title>
    <updated>2025-04-21T14:48:01+00:00</updated>
    <author>
      <name>/u/rorowhat</name>
      <uri>https://old.reddit.com/user/rorowhat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does Ollama have a benchmark tool similar to llama.cpp(llama-bench)? I looked at the docs, but nothing jumped out. Maybe I missed it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rorowhat"&gt; /u/rorowhat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4fcwj/builtin_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4fcwj/builtin_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k4fcwj/builtin_benchmark/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T14:48:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4orc5</id>
    <title>MHKetbi/ nvidia_Llama-3.3-Nemotron-Super-49B-v1</title>
    <updated>2025-04-21T21:10:09+00:00</updated>
    <author>
      <name>/u/Timziito</name>
      <uri>https://old.reddit.com/user/Timziito</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This Model keep crashing my Ollama docker.. what am i doing wrong i got 48gb vram..&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/MHKetbi"&gt;MHKetbi&lt;/a&gt;/&lt;a href="https://ollama.com/MHKetbi/nvidia_Llama-3.3-Nemotron-Super-49B-v1"&gt;nvidia_Llama-3.3-Nemotron-Super-49B-v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Timziito"&gt; /u/Timziito &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4orc5/mhketbi_nvidia_llama33nemotronsuper49bv1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4orc5/mhketbi_nvidia_llama33nemotronsuper49bv1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k4orc5/mhketbi_nvidia_llama33nemotronsuper49bv1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T21:10:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4f2bl</id>
    <title>Is there a good way to pass JSON input instead of raw text ?</title>
    <updated>2025-04-21T14:35:39+00:00</updated>
    <author>
      <name>/u/Unique-Algae-1145</name>
      <uri>https://old.reddit.com/user/Unique-Algae-1145</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want the input to be a JSON because I want to pass multiple paramaters (~5-10) but writing them into a sentence the model has some issues and often either ignores or sometimes replies in the format back (but not consistently enough to extract) or sees it as raw text. If possible I would like to pass a very similar format to the structured output.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unique-Algae-1145"&gt; /u/Unique-Algae-1145 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4f2bl/is_there_a_good_way_to_pass_json_input_instead_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4f2bl/is_there_a_good_way_to_pass_json_input_instead_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k4f2bl/is_there_a_good_way_to_pass_json_input_instead_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T14:35:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1k45btm</id>
    <title>Ollama vs Docker Model Runner - Which One Should You Use?</title>
    <updated>2025-04-21T04:44:00+00:00</updated>
    <author>
      <name>/u/Arindam_200</name>
      <uri>https://old.reddit.com/user/Arindam_200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been exploring local LLM runners lately and wanted to share a quick comparison of two popular options: &lt;strong&gt;Docker Model Runner&lt;/strong&gt; and &lt;strong&gt;Ollama&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;If you're deciding between them, here’s a no-fluff breakdown based on dev experience, API support, hardware compatibility, and more:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Dev Workflow Integration&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Docker Model Runner:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Feels native if you’re already living in Docker-land.&lt;/li&gt; &lt;li&gt;Models are packaged as OCI artifacts and distributed via Docker Hub.&lt;/li&gt; &lt;li&gt;Works seamlessly with Docker Desktop as part of a bigger dev environment.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ollama:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Super lightweight and easy to set up.&lt;/li&gt; &lt;li&gt;Works as a standalone tool, no Docker needed.&lt;/li&gt; &lt;li&gt;Great for folks who want to skip the container overhead.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;Model Availability &amp;amp; Customisation&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Docker Model Runner:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Offers pre-packaged models through a dedicated AI namespace on Docker Hub.&lt;/li&gt; &lt;li&gt;Customization isn’t a big focus (yet), more plug-and-play with trusted sources.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ollama:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tons of models are readily available.&lt;/li&gt; &lt;li&gt;Built for tinkering: Model files let you customize and fine-tune behavior.&lt;/li&gt; &lt;li&gt;Also supports importing &lt;code&gt;GGUF&lt;/code&gt; and &lt;code&gt;Safetensors&lt;/code&gt; formats.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;API &amp;amp; Integrations&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Docker Model Runner:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Offers OpenAI-compatible API (great if you’re porting from the cloud).&lt;/li&gt; &lt;li&gt;Access via Docker flow using a Unix socket or TCP endpoint.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ollama:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Super simple REST API for generation, chat, embeddings, etc.&lt;/li&gt; &lt;li&gt;Has OpenAI-compatible APIs.&lt;/li&gt; &lt;li&gt;Big ecosystem of language SDKs (Python, JS, Go… you name it).&lt;/li&gt; &lt;li&gt;Popular with LangChain, LlamaIndex, and community-built UIs.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;Performance &amp;amp; Platform Support&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Docker Model Runner:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Optimized for Apple Silicon (macOS).&lt;/li&gt; &lt;li&gt;GPU acceleration via Apple Metal.&lt;/li&gt; &lt;li&gt;Windows support (with NVIDIA GPU) is coming in April 2025.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ollama:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cross-platform: Works on macOS, Linux, and Windows.&lt;/li&gt; &lt;li&gt;Built on &lt;code&gt;llama.cpp&lt;/code&gt;, tuned for performance.&lt;/li&gt; &lt;li&gt;Well-documented hardware requirements.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;Community &amp;amp; Ecosystem&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Docker Model Runner:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Still new, but growing fast thanks to Docker’s enterprise backing.&lt;/li&gt; &lt;li&gt;Strong on standards (OCI), great for model versioning and portability.&lt;/li&gt; &lt;li&gt;Good choice for orgs already using Docker.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ollama:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Established open-source project with a huge community.&lt;/li&gt; &lt;li&gt;200+ third-party integrations.&lt;/li&gt; &lt;li&gt;Active Discord, GitHub, Reddit, and more.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;-&amp;gt; TL;DR – Which One Should You Pick?&lt;/p&gt; &lt;p&gt;Go with Docker Model Runner if:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You’re already deep into Docker.&lt;/li&gt; &lt;li&gt;You want OpenAI API compatibility.&lt;/li&gt; &lt;li&gt;You care about standardization and container-based workflows.&lt;/li&gt; &lt;li&gt;You’re on macOS (Apple Silicon).&lt;/li&gt; &lt;li&gt;You need a solution with enterprise vibes.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Go with Ollama if:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You want a standalone tool with minimal setup.&lt;/li&gt; &lt;li&gt;You love customizing models and tweaking behaviors.&lt;/li&gt; &lt;li&gt;You need community plugins or multimodal support.&lt;/li&gt; &lt;li&gt;You’re using LangChain or LlamaIndex.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;BTW, I made a video on how to use Docker Model Runner step-by-step,&lt;/strong&gt; might help if you’re just starting out or curious about trying it: &lt;a href="https://www.youtube.com/watch?v=RH_vdF2iGdo"&gt;Watch Now&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you’re using and why!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arindam_200"&gt; /u/Arindam_200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k45btm/ollama_vs_docker_model_runner_which_one_should/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k45btm/ollama_vs_docker_model_runner_which_one_should/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k45btm/ollama_vs_docker_model_runner_which_one_should/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T04:44:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4d2n4</id>
    <title>Which ollama model would you choose for chatbot ?</title>
    <updated>2025-04-21T13:07:14+00:00</updated>
    <author>
      <name>/u/Effective_Budget7594</name>
      <uri>https://old.reddit.com/user/Effective_Budget7594</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have to create a chatbot with ollama in Msty. I am using llama3.1:8b with mxbai-embed-large. I am giving to the model markdown files with the instructions and the answers that it should give to the questions and also the questions and how to solve problems. The chatbot has to solve customers questions like: how to vinculate the device with the phone or general questions like how much it's cost. Sometimes, the model invents the response even if I put in prompt to use only the files that I give. Could someone give some advices, models, parameters to improve it ? Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective_Budget7594"&gt; /u/Effective_Budget7594 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4d2n4/which_ollama_model_would_you_choose_for_chatbot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4d2n4/which_ollama_model_would_you_choose_for_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k4d2n4/which_ollama_model_would_you_choose_for_chatbot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T13:07:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4apac</id>
    <title>Why Gemma3-4b QAT from ollama website uses twice a much memory versus GGUF</title>
    <updated>2025-04-21T10:58:15+00:00</updated>
    <author>
      <name>/u/ShineNo147</name>
      <uri>https://old.reddit.com/user/ShineNo147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k4apac/why_gemma34b_qat_from_ollama_website_uses_twice_a/"&gt; &lt;img alt="Why Gemma3-4b QAT from ollama website uses twice a much memory versus GGUF" src="https://b.thumbs.redditmedia.com/jS-ZHEQjDn6MrQ7gy5Yd2f_IGmgbzjtW6gZAr08LSAU.jpg" title="Why Gemma3-4b QAT from ollama website uses twice a much memory versus GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay let me rephrase my question Why Gemma3-4b QAT from ollama uses twice a much ram versus GGUF ?&lt;/p&gt; &lt;p&gt;I used ollama run gemma3:4b-it-qat and ollama run hf.co/lmstudio-community/gemma-3-4B-it-qat-GGUF:latest.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sdv0kj3v56we1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34bb0bb13d7fea900ae31c4f2ee9832f16c13a13"&gt;https://preview.redd.it/sdv0kj3v56we1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34bb0bb13d7fea900ae31c4f2ee9832f16c13a13&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dscp7iiw56we1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0bdadadab6290febbf2b1400f69de9b97206688d"&gt;https://preview.redd.it/dscp7iiw56we1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0bdadadab6290febbf2b1400f69de9b97206688d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShineNo147"&gt; /u/ShineNo147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4apac/why_gemma34b_qat_from_ollama_website_uses_twice_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4apac/why_gemma34b_qat_from_ollama_website_uses_twice_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k4apac/why_gemma34b_qat_from_ollama_website_uses_twice_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T10:58:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4aibo</id>
    <title>Are there any good LLMs with 1B or fewer parameters for RAG models?</title>
    <updated>2025-04-21T10:45:49+00:00</updated>
    <author>
      <name>/u/armodrilo10</name>
      <uri>https://old.reddit.com/user/armodrilo10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;br /&gt; I'm working on building a RAG model and I'm aiming to keep it under 1B parameters. The context document I’ll be working with is fairly small, only about 100-200 lines so I don’t need a massive model (like a 4B or 7B parameter model).&lt;/p&gt; &lt;p&gt;Additionally, I’m looking to host the model for free, so keeping it under 1B is a must. Does anyone know of any good LLMs with 1B parameters or fewer that would work well for this kind of use case? If there’s a platform or space where I can compare smaller models, I’d appreciate that info as well!&lt;/p&gt; &lt;p&gt;Thanks in advance for any suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/armodrilo10"&gt; /u/armodrilo10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4aibo/are_there_any_good_llms_with_1b_or_fewer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4aibo/are_there_any_good_llms_with_1b_or_fewer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k4aibo/are_there_any_good_llms_with_1b_or_fewer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T10:45:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1k49h4s</id>
    <title>Why ollama Gemma3:4b QAT uses almost 6GB Memory when LM studio google GGUF uses around 3GB</title>
    <updated>2025-04-21T09:35:47+00:00</updated>
    <author>
      <name>/u/ShineNo147</name>
      <uri>https://old.reddit.com/user/ShineNo147</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;As question above &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShineNo147"&gt; /u/ShineNo147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k49h4s/why_ollama_gemma34b_qat_uses_almost_6gb_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k49h4s/why_ollama_gemma34b_qat_uses_almost_6gb_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k49h4s/why_ollama_gemma34b_qat_uses_almost_6gb_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-21T09:35:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4wlry</id>
    <title>(openshift) - ollama model directory is empty in openshift but podman model directory is ok.</title>
    <updated>2025-04-22T03:23:15+00:00</updated>
    <author>
      <name>/u/Shot_Shallot7446</name>
      <uri>https://old.reddit.com/user/Shot_Shallot7446</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to deploy ollama on openshift in the closed network environment. &lt;/p&gt; &lt;p&gt;I created pulled model ollama for the usage. &lt;/p&gt; &lt;p&gt;podman works well but when I deploy the image to the openshift, model directory is emptry. Is this normal? &lt;/p&gt; &lt;p&gt;Here is my dockerfile: &lt;/p&gt; &lt;p&gt;FROM ollama/ollama&lt;/p&gt; &lt;p&gt;ENV OLLAMA_MODELS=/.ollama/models&lt;/p&gt; &lt;p&gt;RUN ollama serve &amp;amp; server=$! ; sleep 2 ; ollama pull llama3.2&lt;/p&gt; &lt;p&gt;ENTRYPOINT [ &amp;quot;/bin/bash&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;(sleep 2 ; ) &amp;amp; exec /bin/ollama $0&amp;quot; ]&lt;/p&gt; &lt;p&gt;CMD [ &amp;quot;serve&amp;quot; ]&lt;/p&gt; &lt;p&gt;~&lt;/p&gt; &lt;p&gt;podman works find with &amp;quot;ollama list &amp;quot;&lt;/p&gt; &lt;p&gt;However when this image is deployed to the openshift:&lt;/p&gt; &lt;p&gt;1000720000@ollamamodel-69945bd659-pkpgf:/.ollama/models/manifests$ exit&lt;/p&gt; &lt;p&gt;exit&lt;/p&gt; &lt;p&gt;[root@bastion doy]# oc exec -it ollamamodel-69945bd659-pkpgf -- bash&lt;/p&gt; &lt;p&gt;groups: cannot find name for group ID 1000720000&lt;/p&gt; &lt;p&gt;1000720000@ollamamodel-69945bd659-pkpgf:/$ ls -al /.ollama/models/manifests/*&lt;/p&gt; &lt;p&gt;ls: cannot access '/.ollama/models/manifests/*': No such file or directory&lt;/p&gt; &lt;p&gt;1000720000@ollamamodel-69945bd659-pkpgf:/$ ls -al /.ollama/models/manifests/&lt;/p&gt; &lt;p&gt;total 0&lt;/p&gt; &lt;p&gt;drwxr-sr-x. 2 1000720000 1000720000 0 Apr 22 03:00 .&lt;/p&gt; &lt;p&gt;drwxrwsr-x. 4 1000720000 1000720000 2 Apr 22 03:00 ..&lt;/p&gt; &lt;p&gt;1000720000@ollamamodel-69945bd659-pkpgf:/$&lt;/p&gt; &lt;p&gt;$ podman exec -it 1d2f43e64693 bash&lt;/p&gt; &lt;p&gt;1d2f43e64693 localhost/ollamamodel:latest serve 2 hours ago Up About an hour ollamamodel&lt;/p&gt; &lt;p&gt;[root@bastion doy]# podman exec -it 1d2f43e64693 bash&lt;/p&gt; &lt;p&gt;root@1d2f43e64693:/# ls /.ollama/models/manifests/&lt;/p&gt; &lt;p&gt;&lt;a href="http://registry.ollama.ai"&gt;registry.ollama.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;----&lt;/p&gt; &lt;p&gt;Is there anyone who was successful with pulled model ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shot_Shallot7446"&gt; /u/Shot_Shallot7446 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4wlry/openshift_ollama_model_directory_is_empty_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4wlry/openshift_ollama_model_directory_is_empty_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k4wlry/openshift_ollama_model_directory_is_empty_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-22T03:23:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k56ux8</id>
    <title>How to run locally</title>
    <updated>2025-04-22T13:53:00+00:00</updated>
    <author>
      <name>/u/No-One9018</name>
      <uri>https://old.reddit.com/user/No-One9018</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running Dolphin-Llama3:8b in my terminal with Ollama. When I ask the AI if it's running locally or connected to the Internet, it says it's connected to the Internet. Is there some step I miss&lt;/p&gt; &lt;p&gt;i figured it out guys thanks to you all. appreciate it!!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-One9018"&gt; /u/No-One9018 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k56ux8/how_to_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k56ux8/how_to_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k56ux8/how_to_run_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-22T13:53:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5cj6a</id>
    <title>completely obedient ai</title>
    <updated>2025-04-22T17:44:08+00:00</updated>
    <author>
      <name>/u/No-One9018</name>
      <uri>https://old.reddit.com/user/No-One9018</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there an AI model that is completely obedient and does as you say, but still performs well and provides a good experience? I've tried a lot of AI models and dolphin ones, but they just don't do what I want them to do.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-One9018"&gt; /u/No-One9018 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k5cj6a/completely_obedient_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k5cj6a/completely_obedient_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k5cj6a/completely_obedient_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-22T17:44:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5e4v2</id>
    <title>Local AI tax form reader to excel</title>
    <updated>2025-04-22T18:47:24+00:00</updated>
    <author>
      <name>/u/TwistNecessary7182</name>
      <uri>https://old.reddit.com/user/TwistNecessary7182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've experimented with streamlit trying to make a tax form reader. Used ollama seems the easiest to program with python. Also used lawma index with Obama. It's sort of clunky but works. I'm just wondering does anybody know any other open source python or node projects out there to have the AI scan tax forums or could be receipts. Then put them into Excel based a prompt?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TwistNecessary7182"&gt; /u/TwistNecessary7182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k5e4v2/local_ai_tax_form_reader_to_excel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k5e4v2/local_ai_tax_form_reader_to_excel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k5e4v2/local_ai_tax_form_reader_to_excel/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-22T18:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1k52ymn</id>
    <title>Gemma3 27b QAT: impossible to change context size ?</title>
    <updated>2025-04-22T10:28:01+00:00</updated>
    <author>
      <name>/u/yeswearecoding</name>
      <uri>https://old.reddit.com/user/yeswearecoding</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yeswearecoding"&gt; /u/yeswearecoding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1k51ycf/gemma3_27b_qat_impossible_to_change_context_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k52ymn/gemma3_27b_qat_impossible_to_change_context_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k52ymn/gemma3_27b_qat_impossible_to_change_context_size/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-22T10:28:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4voqw</id>
    <title>I uploaded GLM-4-32B-0414 to ollama</title>
    <updated>2025-04-22T02:34:30+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.ollama.com/JollyLlama/GLM-4-32B-0414-Q4_K_M"&gt;https://www.ollama.com/JollyLlama/GLM-4-32B-0414-Q4_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama run JollyLlama/GLM-4-32B-0414-Q4_K_M&lt;/code&gt;&lt;/p&gt; &lt;h3&gt;&lt;strong&gt;&lt;em&gt;This model requires Ollama v0.6.6 or later.&lt;/em&gt;&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama/releases"&gt;https://github.com/ollama/ollama/releases&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Update:&lt;/p&gt; &lt;p&gt;Z1 reasoning model: &lt;/p&gt; &lt;p&gt;ollama run JollyLlama/GLM-Z1-32B-0414-Q4_K_M&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4voqw/i_uploaded_glm432b0414_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4voqw/i_uploaded_glm432b0414_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k4voqw/i_uploaded_glm432b0414_to_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-22T02:34:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4z2to</id>
    <title>MCP client for ollama</title>
    <updated>2025-04-22T05:50:16+00:00</updated>
    <author>
      <name>/u/slow-dash</name>
      <uri>https://old.reddit.com/user/slow-dash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/mihirrd/ollama-mcp-client"&gt;https://github.com/mihirrd/ollama-mcp-client&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slow-dash"&gt; /u/slow-dash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4z2to/mcp_client_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k4z2to/mcp_client_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k4z2to/mcp_client_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-22T05:50:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5fkkv</id>
    <title>Ollama + Semantic Kernel?</title>
    <updated>2025-04-22T19:44:37+00:00</updated>
    <author>
      <name>/u/stingrayer</name>
      <uri>https://old.reddit.com/user/stingrayer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, Has anyone successfully built a project with Semantic Kernel / Kernel Memory frameworks with Ollama tool calling? If so did you have to customize the default prompts to get it working properly? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stingrayer"&gt; /u/stingrayer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k5fkkv/ollama_semantic_kernel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k5fkkv/ollama_semantic_kernel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k5fkkv/ollama_semantic_kernel/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-22T19:44:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5t94m</id>
    <title>Help with Setting Up MythoMax Model in Ollama</title>
    <updated>2025-04-23T07:26:33+00:00</updated>
    <author>
      <name>/u/Cyrar</name>
      <uri>https://old.reddit.com/user/Cyrar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to set up the &lt;strong&gt;MythoMax model&lt;/strong&gt; using Ollama on Windows, but I keep running into errors. I'm also trying to get it to work with Docker using the open-webui. This is what I've done so far:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Downloaded the MythoMax model (file: mythomax-l2-13b.Q4_K_M.gguf) from Hugging Face.&lt;/li&gt; &lt;li&gt;Placed it in the &lt;code&gt;C:\Users\USERNAME\.ollama\models\&lt;/code&gt; folder.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I believe the issue lies with the Modelfile. Whenever I try to integrate external models (such as MythoMax) using the Modelfile method I get errors. But when I simply pull a model that is officially supported (such as Llama3.2) it works with no problems.&lt;br /&gt; If anyone could help that would be great.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cyrar"&gt; /u/Cyrar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k5t94m/help_with_setting_up_mythomax_model_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k5t94m/help_with_setting_up_mythomax_model_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k5t94m/help_with_setting_up_mythomax_model_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-23T07:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5u4ug</id>
    <title>Tool call, and generating regular content</title>
    <updated>2025-04-23T08:31:42+00:00</updated>
    <author>
      <name>/u/omicronns</name>
      <uri>https://old.reddit.com/user/omicronns</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What would be a correct way to implement a feature of sort: generate some content and save it to file with tool call.&lt;/p&gt; &lt;p&gt;I see a lot of people complaining that, streaming doesn't work currently when tool call is being made, but I can't do that even without streaming. I created an example to illustrate, no streaming but no content is returned anyway. Am I doing something wrong? I can retrieve generated joke, when adding &lt;code&gt;content&lt;/code&gt; parameter to &lt;code&gt;save_file&lt;/code&gt; function, but when streaming will be working I would expect to retrieve generated content via regular responses anyway, since it may be large.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import ollama system_prompt = &amp;quot;&amp;quot;&amp;quot; you are a helpful assistant, do whatever user asks for when generating a file conform to format: &amp;lt;file path=&amp;quot;path to file&amp;quot;&amp;gt;file content&amp;lt;/file&amp;gt; &amp;quot;&amp;quot;&amp;quot; user_prompts = [ &amp;quot;generate a joke file, don't save it&amp;quot;, &amp;quot;generate a joke file, and save it to file: joke.txt&amp;quot; ] for user_prompt in user_prompts: rsp = ollama.chat( model=&amp;quot;qwen2.5-coder:14b-ctx24k&amp;quot;, messages=[ {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: system_prompt}, {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: user_prompt}, ], tools=[ { &amp;quot;type&amp;quot;: &amp;quot;function&amp;quot;, &amp;quot;function&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;save_file&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Save a file.&amp;quot;, &amp;quot;parameters&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;, &amp;quot;properties&amp;quot;: { &amp;quot;to&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Destination path&amp;quot;, }, }, &amp;quot;required&amp;quot;: [&amp;quot;to&amp;quot;], }, }, } ], ) print(rsp) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;output:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model='qwen2.5-coder:14b-ctx24k' created_at='2025-04-23T08:32:51.843030683Z' done=True done_reason='stop' total_duration=4339273919 load_duration=11283855 prompt_eval_count=178 prompt_eval_duration=313627121 eval_count=25 eval_duration=4011239016 message=Message(role='assistant', content='&amp;lt;file path=&amp;quot;joke.txt&amp;quot;&amp;gt;Why did the tomato turn red? Because it saw the salad dressing!&amp;lt;/file&amp;gt;', images=None, tool_calls=None) model='qwen2.5-coder:14b-ctx24k' created_at='2025-04-23T08:33:00.286117086Z' done=True done_reason='stop' total_duration=8441806782 load_duration=11481315 prompt_eval_count=182 prompt_eval_duration=422891295 eval_count=49 eval_duration=8005001117 message=Message(role='assistant', content='', images=None, tool_calls=[ToolCall(function=Function(name='save_file', arguments={'to': 'joke.txt'}))]) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omicronns"&gt; /u/omicronns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k5u4ug/tool_call_and_generating_regular_content/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k5u4ug/tool_call_and_generating_regular_content/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k5u4ug/tool_call_and_generating_regular_content/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-23T08:31:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5o4ft</id>
    <title>I Built a Tool to Judge AI with AI</title>
    <updated>2025-04-23T02:14:12+00:00</updated>
    <author>
      <name>/u/Any-Cockroach-3233</name>
      <uri>https://old.reddit.com/user/Any-Cockroach-3233</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Agentic systems are wild. You can’t unit test chaos.&lt;/p&gt; &lt;p&gt;With agents being non-deterministic, traditional testing just doesn’t cut it. So, how do you measure output quality, compare prompts, or evaluate models?&lt;/p&gt; &lt;p&gt;You let an LLM be the judge.&lt;/p&gt; &lt;p&gt;Introducing Evals - LLM as a Judge&lt;br /&gt; A minimal, powerful framework to evaluate LLM outputs using LLMs themselves&lt;/p&gt; &lt;p&gt;✅ Define custom criteria (accuracy, clarity, depth, etc)&lt;br /&gt; ✅ Score on a consistent 1–5 or 1–10 scale&lt;br /&gt; ✅ Get reasoning for every score&lt;br /&gt; ✅ Run batch evals &amp;amp; generate analytics with 2 lines of code&lt;/p&gt; &lt;p&gt;🔧 Built for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agent debugging&lt;/li&gt; &lt;li&gt;Prompt engineering&lt;/li&gt; &lt;li&gt;Model comparisons&lt;/li&gt; &lt;li&gt;Fine-tuning feedback loops&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Star the repository if you wish to: &lt;a href="https://github.com/manthanguptaa/real-world-llm-apps"&gt;https://github.com/manthanguptaa/real-world-llm-apps&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Cockroach-3233"&gt; /u/Any-Cockroach-3233 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k5o4ft/i_built_a_tool_to_judge_ai_with_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k5o4ft/i_built_a_tool_to_judge_ai_with_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k5o4ft/i_built_a_tool_to_judge_ai_with_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-23T02:14:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5v1e4</id>
    <title>Integrating a fully local Ollama setup with Facebook Business Chat (privacy‑first, no external APIs)?</title>
    <updated>2025-04-23T09:37:49+00:00</updated>
    <author>
      <name>/u/dnhanhtai0147</name>
      <uri>https://old.reddit.com/user/dnhanhtai0147</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;br /&gt; I’d like to ask if there’s a way to integrate a local instance of Ollama into replying to customers on Facebook Business Chat. I know there are many websites that support webhooks with a generous amount of API calls, but my customers’ messages must remain confidential, so I want 100 % local processing.&lt;br /&gt; All I need is to use a previously trained dataset to answer customer inquiries, and if a customer agrees to book an appointment, the system should report that back to me.&lt;br /&gt; Sorry, I’m still learning about self‑hosting AI, so please excuse any mistakes. Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dnhanhtai0147"&gt; /u/dnhanhtai0147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k5v1e4/integrating_a_fully_local_ollama_setup_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k5v1e4/integrating_a_fully_local_ollama_setup_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k5v1e4/integrating_a_fully_local_ollama_setup_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-23T09:37:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5q6wt</id>
    <title>Coding CLI agent with ollama support</title>
    <updated>2025-04-23T04:05:05+00:00</updated>
    <author>
      <name>/u/amritk110</name>
      <uri>https://old.reddit.com/user/amritk110</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alternative to codex and Claude code. &lt;a href="https://github.com/amrit110/oli"&gt;https://github.com/amrit110/oli&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amritk110"&gt; /u/amritk110 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k5q6wt/coding_cli_agent_with_ollama_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k5q6wt/coding_cli_agent_with_ollama_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k5q6wt/coding_cli_agent_with_ollama_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-23T04:05:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5kn1o</id>
    <title>Calorie Tracking with Llama3.2 Vision and Ollama</title>
    <updated>2025-04-22T23:20:20+00:00</updated>
    <author>
      <name>/u/oridnary_artist</name>
      <uri>https://old.reddit.com/user/oridnary_artist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k5kn1o/calorie_tracking_with_llama32_vision_and_ollama/"&gt; &lt;img alt="Calorie Tracking with Llama3.2 Vision and Ollama" src="https://external-preview.redd.it/OGcweHlneTh6Z3dlMXVmvc9IhjOrTJG9u9rcsNycF0DqmL-zrOFT8kCBbCOt.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e6c7726e7303ae52db24ac37f21f1928f1da454" title="Calorie Tracking with Llama3.2 Vision and Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, I wanted to share a personal project I’ve been heads‑down on for the past few sprints. It started as a simple AI chat interface and has evolved into a full‑blown nutrition tracking dashboard—built entirely by me as part of &lt;strong&gt;FitAnalytics&lt;/strong&gt;, our AI‑powered fitness companion.&lt;/p&gt; &lt;h1&gt;What’s new?&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Macro Logging&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Now you can track protein, carbs, &lt;strong&gt;and&lt;/strong&gt; fat—alongside calories—for a complete picture of each meal.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;One‑Click Hydration&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Tired of forgetting to log water? We added quick‑add buttons so you hit your H₂O goal in no time.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Progress Bars for Motivation&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Dynamic bars fill up as you log. Seeing that little green/gold/rose slider move is surprisingly addictive.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;“Chat‑to‑Log” Prototype&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Snap a photo of your food, let the AI estimate macros, then tap to log it. Still experimental, but it’s already cutting manual entry way down.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cleaner UI/UX&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Meal grouping, modal pop‑ups, and date navigation powered by Tailwind CSS + Headless UI + Framer Motion. Feels snappy and organized.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I will be releasing the code over here in the next few days : &lt;a href="https://github.com/Pavankunchala/LLM-Learn-PK"&gt;https://github.com/Pavankunchala/LLM-Learn-PK&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;The Stack&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Frontend: React + TypeScript + TanStack Query&lt;/li&gt; &lt;li&gt;Backend: Python (Flask) + SQLite&lt;/li&gt; &lt;li&gt;AI: Ollama/Agno for image &amp;amp; text parsing&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;I’d love your feedback!&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;What’s your biggest pain point with diet‑tracking apps?&lt;/li&gt; &lt;li&gt;Would you try a “photo log” feature if it worked reliably?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Bonus:&lt;/strong&gt; I’m also currently looking for roles in &lt;strong&gt;Computer Vision&lt;/strong&gt; &amp;amp; &lt;strong&gt;LLMs&lt;/strong&gt;. If your team needs a full‑stack engineer who’s obsessed with AI and user‑focused product design, feel free to DM me or reach out at [&lt;a href="mailto:pavankunchalaofficial@gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;](mailto:&lt;a href="mailto:pavankunchalaofficial@gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;). Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oridnary_artist"&gt; /u/oridnary_artist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fymqbey8zgwe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k5kn1o/calorie_tracking_with_llama32_vision_and_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k5kn1o/calorie_tracking_with_llama32_vision_and_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-22T23:20:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1k67utu</id>
    <title>What does your model output? Any preference between these four?</title>
    <updated>2025-04-23T19:21:47+00:00</updated>
    <author>
      <name>/u/BlaiseLabs</name>
      <uri>https://old.reddit.com/user/BlaiseLabs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k67utu/what_does_your_model_output_any_preference/"&gt; &lt;img alt="What does your model output? Any preference between these four?" src="https://preview.redd.it/pwl86tg0a7we1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84f671977c1dd2e4a7ce81d0a00f0a3652361013" title="What does your model output? Any preference between these four?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlaiseLabs"&gt; /u/BlaiseLabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pwl86tg0a7we1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k67utu/what_does_your_model_output_any_preference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k67utu/what_does_your_model_output_any_preference/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-23T19:21:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5wk61</id>
    <title>Writeopia - I create many new text edition Ollama integrations</title>
    <updated>2025-04-23T11:15:39+00:00</updated>
    <author>
      <name>/u/lehen01</name>
      <uri>https://old.reddit.com/user/lehen01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k5wk61/writeopia_i_create_many_new_text_edition_ollama/"&gt; &lt;img alt="Writeopia - I create many new text edition Ollama integrations" src="https://external-preview.redd.it/ZmgzdjVnZTJoa3dlMUQi7Qy7nz6UT6WYXAo9UF2D51W9JLNN2nTM2PGUXdO8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f45a9b73a871253ce763883a7c2c33325b7709ac" title="Writeopia - I create many new text edition Ollama integrations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello hello,&lt;/p&gt; &lt;p&gt;I month ago I posted here about Writeopia, a text editor with integration with Ollama. The reception was super good, and many of you gave super nice feedback and started using it. &lt;/p&gt; &lt;p&gt;I would like to update that the project is evolving and new features are available! You can now just write the structure of the text that you would like to have and click the magic wand to let the model generate the text for you. Instead of generating everything, it goes piece by piece so you can evaluate if it is going in the right direction. &lt;/p&gt; &lt;p&gt;We are working to add a RAG to it so the prompts have better context. Also, the Windows app is on its way, we are just waiting to get a Windows account approved. &lt;/p&gt; &lt;p&gt;Website: &lt;a href="https://writeopia.io"&gt;https://writeopia.io&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Writeopia/Writeopia"&gt;https://github.com/Writeopia/Writeopia&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Feedback about the project is greatly appreciated! We would love to hear how we can integrate Ollama in nicer ways =]. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lehen01"&gt; /u/lehen01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vtt0lfe2hkwe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k5wk61/writeopia_i_create_many_new_text_edition_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k5wk61/writeopia_i_create_many_new_text_edition_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-23T11:15:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1k674xf</id>
    <title>Free Ollama GPU!</title>
    <updated>2025-04-23T18:52:45+00:00</updated>
    <author>
      <name>/u/guuidx</name>
      <uri>https://old.reddit.com/user/guuidx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you run this on Google Collab, you have a free Ollama running GPU!&lt;/p&gt; &lt;p&gt;Do not forgot to enable the GPU in the right upper corner of the Google Collab screen, by clicking on CPU/MEM.&lt;/p&gt; &lt;p&gt;!curl -fsSL &lt;a href="https://molodetz.nl/retoor/uberlama/raw/branch/main/ollama-colab-v2.sh"&gt;https://molodetz.nl/retoor/uberlama/raw/branch/main/ollama-colab-v2.sh&lt;/a&gt; | sh&lt;/p&gt; &lt;p&gt;Read the full script here, and about how to use your Ollama model: &lt;a href="https://molodetz.nl/project/uberlama/ollama-colab-v2.sh.html"&gt;https://molodetz.nl/project/uberlama/ollama-colab-v2.sh.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The idea was not mine, I've read some blog post that gave me the idea.&lt;/p&gt; &lt;p&gt;But the blog post required many steps and had several dependencies.&lt;/p&gt; &lt;p&gt;Mine only has one (Python) dependency: aiohttp. That one gets installed by the script automatically.&lt;/p&gt; &lt;p&gt;To run a different model, you have to update the script. &lt;/p&gt; &lt;p&gt;The whole Ollama hub including server (hub itself) is Open Source.&lt;/p&gt; &lt;p&gt;If you have questions, send me a PM. I like to talk about programming. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/guuidx"&gt; /u/guuidx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k674xf/free_ollama_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k674xf/free_ollama_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k674xf/free_ollama_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-23T18:52:45+00:00</published>
  </entry>
</feed>
