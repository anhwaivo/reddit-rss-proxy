<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-06-11T07:07:24+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1l60vhh</id>
    <title>Librechat issues with ollama</title>
    <updated>2025-06-08T01:48:15+00:00</updated>
    <author>
      <name>/u/Large_Yams</name>
      <uri>https://old.reddit.com/user/Large_Yams</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone have advice for why librechat needs to remain in the foreground while responses are generating? As soon as I change apps for a few seconds, when I go back to librechat the output fails. I would've thought it would keep generating and show me the output when I open it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Large_Yams"&gt; /u/Large_Yams &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l60vhh/librechat_issues_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l60vhh/librechat_issues_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l60vhh/librechat_issues_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-08T01:48:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6763u</id>
    <title>20-30GB used memory despite all models are unloaded.</title>
    <updated>2025-06-08T08:20:51+00:00</updated>
    <author>
      <name>/u/Ne00n</name>
      <uri>https://old.reddit.com/user/Ne00n</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I did get a server to play around with ollama and open webui.&lt;br /&gt; Its nice to be able to unload and load models as you need them.&lt;/p&gt; &lt;p&gt;However, on bigger models, such as the 30B Qwen3, I run into errors.&lt;br /&gt; So, I tired to figure out, why, simple, I get an error message, that tells me I don't have enough free memory.&lt;/p&gt; &lt;p&gt;Which is wired, since no models are loaded, nothing runs, despite that, I see 34GB used memory of 64GB.&lt;br /&gt; Any ideas? Its not cached/buff, its used.&lt;/p&gt; &lt;p&gt;Restarting ollama doesn't fix it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ne00n"&gt; /u/Ne00n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l6763u/2030gb_used_memory_despite_all_models_are_unloaded/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l6763u/2030gb_used_memory_despite_all_models_are_unloaded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l6763u/2030gb_used_memory_despite_all_models_are_unloaded/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-08T08:20:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1l67bjm</id>
    <title>[In Development] Serene Pub, a simpler SillyTavern like roleplay client</title>
    <updated>2025-06-08T08:31:09+00:00</updated>
    <author>
      <name>/u/doolijb</name>
      <uri>https://old.reddit.com/user/doolijb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Ollama to roleplay for a while now. SillyTavern has been fantastic, but I've had some frustrations with it.&lt;/p&gt; &lt;p&gt;I've started developing my own application with the same copy-left license. I am at the point where I want to test the waters and get some feedback and gauge interest.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/doolijb/serene-pub/tree/main"&gt;&lt;strong&gt;Link to the project &amp;amp; screenshots&lt;/strong&gt;&lt;/a&gt; (It's in early alpha, it's not feature complete and there will be bugs.)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;About the project:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Serene Pub is a modern, customizable chat application designed for immersive roleplay and creative conversations.&lt;/p&gt; &lt;p&gt;This app is heavily inspired by Silly Tavern, with the objective of being more intuitive, responsive and simple to configure.&lt;/p&gt; &lt;p&gt;Primary concerns Serene Pub aims to address:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Reduce the number of nested menus and settings.&lt;/li&gt; &lt;li&gt;Reduced visual clutter.&lt;/li&gt; &lt;li&gt;Manage settings server-side to prevent configurations from changing because the user switched windows/devices.&lt;/li&gt; &lt;li&gt;Make API calls &amp;amp; chat completion requests asyncronously server-side so they process regardless of window/device state.&lt;/li&gt; &lt;li&gt;Use sockets for all data, the user will see the same information updated across all windows/devices.&lt;/li&gt; &lt;li&gt;Have compatibility with the majority of Silly Tavern import/exports, i.e. Character Cards&lt;/li&gt; &lt;li&gt;Overall be a well rounded app with a suite of features. Use SillyTavern if you want the most options, features and plugin-support.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;You can read more details in the readme, see the link above.&lt;/p&gt; &lt;p&gt;Thanks everyone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doolijb"&gt; /u/doolijb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l67bjm/in_development_serene_pub_a_simpler_sillytavern/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l67bjm/in_development_serene_pub_a_simpler_sillytavern/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l67bjm/in_development_serene_pub_a_simpler_sillytavern/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-08T08:31:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6d0ao</id>
    <title>C/ua Cloud Containers : Computer Use Agents in the Cloud</title>
    <updated>2025-06-08T14:07:59+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l6d0ao/cua_cloud_containers_computer_use_agents_in_the/"&gt; &lt;img alt="C/ua Cloud Containers : Computer Use Agents in the Cloud" src="https://preview.redd.it/b8ds7cgmnp5f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0be1af8d851814ae7cc1b453b112bb1a422faa96" title="C/ua Cloud Containers : Computer Use Agents in the Cloud" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First cloud platform built for Computer-Use Agents. Open-source backbone. Linux/Windows/macOS desktops in your browser. Works with OpenAI, Anthropic, or any LLM. Pay only for compute time.&lt;/p&gt; &lt;p&gt;Our beta users have deployed 1000s of agents over the past month. Available now in 3 tiers: Small (1 vCPU/4GB), Medium (2 vCPU/8GB), Large (8 vCPU/32GB). Windows &amp;amp; macOS coming soon.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt; ( We are open source !)&lt;/p&gt; &lt;p&gt;Cloud Platform : &lt;a href="https://www.trycua.com/blog/introducing-cua-cloud-containers"&gt;https://www.trycua.com/blog/introducing-cua-cloud-containers&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b8ds7cgmnp5f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l6d0ao/cua_cloud_containers_computer_use_agents_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l6d0ao/cua_cloud_containers_computer_use_agents_in_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-08T14:07:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6fb26</id>
    <title>Anyone else use a memory scrub with ollama?</title>
    <updated>2025-06-08T15:47:27+00:00</updated>
    <author>
      <name>/u/ETBiggs</name>
      <uri>https://old.reddit.com/user/ETBiggs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In testing I'm doing a lot of back to back batch runs in python and often Ollama hasn't completely unloaded before the next run. I created a memory scrub routine that kills the Ollama process and then scrubs the memory - as I am maxing out my memory I need that space - it sometimes clears ut to 7gb ram.&lt;/p&gt; &lt;p&gt;Helpful for avoiding weird intermittent issues when doing back to back testing for me. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ETBiggs"&gt; /u/ETBiggs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l6fb26/anyone_else_use_a_memory_scrub_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l6fb26/anyone_else_use_a_memory_scrub_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l6fb26/anyone_else_use_a_memory_scrub_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-08T15:47:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6676x</id>
    <title>spy-searcher: a open source local host deep research</title>
    <updated>2025-06-08T07:14:11+00:00</updated>
    <author>
      <name>/u/jasonhon2013</name>
      <uri>https://old.reddit.com/user/jasonhon2013</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone. I just love open source. While having the support of Ollama, we can somehow do the deep research with our local machine. I just finished one that is different to other that can write a long report i.e more than 1000 words instead of &amp;quot;deep research&amp;quot; that just have few hundreds words. &lt;/p&gt; &lt;p&gt;currently it is still undergoing develop and I really love your comment and any feature request will be appreciate !&lt;br /&gt; &lt;a href="https://github.com/JasonHonKL/spy-search/blob/main/README.md"&gt;https://github.com/JasonHonKL/spy-search/blob/main/README.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jasonhon2013"&gt; /u/jasonhon2013 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l6676x/spysearcher_a_open_source_local_host_deep_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l6676x/spysearcher_a_open_source_local_host_deep_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l6676x/spysearcher_a_open_source_local_host_deep_research/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-08T07:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6z6d3</id>
    <title>Anybody who can share experiences with Cohere AI Command A (64GB) model for Academic Use? (M4 max, 128gb)</title>
    <updated>2025-06-09T08:07:24+00:00</updated>
    <author>
      <name>/u/Bahaal_1981</name>
      <uri>https://old.reddit.com/user/Bahaal_1981</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am an academic in the social sciences, my use case is to use AI for thinking about problems, programming in R, helping me to (re)write, explain concepts to me, etc. I have no illusions that I can have a full RAG, where I feed it say a bunch of .pdfs and ask it about say the participants in each paper, but there was some RAG functionality mentioned in their example. That piqued my interest. I have an M4 Max with 128gb. Any academics who have used this model before I download the 64gb (yikes). How does it compare to models such as Deepseek / Gemma / Mistral large / Phi? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bahaal_1981"&gt; /u/Bahaal_1981 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l6z6d3/anybody_who_can_share_experiences_with_cohere_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l6z6d3/anybody_who_can_share_experiences_with_cohere_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l6z6d3/anybody_who_can_share_experiences_with_cohere_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-09T08:07:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6vfz1</id>
    <title>Help choosing PC parts</title>
    <updated>2025-06-09T04:07:02+00:00</updated>
    <author>
      <name>/u/Ttaywsenrak</name>
      <uri>https://old.reddit.com/user/Ttaywsenrak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there. I recently got screwed a bit. &lt;/p&gt; &lt;p&gt;I posted a few weeks ago about having some budget left over in a grant that I intended to use to build a local AI machine for kids to practice with in my classroom. &lt;/p&gt; &lt;p&gt;What ended up happening was I had the realization that I had an old 8700k, motherboard, and RAM collecting dust in a closet. I had just enough grant money left to snag some GPUs (sadly only 5070s, as everything else cost too much and 5070tis sold out the moment I went to order them) and they had to be brand new for warranty as its the school's stuff blah blah. &lt;/p&gt; &lt;p&gt;Bottom line is, my grant got me two 5070s, a 1200w psu, 1tb nvme, and some more RAM for the mobo. But, despite the mobo just sitting unused in a closet for the past year and working fine prior, it seems all the RAM slots are dead. This board has been RMAd twice for pcie slot failure, so I guess its finally dead. &lt;/p&gt; &lt;p&gt;But now here I am, with all the hardware to build this machine, minus a functioning motherboard. I could probably find a board to work with the 8700k, but then I'm paying 200+ for 10 year old hardware. But if I buy new, Im sunk even more money. I have some 14th gen i3s sitting around (computer building per the grant), so maybe grabbing a board for those? But then I get concerned about pcie lanes. &lt;/p&gt; &lt;p&gt;I could use some help here, this project was supposed to tidy up a use it or lose it grant, and now its going to cost me a few hundred out of pocket (already had to buy a case, too) just to make it work. &lt;/p&gt; &lt;p&gt;Should I buy an old motherboard, or a new one? Will I have enough PCIe lanes? &lt;/p&gt; &lt;p&gt;Thanks in advance, and if you made it this far thanks for reading. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ttaywsenrak"&gt; /u/Ttaywsenrak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l6vfz1/help_choosing_pc_parts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l6vfz1/help_choosing_pc_parts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l6vfz1/help_choosing_pc_parts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-09T04:07:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1l74jqx</id>
    <title>Hello peeps! I'm new to this. I need your insights</title>
    <updated>2025-06-09T13:16:37+00:00</updated>
    <author>
      <name>/u/in_the_pines__</name>
      <uri>https://old.reddit.com/user/in_the_pines__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The director of my current company wants me to learn ollama which is cool. &lt;/p&gt; &lt;p&gt;They are retail seller of computer monitors, printers, keyboards, cctv cameras. Mainly they take some projects from state government to setup cctv, computers etc at govt. sectors, also they have another wing of building govt. sites using Php. It's type of their family business. &lt;/p&gt; &lt;p&gt;The director really didn't give me any direction apart from asking me to learn how to use it to help in their business :')&lt;/p&gt; &lt;p&gt;Little background description of me: I've completed masters in physics last year, since then I've been learning data analytics and ML. &lt;/p&gt; &lt;p&gt;So any sort of advice, insights are welcome &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/in_the_pines__"&gt; /u/in_the_pines__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l74jqx/hello_peeps_im_new_to_this_i_need_your_insights/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l74jqx/hello_peeps_im_new_to_this_i_need_your_insights/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l74jqx/hello_peeps_im_new_to_this_i_need_your_insights/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-09T13:16:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7hksj</id>
    <title>GPU need help</title>
    <updated>2025-06-09T21:48:34+00:00</updated>
    <author>
      <name>/u/Informal_Catch_4688</name>
      <uri>https://old.reddit.com/user/Informal_Catch_4688</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm currently setting up my assistant everything works great using ollama but it uses my CPU on my windows which makes the response slow 30 seconds form stt whisper to an llama3 8b answer 0.00 to tts , thought I download llama.cpp it works on my GPU and get the answers in 1-4 seconds but this gives me an stupid answers so let's say I ask &amp;quot;how are you ? Then llama responds:&lt;/p&gt; &lt;p&gt;User : how are you ? Llama :I'm doing great # be professional &lt;/p&gt; &lt;p&gt;So TTS reads all of the line together with user and Lamma and # sometimes it goes and says &lt;/p&gt; &lt;p&gt;Python Python User : how are you ? Llama :I'm doing great # be professional user : looking for a new laptop(which I didn't even ask for I only asked how are you )&lt;/p&gt; &lt;p&gt;But that's Lamma.cpp I don't have any of those issues when using ollama but ollama doesn't use my NVIDIA GPU just my CPU &lt;/p&gt; &lt;p&gt;I know there's a way to use ollama on GPU without setting up wls2 &lt;/p&gt; &lt;p&gt;I'm using nvida GPU 12 vram &lt;/p&gt; &lt;p&gt;And I'm using llama3 8b Q4 k-l I think &lt;/p&gt; &lt;p&gt;Version of ollama Ollama version 0.9.0&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Informal_Catch_4688"&gt; /u/Informal_Catch_4688 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7hksj/gpu_need_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7hksj/gpu_need_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l7hksj/gpu_need_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-09T21:48:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7ae3g</id>
    <title>Ollama Email Assistant</title>
    <updated>2025-06-09T17:10:47+00:00</updated>
    <author>
      <name>/u/PleasantCandidate785</name>
      <uri>https://old.reddit.com/user/PleasantCandidate785</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use Zimbra for email. Is there a Chrome or Firefox plugin that can watch for new draft emails to be created, then automatically make grammar / tone suggestions automatically as the email is being written?&lt;/p&gt; &lt;p&gt;I saw the ObserveAI plugin posted earlier today that might be adapted to do what I need. I'd just prefer to avoid having to do a full screenshot, OCR, then process. Would be better if it could just pull the raw text that is being typed from the HTML or browser's memory or something and process that.&lt;/p&gt; &lt;p&gt;I know I could probably use AI to help me write a plugin, but I'm not a PC programmer. I don't even play one on TV. I can fake my way through writing a PERL script pretty good though. (I'm maybe a little better with embedded programming. Maybe.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PleasantCandidate785"&gt; /u/PleasantCandidate785 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7ae3g/ollama_email_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7ae3g/ollama_email_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l7ae3g/ollama_email_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-09T17:10:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1l703uz</id>
    <title>8B model of deepseek can't do the most simple things.</title>
    <updated>2025-06-09T09:11:57+00:00</updated>
    <author>
      <name>/u/DiligentLeader2383</name>
      <uri>https://old.reddit.com/user/DiligentLeader2383</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been playing around with some models. It can't even give a summary of a simple to do list.&lt;/p&gt; &lt;p&gt;I ask things like &amp;quot;What tasks still have to be done?&amp;quot; (There is a clear checklist in the file)&lt;/p&gt; &lt;p&gt;It can't even do that. It often misses many of them. &lt;/p&gt; &lt;p&gt;Is it because its a smaller 8B model, or am I missing something? How is it that it can't even spit out a simple to do list from a larger file, that explicitly has markdown check boxes for the stuff that has to be done. &lt;/p&gt; &lt;p&gt;anyway.. too many hours wasted on this..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DiligentLeader2383"&gt; /u/DiligentLeader2383 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l703uz/8b_model_of_deepseek_cant_do_the_most_simple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l703uz/8b_model_of_deepseek_cant_do_the_most_simple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l703uz/8b_model_of_deepseek_cant_do_the_most_simple/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-09T09:11:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1l75efz</id>
    <title>Run your browser agent with Browser Use and remote headless browsers</title>
    <updated>2025-06-09T13:53:41+00:00</updated>
    <author>
      <name>/u/BlitzBrowser_</name>
      <uri>https://old.reddit.com/user/BlitzBrowser_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l75efz/run_your_browser_agent_with_browser_use_and/"&gt; &lt;img alt="Run your browser agent with Browser Use and remote headless browsers" src="https://preview.redd.it/0i51zvqxnw5f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3c64aa2d913ff8001a0efa1e9e784d9fb505a8a" title="Run your browser agent with Browser Use and remote headless browsers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlitzBrowser_"&gt; /u/BlitzBrowser_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0i51zvqxnw5f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l75efz/run_your_browser_agent_with_browser_use_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l75efz/run_your_browser_agent_with_browser_use_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-09T13:53:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1l79rjg</id>
    <title>Suggest me to choose BEST LLM for similarity match</title>
    <updated>2025-06-09T16:46:44+00:00</updated>
    <author>
      <name>/u/LazyChampionship5819</name>
      <uri>https://old.reddit.com/user/LazyChampionship5819</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey currently in our small company we are running a small project where we get a multiple list of customers data from our clients to update the records in our db. The problem is the list which we get usually has different type like names won't match usually but they are our customers so instead of doing it manually thinking we can do fuzzy matching but that don't have us accuracy as we expected so thinking to use AI but it's too expensive, and I tried Open source LLM but still thinking to which one to use. I'm running a flask small web app that user can upload csv or JSON or sheet and in backend the ai does the magic connecting to our db and do matching and show the result to user. I don't know which one to use now and even my laptop is not that good enough to handle large LLM my laptop is dell Inspiron 16 plus with 32gb ram and and Intel ultra 7 basic arc graphics. Can you give me an idea what to do now? I tried some small LLM but mostly it's giving hallucinations error. My Customer DB has 7k customers and the user uploads the data would be like 3-4 k rows of csv&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LazyChampionship5819"&gt; /u/LazyChampionship5819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l79rjg/suggest_me_to_choose_best_llm_for_similarity_match/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l79rjg/suggest_me_to_choose_best_llm_for_similarity_match/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l79rjg/suggest_me_to_choose_best_llm_for_similarity_match/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-09T16:46:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1l805il</id>
    <title>THE best model ?</title>
    <updated>2025-06-10T14:28:15+00:00</updated>
    <author>
      <name>/u/Livid_Molasses_5824</name>
      <uri>https://old.reddit.com/user/Livid_Molasses_5824</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys for a RX7800XT &amp;amp; a ryzen5600x what's the perfect model ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Livid_Molasses_5824"&gt; /u/Livid_Molasses_5824 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l805il/the_best_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l805il/the_best_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l805il/the_best_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-10T14:28:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7rgkv</id>
    <title>running ollma on vsphere without GPU</title>
    <updated>2025-06-10T06:10:34+00:00</updated>
    <author>
      <name>/u/emaayan</name>
      <uri>https://old.reddit.com/user/emaayan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi , trying to run ollama with qwen 2.5 7b model on a vsphere , gave it a vm with os proton,128 gb memory about 16 cpus and that thing is still slow and unusable than my desktop i9900 with 64gb memory and 4060 16gb vram, &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emaayan"&gt; /u/emaayan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7rgkv/running_ollma_on_vsphere_without_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7rgkv/running_ollma_on_vsphere_without_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l7rgkv/running_ollma_on_vsphere_without_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-10T06:10:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7kc8k</id>
    <title>Multi-Config Switching UI</title>
    <updated>2025-06-09T23:49:31+00:00</updated>
    <author>
      <name>/u/PleasantCandidate785</name>
      <uri>https://old.reddit.com/user/PleasantCandidate785</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw a UI or UI for UIs mentioned in a thread earlier. It was called Multi-&amp;lt;something&amp;gt; but I can't remember what the something was.&lt;/p&gt; &lt;p&gt;As I remember it allowed sharing models between multiple backends like Ollama and ExllamaV2 and also switching UIs. &lt;/p&gt; &lt;p&gt;I've been googling off and on for it all day, but am coming up empty.&lt;/p&gt; &lt;p&gt;Anyone know what I'm talking about?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PleasantCandidate785"&gt; /u/PleasantCandidate785 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7kc8k/multiconfig_switching_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7kc8k/multiconfig_switching_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l7kc8k/multiconfig_switching_ui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-09T23:49:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7fhb5</id>
    <title>best option for personal private and local RAG with Ollama ?</title>
    <updated>2025-06-09T20:25:33+00:00</updated>
    <author>
      <name>/u/LivingSignificant452</name>
      <uri>https://old.reddit.com/user/LivingSignificant452</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;br /&gt; I would like to set up a private , local notebooklm alternative. Using documents I prepare in PDF mainly ( up to 50 very long document 500pages each ). Also !! I need it to work correctly with french language.&lt;br /&gt; for the hardward part, I have a RTX 3090, so I can choose any ollama model working with up to 24Mb of vram.&lt;/p&gt; &lt;p&gt;I have &lt;strong&gt;openwebui&lt;/strong&gt;, and started to make some test with the integrated document feature, but for the option or improve it, it's difficult to understand the impact of each option&lt;/p&gt; &lt;p&gt;I have tested briefly &lt;strong&gt;PageAssist&lt;/strong&gt; in chrome, but honestly, it's like it doesn't work, despite I followed a youtube tutorial.&lt;/p&gt; &lt;p&gt;is there anything else I should try ? I saw a mention to LightRag ?&lt;br /&gt; as things are moving so fast, it's hard to know where to start, and even when it works, you don't know if you are not missing an option or a tip. thanks by advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LivingSignificant452"&gt; /u/LivingSignificant452 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7fhb5/best_option_for_personal_private_and_local_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7fhb5/best_option_for_personal_private_and_local_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l7fhb5/best_option_for_personal_private_and_local_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-09T20:25:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1l6x83v</id>
    <title>Use Ollama to make agents watch your screen!</title>
    <updated>2025-06-09T05:56:48+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l6x83v/use_ollama_to_make_agents_watch_your_screen/"&gt; &lt;img alt="Use Ollama to make agents watch your screen!" src="https://external-preview.redd.it/NHZ4YnNpcXZjdTVmMZ0cZOsTXi-ThTayE7iEfGGYXF4Z17hX-7dpetBO2beo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a3632ea76a07d26b7e0ad2747798f130b121668" title="Use Ollama to make agents watch your screen!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zbl1cgqvcu5f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l6x83v/use_ollama_to_make_agents_watch_your_screen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l6x83v/use_ollama_to_make_agents_watch_your_screen/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-09T05:56:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7orop</id>
    <title>How to Install Open WebUI with Bundled Ollama Support</title>
    <updated>2025-06-10T03:30:51+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l7orop/how_to_install_open_webui_with_bundled_ollama/"&gt; &lt;img alt="How to Install Open WebUI with Bundled Ollama Support" src="https://external-preview.redd.it/WjowGLQbVnk9UeHmNSsY5EBOABBmLH2XCrrr8yydG4o.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8573656176199f099b9ec629705960b89fdad66b" title="How to Install Open WebUI with Bundled Ollama Support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/6oOVZEU_36c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7orop/how_to_install_open_webui_with_bundled_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l7orop/how_to_install_open_webui_with_bundled_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-10T03:30:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7jzqk</id>
    <title>Built coexistAI, building blocks for your own deep research at scale</title>
    <updated>2025-06-09T23:33:43+00:00</updated>
    <author>
      <name>/u/Optimalutopic</name>
      <uri>https://old.reddit.com/user/Optimalutopic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/SPThole/CoexistAI"&gt;https://github.com/SPThole/CoexistAI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi all! I’m excited to share CoexistAI, a modular open-source framework designed to help you streamline and automate your research workflows—right on your own machine. &lt;/p&gt; &lt;h3&gt;What is CoexistAI?&lt;/h3&gt; &lt;p&gt;CoexistAI brings together web, YouTube, and Reddit search, flexible summarization, and geospatial analysis—all powered by LLMs and embedders you choose (local or cloud). It’s built for researchers, students, and anyone who wants to organize, analyze, and summarize information efficiently. &lt;/p&gt; &lt;h3&gt;Key Features&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Open-source and modular:&lt;/strong&gt; Fully open-source and designed for easy customization. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-LLM and embedder support:&lt;/strong&gt; Connect with various LLMs and embedding models, including local and cloud providers (OpenAI, Google, Ollama, and more coming soon). &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unified search:&lt;/strong&gt; Perform web, YouTube, and Reddit searches directly from the framework. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Notebook and API integration:&lt;/strong&gt; Use CoexistAI seamlessly in Jupyter notebooks or via FastAPI endpoints. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible summarization:&lt;/strong&gt; Summarize content from web pages, YouTube videos, and Reddit threads by simply providing a link. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM-powered at every step:&lt;/strong&gt; Language models are integrated throughout the workflow for enhanced automation and insights. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local model compatibility:&lt;/strong&gt; Easily connect to and use local LLMs for privacy and control. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Modular tools:&lt;/strong&gt; Use each feature independently or combine them to build your own research assistant. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Geospatial capabilities:&lt;/strong&gt; Generate and analyze maps, with more enhancements planned. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;On-the-fly RAG:&lt;/strong&gt; Instantly perform Retrieval-Augmented Generation (RAG) on web content. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deploy on your own PC or server:&lt;/strong&gt; Set up once and use across your devices at home or work. &lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;How you might use it&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Research any topic by searching, aggregating, and summarizing from multiple sources &lt;/li&gt; &lt;li&gt;Summarize and compare papers, videos, and forum discussions &lt;/li&gt; &lt;li&gt;Build your own research assistant for any task &lt;/li&gt; &lt;li&gt;Use geospatial tools for location-based research or mapping projects &lt;/li&gt; &lt;li&gt;Automate repetitive research tasks with notebooks or API calls &lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt; CoexistAI on GitHub&lt;/p&gt; &lt;p&gt;&lt;em&gt;Free for non-commercial research &amp;amp; educational use.&lt;/em&gt; &lt;/p&gt; &lt;p&gt;Would love feedback from anyone interested in local-first, modular research tools! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Optimalutopic"&gt; /u/Optimalutopic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7jzqk/built_coexistai_building_blocks_for_your_own_deep/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l7jzqk/built_coexistai_building_blocks_for_your_own_deep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l7jzqk/built_coexistai_building_blocks_for_your_own_deep/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-09T23:33:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1l88vdj</id>
    <title>Instant shutdown and restart when using deepseek-r1:70b</title>
    <updated>2025-06-10T20:06:05+00:00</updated>
    <author>
      <name>/u/Ok_Musician_4872</name>
      <uri>https://old.reddit.com/user/Ok_Musician_4872</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have ollama version is 0.9.0, I tried to play with a few different models, everything works correctly. But when I'm trying to use deepseek-r1:70b it behaves very strangely. I've managed to load the model from cmd line, and enter simple prompt. It worked slowly, but worked. But every time when I'm trying to use it with bigger prompt through API, my PC shutdowns completely (LEDs are off, HDD stops, fans stops), and then after 2-3 seconds it boots normally. Anyone had something like that? What can be the reason? It happens almost immediately when I hit enter...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Musician_4872"&gt; /u/Ok_Musician_4872 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l88vdj/instant_shutdown_and_restart_when_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l88vdj/instant_shutdown_and_restart_when_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l88vdj/instant_shutdown_and_restart_when_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-10T20:06:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1l82z0y</id>
    <title>What’s the Best Method to Determine Cable Length from a Scaled PDF Drawing?</title>
    <updated>2025-06-10T16:19:11+00:00</updated>
    <author>
      <name>/u/ElegantSherbet3945</name>
      <uri>https://old.reddit.com/user/ElegantSherbet3945</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l82z0y/whats_the_best_method_to_determine_cable_length/"&gt; &lt;img alt="What’s the Best Method to Determine Cable Length from a Scaled PDF Drawing?" src="https://b.thumbs.redditmedia.com/5ABubQUjQPHOgMUXmVPEqHhcG39CIgyGFVp4ci0Zc-U.jpg" title="What’s the Best Method to Determine Cable Length from a Scaled PDF Drawing?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a working drawing that was created in AutoCAD and exported as a PDF. The drawing includes a legend and, as shown in the screenshot, a line marked from point A to point B. This line, represented by a purple dotted line, indicates the path of a cable.&lt;/p&gt; &lt;p&gt;Using the scale provided in the drawing, I want to calculate the total length of cable needed to run from point A to point B.&lt;/p&gt; &lt;p&gt;What method or model can I use to determine this?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u6m5vekok46f1.png?width=555&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa096cdd15e93a4f23b6875edeb3ade91e052b2b"&gt;https://preview.redd.it/u6m5vekok46f1.png?width=555&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa096cdd15e93a4f23b6875edeb3ade91e052b2b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElegantSherbet3945"&gt; /u/ElegantSherbet3945 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l82z0y/whats_the_best_method_to_determine_cable_length/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l82z0y/whats_the_best_method_to_determine_cable_length/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l82z0y/whats_the_best_method_to_determine_cable_length/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-10T16:19:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1l85fh8</id>
    <title>Ollama Frontend/GUI</title>
    <updated>2025-06-10T17:53:30+00:00</updated>
    <author>
      <name>/u/Ok_Most9659</name>
      <uri>https://old.reddit.com/user/Ok_Most9659</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for an Ollama frontend/GUI. Preferably can be used offline, is private, works in Linux, and open source.&lt;br /&gt; Any recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Most9659"&gt; /u/Ok_Most9659 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l85fh8/ollama_frontendgui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l85fh8/ollama_frontendgui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l85fh8/ollama_frontendgui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-10T17:53:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8gbzq</id>
    <title>GPU ollama docker</title>
    <updated>2025-06-11T01:33:01+00:00</updated>
    <author>
      <name>/u/Informal_Catch_4688</name>
      <uri>https://old.reddit.com/user/Informal_Catch_4688</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm currently using ollama through WLS for my assistant on windows what I noticed is that it only uses 28% of my GPU but the reply from questions take long time 15secods how can I speed it up ? I was using llama.cpp before that and it was taking around 1-4 seconds to generate answer , I could not use llama.cpp because of hallucinations assistant would day the prompt my question and answer and hashtags etc &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Informal_Catch_4688"&gt; /u/Informal_Catch_4688 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8gbzq/gpu_ollama_docker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8gbzq/gpu_ollama_docker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l8gbzq/gpu_ollama_docker/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T01:33:01+00:00</published>
  </entry>
</feed>
