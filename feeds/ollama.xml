<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-04-08T01:34:21+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1js2vzw</id>
    <title>Best uncensored ollama local model for erotic/porn prompt writing for image generation (sdxl, pony,...)</title>
    <updated>2025-04-05T12:59:34+00:00</updated>
    <author>
      <name>/u/Infinite-Stable-10</name>
      <uri>https://old.reddit.com/user/Infinite-Stable-10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I a trying to find a good local model, less than 15B, that can generate uncensored prompts (sdxl, pony) to feed my Comfy UI workflow for generation of erotic / porn images. Any recommendations? I used in past solar-pro 13B that work quite well fed with examples, but it is a little outdated. &lt;/p&gt; &lt;p&gt;If there is a good model that i 5B or 10B or around that is best so I can load it at the same time of my image generation model. &lt;/p&gt; &lt;p&gt;HELP &lt;/p&gt; &lt;p&gt;🙉&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Infinite-Stable-10"&gt; /u/Infinite-Stable-10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js2vzw/best_uncensored_ollama_local_model_for_eroticporn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js2vzw/best_uncensored_ollama_local_model_for_eroticporn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1js2vzw/best_uncensored_ollama_local_model_for_eroticporn/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T12:59:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1js9nkc</id>
    <title>I built an open source Computer-use framework that uses Local LLMs with Ollama</title>
    <updated>2025-04-05T18:09:51+00:00</updated>
    <author>
      <name>/u/sandropuppo</name>
      <uri>https://old.reddit.com/user/sandropuppo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1js9nkc/i_built_an_open_source_computeruse_framework_that/"&gt; &lt;img alt="I built an open source Computer-use framework that uses Local LLMs with Ollama" src="https://external-preview.redd.it/2AUUbeOoZ7agjDBXWHt091L224zZyg21bhTPn_iKBqY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=624a29b9725e7bb2871c8940c2c220a294d0d3e4" title="I built an open source Computer-use framework that uses Local LLMs with Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandropuppo"&gt; /u/sandropuppo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/trycua/cua"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js9nkc/i_built_an_open_source_computeruse_framework_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1js9nkc/i_built_an_open_source_computeruse_framework_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T18:09:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsu0sr</id>
    <title>Looking for a mistral 7B or equivalent that answer only in french</title>
    <updated>2025-04-06T13:23:34+00:00</updated>
    <author>
      <name>/u/maxorius13</name>
      <uri>https://old.reddit.com/user/maxorius13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;br /&gt; i found it pretty hard to ensure that mistral 7B would answer in french.&lt;br /&gt; Does any one know a model that will do the job ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxorius13"&gt; /u/maxorius13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsu0sr/looking_for_a_mistral_7b_or_equivalent_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsu0sr/looking_for_a_mistral_7b_or_equivalent_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsu0sr/looking_for_a_mistral_7b_or_equivalent_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T13:23:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsat5n</id>
    <title>llama 4</title>
    <updated>2025-04-05T18:59:58+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama4_omni/"&gt;https://www.llama.com/docs/model-cards-and-prompt-formats/llama4_omni/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When I can download it from Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsat5n/llama_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsat5n/llama_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsat5n/llama_4/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T18:59:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt0gy0</id>
    <title>Project Title: SQL Chatbot with Ollama Integration</title>
    <updated>2025-04-06T18:11:08+00:00</updated>
    <author>
      <name>/u/ntnk1999</name>
      <uri>https://old.reddit.com/user/ntnk1999</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jt0gy0/project_title_sql_chatbot_with_ollama_integration/"&gt; &lt;img alt="Project Title: SQL Chatbot with Ollama Integration" src="https://external-preview.redd.it/8uTKcvd9x0EBLwRQcU7QYu6Rejuw6sRtgDwvDj_WT7M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eabfa2e060f7b47ddf9a6ca83e8c7e23cd5d7dd5" title="Project Title: SQL Chatbot with Ollama Integration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, can anybody tell me how to build this chatbot?&lt;/p&gt; &lt;p&gt;I don't have any coding experience—I'm just trying to build it for fun. I tried using Cursor and GitHub Copilot, but after some time, both started looping and generating incorrect code. They kept trying to fix it, but eventually, they seemed to forget what they were building.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ntnk1999"&gt; /u/ntnk1999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://g.co/gemini/share/1f18b52b44b2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jt0gy0/project_title_sql_chatbot_with_ollama_integration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jt0gy0/project_title_sql_chatbot_with_ollama_integration/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T18:11:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsvexq</id>
    <title>Janus pro 7b GGUF</title>
    <updated>2025-04-06T14:30:56+00:00</updated>
    <author>
      <name>/u/Inner-End7733</name>
      <uri>https://old.reddit.com/user/Inner-End7733</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Other posts seem to write the whole idea off in general without much thought. but theoretically you can run GGUF with Ollama, and there are GGUF versions of Janus pro on HF. Anyone done any experimetation with the applicable GGUF on HF? If so, how and to what degree of success? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inner-End7733"&gt; /u/Inner-End7733 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsvexq/janus_pro_7b_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsvexq/janus_pro_7b_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsvexq/janus_pro_7b_gguf/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T14:30:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsujp3</id>
    <title>M4 studio (M4 max 16 core CPU, 40 core GPU 128gb Ram) for LLM (local)</title>
    <updated>2025-04-06T13:49:53+00:00</updated>
    <author>
      <name>/u/Bahaal_1981</name>
      <uri>https://old.reddit.com/user/Bahaal_1981</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been experimenting with local LLMs (ollama) on an M1 Pro macbook (32GB ram) - so far OK but slowish. My desktop needs an upgrade and my use case is academic (assistance with programming with R / Shiny, perhaps some python, proofreading, generating new ideas / criticizing them, perhaps building a RAG to synthesise journal articles in .pdf). I am considering the M4 studio (M4 max, 16+40 - 128GB ram). Some of these tasks need to be done locally as in some use cases the data should not leave my device. I think the above config. should allow for comfortably running deepseek 70b, for example, next to other smaller models. (Other open source models?) And should be fairly futureproof (and allow to run some newer models locally (or quanizations). Any thoughts? Any suggestions for LLM models that would run well locally for the above tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bahaal_1981"&gt; /u/Bahaal_1981 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsujp3/m4_studio_m4_max_16_core_cpu_40_core_gpu_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsujp3/m4_studio_m4_max_16_core_cpu_40_core_gpu_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsujp3/m4_studio_m4_max_16_core_cpu_40_core_gpu_128gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T13:49:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtjgvs</id>
    <title>Ollama is the most easiest local LLM to install and use</title>
    <updated>2025-04-07T11:58:00+00:00</updated>
    <author>
      <name>/u/Puzzled_Estimate_596</name>
      <uri>https://old.reddit.com/user/Puzzled_Estimate_596</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama is the most easiest local llm to install and use, I tried vllm and few others. Could not get started, lot of dependency issues. Apple GPU not supported. Others need a UI to work with. Then some issues with tokenizer not working.&lt;/p&gt; &lt;p&gt;Ollama seems to do a lot of heavy lifting for normal users. Thanks to the team who are brining this to us. One more friendly feature, is to swap models efficiently. Some blogs say other local llm are more performant, but ollama is the most friendliest and quickest to use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Puzzled_Estimate_596"&gt; /u/Puzzled_Estimate_596 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtjgvs/ollama_is_the_most_easiest_local_llm_to_install/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtjgvs/ollama_is_the_most_easiest_local_llm_to_install/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtjgvs/ollama_is_the_most_easiest_local_llm_to_install/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T11:58:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtau8m</id>
    <title>Help picking model</title>
    <updated>2025-04-07T02:26:14+00:00</updated>
    <author>
      <name>/u/Leather-Equipment256</name>
      <uri>https://old.reddit.com/user/Leather-Equipment256</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im using ollama to host a LLM that I use inside of obsidian to quiz me on notes and ask questions. Every model ive tried can’t really quiz me at all. What should I use my ollama is on a Rx 6750 xt 12gb vram and 5600+32gb@3800mhz ram. Ik ollama doesn’t have support for my gpu but im using a forked version that allows gpu acceleration while I wait for official support. So what model to use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leather-Equipment256"&gt; /u/Leather-Equipment256 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtau8m/help_picking_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtau8m/help_picking_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtau8m/help_picking_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T02:26:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1jstwzw</id>
    <title>Smaller Gemma-3-27b QAT GGUF now available on Ollama</title>
    <updated>2025-04-06T13:18:01+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This smaller QAT gguf is faster than the one from Google, and it retains the same quality.&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama run JollyLlama/gemma-3-27b-it-q4_0_Small-QAT&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.ollama.com/JollyLlama/gemma-3-27b-it-q4_0_Small-QAT"&gt;https://www.ollama.com/JollyLlama/gemma-3-27b-it-q4_0_Small-QAT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ollama is having issues when importing Gemma3 ggufs; I have to edit the manifests manually to make the text part of this model work. The vision function doesn't work because Ollama doesn't support this projector.&lt;/p&gt; &lt;p&gt;Original creator of this smaller QAT gguf: &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jsq1so/smaller_gemma3_qat_versions_12b_in_8gb_and_27b_in/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jsq1so/smaller_gemma3_qat_versions_12b_in_8gb_and_27b_in/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jstwzw/smaller_gemma327b_qat_gguf_now_available_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jstwzw/smaller_gemma327b_qat_gguf_now_available_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jstwzw/smaller_gemma327b_qat_gguf_now_available_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T13:18:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsjhr5</id>
    <title>mistral-small:24b-3.1 finally on ollama!</title>
    <updated>2025-04-06T01:59:48+00:00</updated>
    <author>
      <name>/u/DominusVenturae</name>
      <uri>https://old.reddit.com/user/DominusVenturae</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jsjhr5/mistralsmall24b31_finally_on_ollama/"&gt; &lt;img alt="mistral-small:24b-3.1 finally on ollama!" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="mistral-small:24b-3.1 finally on ollama!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw the benchmark comparing it to Llama4 scout and remembered that when 3.0 24b came out it remained far down the list of &amp;quot;Newest Model&amp;quot; filter. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DominusVenturae"&gt; /u/DominusVenturae &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/library/mistral-small:24b-3.1-instruct-2503-q4_K_M"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsjhr5/mistralsmall24b31_finally_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsjhr5/mistralsmall24b31_finally_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T01:59:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt9o0s</id>
    <title>Looking for Collaborators to port and build an agent like manus in smolagents</title>
    <updated>2025-04-07T01:20:58+00:00</updated>
    <author>
      <name>/u/Character-Ad5001</name>
      <uri>https://old.reddit.com/user/Character-Ad5001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on this project for a while now and recently decided to build a UI for it. However, working with langchain and langgraph has been more of a challenge than expected — I’ve had to write a lot of custom solutions for vector stores, semantic chunking, persisting LangGraph with Drizzle, and more. After a lot of trial and error, I realized the simplest and most reliable way to run everything locally (without relying on external SaaS) is to stick with Python, using SQLite as the primary storage layer. While LangChain/LangGraph's JavaScript ecosystem does have solid integrations, they often tie into cloud services, which goes against the local-first goal of this project. I've experimented with almost every agentic library out there, including the newer lightweight ones, and in terms of support, stability, and future potential, smolagents seems like the best fit going forward. The vision for this project is to combine the best parts of various open source tools. Surprisingly, no current open source chat app implements full revision history — tools like LM Studio offer branching, but that’s a different UX model. Revision history needs a parent-child tree model, whereas branching is more like checkpointing (copy-paste). I'm also planning to integrate features like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SearchXNG in-chat search&lt;/li&gt; &lt;li&gt;CAPTCHA-free scraping via Playwright&lt;/li&gt; &lt;li&gt;NotebookLM-inspired source sidebar&lt;/li&gt; &lt;li&gt;Claude-style project handling&lt;/li&gt; &lt;li&gt;Toggleable manus type agent (like toggling on/off search/deepsearch from openai/grok)&lt;/li&gt; &lt;li&gt;And much more — thanks to incredible tools like zep, crawlforai, browser use, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to bring on some collaborators to help push this forward. If you're into LLMs, agentic workflows, and building local-first tools, hit me up! &lt;a href="https://github.com/mantrakp04/manusmcp"&gt;https://github.com/mantrakp04/manusmcp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Character-Ad5001"&gt; /u/Character-Ad5001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jt9o0s/looking_for_collaborators_to_port_and_build_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jt9o0s/looking_for_collaborators_to_port_and_build_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jt9o0s/looking_for_collaborators_to_port_and_build_an/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T01:20:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt62o8</id>
    <title>Open-source Morphik MCP server for technical document search with Ollama client</title>
    <updated>2025-04-06T22:17:54+00:00</updated>
    <author>
      <name>/u/yes-no-maybe_idk</name>
      <uri>https://old.reddit.com/user/yes-no-maybe_idk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; - we built Morphik MCP to solve a common problem: finding specific information across scattered technical docs. We've experimented with GraphRAG, ColPali, contextual embeddings, and more. MCP emerged as the solution that unifies these approaches.&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multimodal search across text, diagrams, and videos&lt;/li&gt; &lt;li&gt;Natural language knowledge base management&lt;/li&gt; &lt;li&gt;Fully open-source with responsive support&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Integration with LibreChat and Open WebUI for Ollama users&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What sets MCP apart is its ability to return images (including diagrams) directly to the MCP client. Users have applied it to search over data ranging from blood tests to patents, and we use this daily with Cursor and Claude.&lt;/p&gt; &lt;p&gt;This makes Morphik MCP an excellent companion for your existing Ollama setup. &lt;/p&gt; &lt;p&gt;Give it a spin, and let us know what you think. &lt;/p&gt; &lt;p&gt;Link to our repo: &lt;a href="https://github.com/morphik-org/morphik-core"&gt;https://github.com/morphik-org/morphik-core&lt;/a&gt;, give it a star!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yes-no-maybe_idk"&gt; /u/yes-no-maybe_idk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jt62o8/opensource_morphik_mcp_server_for_technical/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jt62o8/opensource_morphik_mcp_server_for_technical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jt62o8/opensource_morphik_mcp_server_for_technical/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T22:17:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsljwk</id>
    <title>Github Copilot now supports Ollama and OpenRouter Models 🎉</title>
    <updated>2025-04-06T04:01:00+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jsljwk/github_copilot_now_supports_ollama_and_openrouter/"&gt; &lt;img alt="Github Copilot now supports Ollama and OpenRouter Models 🎉" src="https://b.thumbs.redditmedia.com/w6tm8jDvXneFlfBCf3lkx82SjxvqcNt_DTfDjlvvfmU.jpg" title="Github Copilot now supports Ollama and OpenRouter Models 🎉" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Huge W for programmers (and vibe coders) in the Local LLM community. Github Copilot now supports a much wider range of models from Ollama, OpenRouter, Gemini, and others.&lt;/p&gt; &lt;p&gt;To add your own models, click on &amp;quot;Manage Models&amp;quot; in the prompt field.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jsljwk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsljwk/github_copilot_now_supports_ollama_and_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsljwk/github_copilot_now_supports_ollama_and_openrouter/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T04:01:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtcuwv</id>
    <title>Deterministic output with same seed - example</title>
    <updated>2025-04-07T04:20:30+00:00</updated>
    <author>
      <name>/u/binuuday</name>
      <uri>https://old.reddit.com/user/binuuday</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jtcuwv/deterministic_output_with_same_seed_example/"&gt; &lt;img alt="Deterministic output with same seed - example" src="https://a.thumbs.redditmedia.com/GhsfXzRn2Zg7o5WMYtymEXZzFU2iv6W9dcgYQpxK5K4.jpg" title="Deterministic output with same seed - example" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most experts know this already, this entry is for people who are new to ollama, like me.&lt;/p&gt; &lt;p&gt;During some RAG cases, we need our output to be deterministic. Ollama allows this by setting the seed value, to the same number, for consecutive requests. This will not work in chat mode, or where multiple prompts are sent. (All prompts to the Ollama server needs to be same)&lt;/p&gt; &lt;p&gt;This is a property of the generation function, a random tensor is created upon which the layers act upon. If we don't give seed, or give seed as -1, the initial tensor is filled with truly random numbers. But when same seed value is given the tensor is filled with deterministic random numbers ( assuming you are on the same machine and using the same functionality, process). In Ollama's case we are hitting the same processs running on the same machine too.&lt;/p&gt; &lt;p&gt;If you are using any UI, you have to clear the history, to get deterministic output, because they tend to maintain sessions, and send the history of chat in prompt. Example of curl commands given below.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a6147idz7cte1.png?width=2610&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=690d0527eddea5f2ee7bd965d7879fc26791f77d"&gt;Deterministic Output with Same Seed&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;date curl -s http://localhost:11434/api/chat -d '{ &amp;quot;model&amp;quot;: &amp;quot;llama3.2:latest&amp;quot;, &amp;quot;messages&amp;quot;: [ { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Give 5 random numbers and 5 random animals&amp;quot; } ], &amp;quot;options&amp;quot;: { &amp;quot;seed&amp;quot;: 32988 }, &amp;quot;stream&amp;quot;: false }' | jq '.message.content' Mon Apr 7 09:47:38 IST 2025 &amp;quot;Here are 5 random numbers:\n\n1. 854\n2. 219\n3. 467\n4. 982\n5. 135\n\nAnd here are 5 random animals:\n\n1. Quail\n2. Narwhal\n3. Meerkat\n4. Lemur\n5. Otter&amp;quot; date curl -s http://localhost:11434/api/chat -d '{ &amp;quot;model&amp;quot;: &amp;quot;llama3.2:latest&amp;quot;, &amp;quot;messages&amp;quot;: [ { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Give 5 random numbers and 5 random animals&amp;quot; } ], &amp;quot;options&amp;quot;: { &amp;quot;seed&amp;quot;: 32988 }, &amp;quot;stream&amp;quot;: false }' | jq '.message.content' Mon Apr 7 09:49:03 IST 2025 &amp;quot;Here are 5 random numbers:\n\n1. 854\n2. 219\n3. 467\n4. 982\n5. 135\n\nAnd here are 5 random animals:\n\n1. Quail\n2. Narwhal\n3. Meerkat\n4. Lemur\n5. Otter&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Above are same command at different point of time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/binuuday"&gt; /u/binuuday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtcuwv/deterministic_output_with_same_seed_example/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtcuwv/deterministic_output_with_same_seed_example/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtcuwv/deterministic_output_with_same_seed_example/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T04:20:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtka0k</id>
    <title>Bus/Trucks Vehicle Make and Models Dataset</title>
    <updated>2025-04-07T12:40:54+00:00</updated>
    <author>
      <name>/u/Senior-Reserve3732</name>
      <uri>https://old.reddit.com/user/Senior-Reserve3732</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm wondering if I can find a model that has been trained with all bus and trucks makes and models available worldwide. I would like to use it's trained data to get spareparts products for each of the vehicles.&lt;/p&gt; &lt;p&gt;Is there any way to get this data? I tried a lot of public datasets but none of them is complete.&lt;/p&gt; &lt;p&gt;Thank you in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Senior-Reserve3732"&gt; /u/Senior-Reserve3732 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtka0k/bustrucks_vehicle_make_and_models_dataset/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtka0k/bustrucks_vehicle_make_and_models_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtka0k/bustrucks_vehicle_make_and_models_dataset/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T12:40:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtcsfp</id>
    <title>I want to create a RAG from tabular data (databases). How do I proceed?</title>
    <updated>2025-04-07T04:16:15+00:00</updated>
    <author>
      <name>/u/thecrazytughlaq</name>
      <uri>https://old.reddit.com/user/thecrazytughlaq</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am fairly new to RAG. I have built a RAG to chat with PDFs, based on youtube videos, using Ollama models and ChromaDB. &lt;/p&gt; &lt;p&gt;I want to create a RAG that helps me chat with tabular data. I want to use it to forecast values, look up values etc. I am trying it on PDFs with tables of numerical values first. Can I proceed the same way as I did for text-content PDFs, or are there any other factors I must consider?&lt;/p&gt; &lt;p&gt;As for the next step, connecting it to SQL database, would I need to process the database in any way before I connect it to the langchain sql package? And can I expect reasonable accuracy (as much as I expect from the RAG based on text-based content) ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecrazytughlaq"&gt; /u/thecrazytughlaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtcsfp/i_want_to_create_a_rag_from_tabular_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtcsfp/i_want_to_create_a_rag_from_tabular_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtcsfp/i_want_to_create_a_rag_from_tabular_data/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T04:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtrb0p</id>
    <title>Fine turning pre trained model</title>
    <updated>2025-04-07T17:42:08+00:00</updated>
    <author>
      <name>/u/GeorgeSKG_</name>
      <uri>https://old.reddit.com/user/GeorgeSKG_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fine turning pre trained model&lt;/p&gt; &lt;p&gt;Hello everyone,im trying to train a pre trained model (Mistral 7b) on discord. If you wanna help and join to a project (its a huge project if we have the dataset) comment and I will dm you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GeorgeSKG_"&gt; /u/GeorgeSKG_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtrb0p/fine_turning_pre_trained_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtrb0p/fine_turning_pre_trained_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtrb0p/fine_turning_pre_trained_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T17:42:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtvn86</id>
    <title>Ollama and RooCode/Continue on Mac M1</title>
    <updated>2025-04-07T20:37:29+00:00</updated>
    <author>
      <name>/u/onedjscream</name>
      <uri>https://old.reddit.com/user/onedjscream</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone gotten RooCode and Continue to work well with Ollama on a MacBook Pro M1 16GB? Which models? My setup with starcoder and qwen start to heat up especially with Continue and 1000ms debounce.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onedjscream"&gt; /u/onedjscream &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtvn86/ollama_and_roocodecontinue_on_mac_m1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtvn86/ollama_and_roocodecontinue_on_mac_m1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtvn86/ollama_and_roocodecontinue_on_mac_m1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T20:37:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt3ndd</id>
    <title>How do small models contain so much information?</title>
    <updated>2025-04-06T20:27:33+00:00</updated>
    <author>
      <name>/u/BallPythonTech</name>
      <uri>https://old.reddit.com/user/BallPythonTech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am amazed at how much data small models can re-create. For example, Gemma3:4b, I ask it to list the books of the Old Testament. It leaves some out listing only 35. &lt;/p&gt; &lt;p&gt;But how does it even store that? &lt;/p&gt; &lt;p&gt;List the books by Edgar Allen Poe, it gets most of them, same for Dr Seuss. Published years are often wrong but still. &lt;/p&gt; &lt;p&gt;List publications by Albert Einstein - mostly correct.&lt;/p&gt; &lt;p&gt;List elementary particles - it lists half of them, 17&lt;/p&gt; &lt;p&gt;So how in 3GB is it able to store so much information or is Ollama going out to the internet to get more data?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BallPythonTech"&gt; /u/BallPythonTech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jt3ndd/how_do_small_models_contain_so_much_information/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jt3ndd/how_do_small_models_contain_so_much_information/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jt3ndd/how_do_small_models_contain_so_much_information/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T20:27:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju1pvq</id>
    <title>Ollama Cli results different from the API calls</title>
    <updated>2025-04-08T01:17:49+00:00</updated>
    <author>
      <name>/u/Ibrahimkm</name>
      <uri>https://old.reddit.com/user/Ibrahimkm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everybody,&lt;/p&gt; &lt;p&gt;I was testing some small models as mistral and llama3.1 on Ollama and I found out when I use the CLI the results are different from the one that the model provide when I call it in a python script.&lt;br /&gt; I tried to check the default parameters as temperature or top_P, top_k that the CLI uses but it seems there is no way to know (at least to my knowledge)&lt;/p&gt; &lt;p&gt;I am testing the LLM for a classification task, it will respond with &amp;quot;Attack&amp;quot; or &amp;quot;Benign&amp;quot; the CLI seems to get better results when I manually test the same prompt.&lt;/p&gt; &lt;p&gt;Also I was using ollama models for a long time and I am thinking of testing other version of these models finetuned by users. Where can I find these customized models ? I saw some in huggingface but the search engine wasn't very good there was no way to know how good the model any review or how many person tested it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ibrahimkm"&gt; /u/Ibrahimkm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju1pvq/ollama_cli_results_different_from_the_api_calls/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju1pvq/ollama_cli_results_different_from_the_api_calls/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ju1pvq/ollama_cli_results_different_from_the_api_calls/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T01:17:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtylaj</id>
    <title>Ollama and mistral3.1 cant fit into 24GB Vram</title>
    <updated>2025-04-07T22:45:51+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Why mistral-small3.1:latest b9aaf0c2586a 15 GB goes over 24GB when it is loaded?&lt;br /&gt; And for example Gemma3 which size on disk is larger, 17GB fits fine in 24GB?&lt;/p&gt; &lt;p&gt;What am I doing wrong? How to fit mistral3.1 better? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtylaj/ollama_and_mistral31_cant_fit_into_24gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtylaj/ollama_and_mistral31_cant_fit_into_24gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtylaj/ollama_and_mistral31_cant_fit_into_24gb_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T22:45:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtrfrb</id>
    <title>How do you determine system requirements for different models?</title>
    <updated>2025-04-07T17:47:35+00:00</updated>
    <author>
      <name>/u/some1_online</name>
      <uri>https://old.reddit.com/user/some1_online</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I've been running different models locally but I try to go for the most lightweight models with the least parameters. I'm wondering, how do I determine the system requirements (or speed or efficiency) for each model given my hardware so I can run the best possible models on my machine?&lt;/p&gt; &lt;p&gt;Here's what my hardware looks like for reference:&lt;/p&gt; &lt;p&gt;RTX 3060 12 GB VRAM GPU&lt;/p&gt; &lt;p&gt;16 GB RAM (can be upgraded to 32 easily)&lt;/p&gt; &lt;p&gt;Ryzen 5 4500 6 core, 12 thread CPU&lt;/p&gt; &lt;p&gt;512 GB SSD&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/some1_online"&gt; /u/some1_online &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtrfrb/how_do_you_determine_system_requirements_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtrfrb/how_do_you_determine_system_requirements_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtrfrb/how_do_you_determine_system_requirements_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T17:47:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtwxm7</id>
    <title>Benchmarks comparing only quantized models you can run on a macbook (7B, 8B, 14B)?</title>
    <updated>2025-04-07T21:31:10+00:00</updated>
    <author>
      <name>/u/60secs</name>
      <uri>https://old.reddit.com/user/60secs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone know any benchmark resources which let you filter to models small enough to run on macbook M1-M4 out of the box?&lt;/p&gt; &lt;p&gt;Most of the benchmarks I've seen online show all the models, regardless of the hardware, and models which require an A100/H100 aren't relevant to me running ollama locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/60secs"&gt; /u/60secs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtwxm7/benchmarks_comparing_only_quantized_models_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtwxm7/benchmarks_comparing_only_quantized_models_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtwxm7/benchmarks_comparing_only_quantized_models_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T21:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtycy6</id>
    <title>Working on a cool AI project</title>
    <updated>2025-04-07T22:35:10+00:00</updated>
    <author>
      <name>/u/xKage21x</name>
      <uri>https://old.reddit.com/user/xKage21x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over 6 months or so i have developed an AI system called Trium consisting of three AI personas—Vira, Core, and Echo—running locally on my pc. It uses CUDA with CuPy and cuML for clustering (HDBSCAN, DBSCAN), FAISS for memory indexing, and SentenceTransformers for embeddings. Each persona has a memory bank, recalls clustered events, and acts proactively based on emotional states mapped to polyvagal theory. Temporal rhythms (FFT analysis) guide their autonomy. &lt;/p&gt; &lt;p&gt;Would love to chat or hear ppls thoughts. Happy to share files and info i have ☺️&lt;/p&gt; &lt;p&gt;Anyone who would like to dm me im happy to discuss things more&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xKage21x"&gt; /u/xKage21x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtycy6/working_on_a_cool_ai_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtycy6/working_on_a_cool_ai_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtycy6/working_on_a_cool_ai_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T22:35:10+00:00</published>
  </entry>
</feed>
