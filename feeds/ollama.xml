<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-22T19:48:51+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1iubwp3</id>
    <title>8x Mi50 Server (left) + 8x Mi60 Server (right)</title>
    <updated>2025-02-20T22:53:21+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iubwp3/8x_mi50_server_left_8x_mi60_server_right/"&gt; &lt;img alt="8x Mi50 Server (left) + 8x Mi60 Server (right)" src="https://preview.redd.it/tasjsv1bidke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22693a611e5f5c4d40be93a1739476bd18feedaf" title="8x Mi50 Server (left) + 8x Mi60 Server (right)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tasjsv1bidke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iubwp3/8x_mi50_server_left_8x_mi60_server_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iubwp3/8x_mi50_server_left_8x_mi60_server_right/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-20T22:53:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1iun0ud</id>
    <title>using ollama hosted models with cline — some are super slow, some are normally fast</title>
    <updated>2025-02-21T09:30:27+00:00</updated>
    <author>
      <name>/u/avuvuvui</name>
      <uri>https://old.reddit.com/user/avuvuvui</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;normally fast = the same speed as when I'm chatting with them locally&lt;/p&gt; &lt;p&gt;super slow = when I'm in eg. cline, which has tool use, they get literally 20x slower&lt;/p&gt; &lt;p&gt;any idea what I'm doing wrong&lt;/p&gt; &lt;p&gt;newb here, apologies if too newbish, tried searching&lt;/p&gt; &lt;p&gt;edit: for context, had been lucky enough to have gotten an 128gb m3 macbook from work, that helps with 'pure' usage, ie. without any tool use&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/avuvuvui"&gt; /u/avuvuvui &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iun0ud/using_ollama_hosted_models_with_cline_some_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iun0ud/using_ollama_hosted_models_with_cline_some_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iun0ud/using_ollama_hosted_models_with_cline_some_are/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-21T09:30:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1iur2l4</id>
    <title>Deepseek cuda ptx code?</title>
    <updated>2025-02-21T13:37:29+00:00</updated>
    <author>
      <name>/u/wahnsinnwanscene</name>
      <uri>https://old.reddit.com/user/wahnsinnwanscene</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So i guess it's not in llama.cpp? Is there a way to look at a sample?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wahnsinnwanscene"&gt; /u/wahnsinnwanscene &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iur2l4/deepseek_cuda_ptx_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iur2l4/deepseek_cuda_ptx_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iur2l4/deepseek_cuda_ptx_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-21T13:37:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iujt38</id>
    <title>How To Run Private &amp; Uncensored LLMs Offline | Dolphin Llama 3</title>
    <updated>2025-02-21T05:46:53+00:00</updated>
    <author>
      <name>/u/powerflower_khi</name>
      <uri>https://old.reddit.com/user/powerflower_khi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iujt38/how_to_run_private_uncensored_llms_offline/"&gt; &lt;img alt="How To Run Private &amp;amp; Uncensored LLMs Offline | Dolphin Llama 3" src="https://b.thumbs.redditmedia.com/BdwEHXyUWYg7JDHPMCmb8f44BQjJt0lQoOsNQAyimHg.jpg" title="How To Run Private &amp;amp; Uncensored LLMs Offline | Dolphin Llama 3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Check this out. I give this 5 Star!!!!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l9vyzgqjmfke1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eda1b74ab28218d5b4bfc2fabab349129dfaf87e"&gt;https://preview.redd.it/l9vyzgqjmfke1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eda1b74ab28218d5b4bfc2fabab349129dfaf87e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=eiMSapoeyaU"&gt;https://www.youtube.com/watch?v=eiMSapoeyaU&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/powerflower_khi"&gt; /u/powerflower_khi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iujt38/how_to_run_private_uncensored_llms_offline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iujt38/how_to_run_private_uncensored_llms_offline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iujt38/how_to_run_private_uncensored_llms_offline/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-21T05:46:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1itzioa</id>
    <title>Less than 70b models worth running locally?</title>
    <updated>2025-02-20T14:19:16+00:00</updated>
    <author>
      <name>/u/abrandis</name>
      <uri>https://old.reddit.com/user/abrandis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've been playing with a bunch of local models but have realized that all but the most basic Q&amp;amp;A struggle with decent answers for smaller models....I just wind up going to the cloudz Claude mostly to get better coding and general Q-A results .&lt;/p&gt; &lt;p&gt;My question I guess is are any smallish models practical to run locally,or is there accuracy so poor to not be worth it ..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abrandis"&gt; /u/abrandis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itzioa/less_than_70b_models_worth_running_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itzioa/less_than_70b_models_worth_running_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1itzioa/less_than_70b_models_worth_running_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-20T14:19:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1iuvokl</id>
    <title>Need Help with running R1</title>
    <updated>2025-02-21T16:59:23+00:00</updated>
    <author>
      <name>/u/DifferenceFew4232</name>
      <uri>https://old.reddit.com/user/DifferenceFew4232</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, im running the deepseek r1 14b model locally on my pc and it works. im just wondering tho why my gpu just does not get used at all its just my cpu that is getting utilized even tho i have an rtx 2070 super. i know that vram might be a limiting factor but not using it at all is kind of a waste. Is there a way to get it so that ollama uses my gpu? im on windows and just use cmd.&lt;/p&gt; &lt;p&gt;things i already tries -updating all drivers -checking smi -using OLLAMA_GPU_LAYERS=20&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DifferenceFew4232"&gt; /u/DifferenceFew4232 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iuvokl/need_help_with_running_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iuvokl/need_help_with_running_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iuvokl/need_help_with_running_r1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-21T16:59:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iurwct</id>
    <title>Came into possession of 3x 3060 12gb, looking for a motherboard and CPU(s) that I can use them all in that won’t break the bank. Any suggestions?</title>
    <updated>2025-02-21T14:16:37+00:00</updated>
    <author>
      <name>/u/Herdnerfer</name>
      <uri>https://old.reddit.com/user/Herdnerfer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently have a 12900k and a mobo that will run 2 of the cards but wanting to upgrade. Hoping there might be some used options that won’t cost over $300. I have plenty of DDR4 RAM and a PSU that supports 3 8 pin connectors for PCIe. Just need a mobo and processor to compete the build. Any recommendations would be welcome. Thank you! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Herdnerfer"&gt; /u/Herdnerfer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iurwct/came_into_possession_of_3x_3060_12gb_looking_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iurwct/came_into_possession_of_3x_3060_12gb_looking_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iurwct/came_into_possession_of_3x_3060_12gb_looking_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-21T14:16:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1iv0wq6</id>
    <title>Chroma Auditor</title>
    <updated>2025-02-21T20:33:09+00:00</updated>
    <author>
      <name>/u/ai_hedge_fund</name>
      <uri>https://old.reddit.com/user/ai_hedge_fund</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This week we released a simple open source python UI tool for inspecting chunks in a Chroma database for RAG, editing metadata, exporting to CSV, etc.:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/integral-business-intelligence/chroma-auditor"&gt;https://github.com/integral-business-intelligence/chroma-auditor&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As a Gradio interface it can run completely locally alongside Chroma and Ollama, or can be exposed for network access.&lt;/p&gt; &lt;p&gt;Hope you find it helpful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai_hedge_fund"&gt; /u/ai_hedge_fund &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iv0wq6/chroma_auditor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iv0wq6/chroma_auditor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iv0wq6/chroma_auditor/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-21T20:33:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivce1r</id>
    <title>Ollama not detecting GPU in NixOS</title>
    <updated>2025-02-22T06:22:26+00:00</updated>
    <author>
      <name>/u/TarunRaviYT</name>
      <uri>https://old.reddit.com/user/TarunRaviYT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every once in a while ollama is able to detect and use gpu, but most of the time it doesn't work. I have the NVIDIA 3060. I see this in the ollama logs:&lt;/p&gt; &lt;p&gt;&lt;code&gt; Feb 20 21:00:01 nixos ollama[17426]: time=2025-02-20T21:00:01.870-08:00 level=WARN source=gpu.go:669 msg=&amp;quot;unable to locate gpu dependency libraries&amp;quot; Feb 20 21:00:01 nixos ollama[17426]: time=2025-02-20T21:00:01.870-08:00 level=WARN source=gpu.go:669 msg=&amp;quot;unable to locate gpu dependency libraries&amp;quot; Feb 20 21:00:01 nixos ollama[17426]: time=2025-02-20T21:00:01.870-08:00 level=WARN source=gpu.go:669 msg=&amp;quot;unable to locate gpu dependency libraries&amp;quot; Feb 20 21:00:01 nixos ollama[17426]: time=2025-02-20T21:00:01.870-08:00 level=WARN source=gpu.go:669 msg=&amp;quot;unable to locate gpu dependency libraries&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;nvidia-smi&lt;/code&gt; command runds and detects my GPU.&lt;/p&gt; &lt;p&gt;My config has the following: ``` hardware.graphics.enable = true; hardware.nvidia = { modesetting.enable = true; powerManagement.enable = false; open = true; powerManagement.finegrained = false; nvidiaSettings = true; package = config.boot.kernelPackages.nvidiaPackages.stable; }; services.xserver.videoDrivers = [ &amp;quot;nvidia&amp;quot; ];&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama = { enable = true; acceleration = &amp;quot;cuda&amp;quot;; host = &amp;quot;0.0.0.0&amp;quot;; port = 11434; }; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;environment.systemPackages = with pkgs; [ ollama-cuda ollama&lt;/p&gt; &lt;p&gt;] ```&lt;/p&gt; &lt;p&gt;Am i missing something? I've tried restarting ollama and it still doesn't use the gpu. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TarunRaviYT"&gt; /u/TarunRaviYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivce1r/ollama_not_detecting_gpu_in_nixos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivce1r/ollama_not_detecting_gpu_in_nixos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivce1r/ollama_not_detecting_gpu_in_nixos/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T06:22:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iv2l1t</id>
    <title>Moderate anything that you can describe in natural language locally (open-source, promptable content moderation with moondream)</title>
    <updated>2025-02-21T21:42:49+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iv2l1t/moderate_anything_that_you_can_describe_in/"&gt; &lt;img alt="Moderate anything that you can describe in natural language locally (open-source, promptable content moderation with moondream)" src="https://external-preview.redd.it/dnl6aHhyMzhia2tlMSM7ImBhxEtkDDRssmJq8_dmowRl1bjQos26HvBwXcrZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbb5113cc8fe0677d1ba2cca5a93092c8063cae2" title="Moderate anything that you can describe in natural language locally (open-source, promptable content moderation with moondream)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kblnpr38bkke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iv2l1t/moderate_anything_that_you_can_describe_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iv2l1t/moderate_anything_that_you_can_describe_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-21T21:42:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1iuqghm</id>
    <title>Which deepseek model for 3090 + 64 MB of RAM?</title>
    <updated>2025-02-21T13:06:48+00:00</updated>
    <author>
      <name>/u/Coldaine</name>
      <uri>https://old.reddit.com/user/Coldaine</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, getting into running LLM's locally, was curious which size/flavor of deepseek would be most appropriate to use for local coding feedback? &lt;/p&gt; &lt;p&gt;Or if you can point me to a good resource, I can learn a bit myself!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Coldaine"&gt; /u/Coldaine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iuqghm/which_deepseek_model_for_3090_64_mb_of_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iuqghm/which_deepseek_model_for_3090_64_mb_of_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iuqghm/which_deepseek_model_for_3090_64_mb_of_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-21T13:06:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1iuzv2v</id>
    <title>Image generator</title>
    <updated>2025-02-21T19:48:56+00:00</updated>
    <author>
      <name>/u/10folder</name>
      <uri>https://old.reddit.com/user/10folder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are there any models in Ollama that can do this? I can only find models that can interpret images but haven’t come across one that can perform text to image. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/10folder"&gt; /u/10folder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iuzv2v/image_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iuzv2v/image_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iuzv2v/image_generator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-21T19:48:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivg5hl</id>
    <title>Hello! Student here, how do I go about installing or subscribing to AI software that can process and archive my documents (PDFs etc) but allows me to ask it questions, create more documents or tables from information I fed through it?</title>
    <updated>2025-02-22T10:48:24+00:00</updated>
    <author>
      <name>/u/Direct-Historian97</name>
      <uri>https://old.reddit.com/user/Direct-Historian97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;sorry I am new to all these things&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Direct-Historian97"&gt; /u/Direct-Historian97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivg5hl/hello_student_here_how_do_i_go_about_installing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivg5hl/hello_student_here_how_do_i_go_about_installing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivg5hl/hello_student_here_how_do_i_go_about_installing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T10:48:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivk0vm</id>
    <title>Ollama UI on iPhone</title>
    <updated>2025-02-22T14:33:49+00:00</updated>
    <author>
      <name>/u/SplittyDev</name>
      <uri>https://old.reddit.com/user/SplittyDev</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SplittyDev"&gt; /u/SplittyDev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/shorts/bzEXF1SS70Y?si=hSWRNEm2vZ8BZm_x"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivk0vm/ollama_ui_on_iphone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivk0vm/ollama_ui_on_iphone/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T14:33:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1iv2r5s</id>
    <title>Uncensored model for novel writing</title>
    <updated>2025-02-21T21:50:06+00:00</updated>
    <author>
      <name>/u/SnooDogs7610</name>
      <uri>https://old.reddit.com/user/SnooDogs7610</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am writing a book and when I get a bit stuck, have been using AI to help get me started. It is a murder mystery and I have been running into roadblocks due to the content, is there an uncensored model that has a large enough token memory and the creative ability? I have tried quite a few different models on ollama, but there are so many so I'm sure I've missed one. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooDogs7610"&gt; /u/SnooDogs7610 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iv2r5s/uncensored_model_for_novel_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iv2r5s/uncensored_model_for_novel_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iv2r5s/uncensored_model_for_novel_writing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-21T21:50:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivmzj3</id>
    <title>ollama vs HF API</title>
    <updated>2025-02-22T16:46:35+00:00</updated>
    <author>
      <name>/u/mans-987</name>
      <uri>https://old.reddit.com/user/mans-987</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any comparison between Ollama and HF API for vision LLMs? &lt;/p&gt; &lt;p&gt;In my experience, I noted that when I am asking questions about an image using HF API, the model (in this case &amp;quot;&lt;a href="https://ollama.com/library/moondream"&gt;moondream&lt;/a&gt;&amp;quot; answers better and more accurately than when I am using Ollama. In the comparison, I used the same image and the same prompt but left the other parameters as default (for example, system prompt, temperature...)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mans-987"&gt; /u/mans-987 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivmzj3/ollama_vs_hf_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivmzj3/ollama_vs_hf_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivmzj3/ollama_vs_hf_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T16:46:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivpetq</id>
    <title>Help Needed: Creating a Multi-Agent System with Ollama for Different API Endpoints</title>
    <updated>2025-02-22T18:27:10+00:00</updated>
    <author>
      <name>/u/FlimsyAd9995</name>
      <uri>https://old.reddit.com/user/FlimsyAd9995</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;I'm working on a project where I want to create a multi-agent system using Ollama's LLM. The goal is to use three different API endpoints to retrieve information based on user queries:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Order Details: Retrieves order-related information.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;ServiceNow Incident Details: Retrieves incident details from ServiceNow.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Wikipedia: Retrieves general-information from Wikipedia&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here's the use case:&lt;/p&gt; &lt;p&gt;If a user asks a question about an order, the system should use the order API endpoint, process the response through the trained LLM model, and display it in the chat.&lt;/p&gt; &lt;p&gt;If the user asks about incident details in ServiceNow, it should retrieve the data from the ServiceNow API, process it through the LLM, and show the response.&lt;/p&gt; &lt;p&gt;• For general queries, it should fetch data from Wikipedia, process it, and display the response.&lt;/p&gt; &lt;p&gt;The problem I'm facing is that the system always responds to order-related queries but fails to answer incident queries and general queries. It seems to be stuck on the order API endpoint.&lt;/p&gt; &lt;p&gt;Has anyone faced a similar issue or can provide guidance on how to properly route the queries to the correct API endpoint and process them through the LLM? Any help or suggestions would be greatly appreciated!&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FlimsyAd9995"&gt; /u/FlimsyAd9995 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivpetq/help_needed_creating_a_multiagent_system_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivpetq/help_needed_creating_a_multiagent_system_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivpetq/help_needed_creating_a_multiagent_system_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T18:27:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivqz1x</id>
    <title>Buying a prebuilt desktop, 8GB VRAM, ~$500 budget?</title>
    <updated>2025-02-22T19:33:27+00:00</updated>
    <author>
      <name>/u/No-Abalone1029</name>
      <uri>https://old.reddit.com/user/No-Abalone1029</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Noticed there's a good amount of discussion on building custom setups, I suppose I'd be interested in that, but firstly was curious about purchasing a gaming desktop and just dedicating that to be my 24/7 LLM server at home.&lt;/p&gt; &lt;p&gt;8GB Vram is optimal because it'd let me tinker with a small but good enough LLM. I just don't know the best way to go about this as I'm new to home server development (and GPUs for that matter).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Abalone1029"&gt; /u/No-Abalone1029 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivqz1x/buying_a_prebuilt_desktop_8gb_vram_500_budget/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivqz1x/buying_a_prebuilt_desktop_8gb_vram_500_budget/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivqz1x/buying_a_prebuilt_desktop_8gb_vram_500_budget/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T19:33:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivdshw</id>
    <title>How much of a difference does a GPU Make?</title>
    <updated>2025-02-22T07:57:58+00:00</updated>
    <author>
      <name>/u/Rerouter_</name>
      <uri>https://old.reddit.com/user/Rerouter_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've a 3960X Threadripper with 256GB of RAM which is handling the larger models reasonably well on CPU only ~5 tokens / second for the 2.51bit 671B. &lt;/p&gt; &lt;p&gt;I'm curious if adding say 3x 3060's (Going for pretty cheap nearby) into the machine would make much of a difference seeing as their RAM would not be adding much to the picture, mainly just the ability to process the model faster.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rerouter_"&gt; /u/Rerouter_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivdshw/how_much_of_a_difference_does_a_gpu_make/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivdshw/how_much_of_a_difference_does_a_gpu_make/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivdshw/how_much_of_a_difference_does_a_gpu_make/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T07:57:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1iv7hp9</id>
    <title>Ollama web Search Part 2</title>
    <updated>2025-02-22T01:45:07+00:00</updated>
    <author>
      <name>/u/Pure-Caramel1216</name>
      <uri>https://old.reddit.com/user/Pure-Caramel1216</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As promised, here is the GitHub repository for Ollama Web Search.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/nik549/ollama-search"&gt;GitHub Link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In my previous post, I mentioned plans to launch this project as a tool. If you’re interested in the tool and want to stay updated, please subscribe with &lt;a href="https://subscribepage.io/Ollamaweb"&gt;email&lt;/a&gt; for the latest news and developments.&lt;/p&gt; &lt;p&gt;Looking ahead, two additional versions are in the works:&lt;/p&gt; &lt;p&gt;One version will be faster but slightly less accurate.&lt;/p&gt; &lt;p&gt;The other will be slower yet more precise.&lt;/p&gt; &lt;p&gt;To help the project reach a wider audience, please consider upvoting if you’d like to see further developments.&lt;/p&gt; &lt;p&gt;P.S. Email subscribers will receive all updates first, and I’ll reserve subreddit posts for the most important announcements.&lt;/p&gt; &lt;p&gt;P.S.S. I’d love your suggestions for a name for the tool.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pure-Caramel1216"&gt; /u/Pure-Caramel1216 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iv7hp9/ollama_web_search_part_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iv7hp9/ollama_web_search_part_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iv7hp9/ollama_web_search_part_2/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T01:45:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1iuxn1v</id>
    <title>Perplexity’s R1 1776 is now available in Ollama's library.</title>
    <updated>2025-02-21T18:17:40+00:00</updated>
    <author>
      <name>/u/Frisky_777</name>
      <uri>https://old.reddit.com/user/Frisky_777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iuxn1v/perplexitys_r1_1776_is_now_available_in_ollamas/"&gt; &lt;img alt="Perplexity’s R1 1776 is now available in Ollama's library." src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="Perplexity’s R1 1776 is now available in Ollama's library." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Frisky_777"&gt; /u/Frisky_777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/library/r1-1776"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iuxn1v/perplexitys_r1_1776_is_now_available_in_ollamas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iuxn1v/perplexitys_r1_1776_is_now_available_in_ollamas/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-21T18:17:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivd3ip</id>
    <title>Ollama frontend using ChatterUI</title>
    <updated>2025-02-22T07:08:47+00:00</updated>
    <author>
      <name>/u/----Val----</name>
      <uri>https://old.reddit.com/user/----Val----</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ivd3ip/ollama_frontend_using_chatterui/"&gt; &lt;img alt="Ollama frontend using ChatterUI" src="https://external-preview.redd.it/OWU1M2MwNzc0bmtlMdFQRLeEpBBtEHSSn-qbmJ4l5ADqGikYUAGhEl3DS_ow.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aaea36dd077ddc1ee7db47b15f1d2574fca9e4ad" title="Ollama frontend using ChatterUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! I've been working on my app, ChatterUI for a while now, and I just wanted to show off its use as a frontend for various LLM services, including a few open source projects like Ollama!&lt;/p&gt; &lt;p&gt;You can get the app here (android only): &lt;a href="https://github.com/Vali-98/ChatterUI/releases/latest"&gt;https://github.com/Vali-98/ChatterUI/releases/latest&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/----Val----"&gt; /u/----Val---- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/oy6s7lf74nke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivd3ip/ollama_frontend_using_chatterui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivd3ip/ollama_frontend_using_chatterui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T07:08:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivmel8</id>
    <title>I designed “Prompt Targets” - a higher level abstraction than function-calling. Route to downstream agents, clarify questions and trigger common agentic scenarios</title>
    <updated>2025-02-22T16:21:50+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ivmel8/i_designed_prompt_targets_a_higher_level/"&gt; &lt;img alt="I designed “Prompt Targets” - a higher level abstraction than function-calling. Route to downstream agents, clarify questions and trigger common agentic scenarios" src="https://preview.redd.it/lyhvhusuupke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a73aee9d1cfdc50799b4ac3c8ed979288c5a940" title="I designed “Prompt Targets” - a higher level abstraction than function-calling. Route to downstream agents, clarify questions and trigger common agentic scenarios" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Function calling is now a core primitive now in building agentic applications - but there is still alot of engineering muck and duck tape required to build an accurate conversational experience. Meaning - sometimes you need to forward a prompt to the right down stream agent to handle the query, or ask for clarifying questions before you can trigger/ complete an agentic task.&lt;/p&gt; &lt;p&gt;I’ve designed a higher level abstraction called &amp;quot;prompt targets&amp;quot; inspired and modeled after how load balancers direct traffic to backend servers. The idea is to process prompts, extract critical information from them and effectively route to a downstream agent or task to handle the user prompt. The devex doesn’t deviate too much from function calling semantics - but the functionality operates at a higher level of abstraction to simplify building agentic systems&lt;/p&gt; &lt;p&gt;So how do you get started? Check out the OSS project: &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt; for more &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lyhvhusuupke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivmel8/i_designed_prompt_targets_a_higher_level/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivmel8/i_designed_prompt_targets_a_higher_level/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T16:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivl5fo</id>
    <title>Is it worth running 7b dpsk r1 or should I buy more ram?</title>
    <updated>2025-02-22T15:26:57+00:00</updated>
    <author>
      <name>/u/dTechAnimeGamingGuy</name>
      <uri>https://old.reddit.com/user/dTechAnimeGamingGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My pc specs Amd ryzen 5600g Gpu rx6600 8gb vram Ram 16gb &lt;/p&gt; &lt;p&gt;I usually work with code and reasoning for copywriting or learning. I’m a no code developer / designer and using mainly for generating scripts.&lt;/p&gt; &lt;p&gt;Been using ChatGPT free version till now but thinking to upgrading but I’m not sure if I should buy plus subscription/ get OpenAI/deepseek api or just upgrade my pc for local llm.&lt;/p&gt; &lt;p&gt;My current setup can run bartkowski’s dpsk r1 Q_6 7b/8b somewhat well. &lt;/p&gt; &lt;p&gt;P.S. I know my gpu isn’t officially supported. Found a GitHub repo that bypasses that so it’s ok. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dTechAnimeGamingGuy"&gt; /u/dTechAnimeGamingGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivl5fo/is_it_worth_running_7b_dpsk_r1_or_should_i_buy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivl5fo/is_it_worth_running_7b_dpsk_r1_or_should_i_buy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivl5fo/is_it_worth_running_7b_dpsk_r1_or_should_i_buy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T15:26:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivpm8m</id>
    <title>I Make a Customized RAG Chatbot to Talk to CSV File Using Ollama DeepSeek and Streamlit Full Tutorial Part 2</title>
    <updated>2025-02-22T18:35:42+00:00</updated>
    <author>
      <name>/u/Spirited-Wind6803</name>
      <uri>https://old.reddit.com/user/Spirited-Wind6803</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ivpm8m/i_make_a_customized_rag_chatbot_to_talk_to_csv/"&gt; &lt;img alt="I Make a Customized RAG Chatbot to Talk to CSV File Using Ollama DeepSeek and Streamlit Full Tutorial Part 2" src="https://preview.redd.it/3zxqk8oqiqke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4bd9122479810ab06552684685b83cd3d6fe122" title="I Make a Customized RAG Chatbot to Talk to CSV File Using Ollama DeepSeek and Streamlit Full Tutorial Part 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spirited-Wind6803"&gt; /u/Spirited-Wind6803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3zxqk8oqiqke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivpm8m/i_make_a_customized_rag_chatbot_to_talk_to_csv/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivpm8m/i_make_a_customized_rag_chatbot_to_talk_to_csv/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T18:35:42+00:00</published>
  </entry>
</feed>
