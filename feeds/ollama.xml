<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-05T19:34:47+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1j39jx2</id>
    <title>Nvidia and AMD graphics card at the same time in ollama?</title>
    <updated>2025-03-04T12:22:32+00:00</updated>
    <author>
      <name>/u/Other_Button_3775</name>
      <uri>https://old.reddit.com/user/Other_Button_3775</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently running Ollama on an Ubuntu system with a Nvidia 3060ti and an AMD ROG RX580. I'm trying to set it up so that Ollama uses the 3060ti primarily and falls back to the RX580 if needed.&lt;/p&gt; &lt;p&gt;Has anyone had experience with this kind of setup? Is it even possible? Are there any specific configurations or settings I should be aware of to make sure both GPUs are utilized effectively?&lt;/p&gt; &lt;p&gt;Any help or insights would be greatly appreciated! Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Other_Button_3775"&gt; /u/Other_Button_3775 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j39jx2/nvidia_and_amd_graphics_card_at_the_same_time_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j39jx2/nvidia_and_amd_graphics_card_at_the_same_time_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j39jx2/nvidia_and_amd_graphics_card_at_the_same_time_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T12:22:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3ashf</id>
    <title>Ollama on Windows isn't using my RTX 3090</title>
    <updated>2025-03-04T13:30:25+00:00</updated>
    <author>
      <name>/u/XALC1</name>
      <uri>https://old.reddit.com/user/XALC1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;As the title says my Windows 11 install isn't using my GPU but the CPU. I'm up to date on Windows and NVIDIA drivers. I'm not using docker. Could anyone help me troubleshoot?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XALC1"&gt; /u/XALC1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3ashf/ollama_on_windows_isnt_using_my_rtx_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3ashf/ollama_on_windows_isnt_using_my_rtx_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3ashf/ollama_on_windows_isnt_using_my_rtx_3090/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T13:30:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2j8nt</id>
    <title>I open-sourced Klee today, an Ollama GUI designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities.</title>
    <updated>2025-03-03T13:58:46+00:00</updated>
    <author>
      <name>/u/w-zhong</name>
      <uri>https://old.reddit.com/user/w-zhong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j2j8nt/i_opensourced_klee_today_an_ollama_gui_designed/"&gt; &lt;img alt="I open-sourced Klee today, an Ollama GUI designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." src="https://preview.redd.it/e9n94wxjdhme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cea4c13efc5bd2a090237ef7c3fe7065ceb8f0d9" title="I open-sourced Klee today, an Ollama GUI designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w-zhong"&gt; /u/w-zhong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e9n94wxjdhme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2j8nt/i_opensourced_klee_today_an_ollama_gui_designed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2j8nt/i_opensourced_klee_today_an_ollama_gui_designed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T13:58:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j35t4t</id>
    <title>Recommendations for small but capable LLMs?</title>
    <updated>2025-03-04T07:54:48+00:00</updated>
    <author>
      <name>/u/Apart_Cause_6382</name>
      <uri>https://old.reddit.com/user/Apart_Cause_6382</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From what i understand, the smaller the number of parameters is, the faster the model is and the smaller is it's filesize, but the smaller amount of knowledge it has&lt;/p&gt; &lt;p&gt;I am searching for a very fast yet knowledgeful LLM, any recommendations? Thank you in advance for any comments&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apart_Cause_6382"&gt; /u/Apart_Cause_6382 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j35t4t/recommendations_for_small_but_capable_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j35t4t/recommendations_for_small_but_capable_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j35t4t/recommendations_for_small_but_capable_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T07:54:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3tash</id>
    <title>How to use local ai tools to help aid a beginner to make a video game?</title>
    <updated>2025-03-05T03:01:11+00:00</updated>
    <author>
      <name>/u/Game-Lover44</name>
      <uri>https://old.reddit.com/user/Game-Lover44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im a bit confused on how i should use ai to help in the process of making a game and what tools i can use along with models. People tell me to ask the ai or just do it, but that makes me more stumped.&lt;/p&gt; &lt;p&gt;What would you advise?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Game-Lover44"&gt; /u/Game-Lover44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3tash/how_to_use_local_ai_tools_to_help_aid_a_beginner/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3tash/how_to_use_local_ai_tools_to_help_aid_a_beginner/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3tash/how_to_use_local_ai_tools_to_help_aid_a_beginner/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T03:01:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3pdcm</id>
    <title>Running LLM Training Examples + 8x AMD Instinct Mi60 Server + PYTORCH</title>
    <updated>2025-03-04T23:49:42+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1ob1zr4sfrme1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3pdcm/running_llm_training_examples_8x_amd_instinct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3pdcm/running_llm_training_examples_8x_amd_instinct/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T23:49:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3qsic</id>
    <title>Am I limited to 14b models on my AMD 7900xt?</title>
    <updated>2025-03-05T00:56:31+00:00</updated>
    <author>
      <name>/u/halfam</name>
      <uri>https://old.reddit.com/user/halfam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It has 20GB VRAM and I wish I had a 24GB card. What kind of models are best with the 20GB?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/halfam"&gt; /u/halfam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3qsic/am_i_limited_to_14b_models_on_my_amd_7900xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3qsic/am_i_limited_to_14b_models_on_my_amd_7900xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3qsic/am_i_limited_to_14b_models_on_my_amd_7900xt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T00:56:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3g7i1</id>
    <title>GPU vs. CPU: Deepseek R1 Distill Qwen</title>
    <updated>2025-03-04T17:27:49+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j3g7i1/gpu_vs_cpu_deepseek_r1_distill_qwen/"&gt; &lt;img alt="GPU vs. CPU: Deepseek R1 Distill Qwen" src="https://external-preview.redd.it/Q5jlmvwaxhZCXswmxVeacfiYbEM6IsziXkzDlS6KY3g.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=464630f84bff9a55c18a79fd3c34d59aa503a988" title="GPU vs. CPU: Deepseek R1 Distill Qwen" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/teWusSZoQ-M"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3g7i1/gpu_vs_cpu_deepseek_r1_distill_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3g7i1/gpu_vs_cpu_deepseek_r1_distill_qwen/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T17:27:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j38dim</id>
    <title>Tell me why NVIDIA isn't gatekeeping the future of AI for the wealthy with what ollama brings to every home of every family in the world.</title>
    <updated>2025-03-04T11:06:48+00:00</updated>
    <author>
      <name>/u/Low_Tune7301</name>
      <uri>https://old.reddit.com/user/Low_Tune7301</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR; 4090 shouldn't have been sunsetted, prices out a demographic of the entire world who can't afford the insane prices already and the new products while they're in a gold rush making $113b a year, jesus christ just make it all at this point as if you wouldn't scale everything up and sell to everyone.&lt;/p&gt; &lt;p&gt;I'm big mad lads, tell me why I'm wrong - it's incredible what tools like Ollama can bring to every single home in the world. Every human deserves to experience this revolution in their homes, ollama can support it and I think they're gate-keeping AI for the wealthy.&lt;/p&gt; &lt;p&gt;So we're in a gold rush where this company is the sole real winner of a race where every single human, store and commercial retailer is sitting on the edge of their seats for you to literally not even give new products but existing ones.&lt;/p&gt; &lt;p&gt;You can also literally have your cake and eat it too, you are the worlds leading innovator and driver for AI, you can sell cluster stacks of 50,000 H100's to big tech buddies, and you have every other human in the world playing at home crawling the web relentlessly - not even just for your new products but your old ones as well and people will love you for it forever.&lt;/p&gt; &lt;p&gt;You will be cemented in history for leading the hardware innovation that brought this experience to every home in the world, you have successfully changed the future.&lt;/p&gt; &lt;p&gt;Every product with 24GB+ of VRAM sold out everywhere, every new product you release sold at the drop of a hat - all over the entire world. You've won. You are the winner of capitalism.&lt;/p&gt; &lt;p&gt;So what does the winner receive? - A hundred and thirteen billion dollars in revenue just for last year alone.&lt;/p&gt; &lt;p&gt;So while the world is discovering fire again, the winner:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Sunsets the RTX 4090 - an incredibly performant product that could bring that revolution into every family home in the world at a reasonable price point. Ooh on that note &amp;gt;&lt;/li&gt; &lt;li&gt;NVIDIA is somehow magically undersupplied year on year yet keeps bringing out a new card and the cycle repeats. If every performant product is sold out across the entire world and you're big dog Jensen what do those meetings look like? Are people telling you they can't build enough factories? Did they get confused now after the first god knows how many you've already made? After three years those meetings are still just like nah sorry can't get it right?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It makes a lot of sense though if you've got an agenda. It doesn't make sense you wouldn't scale up and go all in we're in an insane new age you're driving and the world wants everything you could possibly offer.&lt;/p&gt; &lt;p&gt;I'm not even saying don't sell or whatever like run at capitalism big chief you've won I'm just saying sell to both sides because instead you choose to keep repeating the pattern of under-supplying and gatekeeping AI for the wealthy which is disgusting with what's possible with Ollama and tools.&lt;/p&gt; &lt;p&gt;I deliberately use the word choose because once is an accident, twice is coincidence, three times is a pattern.&lt;/p&gt; &lt;p&gt;But go on, keep promising the next big thing while under-supplying and gatekeeping this revolution, price an entire demographic of the world out and just keep making H100's for those clusters your big tech buddies want.&lt;/p&gt; &lt;p&gt;Look I'm just saying sell to both sides. The world is discovering fire again and every human in the world deserves to experience that, you can still have your cake and eat it too. Make everything.&lt;/p&gt; &lt;p&gt;What you might say to this is people can still use the products the big tech companies create and connect to commercial models which you're right but something about the fact they just commercialized hitting the enter key to the lower socio-economic demographics in the world just doesn't feel right to me but hey I never liked the pokies anyway.&lt;/p&gt; &lt;p&gt;So go, there's my ollama rant - tell me why i'm cooked for thinking a company who made $113.26 billion in revenue last year and god knows how much the years before shouldn't have figured it out at this point and isn't just gatekeeping AI which is disgusting in a world with what tools combined with ollama make possible for every family in the world.&lt;/p&gt; &lt;p&gt;God damn I hope the open source community distilling these models for cheaper hardware godspeed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Low_Tune7301"&gt; /u/Low_Tune7301 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j38dim/tell_me_why_nvidia_isnt_gatekeeping_the_future_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j38dim/tell_me_why_nvidia_isnt_gatekeeping_the_future_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j38dim/tell_me_why_nvidia_isnt_gatekeeping_the_future_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T11:06:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j40svi</id>
    <title>Problem with embeddings API and OpenWebUI?</title>
    <updated>2025-03-05T11:18:16+00:00</updated>
    <author>
      <name>/u/SnowBoy_00</name>
      <uri>https://old.reddit.com/user/SnowBoy_00</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I just updated Ollama to its latest version (0.5.13) and I am encountering issues when using an embedding model served through Ollama in OpenWebUI. According to OpenWebUI's log, the problem is:&lt;br /&gt; requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: &lt;a href="http://host.docker.internal:11434/api/embed"&gt;http://host.docker.internal:11434/api/embed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was just wondering, am I the only one facing this issue with the latest Ollama version? Downgrading seems to fix it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnowBoy_00"&gt; /u/SnowBoy_00 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j40svi/problem_with_embeddings_api_and_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j40svi/problem_with_embeddings_api_and_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j40svi/problem_with_embeddings_api_and_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T11:18:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3uc08</id>
    <title>Anyone managed to use free LLM models in cursor editor?</title>
    <updated>2025-03-05T03:56:35+00:00</updated>
    <author>
      <name>/u/blnkslt</name>
      <uri>https://old.reddit.com/user/blnkslt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j3uc08/anyone_managed_to_use_free_llm_models_in_cursor/"&gt; &lt;img alt="Anyone managed to use free LLM models in cursor editor?" src="https://external-preview.redd.it/DVMPuS2zjSAzR8RNilSvelF0QhBxxZrn22PYYxcxQcw.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de4b5fc3ab79b856565c38b6eca8d6e9fa679e4b" title="Anyone managed to use free LLM models in cursor editor?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It used to be possible to use your local model in Cursor by prxying your localhost through something like ngrok. But when I followed &lt;a href="https://www.youtube.com/watch?v=Q_sy45XggeM"&gt;this tutorial&lt;/a&gt; to use my local Qwen model, I failed and got the error below. Seems like they have closed the loophole to cage you into their paid models. So I'm wondering if any one recently has been successful in doing so?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/we1f2wwhnsme1.png?width=1032&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2d7ccaff388b4528ecf36972c0e16285fea51e36"&gt;https://preview.redd.it/we1f2wwhnsme1.png?width=1032&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2d7ccaff388b4528ecf36972c0e16285fea51e36&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blnkslt"&gt; /u/blnkslt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3uc08/anyone_managed_to_use_free_llm_models_in_cursor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3uc08/anyone_managed_to_use_free_llm_models_in_cursor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3uc08/anyone_managed_to_use_free_llm_models_in_cursor/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T03:56:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3wobw</id>
    <title>Best Approach for Faster LLM Inference on Mac M3?</title>
    <updated>2025-03-05T06:14:34+00:00</updated>
    <author>
      <name>/u/genzo-w</name>
      <uri>https://old.reddit.com/user/genzo-w</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hi everyone,&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm currently learning about Generative AI and experimenting with LLMs for summarization tasks. However, I’m facing some challenges with inference speed and access to APIs.&lt;/p&gt; &lt;h1&gt;What I've Tried So Far:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;ChatGPT API&lt;/strong&gt; – Limited access, so not a feasible option for my use case.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama (Running Locally)&lt;/strong&gt; – Works but takes around &lt;strong&gt;2 minutes&lt;/strong&gt; to generate a summary, which is too slow.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LM Studio&lt;/strong&gt; – Found that &lt;strong&gt;llama.cpp&lt;/strong&gt; utilizes &lt;strong&gt;Metal&lt;/strong&gt; capabilities on Mac Silicon for some models, but I’m still exploring if this improves inference significantly.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;My Setup:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;MacBook with &lt;strong&gt;M3 chip&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Running models locally (since API access is limited)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I’m Looking For:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Faster inference locally&lt;/strong&gt; – Are there any optimizations for LLM inference on Mac M3?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Free API alternatives&lt;/strong&gt; – Any free services that provide GPT-like APIs with better access?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Better local solutions&lt;/strong&gt; – Does something like &lt;strong&gt;llama.cpp&lt;/strong&gt; + optimized quantization (like GPTQ or GGUF) help significantly?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Would love to hear suggestions from those who have tackled similar issues! Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/genzo-w"&gt; /u/genzo-w &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3wobw/best_approach_for_faster_llm_inference_on_mac_m3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3wobw/best_approach_for_faster_llm_inference_on_mac_m3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3wobw/best_approach_for_faster_llm_inference_on_mac_m3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T06:14:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1j45i3h</id>
    <title>Server config suggestions?</title>
    <updated>2025-03-05T15:22:59+00:00</updated>
    <author>
      <name>/u/htxgaybro</name>
      <uri>https://old.reddit.com/user/htxgaybro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking to setup Ollama for work to be used between 10-20 people. What should be the server config I should request my infra team for? We plan to use it as our LLM assistant and use it for helping us with our work. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/htxgaybro"&gt; /u/htxgaybro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j45i3h/server_config_suggestions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j45i3h/server_config_suggestions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j45i3h/server_config_suggestions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T15:22:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j46o1k</id>
    <title>AI moderates movies so editors don't have to: Automatic Smoking Disclaimer Tool (open source, runs 100% locally)</title>
    <updated>2025-03-05T16:13:05+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j46o1k/ai_moderates_movies_so_editors_dont_have_to/"&gt; &lt;img alt="AI moderates movies so editors don't have to: Automatic Smoking Disclaimer Tool (open source, runs 100% locally)" src="https://external-preview.redd.it/Y2kyemNrNWNid21lMYil-mvvsBaemYDUlRSkvBniidK7oobmA4FpXb-7Z2sJ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5efd2bc2fee5e7089343ee136b70e07383815723" title="AI moderates movies so editors don't have to: Automatic Smoking Disclaimer Tool (open source, runs 100% locally)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vwb6qk5cbwme1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j46o1k/ai_moderates_movies_so_editors_dont_have_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j46o1k/ai_moderates_movies_so_editors_dont_have_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T16:13:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3y8rg</id>
    <title>What models could I reasonably use on a system with 32G RAM and 8G VRAM?</title>
    <updated>2025-03-05T08:06:49+00:00</updated>
    <author>
      <name>/u/prodego</name>
      <uri>https://old.reddit.com/user/prodego</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Arch&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prodego"&gt; /u/prodego &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3y8rg/what_models_could_i_reasonably_use_on_a_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3y8rg/what_models_could_i_reasonably_use_on_a_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3y8rg/what_models_could_i_reasonably_use_on_a_system/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T08:06:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4893q</id>
    <title>Ollama working in CLI not API</title>
    <updated>2025-03-05T17:17:49+00:00</updated>
    <author>
      <name>/u/Antoni_Nabzdyk</name>
      <uri>https://old.reddit.com/user/Antoni_Nabzdyk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j4893q/ollama_working_in_cli_not_api/"&gt; &lt;img alt="Ollama working in CLI not API" src="https://b.thumbs.redditmedia.com/SJwDmkzg5Ij9kJ-qPPc1DnIGSr6qK3BSL-B3wx_sNmE.jpg" title="Ollama working in CLI not API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys&lt;/p&gt; &lt;p&gt;So a pretty strange issue, where my CLI is currently giving me responses on AAPL stock analysis, and my API isn't (More details on images) &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9xjrt0yjmwme1.png?width=899&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=91b3598de9e3e252718963fe18459dd2d79c40d7"&gt;https://preview.redd.it/9xjrt0yjmwme1.png?width=899&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=91b3598de9e3e252718963fe18459dd2d79c40d7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u5l017somwme1.png?width=910&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5efd78d2b77f07857fef8fd8b660ed2d8aa1cae7"&gt;https://preview.redd.it/u5l017somwme1.png?width=910&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5efd78d2b77f07857fef8fd8b660ed2d8aa1cae7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The API is stuck at that point. What should I do? I sue a VPS with 8gb of Ram.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ou3m5rgtmwme1.png?width=683&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dbd3962cb7e2738380aa15d6f3729643c0dee5a1"&gt;https://preview.redd.it/ou3m5rgtmwme1.png?width=683&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dbd3962cb7e2738380aa15d6f3729643c0dee5a1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What would you do? I'm a new person to this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Antoni_Nabzdyk"&gt; /u/Antoni_Nabzdyk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4893q/ollama_working_in_cli_not_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4893q/ollama_working_in_cli_not_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4893q/ollama_working_in_cli_not_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T17:17:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3riix</id>
    <title>OpenArc v1.0.1: openai endpoints, gradio dashboard with chat- get faster inference on intel CPUs, GPUs and NPUs</title>
    <updated>2025-03-05T01:31:25+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;My project, &lt;a href="https://github.com/SearchSavior/OpenArc"&gt;OpenArc&lt;/a&gt;, is an inference engine built with OpenVINO for leveraging hardware acceleration on Intel CPUs, GPUs and NPUs. Users can expect similar workflows to what's possible with Ollama, LM-Studio, Jan, OpenRouter, including a built in gradio chat, management dashboard and tools for working with Intel devices.&lt;/p&gt; &lt;p&gt;OpenArc is one of the first FOSS projects to offer a model agnostic serving engine taking full advantage of the OpenVINO runtime available from Transformers. Many other projects have support for OpenVINO as an extension but OpenArc features detailed documentation, GUI tools and discussion. Infer at the edge with text-based large language models with openai compatible endpoints tested with Gradio, OpenWebUI and SillyTavern. &lt;/p&gt; &lt;p&gt;Vision support is coming soon.&lt;/p&gt; &lt;p&gt;Since launch community support has been overwhelming; I even have a funding opportunity for OpenArc! For my first project that's pretty cool.&lt;/p&gt; &lt;p&gt;One thing we talked about was that OpenArc needs contributors who are excited about inference and getting good performance from their Intel devices.&lt;/p&gt; &lt;p&gt;Here's the ripcord:&lt;/p&gt; &lt;p&gt;An official &lt;a href="https://discord.gg/PnuTBVcr"&gt;Discord!&lt;/a&gt; - Best way to reach me. - If you are interested in contributing join the Discord!&lt;/p&gt; &lt;p&gt;Discussions on GitHub for:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SearchSavior/OpenArc/discussions/11"&gt;Linux Drivers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SearchSavior/OpenArc/discussions/12"&gt;Windows Drivers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SearchSavior/OpenArc/discussions/13"&gt;Environment Setup&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Instructions and models for testing out text generation for &lt;a href="https://github.com/SearchSavior/OpenArc/issues/14"&gt;NPU devices&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;A sister repo, &lt;a href="https://github.com/SearchSavior/OpenArcProjects"&gt;OpenArcProjects&lt;/a&gt;! - Share the things you build with OpenArc, OpenVINO, oneapi toolkit, IPEX-LLM and future tooling from Intel&lt;/p&gt; &lt;p&gt;Thanks for checking out OpenArc. I hope it ends up being a useful tool.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3riix/openarc_v101_openai_endpoints_gradio_dashboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3riix/openarc_v101_openai_endpoints_gradio_dashboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3riix/openarc_v101_openai_endpoints_gradio_dashboard/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T01:31:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3smbl</id>
    <title>Recommendations for a 24GB VRAM card for running Ollama and mistral-small?</title>
    <updated>2025-03-05T02:26:32+00:00</updated>
    <author>
      <name>/u/THenrich</name>
      <uri>https://old.reddit.com/user/THenrich</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recommendations for a 24GB VRAM card for running Ollama and mistral-small?&lt;br /&gt; Preferably under $1000.&lt;/p&gt; &lt;p&gt;I might run larger models in the future. I am going to send thousands of prompts to Ollama so I need something performant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/THenrich"&gt; /u/THenrich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3smbl/recommendations_for_a_24gb_vram_card_for_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3smbl/recommendations_for_a_24gb_vram_card_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3smbl/recommendations_for_a_24gb_vram_card_for_running/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T02:26:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4anpe</id>
    <title>Anyone else having slow LLM response issues with 0.5.13?</title>
    <updated>2025-03-05T18:53:02+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;0.5.13 (accessed via Open WebUI 0.5.18) seems to make everything very slow, non-responsive and generally unusable for me. I’ve tried it on 3 different computers and same issues on all of them. I downgraded to 0.5.12 and things are running perfectly on that release but 0.5.13 runs like hot garbage. Anyone experiencing similar issues? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4anpe/anyone_else_having_slow_llm_response_issues_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j4anpe/anyone_else_having_slow_llm_response_issues_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j4anpe/anyone_else_having_slow_llm_response_issues_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T18:53:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3xirv</id>
    <title>Ollama autocomplete plugin for vim</title>
    <updated>2025-03-05T07:12:28+00:00</updated>
    <author>
      <name>/u/EMurph55</name>
      <uri>https://old.reddit.com/user/EMurph55</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j3xirv/ollama_autocomplete_plugin_for_vim/"&gt; &lt;img alt="Ollama autocomplete plugin for vim" src="https://external-preview.redd.it/0acOiSmWxXJhzHA-0AmG5I6VnFmecE3jldQQztOfa0E.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be3b91aa1b587ec673db53bdb1eb9939b81aaaa4" title="Ollama autocomplete plugin for vim" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EMurph55"&gt; /u/EMurph55 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/whatever555/free-pilot-vim"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3xirv/ollama_autocomplete_plugin_for_vim/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3xirv/ollama_autocomplete_plugin_for_vim/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T07:12:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j484ba</id>
    <title>Is it possible to run models on a pc with 2 gpu-s, but one is amd and one is nvidia? Has anyone tried that</title>
    <updated>2025-03-05T17:12:32+00:00</updated>
    <author>
      <name>/u/fracturedbudhole</name>
      <uri>https://old.reddit.com/user/fracturedbudhole</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello llama friends. I am wondering if it is possible to run local models on a pc with 2 gpu-s, but one is amd amd one ia nvidia. I am currently running on amd 6800xt 16gb and i would like to get more video memory to run bigger models, recently i have seen a GTX 1080ti offering used for a good deal - 200e i think, so i am wondering if it is possible to use both cards to run moddels with 26gb combined vram. Or is it better to get another amd gpu (does it need to be the same or could it be newer model)? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fracturedbudhole"&gt; /u/fracturedbudhole &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j484ba/is_it_possible_to_run_models_on_a_pc_with_2_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j484ba/is_it_possible_to_run_models_on_a_pc_with_2_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j484ba/is_it_possible_to_run_models_on_a_pc_with_2_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T17:12:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j453s7</id>
    <title>Apple released Mac Studio with M4 Max and M3 Ultra</title>
    <updated>2025-03-05T15:05:06+00:00</updated>
    <author>
      <name>/u/vsurresh</name>
      <uri>https://old.reddit.com/user/vsurresh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;M3 Ultra supports up to 512 GB of RAM for almost £10k&lt;/p&gt; &lt;p&gt;M4 Max with 128 GB of RAM is around £3600&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.apple.com/uk/shop/buy-mac/mac-studio"&gt;https://www.apple.com/uk/shop/buy-mac/mac-studio&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vsurresh"&gt; /u/vsurresh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j453s7/apple_released_mac_studio_with_m4_max_and_m3_ultra/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j453s7/apple_released_mac_studio_with_m4_max_and_m3_ultra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j453s7/apple_released_mac_studio_with_m4_max_and_m3_ultra/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T15:05:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3nwiw</id>
    <title>Generate an Entire Project from ONE Prompt</title>
    <updated>2025-03-04T22:44:22+00:00</updated>
    <author>
      <name>/u/No-Mulberry6961</name>
      <uri>https://old.reddit.com/user/No-Mulberry6961</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created an AI platform that allows a user to enter a single prompt with technical requirements and the LLM of choice thoroughly plans out and builds the entire thing nonstop until it is completely finished.&lt;/p&gt; &lt;p&gt;Here is a project it built last night using Claude 3.7, which took about 3 hours and has 214 files (can use any LLM, local, API, ollama etc…)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/justinlietz93/neuroca"&gt;https://github.com/justinlietz93/neuroca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m improving it every day as well and building an extension that locks into existing projects to finish them or add functionality&lt;/p&gt; &lt;p&gt;I have been asked to use my system to finish this project:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Recruitler/SortableTS"&gt;https://github.com/Recruitler/SortableTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If it is capable of doing that with a single prompt, then I can prove this legitimately is a novel and potentially breakthrough strategy for software development using AI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Mulberry6961"&gt; /u/No-Mulberry6961 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3nwiw/generate_an_entire_project_from_one_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3nwiw/generate_an_entire_project_from_one_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3nwiw/generate_an_entire_project_from_one_prompt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T22:44:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3fh7d</id>
    <title>Ollama-OCR</title>
    <updated>2025-03-04T16:58:47+00:00</updated>
    <author>
      <name>/u/imanoop7</name>
      <uri>https://old.reddit.com/user/imanoop7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I open-sourced &lt;strong&gt;Ollama-OCR&lt;/strong&gt; – an advanced &lt;strong&gt;OCR tool&lt;/strong&gt; powered by &lt;strong&gt;LLaVA 7B&lt;/strong&gt; and &lt;strong&gt;Llama 3.2 Vision&lt;/strong&gt; to extract text from images with high accuracy! 🚀&lt;/p&gt; &lt;p&gt;🔹 &lt;strong&gt;Features:&lt;/strong&gt;&lt;br /&gt; ✅ Supports &lt;strong&gt;Markdown, Plain Text, JSON, Structured, Key-Value Pairs&lt;/strong&gt;&lt;br /&gt; ✅ &lt;strong&gt;Batch processing&lt;/strong&gt; for handling multiple images efficiently&lt;br /&gt; ✅ Uses &lt;strong&gt;state-of-the-art vision-language models&lt;/strong&gt; for better OCR&lt;br /&gt; ✅ Ideal for &lt;strong&gt;document digitization, data extraction, and automation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Check it out &amp;amp; contribute! 🔗 &lt;a href="https://github.com/imanoop7/Ollama-OCR"&gt;GitHub: Ollama-OCR&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Details about Python Package -&lt;a href="https://medium.com/@mauryaanoop3/ollama-ocr-now-available-as-a-python-package-ff5e4240eb26"&gt; &lt;strong&gt;Guide&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thoughts? Feedback? Let’s discuss! 🔥&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/imanoop7"&gt; /u/imanoop7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3fh7d/ollamaocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3fh7d/ollamaocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3fh7d/ollamaocr/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T16:58:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j44psq</id>
    <title>Server Room / Storage</title>
    <updated>2025-03-05T14:47:54+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j44psq/server_room_storage/"&gt; &lt;img alt="Server Room / Storage" src="https://preview.redd.it/u0r5ec7xuvme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7656952a35b5e1c04c19106e61522e536e80e6ae" title="Server Room / Storage" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u0r5ec7xuvme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j44psq/server_room_storage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j44psq/server_room_storage/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T14:47:54+00:00</published>
  </entry>
</feed>
