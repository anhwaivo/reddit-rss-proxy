<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-04-08T17:36:13+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jtcuwv</id>
    <title>Deterministic output with same seed - example</title>
    <updated>2025-04-07T04:20:30+00:00</updated>
    <author>
      <name>/u/binuuday</name>
      <uri>https://old.reddit.com/user/binuuday</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jtcuwv/deterministic_output_with_same_seed_example/"&gt; &lt;img alt="Deterministic output with same seed - example" src="https://a.thumbs.redditmedia.com/GhsfXzRn2Zg7o5WMYtymEXZzFU2iv6W9dcgYQpxK5K4.jpg" title="Deterministic output with same seed - example" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most experts know this already, this entry is for people who are new to ollama, like me.&lt;/p&gt; &lt;p&gt;During some RAG cases, we need our output to be deterministic. Ollama allows this by setting the seed value, to the same number, for consecutive requests. This will not work in chat mode, or where multiple prompts are sent. (All prompts to the Ollama server needs to be same)&lt;/p&gt; &lt;p&gt;This is a property of the generation function, a random tensor is created upon which the layers act upon. If we don't give seed, or give seed as -1, the initial tensor is filled with truly random numbers. But when same seed value is given the tensor is filled with deterministic random numbers ( assuming you are on the same machine and using the same functionality, process). In Ollama's case we are hitting the same processs running on the same machine too.&lt;/p&gt; &lt;p&gt;If you are using any UI, you have to clear the history, to get deterministic output, because they tend to maintain sessions, and send the history of chat in prompt. Example of curl commands given below.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a6147idz7cte1.png?width=2610&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=690d0527eddea5f2ee7bd965d7879fc26791f77d"&gt;Deterministic Output with Same Seed&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;date curl -s http://localhost:11434/api/chat -d '{ &amp;quot;model&amp;quot;: &amp;quot;llama3.2:latest&amp;quot;, &amp;quot;messages&amp;quot;: [ { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Give 5 random numbers and 5 random animals&amp;quot; } ], &amp;quot;options&amp;quot;: { &amp;quot;seed&amp;quot;: 32988 }, &amp;quot;stream&amp;quot;: false }' | jq '.message.content' Mon Apr 7 09:47:38 IST 2025 &amp;quot;Here are 5 random numbers:\n\n1. 854\n2. 219\n3. 467\n4. 982\n5. 135\n\nAnd here are 5 random animals:\n\n1. Quail\n2. Narwhal\n3. Meerkat\n4. Lemur\n5. Otter&amp;quot; date curl -s http://localhost:11434/api/chat -d '{ &amp;quot;model&amp;quot;: &amp;quot;llama3.2:latest&amp;quot;, &amp;quot;messages&amp;quot;: [ { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Give 5 random numbers and 5 random animals&amp;quot; } ], &amp;quot;options&amp;quot;: { &amp;quot;seed&amp;quot;: 32988 }, &amp;quot;stream&amp;quot;: false }' | jq '.message.content' Mon Apr 7 09:49:03 IST 2025 &amp;quot;Here are 5 random numbers:\n\n1. 854\n2. 219\n3. 467\n4. 982\n5. 135\n\nAnd here are 5 random animals:\n\n1. Quail\n2. Narwhal\n3. Meerkat\n4. Lemur\n5. Otter&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Above are same command at different point of time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/binuuday"&gt; /u/binuuday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtcuwv/deterministic_output_with_same_seed_example/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtcuwv/deterministic_output_with_same_seed_example/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtcuwv/deterministic_output_with_same_seed_example/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T04:20:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtcsfp</id>
    <title>I want to create a RAG from tabular data (databases). How do I proceed?</title>
    <updated>2025-04-07T04:16:15+00:00</updated>
    <author>
      <name>/u/thecrazytughlaq</name>
      <uri>https://old.reddit.com/user/thecrazytughlaq</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am fairly new to RAG. I have built a RAG to chat with PDFs, based on youtube videos, using Ollama models and ChromaDB. &lt;/p&gt; &lt;p&gt;I want to create a RAG that helps me chat with tabular data. I want to use it to forecast values, look up values etc. I am trying it on PDFs with tables of numerical values first. Can I proceed the same way as I did for text-content PDFs, or are there any other factors I must consider?&lt;/p&gt; &lt;p&gt;As for the next step, connecting it to SQL database, would I need to process the database in any way before I connect it to the langchain sql package? And can I expect reasonable accuracy (as much as I expect from the RAG based on text-based content) ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecrazytughlaq"&gt; /u/thecrazytughlaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtcsfp/i_want_to_create_a_rag_from_tabular_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtcsfp/i_want_to_create_a_rag_from_tabular_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtcsfp/i_want_to_create_a_rag_from_tabular_data/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T04:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtrb0p</id>
    <title>Fine turning pre trained model</title>
    <updated>2025-04-07T17:42:08+00:00</updated>
    <author>
      <name>/u/GeorgeSKG_</name>
      <uri>https://old.reddit.com/user/GeorgeSKG_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fine turning pre trained model&lt;/p&gt; &lt;p&gt;Hello everyone,im trying to train a pre trained model (Mistral 7b) on discord. If you wanna help and join to a project (its a huge project if we have the dataset) comment and I will dm you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GeorgeSKG_"&gt; /u/GeorgeSKG_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtrb0p/fine_turning_pre_trained_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtrb0p/fine_turning_pre_trained_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtrb0p/fine_turning_pre_trained_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T17:42:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtka0k</id>
    <title>Bus/Trucks Vehicle Make and Models Dataset</title>
    <updated>2025-04-07T12:40:54+00:00</updated>
    <author>
      <name>/u/Senior-Reserve3732</name>
      <uri>https://old.reddit.com/user/Senior-Reserve3732</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm wondering if I can find a model that has been trained with all bus and trucks makes and models available worldwide. I would like to use it's trained data to get spareparts products for each of the vehicles.&lt;/p&gt; &lt;p&gt;Is there any way to get this data? I tried a lot of public datasets but none of them is complete.&lt;/p&gt; &lt;p&gt;Thank you in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Senior-Reserve3732"&gt; /u/Senior-Reserve3732 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtka0k/bustrucks_vehicle_make_and_models_dataset/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtka0k/bustrucks_vehicle_make_and_models_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtka0k/bustrucks_vehicle_make_and_models_dataset/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T12:40:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju2uqy</id>
    <title>I made an App to fit AI into your keyboard</title>
    <updated>2025-04-08T02:15:42+00:00</updated>
    <author>
      <name>/u/Ehsan1238</name>
      <uri>https://old.reddit.com/user/Ehsan1238</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I'm a college student working hard on &lt;a href="http://shiftappai.com/"&gt;Shift&lt;/a&gt;. It basically lets you instantly use Claude (and other AI models) right from your keyboard, anywhere on your laptop, no copy-pasting, no app-switching.&lt;/p&gt; &lt;p&gt;There will be local LLMs added soon as well!&lt;/p&gt; &lt;p&gt;I currently have 140 users but trying hard to expand more and get more people to try it and get more feedback!&lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;p&gt;* Highlight text or code anywhere.&lt;/p&gt; &lt;p&gt;* Double-tap Shift.&lt;/p&gt; &lt;p&gt;* Type your prompt and let Claude handle the rest.&lt;/p&gt; &lt;p&gt;You can keep contexts, chat interactively, save custom prompts, and even integrate other models like GPT and Gemini directly. It's made my workflow smoother, and I'm genuinely excited to hear what you all think!&lt;/p&gt; &lt;p&gt;There is also a feature called shortcuts where you can link a prompt to a keyboard combination like linking &amp;quot;rephrase this&amp;quot; or &amp;quot;comment this code&amp;quot; to a keyboard combo like Shift+Command.&lt;/p&gt; &lt;p&gt;I've been working on this for months now and honestly, it's been a game-changer for my own productivity. I built it because I was tired of constantly switching between windows and copying/pasting stuff just to use AI tools.&lt;/p&gt; &lt;p&gt;Anyway, I'm happy to answer any questions, and of course, your feedback would mean a lot to me. I'm just a solo dev trying to make something useful, so hearing from real users helps tremendously!&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;p&gt;Also if you want to see demos I show daily use cases of how it can be used here on this youtube channel: &lt;a href="https://www.youtube.com/@Shiftappai"&gt;https://www.youtube.com/@Shiftappai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Or just Shift's subreddit: &lt;a href="https://www.reddit.com/r/ShiftApp/"&gt;r/ShiftApp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ehsan1238"&gt; /u/Ehsan1238 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju2uqy/i_made_an_app_to_fit_ai_into_your_keyboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju2uqy/i_made_an_app_to_fit_ai_into_your_keyboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ju2uqy/i_made_an_app_to_fit_ai_into_your_keyboard/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T02:15:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtvn86</id>
    <title>Ollama and RooCode/Continue on Mac M1</title>
    <updated>2025-04-07T20:37:29+00:00</updated>
    <author>
      <name>/u/onedjscream</name>
      <uri>https://old.reddit.com/user/onedjscream</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone gotten RooCode and Continue to work well with Ollama on a MacBook Pro M1 16GB? Which models? My setup with starcoder and qwen start to heat up especially with Continue and 1000ms debounce.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onedjscream"&gt; /u/onedjscream &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtvn86/ollama_and_roocodecontinue_on_mac_m1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtvn86/ollama_and_roocodecontinue_on_mac_m1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtvn86/ollama_and_roocodecontinue_on_mac_m1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T20:37:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt3ndd</id>
    <title>How do small models contain so much information?</title>
    <updated>2025-04-06T20:27:33+00:00</updated>
    <author>
      <name>/u/BallPythonTech</name>
      <uri>https://old.reddit.com/user/BallPythonTech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am amazed at how much data small models can re-create. For example, Gemma3:4b, I ask it to list the books of the Old Testament. It leaves some out listing only 35. &lt;/p&gt; &lt;p&gt;But how does it even store that? &lt;/p&gt; &lt;p&gt;List the books by Edgar Allen Poe, it gets most of them, same for Dr Seuss. Published years are often wrong but still. &lt;/p&gt; &lt;p&gt;List publications by Albert Einstein - mostly correct.&lt;/p&gt; &lt;p&gt;List elementary particles - it lists half of them, 17&lt;/p&gt; &lt;p&gt;So how in 3GB is it able to store so much information or is Ollama going out to the internet to get more data?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BallPythonTech"&gt; /u/BallPythonTech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jt3ndd/how_do_small_models_contain_so_much_information/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jt3ndd/how_do_small_models_contain_so_much_information/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jt3ndd/how_do_small_models_contain_so_much_information/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T20:27:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju1pvq</id>
    <title>Ollama Cli results different from the API calls</title>
    <updated>2025-04-08T01:17:49+00:00</updated>
    <author>
      <name>/u/Ibrahimkm</name>
      <uri>https://old.reddit.com/user/Ibrahimkm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everybody,&lt;/p&gt; &lt;p&gt;I was testing some small models as mistral and llama3.1 on Ollama and I found out when I use the CLI the results are different from the one that the model provide when I call it in a python script.&lt;br /&gt; I tried to check the default parameters as temperature or top_P, top_k that the CLI uses but it seems there is no way to know (at least to my knowledge)&lt;/p&gt; &lt;p&gt;I am testing the LLM for a classification task, it will respond with &amp;quot;Attack&amp;quot; or &amp;quot;Benign&amp;quot; the CLI seems to get better results when I manually test the same prompt.&lt;/p&gt; &lt;p&gt;Also I was using ollama models for a long time and I am thinking of testing other version of these models finetuned by users. Where can I find these customized models ? I saw some in huggingface but the search engine wasn't very good there was no way to know how good the model any review or how many person tested it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ibrahimkm"&gt; /u/Ibrahimkm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju1pvq/ollama_cli_results_different_from_the_api_calls/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju1pvq/ollama_cli_results_different_from_the_api_calls/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ju1pvq/ollama_cli_results_different_from_the_api_calls/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T01:17:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju6j3y</id>
    <title>Cheap/free temporary cloud</title>
    <updated>2025-04-08T05:52:16+00:00</updated>
    <author>
      <name>/u/cucca77</name>
      <uri>https://old.reddit.com/user/cucca77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I tried to do some tests of rag with the hardware at my disposal (intel cpu, 16gb, amd gpu) and the results were obviously terrible in terms of performance and results compared to chatgpt. I would still like to test a self-hosted rag and so I was wondering if there were any free or very cheap clouds with the possibility of subscribing for a single month to do some tests. I think it is difficult/impossible, but I ask you experts... do you know anyone? &lt;/p&gt; &lt;p&gt;thanks to everyone&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cucca77"&gt; /u/cucca77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju6j3y/cheapfree_temporary_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju6j3y/cheapfree_temporary_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ju6j3y/cheapfree_temporary_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T05:52:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju6nwu</id>
    <title>Advice needed</title>
    <updated>2025-04-08T06:01:23+00:00</updated>
    <author>
      <name>/u/RrayAgent_art</name>
      <uri>https://old.reddit.com/user/RrayAgent_art</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working on a project for my c++ class, where I need to create a chess game with an ai assisted bot. And I was wondering if there was someway to have the host and client rolled into the application? I found ollama.hpp, but since I need to submit it I need to make sure it can be accessed from any windows application.&lt;/p&gt; &lt;p&gt;Thank you in advance for any help you can give.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RrayAgent_art"&gt; /u/RrayAgent_art &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju6nwu/advice_needed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju6nwu/advice_needed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ju6nwu/advice_needed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T06:01:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtrfrb</id>
    <title>How do you determine system requirements for different models?</title>
    <updated>2025-04-07T17:47:35+00:00</updated>
    <author>
      <name>/u/some1_online</name>
      <uri>https://old.reddit.com/user/some1_online</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I've been running different models locally but I try to go for the most lightweight models with the least parameters. I'm wondering, how do I determine the system requirements (or speed or efficiency) for each model given my hardware so I can run the best possible models on my machine?&lt;/p&gt; &lt;p&gt;Here's what my hardware looks like for reference:&lt;/p&gt; &lt;p&gt;RTX 3060 12 GB VRAM GPU&lt;/p&gt; &lt;p&gt;16 GB RAM (can be upgraded to 32 easily)&lt;/p&gt; &lt;p&gt;Ryzen 5 4500 6 core, 12 thread CPU&lt;/p&gt; &lt;p&gt;512 GB SSD&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/some1_online"&gt; /u/some1_online &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtrfrb/how_do_you_determine_system_requirements_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtrfrb/how_do_you_determine_system_requirements_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtrfrb/how_do_you_determine_system_requirements_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T17:47:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtylaj</id>
    <title>Ollama and mistral3.1 cant fit into 24GB Vram</title>
    <updated>2025-04-07T22:45:51+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Why mistral-small3.1:latest b9aaf0c2586a 15 GB goes over 24GB when it is loaded?&lt;br /&gt; And for example Gemma3 which size on disk is larger, 17GB fits fine in 24GB?&lt;/p&gt; &lt;p&gt;What am I doing wrong? How to fit mistral3.1 better? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtylaj/ollama_and_mistral31_cant_fit_into_24gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtylaj/ollama_and_mistral31_cant_fit_into_24gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtylaj/ollama_and_mistral31_cant_fit_into_24gb_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T22:45:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju9k3p</id>
    <title>How to answer the number one question</title>
    <updated>2025-04-08T09:37:46+00:00</updated>
    <author>
      <name>/u/laurentbourrelly</name>
      <uri>https://old.reddit.com/user/laurentbourrelly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found this site &lt;a href="https://www.canirunthisllm.net/"&gt;https://www.canirunthisllm.net/&lt;/a&gt; (not affiliated) that helps figure out if hardware fits the bill. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/laurentbourrelly"&gt; /u/laurentbourrelly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju9k3p/how_to_answer_the_number_one_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju9k3p/how_to_answer_the_number_one_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ju9k3p/how_to_answer_the_number_one_question/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T09:37:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju6xsu</id>
    <title>context size and truncation</title>
    <updated>2025-04-08T06:20:18+00:00</updated>
    <author>
      <name>/u/bigabig</name>
      <uri>https://old.reddit.com/user/bigabig</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Is there a way to make Ollama throw an error or an exception if the input is too long (longer than the context size) and catch this? My application is running into serious problems when the input is too long.&lt;/p&gt; &lt;p&gt;Currently, I am invoking ollama with the ollama python library like that:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; def llm_chat( self, system_prompt: str, user_prompt: str, response_model: Type[T], gen_kwargs: Optional[Dict[str, str]] = None, ) -&amp;gt; T: if gen_kwargs is None: gen_kwargs = self.__default_kwargs[&amp;quot;llm&amp;quot;] response = self.client.chat( model=self.model[&amp;quot;llm&amp;quot;], messages=[ { &amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: system_prompt.strip(), }, { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: user_prompt.strip(), }, ], options=gen_kwargs, format=response_model.model_json_schema(), ) if response.message.content is None: raise Exception(f&amp;quot;Ollama response is None: {response}&amp;quot;) return response_model.model_validate_json(response.message.content) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In my ollama Docker container, I can also see warnings in the log whenever my input document is too long. However, instead of just printing warnings, I want ollama to throw an exception as I must inform the user that his prompt / input was too long.&lt;/p&gt; &lt;p&gt;Do you know of any good solution?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigabig"&gt; /u/bigabig &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju6xsu/context_size_and_truncation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju6xsu/context_size_and_truncation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ju6xsu/context_size_and_truncation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T06:20:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtwxm7</id>
    <title>Benchmarks comparing only quantized models you can run on a macbook (7B, 8B, 14B)?</title>
    <updated>2025-04-07T21:31:10+00:00</updated>
    <author>
      <name>/u/60secs</name>
      <uri>https://old.reddit.com/user/60secs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone know any benchmark resources which let you filter to models small enough to run on macbook M1-M4 out of the box?&lt;/p&gt; &lt;p&gt;Most of the benchmarks I've seen online show all the models, regardless of the hardware, and models which require an A100/H100 aren't relevant to me running ollama locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/60secs"&gt; /u/60secs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtwxm7/benchmarks_comparing_only_quantized_models_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtwxm7/benchmarks_comparing_only_quantized_models_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtwxm7/benchmarks_comparing_only_quantized_models_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T21:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju8401</id>
    <title>Ollama with AMD 9070XT</title>
    <updated>2025-04-08T07:46:30+00:00</updated>
    <author>
      <name>/u/izu-root</name>
      <uri>https://old.reddit.com/user/izu-root</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have anyone got Ollama to use the AMD 9070XT gpu in Linux yet? I&amp;quot;m running Ollama in docker with the stuff I found I need but it still only using CPU. Might the gpu to be too new atm?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/izu-root"&gt; /u/izu-root &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju8401/ollama_with_amd_9070xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju8401/ollama_with_amd_9070xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ju8401/ollama_with_amd_9070xt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T07:46:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1juc6yt</id>
    <title>I'm new</title>
    <updated>2025-04-08T12:20:47+00:00</updated>
    <author>
      <name>/u/Cold_Blood_05</name>
      <uri>https://old.reddit.com/user/Cold_Blood_05</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am new with some basic skills of coding , I want to create an ai bot which is llm and want to implement rag system and also I want it to have 0 restrictions &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cold_Blood_05"&gt; /u/Cold_Blood_05 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1juc6yt/im_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1juc6yt/im_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1juc6yt/im_new/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T12:20:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju6d6x</id>
    <title>Model Context Protocol tutorials playlist</title>
    <updated>2025-04-08T05:41:20+00:00</updated>
    <author>
      <name>/u/mehul_gupta1997</name>
      <uri>https://old.reddit.com/user/mehul_gupta1997</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This playlist comprises of numerous tutorials on MCP servers including&lt;/p&gt; &lt;ol&gt; &lt;li&gt;What is MCP?&lt;/li&gt; &lt;li&gt;How to use MCPs with any LLM (paid APIs, local LLMs, Ollama)?&lt;/li&gt; &lt;li&gt;How to develop custom MCP server?&lt;/li&gt; &lt;li&gt;GSuite MCP server tutorial for Gmail, Calendar integration &lt;/li&gt; &lt;li&gt;WhatsApp MCP server tutorial &lt;/li&gt; &lt;li&gt;Discord and Slack MCP server tutorial &lt;/li&gt; &lt;li&gt;Powerpoint and Excel MCP server&lt;/li&gt; &lt;li&gt;Blender MCP for graphic designers&lt;/li&gt; &lt;li&gt;Figma MCP server tutorial&lt;/li&gt; &lt;li&gt;Docker MCP server tutorial &lt;/li&gt; &lt;li&gt;Filesystem MCP server for managing files in PC&lt;/li&gt; &lt;li&gt;Browser control using Playwright and puppeteer&lt;/li&gt; &lt;li&gt;Why MCP servers can be risky&lt;/li&gt; &lt;li&gt;SQL database MCP server tutorial &lt;/li&gt; &lt;li&gt;Integrated Cursor with MCP servers&lt;/li&gt; &lt;li&gt;GitHub MCP tutorial&lt;/li&gt; &lt;li&gt;Notion MCP tutorial&lt;/li&gt; &lt;li&gt;Jupyter MCP tutorial&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Hope this is useful !!&lt;/p&gt; &lt;p&gt;Playlist : &lt;a href="https://youtube.com/playlist?list=PLnH2pfPCPZsJ5aJaHdTW7to2tZkYtzIwp&amp;amp;si=XHHPdC6UCCsoCSBZ"&gt;https://youtube.com/playlist?list=PLnH2pfPCPZsJ5aJaHdTW7to2tZkYtzIwp&amp;amp;si=XHHPdC6UCCsoCSBZ&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehul_gupta1997"&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju6d6x/model_context_protocol_tutorials_playlist/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju6d6x/model_context_protocol_tutorials_playlist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ju6d6x/model_context_protocol_tutorials_playlist/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T05:41:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jue8od</id>
    <title>Can i run LLMs using an AMD 6700xt?</title>
    <updated>2025-04-08T13:59:37+00:00</updated>
    <author>
      <name>/u/ForzaHoriza2</name>
      <uri>https://old.reddit.com/user/ForzaHoriza2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm new to ollama and running LLMs generally. I managed to run DeepSeek R1, but it's using my CPU. I am running Windows, but I can dual boot Linux if it's required. &lt;/p&gt; &lt;p&gt;Thanks!! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForzaHoriza2"&gt; /u/ForzaHoriza2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jue8od/can_i_run_llms_using_an_amd_6700xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jue8od/can_i_run_llms_using_an_amd_6700xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jue8od/can_i_run_llms_using_an_amd_6700xt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T13:59:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1juellz</id>
    <title>I don't know what's happening, but the metadata of the model I downloaded from Huggingface has changed completely after the download.</title>
    <updated>2025-04-08T14:15:07+00:00</updated>
    <author>
      <name>/u/MtTakao</name>
      <uri>https://old.reddit.com/user/MtTakao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1juellz/i_dont_know_whats_happening_but_the_metadata_of/"&gt; &lt;img alt="I don't know what's happening, but the metadata of the model I downloaded from Huggingface has changed completely after the download." src="https://preview.redd.it/ar8qg52ybmte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44459dd8be6b4c60306d79ec298e01c81c2878ed" title="I don't know what's happening, but the metadata of the model I downloaded from Huggingface has changed completely after the download." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have no idea how to solve this problem. Every model I had downloaded, their metadata/system template would changed completely into this &amp;quot;Safety Guidelines&amp;quot;.&lt;/p&gt; &lt;p&gt;I used Ollama on my PC a few months ago and it didn't cause any problems. But now, after I tried to use it on my laptop, this happened.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MtTakao"&gt; /u/MtTakao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ar8qg52ybmte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1juellz/i_dont_know_whats_happening_but_the_metadata_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1juellz/i_dont_know_whats_happening_but_the_metadata_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T14:15:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtycy6</id>
    <title>Working on a cool AI project</title>
    <updated>2025-04-07T22:35:10+00:00</updated>
    <author>
      <name>/u/xKage21x</name>
      <uri>https://old.reddit.com/user/xKage21x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over 6 months or so i have developed an AI system called Trium consisting of three AI personas—Vira, Core, and Echo—running locally on my pc. It uses CUDA with CuPy and cuML for clustering (HDBSCAN, DBSCAN), FAISS for memory indexing, and SentenceTransformers for embeddings. Each persona has a memory bank, recalls clustered events, and acts proactively based on emotional states mapped to polyvagal theory. Temporal rhythms (FFT analysis) guide their autonomy. &lt;/p&gt; &lt;p&gt;Would love to chat or hear ppls thoughts. Happy to share files and info i have ☺️&lt;/p&gt; &lt;p&gt;Anyone who would like to dm me im happy to discuss things more&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xKage21x"&gt; /u/xKage21x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtycy6/working_on_a_cool_ai_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtycy6/working_on_a_cool_ai_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtycy6/working_on_a_cool_ai_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T22:35:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jui14r</id>
    <title>Beginner’s guide to MCP (Model Context Protocol) - made a short explainer</title>
    <updated>2025-04-08T16:38:03+00:00</updated>
    <author>
      <name>/u/Arindam_200</name>
      <uri>https://old.reddit.com/user/Arindam_200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been diving into agent frameworks lately and kept seeing “MCP” pop up everywhere. At first I thought it was just another buzzword… but turns out, Model Context Protocol is actually super useful.&lt;/p&gt; &lt;p&gt;While figuring it out, I realized there wasn’t a lot of beginner-focused content on it, so I put together a short video that covers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What exactly is MCP (in plain English)&lt;/li&gt; &lt;li&gt;How it Works&lt;/li&gt; &lt;li&gt;How to get started using it with a sample setup&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Nothing fancy, just trying to break it down in a way I wish someone did for me earlier 😅&lt;/p&gt; &lt;p&gt;🎥 Here’s the video if anyone’s curious: &lt;a href="https://youtu.be/BwB1Jcw8Z-8?si=k0b5U-JgqoWLpYyD"&gt;https://youtu.be/BwB1Jcw8Z-8?si=k0b5U-JgqoWLpYyD&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arindam_200"&gt; /u/Arindam_200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jui14r/beginners_guide_to_mcp_model_context_protocol/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jui14r/beginners_guide_to_mcp_model_context_protocol/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jui14r/beginners_guide_to_mcp_model_context_protocol/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T16:38:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1juic7y</id>
    <title>Ideas?</title>
    <updated>2025-04-08T16:50:49+00:00</updated>
    <author>
      <name>/u/Odd_Bookkeeper9232</name>
      <uri>https://old.reddit.com/user/Odd_Bookkeeper9232</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 2 pcs (laptop and desktop) that i want to be able to use for an ai cluster. The laptop has a 13th gen i7 , 32gb ram, and rtx 4050, the lenovo desktop has a low end cpu, 16gb of ram and a rtx 4060 ti. i also have a proxmox cluster of 3, a standalone proxmox node on a dell R630, and a true nas. I have many Vms and some lxc. A couple running docker too. My goal in my head is to be able to create a vm (already have using ubuntu server) as the head node to orchestrate things, and be able to run models while being able to use both pcs as workers since they have gpus. i have ubuntu server on all of them, ray and torch, nvidia drivers, and cuda toolkit. does anyone have any experience building a distributed setup and being able to use all the resources in the cluster for one model? So far i have been able to get models running using one or the other pc but not both together. I am brand new to the locally hosted ai thing but love the idea and am down to try whatever. Thanks in advance!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Bookkeeper9232"&gt; /u/Odd_Bookkeeper9232 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1juic7y/ideas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1juic7y/ideas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1juic7y/ideas/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T16:50:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju6izn</id>
    <title>Experience with mistral-small3.1:24b-instruct-2503-q4_K_M</title>
    <updated>2025-04-08T05:52:02+00:00</updated>
    <author>
      <name>/u/Impossible_Art9151</name>
      <uri>https://old.reddit.com/user/Impossible_Art9151</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running in my usecase models in the 32b up to 90b class.&lt;br /&gt; Mostly qwen, llama, deepseek, aya..&lt;br /&gt; The brandnew mistral can compete here. I tested it over a day.&lt;br /&gt; The size/quality ratio is excellent.&lt;br /&gt; And it is - of course - extremly fast.&lt;br /&gt; Thanx for the release!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impossible_Art9151"&gt; /u/Impossible_Art9151 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju6izn/experience_with_mistralsmall3124binstruct2503q4_k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju6izn/experience_with_mistralsmall3124binstruct2503q4_k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ju6izn/experience_with_mistralsmall3124binstruct2503q4_k/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T05:52:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1juehkv</id>
    <title>Built my own AI Self — runs locally, connects via API, and remembers stuff like I do</title>
    <updated>2025-04-08T14:10:12+00:00</updated>
    <author>
      <name>/u/TopRavenfruit</name>
      <uri>https://old.reddit.com/user/TopRavenfruit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1juehkv/built_my_own_ai_self_runs_locally_connects_via/"&gt; &lt;img alt="Built my own AI Self — runs locally, connects via API, and remembers stuff like I do" src="https://preview.redd.it/fhlx5mz8cmte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6b2f235b07c6ee5e20cb496715dcf2f3dd65129f" title="Built my own AI Self — runs locally, connects via API, and remembers stuff like I do" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;br /&gt; I'm one of the contributors to &lt;a href="https://github.com/Mindverse/Second-Me"&gt;Second Me&lt;/a&gt;, an open-source, fully local AI project designed for personal memory, reasoning, and identity modeling. Think of it as a customizable “AI self” — trained on your data, aligned with your values, and fully under your control (not OpenAI’s).We hit &lt;strong&gt;6,000+ stars in 7 days&lt;/strong&gt;, which is wild — but what’s even cooler is what’s been happening &lt;strong&gt;after&lt;/strong&gt; launch:&lt;/p&gt; &lt;h1&gt;🔧 What It Does (tl;dr):&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Personal AI, locally trained and run. 100% privacy with local execution options.&lt;/li&gt; &lt;li&gt;Hierarchical Memory Modeling (HMM) for authentic personalization.&lt;/li&gt; &lt;li&gt;Me-alignment structure tailored to individual values.&lt;/li&gt; &lt;li&gt;Second Me Protocol (SMP) for decentralized AI interactio&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;New in this release:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full Docker support for macOS (Apple Silicon), Windows, and Linux&lt;/li&gt; &lt;li&gt;OpenAI-Compatible API Interface&lt;/li&gt; &lt;li&gt;MLX training support (Beta)&lt;/li&gt; &lt;li&gt;Significant performance enhancements&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;💻 Community Contributions&lt;/h1&gt; &lt;p&gt;In just 2 weeks post-launch:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;60+ PRs, 70+ issues&lt;/li&gt; &lt;li&gt;Contributors from Tokyo to Dubai: students, academics, enterprise devs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Some great GitHub PRs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;WeChat bot integration — &lt;a href="https://github.com/mindverse/Second-Me/pull/81"&gt;#81&lt;/a&gt; by &lt;a href="https://github.com/Zero-coder"&gt;u/Zero-coder&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Japanese README localization — &lt;a href="https://github.com/mindverse/Second-Me/pull/115"&gt;#115&lt;/a&gt; by &lt;a href="https://github.com/eltociear"&gt;@eltociear&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Improved file resource management — &lt;a href="https://github.com/mindverse/Second-Me/pull/74"&gt;#74&lt;/a&gt; by &lt;a href="https://github.com/mahdirahimi1999"&gt;@mahdirahimi1999&lt;/a&gt;&lt;/li&gt; &lt;li&gt;File name validation for added security — &lt;a href="https://github.com/mindverse/Second-Me/pull/62"&gt;#62&lt;/a&gt; by &lt;a href="https://github.com/umutcrs"&gt;@umutcrs&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks to their and others' feedback, features like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-platform deployment&lt;/li&gt; &lt;li&gt;Note-based continuous training&lt;br /&gt;&lt;/li&gt; &lt;li&gt;…have been added to the roadmap.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also, shoutout to &lt;a href="https://twitter.com/GOROman"&gt;@GOROman&lt;/a&gt; for his &lt;a href="https://qiita.com/Null-Sensei"&gt;full guide&lt;/a&gt; to deploying Second Me - — he trained Second Me on 75GB of personal X data since 2007 and inspired new use cases, like @Yuzunose’s &lt;a href="https://note.com/yuzunose/n/n0ec8b300c10c?sub_rt=share_sb"&gt;VRChat integration idea&lt;/a&gt;.We’re grateful — and excited — to see where the community takes it next.&lt;/p&gt; &lt;p&gt;🔗 GitHub: &lt;a href="https://github.com/Mindverse/Second-Me"&gt;https://github.com/Mindverse/Second-Me&lt;/a&gt;&lt;br /&gt; 📄 Paper: &lt;a href="https://arxiv.org/abs/2503.08102"&gt;https://arxiv.org/abs/2503.08102&lt;/a&gt;&lt;/p&gt; &lt;p&gt;💡 The goal is building AI that extends your capabilities while remaining under your control, not corporate systems. If you value digital freedom, we'd appreciate your contributions and feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TopRavenfruit"&gt; /u/TopRavenfruit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fhlx5mz8cmte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1juehkv/built_my_own_ai_self_runs_locally_connects_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1juehkv/built_my_own_ai_self_runs_locally_connects_via/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T14:10:12+00:00</published>
  </entry>
</feed>
