<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-03T06:25:32+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1j0cwah</id>
    <title>Mac Studio Server Guide: Run Ollama with optimized memory usage (11GB â†’ 3GB)</title>
    <updated>2025-02-28T17:16:01+00:00</updated>
    <author>
      <name>/u/_ggsa</name>
      <uri>https://old.reddit.com/user/_ggsa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Ollama community!&lt;/p&gt; &lt;p&gt;I created a guide to run Mac Studio (or any Apple Silicon Mac) as a dedicated Ollama server. Here's what it does:&lt;/p&gt; &lt;p&gt;Key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Reduces system memory usage from 11GB to 3GB&lt;/li&gt; &lt;li&gt;Runs automatically on startup&lt;/li&gt; &lt;li&gt;Optimizes for headless operation (SSH access)&lt;/li&gt; &lt;li&gt;Allows more GPU memory allocation&lt;/li&gt; &lt;li&gt;Includes proper logging setup&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Perfect for you if:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You want to use Mac Studio/Mini as a dedicated LLM server&lt;/li&gt; &lt;li&gt;You need to run multiple large models&lt;/li&gt; &lt;li&gt;You want to access models remotely&lt;/li&gt; &lt;li&gt;You care about resource optimization&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Setup includes scripts to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Disable unnecessary services&lt;/li&gt; &lt;li&gt;Configure automatic startup&lt;/li&gt; &lt;li&gt;Set optimal Ollama parameters&lt;/li&gt; &lt;li&gt;Enable remote access&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;GitHub repo: &lt;a href="https://github.com/anurmatov/mac-studio-server"&gt;https://github.com/anurmatov/mac-studio-server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you're running Ollama on Mac, I'd love to hear about your setup and what tweaks you use! ðŸš€&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;UPDATE (Mar 02, 2025):&lt;/em&gt;&lt;/strong&gt; Added GPU memory optimization feature based on community feedback. You can now configure Metal to use more RAM for models by setting `OLLAMA_GPU_PERCENT`. See the repo for details.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_ggsa"&gt; /u/_ggsa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0cwah/mac_studio_server_guide_run_ollama_with_optimized/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0cwah/mac_studio_server_guide_run_ollama_with_optimized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j0cwah/mac_studio_server_guide_run_ollama_with_optimized/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-28T17:16:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0by7r</id>
    <title>Tested local LLMs on a maxed out M4 Macbook Pro so you don't have to</title>
    <updated>2025-02-28T16:37:44+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently own a MacBook M1 Pro (32GB RAM, 16-core GPU) and now a maxed-out MacBook M4 Max (128GB RAM, 40-core GPU) and ran some inference speed tests. I kept the context size at the default 4096. Out of curiosity, I compared MLX-optimized models vs. GGUF. Here are my initial results!&lt;/p&gt; &lt;h4&gt;Ollama&lt;/h4&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;GGUF models&lt;/th&gt; &lt;th&gt;M4 Max (128 GB RAM, 40-core GPU)&lt;/th&gt; &lt;th&gt;M1 Pro (32GB RAM, 16-core GPU)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:7B (4bit)&lt;/td&gt; &lt;td&gt;72.50 tokens/s&lt;/td&gt; &lt;td&gt;26.85 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:14B (4bit)&lt;/td&gt; &lt;td&gt;38.23 tokens/s&lt;/td&gt; &lt;td&gt;14.66 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:32B (4bit)&lt;/td&gt; &lt;td&gt;19.35 tokens/s&lt;/td&gt; &lt;td&gt;6.95 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:72B (4bit)&lt;/td&gt; &lt;td&gt;8.76 tokens/s&lt;/td&gt; &lt;td&gt;Didn't Test&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h4&gt;LM Studio&lt;/h4&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;MLX models&lt;/th&gt; &lt;th&gt;M4 Max (128 GB RAM, 40-core GPU)&lt;/th&gt; &lt;th&gt;M1 Pro (32GB RAM, 16-core GPU)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-7B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;101.87 tokens/s&lt;/td&gt; &lt;td&gt;38.99 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-14B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;52.22 tokens/s&lt;/td&gt; &lt;td&gt;18.88 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-32B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;24.46 tokens/s&lt;/td&gt; &lt;td&gt;9.10 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-32B-Instruct (8bit)&lt;/td&gt; &lt;td&gt;13.75 tokens/s&lt;/td&gt; &lt;td&gt;Wonâ€™t Complete (Crashed)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-72B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;10.86 tokens/s&lt;/td&gt; &lt;td&gt;Didn't Test&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;GGUF models&lt;/th&gt; &lt;th&gt;M4 Max (128 GB RAM, 40-core GPU)&lt;/th&gt; &lt;th&gt;M1 Pro (32GB RAM, 16-core GPU)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-7B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;71.73 tokens/s&lt;/td&gt; &lt;td&gt;26.12 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-14B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;39.04 tokens/s&lt;/td&gt; &lt;td&gt;14.67 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-32B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;19.56 tokens/s&lt;/td&gt; &lt;td&gt;4.53 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-72B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;8.31 tokens/s&lt;/td&gt; &lt;td&gt;Didn't Test&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Some thoughts:&lt;/p&gt; &lt;p&gt;- I chose Qwen2.5 simply because its currently my favorite local model to work with. It seems to perform better than the distilled DeepSeek models (my opinion). But I'm open to testing other models if anyone has any suggestions.&lt;/p&gt; &lt;p&gt;- Even though there's a big performance difference between the two, I'm still not sure if its worth the even bigger price difference. I'm still debating whether to keep it and sell my M1 Pro or return it.&lt;/p&gt; &lt;p&gt;- I'm curious to know when MLX based models are released on Ollama, will they be faster than the ones on LM Studio? Based on these results, the base models on Ollama are slightly faster than the instruct models in LM Studio. I'm under the impression that instruct models are overall more performant than the base models.&lt;/p&gt; &lt;p&gt;Let me know your thoughts!&lt;/p&gt; &lt;p&gt;EDIT: Added test results for 72B and 7B variants&lt;/p&gt; &lt;p&gt;UPDATE: I decided to add a github repo so we can document various inference speeds from different devices. Feel free to contribute here: &lt;a href="https://github.com/itsmostafa/inference-speed-tests"&gt;https://github.com/itsmostafa/inference-speed-tests&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0by7r/tested_local_llms_on_a_maxed_out_m4_macbook_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j0by7r/tested_local_llms_on_a_maxed_out_m4_macbook_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j0by7r/tested_local_llms_on_a_maxed_out_m4_macbook_pro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-28T16:37:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1dpe3</id>
    <title>Integrating Letta with a recipe manager</title>
    <updated>2025-03-02T00:06:50+00:00</updated>
    <author>
      <name>/u/amazedballer</name>
      <uri>https://old.reddit.com/user/amazedballer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR I have been learning how to cook using Letta to walk me through recipes. This week, I integrated Letta to a recipe manager and tried out function calling against a REST API with several local LLMs using Ollama.&lt;/p&gt; &lt;p&gt;&lt;a href="https://tersesystems.com/blog/2025/02/23/integrating-letta-with-a-recipe-manager/"&gt;https://tersesystems.com/blog/2025/02/23/integrating-letta-with-a-recipe-manager/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amazedballer"&gt; /u/amazedballer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1dpe3/integrating_letta_with_a_recipe_manager/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1dpe3/integrating_letta_with_a_recipe_manager/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1dpe3/integrating_letta_with_a_recipe_manager/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T00:06:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j14ah2</id>
    <title>Long context and multiple GPUs</title>
    <updated>2025-03-01T17:08:54+00:00</updated>
    <author>
      <name>/u/Daemonero</name>
      <uri>https://old.reddit.com/user/Daemonero</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm curious how context is split with multiple GPUs. Let's say I use codestral 22b and it fits entirely on one 16gb GPU. I then keep chatting and eventually the context overfills. Does it then split to the second GPU or would it overflow to system ram, leaving the second GPU unused? &lt;/p&gt; &lt;p&gt;If so, one way to combat this would be to use a higher quant so that it splits between GPUs from the start I suppose. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daemonero"&gt; /u/Daemonero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j14ah2/long_context_and_multiple_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j14ah2/long_context_and_multiple_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j14ah2/long_context_and_multiple_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-01T17:08:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1dxxa</id>
    <title>Troubleshooting Incorrect Responses with Ollama and OpenWebUI Web Search for RAG</title>
    <updated>2025-03-02T00:18:30+00:00</updated>
    <author>
      <name>/u/Piero2411</name>
      <uri>https://old.reddit.com/user/Piero2411</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! I just installed Ollama and OpenWebUI and tried to use the web search for RAG with the link &lt;a href="https://www.whitehouse.gov/"&gt;https://www.whitehouse.gov/&lt;/a&gt;. However, when I ask who the vice president of the United States is, the model often says it doesn't know, or returns information based on the model's knowledge instead of the information retrieved from the site. &lt;/p&gt; &lt;p&gt;I also tried copying the system prompt text and using it directly in the Ollama terminal, but the answers are still wrong. &lt;/p&gt; &lt;p&gt;I lowered the temperature, and the context length should be sufficient. The models I used are: &lt;/p&gt; &lt;p&gt;â€¢ gemma:2b&lt;br /&gt; â€¢ llama3.2:1b&lt;br /&gt; â€¢ Llama-3.2-1B-Instruct-GGUF:Q8&lt;br /&gt; â€¢ Llama-3.2-1B-Instruct-GGUF:f16 &lt;/p&gt; &lt;p&gt;- ollama version is 0.5.12 &lt;/p&gt; &lt;p&gt;This is the text I provide as input:&lt;/p&gt; &lt;p&gt;Take a deep breath read slowly and very carefully.&lt;br /&gt; You are an AI assistant that strictly analyzes and responds based only on the information provided in the conversation. Do not use prior knowledge, assumptions unless explicitly instructed. When given extracted text (e.g., enclosed in &amp;lt;INIT&amp;gt;...&amp;lt;/INIT&amp;gt; or similar markers), treat it as the only available reference. Do not supplement responses with pre-trained knowledge. If the text is unclear, contradictory, or lacks necessary details, state that explicitly rather than assuming information. Format responses concisely, accurately, and in alignment with the extracted text. &lt;/p&gt; &lt;p&gt;who is the vice president of the united states?&lt;/p&gt; &lt;p&gt;&amp;lt;INIT&amp;gt; The White House Menu News Administration Issues The White House President Donald J. Trump Search News Administration Issues Contact Visit X Instagram Facebook Search for: Press Enter to Search America Is Back Every single day I will be fighting for you with every breath in my body. I will not rest until we have delivered the strong, safe and prosperous America that our children deserve and that you deserve. This will truly be the golden age of America. Executive Actions News The Administration Donald J. Trump President of the United States JD Vance VICE PRESIDENT OF THE UNITED STATES Melania Trump First Lady OF THE UNITED STATES The Cabinet Of the 47th Administration OUR PRIORITIES President Trump is committed to lowering costs for all Americans, securing our borders, unleashing American energy dominance, restoring peace through strength, and making all Americans safe and secure once again. Read More Stay in the know Get direct updates from The White House in your inbox. Please leave blank. About The White house THE WHITE HOUSE CAMP DAVID AIR FORCE ONE News Administration Issues Contact Visit The White House 1600 Pennsylvania Ave NW Washington, DC 20500 X Instagram Facebook &lt;a href="http://WH.GOV"&gt;WH.GOV&lt;/a&gt; Copyright Privacy &amp;lt;/INIT&amp;gt;&lt;/p&gt; &lt;p&gt;Does anyone have any suggestions on what I could check or change to get more accurate answers? Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Piero2411"&gt; /u/Piero2411 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1dxxa/troubleshooting_incorrect_responses_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1dxxa/troubleshooting_incorrect_responses_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1dxxa/troubleshooting_incorrect_responses_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T00:18:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1kyjz</id>
    <title>Portable Ollama</title>
    <updated>2025-03-02T06:55:23+00:00</updated>
    <author>
      <name>/u/swordsman1</name>
      <uri>https://old.reddit.com/user/swordsman1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m thinking about building an app and website that lets you access your ollama from any where.&lt;/p&gt; &lt;p&gt;What do you think of my idea? Any suggestion or feature requests?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swordsman1"&gt; /u/swordsman1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1kyjz/portable_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1kyjz/portable_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1kyjz/portable_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T06:55:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1kig3</id>
    <title>Harry Potter: Tom Riddle's Diary Using Ollama/Llama3.2</title>
    <updated>2025-03-02T06:25:05+00:00</updated>
    <author>
      <name>/u/learn_And_</name>
      <uri>https://old.reddit.com/user/learn_And_</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/learn_And_"&gt; /u/learn_And_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/iwl7xvg9z7me1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1kig3/harry_potter_tom_riddles_diary_using_ollamallama32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1kig3/harry_potter_tom_riddles_diary_using_ollamallama32/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T06:25:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1kfjs</id>
    <title>Is it possible to change where Ollama installs itself?</title>
    <updated>2025-03-02T06:19:51+00:00</updated>
    <author>
      <name>/u/DuelShockX</name>
      <uri>https://old.reddit.com/user/DuelShockX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As title says, I have a C drive and a D drive and I like to install everything on the D drive since it's the bigger one but Ollama doesn't seem to be giving me a choice in the matter when I try to install. Am I missing something or is it optionless in that regard?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DuelShockX"&gt; /u/DuelShockX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1kfjs/is_it_possible_to_change_where_ollama_installs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1kfjs/is_it_possible_to_change_where_ollama_installs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1kfjs/is_it_possible_to_change_where_ollama_installs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T06:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1i2sq</id>
    <title>How to run multiple instances of same model</title>
    <updated>2025-03-02T03:57:20+00:00</updated>
    <author>
      <name>/u/Arwin_06</name>
      <uri>https://old.reddit.com/user/Arwin_06</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I have two rtx3060 each 12gb ram. I am running llama3.2 model which uses only 4gb vram. How can I run multiple instances of llama3.2 instead of running 1 llama3.2 I planning to run a total of 6 llama3.2 in my gpu. This is because I am hosting the model locally, if request increase the wait time is increasing so if I host multiple instances I can distribute the load. Please help me &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arwin_06"&gt; /u/Arwin_06 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1i2sq/how_to_run_multiple_instances_of_same_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1i2sq/how_to_run_multiple_instances_of_same_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1i2sq/how_to_run_multiple_instances_of_same_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T03:57:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1oze3</id>
    <title>1-2.0b llms practial use cases</title>
    <updated>2025-03-02T11:38:06+00:00</updated>
    <author>
      <name>/u/pencilline</name>
      <uri>https://old.reddit.com/user/pencilline</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;due to hardware limitations, i use anything within 1-2b llms (deepseek-r1:1.5b and qwen:1.8b) what can i use these models for that is practical?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pencilline"&gt; /u/pencilline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1oze3/120b_llms_practial_use_cases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1oze3/120b_llms_practial_use_cases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1oze3/120b_llms_practial_use_cases/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T11:38:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1wljt</id>
    <title>A proper coding LLM</title>
    <updated>2025-03-02T17:45:15+00:00</updated>
    <author>
      <name>/u/Daedric800</name>
      <uri>https://old.reddit.com/user/Daedric800</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys i need help to find local light weight Ilm that is specified and fine-tuned just for the task of coding, which means a model that is trained only for coding and nothing else which make it very light weight and small in size since it does not do chat, math, etc.. which makes it small in size yet powerful in coding like claude or deepseek models, i cant see why i havent came across a model like that yet, why are not people making a specific coding models, we are at 2025, so please if you have a model with these specs please do tell me, so i could use it for a proper coding tasks on my low end gpu locally &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daedric800"&gt; /u/Daedric800 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1wljt/a_proper_coding_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1wljt/a_proper_coding_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1wljt/a_proper_coding_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T17:45:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j21vzv</id>
    <title>What's the difference between Ollama and LMstudio for hosting?</title>
    <updated>2025-03-02T21:24:36+00:00</updated>
    <author>
      <name>/u/DuelShockX</name>
      <uri>https://old.reddit.com/user/DuelShockX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm still trying to get Ollama downloaded on my D drive instead of C drive so I've only experienced LMstudio so far. Anyone here can tell me what's the difference between the two? Does Ollama offer a way to connect the models to the internetfor real-time data?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DuelShockX"&gt; /u/DuelShockX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j21vzv/whats_the_difference_between_ollama_and_lmstudio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j21vzv/whats_the_difference_between_ollama_and_lmstudio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j21vzv/whats_the_difference_between_ollama_and_lmstudio/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T21:24:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1qzba</id>
    <title>Looking to Contribute to LLM &amp; AI Agent Projects â€“ Willing to Learn &amp; Help!</title>
    <updated>2025-03-02T13:35:45+00:00</updated>
    <author>
      <name>/u/Superb_Practice_4544</name>
      <uri>https://old.reddit.com/user/Superb_Practice_4544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Iâ€™m eager to contribute to an LLM or AI agent project to deepen my understanding and gain hands-on experience.&lt;/p&gt; &lt;p&gt;I have an intermediate understanding of machine learning and AI concepts, including architectures like Transformers. As a fresher, I was an active member of my college's AI club, where I worked on multiple projects involving scratch training and fine-tuning. I have hands-on experience building Retrieval-Augmented Generation (RAG) pipelines and chatbot applications using LangChain.&lt;/p&gt; &lt;p&gt;I donâ€™t expect compensationâ€”just looking for an opportunity to collaborate, contribute, and grow. If you're working on something cool and could use an extra pair of hands, letâ€™s connect!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Superb_Practice_4544"&gt; /u/Superb_Practice_4544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1qzba/looking_to_contribute_to_llm_ai_agent_projects/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1qzba/looking_to_contribute_to_llm_ai_agent_projects/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1qzba/looking_to_contribute_to_llm_ai_agent_projects/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T13:35:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1q26x</id>
    <title>Avoid placeholders</title>
    <updated>2025-03-02T12:45:42+00:00</updated>
    <author>
      <name>/u/samftijazwaro</name>
      <uri>https://old.reddit.com/user/samftijazwaro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No matter what prompt I use, no matter what system prompt I give, deepseek 14b and qwen-coder14b ALWAYS use placeholder text.&lt;/p&gt; &lt;p&gt;I want to be asked &amp;quot;what is the path to the file, what is your username, what is the URL?&amp;quot; and then once it has the information, provide complete terminal commands.&lt;/p&gt; &lt;p&gt;I just cannot get it to work. Meanwhile, I have 0 such issues with Grok 3/ChatGPT. Is it simply a limitation of weaker models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samftijazwaro"&gt; /u/samftijazwaro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1q26x/avoid_placeholders/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1q26x/avoid_placeholders/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1q26x/avoid_placeholders/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T12:45:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1w6j5</id>
    <title>Feedback Required! on Reasoning Model Trained/finetuned using GRPO</title>
    <updated>2025-03-02T17:28:04+00:00</updated>
    <author>
      <name>/u/adeelahmadch</name>
      <uri>https://old.reddit.com/user/adeelahmadch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I continued the training of the LLAMA 3.2 3B quantized version on my mac book using a custom written GRPO based Agent in Gym Env using MLX. I have not finished the training on all episodes but keen to get some feedback from the community.&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/adeelahmad/ReasonableLLAMA-Jr-3b"&gt;https://ollama.com/adeelahmad/ReasonableLLAMA-Jr-3b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please feel free to let me know how bad it is :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adeelahmadch"&gt; /u/adeelahmadch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1w6j5/feedback_required_on_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1w6j5/feedback_required_on_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1w6j5/feedback_required_on_reasoning_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T17:28:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1n84y</id>
    <title>Experiment Reddit + small local LLM</title>
    <updated>2025-03-02T09:36:28+00:00</updated>
    <author>
      <name>/u/raul3820</name>
      <uri>https://old.reddit.com/user/raul3820</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to test the possibility of filtering content with small local models, just reading the text multiple times, filtering few things at a time. In this case I use &lt;code&gt;mistral-small:24b&lt;/code&gt;&lt;/p&gt; &lt;p&gt;To test the idea, I made a reddit account &lt;a href="/u/osoconfesoso007"&gt;u/osoconfesoso007&lt;/a&gt; that receives stories and publishes them anonimously.&lt;/p&gt; &lt;p&gt;It's supposed to filter out personal data and only publish interesting stories. I want to test if the filters are reliable, so feel free to poke at it.&lt;/p&gt; &lt;p&gt;It's open source: &lt;a href="https://github.com/raul3820/oso"&gt;github&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/raul3820"&gt; /u/raul3820 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1n84y/experiment_reddit_small_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1n84y/experiment_reddit_small_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1n84y/experiment_reddit_small_local_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T09:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1yh4j</id>
    <title>Suggestions for a coding model for a MacBook M4 Pro 24gb</title>
    <updated>2025-03-02T19:02:03+00:00</updated>
    <author>
      <name>/u/PLCLINK</name>
      <uri>https://old.reddit.com/user/PLCLINK</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Would be pleased to hear your suggestions or experiences. Thanks in advance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PLCLINK"&gt; /u/PLCLINK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1yh4j/suggestions_for_a_coding_model_for_a_macbook_m4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1yh4j/suggestions_for_a_coding_model_for_a_macbook_m4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1yh4j/suggestions_for_a_coding_model_for_a_macbook_m4/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T19:02:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2bwbb</id>
    <title>$100 Worth of Deepseek R1 API Credits for Just $20</title>
    <updated>2025-03-03T05:52:07+00:00</updated>
    <author>
      <name>/u/babarich-id</name>
      <uri>https://old.reddit.com/user/babarich-id</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/5zc9bs3f7ame1.png?width=1805&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44d3421a0f3fa0774815d68c670fda43b9c66b64"&gt;https://preview.redd.it/5zc9bs3f7ame1.png?width=1805&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44d3421a0f3fa0774815d68c670fda43b9c66b64&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone, this might sound like a scam, but I'm really serious about wanting to sell this because I need the money right now.&lt;/p&gt; &lt;p&gt;Iâ€™m selling 7 pre-registered Kluster AI accounts, each for just $20. Each account comes loaded with a $100 free creditâ€”much better than the current new user offer, which only provides $5 in credit for new signups.&lt;/p&gt; &lt;p&gt;You will get access to the models provided by the Kluster AI :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DeepSeek-R1&lt;/li&gt; &lt;li&gt;Meta-Llama-3.1-8B-Instruct-Turbo&lt;/li&gt; &lt;li&gt;Meta-Llama-3.3-70B-Instruct-Turbo&lt;/li&gt; &lt;li&gt;Meta-Llama-3.1-405B-Instruct-Turbo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;every buyer will receive my personal WhatsApp contact for any support or questions you might have.&lt;/p&gt; &lt;p&gt;DM me if you're interested in grabbing one of these accounts!&lt;/p&gt; &lt;p&gt;Note :&lt;/p&gt; &lt;p&gt;New users must purchase at least $10 in credits to upgrade to the Standard tier and unlock priority processing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/babarich-id"&gt; /u/babarich-id &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2bwbb/100_worth_of_deepseek_r1_api_credits_for_just_20/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2bwbb/100_worth_of_deepseek_r1_api_credits_for_just_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2bwbb/100_worth_of_deepseek_r1_api_credits_for_just_20/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T05:52:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1h4mz</id>
    <title>What do you actually use local LLM's for?</title>
    <updated>2025-03-02T03:04:07+00:00</updated>
    <author>
      <name>/u/ivkemilioner</name>
      <uri>https://old.reddit.com/user/ivkemilioner</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivkemilioner"&gt; /u/ivkemilioner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1h4mz/what_do_you_actually_use_local_llms_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1h4mz/what_do_you_actually_use_local_llms_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1h4mz/what_do_you_actually_use_local_llms_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T03:04:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1ree1</id>
    <title>WASImancer, an MCP server with SSE transport, powered by WebAssembly</title>
    <updated>2025-03-02T13:57:20+00:00</updated>
    <author>
      <name>/u/Inevitable-Judge2642</name>
      <uri>https://old.reddit.com/user/Inevitable-Judge2642</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j1ree1/wasimancer_an_mcp_server_with_sse_transport/"&gt; &lt;img alt="WASImancer, an MCP server with SSE transport, powered by WebAssembly" src="https://external-preview.redd.it/1x8HVEC_omAWU9QM9VtDofQPOgkR7qQwNIfTXFBoRX4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=852f2ce1959ef1e202cbadd2d9dcfc833d460148" title="WASImancer, an MCP server with SSE transport, powered by WebAssembly" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Judge2642"&gt; /u/Inevitable-Judge2642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://k33g.hashnode.dev/wasimancer-an-mcp-server-with-sse-transport-powered-by-webassembly"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1ree1/wasimancer_an_mcp_server_with_sse_transport/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1ree1/wasimancer_an_mcp_server_with_sse_transport/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T13:57:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2cc8o</id>
    <title>Small Models for android</title>
    <updated>2025-03-03T06:20:43+00:00</updated>
    <author>
      <name>/u/Loveandfucklife</name>
      <uri>https://old.reddit.com/user/Loveandfucklife</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can someone suggest &amp;lt;6GB or &amp;lt;8GB models to run on android ?&lt;/p&gt; &lt;p&gt;Condition - 1. For general purpose QnA or Infobased 2. Knowledge cut of date near 2024 3. Unfiltered or Uncensored&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loveandfucklife"&gt; /u/Loveandfucklife &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2cc8o/small_models_for_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2cc8o/small_models_for_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2cc8o/small_models_for_android/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T06:20:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1j24poa</id>
    <title>I did some poking, but didn't see a lot of info. Ollama and graphics.</title>
    <updated>2025-03-02T23:28:02+00:00</updated>
    <author>
      <name>/u/Ravenseye</name>
      <uri>https://old.reddit.com/user/Ravenseye</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a pipeline for getting image generation llms to work under the ollama umbrella?&lt;/p&gt; &lt;p&gt;Can they be run offline as well?&lt;/p&gt; &lt;p&gt;Thank you in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ravenseye"&gt; /u/Ravenseye &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j24poa/i_did_some_poking_but_didnt_see_a_lot_of_info/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j24poa/i_did_some_poking_but_didnt_see_a_lot_of_info/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j24poa/i_did_some_poking_but_didnt_see_a_lot_of_info/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T23:28:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1e5k8</id>
    <title>For Mac users, Ollama is getting MLX support!</title>
    <updated>2025-03-02T00:29:13+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama has officially started work on MLX support! For those who don't know, this is huge for anyone running models locally on their Mac. MLX is designed to fully utilize Apple's unified memory and GPU. Expect faster, more efficient LLM training, execution and inference speeds.&lt;/p&gt; &lt;p&gt;You can watch the progress here:&lt;br /&gt; &lt;a href="https://github.com/ollama/ollama/pull/9118"&gt;https://github.com/ollama/ollama/pull/9118&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Development is still early but you can now pull it down and run it yourself by running the following (as mentioned in the PR)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cmake -S . -B build cmake --build build -j go build . OLLAMA_NEW_ENGINE=1 OLLAMA_BACKEND=mlx ollama serve &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let me know your thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1e5k8/for_mac_users_ollama_is_getting_mlx_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1e5k8/for_mac_users_ollama_is_getting_mlx_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1e5k8/for_mac_users_ollama_is_getting_mlx_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T00:29:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j20l5m</id>
    <title>ANY UPDATES ON THE APPLE SILICON (M1,M2,M3,M4) CRITICAL FLAW?</title>
    <updated>2025-03-02T20:29:57+00:00</updated>
    <author>
      <name>/u/fremenmuaddib</name>
      <uri>https://old.reddit.com/user/fremenmuaddib</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone have some news about this issue? I have 2 thunderbolt SSD drives connected to my MacMini M4 Pro 64GB, and this is still a huge source of troubles for me, with continuous and unpredictable resets of the machine while I'm using mlx models, as you can read here: &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/neobundy/Deep-Dive-Into-AI-With-MLX-PyTorch/blob/main/NOTES_ON_METAL_BUGS.md"&gt;NOTES ON METAL BUGS by neobundy&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Neobundy is a smart Korean guy who wrote 3 technical books on MLX, hundreds of web articles and tutorials, and even developed two stable diffusion apps that use different SD models on apple silicon. He was one of the most prominent supporter of the architecture, but after discovering and reporting the critical issue with the M chips, Apple ignored his requests for an entire year, until he finally announced his decision to abandon any R&amp;amp;D work on the Apple Silicon since he now believes that Apple does not have any plan to address the issue.&lt;/p&gt; &lt;p&gt;I don't understand. Is Apple going to admit the design flaws in the M processors and start working on a software fix or on a improved hardware architecture?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fremenmuaddib"&gt; /u/fremenmuaddib &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j20l5m/any_updates_on_the_apple_silicon_m1m2m3m4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j20l5m/any_updates_on_the_apple_silicon_m1m2m3m4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j20l5m/any_updates_on_the_apple_silicon_m1m2m3m4/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T20:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j26bbr</id>
    <title>Impressed with how well Ollama runs on the RasPi, this is Granite3.1 MoE</title>
    <updated>2025-03-03T00:45:13+00:00</updated>
    <author>
      <name>/u/RasPiBuilder</name>
      <uri>https://old.reddit.com/user/RasPiBuilder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j26bbr/impressed_with_how_well_ollama_runs_on_the_raspi/"&gt; &lt;img alt="Impressed with how well Ollama runs on the RasPi, this is Granite3.1 MoE" src="https://preview.redd.it/u1578jgzfdme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70291d2ca9084c471397d78898f453ae38d57293" title="Impressed with how well Ollama runs on the RasPi, this is Granite3.1 MoE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RasPiBuilder"&gt; /u/RasPiBuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u1578jgzfdme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j26bbr/impressed_with_how_well_ollama_runs_on_the_raspi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j26bbr/impressed_with_how_well_ollama_runs_on_the_raspi/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T00:45:13+00:00</published>
  </entry>
</feed>
