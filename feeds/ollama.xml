<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-27T23:05:45+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1iavfc6</id>
    <title>8x AMD Instinct Mi60 Server + vLLM + unsloth/DeepSeek-R1-Distill-Qwen-32B FP16</title>
    <updated>2025-01-27T00:46:36+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vpaab64pkffe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iavfc6/8x_amd_instinct_mi60_server_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iavfc6/8x_amd_instinct_mi60_server_vllm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T00:46:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib939q</id>
    <title>Idea on how to deal with dates on RAG</title>
    <updated>2025-01-27T13:31:22+00:00</updated>
    <author>
      <name>/u/AmrElsayedEGY</name>
      <uri>https://old.reddit.com/user/AmrElsayedEGY</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a RAG pipeline that fetch the data from vector DB (Chroma) and then pass it to LLM model (Ollama), My vector db has info for sales and customers, &lt;/p&gt; &lt;p&gt;So if user asked something like &amp;quot;What is the latest order?&amp;quot;, The search inside Vector DB probably will get wrong answers cause it will not consider date, it only will check for similarity between query and the DB, So it will get random documents, (k is something around 10)&lt;/p&gt; &lt;p&gt;So my question is, What approaches should i use to accomplish this? I need the context being passed to LLM to contain the correct data, I have both customer and sales info in the same vector DB&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AmrElsayedEGY"&gt; /u/AmrElsayedEGY &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib939q/idea_on_how_to_deal_with_dates_on_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib939q/idea_on_how_to_deal_with_dates_on_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib939q/idea_on_how_to_deal_with_dates_on_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T13:31:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib98w0</id>
    <title>DeepSeek with VSCode very chatty</title>
    <updated>2025-01-27T13:37:56+00:00</updated>
    <author>
      <name>/u/liquidnitrogen</name>
      <uri>https://old.reddit.com/user/liquidnitrogen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying DeepSeek 8b with VSCode connected via `Connect` plugin - if i ask DeepSeek in VSCode to review the code it goes into a long rant talking to itself before spitting out the answer, however if i copy paste the code to &lt;a href="http://chat.deepseek.com"&gt;chat.deepseek.com&lt;/a&gt; or ask locally on web-openui and ask to do the same, it right away give the concise answer. Can someone please explain why there is a difference and how I can make it work like the web interface?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liquidnitrogen"&gt; /u/liquidnitrogen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib98w0/deepseek_with_vscode_very_chatty/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib98w0/deepseek_with_vscode_very_chatty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib98w0/deepseek_with_vscode_very_chatty/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T13:37:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib9m4s</id>
    <title>Ollama not installing</title>
    <updated>2025-01-27T13:55:25+00:00</updated>
    <author>
      <name>/u/Kind_Hedgehog5496</name>
      <uri>https://old.reddit.com/user/Kind_Hedgehog5496</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can someone please help. I am very new to much of this and trying to understand what went wrong. I am trying to install Ollama on my mac and went through all the steps but I keep getting this response. when I enter into terminal &lt;/p&gt; &lt;p&gt;&amp;quot;ollama run llama3.2&amp;quot;&lt;/p&gt; &lt;p&gt;it responses with &lt;/p&gt; &lt;p&gt;&amp;quot;zsh: command not found: ollama&amp;quot;&lt;/p&gt; &lt;p&gt;can someone help me fix this issue. &lt;/p&gt; &lt;p&gt;i downloaded the ollama from the website and followed the steps for the install but still get this. &lt;/p&gt; &lt;p&gt;thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kind_Hedgehog5496"&gt; /u/Kind_Hedgehog5496 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib9m4s/ollama_not_installing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib9m4s/ollama_not_installing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib9m4s/ollama_not_installing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T13:55:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibaogt</id>
    <title>homebrew ollama client problems with local network access</title>
    <updated>2025-01-27T14:44:49+00:00</updated>
    <author>
      <name>/u/sandipb</name>
      <uri>https://old.reddit.com/user/sandipb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running ollama on a remote host in my local network. On my laptop, I installed ollama via homebrew, and set OLLAMA_HOST correctly.&lt;/p&gt; &lt;p&gt;I can reach the host via curl, and I can use the ollama commandline via sudo to access the remote host. However, it doesn't work with my user privileges.&lt;/p&gt; &lt;p&gt;I am suspecting that this is due to the Local Network access permissions that Sequoia 15.2 has. Since the commandline is not prompting a dialog to ask for the permission, I cannot whitelist it.&lt;/p&gt; &lt;p&gt;Doesn anyone have a workaround I can use? I am considering reverting to the ollama desktop app to fix this, but I prefer homebrew to keep such apps updated.&lt;/p&gt; &lt;p&gt;```shell-session $ ollama list Error: Head &amp;quot;&lt;a href="http://192.168.1.20:11434/%22:"&gt;http://192.168.1.20:11434/&amp;quot;:&lt;/a&gt; dial tcp 192.168.1.20:11434: connect: no route to host&lt;/p&gt; &lt;p&gt;$ curl -s $OLLAMA_HOST;echo Ollama is running&lt;/p&gt; &lt;p&gt;$ sudo -E ollama list Password: NAME ID SIZE MODIFIED&lt;/p&gt; &lt;p&gt;hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF:Q8_0 69f0e15b99a1 8.5 GB 6 days ago&lt;/p&gt; &lt;p&gt;llama3.2:latest a80c4f17acd5 2.0 GB 5 weeks ago ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandipb"&gt; /u/sandipb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibaogt/homebrew_ollama_client_problems_with_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibaogt/homebrew_ollama_client_problems_with_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibaogt/homebrew_ollama_client_problems_with_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T14:44:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibbpzo</id>
    <title>Difference in Llama 3.2 1B instruct and Llama 3.1 8B</title>
    <updated>2025-01-27T15:28:11+00:00</updated>
    <author>
      <name>/u/lonesomhelme</name>
      <uri>https://old.reddit.com/user/lonesomhelme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y'all, noob LLM tweaker here. I wanted to know the difference when working with these models. &lt;/p&gt; &lt;p&gt;For context, I deployed both these models on a cloud provider using vLLM inference. From what I noticed it was not easy to get proper responses from 3.2 1b model and the response were short. When I switched to 3.1 8b I could get better responses.&lt;/p&gt; &lt;p&gt;I want to understand what could've been the issue - is it my setup or the model?&lt;/p&gt; &lt;p&gt;More Context: I tried Llama 3.2 1B from Ollama and everything worked great. But when I switched to pulling the model from HuggingFace it messed up everything. Essentially, it's the same model, so it should work the same. What could be wrong here??&lt;/p&gt; &lt;p&gt;Appreciate any help in understanding this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lonesomhelme"&gt; /u/lonesomhelme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibbpzo/difference_in_llama_32_1b_instruct_and_llama_31_8b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibbpzo/difference_in_llama_32_1b_instruct_and_llama_31_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibbpzo/difference_in_llama_32_1b_instruct_and_llama_31_8b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T15:28:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibdwc2</id>
    <title>Deepseek R1 on Control Plane. Run Deepseek in your own private environment</title>
    <updated>2025-01-27T16:58:11+00:00</updated>
    <author>
      <name>/u/Miserable_Ad_5328</name>
      <uri>https://old.reddit.com/user/Miserable_Ad_5328</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ibdwc2/deepseek_r1_on_control_plane_run_deepseek_in_your/"&gt; &lt;img alt="Deepseek R1 on Control Plane. Run Deepseek in your own private environment" src="https://external-preview.redd.it/QDzC67FFjDHawiDLy5QG7RgetcxH0ztsc446fD0rc7U.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d49629f7bccf71e6082f0f906a1e408c683df6c1" title="Deepseek R1 on Control Plane. Run Deepseek in your own private environment" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Miserable_Ad_5328"&gt; /u/Miserable_Ad_5328 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=2wHXsXLhTII&amp;amp;si=MNLmktCu4ixFcDt0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibdwc2/deepseek_r1_on_control_plane_run_deepseek_in_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibdwc2/deepseek_r1_on_control_plane_run_deepseek_in_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T16:58:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibdxdt</id>
    <title>Questions on deepseek</title>
    <updated>2025-01-27T16:59:23+00:00</updated>
    <author>
      <name>/u/CampaignWeird5453</name>
      <uri>https://old.reddit.com/user/CampaignWeird5453</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm kinda new to this community and I understand a 8b model generates a lot of shit and stuff. I am attracted to it but I want some actual benchmarking going on, like prompts or something, I had no experience doing stuff like this so anybody who had done a benchmarking on the model could please share ur opinions and experiences with the model and probably explain stuff. If I had any misunderstanding of stuff plz let me know&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CampaignWeird5453"&gt; /u/CampaignWeird5453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibdxdt/questions_on_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibdxdt/questions_on_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibdxdt/questions_on_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T16:59:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib8ysc</id>
    <title>Guidance for Creating RAG with Ollama and LangChain for Java Code Analysis</title>
    <updated>2025-01-27T13:26:04+00:00</updated>
    <author>
      <name>/u/Terrible-Parfait-868</name>
      <uri>https://old.reddit.com/user/Terrible-Parfait-868</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Team,&lt;/p&gt; &lt;p&gt;I need assistance in creating a Retrieval-Augmented Generation (RAG) setup using Ollama and LangChain to analyze Java code.&lt;/p&gt; &lt;p&gt;Expectations:&lt;/p&gt; &lt;p&gt;The system should identify code connections using import statements.&lt;/p&gt; &lt;p&gt;It should track values or functions from imports.&lt;/p&gt; &lt;p&gt;It should answer questions like:&lt;/p&gt; &lt;p&gt;What does a specific function do?&lt;/p&gt; &lt;p&gt;What input is expected for a given function?&lt;/p&gt; &lt;p&gt;Challenge: The project comprises over 200 Java files and exceeds 200k total tokens, making efficient handling critical.&lt;/p&gt; &lt;p&gt;Looking forward to your suggestions or guidance on how to approach this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terrible-Parfait-868"&gt; /u/Terrible-Parfait-868 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib8ysc/guidance_for_creating_rag_with_ollama_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib8ysc/guidance_for_creating_rag_with_ollama_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib8ysc/guidance_for_creating_rag_with_ollama_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T13:26:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib0ddn</id>
    <title>ollama server</title>
    <updated>2025-01-27T05:01:35+00:00</updated>
    <author>
      <name>/u/Choice_Complaint9171</name>
      <uri>https://old.reddit.com/user/Choice_Complaint9171</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i want to setup ollama to be able to receive request from me in other words i want to be able to communicate with my local ollama on my desktop pc remotely through my iphone can someone confirm if this is possible i would assume you can make this possible with webui something like assigning authentication through ip address for remote use &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Choice_Complaint9171"&gt; /u/Choice_Complaint9171 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib0ddn/ollama_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib0ddn/ollama_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib0ddn/ollama_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T05:01:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibfuud</id>
    <title>Deepseek in RTX 2060 for Laptop?</title>
    <updated>2025-01-27T18:15:29+00:00</updated>
    <author>
      <name>/u/LordCapeNSword</name>
      <uri>https://old.reddit.com/user/LordCapeNSword</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does someone tried to run deepseek in a 2060?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LordCapeNSword"&gt; /u/LordCapeNSword &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibfuud/deepseek_in_rtx_2060_for_laptop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibfuud/deepseek_in_rtx_2060_for_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibfuud/deepseek_in_rtx_2060_for_laptop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T18:15:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibhc3o</id>
    <title>Help? Stuck on something</title>
    <updated>2025-01-27T19:13:57+00:00</updated>
    <author>
      <name>/u/Powerful_Advice2619</name>
      <uri>https://old.reddit.com/user/Powerful_Advice2619</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. I just installed ollama, and I've been trying to run it again.&lt;/p&gt; &lt;p&gt;However, when using the command for ollama (ollama serve), it doesnt work and instead gets stuck on this. &lt;/p&gt; &lt;p&gt;What now??&lt;/p&gt; &lt;p&gt;in case the image is too blurry, it says this&lt;/p&gt; &lt;p&gt;2025/01/27 14:09:15 routes.go:1187: INFO server config env=&amp;quot;map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:&lt;a href="http://127.0.0.1:11434"&gt;http://127.0.0.1:11434&lt;/a&gt; OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\jchoi\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* &lt;a href="http://127.0.0.1"&gt;http://127.0.0.1&lt;/a&gt; &lt;a href="https://127.0.0.1"&gt;https://127.0.0.1&lt;/a&gt; &lt;a href="http://127.0.0.1:%5C*"&gt;http://127.0.0.1:\*&lt;/a&gt; &lt;a href="https://127.0.0.1:%5C*"&gt;https://127.0.0.1:\*&lt;/a&gt; &lt;a href="http://0.0.0.0"&gt;http://0.0.0.0&lt;/a&gt; &lt;a href="https://0.0.0.0"&gt;https://0.0.0.0&lt;/a&gt; &lt;a href="http://0.0.0.0:%5C*"&gt;http://0.0.0.0:\*&lt;/a&gt; &lt;a href="https://0.0.0.0:%5C*"&gt;https://0.0.0.0:\*&lt;/a&gt; app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-01-27T14:09:15.622-05:00 level=INFO source=images.go:432 msg=&amp;quot;total blobs: 9&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-01-27T14:09:15.623-05:00 level=INFO source=images.go:439 msg=&amp;quot;total unused blobs removed: 0&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-01-27T14:09:15.624-05:00 level=INFO source=routes.go:1238 msg=&amp;quot;Listening on 127.0.0.1:11434 (version 0.5.7)&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-01-27T14:09:15.625-05:00 level=INFO source=routes.go:1267 msg=&amp;quot;Dynamic LLM libraries&amp;quot; runners=&amp;quot;[cpu_avx cpu_avx2 cuda_v11_avx cuda_v12_avx rocm_avx cpu]&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-01-27T14:09:15.625-05:00 level=INFO source=gpu.go:226 msg=&amp;quot;looking for compatible GPUs&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-01-27T14:09:15.625-05:00 level=INFO source=gpu_windows.go:167 msg=packages count=1&lt;/p&gt; &lt;p&gt;time=2025-01-27T14:09:15.625-05:00 level=INFO source=gpu_windows.go:214 msg=&amp;quot;&amp;quot; package=0 cores=6 efficiency=0 threads=12&lt;/p&gt; &lt;p&gt;time=2025-01-27T14:09:15.931-05:00 level=INFO source=gpu.go:334 msg=&amp;quot;detected OS VRAM overhead&amp;quot; id=GPU-506f81d8-ec0c-5810-dce7-dcb370d3d194 library=cuda compute=8.6 driver=12.7 name=&amp;quot;NVIDIA GeForce RTX 3060 Ti&amp;quot; overhead=&amp;quot;554.3 MiB&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-01-27T14:09:15.932-05:00 level=INFO source=types.go:131 msg=&amp;quot;inference compute&amp;quot; id=GPU-506f81d8-ec0c-5810-dce7-dcb370d3d194 library=cuda variant=v12 compute=8.6 driver=12.7 name=&amp;quot;NVIDIA GeForce RTX 3060 Ti&amp;quot; total=&amp;quot;8.0 GiB&amp;quot; available=&amp;quot;7.0 GiB&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful_Advice2619"&gt; /u/Powerful_Advice2619 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibhc3o/help_stuck_on_something/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibhc3o/help_stuck_on_something/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibhc3o/help_stuck_on_something/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T19:13:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibhxvm</id>
    <title>Guide to Installing and Locally Running Ollama LLM models in Comfy (ELI5 Level)</title>
    <updated>2025-01-27T19:38:29+00:00</updated>
    <author>
      <name>/u/GreyScope</name>
      <uri>https://old.reddit.com/user/GreyScope</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ibhxvm/guide_to_installing_and_locally_running_ollama/"&gt; &lt;img alt="Guide to Installing and Locally Running Ollama LLM models in Comfy (ELI5 Level)" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="Guide to Installing and Locally Running Ollama LLM models in Comfy (ELI5 Level)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Firstly, due diligence still applies to checking out any security issues to all models and software.&lt;/p&gt; &lt;p&gt;Secondly, this is written in the (kiss) style of all my guides : simple steps, it is not a technical paper, nor is it written for people who have greater technical knowledge, they are written as best I can in ELI5 style .&lt;/p&gt; &lt;h1&gt;Pre-requisites&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;A (quick) internet connection (if downloading large models&lt;/li&gt; &lt;li&gt;A working install of ComfyUI&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Usage Case:&lt;/h1&gt; &lt;p&gt;1. For Stable Diffusion purposes it’s for writing or expanding prompts, ie to make descriptions or make them more detailed / refined for a purpose (eg like a video) if used on an existing bare bones prompt .&lt;/p&gt; &lt;p&gt;2. If the LLM is used to describe an existing image, it can help replicate the style or substance of it.&lt;/p&gt; &lt;p&gt;3. Use it as a Chat bot or as a LLM front end for whatever you want (eg coding)&lt;/p&gt; &lt;h1&gt;Basic Steps to carry out (Part 1):&lt;/h1&gt; &lt;p&gt;1. Download Ollama itself&lt;/p&gt; &lt;p&gt;2. Turn off Ollama’s Autostart entry (&amp;amp; start when needed) or leave it&lt;/p&gt; &lt;p&gt;3. Set the Ollama ENV in Windows – to set where it saves the models that it uses&lt;/p&gt; &lt;p&gt;4. Run Ollama in a CMD window and download a model&lt;/p&gt; &lt;p&gt;5. Run Ollama with the model you just downloaded&lt;/p&gt; &lt;h1&gt;Basic Steps to carry out (Part 2):&lt;/h1&gt; &lt;p&gt;1. For use within Comfy download/install nodes for its use&lt;/p&gt; &lt;p&gt;2. Setup nodes within your own flow or download a flow with them in&lt;/p&gt; &lt;p&gt;3. Setup the settings within the LLM node to use Ollama &lt;/p&gt; &lt;h1&gt;Basic Explanation of Terms&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;An &lt;strong&gt;LLM (Large Language Model)&lt;/strong&gt; is an AI system trained on vast amounts of text data to understand, generate, and manipulate human-like language for various tasks - like coding, describing images, writing text etc&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt; is a tool that allows users to easily download, run, and manage open-source large language models (LLMs) locally on their own hardware.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;---------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Part 1 - Ollama&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;DownLoad Ollama&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Download Ollama and install from - &lt;a href="https://ollama.com/"&gt;https://ollama.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You will see nothing after it installs but if you go down the bottom right of the taskbar in the Notification section, you'll see it is active (running a background server).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/omup1pj0nkfe1.png?width=453&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c5484788c42d83c84cb53f38a5143afc62d9155"&gt;https://preview.redd.it/omup1pj0nkfe1.png?width=453&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c5484788c42d83c84cb53f38a5143afc62d9155&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Ollama and Autostart&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Be aware that Ollama autoruns on your PC’s startup, if you don’t want that then turn off its Autostart on (Ctrl -Alt-Del to start the Task Manager and then click on Startup Apps and lastly just right clock on its entry on the list and select ‘Disabled’)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Set Ollama's ENV settings&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Now setup where you want Ollama to save its models (eg your hard drive with your SD installs on or the one with the most space)&lt;/p&gt; &lt;p&gt;Type ‘ENV’ into search box on your taskbar&lt;/p&gt; &lt;p&gt;Select &amp;quot;Edit the System Environment Variables&amp;quot; (part of Windows Control Panel) , see below&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y03ubm8dnkfe1.png?width=393&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=179daa73ce2009eb1ea2343dd225a07c7a772455"&gt;https://preview.redd.it/y03ubm8dnkfe1.png?width=393&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=179daa73ce2009eb1ea2343dd225a07c7a772455&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On the newly opened ‘System Properties‘ window, click on &amp;quot;Environment Variables&amp;quot; (bottom right on pic below)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f2tv16minkfe1.png?width=283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=919004d372bd169d80d4a0378be070962a79f78d"&gt;https://preview.redd.it/f2tv16minkfe1.png?width=283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=919004d372bd169d80d4a0378be070962a79f78d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;System Variables are split into two sections of User and System - click on New under &amp;quot;User Variables&amp;quot; (top section on pic below)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zhbudfjnnkfe1.png?width=414&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef70cb35da716b05a365d8eab65ca7406eef7fdb"&gt;https://preview.redd.it/zhbudfjnnkfe1.png?width=414&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef70cb35da716b05a365d8eab65ca7406eef7fdb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On the new input window, input the following -&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Variable name:&lt;/strong&gt; OLLAMA_MODELS&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Variable value:&lt;/strong&gt; (input directory path you wish to save models to. Make your folder structure as you wish ( eg H:\Ollama\Models).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4xitvv0ynkfe1.png?width=535&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e07784593316bd1147b7b02547eece860a49fb0c"&gt;https://preview.redd.it/4xitvv0ynkfe1.png?width=535&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e07784593316bd1147b7b02547eece860a49fb0c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NB&lt;/strong&gt; Don’t change the ‘Variable name’ or Ollama will not save to the directory you wish.&lt;/p&gt; &lt;p&gt;Click OK on each screen until the Environment Variables windows and then the System Properties windows close down (the variables are not saved until they're all closed)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open a CMD window and type 'Ollama' it will return its commands that you can use (see pic below)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nazdqgraokfe1.png?width=353&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=badd182f68014c2e38d69cfa06e8eeecbec96763"&gt;https://preview.redd.it/nazdqgraokfe1.png?width=353&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=badd182f68014c2e38d69cfa06e8eeecbec96763&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here’s a list of popular Large Language Models (LLMs) available on &lt;strong&gt;Ollama&lt;/strong&gt;, categorized by their simplified use cases. These models can be downloaded and run locally using Ollama or any others that are available (due diligence required) :&lt;/p&gt; &lt;h1&gt;A. Chat Models&lt;/h1&gt; &lt;p&gt;These models are optimized for conversational AI and interactive chat applications.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Llama 2 (7B, 13B, 70B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: General-purpose chat, conversational AI, and answering questions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run llama2&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mistral (7B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Lightweight and efficient chat model for conversational tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run mistral&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;B. Text Generation Models&lt;/h1&gt; &lt;p&gt;These models excel at generating coherent and creative text for various purposes.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OpenLLaMA (7B, 13B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Open-source alternative for text generation and summarization.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run openllama&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;C. Coding Models&lt;/h1&gt; &lt;p&gt;These models are specialized for code generation, debugging, and programming assistance.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CodeLlama (7B, 13B, 34B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Code generation, debugging, and programming assistance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run codellama&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;C. Image Description Models&lt;/h1&gt; &lt;p&gt;These models are designed to generate text descriptions of images (multimodal capabilities).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LLaVA (7B, 13B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Image captioning, visual question answering, and multimodal tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run llava&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;D. Multimodal Models&lt;/h1&gt; &lt;p&gt;These models combine text and image understanding for advanced tasks.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fuyu (8B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Multimodal tasks, including image understanding and text generation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run fuyu&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;E. Specialized Models&lt;/h1&gt; &lt;p&gt;These models are fine-tuned for specific tasks or domains.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;WizardCoder (15B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Specialized in coding tasks and programming assistance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run wizardcoder&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Alpaca (7B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Instruction-following tasks and fine-tuned conversational AI.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run alpaca&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Model Strengths&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;As you can see above, an LLM is focused to a particular strength, it's for the best to expect a Coding biased LLM to provide a good description of an image.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Size&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Go into the Ollama website and pick a variant (noted by the number and followed by a B in brackets after each model) to fit into your graphics cards VRAM.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Downloading a model - When you have decided which model you want, say the Gemma 2 model in its smallest 2b variant at 1.6G (pic below). The arrow shows the command to put into the CMD window to download and run it (it autodownloads and then runs)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gvb8dq6rukfe1.png?width=504&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16ad8f0c116f45cbdff67179599af06ffedefa6a"&gt;https://preview.redd.it/gvb8dq6rukfe1.png?width=504&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16ad8f0c116f45cbdff67179599af06ffedefa6a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9cutop61wkfe1.png?width=429&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b406060c153a068c78537275b70193f71cf6976"&gt;Models downloads and then runs - I asked it what an LLM is. Typing 'ollama list' tells you the models you have.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;-------------------------------------------------------.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Part 2 - Comfy&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I prefer a working workflow to have everything in a state where you can work on and adjust it to your needs / interests.&lt;/p&gt; &lt;p&gt;This is a great example from a user here &lt;a href="/u/EnragedAntelope"&gt;u/EnragedAntelope&lt;/a&gt; posted on Civitai - its for a workflow that uses LLMs in picture description for Cosmos I2V.&lt;/p&gt; &lt;p&gt;&lt;a href="https://civitai.com/models/1145020/cosmos-automated-image-to-video-i2v-enragedantelope"&gt;Cosmos AUTOMATED Image to Video (I2V) - EnragedAntelope - v1.2 | Other Workflows | Civitai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The initial LLM (Florence2) auto-downloads and installs itself , it then carries out the initial Image description (bottom right text box)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jh8rvq4oxkfe1.png?width=1117&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35f30f6ea6bd6a318c6ec75414400f30006175aa"&gt;https://preview.redd.it/jh8rvq4oxkfe1.png?width=1117&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35f30f6ea6bd6a318c6ec75414400f30006175aa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The text in the initial description is then passed to the second LLM module (within the Plush nodes) , this is initially set to use bigger internet based LLMs.&lt;/p&gt; &lt;p&gt;From everything carried out above, this can be changed to use your local Ollama install. Ensure the server is running (Llama in the notification area) - note the settings in the Advanced Prompt Enhancer node in the pic below.&lt;/p&gt; &lt;p&gt;That node is from the &lt;a href="https://github.com/glibsonoran/Plush-for-ComfyUI"&gt;https://github.com/glibsonoran/Plush-for-ComfyUI&lt;/a&gt; , let manager sort it all out for you.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qzl0bt0fykfe1.png?width=1190&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b2a9a26ba3e39ee647171b937735d969812ca37"&gt;Advanced Prompt Generator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You select the Ollama model from your downloads with a simple click on the box (see pic below) .&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5zva3rqnykfe1.png?width=468&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=93e02f32d2ba61cb00b0c470b8a06fc16e4dc4a5"&gt;Ollama Model selection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the context of this workflow, the added second LLM is given the purpose of rewriting the prompt for a video to increase the quality.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ibhxvm/video/44vn5inmzkfe1/player"&gt;https://reddit.com/link/1ibhxvm/video/44vn5inmzkfe1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ibhxvm/video/3conlucvzkfe1/player"&gt;https://reddit.com/link/1ibhxvm/video/3conlucvzkfe1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreyScope"&gt; /u/GreyScope &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibhxvm/guide_to_installing_and_locally_running_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibhxvm/guide_to_installing_and_locally_running_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibhxvm/guide_to_installing_and_locally_running_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T19:38:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibi8xj</id>
    <title>H100nvl</title>
    <updated>2025-01-27T19:50:55+00:00</updated>
    <author>
      <name>/u/Internal_Junket_25</name>
      <uri>https://old.reddit.com/user/Internal_Junket_25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which is The Max (Good usable) llm with this system:&lt;/p&gt; &lt;p&gt;128gb ram 2x H100nvl + nvlink&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Internal_Junket_25"&gt; /u/Internal_Junket_25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibi8xj/h100nvl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibi8xj/h100nvl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibi8xj/h100nvl/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T19:50:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1iawxvm</id>
    <title>Deepseek-R1:8b</title>
    <updated>2025-01-27T02:02:52+00:00</updated>
    <author>
      <name>/u/Choice_Complaint9171</name>
      <uri>https://old.reddit.com/user/Choice_Complaint9171</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone please help me from my ignorance is it me or does deepseek R1 :8b seem to resemble a young adult with high IQ and a lot of self doubt &lt;/p&gt; &lt;p&gt;To confirm I’m not ignorant myself try asking “is buffalo a part of New York?” In your setup. &lt;/p&gt; &lt;h1&gt;just your Average AI Contributor&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Choice_Complaint9171"&gt; /u/Choice_Complaint9171 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iawxvm/deepseekr18b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iawxvm/deepseekr18b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iawxvm/deepseekr18b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T02:02:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibkyv5</id>
    <title>How to login to Ollama on Mac</title>
    <updated>2025-01-27T21:41:53+00:00</updated>
    <author>
      <name>/u/tosinakanle</name>
      <uri>https://old.reddit.com/user/tosinakanle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an Ollama desktop app installation on my MacBook. I also have an Ollama account on the website. However, I have not been able to login to my Ollama account on my Ollama desktop app. How do I do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tosinakanle"&gt; /u/tosinakanle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibkyv5/how_to_login_to_ollama_on_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibkyv5/how_to_login_to_ollama_on_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibkyv5/how_to_login_to_ollama_on_mac/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T21:41:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibd000</id>
    <title>I asked deepseek-r1:14b my preferred Turing question using ollama via Open WebUI with the default settings., and it failed miserably</title>
    <updated>2025-01-27T16:21:37+00:00</updated>
    <author>
      <name>/u/12358</name>
      <uri>https://old.reddit.com/user/12358</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After seeing the Apple paper &lt;a href="https://youtu.be/tTG_a0KPJAc"&gt;https://youtu.be/tTG_a0KPJAc&lt;/a&gt; that confirmed to me that LLMs are knowledgeable, but stupid (as they don't understand word problems), I sought to try my preferred Turing test on deepseek-r1:14b. I'm not impressed; we still have a long way to go.&lt;/p&gt; &lt;p&gt;I pasted my prompt and some of its first response below. I had to delete the rest of the response, because it was more than 10,000 words long, which included 131 &amp;quot;wait&amp;quot; expressions. This was using Ollama locally via Open WebUI, with the default settings.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;How many standard pallets can fit into a 1 TEU container without stacking?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;deepseek-r1:14b&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;lt;think&amp;gt;&lt;/p&gt; &lt;p&gt;Okay, so I need to figure out how many standard pallets can fit into a 1 TEU container without stacking. Hmm, I've heard about TEUs before, but I'm not entirely sure what they are exactly. From what I remember, TEU stands for Twenty-Equivalent Unit, which is a measure used in shipping containers. One TEU is the size of a standard container that's 20 feet long. So, a 1 TEU container would be a 20-foot container.&lt;/p&gt; &lt;p&gt;Now, about pallets. A standard pallet size I think is usually either Euro pallet or another type. The most common ones I've heard are around 4 feet by 4 feet (or roughly 1.2 meters by 1.2 meters). But wait, maybe they're a bit bigger? Let me double-check that. Yeah, actually, the standard European pallet is 800mm x 1200mm, which is about 31.5 inches by 47.2 inches. So each pallet is pretty sizable.&lt;/p&gt; &lt;p&gt;The container is 20 feet long. I know that 1 foot is 12 inches, so 20 feet would be 240 inches. Wait, no, wait—that's not right because the container length is usually given in feet and converted differently. Let me think—actually, a 20-foot container is about 6.05 meters long. But how does that translate to fitting pallets inside?&lt;/p&gt; &lt;p&gt;So, if each pallet is around 1.2 meters wide and 2.4 meters long (wait, no, I might be mixing up the dimensions). Actually, standard Euro pallets are 800mm x 1200mm, which translates to about 31.5 inches by 47.2 inches. So lengthwise, each pallet is longer than it is wide.&lt;/p&gt; &lt;p&gt;If the container is 20 feet long (which is approximately 6.05 meters), and the pallets are 1.2 meters in one dimension, how many can fit along the length? Let's calculate that: 6.05 meters divided by 1.2 meters per pallet. That gives roughly 5.04, so you could fit about 5 pallets along the length if arranged correctly.&lt;/p&gt; &lt;p&gt;Wait, but hold on—containers have different dimensions than just their length. They also have width and height. I think a standard 20-foot container has an internal dimension of 20 feet (length) x 7.87 feet (width) x 6.96 feet (height). Converting that to meters, it's approximately 6.05m x 2.4m x 2.13m.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;... it goes on and on for more than 10,000 words....&lt;/p&gt; &lt;p&gt;It eventually stated:&lt;/p&gt; &lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;The maximum number of standard European pallets (EUR) that can fit into a standard 20-foot shipping container is 270.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/12358"&gt; /u/12358 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibd000/i_asked_deepseekr114b_my_preferred_turing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibd000/i_asked_deepseekr114b_my_preferred_turing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibd000/i_asked_deepseekr114b_my_preferred_turing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T16:21:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibm4zc</id>
    <title>Is there a shortcut and mini window for ollama like for chatgpt</title>
    <updated>2025-01-27T22:29:43+00:00</updated>
    <author>
      <name>/u/Simple-Ad-5067</name>
      <uri>https://old.reddit.com/user/Simple-Ad-5067</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I might be missing something obvious here, I really love the mini window that chatpgt uses when you use a shortcut (I'm on mac so option+space bar) is there a similar option for ollama? Or an alternative tool that uses ollama to create this window? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Simple-Ad-5067"&gt; /u/Simple-Ad-5067 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibm4zc/is_there_a_shortcut_and_mini_window_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibm4zc/is_there_a_shortcut_and_mini_window_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibm4zc/is_there_a_shortcut_and_mini_window_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T22:29:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibmlty</id>
    <title>Can someone suggest me a model to use for a personal project</title>
    <updated>2025-01-27T22:49:49+00:00</updated>
    <author>
      <name>/u/AVBGaming</name>
      <uri>https://old.reddit.com/user/AVBGaming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am interested in implementing AI/LLM into my note taking software (Obsidian). What i’m envisioning is a tool i can use which i can enter text prompts (like asking where a certain note is, or a general question that could be answered with something in my notes) and get a response. Any pointers on where i could start? i’m not sure how many parameters would suffice for this task, and im not sure if i need to look exclusively at models i can run locally vs on the cloud.&lt;/p&gt; &lt;p&gt;I have a 4070 8GB VRAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AVBGaming"&gt; /u/AVBGaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibmlty/can_someone_suggest_me_a_model_to_use_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibmlty/can_someone_suggest_me_a_model_to_use_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibmlty/can_someone_suggest_me_a_model_to_use_for_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T22:49:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibh60h</id>
    <title>Once at the Ollama &gt;&gt;&gt; prompt, how to include /tmp/context.txt</title>
    <updated>2025-01-27T19:07:10+00:00</updated>
    <author>
      <name>/u/immediate_a982</name>
      <uri>https://old.reddit.com/user/immediate_a982</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yes, I am trying to read a file once I’m already inside the ollama &amp;gt;&amp;gt;&amp;gt; environment. I know I can pass a file from the command line like (ollama run llama3.2 “summarize: $(cat /tmp/context.chat)”), but it will not remain in prompt mode for any follow up questions. I do not want to go into open-webUI to do this. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/immediate_a982"&gt; /u/immediate_a982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibh60h/once_at_the_ollama_prompt_how_to_include/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibh60h/once_at_the_ollama_prompt_how_to_include/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibh60h/once_at_the_ollama_prompt_how_to_include/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T19:07:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibmf8q</id>
    <title>[v1.0.8] Notate - New Reasoning Layer Added (Agent actions will be added) | UI/UX Improvements &amp; Deepseek Integration - Have suggestions? love to hear em!</title>
    <updated>2025-01-27T22:41:52+00:00</updated>
    <author>
      <name>/u/Hairetsu</name>
      <uri>https://old.reddit.com/user/Hairetsu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ibmf8q/v108_notate_new_reasoning_layer_added_agent/"&gt; &lt;img alt="[v1.0.8] Notate - New Reasoning Layer Added (Agent actions will be added) | UI/UX Improvements &amp;amp; Deepseek Integration - Have suggestions? love to hear em!" src="https://external-preview.redd.it/mLmb6PM3DcPIkGqR7oQ2EX6QH1e3GfDbIdNH1M_vKQw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=769808d36520d2e157bfc35efdd17fa7406c869f" title="[v1.0.8] Notate - New Reasoning Layer Added (Agent actions will be added) | UI/UX Improvements &amp;amp; Deepseek Integration - Have suggestions? love to hear em!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hairetsu"&gt; /u/Hairetsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/CNTRLAI/Notate"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibmf8q/v108_notate_new_reasoning_layer_added_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibmf8q/v108_notate_new_reasoning_layer_added_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T22:41:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibipbg</id>
    <title>Ollama not consistently utilizing GPU</title>
    <updated>2025-01-27T20:09:26+00:00</updated>
    <author>
      <name>/u/Abarn1024</name>
      <uri>https://old.reddit.com/user/Abarn1024</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a recurring issue of Ollama not consistently using my GPU. I have tried multiple configurations and still the GPU is not consistently utilized, despite having plenty of VRAM for the models I am running. My current setup is Ollama installed on the host, Open WebUI running in Docker container, NVIDIA cuda toolkit installed on host and the NVIDIA container toolkit installed. I access Open WebUI at &lt;a href="http://hostipaddress:8080/"&gt;http://hostipaddress:8080/&lt;/a&gt;. GPU is a RTX 4090. All models I have tested are small enough to fit in the VRAM of the GPU. After a fresh install, Ollama uses the GPU as expected. This lasts for a short period of time and then inevitably completely stops using the GPU and only uses the CPU. Sometimes restarting Ollama will temporarily fix this but then reverts back to only using the CPU. Sometimes restarting Ollama won't fix the problem and I have to uninstall and reinstall Ollama to get it temporarily working again. I am at a loss. Any suggestions on how to troubleshoot this further?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Abarn1024"&gt; /u/Abarn1024 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibipbg/ollama_not_consistently_utilizing_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibipbg/ollama_not_consistently_utilizing_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibipbg/ollama_not_consistently_utilizing_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T20:09:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibmmoq</id>
    <title>GFX 1030: AMD 6800XT with Koboldcpp finally letting use my amd gpu on Windows</title>
    <updated>2025-01-27T22:50:51+00:00</updated>
    <author>
      <name>/u/Wreid23</name>
      <uri>https://old.reddit.com/user/Wreid23</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Small Win or old news I dunno but here goes.&lt;/p&gt; &lt;p&gt;6800XT Unsupported I know I know... and can finally run Ollama on Windows with Koboldcpp it has a precompiled gfx1030 config that actually works (long time coming or maybe im late) as no matter what combo I tried with amd ollama it would not work initially.&lt;/p&gt; &lt;p&gt;Install Roc M:&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@afilipe362/using-ollama-on-older-amd-gpus-with-rocm-on-windows-a0b08de2cf02"&gt;https://medium.com/@afilipe362/using-ollama-on-older-amd-gpus-with-rocm-on-windows-a0b08de2cf02&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.amd.com/en/developer/resources/rocm-hub/hip-sdk.html"&gt;https://www.amd.com/en/developer/resources/rocm-hub/hip-sdk.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Optional Step: Move to a Secondary Drive&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="https://github.com/ollama/ollama/issues/2551"&gt;https://github.com/ollama/ollama/issues/2551&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;if you guys get confused, let me explain step by step:&lt;/p&gt; &lt;p&gt;1-first of all uninstall ollama (if you already installed) unless you want to keep it on your Main C Drive&lt;/p&gt; &lt;p&gt;2-then follow this:&lt;/p&gt; &lt;p&gt;Open Windows Settings.&lt;/p&gt; &lt;p&gt;Go to System.&lt;/p&gt; &lt;p&gt;Select About&lt;/p&gt; &lt;p&gt;Select Advanced System Settings.&lt;/p&gt; &lt;p&gt;Go to the Advanced tab.&lt;/p&gt; &lt;p&gt;Select Environment Variables....&lt;/p&gt; &lt;p&gt;Click on New...&lt;/p&gt; &lt;p&gt;And create a variable called OLLAMA_MODELS pointing to where you want to store the models(set path for store models)&lt;/p&gt; &lt;p&gt;3-after adjusting that then open the path that you downloaded OllamaSetup.exe (setup file)&lt;/p&gt; &lt;p&gt;4-then on file explorer write cmd to open cmd on that path&lt;/p&gt; &lt;p&gt;5-on cmd write--&amp;gt; OllamaSetup.exe /DIR=D:\MYDIRECTORY&lt;/p&gt; &lt;p&gt;6-after installation everything should works well and both ollama and models will save same drive you defined for&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Install Ollama for AMD&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://github.com/likelovewant/ollama-for-amd"&gt;https://github.com/likelovewant/ollama-for-amd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Do The Above but then: After Step 2 Just replace all the rocblas folders with &lt;a href="https://github.com/YellowRoseCx/koboldcpp-rocm/releases/tag/v1.82.1.yr0-ROCm"&gt;https://github.com/YellowRoseCx/koboldcpp-rocm/releases/tag/v1.82.1.yr0-ROCm&lt;/a&gt; files in&lt;/p&gt; &lt;p&gt;Download koboldcpp_rocm_files.zip&lt;/p&gt; &lt;p&gt;In The Following Locations:&lt;/p&gt; &lt;p&gt;Ollama Appdata (if its on c)&lt;/p&gt; &lt;p&gt;C:\Users\username\AppData\Local\Programs\Ollama\lib\ollama&lt;/p&gt; &lt;p&gt;If its on another disk navigate to ollama\lib\ollama &amp;gt; Overwrite &amp;amp; Replace the Rocblas Folder with rocblas from koboldcpp_rocm_files.zip&lt;/p&gt; &lt;p&gt;Go to Your Installed ROCM Folders:&lt;/p&gt; &lt;p&gt;C:\Program Files\AMD\ROCm\5.7\bin (depending on your gpu you might be using this one or 6.1)&lt;/p&gt; &lt;p&gt;C:\Program Files\AMD\ROCm\6.1\bin&lt;/p&gt; &lt;p&gt;If you cant find the above folders above download wiztree: &lt;a href="https://diskanalyzer.com/"&gt;https://diskanalyzer.com/&lt;/a&gt; and search for the file name or some of the path you will get there&lt;/p&gt; &lt;p&gt;Whichever one applies to you^&lt;/p&gt; &lt;p&gt;Download and Open KoboldCPP: &lt;a href="https://github.com/YellowRoseCx/koboldcpp-rocm/releases/download/v1.82.1.yr0-ROCm/koboldcpp_rocm.exe"&gt;https://github.com/YellowRoseCx/koboldcpp-rocm/releases/download/v1.82.1.yr0-ROCm/koboldcpp_rocm.exe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let it open attach a model from your Current Ollama folder: it will be the giant files in models folder check the dates and file sizes when you open it it will show you which model you are using in the command prompt.&lt;/p&gt; &lt;p&gt;Attach the Model You want to Use and test that works in the lil ui that launches.&lt;/p&gt; &lt;p&gt;put up 2 windows side to side task manager and the ui window&lt;/p&gt; &lt;p&gt;Open Task Manager and ask a semi complicated question like: what is the largest city per capita&lt;/p&gt; &lt;p&gt;It should spike GPU to 80%&lt;/p&gt; &lt;p&gt;You can test against running regular ollama as well&lt;/p&gt; &lt;p&gt;Finally.... Read the Kobold docs to see how much you can speed up the models based on your vram but the auto config in kobold does an ok job&lt;/p&gt; &lt;p&gt;You can also copy shortcut to your windows startup folder and launch it automatically.&lt;/p&gt; &lt;p&gt;For MSTY, OPENWEBUI ETC &amp;gt; Add a Local or Remote Open AI Compatible Model&lt;/p&gt; &lt;p&gt;Use Local Open AI Compatible Model: &lt;a href="http://localhost:5001/v1"&gt;http://localhost:5001/v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Change the Model to Kobold or whatever you name it and you will again see it using your GPU&lt;/p&gt; &lt;p&gt;API KEY: 0&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/open-webui/open-webui/discussions/2700"&gt;https://github.com/open-webui/open-webui/discussions/2700&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wreid23"&gt; /u/Wreid23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibmmoq/gfx_1030_amd_6800xt_with_koboldcpp_finally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibmmoq/gfx_1030_amd_6800xt_with_koboldcpp_finally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibmmoq/gfx_1030_amd_6800xt_with_koboldcpp_finally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T22:50:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibadzz</id>
    <title>LLM powered scraping with ollama</title>
    <updated>2025-01-27T14:31:29+00:00</updated>
    <author>
      <name>/u/Financial-Article-12</name>
      <uri>https://old.reddit.com/user/Financial-Article-12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been developing Parsera, a simple yet powerful Python library that leverages LLMs for web scraping. Many users requested Ollama support, and now that I’ve added it, I wanted to share it with the Ollama community.&lt;/p&gt; &lt;p&gt;If you are looking for a way to extract data from websites (especially when dealing with multiple websites), Parsera lets you do it with just a few lines of code, without the need to develop custom scrapers.&lt;/p&gt; &lt;p&gt;Give it a try and let me know what you think. Feedback is always welcome!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/raznem/parsera"&gt;github.com/raznem/parsera&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial-Article-12"&gt; /u/Financial-Article-12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibadzz/llm_powered_scraping_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibadzz/llm_powered_scraping_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibadzz/llm_powered_scraping_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T14:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iblmdi</id>
    <title>Is Deepseek R-1 overthinking everything?</title>
    <updated>2025-01-27T22:08:10+00:00</updated>
    <author>
      <name>/u/nPrevail</name>
      <uri>https://old.reddit.com/user/nPrevail</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I like the critical thinking that Deep Seek R-1 (reason) has, but I also think that it's been overthinking a lot, and giving me too many alternatives and options, and it keeps running ideas beyond reason (ha ha ha).&lt;/p&gt; &lt;p&gt;At worst, it'll give me an incorrect answer, and will talk to itself, saying things like &amp;quot;Oh, that not the right answer. Let me try it again&amp;quot; type of conversation with itself, but it will&lt;/p&gt; &lt;p&gt;Does anyone else experience?&lt;/p&gt; &lt;p&gt;I almost want to shift back to using llama3 as my main LLM. It was pretty straightforward, despite not giving me any critical responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nPrevail"&gt; /u/nPrevail &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iblmdi/is_deepseek_r1_overthinking_everything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iblmdi/is_deepseek_r1_overthinking_everything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iblmdi/is_deepseek_r1_overthinking_everything/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T22:08:10+00:00</published>
  </entry>
</feed>
