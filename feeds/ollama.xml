<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-06-13T16:52:29+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1l8nz26</id>
    <title>Thank you very much for the harmony of beautiful moments</title>
    <updated>2025-06-11T09:10:05+00:00</updated>
    <author>
      <name>/u/Electronic_Hat_7519</name>
      <uri>https://old.reddit.com/user/Electronic_Hat_7519</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l8nz26/thank_you_very_much_for_the_harmony_of_beautiful/"&gt; &lt;img alt="Thank you very much for the harmony of beautiful moments" src="https://external-preview.redd.it/fDZYiI4uAZFL0TXT29rrw-VG9UeFD332Z2q_PtKtCx4.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07c7662b37cffdf535f0e7d0c7ab4be755af2306" title="Thank you very much for the harmony of beautiful moments" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic_Hat_7519"&gt; /u/Electronic_Hat_7519 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://suno.com/s/tXFtMXgWrRV947iu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8nz26/thank_you_very_much_for_the_harmony_of_beautiful/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l8nz26/thank_you_very_much_for_the_harmony_of_beautiful/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T09:10:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8gbzq</id>
    <title>GPU ollama docker</title>
    <updated>2025-06-11T01:33:01+00:00</updated>
    <author>
      <name>/u/Informal_Catch_4688</name>
      <uri>https://old.reddit.com/user/Informal_Catch_4688</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm currently using ollama through WLS for my assistant on windows what I noticed is that it only uses 28% of my GPU but the reply from questions take long time 15secods how can I speed it up ? I was using llama.cpp before that and it was taking around 1-4 seconds to generate answer , I could not use llama.cpp because of hallucinations assistant would day the prompt my question and answer and hashtags etc &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Informal_Catch_4688"&gt; /u/Informal_Catch_4688 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8gbzq/gpu_ollama_docker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8gbzq/gpu_ollama_docker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l8gbzq/gpu_ollama_docker/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T01:33:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8vtdj</id>
    <title>Name the Llm that can do this</title>
    <updated>2025-06-11T15:35:55+00:00</updated>
    <author>
      <name>/u/matthewstevensdotorg</name>
      <uri>https://old.reddit.com/user/matthewstevensdotorg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Write a strictly rhyming poem where the words increase in syllable length according to ANY segment of the Fibonacci sequence&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matthewstevensdotorg"&gt; /u/matthewstevensdotorg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8vtdj/name_the_llm_that_can_do_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8vtdj/name_the_llm_that_can_do_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l8vtdj/name_the_llm_that_can_do_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T15:35:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1l85fh8</id>
    <title>Ollama Frontend/GUI</title>
    <updated>2025-06-10T17:53:30+00:00</updated>
    <author>
      <name>/u/Ok_Most9659</name>
      <uri>https://old.reddit.com/user/Ok_Most9659</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for an Ollama frontend/GUI. Preferably can be used offline, is private, works in Linux, and open source.&lt;br /&gt; Any recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Most9659"&gt; /u/Ok_Most9659 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l85fh8/ollama_frontendgui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l85fh8/ollama_frontendgui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l85fh8/ollama_frontendgui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-10T17:53:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8pyb4</id>
    <title>Are we supposed to always wrap content text with special tokens?</title>
    <updated>2025-06-11T11:14:43+00:00</updated>
    <author>
      <name>/u/SeaworthinessLeft160</name>
      <uri>https://old.reddit.com/user/SeaworthinessLeft160</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using Ollama and Pydantic for my structured output. It's pretty bare bones. However, in my system message content, the text lacks special tokens; the user role content is the same.&lt;/p&gt; &lt;p&gt;I've seen tutorials in video and article formats, and sometimes authors use special tokens, sometimes not.&lt;/p&gt; &lt;p&gt;Is it that the framework they use already creates the special tokens to wrap the text, specific to the model being used? If I use Ollama and Pydantic, am I supposed to manually add those special tokens?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SeaworthinessLeft160"&gt; /u/SeaworthinessLeft160 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8pyb4/are_we_supposed_to_always_wrap_content_text_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8pyb4/are_we_supposed_to_always_wrap_content_text_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l8pyb4/are_we_supposed_to_always_wrap_content_text_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T11:14:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1l92dox</id>
    <title>üéôÔ∏è Looking for Beta Testers ‚Äì Get 24 Hours of Free TTS Audio</title>
    <updated>2025-06-11T19:52:48+00:00</updated>
    <author>
      <name>/u/mythicinfinity</name>
      <uri>https://old.reddit.com/user/mythicinfinity</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm launching a new TTS (text-to-speech) service and I'm looking for a few early users to help test it out. If you're into AI voices, audio content, or just want to convert a lot of text to audio, this is a great chance to try it for free.&lt;/p&gt; &lt;p&gt;‚úÖ Beta testers get &lt;strong&gt;24 hours of audio generation&lt;/strong&gt; (no strings attached)&lt;br /&gt; ‚úÖ Supports multiple voices and formats&lt;br /&gt; ‚úÖ Ideal for podcasts, audiobooks, screenreaders, etc.&lt;/p&gt; &lt;p&gt;If you're interested, &lt;strong&gt;DM me&lt;/strong&gt; and I'll get you set up with access. Feedback is optional but appreciated!&lt;/p&gt; &lt;p&gt;Thanks! üôå&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mythicinfinity"&gt; /u/mythicinfinity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l92dox/looking_for_beta_testers_get_24_hours_of_free_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l92dox/looking_for_beta_testers_get_24_hours_of_free_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l92dox/looking_for_beta_testers_get_24_hours_of_free_tts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T19:52:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8tyzb</id>
    <title>giving deepseek R1 a new chance, model-choice, gguf import</title>
    <updated>2025-06-11T14:22:17+00:00</updated>
    <author>
      <name>/u/Impossible_Art9151</name>
      <uri>https://old.reddit.com/user/Impossible_Art9151</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;hopefully someone can give me a few hints.&lt;br /&gt; I once tested deepseek r1:70b when released. But I was fine with qwen2.5 and llama3.3 and deleted deepseek after a while.&lt;/p&gt; &lt;p&gt;I would like to give it a new chance. I own a Dual AMD workstation with 320GB RAM and a nvidia A6000 - 48GB VRAM&lt;br /&gt; Further I am using ubuntu, ollama (non-docker) and openwebui (non-docker).&lt;/p&gt; &lt;p&gt;I want to test highest quality, not on speed!&lt;br /&gt; Any quant recommendations for my hardware? unsloth, bartowski?&lt;br /&gt; Does for example run a hf.co/unsloth/DeepSeek-R1-0528-GGUF:Q3_K_S in my setup? Since I haven't used hf-gguf for a long time, can someone provide a step-by-step description, tutorial?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impossible_Art9151"&gt; /u/Impossible_Art9151 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8tyzb/giving_deepseek_r1_a_new_chance_modelchoice_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8tyzb/giving_deepseek_r1_a_new_chance_modelchoice_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l8tyzb/giving_deepseek_r1_a_new_chance_modelchoice_gguf/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T14:22:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1l912g0</id>
    <title>Local LLM and Agentic Use Cases?</title>
    <updated>2025-06-11T19:01:05+00:00</updated>
    <author>
      <name>/u/Ok_Most9659</name>
      <uri>https://old.reddit.com/user/Ok_Most9659</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do the smaller distilled and quantized models have capability for agentic use cases given their limits?&lt;br /&gt; If so, what are some of the use cases you are employing your local AI for and model are you using (including parameter/bits)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Most9659"&gt; /u/Ok_Most9659 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l912g0/local_llm_and_agentic_use_cases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l912g0/local_llm_and_agentic_use_cases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l912g0/local_llm_and_agentic_use_cases/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T19:01:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8r68f</id>
    <title>Ollama not releasing VRAM after running a model</title>
    <updated>2025-06-11T12:18:24+00:00</updated>
    <author>
      <name>/u/Siderox</name>
      <uri>https://old.reddit.com/user/Siderox</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been using Ollama (without Docker) to run a few models (mainly Gemma3:12b) for a couple months and noticed that it often does not release VRAM after it runs the model. For example, the VRAM usage will be at, say, 0.5GB before running the model, then 5.5GB while running, then remaining at 5.5GB. If you run the model again the usage will drop back down to 0.5GB for a second then back up to 5.5GB, suggesting it only clears the memory right before reloading the model. Seems to work that way regardless of whether I‚Äôm using the model on vanilla settings in powershell or on customised settings in OpenWebUI. Culling Ollama will bring GPU usage back to baseline, though, so it‚Äôs not a fatal issue, just a bit odd. Anyone else had this issue? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Siderox"&gt; /u/Siderox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8r68f/ollama_not_releasing_vram_after_running_a_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8r68f/ollama_not_releasing_vram_after_running_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l8r68f/ollama_not_releasing_vram_after_running_a_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T12:18:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1l90c7r</id>
    <title>i made a commit message generator that can be used offline and for free</title>
    <updated>2025-06-11T18:32:30+00:00</updated>
    <author>
      <name>/u/mehmetflix_</name>
      <uri>https://old.reddit.com/user/mehmetflix_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i made a commit message generator by finetuning qwen2.5 coder 7b instruct, it is quantized to 8bits so it has a 8.1gb size. if anyone wants to try it here is the link &lt;a href="https://pypi.org/project/ezcmt/"&gt;https://pypi.org/project/ezcmt/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;if you try it out tell me if theres anything that can be added or a bug that can be fixed&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehmetflix_"&gt; /u/mehmetflix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l90c7r/i_made_a_commit_message_generator_that_can_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l90c7r/i_made_a_commit_message_generator_that_can_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l90c7r/i_made_a_commit_message_generator_that_can_be/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T18:32:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1l95vfw</id>
    <title>Keeping Ollama chats persistent (Docker, Web UI)</title>
    <updated>2025-06-11T22:14:52+00:00</updated>
    <author>
      <name>/u/redpandafire</name>
      <uri>https://old.reddit.com/user/redpandafire</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New. Able to install and launch a container of Ollama running gemma3. It works, great. Shut down the computer. Everything is gone. Starting an image creates a brand new container. Unable to launch previous containers, it gets stuck on downloading 30/30 files. I believe the command is:&lt;/p&gt; &lt;p&gt;Docker ps -a Docker start (container id) [options]&lt;/p&gt; &lt;p&gt;Everytime I do this, Docker runs in command interface a bunch of lines and gets stuck downloading files 30/30.&lt;/p&gt; &lt;p&gt;TL;DR I just want to stop and start a specific container, that I believe, contains all my work and chats.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redpandafire"&gt; /u/redpandafire &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l95vfw/keeping_ollama_chats_persistent_docker_web_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l95vfw/keeping_ollama_chats_persistent_docker_web_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l95vfw/keeping_ollama_chats_persistent_docker_web_ui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T22:14:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9d3h3</id>
    <title>chat with mysql using ollama</title>
    <updated>2025-06-12T04:08:00+00:00</updated>
    <author>
      <name>/u/Specialist_Figure_31</name>
      <uri>https://old.reddit.com/user/Specialist_Figure_31</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there any open source github that can be used to chat with my mysql &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist_Figure_31"&gt; /u/Specialist_Figure_31 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9d3h3/chat_with_mysql_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9d3h3/chat_with_mysql_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l9d3h3/chat_with_mysql_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-12T04:08:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1l906f4</id>
    <title>Why use docker with ollama and Open WebuI?</title>
    <updated>2025-06-11T18:26:13+00:00</updated>
    <author>
      <name>/u/Ok_Most9659</name>
      <uri>https://old.reddit.com/user/Ok_Most9659</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have seen people recommend using Docker with Ollama and Open WebUI. I am not a programmer and new to local LLM, but my understanding is that its to ensure both programs run well on your system as it avoids potential local environment issues your system may have that could impede running Ollama or Open Webui. I have installed Ollama directly from their website without Docker and it runs without issue on my system. I have yet to download Open Webui and debating on downloading Docker first. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Is ensuring the program will run on any system the sole reason to run Ollama and Open WebUI through Docker container?&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Are there any benefits to running a program in a container for security or privacy?&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Any benefits to GPU efficiency for running a program in a container?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Most9659"&gt; /u/Ok_Most9659 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l906f4/why_use_docker_with_ollama_and_open_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l906f4/why_use_docker_with_ollama_and_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l906f4/why_use_docker_with_ollama_and_open_webui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T18:26:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9bxf2</id>
    <title>What is the best model to help with writing?</title>
    <updated>2025-06-12T03:04:49+00:00</updated>
    <author>
      <name>/u/VajraXL</name>
      <uri>https://old.reddit.com/user/VajraXL</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What model would you recommend as a writing assistant for a writer who is not a native English speaker and needs help with grammar and style corrections, and perhaps suggestions for alternative phrasing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VajraXL"&gt; /u/VajraXL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9bxf2/what_is_the_best_model_to_help_with_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9bxf2/what_is_the_best_model_to_help_with_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l9bxf2/what_is_the_best_model_to_help_with_writing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-12T03:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8otdn</id>
    <title>Finally ChatGPT did it!!</title>
    <updated>2025-06-11T10:06:06+00:00</updated>
    <author>
      <name>/u/theMonarch776</name>
      <uri>https://old.reddit.com/user/theMonarch776</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l8otdn/finally_chatgpt_did_it/"&gt; &lt;img alt="Finally ChatGPT did it!!" src="https://preview.redd.it/tc3mi0b5v96f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40cf4649c9497de9ae34f27c933a9d28b7018f53" title="Finally ChatGPT did it!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;finally it told there are 3 'r's in Strawberry&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theMonarch776"&gt; /u/theMonarch776 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tc3mi0b5v96f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l8otdn/finally_chatgpt_did_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l8otdn/finally_chatgpt_did_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-11T10:06:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9gvu5</id>
    <title>Run Ollama in your documents with Writeopia. Windows app now available!</title>
    <updated>2025-06-12T08:05:54+00:00</updated>
    <author>
      <name>/u/lehen01</name>
      <uri>https://old.reddit.com/user/lehen01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l9gvu5/run_ollama_in_your_documents_with_writeopia/"&gt; &lt;img alt="Run Ollama in your documents with Writeopia. Windows app now available!" src="https://external-preview.redd.it/dTdpM2pnNGhlZzZmMVhZcvYgeFNt2CRk4Y47xoCdZZ4Xui688StqpxFWpMYy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e9e4f38f67b08aa746c08a2a69827378720736a" title="Run Ollama in your documents with Writeopia. Windows app now available!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello hello. &lt;/p&gt; &lt;p&gt;Sometime ago, I shared my project Writeopia in &lt;a href="https://www.reddit.com/r/ollama/comments/1jdb4zm/i_created_a_text_editor_that_integrates_with/"&gt;this post&lt;/a&gt; and it had a super nice reception. Many users asked about the Windows app, because at that time, only macOS and Linux were available. &lt;/p&gt; &lt;p&gt;We are happy to announce that the Windows app is finally available. You can download it from the &lt;a href="https://apps.microsoft.com/detail/9NW5WL8NRM4H?hl=en-us&amp;amp;gl=NL&amp;amp;ocid=pdpshare"&gt;Windows Store&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;If you like the project, don't forget to star us on Github: &lt;a href="https://github.com/Writeopia/Writeopia"&gt;https://github.com/Writeopia/Writeopia&lt;/a&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lehen01"&gt; /u/lehen01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1i0njg4heg6f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9gvu5/run_ollama_in_your_documents_with_writeopia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l9gvu5/run_ollama_in_your_documents_with_writeopia/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-12T08:05:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1la6i0u</id>
    <title>[First Release!] Serene Pub - 0.1.0 Alpha - Linux/MacOS/Windows - Silly Tavern alternative</title>
    <updated>2025-06-13T03:57:34+00:00</updated>
    <author>
      <name>/u/doolijb</name>
      <uri>https://old.reddit.com/user/doolijb</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doolijb"&gt; /u/doolijb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1la6h3y"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1la6i0u/first_release_serene_pub_010_alpha/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1la6i0u/first_release_serene_pub_010_alpha/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-13T03:57:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9wvnr</id>
    <title>What's the best model for RAG with docs?</title>
    <updated>2025-06-12T20:23:29+00:00</updated>
    <author>
      <name>/u/Green-Ad-3964</name>
      <uri>https://old.reddit.com/user/Green-Ad-3964</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for the best model to use with llama.cpp or ollama on a RAG project. &lt;/p&gt; &lt;p&gt;I need it to never (ehm) allucinate and to be able to answer simple, plain questions about the docs both in a [yes/no] way and in a descriptive way, i.e. explaining something from the doc. &lt;/p&gt; &lt;p&gt;I have a 5090 so 32GB local memory. What's the best I could use? With or without reasoning? Is the more parameter the better for this task?&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Green-Ad-3964"&gt; /u/Green-Ad-3964 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9wvnr/whats_the_best_model_for_rag_with_docs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9wvnr/whats_the_best_model_for_rag_with_docs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l9wvnr/whats_the_best_model_for_rag_with_docs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-12T20:23:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1labojz</id>
    <title>Are there any small models (7B or smaller) that are good with German copywriting?</title>
    <updated>2025-06-13T09:34:42+00:00</updated>
    <author>
      <name>/u/n0nikk</name>
      <uri>https://old.reddit.com/user/n0nikk</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/n0nikk"&gt; /u/n0nikk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1labojz/are_there_any_small_models_7b_or_smaller_that_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1labojz/are_there_any_small_models_7b_or_smaller_that_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1labojz/are_there_any_small_models_7b_or_smaller_that_are/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-13T09:34:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1labu0o</id>
    <title>Building a pc for local llm (help needed)</title>
    <updated>2025-06-13T09:45:25+00:00</updated>
    <author>
      <name>/u/anirudhisonline</name>
      <uri>https://old.reddit.com/user/anirudhisonline</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anirudhisonline"&gt; /u/anirudhisonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1labraz/building_a_pc_for_local_llm_help_needed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1labu0o/building_a_pc_for_local_llm_help_needed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1labu0o/building_a_pc_for_local_llm_help_needed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-13T09:45:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9py3c</id>
    <title>üßô‚Äç‚ôÇÔ∏è I Built a Local AI Dungeon Master ‚Äì Meet Dungeo_ai (Open Source &amp; Powered by ollama)</title>
    <updated>2025-06-12T15:52:05+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l9py3c/i_built_a_local_ai_dungeon_master_meet_dungeo_ai/"&gt; &lt;img alt="üßô‚Äç‚ôÇÔ∏è I Built a Local AI Dungeon Master ‚Äì Meet Dungeo_ai (Open Source &amp;amp; Powered by ollama)" src="https://external-preview.redd.it/VKXieTbOFzGU2aZe9EDyviI58NBmBHkcxoJcwzIpt0A.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78f390b5728c07a17be669aafa3632a5c3408b7e" title="üßô‚Äç‚ôÇÔ∏è I Built a Local AI Dungeon Master ‚Äì Meet Dungeo_ai (Open Source &amp;amp; Powered by ollama)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1l9py3c/video/cswkxr8rpi6f1/player"&gt;https://reddit.com/link/1l9py3c/video/cswkxr8rpi6f1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey folks!&lt;br /&gt; I‚Äôve been building something I'm super excited to finally share:&lt;br /&gt; üé≤ &lt;a href="https://github.com/Laszlobeer/Dungeo_ai/tree/main"&gt;&lt;strong&gt;Dungeo_ai&lt;/strong&gt;&lt;/a&gt; ‚Äì a fully local, AI-powered Dungeon Master designed for immersive solo RPGs, worldbuilding, and roleplay.&lt;/p&gt; &lt;p&gt;This project it's free and for now it connect to ollama(llm) and alltalktts(tts) &lt;/p&gt; &lt;p&gt;üõ†Ô∏è What it can do:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üíª Runs entirely &lt;strong&gt;locally&lt;/strong&gt; (with support for Ollama )&lt;/li&gt; &lt;li&gt;üß† Persists memory, character state, and custom personalities&lt;/li&gt; &lt;li&gt;üìú Simulates D&amp;amp;D-like dialogue and encounters dynamically&lt;/li&gt; &lt;li&gt;üó∫Ô∏è Expands lore over time with each interaction&lt;/li&gt; &lt;li&gt;üßô Great for solo campaigns, worldbuilding, or even prototyping NPCs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It‚Äôs still early days, but it‚Äôs usable and growing. I‚Äôd love feedback, collab ideas, or even just to know what kind of characters &lt;em&gt;you‚Äôd&lt;/em&gt; throw into it.&lt;/p&gt; &lt;p&gt;Here‚Äôs the link again:&lt;br /&gt; üëâ &lt;a href="https://github.com/Laszlobeer/Dungeo_ai/tree/main"&gt;https://github.com/Laszlobeer/Dungeo_ai/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for checking it out‚Äîand if you give it a spin, let me know how your first AI encounter goes. üòÑ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9py3c/i_built_a_local_ai_dungeon_master_meet_dungeo_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9py3c/i_built_a_local_ai_dungeon_master_meet_dungeo_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l9py3c/i_built_a_local_ai_dungeon_master_meet_dungeo_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-12T15:52:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lai0jn</id>
    <title>Planning a 7‚Äì8B Model Benchmark on 8GB GPU ‚Äî What Should I Test &amp; Measure?</title>
    <updated>2025-06-13T14:55:35+00:00</updated>
    <author>
      <name>/u/kekePower</name>
      <uri>https://old.reddit.com/user/kekePower</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;Following up on my last deep-dive into the 24B &lt;a href="https://www.reddit.com/r/MistralAI/comments/1lagg0o/performance_cost_deep_dive_benchmarking_the/"&gt;magistral&lt;/a&gt; model, I‚Äôm now gearing up for a new round of benchmarks - this time focused entirely on &lt;strong&gt;7‚Äì8B models that actually run on consumer-grade GPUs&lt;/strong&gt; (I'm testing on an RTX 3070, 8GB VRAM).&lt;/p&gt; &lt;p&gt;To make this genuinely useful, I want your input on how to approach the testing. Here‚Äôs what I‚Äôm looking for:&lt;/p&gt; &lt;h1&gt;1. Model Suggestions&lt;/h1&gt; &lt;p&gt;Which 7‚Äì8B models &lt;em&gt;need&lt;/em&gt; to be on the list?&lt;br /&gt; I'm looking for daily drivers, hidden gems, or just models you're curious about ‚Äî instruct, chat, or code variants welcome.&lt;/p&gt; &lt;h1&gt;2. Challenging Prompts&lt;/h1&gt; &lt;p&gt;Got a small handful (1‚Äì3 max) of &lt;strong&gt;killer prompts&lt;/strong&gt; that stress test these models?&lt;br /&gt; Think:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;multi-step reasoning&lt;/li&gt; &lt;li&gt;instruction-following&lt;/li&gt; &lt;li&gt;short code gen&lt;/li&gt; &lt;li&gt;abstract or creative tasks&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. What Should I Measure?&lt;/h1&gt; &lt;p&gt;Beyond just ‚Äúdoes it work,‚Äù I want to dig into what actually &lt;em&gt;matters&lt;/em&gt;. Here‚Äôs what I‚Äôve got so far:&lt;/p&gt; &lt;h1&gt;Quantitative Metrics:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Inference speed (tokens/sec)&lt;/li&gt; &lt;li&gt;VRAM usage during inference&lt;/li&gt; &lt;li&gt;Total token count per response&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Qualitative Metrics (more subjective):&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Reasoning &amp;amp; logic&lt;/li&gt; &lt;li&gt;Instruction-following fidelity&lt;/li&gt; &lt;li&gt;Code quality / creativity&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Got thoughts on how I should compare quality? Any scoring frameworks or benchmarks you‚Äôve seen done &lt;em&gt;right&lt;/em&gt;?&lt;/p&gt; &lt;p&gt;I‚Äôll keep the testing &lt;strong&gt;fair&lt;/strong&gt;, &lt;strong&gt;replicable&lt;/strong&gt;, and free of cherry-picked results. Just a straight-up look at what these small models can ‚Äî and can‚Äôt ‚Äî do.&lt;/p&gt; &lt;p&gt;If your suggestions make it into the final write-up, you‚Äôll be credited in the article. Thanks in advance ‚Äî this subreddit has some of the sharpest minds in the local LLM scene, and I know the feedback will make the piece better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kekePower"&gt; /u/kekePower &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lai0jn/planning_a_78b_model_benchmark_on_8gb_gpu_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lai0jn/planning_a_78b_model_benchmark_on_8gb_gpu_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lai0jn/planning_a_78b_model_benchmark_on_8gb_gpu_what/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-13T14:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9v3lk</id>
    <title>Are there any good models of less than 8Gb we can trust for simple tasks?</title>
    <updated>2025-06-12T19:11:46+00:00</updated>
    <author>
      <name>/u/New_Cranberry_6451</name>
      <uri>https://old.reddit.com/user/New_Cranberry_6451</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been testing models with a very simple set of tests, things like &amp;quot;Write the word Atom reversed&amp;quot; and I am quite dissapointed with the results as almost no models I have tested (Gemma3, Qwen3, Qwen2.5 in their small versions around 4.7Gb or 8Gb in the case of Gemma3) got it right on the first try. I am wondering if I am using Ollama the right way. I have made a simple JS client to work against the API, nothing fancy, just the common things following the official documentation. Do you have any advise? Or am I directly wasting my time with small models? If small models can't handle something as trivial as this, is there any real application for them? I feel like the enterprise closed models are light years ahead of what is being released in the open source community...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Cranberry_6451"&gt; /u/New_Cranberry_6451 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9v3lk/are_there_any_good_models_of_less_than_8gb_we_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9v3lk/are_there_any_good_models_of_less_than_8gb_we_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l9v3lk/are_there_any_good_models_of_less_than_8gb_we_can/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-12T19:11:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9z4vh</id>
    <title>New Agent Creator with Observer AI üöÄ!</title>
    <updated>2025-06-12T21:56:31+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l9z4vh/new_agent_creator_with_observer_ai/"&gt; &lt;img alt="New Agent Creator with Observer AI üöÄ!" src="https://external-preview.redd.it/eTI4dHYzZXRpazZmMdAGXmZBeVc_QZpDk3TS6rDj0o4TMoRZ42ebNhgLEVSd.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d831b0b22e36f4d633ee6a0ca0f405b74f3c747a" title="New Agent Creator with Observer AI üöÄ!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey ollama family! first of all I wanted to thank you so much for your support and feedback on running ollama with ObserverAI! I'm super grateful for your support and i'll keep adding features! Here are some features i just added:&lt;br /&gt; * AI Agent Builder&lt;br /&gt; * Template Agent Builder&lt;br /&gt; * SMS message notifications&lt;br /&gt; * Camera input&lt;br /&gt; * Microphone input (still needs work)&lt;br /&gt; * Whatsapp message notifiaction (rolled back but coming soon!, still needs work, got Meta account flagged for spam hahaha)&lt;br /&gt; * Computer audio transcription (beta, coming soon!)&lt;/p&gt; &lt;p&gt;Please check it out at &lt;a href="http://app.observer-ai.com"&gt;app.observer-ai.com&lt;/a&gt;, the project is 100% Open Source, and you can run it locally! (inference with ollama and webapp) &lt;a href="http://github.com/Roy3838/Observer"&gt;github.com/Roy3838/Observer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks so much Ollama community! You guys are awesome, I hope you can check it out and give me feedback on what to add next!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ltagz5etik6f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l9z4vh/new_agent_creator_with_observer_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l9z4vh/new_agent_creator_with_observer_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-12T21:56:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lafs6v</id>
    <title>Need help on RAG based project in legal domain.</title>
    <updated>2025-06-13T13:21:20+00:00</updated>
    <author>
      <name>/u/amitsingh80108</name>
      <uri>https://old.reddit.com/user/amitsingh80108</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I am currently learning RAG and trying to make domain specific RAG. &lt;/p&gt; &lt;p&gt;In legal domain the laws are very much similar and one word can change entire meaning. Hence the query from me is not able to retrieve the correct laws as I don't have knowledge of laws. &lt;/p&gt; &lt;p&gt;Instead I took case details, passed it to LLM and asked write 5 rag queries to retrieve relevant laws from vector database. &lt;/p&gt; &lt;p&gt;This seems to work at 50-60% accuracy. So I tried reranker and badly failed. Reranker reduced accuracy to 10-20%. I assume reranker may not be able to understand legal laws while reranking ? &lt;/p&gt; &lt;p&gt;Here I want some guidance from you all. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Am I doing correct thing ? &lt;/li&gt; &lt;li&gt;Chunk size I tried from 160 tokens till 500 tokens and above 400 tokens is what giving good accuracy. &lt;/li&gt; &lt;li&gt;Will fine tuning llm is of any use here? I am not sure if I train llm it will hallucinate or not. &lt;/li&gt; &lt;li&gt;Embeddings is from e5-large-instruct and it's the best in my testing. &lt;/li&gt; &lt;li&gt;If I want to host my LLM say Gemma 3 27B, how much ram it will take and also will there be OOM errors ? And what if multiple people use it at the same time will I see ram issues ? &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks guys. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amitsingh80108"&gt; /u/amitsingh80108 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lafs6v/need_help_on_rag_based_project_in_legal_domain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lafs6v/need_help_on_rag_based_project_in_legal_domain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lafs6v/need_help_on_rag_based_project_in_legal_domain/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-13T13:21:20+00:00</published>
  </entry>
</feed>
