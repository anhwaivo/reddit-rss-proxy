<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-31T20:06:20+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1idpjtj</id>
    <title>What would be the simplest way to train deepseek model on self hosted server.</title>
    <updated>2025-01-30T15:31:52+00:00</updated>
    <author>
      <name>/u/shaxadhere</name>
      <uri>https://old.reddit.com/user/shaxadhere</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to train model on specific set of data, I have the data in raw text and I have questions and answers from that raw text.&lt;/p&gt; &lt;p&gt;Is there any way I can train my model on all of that data.&lt;/p&gt; &lt;p&gt;Documents in total are 400,000 pages, and questions and answers are around 1 million+&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shaxadhere"&gt; /u/shaxadhere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idpjtj/what_would_be_the_simplest_way_to_train_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idpjtj/what_would_be_the_simplest_way_to_train_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idpjtj/what_would_be_the_simplest_way_to_train_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T15:31:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1idvw46</id>
    <title>Recommended deepseek model for coding tasks for an average PC?</title>
    <updated>2025-01-30T19:57:31+00:00</updated>
    <author>
      <name>/u/Upset_Hippo_5304</name>
      <uri>https://old.reddit.com/user/Upset_Hippo_5304</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Want to join the gang and try this stuff for coding tasks.&lt;/p&gt; &lt;p&gt;Py, dart, js, c#&lt;/p&gt; &lt;p&gt;32GB ram RTX 3070 Ryzen 5 5600x&lt;/p&gt; &lt;p&gt;Cheers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Upset_Hippo_5304"&gt; /u/Upset_Hippo_5304 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idvw46/recommended_deepseek_model_for_coding_tasks_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idvw46/recommended_deepseek_model_for_coding_tasks_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idvw46/recommended_deepseek_model_for_coding_tasks_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T19:57:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie6fzy</id>
    <title>Empty response for Deepseek Models</title>
    <updated>2025-01-31T04:09:36+00:00</updated>
    <author>
      <name>/u/TheHarinator</name>
      <uri>https://old.reddit.com/user/TheHarinator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone have a problem when trying to interact with the Deepseek-r1 models? The response always is empty for me. I have tried removing and re-pulling these models too. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheHarinator"&gt; /u/TheHarinator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie6fzy/empty_response_for_deepseek_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie6fzy/empty_response_for_deepseek_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie6fzy/empty_response_for_deepseek_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T04:09:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1iealuo</id>
    <title>why Ai Model requirement always show Memory instead GPU VRAM size?</title>
    <updated>2025-01-31T08:51:06+00:00</updated>
    <author>
      <name>/u/TheLastAirbender2025</name>
      <uri>https://old.reddit.com/user/TheLastAirbender2025</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, &lt;/p&gt; &lt;p&gt;Apologies in advance i am so new to Ai and bit confused here about term RAM mean. Does it mean memory in the GPU or Memory physically on the pc? I am windows user and my pc is bit old meaning 13 years at lest if not longer. &lt;/p&gt; &lt;p&gt;Example &lt;/p&gt; &lt;ul&gt; &lt;li&gt;7b models generally require at least 8GB of RAM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So when i installed a model onto my pc i see in system resources Ram hitting 9 GB and CPU % is like 30 but GPU is not even hitting 1% and 3D part show no activity. &lt;/p&gt; &lt;p&gt;I have Sparkle Intel Arc A380 ELF, 6GB GDDR6, Single Fan, SA380E-6G &lt;/p&gt; &lt;p&gt;Can someone explain this to me please &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLastAirbender2025"&gt; /u/TheLastAirbender2025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iealuo/why_ai_model_requirement_always_show_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iealuo/why_ai_model_requirement_always_show_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iealuo/why_ai_model_requirement_always_show_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T08:51:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1iebe80</id>
    <title>Deepseek unsecure?</title>
    <updated>2025-01-31T09:53:57+00:00</updated>
    <author>
      <name>/u/Emergency-Radish-696</name>
      <uri>https://old.reddit.com/user/Emergency-Radish-696</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My deepseek r1 will not do creative writing when I give it inputs with people's names or business. ( Writing sci-fi ) Is there an open unsecure model that isn't so bashful?( A deepseek model: unsecure?)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emergency-Radish-696"&gt; /u/Emergency-Radish-696 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iebe80/deepseek_unsecure/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iebe80/deepseek_unsecure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iebe80/deepseek_unsecure/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T09:53:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieam9t</id>
    <title>I can't see qwen2.5 max (the latest model) in ollama models list ? how can I run this locally ?</title>
    <updated>2025-01-31T08:52:01+00:00</updated>
    <author>
      <name>/u/AxelBlaze20850</name>
      <uri>https://old.reddit.com/user/AxelBlaze20850</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AxelBlaze20850"&gt; /u/AxelBlaze20850 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieam9t/i_cant_see_qwen25_max_the_latest_model_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieam9t/i_cant_see_qwen25_max_the_latest_model_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ieam9t/i_cant_see_qwen25_max_the_latest_model_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T08:52:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1idh8ft</id>
    <title>Deepseek r1 671b on my local PC</title>
    <updated>2025-01-30T07:03:27+00:00</updated>
    <author>
      <name>/u/Geschirrtuch</name>
      <uri>https://old.reddit.com/user/Geschirrtuch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;Two days ago, I turned night into day, and in the end, I managed to get R1 running on my local PC. Yesterday, I uploaded a video on YouTube showing how I did it: &lt;a href="https://www.youtube.com/watch?v=O3Lk3xSkAdk"&gt;https://www.youtube.com/watch?v=O3Lk3xSkAdk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I don't post here often, so I'm not sure if sharing the link is okay—I hope it is.&lt;/p&gt; &lt;p&gt;The video is in German, but with subtitles, everyone should be able to understand it.&lt;br /&gt; Be careful if you want to try this yourself! ;)&lt;/p&gt; &lt;p&gt;Update:&lt;/p&gt; &lt;p&gt;For those who don't feel like watching the video: The &amp;quot;trick&amp;quot; was using Windows' pagefile. I set up three of them on three different SSDs, which gave me around 750GB of virtual memory in total.&lt;/p&gt; &lt;p&gt;Loading the model and answering a question took my PC about 90 minutes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Geschirrtuch"&gt; /u/Geschirrtuch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T07:03:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieby8e</id>
    <title>Is there a way to limit the number of concurrent downloads when pulling a model?</title>
    <updated>2025-01-31T10:35:17+00:00</updated>
    <author>
      <name>/u/MarinatedPickachu</name>
      <uri>https://old.reddit.com/user/MarinatedPickachu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm on windows and I get a lot of problems when pulling models, often the progress bar reverts to a lower number and many times i end up getting a &amp;quot;max retries exceeded&amp;quot; error completely cancelling the pull. Sometimes it succeeds, but rarely. I think it's caused by ollama creating many parallel downloads at once and cancelling them as soon as they stall for 5 seconds. Is there a way to limit the number of simultaneous downoads or to increase the timeout?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarinatedPickachu"&gt; /u/MarinatedPickachu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieby8e/is_there_a_way_to_limit_the_number_of_concurrent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieby8e/is_there_a_way_to_limit_the_number_of_concurrent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ieby8e/is_there_a_way_to_limit_the_number_of_concurrent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T10:35:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie42ld</id>
    <title>Deepseek-r1:8b is 4 times slower than llama3.2:8b on Ollama running locally</title>
    <updated>2025-01-31T02:03:05+00:00</updated>
    <author>
      <name>/u/PawanAgarwal</name>
      <uri>https://old.reddit.com/user/PawanAgarwal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am running both llama3.2:8b and deepseek-r1:8b locally on my mac. I noticed than latency per token for deepseek-r1:8b model is 4x llama3.2:8b. Since both are 8b versions, I was hoping latency would be similar. Anyone else also seeing that? Any configs needed for ollama for deepseek serving?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PawanAgarwal"&gt; /u/PawanAgarwal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie42ld/deepseekr18b_is_4_times_slower_than_llama328b_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie42ld/deepseekr18b_is_4_times_slower_than_llama328b_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie42ld/deepseekr18b_is_4_times_slower_than_llama328b_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T02:03:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1iecg2d</id>
    <title>What is wrong with the fp16 llama3.2 model?</title>
    <updated>2025-01-31T11:11:12+00:00</updated>
    <author>
      <name>/u/Octopus0nFire</name>
      <uri>https://old.reddit.com/user/Octopus0nFire</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iecg2d/what_is_wrong_with_the_fp16_llama32_model/"&gt; &lt;img alt="What is wrong with the fp16 llama3.2 model?" src="https://a.thumbs.redditmedia.com/X9t_AQnuJ6IunxNin7I8EXVXmN37kil_4Wm0LgjbQo8.jpg" title="What is wrong with the fp16 llama3.2 model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4pyucoo0bbge1.png?width=639&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67afdd30cb3efb720edb25967e88677f6168f6e3"&gt;https://preview.redd.it/4pyucoo0bbge1.png?width=639&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67afdd30cb3efb720edb25967e88677f6168f6e3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model with the tag 'latest' work as expexted, but the fp16 (and also the q8_0 llama 3.1 model) seem to be absolutely insane. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Octopus0nFire"&gt; /u/Octopus0nFire &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iecg2d/what_is_wrong_with_the_fp16_llama32_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iecg2d/what_is_wrong_with_the_fp16_llama32_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iecg2d/what_is_wrong_with_the_fp16_llama32_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T11:11:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1iecgob</id>
    <title>LLama2 absurd response</title>
    <updated>2025-01-31T11:12:29+00:00</updated>
    <author>
      <name>/u/ppadiya</name>
      <uri>https://old.reddit.com/user/ppadiya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iecgob/llama2_absurd_response/"&gt; &lt;img alt="LLama2 absurd response" src="https://b.thumbs.redditmedia.com/4MKGfDk6ED6EOXxngZ1hATaXUNpkIBZRADr4clqZfpE.jpg" title="LLama2 absurd response" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/cwaw8c13bbge1.png?width=1731&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a56793cfeaefc285ee5bb39163047556a21c49b7"&gt;https://preview.redd.it/cwaw8c13bbge1.png?width=1731&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a56793cfeaefc285ee5bb39163047556a21c49b7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What is happening here? I just loaded it and asked 'Hi, How are you' and it went on and on about learning Chinese. I had to interrupt it as it went on and on for over 5 mins.&lt;br /&gt; I then loaded it on open-webui and asked the same question. it again started a very long response on 'aspiring writers and authors' &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ppadiya"&gt; /u/ppadiya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iecgob/llama2_absurd_response/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iecgob/llama2_absurd_response/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iecgob/llama2_absurd_response/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T11:12:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iecleb</id>
    <title>Running DeepSeek R1 on my M4 Pro Mac mini with Ollama</title>
    <updated>2025-01-31T11:21:31+00:00</updated>
    <author>
      <name>/u/ope_poe</name>
      <uri>https://old.reddit.com/user/ope_poe</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ope_poe"&gt; /u/ope_poe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/macmini/comments/1idfuew/running_deepseek_r1_on_my_m4_pro_mac_mini_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iecleb/running_deepseek_r1_on_my_m4_pro_mac_mini_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iecleb/running_deepseek_r1_on_my_m4_pro_mac_mini_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T11:21:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1iedn22</id>
    <title>a 5B model? need some help ( cause of some words, not very nsfw but eh)</title>
    <updated>2025-01-31T12:26:52+00:00</updated>
    <author>
      <name>/u/Ardion63</name>
      <uri>https://old.reddit.com/user/Ardion63</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's ups guys, im first timer here in this reddit page but i need some help&lt;/p&gt; &lt;p&gt;i got a rtx 3060 6 gb vrm , i can use 7B models but the speed is a bit slow, especially longer sentences&lt;br /&gt; do you guys know any good 5B models?&lt;/p&gt; &lt;p&gt;Prefer Uncensored or abliterated ones cause i want to see how much &amp;quot;personality&amp;quot; an AI can get with its text / make it be my AI for the rest of my life, so a little good buddy AI just wont cut but oh well&lt;/p&gt; &lt;p&gt;i tried a bunch 7B models, they are alright but you know vram limited ( the rest of my laptop specs are much better yea) i am also trying the prithivMLmods/Triangulum-5B&lt;/p&gt; &lt;p&gt;but other then that i haven't seen any 5B ( 3B will make the AI impossible to get what i wanted sooo 5B is the sweet spot)&lt;/p&gt; &lt;p&gt;thanks!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ardion63"&gt; /u/Ardion63 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iedn22/a_5b_model_need_some_help_cause_of_some_words_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iedn22/a_5b_model_need_some_help_cause_of_some_words_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iedn22/a_5b_model_need_some_help_cause_of_some_words_not/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T12:26:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie1aui</id>
    <title>Does `ollama create` actually build a new model?</title>
    <updated>2025-01-30T23:50:04+00:00</updated>
    <author>
      <name>/u/homelab2946</name>
      <uri>https://old.reddit.com/user/homelab2946</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded a GGUF file and create a Modelfile to extend it. Then I run `ollama create model_name -f Modelfile`, a `model_name:latest` model is created and shows in `ollama list` 10 GB. The GGUF file is also around the same 10 GB. Does `ollama create` not just add instruction on a base model but actually acting more like `docker build`? Would it then be fine to remove the GGUF file after the build?&lt;/p&gt; &lt;p&gt;Another scenario is through a supported ollama model, like `llama3`. Does Ollama &amp;quot;build&amp;quot; a new image if I create a new Modelfile from `llama3`, so it takes double the storage?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/homelab2946"&gt; /u/homelab2946 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1aui/does_ollama_create_actually_build_a_new_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1aui/does_ollama_create_actually_build_a_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie1aui/does_ollama_create_actually_build_a_new_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T23:50:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iegqef</id>
    <title>AI models basics for newcomers</title>
    <updated>2025-01-31T15:03:14+00:00</updated>
    <author>
      <name>/u/Level_Fennel8071</name>
      <uri>https://old.reddit.com/user/Level_Fennel8071</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there any good place to underatand basic terms and specs for different models,and how it gonna affect the model usecases and hw requirements, terms like parameter size, quantization, model type, model format, context window....etc.&lt;/p&gt; &lt;p&gt;my goal is to run model that helps me studying, mostly summarizing books and docs, being able to chat with the model about some materials is plus&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Level_Fennel8071"&gt; /u/Level_Fennel8071 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegqef/ai_models_basics_for_newcomers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegqef/ai_models_basics_for_newcomers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iegqef/ai_models_basics_for_newcomers/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T15:03:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iejgwp</id>
    <title>Hugging face TAG for models supporting "tools" mode?</title>
    <updated>2025-01-31T17:01:19+00:00</updated>
    <author>
      <name>/u/HeadGr</name>
      <uri>https://old.reddit.com/user/HeadGr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm testing models for AI mysql agent, and can't find appropriate tag on HuggingFace models lisl to pick ones capable. Please, point me to correct one(s)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HeadGr"&gt; /u/HeadGr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iejgwp/hugging_face_tag_for_models_supporting_tools_mode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iejgwp/hugging_face_tag_for_models_supporting_tools_mode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iejgwp/hugging_face_tag_for_models_supporting_tools_mode/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T17:01:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1iedpso</id>
    <title>Is anyone else's Deepseek a bit chatty?</title>
    <updated>2025-01-31T12:31:06+00:00</updated>
    <author>
      <name>/u/Jimmy_drumstix</name>
      <uri>https://old.reddit.com/user/Jimmy_drumstix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've installed deepseek-r1:8b on my computer using ollama. I am asking it to provide shorter answers but it has verbal diarrhoea. Has anyone else experienced this?&lt;/p&gt; &lt;p&gt;Here's from the last two prompts I gave it:&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jimmy_drumstix"&gt; /u/Jimmy_drumstix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iedpso/is_anyone_elses_deepseek_a_bit_chatty/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iedpso/is_anyone_elses_deepseek_a_bit_chatty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iedpso/is_anyone_elses_deepseek_a_bit_chatty/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T12:31:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1iemeqp</id>
    <title>Which deepseek model to install locally?</title>
    <updated>2025-01-31T19:02:39+00:00</updated>
    <author>
      <name>/u/_weshall</name>
      <uri>https://old.reddit.com/user/_weshall</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using an M1 Macbook Air with 16GB of memory. I want to run a distilled deepseek model locally. What is the best model I can download that will be able to run on this machine without completely choking up the system?&lt;/p&gt; &lt;p&gt;From the limited information I was able to get on the internet the 14b model &lt;em&gt;could&lt;/em&gt; be the one I want but it seems like it performs worse than the 32b model on coding tasks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_weshall"&gt; /u/_weshall &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iemeqp/which_deepseek_model_to_install_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iemeqp/which_deepseek_model_to_install_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iemeqp/which_deepseek_model_to_install_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T19:02:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieeqcf</id>
    <title>What's going on with my download? keeps going backwards after it makes some progress?</title>
    <updated>2025-01-31T13:26:43+00:00</updated>
    <author>
      <name>/u/dusty_whale</name>
      <uri>https://old.reddit.com/user/dusty_whale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ieeqcf/whats_going_on_with_my_download_keeps_going/"&gt; &lt;img alt="What's going on with my download? keeps going backwards after it makes some progress?" src="https://external-preview.redd.it/MTljMHJtN2h6YmdlMfBoFUu8tfPjIjApOHTrjSrxAuMuJVPtuHQoaGzrhN1h.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5315b7f329164fa7d7289dc1353237595c527220" title="What's going on with my download? keeps going backwards after it makes some progress?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dusty_whale"&gt; /u/dusty_whale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rfoy8p7hzbge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieeqcf/whats_going_on_with_my_download_keeps_going/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ieeqcf/whats_going_on_with_my_download_keeps_going/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T13:26:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1idqxto</id>
    <title>Why Are All Local AI Models So Bad? No One Talks About This!</title>
    <updated>2025-01-30T16:31:59+00:00</updated>
    <author>
      <name>/u/NikkEvan</name>
      <uri>https://old.reddit.com/user/NikkEvan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with local AI models, even &amp;quot;high-end ones&amp;quot; like the recent DeepSeek-R1 32B, using Open WebUI.&lt;br /&gt; I expected them to be weaker than online models, but the gap is just ridiculous.&lt;br /&gt; Even for the simplest questions, they either fail, give nonsense answers, or completely misunderstand the input.&lt;/p&gt; &lt;p&gt;I’ve set the parameters and all the settings at the best i could, tried different setups, system prompts, and still , even after parsing a basic document just a few pages long, is a struggle.&lt;br /&gt; If it already fails here, how am I supposed to use it for hundreds of internal company documents?&lt;/p&gt; &lt;p&gt;The crazy part? No one talks about this!&lt;br /&gt; Instead, i see every video in youtube saying :&lt;br /&gt; &amp;quot;How to run locally (modelname) much better than chat-gpt&amp;quot;&lt;br /&gt; &amp;quot;Local Deepseek beats Chat-gpt&amp;quot;&lt;br /&gt; Than the question they ask to those local models are : How many 'R' are in the word Strawberry and the model answer: 2 ... lol &lt;/p&gt; &lt;p&gt;Why is the performance so bad, even on 32B models?&lt;/p&gt; &lt;p&gt;Why are there no proper guides to get the best out of local AI?&lt;br /&gt; Having a big hardware such as the Nvidia project DIGITS will make a big model work close to the online Chat-gpt 3 or 4 ? I see those has 175b parameters. &lt;/p&gt; &lt;p&gt;What are we missing?&lt;/p&gt; &lt;p&gt;I really want to make local AI work as close as the online models, even buying bigger and stronger hardware, but, right now, it just feels like a waste of time.&lt;br /&gt; Has anyone actually succeeded in making these models work well? If so, how? And , what do you intend for Working Well for a local Model ? &lt;/p&gt; &lt;p&gt;Let’s discuss this! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NikkEvan"&gt; /u/NikkEvan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T16:31:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iem982</id>
    <title>RAG and Perplexity like Search</title>
    <updated>2025-01-31T18:56:29+00:00</updated>
    <author>
      <name>/u/atomique90</name>
      <uri>https://old.reddit.com/user/atomique90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wonder which stack you guys use to solve the following tasks: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;RAG (Pipeline): How do you guys get your data chunked and stored into a vectorized database? Especially what do you do to upsert or update the data? I am interested in solutions that are local only and dont rely on a cloud service. &lt;/li&gt; &lt;li&gt;How do you realize perplexity like chat search with ollama? I tried perplexica but especially with k8s I had many problems to kickstart it. I would love to have it in an UI like open webui. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I already tried to play a bit with ollama, perplexica, open webui, flowise, n8n, qdrant and psql (chat memory). &lt;/p&gt; &lt;p&gt;I am just curious what would be the best way to solve this and improve my journey! &lt;/p&gt; &lt;p&gt;Thanks a lot&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atomique90"&gt; /u/atomique90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iem982/rag_and_perplexity_like_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iem982/rag_and_perplexity_like_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iem982/rag_and_perplexity_like_search/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T18:56:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iegbea</id>
    <title>Why is open-webui this size?</title>
    <updated>2025-01-31T14:43:53+00:00</updated>
    <author>
      <name>/u/DevuDixit</name>
      <uri>https://old.reddit.com/user/DevuDixit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iegbea/why_is_openwebui_this_size/"&gt; &lt;img alt="Why is open-webui this size?" src="https://external-preview.redd.it/Erly_C0iblWMXf_1Jomyy7GuVEMgel-rmrf80xwRk9I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa064b742fd7f951e2c00ee5d40558b26fb7d7d2" title="Why is open-webui this size?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/87a620piccge1.png?width=1102&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=824a6c45bcd69a837b8fe314f55546e743df3eee"&gt;https://preview.redd.it/87a620piccge1.png?width=1102&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=824a6c45bcd69a837b8fe314f55546e743df3eee&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/id1h4wgudcge1.png?width=1089&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f88f10529c414c7eaa7e3b463f062079af32aeb1"&gt;https://preview.redd.it/id1h4wgudcge1.png?width=1089&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f88f10529c414c7eaa7e3b463f062079af32aeb1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I was just trying to install and use the &lt;a href="https://github.com/open-webui/open-webui"&gt;open-webui&lt;/a&gt; for using the gui and I was curious about these files (marked by red) it is pulling. I have used some local web based guis in the past and all of them were under 100 MBs of size. So why is this comparatively bulky ? ( just curious ; )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DevuDixit"&gt; /u/DevuDixit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegbea/why_is_openwebui_this_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegbea/why_is_openwebui_this_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iegbea/why_is_openwebui_this_size/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T14:43:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1iegog2</id>
    <title>Would x2 RTX 3060 12GB suffice advanced models like DeepSeek R1 14B or higher?</title>
    <updated>2025-01-31T15:00:54+00:00</updated>
    <author>
      <name>/u/GamerGuy95953</name>
      <uri>https://old.reddit.com/user/GamerGuy95953</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am determining if I should buy 2 RTX 3060 12GB (around $270 US each.) wondering if it’s even worth it over other options. Based on my research AI models perform better with lots of VRAM over clock speeds. &lt;/p&gt; &lt;p&gt;My server PC setup currently is a GTX 1050 and a Ryzen 7 2700X. I am planning to also upgrade the CPU to R9 something. It depending on the price for the R9 models. &lt;/p&gt; &lt;p&gt;Any suggestions are appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GamerGuy95953"&gt; /u/GamerGuy95953 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegog2/would_x2_rtx_3060_12gb_suffice_advanced_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegog2/would_x2_rtx_3060_12gb_suffice_advanced_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iegog2/would_x2_rtx_3060_12gb_suffice_advanced_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T15:00:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie210z</id>
    <title>Got Deepseek R1 1.5b running locally on Pixel 8 pro</title>
    <updated>2025-01-31T00:24:19+00:00</updated>
    <author>
      <name>/u/Teradyyne</name>
      <uri>https://old.reddit.com/user/Teradyyne</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"&gt; &lt;img alt="Got Deepseek R1 1.5b running locally on Pixel 8 pro" src="https://external-preview.redd.it/cTk0emwwZ3ozOGdlMYbCJM1MQLfOpw8fF1FnxkqAh7visCMP7lFjyVXppg7i.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=263215164e10efcd203b94bae4672c47e11c63d8" title="Got Deepseek R1 1.5b running locally on Pixel 8 pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Teradyyne"&gt; /u/Teradyyne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cha0fplz38ge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T00:24:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1iem5it</id>
    <title>AI Tools as a Hiring Requirement? Just Saw a Job Post That Blew My Mind</title>
    <updated>2025-01-31T18:52:03+00:00</updated>
    <author>
      <name>/u/Far_Flamingo5333</name>
      <uri>https://old.reddit.com/user/Far_Flamingo5333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just came across a job listing that explicitly requires experience with Cursor and Windsurf as part of the stack. Not “nice to have” it’s actually listed as a preference for hiring.&lt;/p&gt; &lt;p&gt;The post reads:&lt;/p&gt; &lt;p&gt;“We’re hiring our first engineer(s)!&lt;/p&gt; &lt;p&gt;👾 Prefer AI-native (you use Cursor, Windsurf, or built your own setup)&lt;/p&gt; &lt;p&gt;👾 Can showcase past projects/work&lt;/p&gt; &lt;p&gt;👾 Based in the Bay Area &amp;amp; down for 3 days/week in-person&lt;/p&gt; &lt;p&gt;…we don’t technically have an office yet, so you can help us decide where to go”&lt;/p&gt; &lt;p&gt;I’m honestly amazed and astonished. This is the first time I’ve seen AI coding tools being treated as a must-have skill rather than just a productivity boost. It makes me wonder:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Are we at the point where AI-assisted coding is a hard requirement for top tech jobs&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Will future engineers be judged not just on their raw coding ability but how well they integrate AI into their workflow&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;How long until AI-native workflows become the default expectation everywhere?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Would love to hear thoughts from others, are you seeing this trend in hiring, or is this just an early sign of what’s to come?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far_Flamingo5333"&gt; /u/Far_Flamingo5333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iem5it/ai_tools_as_a_hiring_requirement_just_saw_a_job/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iem5it/ai_tools_as_a_hiring_requirement_just_saw_a_job/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iem5it/ai_tools_as_a_hiring_requirement_just_saw_a_job/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T18:52:03+00:00</published>
  </entry>
</feed>
