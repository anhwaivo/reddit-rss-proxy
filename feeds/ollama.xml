<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-04-03T12:48:54+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jp3rbz</id>
    <title>GenAI Job Roles</title>
    <updated>2025-04-01T19:01:23+00:00</updated>
    <author>
      <name>/u/Electrical-Button635</name>
      <uri>https://old.reddit.com/user/Electrical-Button635</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Good people of Reddit.&lt;/p&gt; &lt;p&gt;As i recently transitioning from a full stack dev (laravel LAMP stack) to GenAI role internal transition. &lt;/p&gt; &lt;p&gt;My main task is to integrate llms using frameworks like langchain and langraph. Llm Monitoring using langsmith.&lt;/p&gt; &lt;p&gt;Implementation of RAGs using ChromaDB to cover business specific usecases mainly to reduce hallucinations in responses. Still learning tho.&lt;/p&gt; &lt;p&gt;My next step is to learn langsmith for Agents and tool calling And learn &amp;quot;Fine-tuning a model&amp;quot; then gradually move to multi-modal implementations usecases such as images and stuff.&lt;/p&gt; &lt;p&gt;As it's been roughly 2months as of now i feel like I'm still majorly doing webdev but pipelining llm calls for smart saas.&lt;/p&gt; &lt;p&gt;I Mainly work in Django and fastAPI.&lt;/p&gt; &lt;p&gt;My motive is to switch for a proper genAi role in maybe 3-4 months.&lt;/p&gt; &lt;p&gt;People working in a genAi roles what's your actual day like means do you also deals with above topics or is it totally different story. Sorry i don't have much knowledge in this field I'm purely driven by passion here so i might sound naive.&lt;/p&gt; &lt;p&gt;I'll be glad if you could suggest what topics should i focus on and just some insights in this field I'll be forever grateful. Or maybe some great resources which can help me out here.&lt;/p&gt; &lt;p&gt;Thanks for your time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electrical-Button635"&gt; /u/Electrical-Button635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jp3rbz/genai_job_roles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jp3rbz/genai_job_roles/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jp3rbz/genai_job_roles/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-01T19:01:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp1q07</id>
    <title>Ollama parallel request tuning on M4 MacMini</title>
    <updated>2025-04-01T17:41:13+00:00</updated>
    <author>
      <name>/u/icbts</name>
      <uri>https://old.reddit.com/user/icbts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jp1q07/ollama_parallel_request_tuning_on_m4_macmini/"&gt; &lt;img alt="Ollama parallel request tuning on M4 MacMini" src="https://external-preview.redd.it/gIY3BxKRS1Rg5xMnJJyphiZvjRXHfeEXBsQzC2O1sSY.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3440b1ace8a83f475823d8301e4ba2f7d1082b0d" title="Ollama parallel request tuning on M4 MacMini" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In this video we tune Ollama's Parallel Request settings with several LLMs, if your model is somewhat small (7B and below), tuning towards 16 to 32 contexts will give you much better throughput performance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/icbts"&gt; /u/icbts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=hAHCQR-kD0U"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jp1q07/ollama_parallel_request_tuning_on_m4_macmini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jp1q07/ollama_parallel_request_tuning_on_m4_macmini/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-01T17:41:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jphiq2</id>
    <title>Server Help</title>
    <updated>2025-04-02T05:47:21+00:00</updated>
    <author>
      <name>/u/Far-Guarantee2097</name>
      <uri>https://old.reddit.com/user/Far-Guarantee2097</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to upload ollama's mistral model to my college server, but for some reason it isnt accepting the model path in my MacBook Pro.&lt;/p&gt; &lt;p&gt;I pulled the path of the models from my Finder and then used that, but it says the path doesn't exist. Can anyone let me know why this is happening or what else can I try?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Guarantee2097"&gt; /u/Far-Guarantee2097 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jphiq2/server_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jphiq2/server_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jphiq2/server_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T05:47:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpixcu</id>
    <title>(HELP) Building a RAG system</title>
    <updated>2025-04-02T07:28:23+00:00</updated>
    <author>
      <name>/u/Rambr1516</name>
      <uri>https://old.reddit.com/user/Rambr1516</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jpixcu/help_building_a_rag_system/"&gt; &lt;img alt="(HELP) Building a RAG system" src="https://b.thumbs.redditmedia.com/Z-bgtw2jkOM95ZHe1neHR0K4AGtjlhmQjsAxiV8kTKE.jpg" title="(HELP) Building a RAG system" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone - I need some help. I am a very beginner programmer with very VERY basic knowledge and I want to set up a RAG system with my obsidian vault (hundreds of markdown files totaling over 200k words) I also only have a machine with 16gb of ram (m1 pro macbook) but would love to use this RAG with local models and my open router integrations. &lt;/p&gt; &lt;p&gt;As I said I am a noob with programming, but absolutely not a noob with computer, I want this to be something I can learn and then update as time goes on, and especially update when I get a beefier system (MORE RAM). Ideally I would love to get on a call with someone, or just get a place to start learning. ChatGPT said something about chromaDB and LangChain but that is all greek to me. &lt;/p&gt; &lt;p&gt;Thank you so much in advance - if you are a pro at this shit lmk, im broke but a call would take time (like an hour or less) and time is money :) &lt;/p&gt; &lt;p&gt;have a good day&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b25wrgh1jdse1.png?width=278&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=114ac90de061b23d8deac469d69d55113ceda0f6"&gt;lots of words lol&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/khcyaqi2jdse1.png?width=566&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62607e7283571d3e255dbfdf8f9740350b3823d4"&gt;DISREGARD ATTACHMENTS - I only want MD files&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rambr1516"&gt; /u/Rambr1516 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpixcu/help_building_a_rag_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpixcu/help_building_a_rag_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jpixcu/help_building_a_rag_system/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T07:28:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpk3ty</id>
    <title>Deepseek V3 0324 modelfile</title>
    <updated>2025-04-02T09:00:00+00:00</updated>
    <author>
      <name>/u/razekery</name>
      <uri>https://old.reddit.com/user/razekery</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, i want to run Deepseek V3 locally with ollama &amp;amp; open webui, specifically the &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF&lt;/a&gt; version Q4_K_M that i merged priorly.&lt;/p&gt; &lt;p&gt;Can you guys review my modelfile and tell me if it's ok ?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;FROM D:/AI/DeepSeek-V3-0324-Q4_K_M-merged.gguf # --- Prompt Template --- TEMPLATE &amp;quot;&amp;quot;&amp;quot;{{- range $i, $_ := .Messages }} {{- if eq .Role &amp;quot;user&amp;quot; }}&amp;lt;｜User｜&amp;gt; {{- else if eq .Role &amp;quot;assistant&amp;quot; }}&amp;lt;｜Assistant｜&amp;gt; {{- end }}{{ .Content }} {{- if eq (len (slice $.Messages $i)) 1 }} {{- if eq .Role &amp;quot;user&amp;quot; }}&amp;lt;｜Assistant｜&amp;gt; {{- end }} {{- else if eq .Role &amp;quot;assistant&amp;quot; }}&amp;lt;｜end▁of▁sentence｜&amp;gt;&amp;lt;｜begin▁of▁sentence｜&amp;gt; {{- end }} {{- end }}&amp;quot;&amp;quot;&amp;quot; # --- Core Parameters --- PARAMETER stop &amp;quot;&amp;lt;｜begin▁of▁sentence｜&amp;gt;&amp;quot; PARAMETER stop &amp;quot;&amp;lt;｜end▁of▁sentence｜&amp;gt;&amp;quot; PARAMETER stop &amp;quot;&amp;lt;｜User｜&amp;gt;&amp;quot; PARAMETER stop &amp;quot;&amp;lt;｜Assistant｜&amp;gt;&amp;quot; PARAMETER num_gpu -1 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/razekery"&gt; /u/razekery &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpk3ty/deepseek_v3_0324_modelfile/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpk3ty/deepseek_v3_0324_modelfile/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jpk3ty/deepseek_v3_0324_modelfile/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T09:00:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpkycf</id>
    <title>Saving Ollama Conversation State</title>
    <updated>2025-04-02T10:03:23+00:00</updated>
    <author>
      <name>/u/sasizza</name>
      <uri>https://old.reddit.com/user/sasizza</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I'm currently using Ollama and finding it very useful, but I'm encountering difficulty in saving the conversation state (without &amp;quot;preloadd&amp;quot;). Is there a method to export or persist the chat history for later resumption? Any assistance or guidance would be greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sasizza"&gt; /u/sasizza &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpkycf/saving_ollama_conversation_state/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpkycf/saving_ollama_conversation_state/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jpkycf/saving_ollama_conversation_state/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T10:03:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1joyhf5</id>
    <title>I made this simple local RAG example using Langchain, ChromaDB &amp; Ollama</title>
    <updated>2025-04-01T15:31:16+00:00</updated>
    <author>
      <name>/u/yussufbyk</name>
      <uri>https://old.reddit.com/user/yussufbyk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made this after seeing that basically nobody on the internet have made a readable and clean code about this that was still working.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/yussufbiyik/langchain-chromadb-rag-example"&gt;https://github.com/yussufbiyik/langchain-chromadb-rag-example&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to contribute or test it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yussufbyk"&gt; /u/yussufbyk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1joyhf5/i_made_this_simple_local_rag_example_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1joyhf5/i_made_this_simple_local_rag_example_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1joyhf5/i_made_this_simple_local_rag_example_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-01T15:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpm7ur</id>
    <title>Ollama light model to train</title>
    <updated>2025-04-02T11:26:29+00:00</updated>
    <author>
      <name>/u/Duckmastermind1</name>
      <uri>https://old.reddit.com/user/Duckmastermind1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, so I'm new to Ollama and IA (installed 30min ago), is there any light model that can talk basically and is fast, and I can teach it any data I have. &lt;/p&gt; &lt;p&gt;On other platforms I saw people mention to just pass data via prompt, yet, I prefer not to do it that way. &lt;/p&gt; &lt;p&gt;Any idea, or am I at the wrong place and have to use other method to train a basic model? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Duckmastermind1"&gt; /u/Duckmastermind1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpm7ur/ollama_light_model_to_train/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpm7ur/ollama_light_model_to_train/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jpm7ur/ollama_light_model_to_train/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T11:26:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jphngr</id>
    <title>How can I reduce hallucinations with ollama</title>
    <updated>2025-04-02T05:56:12+00:00</updated>
    <author>
      <name>/u/Pure-Caramel1216</name>
      <uri>https://old.reddit.com/user/Pure-Caramel1216</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to build an app using ollama api with the chat endpoint but the thing is it sometimes hallucinates a lot, how can make it so it does not hallucinatite (or hallucinates less)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pure-Caramel1216"&gt; /u/Pure-Caramel1216 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jphngr/how_can_i_reduce_hallucinations_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jphngr/how_can_i_reduce_hallucinations_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jphngr/how_can_i_reduce_hallucinations_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T05:56:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jppo3z</id>
    <title>tried a bunch of open models with goose</title>
    <updated>2025-04-02T14:16:42+00:00</updated>
    <author>
      <name>/u/lifelonglearn3r</name>
      <uri>https://old.reddit.com/user/lifelonglearn3r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jppo3z/tried_a_bunch_of_open_models_with_goose/"&gt; &lt;img alt="tried a bunch of open models with goose" src="https://external-preview.redd.it/8ypLw4iI5hM69IbWKzJkC6YTV7o1sva9BZJYBWMD5KY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cd7ebb67fdbf8351f9a6eedeebf6f17a1be026ca" title="tried a bunch of open models with goose" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lifelonglearn3r"&gt; /u/lifelonglearn3r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1jpbptf/tried_a_bunch_of_open_models_with_goose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jppo3z/tried_a_bunch_of_open_models_with_goose/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jppo3z/tried_a_bunch_of_open_models_with_goose/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T14:16:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpl3p3</id>
    <title>Funny response from deepseekr1:14b: How many 'r' characters are there in the word "strawberry"?</title>
    <updated>2025-04-02T10:14:11+00:00</updated>
    <author>
      <name>/u/Agreeable_Repeat_568</name>
      <uri>https://old.reddit.com/user/Agreeable_Repeat_568</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fyi this was on on deepseek r1:14b running in openweb-ui. The thinking part of this answer is a but crazy with this response. IDK I am new to ai I am guessing this is common with deepseek?&lt;/p&gt; &lt;p&gt;&amp;lt;details type=&amp;quot;reasoning&amp;quot; done=&amp;quot;true&amp;quot; duration=&amp;quot;38&amp;quot;&amp;gt;&lt;/p&gt; &lt;p&gt;&amp;lt;summary&amp;gt;Thought for 38 seconds&amp;lt;/summary&amp;gt;&lt;/p&gt; &lt;p&gt;&amp;gt; Okay, so I need to figure out how many times the letter 'r' appears in the word &amp;quot;strawberry&amp;quot;. Let me start by writing down the word clearly: s-t-r-a-w-b-e-r-r-y. Wait, no, that's not exactly right. The correct spelling is s-t-r-a-w-b-e-r-r-y? Or maybe I'm misremembering. Hmm.&lt;/p&gt; &lt;p&gt;&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;gt; Let me break it down letter by letter to make sure I don't miss any 'r's. So starting from the beginning:&lt;/p&gt; &lt;p&gt;&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;gt; 1. S&lt;/p&gt; &lt;p&gt;&amp;gt; 2. T&lt;/p&gt; &lt;p&gt;&amp;gt; 3. R – okay, that's one 'r' right there.&lt;/p&gt; &lt;p&gt;&amp;gt; 4. A&lt;/p&gt; &lt;p&gt;&amp;gt; 5. W&lt;/p&gt; &lt;p&gt;&amp;gt; 6. B&lt;/p&gt; &lt;p&gt;&amp;gt; 7. E&lt;/p&gt; &lt;p&gt;&amp;gt; 8. R – another 'r', so now we're at two.&lt;/p&gt; &lt;p&gt;&amp;gt; 9. R again? Wait, does &amp;quot;strawberry&amp;quot; have three 'r's? Let me check.&lt;/p&gt; &lt;p&gt;&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;gt; Wait, no. The word is s-t-r-a-w-b-e-r-r-y. So after the first 'r' at position 3, there are two more 'r's later on. That makes it three in total? Or maybe I'm counting wrong. Alternatively, perhaps &amp;quot;strawberry&amp;quot; only has two 'r's.&lt;/p&gt; &lt;p&gt;&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;gt; Let me write it out: s t r a w b e r r y. So the letters are s, t, r, a, w, b, e, r, r, y. That's 10 letters in total. Now, looking for 'r's:&lt;/p&gt; &lt;p&gt;&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;gt; - The third letter is 'r'.&lt;/p&gt; &lt;p&gt;&amp;gt; - Then later on, after 'e', there are two more consecutive 'r's.&lt;/p&gt; &lt;p&gt;&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;gt; So that would make three 'r's in the word. Wait, but I'm not sure if it's two or three. Maybe I should count again carefully.&lt;/p&gt; &lt;p&gt;&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;gt; s (1), t (2), r (3) – first 'r'. Then a, w, b, e, then another r at position 8 and another at position 9. So that's positions 3, 8, 9. Wait, but &amp;quot;strawberry&amp;quot; is spelled s-t-r-a-w-b-e-r-r-y, right? Or does it have two 'r's after the 'e'? Let me confirm.&lt;/p&gt; &lt;p&gt;&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;gt; Yes, &amp;quot;strawberry&amp;quot; has an 'r' in the third position and then two more 'r's towards the end. So that's three 'r's total. But wait, when I think about it, sometimes people might not pronounce both 'r's clearly, but for spelling purposes, they are there.&lt;/p&gt; &lt;p&gt;&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;gt; Wait a second, no. Maybe I'm overcomplicating. Let me write each letter down:&lt;/p&gt; &lt;p&gt;&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;gt; 1. S&lt;/p&gt; &lt;p&gt;&amp;gt; 2. T&lt;/p&gt; &lt;p&gt;&amp;gt; 3. R&lt;/p&gt; &lt;p&gt;&amp;gt; 4. A&lt;/p&gt; &lt;p&gt;&amp;gt; 5. W&lt;/p&gt; &lt;p&gt;&amp;gt; 6. B&lt;/p&gt; &lt;p&gt;&amp;gt; 7. E&lt;/p&gt; &lt;p&gt;&amp;gt; 8. R&lt;/p&gt; &lt;p&gt;&amp;gt; 9. R&lt;/p&gt; &lt;p&gt;&amp;gt; 10. Y&lt;/p&gt; &lt;p&gt;&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;gt; So from 3, 8, and 9 are 'r's. That makes three in total. But I'm a bit confused because sometimes I hear people say &amp;quot;strawberry&amp;quot; without the second 'r' sound, but that doesn't change the spelling.&lt;/p&gt; &lt;p&gt;&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;gt; Wait, no, maybe it's only two 'r's. Let me check online quickly to confirm. Oh wait, I can't access external resources, so I have to rely on my knowledge. The word &amp;quot;strawberry&amp;quot; is spelled with an 'r', then later has two more 'r's after the 'e'. So that makes three 'r's.&lt;/p&gt; &lt;p&gt;&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;gt; But now I'm second-guessing myself because sometimes in words, letters can be silent or not pronounced, but in spelling, they are present. So yes, &amp;quot;strawberry&amp;quot; should have three 'r's.&lt;/p&gt; &lt;p&gt;&amp;lt;/details&amp;gt;&lt;/p&gt; &lt;p&gt;The letter 'r' appears three times in the word &amp;quot;strawberry&amp;quot;. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;**Answer:** There are 3 'r' characters in &amp;quot;strawberry&amp;quot;.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agreeable_Repeat_568"&gt; /u/Agreeable_Repeat_568 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpl3p3/funny_response_from_deepseekr114b_how_many_r/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpl3p3/funny_response_from_deepseekr114b_how_many_r/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jpl3p3/funny_response_from_deepseekr114b_how_many_r/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T10:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpryie</id>
    <title>Best Model for json parser analyser.</title>
    <updated>2025-04-02T15:50:44+00:00</updated>
    <author>
      <name>/u/Proper-Acanthaceae39</name>
      <uri>https://old.reddit.com/user/Proper-Acanthaceae39</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, im new in the local LLM world, and im still learning.&lt;/p&gt; &lt;p&gt;Im running in my local a Ollama with gemma:2b, but im not sure if is the best one for what im doing.&lt;/p&gt; &lt;p&gt;Basically with python, in extracting a pdf with pdfplumber to a json.&lt;br /&gt; I want to send this json to the LLM, so it can understand the json and return me another parsed JSON.&lt;/p&gt; &lt;p&gt;However, I'm facing two main issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It seems like gemma only supports around 12k characters of context, which is hard to manage since the extracted JSON varies a lot depending on the PDF.&lt;/li&gt; &lt;li&gt;Its tooo slow, to process a small pdf, its taking too much time&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm also concerned about accuracy, I'm not sure if this is the most suitable model for structured data parsing. &lt;/p&gt; &lt;p&gt;Some one can help me with tips?&lt;/p&gt; &lt;p&gt;Also, here its the code&lt;/p&gt; &lt;pre&gt;&lt;code&gt;#aiProcessor.py import json import os import uuid import requests from typing import Optional def load_prompt(path: str) -&amp;gt; str: with open(path, &amp;quot;r&amp;quot;, encoding=&amp;quot;utf-8&amp;quot;) as f: return f.read().strip() def call_llm(pdf_json_data: list, filename: str, model: str = &amp;quot;gemma:2b&amp;quot;) -&amp;gt; str: client_prompt = load_prompt(&amp;quot;../json/client.prompt&amp;quot;) purchase_prompt = load_prompt(&amp;quot;../json/purchase.prompt&amp;quot;) full_prompt = f&amp;quot;&amp;quot;&amp;quot; You are an intelligent invoice parser. Based on the structured data extracted from a Brazilian invoice PDF (below), extract and return exactly TWO JSONs: First JSON: {client_prompt} Second JSON: {purchase_prompt} Only return valid JSON. Do not explain. Structured invoice data: {json.dumps(pdf_json_data, indent=2, ensure_ascii=False)[:12000]} Filename: {filename} &amp;quot;&amp;quot;&amp;quot; response = requests.post( &amp;quot;http://localhost:11434/api/generate&amp;quot;, json={&amp;quot;model&amp;quot;: model, &amp;quot;prompt&amp;quot;: full_prompt}, stream=True, timeout=300 ) result = &amp;quot;&amp;quot; for line in response.iter_lines(): if line: try: chunk = json.loads(line.decode(&amp;quot;utf-8&amp;quot;)) result += chunk.get(&amp;quot;response&amp;quot;, &amp;quot;&amp;quot;) except: continue return result.strip() def extract_two_jsons(text: str): import re candidates = re.findall(r'\{(?:[^{}]|\{[^{}]*\})*\}', text) if len(candidates) &amp;gt;= 2: return candidates[0], candidates[1] return None, None def process_with_ai( extracted_json: list, filename: str, save_to_disk: bool = False, output_dir: str = &amp;quot;output/ai&amp;quot; ) -&amp;gt; Optional[dict]: &amp;quot;&amp;quot;&amp;quot; Processa o JSON extraído do PDF com a IA e retorna dois JSONs: cliente e compra. &amp;quot;&amp;quot;&amp;quot; result_text = call_llm(extracted_json, filename) client_str, purchase_str = extract_two_jsons(result_text) if not client_str or not purchase_str: print(f&amp;quot;⚠️ Could not extract two JSONs from AI result for {filename}&amp;quot;) if save_to_disk: os.makedirs(f&amp;quot;{output_dir}/fallback&amp;quot;, exist_ok=True) with open(f&amp;quot;{output_dir}/fallback/{filename}.txt&amp;quot;, &amp;quot;w&amp;quot;, encoding=&amp;quot;utf-8&amp;quot;) as f: f.write(result_text) return None try: client_json = json.loads(client_str) purchase_json = json.loads(purchase_str) except json.JSONDecodeError as e: print(f&amp;quot;❌ JSON parse error for {filename}: {e}&amp;quot;) return None client_id = str(uuid.uuid4()) purchase_id = str(uuid.uuid4()) client_json[&amp;quot;id&amp;quot;] = client_id if &amp;quot;client&amp;quot; in purchase_json: purchase_json[&amp;quot;client&amp;quot;][&amp;quot;id&amp;quot;] = client_id purchase_json[&amp;quot;id&amp;quot;] = purchase_id if save_to_disk: os.makedirs(f&amp;quot;{output_dir}/clientes&amp;quot;, exist_ok=True) os.makedirs(f&amp;quot;{output_dir}/compras&amp;quot;, exist_ok=True) with open(f&amp;quot;{output_dir}/clientes/{client_id}.json&amp;quot;, &amp;quot;w&amp;quot;, encoding=&amp;quot;utf-8&amp;quot;) as f: json.dump(client_json, f, indent=2, ensure_ascii=False) with open(f&amp;quot;{output_dir}/compras/{purchase_id}.json&amp;quot;, &amp;quot;w&amp;quot;, encoding=&amp;quot;utf-8&amp;quot;) as f: json.dump(purchase_json, f, indent=2, ensure_ascii=False) return {&amp;quot;client&amp;quot;: client_json, &amp;quot;purchase&amp;quot;: purchase_json} # extractor.py import fitz # PyMuPDF import pdfplumber import json import os from typing import Union, Optional from io import BytesIO def extract_pdf_structure( file: Union[str, BytesIO], save_to_file: bool = False, output_path: Optional[str] = None ) -&amp;gt; Optional[list]: data = [] doc = fitz.open(stream=file.read(), filetype=&amp;quot;pdf&amp;quot;) if isinstance(file, BytesIO) else fitz.open(file) for page_num, page in enumerate(doc, start=1): page_data = { &amp;quot;page&amp;quot;: page_num, &amp;quot;text_blocks&amp;quot;: [], &amp;quot;tables&amp;quot;: [] } blocks = page.get_text(&amp;quot;dict&amp;quot;)[&amp;quot;blocks&amp;quot;] for block in blocks: if &amp;quot;lines&amp;quot; in block: text_content = &amp;quot;&amp;quot; for line in block[&amp;quot;lines&amp;quot;]: for span in line[&amp;quot;spans&amp;quot;]: text_content += span[&amp;quot;text&amp;quot;] + &amp;quot; &amp;quot; page_data[&amp;quot;text_blocks&amp;quot;].append({ &amp;quot;bbox&amp;quot;: block[&amp;quot;bbox&amp;quot;], &amp;quot;text&amp;quot;: text_content.strip() }) data.append(page_data) doc.close() plumber_doc = pdfplumber.open(file) if isinstance(file, str) else pdfplumber.open(BytesIO(file.getvalue())) for i, page in enumerate(plumber_doc.pages): try: tables = page.extract_tables() if tables: data[i][&amp;quot;tables&amp;quot;] = tables except: continue plumber_doc.close() if save_to_file and output_path: os.makedirs(os.path.dirname(output_path), exist_ok=True) with open(output_path, &amp;quot;w&amp;quot;, encoding=&amp;quot;utf-8&amp;quot;) as f: json.dump(data, f, indent=2, ensure_ascii=False) return data if not save_to_file else None &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proper-Acanthaceae39"&gt; /u/Proper-Acanthaceae39 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpryie/best_model_for_json_parser_analyser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpryie/best_model_for_json_parser_analyser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jpryie/best_model_for_json_parser_analyser/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T15:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jq249r</id>
    <title>Why do I get this error when downloading Gemma3 -- any ideas?</title>
    <updated>2025-04-02T22:40:58+00:00</updated>
    <author>
      <name>/u/Feckin_Eejit_69</name>
      <uri>https://old.reddit.com/user/Feckin_Eejit_69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ollama 0.5.4 — open-webui 0.6 —Linux/Ubuntu&lt;/p&gt; &lt;p&gt;I've been trying to download Gemma3 (any variant) using open-webui and every time I try I get an error message right (pop-up at upper right corner) at the beginning. It fails for every variant.&lt;/p&gt; &lt;p&gt;Downloads of all other models (e.g. from mistral, deepseek, etc) all work fine. It's only the Gemma3 models that give me the error.&lt;/p&gt; &lt;p&gt;Any ideas what could be the reason? (and what I should try to fix?)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Feckin_Eejit_69"&gt; /u/Feckin_Eejit_69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jq249r/why_do_i_get_this_error_when_downloading_gemma3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jq249r/why_do_i_get_this_error_when_downloading_gemma3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jq249r/why_do_i_get_this_error_when_downloading_gemma3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T22:40:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jq296o</id>
    <title>I made an almost universal LLM Creator/Trainer</title>
    <updated>2025-04-02T22:46:48+00:00</updated>
    <author>
      <name>/u/KiloXii</name>
      <uri>https://old.reddit.com/user/KiloXii</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created my own LLM creator/trainer to simplify the creation and training of huggingface models for use with ollama.&lt;/p&gt; &lt;p&gt;Essentially, you choose your base model from huggingface. (I don't know if it works with gated models yet but it works with normal ones)&lt;/p&gt; &lt;p&gt;then you give it a specifically formatted dataset, a system prompt, and a name and it will train the base model on all that info, merge the trained data with the model permanently, then create a gguf of your new model for download which you can use to make a modelfile for ollama.&lt;/p&gt; &lt;p&gt;And it's built using gradio for a simplified interface as well so the user only needs to learn minimal code to just to set up and then they can just run it from their browser locally.&lt;/p&gt; &lt;p&gt;In theory, it should work with most different types of models such as LLama, GPT, Mistral, Falcon, however so far I have only tested it with DeepSeek-R1-Distill-Qwen-1.5B and dolphin-LLama and it works for both of those.&lt;/p&gt; &lt;p&gt;Right now it doesnt work with models that don't have a chat template built into their tokenizer though such as wizardlm-uncensored, so I have to fix that later.&lt;/p&gt; &lt;p&gt;Anyways, I feel like this program may help a few people make their own models so this is the link to the github for it if anyone is interested:&lt;br /&gt; &lt;a href="https://github.com/KiloXiix/Kilos_Custom_LLM_Creator_Universal"&gt;https://github.com/KiloXiix/Kilos_Custom_LLM_Creator_Universal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what y'all think and if you find any bugs please as I want to make it better overall&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KiloXii"&gt; /u/KiloXii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jq296o/i_made_an_almost_universal_llm_creatortrainer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jq296o/i_made_an_almost_universal_llm_creatortrainer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jq296o/i_made_an_almost_universal_llm_creatortrainer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T22:46:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jq2gj4</id>
    <title>How do I select installation directories?</title>
    <updated>2025-04-02T22:55:56+00:00</updated>
    <author>
      <name>/u/ImpossibleBritches</name>
      <uri>https://old.reddit.com/user/ImpossibleBritches</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Earlier this morning I began experimenting with llama-stack.&lt;/p&gt; &lt;p&gt;I discovered that the llama cli either offers no way for the user to select installation directories, or if it does then this feature is not documented.&lt;/p&gt; &lt;p&gt;I removed it and installed ollama.&lt;/p&gt; &lt;p&gt;However, I'm having trouble discovering how to tell ollama where to install models.&lt;/p&gt; &lt;p&gt;Most of my system is on a crowded ssd. But I've got a secondary ssd where I've installed image models. There is a lot of space on my secondary ssd. I'd like to install llm's there.&lt;/p&gt; &lt;p&gt;How can I direct ollama to install models in a specified directory?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ImpossibleBritches"&gt; /u/ImpossibleBritches &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jq2gj4/how_do_i_select_installation_directories/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jq2gj4/how_do_i_select_installation_directories/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jq2gj4/how_do_i_select_installation_directories/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T22:55:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpjntg</id>
    <title>I made an Ollama hub</title>
    <updated>2025-04-02T08:24:58+00:00</updated>
    <author>
      <name>/u/retoor42</name>
      <uri>https://old.reddit.com/user/retoor42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made an Ollama hub where you can share your Ollama resources with others to be used.&lt;/p&gt; &lt;p&gt;Long story, see &lt;a href="https://ollama.molodetz.nl"&gt;https://ollama.molodetz.nl&lt;/a&gt; for information.&lt;/p&gt; &lt;p&gt;Uses of the API can just use their default api clients! For security people can only call chat completions api on the shared resources. Content gets validated before forwarded to your Ollama instance if you're a host.&lt;/p&gt; &lt;p&gt;I hope you guys like this concept. Donate your server!&lt;/p&gt; &lt;p&gt;See model availability here: &lt;a href="https://ollama.molodetz.nl/models"&gt;https://ollama.molodetz.nl/models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/retoor42"&gt; /u/retoor42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpjntg/i_made_an_ollama_hub/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpjntg/i_made_an_ollama_hub/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jpjntg/i_made_an_ollama_hub/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T08:24:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpnyx5</id>
    <title>Using LLM to work with documents?</title>
    <updated>2025-04-02T13:00:20+00:00</updated>
    <author>
      <name>/u/TheseMarionberry2902</name>
      <uri>https://old.reddit.com/user/TheseMarionberry2902</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ll jump in the use case: We have around 100 documents so far with an average of 50 pages each, and we are expanding this. We wanted to sort the information, search inside, map the information and their interlinks. The thing is that each document may or may not be directly linked to the other.&lt;/p&gt; &lt;p&gt;One idea was use make a gitlab wiki or a mindmap, and structure the documents and interlink them while having the documents on the wiki (for example a tree of information and their interlinks, and link to documents). Another thing is that the documents are on a MS sharepoint&lt;/p&gt; &lt;p&gt;I was suggesting to download a local LLM, and &amp;quot;upload&amp;quot; the documents and work directly and locally on a secure basis (no internet). Now imo that will help us easily to locate information within documents, analyse and work directly. It can help us even make the mindmap and visualizations.&lt;/p&gt; &lt;p&gt;Which is the right solution? Is my understanding correct? And what do I need to make it work? &lt;/p&gt; &lt;p&gt;Thank you. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheseMarionberry2902"&gt; /u/TheseMarionberry2902 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpnyx5/using_llm_to_work_with_documents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpnyx5/using_llm_to_work_with_documents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jpnyx5/using_llm_to_work_with_documents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T13:00:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jq8r5n</id>
    <title>Ram issue in ollama</title>
    <updated>2025-04-03T03:59:31+00:00</updated>
    <author>
      <name>/u/yakinrubaiyat</name>
      <uri>https://old.reddit.com/user/yakinrubaiyat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am facing an issue where using Ollama to make continuous calls (around 200+) to Gemma 3 uses up all my 32GB of RAM and then crashes. I can see the RAM usage increasing in Task Manager, and after some time, the system crashes. Does anyone have any suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yakinrubaiyat"&gt; /u/yakinrubaiyat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jq8r5n/ram_issue_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jq8r5n/ram_issue_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jq8r5n/ram_issue_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T03:59:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqa8xx</id>
    <title>Running Ollama model in a cloud service? It's murdering my Mac</title>
    <updated>2025-04-03T05:25:10+00:00</updated>
    <author>
      <name>/u/Wild_King_1035</name>
      <uri>https://old.reddit.com/user/Wild_King_1035</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm building a React Native app that sends user audio to llama 3.2, which is in a python backend that im running locally on my Macbook Pro.&lt;/p&gt; &lt;p&gt;I know its a terrible idea to run Ollama models on a Mac, and it is, even a single request eats up available CPU and threatens to crash my computer.&lt;/p&gt; &lt;p&gt;I realize I can't run it locally any longer, I need to host it somewhere but still have it available to continue working and testing it. &lt;/p&gt; &lt;p&gt;How can I host my backend for an affordable price? This is just a personal project, and I haven't hosted a backend this in-depth before. I'd prefer to host it now in a cloud service that I will eventually use if and when the app goes into production.&lt;/p&gt; &lt;p&gt;Thanks in advance all&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wild_King_1035"&gt; /u/Wild_King_1035 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqa8xx/running_ollama_model_in_a_cloud_service_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqa8xx/running_ollama_model_in_a_cloud_service_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jqa8xx/running_ollama_model_in_a_cloud_service_its/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T05:25:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jptd2v</id>
    <title>Fully Unified Model</title>
    <updated>2025-04-02T16:47:38+00:00</updated>
    <author>
      <name>/u/No-Mulberry6961</name>
      <uri>https://old.reddit.com/user/No-Mulberry6961</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From that one guy who brought you AMN&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Modern-Prometheus-AI/FullyUnifiedModel"&gt;https://github.com/Modern-Prometheus-AI/FullyUnifiedModel&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here is the repository to Fully Unified Model (FUM), an ambitious open-source AI project available on GitHub, developed by the creator of AMN. This repository explores the integration of diverse cognitive functions into a single framework. It features advanced concepts including a Self-Improvement Engine (SIE) driving learning through complex internal rewards (novelty, habituation) and an emergent Unified Knowledge Graph (UKG) built on neural activity and plasticity (STDP).&lt;/p&gt; &lt;p&gt;FUM is currently in active development (consider it alpha/beta stage). This project represents ongoing research into creating more holistic, potentially neuromorphic AI. Documentation is evolving. Feedback, questions, and potential contributions are highly encouraged via GitHub issues/discussions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Mulberry6961"&gt; /u/No-Mulberry6961 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jptd2v/fully_unified_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jptd2v/fully_unified_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jptd2v/fully_unified_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T16:47:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jq5l9v</id>
    <title>does anyone know why Gemma is doing this?(Gemma3:1b using through open-webui)</title>
    <updated>2025-04-03T01:21:32+00:00</updated>
    <author>
      <name>/u/mobheadfireball</name>
      <uri>https://old.reddit.com/user/mobheadfireball</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jq5l9v/does_anyone_know_why_gemma_is_doing_thisgemma31b/"&gt; &lt;img alt="does anyone know why Gemma is doing this?(Gemma3:1b using through open-webui)" src="https://preview.redd.it/b77n4zhpuise1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=728b201f97a590371b91ec2b5084713b03cd4534" title="does anyone know why Gemma is doing this?(Gemma3:1b using through open-webui)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mobheadfireball"&gt; /u/mobheadfireball &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b77n4zhpuise1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jq5l9v/does_anyone_know_why_gemma_is_doing_thisgemma31b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jq5l9v/does_anyone_know_why_gemma_is_doing_thisgemma31b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T01:21:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqfy9m</id>
    <title>I made my own CLI vibe tool</title>
    <updated>2025-04-03T11:29:44+00:00</updated>
    <author>
      <name>/u/retoor42</name>
      <uri>https://old.reddit.com/user/retoor42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I made my own CLI vibe tool using C with support for:&lt;br /&gt; - ollama&lt;br /&gt; - anthropic claude&lt;br /&gt; - openai (default, my key with gpt3.5 limited is included, works out of the box).&lt;/p&gt; &lt;p&gt;You make something like this in minutes: &lt;a href="https://molodetz.nl/project/streamii/README.md.html"&gt;https://molodetz.nl/project/streamii/README.md.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm using it for over a week now and it's a blazing useful tool. What ever c compile you have to compile, if you execute it in the CLI and it sees errors, it will fix everything instant for you! 20% of this tool is vibed by himself. It could generate the tool calls at a certain moment. &lt;/p&gt; &lt;p&gt;It's for linux only. &lt;/p&gt; &lt;p&gt;This is the project page: &lt;a href="https://molodetz.nl/project/r/README.md.html"&gt;https://molodetz.nl/project/r/README.md.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have not much experience with the Ollama version, since I do not have a beefii machine. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/retoor42"&gt; /u/retoor42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqfy9m/i_made_my_own_cli_vibe_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqfy9m/i_made_my_own_cli_vibe_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jqfy9m/i_made_my_own_cli_vibe_tool/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T11:29:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqgzmw</id>
    <title>Need Advice on API Key Management with Ollama &amp; Terms of Service</title>
    <updated>2025-04-03T12:22:10+00:00</updated>
    <author>
      <name>/u/Lazy-Dragonfly7825</name>
      <uri>https://old.reddit.com/user/Lazy-Dragonfly7825</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm setting up an internal API service in my college to provide students with access to Ollama while ensuring proper resource utilization and fair access for everyone. The system will issue API keys to track usage. I have a couple of questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;After authentication, my backend currently interacts with Ollama using the Ollama SDK. Is this the right approach for an internal setup, or should I make direct API calls instead?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;For terms and conditions, should I follow a structure similar to Ollama's model-related terms, or do I need a more detailed agreement outlining usage policies?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Would love to hear your thoughts and best practices! Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lazy-Dragonfly7825"&gt; /u/Lazy-Dragonfly7825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqgzmw/need_advice_on_api_key_management_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqgzmw/need_advice_on_api_key_management_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jqgzmw/need_advice_on_api_key_management_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T12:22:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jq85ks</id>
    <title>Just for fun the Playstation 2 gets in on some NLP Olamma Hybrid chat action</title>
    <updated>2025-04-03T03:28:08+00:00</updated>
    <author>
      <name>/u/doscore</name>
      <uri>https://old.reddit.com/user/doscore</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jq85ks/just_for_fun_the_playstation_2_gets_in_on_some/"&gt; &lt;img alt="Just for fun the Playstation 2 gets in on some NLP Olamma Hybrid chat action" src="https://preview.redd.it/32rqi9wwgjse1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12af3a5cf385862372c5a7229da55d3a0ecbe107" title="Just for fun the Playstation 2 gets in on some NLP Olamma Hybrid chat action" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I trained a really really small model on a dictionary and NLP for telling stories, it can also access my ollama setup via the network and store and use the context to write new and better stories.&lt;/p&gt; &lt;p&gt;this ps2 is running debian 6, 300mhz 32bm ram with a 40 gig seagate hdd.&lt;/p&gt; &lt;p&gt;it takes around 5 mins for it to generate a story and much quicker if you just use ollama obviously&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doscore"&gt; /u/doscore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/32rqi9wwgjse1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jq85ks/just_for_fun_the_playstation_2_gets_in_on_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jq85ks/just_for_fun_the_playstation_2_gets_in_on_some/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T03:28:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqajhl</id>
    <title>DocuMind (RAG app using Ollama)</title>
    <updated>2025-04-03T05:42:50+00:00</updated>
    <author>
      <name>/u/harry0027</name>
      <uri>https://old.reddit.com/user/harry0027</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jqajhl/documind_rag_app_using_ollama/"&gt; &lt;img alt="DocuMind (RAG app using Ollama)" src="https://external-preview.redd.it/dh-_Wu3nlQIF6oJL5CtgqrVT_37A3bEVuNY-wVXwauM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75ddf988c05b0a6a647e7b1644b7a61cebb09aaf" title="DocuMind (RAG app using Ollama)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m excited to share &lt;a href="https://github.com/Harry-027/DocuMind"&gt;DocuMind&lt;/a&gt;, a RAG (Retrieval-Augmented Generation) desktop app I built to make document management smarter and more efficient. It uses Ollama at backend to connect with LLMs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Harry-027/DocuMind"&gt;Github: DocuMind&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With DocuMind, you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🔎 Quickly search and retrieve relevant information from large pdf files.&lt;/li&gt; &lt;li&gt;🔄 Generate insightful answers using AI based on the context.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Building this app was an incredible experience, and it deepened my understanding of retrieval-augmented generation and AI-powered solutions.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1jqajhl/video/iqv2xswxhkse1/player"&gt;Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;#AI #RAG #Ollama #Rust #Tauri #Axum #QdrantDB&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/harry0027"&gt; /u/harry0027 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqajhl/documind_rag_app_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jqajhl/documind_rag_app_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jqajhl/documind_rag_app_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-03T05:42:50+00:00</published>
  </entry>
</feed>
