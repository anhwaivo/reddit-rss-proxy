<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-04T17:28:13+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mg3elr</id>
    <title>mcp:swap ollama run w/ codex exec</title>
    <updated>2025-08-02T22:40:11+00:00</updated>
    <author>
      <name>/u/neurostream</name>
      <uri>https://old.reddit.com/user/neurostream</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR : &amp;quot;ollama pull/serve&amp;quot; supports MCP with tool models, but &amp;quot;ollama run&amp;quot; can't use them - so i replaced &amp;quot;ollama run&amp;quot; with &amp;quot;codex exec&amp;quot; (works with local airgapped ollama) in my bash scripts.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;i'm new to LLMs and not an AI dev, but i hella script in bash - so it was neat to find that i could &amp;quot;ollama run&amp;quot; in shell loops to pipe its stdout back into stdin and fun stuff like that.&lt;/p&gt; &lt;p&gt;but as mcp emerged, &amp;quot;ollama run&amp;quot; still client can't speak Model Context Protocol, even though models you can &amp;quot;ollama pull/serve&amp;quot; are tagged with &amp;quot;tools&amp;quot;, &amp;quot;vision&amp;quot;, &amp;quot;thinking&amp;quot;, etc.&lt;/p&gt; &lt;p&gt;so my local bash scripts using &amp;quot;ollama run&amp;quot; never get to benefit from the tool calls those models are dying to make. scripting it with curl and jq works, but it's a pain.&lt;/p&gt; &lt;p&gt;keep &amp;quot;ollama serve&amp;quot;, swap the client: the openai codex cli (github.com/openai/codex) &lt;em&gt;does&lt;/em&gt; speak tools/MCP. you can point it to your &amp;quot;ollama serve&amp;quot; address and no api keys/accounts needed, nor do external network calls to openai happen. invoking &amp;quot;codex exec&amp;quot;, suddenly the tool side-channel to &amp;quot;ollama serve&amp;quot; light up:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;the model now emits json tool requests&lt;/li&gt; &lt;li&gt;codex executes them, sending results back to the model&lt;/li&gt; &lt;li&gt;you get the final answer once the tool loop is done&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;unlike &amp;quot;ollama run&amp;quot;, &amp;quot;codex exec&amp;quot; accepts an additional option listing the mcp commands in your PATH that you want to let the LLM run on your local through that json side channel (which you can watch on stderr). It holds the main chat stream open on stdout waiting for the final response to be written out.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;What other ollama cli clients do MCP?&lt;/p&gt; &lt;p&gt;When will new ollama versions allow &amp;quot;ollama run&amp;quot; to do mcp stuff?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neurostream"&gt; /u/neurostream &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mg3elr/mcpswap_ollama_run_w_codex_exec/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mg3elr/mcpswap_ollama_run_w_codex_exec/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mg3elr/mcpswap_ollama_run_w_codex_exec/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T22:40:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfoh8g</id>
    <title>Running Qwen3-Coder 30B at Full 256K Context: 25 tok/s with 96GB RAM + RTX 5080</title>
    <updated>2025-08-02T11:41:54+00:00</updated>
    <author>
      <name>/u/ajmusic15</name>
      <uri>https://old.reddit.com/user/ajmusic15</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mfoh8g/running_qwen3coder_30b_at_full_256k_context_25/"&gt; &lt;img alt="Running Qwen3-Coder 30B at Full 256K Context: 25 tok/s with 96GB RAM + RTX 5080" src="https://b.thumbs.redditmedia.com/sEwV4Z51XK64PlaKqqwGitLFVrGJ76H1E4yW6sAlukc.jpg" title="Running Qwen3-Coder 30B at Full 256K Context: 25 tok/s with 96GB RAM + RTX 5080" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I come to share with you my happiness running Qwen3-Coder 30B at its maximum unstretched context (256K).&lt;/p&gt; &lt;p&gt;To take full advantage of my processor cache without introducing additional latencies I'm using the LM Studio with 12 cores repartitioner equally between the two CCDs (6 CCD1 + 6 CCD2) using the affinity control of the task manager. I have noticed that using an unbalanced amount of cores between both CCD's decreases the amount of tokens per second but also using all cores.&lt;/p&gt; &lt;p&gt;As you can see, in order to run Qwen3-Coder 30B on my 96 GB RAM + 16 GB VRAM (5080) hardware I have had to load the whole model in Q3_K_M on the GPU but I have offloaded the context to the CPU, that makes the GPU just to do the inference to the model while the CPU is in charge of handling the context.&lt;/p&gt; &lt;p&gt;This way I could run Qwen3-Coder 30B at its 256K of context at ~25tk/s.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xcyh8vwr9lgf1.png?width=568&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75d6998987c43ce163e8c9611a53507fe244f8fb"&gt;https://preview.redd.it/xcyh8vwr9lgf1.png?width=568&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75d6998987c43ce163e8c9611a53507fe244f8fb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r2wesuyt9lgf1.png?width=1757&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7668da251043470250287d49abceea604130dee"&gt;https://preview.redd.it/r2wesuyt9lgf1.png?width=1757&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7668da251043470250287d49abceea604130dee&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajmusic15"&gt; /u/ajmusic15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfoh8g/running_qwen3coder_30b_at_full_256k_context_25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfoh8g/running_qwen3coder_30b_at_full_256k_context_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mfoh8g/running_qwen3coder_30b_at_full_256k_context_25/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T11:41:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mggmmj</id>
    <title>Â¿XBai-04 Es Real?</title>
    <updated>2025-08-03T11:10:23+00:00</updated>
    <author>
      <name>/u/Ordinary_Mud7430</name>
      <uri>https://old.reddit.com/user/Ordinary_Mud7430</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ordinary_Mud7430"&gt; /u/Ordinary_Mud7430 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mggku0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mggmmj/xbai04_es_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mggmmj/xbai04_es_real/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-03T11:10:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mggh28</id>
    <title>I made a opensource CAL-AI alternative using ollama which runs completely locally and for is fully free.</title>
    <updated>2025-08-03T11:01:18+00:00</updated>
    <author>
      <name>/u/mehmetflix_</name>
      <uri>https://old.reddit.com/user/mehmetflix_</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehmetflix_"&gt; /u/mehmetflix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1mfrec0/i_made_a_opensource_calai_alternative_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mggh28/i_made_a_opensource_calai_alternative_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mggh28/i_made_a_opensource_calai_alternative_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-03T11:01:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mggx5w</id>
    <title>Cursor Agent System Prompt Leaked- Ollama natively works with cursor - just need ngrok</title>
    <updated>2025-08-03T11:28:01+00:00</updated>
    <author>
      <name>/u/Evening_Weight_4234</name>
      <uri>https://old.reddit.com/user/Evening_Weight_4234</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Evening_Weight_4234"&gt; /u/Evening_Weight_4234 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1mggetn/just_set_up_a_powerful_local_ai_development/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mggx5w/cursor_agent_system_prompt_leaked_ollama_natively/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mggx5w/cursor_agent_system_prompt_leaked_ollama_natively/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-03T11:28:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgcfbn</id>
    <title>Basic 9060XT 16G on Ryzen - what size models should I be running for 'best bang for buck' results?</title>
    <updated>2025-08-03T06:38:14+00:00</updated>
    <author>
      <name>/u/NoButterscotch8359</name>
      <uri>https://old.reddit.com/user/NoButterscotch8359</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basic 9060XT 16G on Ryzen system - Running local LLMs on Ollama. What size models should I be running for best bang for buck results? &lt;/p&gt; &lt;p&gt;So many 'what should I run ...' and 'recommend me a ... ' threads out there.. Thought I would ad to it all. &lt;/p&gt; &lt;p&gt;Newb, and, basic specs: 128G DDR5, Ryzen 7700, 9060XT 16G on gigabyte X870E.&lt;/p&gt; &lt;p&gt;My 'research' tells me to use the biggest model I can fit on my 16G GPU, that being about 15G or maybe 16G model, but after experimenting with QWEN, magistral, Deepseek maxed at 15 &amp;amp; 16G models etc, I almost feel I'm getting better results from the 6-8G version of the same models. Accessing them all with Ollama on Fedora 42 Linux via bash. Using radeontool and ollama ps tells me I'm using my system to 'good capacity'. &lt;/p&gt; &lt;p&gt;TBH, I'm new at this, been lurking for weeks, and its a hell of a learning curve and now hit 'analysis paralysis'. My gut tells me I need to run bigger models and that would mean buying another GPU - thinking another 9060XT 16G and run it bifurcated off the one pcie (pcie 5.0). Its a great excuse to spend money I don't have and chalk it up to visa and whilst I'd rather not do that the itch to spend money on tech is ever present. &lt;/p&gt; &lt;p&gt;Using LLM for basic legal work and soon pinescripts in TradingView so its nothing too 'heavy'. &lt;/p&gt; &lt;p&gt;There is lots of 'A.I. created tutorials' on 'how to use A.I.' and I'm getting sick of it. Need a humans perspective. Suggestions..? &lt;/p&gt; &lt;p&gt;Is there anyone who has bifurcated a pcie 5.0 to run two gpus off the 'one slot'..? The x870e should have no problem doing it re the pcie 5.0 bandwidth, its just he logistics of doing so and, if I do, the 32G of vram is a hell of a lot better than 16G. Am I going to see massively different results by outlaying for another 16G gpu? Is it worth it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoButterscotch8359"&gt; /u/NoButterscotch8359 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mgcfbn/basic_9060xt_16g_on_ryzen_what_size_models_should/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mgcfbn/basic_9060xt_16g_on_ryzen_what_size_models_should/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mgcfbn/basic_9060xt_16g_on_ryzen_what_size_models_should/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-03T06:38:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg02kc</id>
    <title>"Private ChatGPT conversations show up on Search Engine, leaving internet users shocked again"</title>
    <updated>2025-08-02T20:12:58+00:00</updated>
    <author>
      <name>/u/cashout__103</name>
      <uri>https://old.reddit.com/user/cashout__103</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few days back, I saw a post explaining that if you search this on Google,&lt;/p&gt; &lt;p&gt;&lt;strong&gt;site:chatgpt.com/share intext:&amp;quot;keyword you want to find in the conversation&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It would show you a lot of ChatGPT shared chats that people were not aware were available to the public so easily.&lt;/p&gt; &lt;p&gt;And fortunately, it got patched,&lt;/p&gt; &lt;p&gt;at least that's what I thought until I found out&lt;/p&gt; &lt;p&gt;&lt;strong&gt;it was still working on the Brave search engine&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cashout__103"&gt; /u/cashout__103 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mg02kc/private_chatgpt_conversations_show_up_on_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mg02kc/private_chatgpt_conversations_show_up_on_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mg02kc/private_chatgpt_conversations_show_up_on_search/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T20:12:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mggbf0</id>
    <title>Looking for GPU</title>
    <updated>2025-08-03T10:51:48+00:00</updated>
    <author>
      <name>/u/iShane94</name>
      <uri>https://old.reddit.com/user/iShane94</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. Recently I also got into the AI train and most I want to replace ChatGPT and Gemini. However Iâm currently confused and looking for advice and or a guide to the right way/path!&lt;/p&gt; &lt;p&gt;Iâm planning to use my existing 24/7 server (amd epyc/ 256gb exc memory, etc.) I have a plan of splitting one X16 to x8x8 using bifurcation and use these lines to connect : &lt;/p&gt; &lt;p&gt;Option one : 2x RTX 3060 12gb Option two : 2x Radeon Mi GPU Option three : one of option 1 or 2.&lt;/p&gt; &lt;p&gt;Do ollama supports multiple gpu for single client? What models can I run on either of those configs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iShane94"&gt; /u/iShane94 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mggbf0/looking_for_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mggbf0/looking_for_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mggbf0/looking_for_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-03T10:51:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfqqxp</id>
    <title>Ollamacode - Local AI assistant that can create, run and understand the task at hand!</title>
    <updated>2025-08-02T13:37:23+00:00</updated>
    <author>
      <name>/u/Loud-Consideration-2</name>
      <uri>https://old.reddit.com/user/Loud-Consideration-2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mfqqxp/ollamacode_local_ai_assistant_that_can_create_run/"&gt; &lt;img alt="Ollamacode - Local AI assistant that can create, run and understand the task at hand!" src="https://preview.redd.it/yr8fwz5ezlgf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=93ef0e8b639c23f1c2a91ced55f0b129b49ba11e" title="Ollamacode - Local AI assistant that can create, run and understand the task at hand!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a project called OllamaCode, and I'd love to share it with you. It's an AI coding assistant that runs entirely locally with Ollama. The main idea was to create a tool that actually executes the code it writes, rather than just showing you blocks to copy and paste.&lt;/p&gt; &lt;p&gt;Here are a few things I've focused on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It can create and run files automatically from natural language.&lt;/li&gt; &lt;li&gt;I've tried to make it smart about executing tools like git, search, and bash commands.&lt;/li&gt; &lt;li&gt;It's designed to work with any Ollama model that supports function calling.&lt;/li&gt; &lt;li&gt;A big priority for me was to keep it 100% local to ensure privacy.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's still in the very early days, and there's a lot I still want to improve. It's been really helpful for my own workflow, and I would be incredibly grateful for any feedback from the community to help make it better.&lt;/p&gt; &lt;p&gt;Repo:&lt;a href="https://github.com/tooyipjee/ollamacode"&gt;https://github.com/tooyipjee/ollamacode&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loud-Consideration-2"&gt; /u/Loud-Consideration-2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yr8fwz5ezlgf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfqqxp/ollamacode_local_ai_assistant_that_can_create_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mfqqxp/ollamacode_local_ai_assistant_that_can_create_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T13:37:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgb7d2</id>
    <title>Open Source Status</title>
    <updated>2025-08-03T05:23:26+00:00</updated>
    <author>
      <name>/u/GhostInThePudding</name>
      <uri>https://old.reddit.com/user/GhostInThePudding</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, do we actually have any official word on what the deal is?&lt;/p&gt; &lt;p&gt;We have a new Windows/Mac GUI that is closed and there is no option on the download page to install the open source, non GUI having version. I can see the CLI versions are still accessible via Github, but is this a move to fully closing the project, or is the plan to open the GUI at some point?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GhostInThePudding"&gt; /u/GhostInThePudding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mgb7d2/open_source_status/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mgb7d2/open_source_status/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mgb7d2/open_source_status/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-03T05:23:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgaxv6</id>
    <title>Why is the new Ollama logo so angry?</title>
    <updated>2025-08-03T05:07:35+00:00</updated>
    <author>
      <name>/u/triynizzles1</name>
      <uri>https://old.reddit.com/user/triynizzles1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mgaxv6/why_is_the_new_ollama_logo_so_angry/"&gt; &lt;img alt="Why is the new Ollama logo so angry?" src="https://b.thumbs.redditmedia.com/ObBdDjQuE_WGOlhTrKP_GMas5HGVaPTke5ePURXfiro.jpg" title="Why is the new Ollama logo so angry?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I edited the eyebrows for friendliness :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/triynizzles1"&gt; /u/triynizzles1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mgaxv6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mgaxv6/why_is_the_new_ollama_logo_so_angry/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mgaxv6/why_is_the_new_ollama_logo_so_angry/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-03T05:07:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mh0nwg</id>
    <title>I built a coding agent routing solution via ollama - decoupling route selection from model assignment</title>
    <updated>2025-08-04T01:40:06+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mh0nwg/i_built_a_coding_agent_routing_solution_via/"&gt; &lt;img alt="I built a coding agent routing solution via ollama - decoupling route selection from model assignment" src="https://preview.redd.it/paoxewwxpwgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da18a520c59fa05247bac266249cd0623ff1e635" title="I built a coding agent routing solution via ollama - decoupling route selection from model assignment" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Coding tasks span from understanding and debugging code to writing and patching it, each with their unique objectives. While some workflows demand a foundational model for great performance, other workflows like &amp;quot;explain this function to me&amp;quot; require low-latency, cost-effective models that deliver a better user experience. In other words, I don't need to get coffee every time I prompt the coding agent.&lt;/p&gt; &lt;p&gt;This type of dynamic task understanding and model routing wasn't possible without incurring a heavy cost on first prompting a foundational model, which would incur ~2x the token cost and ~2x the latency (upper bound). So I designed an built a lightweight 1.5B autoregressive model that can run on ollama to decouple route selection from model assignment. This approach achieves latency as low as ~50ms, costs roughly 1/100th of engaging a large LLM for this routing task, and doesn't require expensive re-training all the time.&lt;/p&gt; &lt;p&gt;Full research paper can be found here: &lt;a href="https://arxiv.org/abs/2506.16655"&gt;https://arxiv.org/abs/2506.16655&lt;/a&gt;&lt;br /&gt; If you want to try it out, you can simply have your coding agent proxy requests via &lt;a href="https://github.com/katanemo/archgw"&gt;archgw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The router model isn't specific to coding - you can use it to define route policies like &amp;quot;image editing&amp;quot;, &amp;quot;creative writing&amp;quot;, etc but its roots and training have seen a lot of coding data. Try it out, would love the feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/paoxewwxpwgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mh0nwg/i_built_a_coding_agent_routing_solution_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mh0nwg/i_built_a_coding_agent_routing_solution_via/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-04T01:40:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgddnr</id>
    <title>Is the 60 dollar P102-100 still a viable option for LLM?</title>
    <updated>2025-08-03T07:39:09+00:00</updated>
    <author>
      <name>/u/Boricua-vet</name>
      <uri>https://old.reddit.com/user/Boricua-vet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mgddnr/is_the_60_dollar_p102100_still_a_viable_option/"&gt; &lt;img alt="Is the 60 dollar P102-100 still a viable option for LLM?" src="https://preview.redd.it/0198z1fz2rgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc68e14085a3e5b83fe5f3852ed06c7c8223c73c" title="Is the 60 dollar P102-100 still a viable option for LLM?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have seen thousands of posts of people asking what card to buy and there is two points of view. One is buy expensive 3090, or even more expensive 5000 series or, buy cheap and try it. This post will cover why the P102-100 is still relevant and why it is simply the best budget card to get at 60 dollars.&lt;/p&gt; &lt;p&gt;If you are just doing LLM, Vision and no image or video generation. This is hands down the best budget card to get all because of its memory bandwidth. This list covers entry level cards form all series. Yes I know there are better cards but I am comparing the P102-100 with all entry level cards only and those better cards are 10x more.This is for the budget build people. &lt;/p&gt; &lt;p&gt;2060 - 336.0 GB/s - $150 8GB&lt;br /&gt; 3060 - 360.0 GB/s - $200+ 8GB&lt;/p&gt; &lt;p&gt;4060 - 272.0 GB/s - $260+ 8GB&lt;/p&gt; &lt;p&gt;5060 - 448.0 GB/s - $350+ 8GB&lt;/p&gt; &lt;p&gt;P102-100 - 440.3 GB/s - $60 10GB.&lt;/p&gt; &lt;p&gt;Is the P102-100 faster than an &lt;/p&gt; &lt;p&gt;entry 2060 = yes&lt;/p&gt; &lt;p&gt;entry 3060 = yes&lt;/p&gt; &lt;p&gt;entry 4060 = yes.&lt;/p&gt; &lt;p&gt;only a 5060 would be faster and not by much. &lt;/p&gt; &lt;p&gt;Does the P102-100 load slower, yes it takes about 1 second per GB on the model. PCie 1x4 =1GB/s but once the model is leaded it will be normal with no delays on all your queries.&lt;/p&gt; &lt;p&gt;I have attached screenshots of a bunch of models, all with 32K context so you can see what to expect. Compare those results with other entry cards using the same 32K context and you will for yourself. Make sure they are using 32K context as the P102-100 would also be faster with lower context.&lt;/p&gt; &lt;p&gt;so if you want to try LLM's and not go broke, the P102-100 is a solid card to try for 60 bucks. I have 2 of them and those results are using 2 cards so I have 20GB VRAM for 70 bucks at 35 each when I bought them. Now they would be 120 bucks. I am not sure if you can get 20GB VRAM for less than is as fast as this.&lt;/p&gt; &lt;p&gt;I hope this helps other people that have been afraid to try local private ai because of the costs. I hope this motivates you to at least try. It is just 60 bucks.&lt;/p&gt; &lt;p&gt;I will probably be updating this next week as I have a third card and I am moving up to 30GB. I should be able to run these models with higher context, 128k, 256k and even bigger models. I will post some updates for anyone interested.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Boricua-vet"&gt; /u/Boricua-vet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0198z1fz2rgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mgddnr/is_the_60_dollar_p102100_still_a_viable_option/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mgddnr/is_the_60_dollar_p102100_still_a_viable_option/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-03T07:39:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mh6zll</id>
    <title>Can I run GLM 4.5 Air on my M1 Max 64gb Unified Ram 1Tb SSD??</title>
    <updated>2025-08-04T07:28:52+00:00</updated>
    <author>
      <name>/u/Haunting_Stomach8967</name>
      <uri>https://old.reddit.com/user/Haunting_Stomach8967</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Haunting_Stomach8967"&gt; /u/Haunting_Stomach8967 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1mh6z4j/can_i_run_glm_45_air_on_my_m1_max_64gb_unified/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mh6zll/can_i_run_glm_45_air_on_my_m1_max_64gb_unified/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mh6zll/can_i_run_glm_45_air_on_my_m1_max_64gb_unified/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-04T07:28:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgfcwe</id>
    <title>My AI-powered NPCs teach sustainable farming with Gemma 3n â all local, no cloud, fully open source!</title>
    <updated>2025-08-03T09:50:52+00:00</updated>
    <author>
      <name>/u/Code-Forge-Temple</name>
      <uri>https://old.reddit.com/user/Code-Forge-Temple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ð Hey folks! I recently built an open-source 2D game using &lt;strong&gt;Godot 4.x&lt;/strong&gt; where &lt;strong&gt;NPCs are powered by a local LLM&lt;/strong&gt; â Google's new &lt;strong&gt;Gemma 3n&lt;/strong&gt; model running on &lt;strong&gt;Ollama&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;ð¯ The goal: create private, offline-first educational experiences â in this case, the NPCs &lt;strong&gt;teach sustainable farming and botany&lt;/strong&gt; through rich, Socratic-style dialogue.&lt;/p&gt; &lt;p&gt;ð¡ Itâs built for the &lt;a href="https://www.kaggle.com/competitions/google-gemma-3n-hackathon"&gt;Google Gemma 3n Hackathon&lt;/a&gt;, which focuses on building real-world solutions with on-device, multimodal AI.&lt;/p&gt; &lt;h2&gt;ð§ Tech stack:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Godot 4.x (C#)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt; to run Gemma 3n locally (on LAN or localhost)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom NPC component&lt;/strong&gt; for setting system prompts + model endpoint&lt;/li&gt; &lt;li&gt;No cloud APIs, no vendor lock-in â everything runs locally&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;ð Fully Open Source:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/code-forge-temple/local-llm-npc"&gt;https://github.com/code-forge-temple/local-llm-npc&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;ð¹ 2-minute demo video:&lt;/h2&gt; &lt;p&gt;ð &lt;a href="https://www.youtube.com/watch?v=kGyafSgyRWA"&gt;Watch here&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;ð Would love your feedback on: - Potential to extend to other educational domains after the hackathon - Opportunities to enhance accessibility, local education, and agriculture in future versions - Ideas for making the AI NPC system more modular and adaptable post-competition&lt;/p&gt; &lt;p&gt;Thanks for checking it out! ð§ ð±&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Code-Forge-Temple"&gt; /u/Code-Forge-Temple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mgfcwe/my_aipowered_npcs_teach_sustainable_farming_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mgfcwe/my_aipowered_npcs_teach_sustainable_farming_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mgfcwe/my_aipowered_npcs_teach_sustainable_farming_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-03T09:50:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgtz4a</id>
    <title>Best Ollama model for offline Agentic tool calling AI</title>
    <updated>2025-08-03T20:42:33+00:00</updated>
    <author>
      <name>/u/TheCarBun</name>
      <uri>https://old.reddit.com/user/TheCarBun</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys. I love how supportive everyone is in this sub. I need to use an offline model so I need a little advice. &lt;/p&gt; &lt;p&gt;I'm exploring Ollama and I want to use an offline model as an AI agent with tool calling capabilities. Which models would you suggest for a 16GB RAM, 11th Gen i7 and RTX 3050Ti laptop? &lt;/p&gt; &lt;p&gt;I don't want to stress my laptop much but I would love to be able to use an offline model. Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheCarBun"&gt; /u/TheCarBun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mgtz4a/best_ollama_model_for_offline_agentic_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mgtz4a/best_ollama_model_for_offline_agentic_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mgtz4a/best_ollama_model_for_offline_agentic_tool/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-03T20:42:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mh3z91</id>
    <title>Recommendations on RAG for tabular data</title>
    <updated>2025-08-04T04:27:13+00:00</updated>
    <author>
      <name>/u/velu4080</name>
      <uri>https://old.reddit.com/user/velu4080</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am trying to integrate a RAG that could help retrieve insights from numerical data from Postgres or MongoDB or Loki/Mimir via Trino. I have been experimenting on Vanna AI.&lt;/p&gt; &lt;p&gt;Pls share your thoughts or suggestions on alternatives or links that could help me proceed with additional testing or benchmarking.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/velu4080"&gt; /u/velu4080 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mh3z91/recommendations_on_rag_for_tabular_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mh3z91/recommendations_on_rag_for_tabular_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mh3z91/recommendations_on_rag_for_tabular_data/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-04T04:27:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhfd1n</id>
    <title>Build a Chatbot with Memory using Deepseek, LangGraph, and Streamlit</title>
    <updated>2025-08-04T14:43:45+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mhfd1n/build_a_chatbot_with_memory_using_deepseek/"&gt; &lt;img alt="Build a Chatbot with Memory using Deepseek, LangGraph, and Streamlit" src="https://external-preview.redd.it/SqpEVWEi4JUF04QCKQlqCfgE_PBHaGqZ1iQt55AVkRU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=905c53e36ac79d2b3b3444b5070c1a4c7a62c537" title="Build a Chatbot with Memory using Deepseek, LangGraph, and Streamlit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=OtKMxYphw98&amp;amp;list=PLp01ObP3udmq2quR-RfrX4zNut_t_kNot&amp;amp;index=24"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhfd1n/build_a_chatbot_with_memory_using_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mhfd1n/build_a_chatbot_with_memory_using_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-04T14:43:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhawp4</id>
    <title>Local database agent</title>
    <updated>2025-08-04T11:31:23+00:00</updated>
    <author>
      <name>/u/Whywhoo</name>
      <uri>https://old.reddit.com/user/Whywhoo</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whywhoo"&gt; /u/Whywhoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1mhaw4g/local_database_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhawp4/local_database_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mhawp4/local_database_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-04T11:31:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhgg65</id>
    <title>Weird issue with OLlama 0.10.1 - loaded model unloads and a different one loads automatically.</title>
    <updated>2025-08-04T15:24:18+00:00</updated>
    <author>
      <name>/u/Vivid-Competition-20</name>
      <uri>https://old.reddit.com/user/Vivid-Competition-20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Steps to reproduce (on my Windows 10 machine). Using the command line, âollama run gemma3:3b âkeepalive 1hâ. I use it to chat with some prompts. In another Windows Terminal I do âollama psâ. I see the Gemma model being used. Then I go d something else and come back. Do another âollama psâ and see a different model, say a IBM Granite model. It doesnât make a difference which models I run.&lt;/p&gt; &lt;p&gt;Anyone else who can confirm?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vivid-Competition-20"&gt; /u/Vivid-Competition-20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhgg65/weird_issue_with_ollama_0101_loaded_model_unloads/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhgg65/weird_issue_with_ollama_0101_loaded_model_unloads/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mhgg65/weird_issue_with_ollama_0101_loaded_model_unloads/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-04T15:24:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhjpe3</id>
    <title>qwen3-30b-a3b thinking vs non-thinking</title>
    <updated>2025-08-04T17:23:47+00:00</updated>
    <author>
      <name>/u/randygeneric</name>
      <uri>https://old.reddit.com/user/randygeneric</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mhjpe3/qwen330ba3b_thinking_vs_nonthinking/"&gt; &lt;img alt="qwen3-30b-a3b thinking vs non-thinking" src="https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd872c4c3958b52ad860a6db5ba53994da65552e" title="qwen3-30b-a3b thinking vs non-thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some users have argued in other threads that Qwen3-30B-A3B Thinking and Non-Thinking models are nearly identical. So, I summarized the info from their Hugging Face pages (links for both: Qwen3-30B-A3B-Thinking-2507 and Qwen3-30B-A3B-Instruct-2507). To me, the Thinking model actually shows clear advantages in reasoning, coding, and agentic abilities. The only area where the current Instruct (Non-Thinking) model matches or is slightly better is alignment.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wjxg579wc1hf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6d7f1ff7705966ee3ea388b3256fc18f794caf41"&gt;https://preview.redd.it/wjxg579wc1hf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6d7f1ff7705966ee3ea388b3256fc18f794caf41&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Did I miss a point / misinterprete some data?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randygeneric"&gt; /u/randygeneric &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhjpe3/qwen330ba3b_thinking_vs_nonthinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhjpe3/qwen330ba3b_thinking_vs_nonthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mhjpe3/qwen330ba3b_thinking_vs_nonthinking/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-04T17:23:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhdotn</id>
    <title>Ollama, PostgreSQL, and the DBeaver AI Assistant</title>
    <updated>2025-08-04T13:38:03+00:00</updated>
    <author>
      <name>/u/justintxdave</name>
      <uri>https://old.reddit.com/user/justintxdave</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://stokerpostgresql.blogspot.com/2025/07/postgresql-ollama-and-dbeaver-ai.html"&gt;https://stokerpostgresql.blogspot.com/2025/07/postgresql-ollama-and-dbeaver-ai.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/justintxdave"&gt; /u/justintxdave &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhdotn/ollama_postgresql_and_the_dbeaver_ai_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhdotn/ollama_postgresql_and_the_dbeaver_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mhdotn/ollama_postgresql_and_the_dbeaver_ai_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-04T13:38:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgyslw</id>
    <title>Qwen3 30B A3B 2507 series personal experience + Qwen Code doesn't work?</title>
    <updated>2025-08-04T00:09:38+00:00</updated>
    <author>
      <name>/u/Holiday_Purpose_3166</name>
      <uri>https://old.reddit.com/user/Holiday_Purpose_3166</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all. Been a while since I've used Reddit, but kept lurking for useful information, so I suppose I can offer some personal experience about the latest Qwen3 30B series. &lt;/p&gt; &lt;p&gt;I mainly build apps in Rust and I find open-source LLMs to be least proficient with it out-of-the-box. Using Context7 helps massively, but would eat context window (until now). &lt;/p&gt; &lt;p&gt;I've been currently working on full stack Rust financial project for the past 3 months, with over 10k lines of code. As a solo Dev, I needed some assistance to help push through some really hard parts. &lt;/p&gt; &lt;p&gt;Tried using Qwen3 32B and 30B (previous gen.), and none of them were very successful, until last Devstral update. Still... &lt;/p&gt; &lt;p&gt;Had to resort to using Gemini 2.5 Pro and Flash. &lt;/p&gt; &lt;p&gt;Despite using a custom RAG system to save me 90% of context, Qwen3 models were not up to it. &lt;/p&gt; &lt;p&gt;My daily drivers were Q4_K_M and highest I could go with 30B was about 40k context window on RTX 5090, via Ollama, stock. &lt;/p&gt; &lt;p&gt;After setting up unsloth's UDQ4_K_XL models (Coder+Instruct+Thinking), I couldn't believe how much better it was - better than Gemini 2.5 Flash. &lt;/p&gt; &lt;p&gt;I could spend around 1-4 million tokens to resolve some issues with the codebase with Gemini CLI, where Qwen3 30B Coder could solve in under 70k tokens. 80-90k if I mixed Thinking model for architect mode in Cline. &lt;/p&gt; &lt;p&gt;Learned recently to turn on Flash Attention, and prompt tested the quality output with KV Cache at Q8_0. The results were as just as good as FP16 - better in some cases, oddly. &lt;/p&gt; &lt;p&gt;I was able to push context window up to 250k with 30.5GB VRAM - leaving buffer for system resources. At FP16 it sits at 140k context window. I get about 139 tokens/s. &lt;/p&gt; &lt;p&gt;Wanted to try Qwen-code CLI but seems to be hanging by not using the tools, so Cline has been more useful, yet I see some cases people can't use Cline but Qwen3 30B Coder works? &lt;/p&gt; &lt;p&gt;Thanks for the attention. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Purpose_3166"&gt; /u/Holiday_Purpose_3166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mgyslw/qwen3_30b_a3b_2507_series_personal_experience/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mgyslw/qwen3_30b_a3b_2507_series_personal_experience/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mgyslw/qwen3_30b_a3b_2507_series_personal_experience/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-04T00:09:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhj1db</id>
    <title>Data Security and AI - Sharing Your PostgreSQL Database With Ollama</title>
    <updated>2025-08-04T17:00:11+00:00</updated>
    <author>
      <name>/u/justintxdave</name>
      <uri>https://old.reddit.com/user/justintxdave</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://stokerpostgresql.blogspot.com/2025/08/data-security-and-ai-sharing-your.html"&gt;https://stokerpostgresql.blogspot.com/2025/08/data-security-and-ai-sharing-your.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/justintxdave"&gt; /u/justintxdave &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhj1db/data_security_and_ai_sharing_your_postgresql/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mhj1db/data_security_and_ai_sharing_your_postgresql/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mhj1db/data_security_and_ai_sharing_your_postgresql/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-04T17:00:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mh9jac</id>
    <title>Is this the best value machine to run Local LLMs?</title>
    <updated>2025-08-04T10:14:19+00:00</updated>
    <author>
      <name>/u/optimism0007</name>
      <uri>https://old.reddit.com/user/optimism0007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mh9jac/is_this_the_best_value_machine_to_run_local_llms/"&gt; &lt;img alt="Is this the best value machine to run Local LLMs?" src="https://preview.redd.it/4m8omr1w9zgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc0a9517363a39bcaaf77f833193098a6e7ae1dd" title="Is this the best value machine to run Local LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/optimism0007"&gt; /u/optimism0007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4m8omr1w9zgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mh9jac/is_this_the_best_value_machine_to_run_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mh9jac/is_this_the_best_value_machine_to_run_local_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-04T10:14:19+00:00</published>
  </entry>
</feed>
