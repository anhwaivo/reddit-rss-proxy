<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-15T21:05:18+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ipgrcj</id>
    <title>How can I know the max value of `num_layers` of each LM in ollama?</title>
    <updated>2025-02-14T18:05:01+00:00</updated>
    <author>
      <name>/u/Responsible-Sky8889</name>
      <uri>https://old.reddit.com/user/Responsible-Sky8889</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like the know whats the architecture of each LM that I'm running locally using Ollama, so that I can get the most out of running it locally, by setting the parameter num_layers. However, I checked the official documentation of each model in ollama and I didnt manage to find it.&lt;/p&gt; &lt;p&gt;Is is something that can vary depending on the cuantization? Still, how can I know whats tha max value that each model supports?&lt;/p&gt; &lt;p&gt;Thanks a lot and sorry if this question is stupid for expertised programmers in ollama!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible-Sky8889"&gt; /u/Responsible-Sky8889 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipgrcj/how_can_i_know_the_max_value_of_num_layers_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipgrcj/how_can_i_know_the_max_value_of_num_layers_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipgrcj/how_can_i_know_the_max_value_of_num_layers_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T18:05:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip9nnn</id>
    <title>Ollama building problem</title>
    <updated>2025-02-14T12:38:28+00:00</updated>
    <author>
      <name>/u/AdhesivenessLatter57</name>
      <uri>https://old.reddit.com/user/AdhesivenessLatter57</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using ollama since starting, but earlier building from source code was easy. Since it moved to cmake system some times it builds with nvidia and some times not.&lt;/p&gt; &lt;p&gt;I am using following: cmake -B build cmake -- build build go build .&lt;/p&gt; &lt;p&gt;Cuda toolkit for nvidia is installed, and cmake build detects it.&lt;/p&gt; &lt;p&gt;But when running ollama it doesn't use nvidia gpu.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdhesivenessLatter57"&gt; /u/AdhesivenessLatter57 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip9nnn/ollama_building_problem/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip9nnn/ollama_building_problem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ip9nnn/ollama_building_problem/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T12:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipm76f</id>
    <title>What is the maximum value for OLLAMA_NUM_PARALLEL in Ollama?</title>
    <updated>2025-02-14T21:59:57+00:00</updated>
    <author>
      <name>/u/Responsible-Sky8889</name>
      <uri>https://old.reddit.com/user/Responsible-Sky8889</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm setting up Ollama and looking to optimize performance for handling multiple concurrent requests. I know that the OLLAMA_NUM_PARALLEL parameter controls the number of parallel inferences, but I haven‚Äôt found any clear documentation on its upper limit. ‚Ä¢ What is the maximum value that can be assigned to OLLAMA_NUM_PARALLEL? ‚Ä¢ Is it hardware-dependent, or does Ollama have an internal cap? ‚Ä¢ Has anyone experimented with increasing it, and what impact did it have on performance?&lt;/p&gt; &lt;p&gt;I‚Äôd like to build a personal server for a side project that allows multiple inferences at once. &lt;/p&gt; &lt;p&gt;In the case that this parameter has some limitations? Can it be done using some other software?&lt;/p&gt; &lt;p&gt;Any insights or recommendations would be greatly appreciated! Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible-Sky8889"&gt; /u/Responsible-Sky8889 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipm76f/what_is_the_maximum_value_for_ollama_num_parallel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipm76f/what_is_the_maximum_value_for_ollama_num_parallel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipm76f/what_is_the_maximum_value_for_ollama_num_parallel/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T21:59:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioscui</id>
    <title>This is pure genius! Thank you!</title>
    <updated>2025-02-13T20:13:52+00:00</updated>
    <author>
      <name>/u/Apprehensive_Row9873</name>
      <uri>https://old.reddit.com/user/Apprehensive_Row9873</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all. I'm new here, I'm a french engineer. I was searching for a solution to self-host Mistral for days and couldn‚Äôt find the right way to do it correctly with Python and llama.cpp. I just couldn‚Äôt manage to offload the model to the GPU without CUDA errors. After lots of digging, I discovered vLLM and then Ollama. Just want to say THANK YOU! üôå This program works flawlessly from scratch on Docker üê≥, and I‚Äôll now implement it to auto-start Mistral and run directly in memory üß†‚ö°. This is incredible, huge thanks to the devs! üöÄüî•&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apprehensive_Row9873"&gt; /u/Apprehensive_Row9873 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioscui/this_is_pure_genius_thank_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioscui/this_is_pure_genius_thank_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ioscui/this_is_pure_genius_thank_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T20:13:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipnx82</id>
    <title>Best Metrics for Evaluating Locally Running Models</title>
    <updated>2025-02-14T23:18:43+00:00</updated>
    <author>
      <name>/u/thegauravverma</name>
      <uri>https://old.reddit.com/user/thegauravverma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to run multiple models locally to find the best one for my RAG-based app in terms of speed and efficiency. What key metrics should I focus on? I've been looking at eval rate, prompt eval rate (which seems to keep increasing with context), load duration, etc. Any other important factors to consider?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thegauravverma"&gt; /u/thegauravverma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipnx82/best_metrics_for_evaluating_locally_running_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipnx82/best_metrics_for_evaluating_locally_running_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipnx82/best_metrics_for_evaluating_locally_running_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T23:18:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipsa8l</id>
    <title>Uploading custom source book to ollama...?</title>
    <updated>2025-02-15T03:09:45+00:00</updated>
    <author>
      <name>/u/Croestalker</name>
      <uri>https://old.reddit.com/user/Croestalker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For my campaign I've made a custom source book, but would like to have llama create quests and additional NPC's etc. what's the best way to have it retain info (give it a memory?) and feed my source book and it's updates?&lt;/p&gt; &lt;p&gt;Also, what model do you suggest? I only have a 1080 (8gb VRAM) with 32 GB Ram. So huge models won't work obviously. :(&lt;/p&gt; &lt;p&gt;(Can't accept my first post with a typo so deleted and re posted, haha.) Edit: can't accept my two typos... Ugh.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Croestalker"&gt; /u/Croestalker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipsa8l/uploading_custom_source_book_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipsa8l/uploading_custom_source_book_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipsa8l/uploading_custom_source_book_to_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T03:09:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip9772</id>
    <title>Ollama in Docker reports 100% GPU, but runs on CPU instead</title>
    <updated>2025-02-14T12:10:28+00:00</updated>
    <author>
      <name>/u/AmphibianFrog</name>
      <uri>https://old.reddit.com/user/AmphibianFrog</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had everything running really nicely on a Debian linux server with 3 GPUs. I bought a new AMD Threadripper CPU and motherboard, reinstalled everything, and now I am getting weird behaviour.&lt;/p&gt; &lt;p&gt;I have everything running in docker. If I restart ollama, and then load up a model it will run in the GPU. I can see it working in nvtop and it's very fast.&lt;/p&gt; &lt;p&gt;However, the next time I try to run a model after some time has passed it runs completely in my CPU.&lt;/p&gt; &lt;p&gt;If I do &lt;code&gt;ollama ps&lt;/code&gt; I see the following:&lt;/p&gt; &lt;p&gt;&lt;code&gt; NAME ID SIZE PROCESSOR UNTIL mistral-small:22b-instruct-2409-q8_0 ebe30125ec3c 29 GB 100% GPU 29 minutes from now &lt;/code&gt;&lt;/p&gt; &lt;p&gt;But inference is really slow, my GPUs are at 0% VRAM usage and about half of my CPU cores go to 100%.&lt;/p&gt; &lt;p&gt;If I restart ollama it will work again for a while and then revert to this.&lt;/p&gt; &lt;p&gt;I can't even tell if this is a problem with docker or ollama. Has anyone seen this before and does anyone know how to fix it?&lt;/p&gt; &lt;p&gt;Here is my output to nvidia-smi:&lt;/p&gt; &lt;p&gt;``` Fri Feb 14 12:10:59 2025&lt;br /&gt; +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 535.216.01 Driver Version: 535.216.01 CUDA Version: 12.2 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+======================+======================| | 0 NVIDIA GeForce RTX 3090 On | 00000000:21:00.0 Off | N/A | | 0% 39C P8 23W / 370W | 3MiB / 24576MiB | 0% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ | 1 NVIDIA GeForce RTX 3060 On | 00000000:49:00.0 Off | N/A | | 0% 54C P8 17W / 170W | 3MiB / 12288MiB | 0% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ | 2 NVIDIA GeForce RTX 3070 Ti On | 00000000:4A:00.0 Off | N/A | | 0% 45C P8 17W / 290W | 3MiB / 8192MiB | 0% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+&lt;/p&gt; &lt;p&gt;+---------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=======================================================================================| | No running processes found | +---------------------------------------------------------------------------------------+ ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AmphibianFrog"&gt; /u/AmphibianFrog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip9772/ollama_in_docker_reports_100_gpu_but_runs_on_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip9772/ollama_in_docker_reports_100_gpu_but_runs_on_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ip9772/ollama_in_docker_reports_100_gpu_but_runs_on_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T12:10:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1iphebt</id>
    <title>Can LLM access git repo ?</title>
    <updated>2025-02-14T18:32:07+00:00</updated>
    <author>
      <name>/u/Mundane-Tree-9336</name>
      <uri>https://old.reddit.com/user/Mundane-Tree-9336</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I would like to setup a local LLM using Ollama to get some help for a specific project. I was wondering if it's possible to give the LLM access to a private git repo ?&lt;/p&gt; &lt;p&gt;I'm working on a large project with a team, and I'd like to be able to ask question about this specific project (i.e. what are the dependencies of that file?, where is this function called?) , or get some help coding (i.e. Optimize that function, and it will optimize the function but also the function called inside of it, that might be in different files, or refactor that code, allowing it to move some function to different files, etc...)&lt;/p&gt; &lt;p&gt;Although all of these features might not be possible, I'd still like to connect the LLM to a code base (git repo, or local code).&lt;/p&gt; &lt;p&gt;Is this possible ?&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;p&gt;Edit: So far, my solution is to run ollama on a local server, and connect PyCharm to it using Continue plugin. So far, this works. It seems openweb-ui also has a RAG included (&lt;a href="https://docs.openwebui.com/tutorials/tips/rag-tutorial"&gt;https://docs.openwebui.com/tutorials/tips/rag-tutorial&lt;/a&gt;), so I might look into that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mundane-Tree-9336"&gt; /u/Mundane-Tree-9336 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iphebt/can_llm_access_git_repo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iphebt/can_llm_access_git_repo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iphebt/can_llm_access_git_repo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T18:32:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipmdu3</id>
    <title>Hardware question: Does anyone know what the AMD AI accelerator cores do ?</title>
    <updated>2025-02-14T22:07:50+00:00</updated>
    <author>
      <name>/u/Living-Cheek-2273</name>
      <uri>https://old.reddit.com/user/Living-Cheek-2273</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to upgrade my gtx 970 to be actually able to run AI and my options are:&lt;/p&gt; &lt;p&gt;-the 6700xt 12gb of vram (250‚Ç¨) faster and easy to get &lt;/p&gt; &lt;p&gt;-the 7600xt 16gb of vram (300‚Ç¨) hard to find but gets the AI accelerator cores and more vram&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So I guess I want to find out if the extra vram and the AI cores are worth it for AI&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Living-Cheek-2273"&gt; /u/Living-Cheek-2273 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipmdu3/hardware_question_does_anyone_know_what_the_amd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipmdu3/hardware_question_does_anyone_know_what_the_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipmdu3/hardware_question_does_anyone_know_what_the_amd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T22:07:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipekhy</id>
    <title>x2 RTX 3060 12GB VRAM</title>
    <updated>2025-02-14T16:32:47+00:00</updated>
    <author>
      <name>/u/VariousGrand</name>
      <uri>https://old.reddit.com/user/VariousGrand</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you think that having two RTX 360 with 12Gb VRAM each is enough to run deepseek-r1 32b?&lt;/p&gt; &lt;p&gt;Or there any other option you think it will have better performance?&lt;/p&gt; &lt;p&gt;Would be better maybe to have Titan RTX with 24gb of vram? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VariousGrand"&gt; /u/VariousGrand &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipekhy/x2_rtx_3060_12gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipekhy/x2_rtx_3060_12gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipekhy/x2_rtx_3060_12gb_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T16:32:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipmpr0</id>
    <title>What's the best LLM I can run with at least 10 t/s on 24 cores, 215GB ram &amp; 8GB vram?</title>
    <updated>2025-02-14T22:22:15+00:00</updated>
    <author>
      <name>/u/MarinatedPickachu</name>
      <uri>https://old.reddit.com/user/MarinatedPickachu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an older workstation with plenty of ram and 2x12 2.9ghz cores, but only an rtx 2070 super. The memory is afaik also divided into two numa-nodes, not sure how this would affect LLM performance. Is there anything interesting I could run on this at reasonable speed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarinatedPickachu"&gt; /u/MarinatedPickachu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipmpr0/whats_the_best_llm_i_can_run_with_at_least_10_ts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipmpr0/whats_the_best_llm_i_can_run_with_at_least_10_ts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipmpr0/whats_the_best_llm_i_can_run_with_at_least_10_ts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T22:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip99f5</id>
    <title>I built this GUI for Ollama, also have built-in knowledge base and note, hope you like it!</title>
    <updated>2025-02-14T12:14:12+00:00</updated>
    <author>
      <name>/u/w-zhong</name>
      <uri>https://old.reddit.com/user/w-zhong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ip99f5/i_built_this_gui_for_ollama_also_have_builtin/"&gt; &lt;img alt="I built this GUI for Ollama, also have built-in knowledge base and note, hope you like it!" src="https://preview.redd.it/242otrvdi4ie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7cac77d270c8ee5ddc11b42dfa6096d5d230fb61" title="I built this GUI for Ollama, also have built-in knowledge base and note, hope you like it!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w-zhong"&gt; /u/w-zhong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/242otrvdi4ie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ip99f5/i_built_this_gui_for_ollama_also_have_builtin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ip99f5/i_built_this_gui_for_ollama_also_have_builtin/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T12:14:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipla7v</id>
    <title>Promptable Video Redaction: Use Moondream to redact content with a prompt (open source)</title>
    <updated>2025-02-14T21:19:00+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ipla7v/promptable_video_redaction_use_moondream_to/"&gt; &lt;img alt="Promptable Video Redaction: Use Moondream to redact content with a prompt (open source)" src="https://external-preview.redd.it/Zng2d3BhbWc4NmplMZN2WL68RoAkfEFkGlg6y4sh7yXh5lDDNxO3LBLK1287.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=599aed209532bc7ea0e78aa6c93c55dd068b89e9" title="Promptable Video Redaction: Use Moondream to redact content with a prompt (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/djtn6gmg86je1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipla7v/promptable_video_redaction_use_moondream_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipla7v/promptable_video_redaction_use_moondream_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T21:19:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq2ehq</id>
    <title>Has anyone run parallel 2B-4B LLMs fine-tuned with Axolotl/Ollama?</title>
    <updated>2025-02-15T14:24:17+00:00</updated>
    <author>
      <name>/u/Every_Gold4726</name>
      <uri>https://old.reddit.com/user/Every_Gold4726</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, new to this subreddit, and first time posting.&lt;/p&gt; &lt;p&gt;Curious if anyone has managed to run two small models (2-4B parameters) in parallel, each fine-tuned for different tasks? Looking at using Axolotl for fine-tuning and running them through Ollama/Docker.&lt;/p&gt; &lt;p&gt;The idea is to have both models running simultaneously, each handling their own specific task. Models like Phi-2, TinyLlama-2B, or similar size ranges.&lt;/p&gt; &lt;p&gt;Has anyone tried this setup? Would love to hear if it worked for you!&lt;/p&gt; &lt;p&gt;Thanks!‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Every_Gold4726"&gt; /u/Every_Gold4726 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq2ehq/has_anyone_run_parallel_2b4b_llms_finetuned_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq2ehq/has_anyone_run_parallel_2b4b_llms_finetuned_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iq2ehq/has_anyone_run_parallel_2b4b_llms_finetuned_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T14:24:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq3zcy</id>
    <title>Got a question about creating a system prompt in modelfile</title>
    <updated>2025-02-15T15:39:44+00:00</updated>
    <author>
      <name>/u/Ardion63</name>
      <uri>https://old.reddit.com/user/Ardion63</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, got a question that i have been trying to figure out on my own but im stuck .&lt;/p&gt; &lt;p&gt;I have been trying 1B-7B models which works good, (great)&lt;br /&gt; but i wanted to create something specific for myself &lt;/p&gt; &lt;p&gt;I wanted to make a : chatbot style but still a Chatgpt style but with a character feel to it &lt;/p&gt; &lt;p&gt;but im not sure about a few stuff&lt;br /&gt; 1) how long can the prompt be? since i worry a long loading time&lt;br /&gt; 2) how to make sure the AI stays in character?&lt;br /&gt; 3) do i need to do anything other then the system prompt? maybe the Template as well?&lt;br /&gt; 4) should i go for Uncensored? i dont usually do nsfw stories or whatever , i just want the AI to be able to emulate human speech (yea, i mean no over friendly / therapist feel lol)&lt;/p&gt; &lt;p&gt;Yes&amp;lt; im doing this cause right now im going a though time and i need some thing to put my focus on (this AI stuff basically)&lt;/p&gt; &lt;p&gt;If anyone got any idea that would be great, and&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ardion63"&gt; /u/Ardion63 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq3zcy/got_a_question_about_creating_a_system_prompt_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq3zcy/got_a_question_about_creating_a_system_prompt_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iq3zcy/got_a_question_about_creating_a_system_prompt_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T15:39:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipeutl</id>
    <title>I created a free, open source Web extension to run Ollama</title>
    <updated>2025-02-14T16:45:21+00:00</updated>
    <author>
      <name>/u/gerpann</name>
      <uri>https://old.reddit.com/user/gerpann</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey fellow developers! üëã I'm excited to introduce &lt;strong&gt;Ollamazing&lt;/strong&gt;, a browser extension that brings the power of local AI models directly into your browsing experience. Let me share why you might want to give it a try.&lt;/p&gt; &lt;h1&gt;What is Ollamazing?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Ollamazing&lt;/strong&gt; is a free, open-source browser extension that connects with &lt;strong&gt;Ollama&lt;/strong&gt; to run AI models locally on your machine. Think of it as having ChatGPT-like (or even Deepseek for newer) capabilities, but with complete privacy and no subscription fees.&lt;/p&gt; &lt;h1&gt;üåü Key Features&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;100% Free and Open Source &lt;ul&gt; &lt;li&gt;No hidden costs or subscription fees&lt;/li&gt; &lt;li&gt;Fully open-source codebase&lt;/li&gt; &lt;li&gt;Community-driven development&lt;/li&gt; &lt;li&gt;Transparent about how your data is handled&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Local AI Processing &lt;ul&gt; &lt;li&gt;Thanks to Ollama, we can run AI models directly on your machine&lt;/li&gt; &lt;li&gt;Complete privacy - your data never leaves your computer&lt;/li&gt; &lt;li&gt;Works offline once models are downloaded&lt;/li&gt; &lt;li&gt;Support for various open-source models (&lt;em&gt;llama3.3&lt;/em&gt;, &lt;em&gt;gemma&lt;/em&gt;, &lt;em&gt;phi4&lt;/em&gt;, &lt;em&gt;qwen&lt;/em&gt;, &lt;em&gt;mistral&lt;/em&gt;, &lt;em&gt;codellama&lt;/em&gt;, etc.) and specially &lt;strong&gt;&lt;em&gt;deepseek-r1&lt;/em&gt;&lt;/strong&gt; - the most popular open source model at current time.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Seamless Browser Integration &lt;ul&gt; &lt;li&gt;Chat with AI right from your browser sidebar&lt;/li&gt; &lt;li&gt;Text selection support for quick queries&lt;/li&gt; &lt;li&gt;Context-aware responses based on the current webpage&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Developer-Friendly Features &lt;ul&gt; &lt;li&gt;Code completion and explanation&lt;/li&gt; &lt;li&gt;Documentation generation&lt;/li&gt; &lt;li&gt;Code review assistance&lt;/li&gt; &lt;li&gt;Bug fixing suggestions&lt;/li&gt; &lt;li&gt;Multiple programming language support&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Easy Setup &lt;ul&gt; &lt;li&gt;Install Ollama on your machine or any remote server&lt;/li&gt; &lt;li&gt;Download your preferred models&lt;/li&gt; &lt;li&gt;Install the Ollamazing browser extension&lt;/li&gt; &lt;li&gt;Start chatting with AI!&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;üöÄ Getting Started&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# 1. Install Ollama curl -fsSL https://ollama.com/install.sh | sh # 2. Pull your first model (e.g., Deepseek R1 7 billion parameters) ollama pull deepseek-r1:7b &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then simply install the extension from your browser's extension store, and you're ready to go!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;For more information about Ollama, please visit the &lt;a href="https://ollama.com/"&gt;official website&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: If you run Ollama on local machine, ensure to setup the &lt;code&gt;OLLAMA_ORIGINS&lt;/code&gt; to allow the extension can connect to the server. For more details, read &lt;a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-configure-ollama-server"&gt;Ollama FAQ&lt;/a&gt;, set the &lt;code&gt;OLLAMA_ORIGINS&lt;/code&gt; to &lt;code&gt;*&lt;/code&gt; or &lt;code&gt;chrome-extension://*&lt;/code&gt; or the domain you want to allow.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;üí° Use Cases&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Code completion and explanation&lt;/li&gt; &lt;li&gt;Documentation generation&lt;/li&gt; &lt;li&gt;Code review assistance&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üîí Privacy First&lt;/h1&gt; &lt;p&gt;Unlike cloud-based AI assistants, Ollamazing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Keeps your data on your machine&lt;/li&gt; &lt;li&gt;Doesn't require an internet connection for inference&lt;/li&gt; &lt;li&gt;Gives you full control over which model to use&lt;/li&gt; &lt;li&gt;Allows you to audit the code and know exactly what's happening with your data&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üõ†Ô∏è Technical Stack&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Use framework &lt;a href="https://wxt.dev/"&gt;WXT&lt;/a&gt; to build the extension&lt;/li&gt; &lt;li&gt;Built with React and TypeScript&lt;/li&gt; &lt;li&gt;Uses Valtio for state management&lt;/li&gt; &lt;li&gt;Implements TanStack Query for efficient data fetching&lt;/li&gt; &lt;li&gt;Follows modern web extension best practices&lt;/li&gt; &lt;li&gt;Utilizes Shadcn/UI for a clean, modern interface&lt;/li&gt; &lt;li&gt;Use i18n for multi-language support&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;ü§ù Contributing&lt;/h1&gt; &lt;p&gt;We welcome contributions! Whether it's:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Adding new features&lt;/li&gt; &lt;li&gt;Improving documentation&lt;/li&gt; &lt;li&gt;Reporting bugs&lt;/li&gt; &lt;li&gt;Suggesting enhancements&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out our GitHub repository &lt;a href="https://github.com/buiducnhat/ollamazing"&gt;https://github.com/buiducnhat/ollamazing&lt;/a&gt; to get started!&lt;/p&gt; &lt;h1&gt;üîÆ Future Plans&lt;/h1&gt; &lt;p&gt;We're working on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Enhanced context awareness&lt;/li&gt; &lt;li&gt;Custom model fine-tuning support&lt;/li&gt; &lt;li&gt;Improve UI/UX&lt;/li&gt; &lt;li&gt;Improved performance optimizations&lt;/li&gt; &lt;li&gt;Additional browser support&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try It Today!&lt;/h1&gt; &lt;p&gt;Ready to experience local AI in your browser? Get started with Ollamazing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Chrome web store: &lt;a href="https://chromewebstore.google.com/detail/ollamazing/bfndpdpimcehljfgjdacbpapgbkecahi"&gt;https://chromewebstore.google.com/detail/ollamazing/bfndpdpimcehljfgjdacbpapgbkecahi&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub repository: &lt;a href="https://github.com/buiducnhat/ollamazing"&gt;https://github.com/buiducnhat/ollamazing&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Product Hunt: &lt;a href="https://www.producthunt.com/posts/ollamazing"&gt;https://www.producthunt.com/posts/ollamazing&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me know in the comments if you have any questions or feedback! Have you tried running AI models locally before? What features would you like to see in Ollamazing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gerpann"&gt; /u/gerpann &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipeutl/i_created_a_free_open_source_web_extension_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipeutl/i_created_a_free_open_source_web_extension_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipeutl/i_created_a_free_open_source_web_extension_to_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-14T16:45:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq9y68</id>
    <title>Downloading Deepseek671b from Ollama to anywhere except my C Drive</title>
    <updated>2025-02-15T20:03:43+00:00</updated>
    <author>
      <name>/u/0xBlackSwan</name>
      <uri>https://old.reddit.com/user/0xBlackSwan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I have the smaller DeepSeek models up to 32b downloaded and am using it great, but I wanted to download the 671b model just to have. The problem is this is almost half a terabyte and I do not have that much storage on my C Drive. I have plenty of space on my D drive and have several externals with plenty of space, but when I try to download on Ollama it automatically send it to my C drive. Is there a way around this? I have a PC and am running WIndows 11 if that info is necessary but I'm way more familiar with mac. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/0xBlackSwan"&gt; /u/0xBlackSwan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq9y68/downloading_deepseek671b_from_ollama_to_anywhere/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq9y68/downloading_deepseek671b_from_ollama_to_anywhere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iq9y68/downloading_deepseek671b_from_ollama_to_anywhere/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T20:03:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqa6eu</id>
    <title>Gpu acceleration without avx</title>
    <updated>2025-02-15T20:13:25+00:00</updated>
    <author>
      <name>/u/Falloutgamerlol</name>
      <uri>https://old.reddit.com/user/Falloutgamerlol</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an old hp compaq I Frankensteined into a gaming pc a while ago with a gpu, riser cable and external psu. It's got a heftyly overclocked evga gtx 970 innit. So It handles other ai tasks well like stable diffusion. My only problem with ollama tho is that the amd phenom 955 doesn't support avx and the newest cpu compatible with the mobo also doesn't support avx. So I'm just wondering if there is a bypass.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Falloutgamerlol"&gt; /u/Falloutgamerlol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqa6eu/gpu_acceleration_without_avx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqa6eu/gpu_acceleration_without_avx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqa6eu/gpu_acceleration_without_avx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T20:13:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq4051</id>
    <title>Best approach for using ollama with SQLite for a simple chatbot?</title>
    <updated>2025-02-15T15:40:42+00:00</updated>
    <author>
      <name>/u/KuuBoo</name>
      <uri>https://old.reddit.com/user/KuuBoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! &lt;/p&gt; &lt;p&gt;I'm new to this, but I want to use Ollama locally to chat with an SQLite database for a simple project management system. The idea is to ask things like:&lt;/p&gt; &lt;p&gt;‚ÄúHave all invoices for Project X been paid?‚Äù&lt;/p&gt; &lt;p&gt;‚ÄúShow me overdue projects.‚Äù&lt;/p&gt; &lt;p&gt;I guess that in the background Ollama should generate SQL, run it, and return the answer.&lt;/p&gt; &lt;p&gt;What‚Äôs the best way to set this up from scratch? Any advice on making it accurate?&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KuuBoo"&gt; /u/KuuBoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq4051/best_approach_for_using_ollama_with_sqlite_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq4051/best_approach_for_using_ollama_with_sqlite_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iq4051/best_approach_for_using_ollama_with_sqlite_for_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T15:40:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq4vkl</id>
    <title>Identifying hardware contributions to token generation speed.</title>
    <updated>2025-02-15T16:20:24+00:00</updated>
    <author>
      <name>/u/FrederikSchack</name>
      <uri>https://old.reddit.com/user/FrederikSchack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm trying to gather data related to token generation speed on a more wide range of hardware than just graphics card.&lt;/p&gt; &lt;p&gt;The data I have indicates that maybe GPU's running in connection with AMD processors run slower, but I need more data.&lt;/p&gt; &lt;p&gt;Please help me by making a small simple test on your Ollama described here:&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ip7zaz"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ip7zaz&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrederikSchack"&gt; /u/FrederikSchack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq4vkl/identifying_hardware_contributions_to_token/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq4vkl/identifying_hardware_contributions_to_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iq4vkl/identifying_hardware_contributions_to_token/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T16:20:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq733c</id>
    <title>Help Needed: LLaVA/BakLLaVA Image Tagging ‚Äì Too Many Hallucinations</title>
    <updated>2025-02-15T17:58:47+00:00</updated>
    <author>
      <name>/u/tumbling_pdx</name>
      <uri>https://old.reddit.com/user/tumbling_pdx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been experimenting with &lt;strong&gt;various open-source image-to-text models&lt;/strong&gt; via &lt;strong&gt;Ollama&lt;/strong&gt;, including &lt;strong&gt;LLaVA, LLaVA-phi3, and BakLLaVA&lt;/strong&gt;, to generate &lt;strong&gt;structured image tags&lt;/strong&gt; for my photography collection. However, I keep running into &lt;strong&gt;hallucinations and irrelevant tags&lt;/strong&gt;, and I'm hoping someone here has insight into improving this process.&lt;/p&gt; &lt;h1&gt;What My Code Does&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Loads configuration settings (Ollama endpoint, model, confidence threshold, max tags, etc.).&lt;/li&gt; &lt;li&gt;Supports &lt;strong&gt;JPEG, PNG, and RAW&lt;/strong&gt; images (NEF, DNG, CR2, etc.), converting RAW files to RGB if needed.&lt;/li&gt; &lt;li&gt;Resizes images before sending them to &lt;strong&gt;Ollama‚Äôs API&lt;/strong&gt; as a &lt;strong&gt;base64-encoded payload&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Uses a structured &lt;strong&gt;prompt&lt;/strong&gt; to request a &lt;strong&gt;caption&lt;/strong&gt; and at least &lt;strong&gt;20 relevant tags&lt;/strong&gt; per image.&lt;/li&gt; &lt;li&gt;Parses the API response, extracts keywords, assigns confidence scores, and filters out low-confidence tags.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Current Prompt:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Your task is to first generate a detailed description for the image. If a description is included with the image, use that one. Next, generate at least 20 unique Keywords for the image. Include: - Actions - Setting, location, and background - Items and structures - Colors and textures - Composition, framing - Photographic style - If there is one or more person: - Subjects - Physical appearance - Clothing - Gender - Age - Professions - Relationships between subjects and objects in the image. Provide one word per entry; if more than one word is required, split into two entries. Do not combine words. Generate ONLY a JSON object with the keys `Caption` and `Keywords` as follows: &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;The Issue&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Models often generate &lt;strong&gt;long descriptions&lt;/strong&gt; instead of structured &lt;strong&gt;one-word&lt;/strong&gt; tags.&lt;/li&gt; &lt;li&gt;Many tags are &lt;strong&gt;hallucinated&lt;/strong&gt; (e.g., objects or people that don‚Äôt exist in the image).&lt;/li&gt; &lt;li&gt;Some outputs contain &lt;strong&gt;redundant, vague, or overly poetic&lt;/strong&gt; descriptions instead of usable metadata.&lt;/li&gt; &lt;li&gt;I've tested &lt;strong&gt;multiple models (LLaVA, LLaVA-phi3, BakLLaVA, etc.)&lt;/strong&gt;, and all exhibit similar behavior.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I Need Help With&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Prompt optimization&lt;/strong&gt;: How can I make the instructions &lt;strong&gt;clearer&lt;/strong&gt; so models generate &lt;strong&gt;concise and accurate tags&lt;/strong&gt; instead of descriptions?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fine-tuning options&lt;/strong&gt;: Are there ways to &lt;strong&gt;reduce hallucinations&lt;/strong&gt; without manually filtering every output?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Better models for tagging&lt;/strong&gt;: Is there an &lt;strong&gt;open-source alternative&lt;/strong&gt; that works better for structured image metadata?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm happy to &lt;strong&gt;share my full code&lt;/strong&gt; if anyone is interested. Any help or suggestions would be greatly appreciated!&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tumbling_pdx"&gt; /u/tumbling_pdx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq733c/help_needed_llavabakllava_image_tagging_too_many/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq733c/help_needed_llavabakllava_image_tagging_too_many/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iq733c/help_needed_llavabakllava_image_tagging_too_many/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T17:58:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipw9og</id>
    <title>Is there a model that actually can examine and read all of a pdf?</title>
    <updated>2025-02-15T07:26:33+00:00</updated>
    <author>
      <name>/u/Aleilnonno</name>
      <uri>https://old.reddit.com/user/Aleilnonno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried Llama3.1 8b and Phi4 14b, but they just make a short summary and then they proceed to analyse. How can I solve?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aleilnonno"&gt; /u/Aleilnonno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipw9og/is_there_a_model_that_actually_can_examine_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ipw9og/is_there_a_model_that_actually_can_examine_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ipw9og/is_there_a_model_that_actually_can_examine_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T07:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq6054</id>
    <title>How do you decide which model to run?</title>
    <updated>2025-02-15T17:10:46+00:00</updated>
    <author>
      <name>/u/Serious-Mode</name>
      <uri>https://old.reddit.com/user/Serious-Mode</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have Ollama up and running, along with Open WebUI. Wanting to use the best general purpose model I can on my Windows 10 PC with an RTX 4060TI 16GB, but not quite sure how to make that decision. I believe with my card I can pull off 14b models? Is the deepseek-r1 model from the Ollama library the go to at the moment? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Serious-Mode"&gt; /u/Serious-Mode &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq6054/how_do_you_decide_which_model_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq6054/how_do_you_decide_which_model_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iq6054/how_do_you_decide_which_model_to_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T17:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq7zyg</id>
    <title>Any Tarantino fans?</title>
    <updated>2025-02-15T18:38:20+00:00</updated>
    <author>
      <name>/u/Sufficient-Wealth-78</name>
      <uri>https://old.reddit.com/user/Sufficient-Wealth-78</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iq7zyg/any_tarantino_fans/"&gt; &lt;img alt="Any Tarantino fans?" src="https://b.thumbs.redditmedia.com/0-aQWQeXVszY1S13UnlFzwgTFVs6rLhIUORwhHX3yhs.jpg" title="Any Tarantino fans?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is this some multidimensional parallel universe thingy? I mean if there's somewhere in different dimension Django sequel, I am moving&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sufficient-Wealth-78"&gt; /u/Sufficient-Wealth-78 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iq7zyg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq7zyg/any_tarantino_fans/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iq7zyg/any_tarantino_fans/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T18:38:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq5gr9</id>
    <title>Building a High-Performance AI Setup on a ‚Ç¨5000 Budget</title>
    <updated>2025-02-15T16:47:22+00:00</updated>
    <author>
      <name>/u/Severe_Biscotti2349</name>
      <uri>https://old.reddit.com/user/Severe_Biscotti2349</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iq5gr9/building_a_highperformance_ai_setup_on_a_5000/"&gt; &lt;img alt="Building a High-Performance AI Setup on a ‚Ç¨5000 Budget" src="https://external-preview.redd.it/FLrvkgnQaxhXP5T5ghlV_Eex6-uyrt_3lBcKa0Bh6Vk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a5027bcdc7a7acd346a77e15fd1f88af8fadff3" title="Building a High-Performance AI Setup on a ‚Ç¨5000 Budget" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm diving into building my own setup to run 70B LLMs in 4-bit with Ollama + OpenWebUI, and I‚Äôd love your insights! My budget is around ‚Ç¨5000, and I‚Äôm considering a dual RTX 3090 setup. I came across this configuration: &lt;a href="https://github.com/letsRTFM/AI-Workstation?tab=readme-ov-file"&gt;https://github.com/letsRTFM/AI-Workstation?tab=readme-ov-file&lt;/a&gt; . Does this look like a solid choice? Any recommendations for optimizations? (Also i wanted to use that pc for test and gaming, so i was thinking of a dual boot with ubuntu for dev and Windows for gaming, not a fan of wsl) &lt;/p&gt; &lt;p&gt;I‚Äôm also starting to help small company to implement AI solutions but 100% local also so i‚Äôm curious about the requirements. For a team of 20-30 people, handling around 2-3 simultaneous queries, what kind of internal setup would be needed to keep things running smoothly? (Also the cloud solution are intresting but some clients need physical servers) &lt;/p&gt; &lt;p&gt;I‚Äôm eager to learn and work on projects where I can gain hands-on experience. Looking forward to your thoughts and advice!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe_Biscotti2349"&gt; /u/Severe_Biscotti2349 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/letsRTFM/AI-Workstation?tab=readme-ov-file"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iq5gr9/building_a_highperformance_ai_setup_on_a_5000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iq5gr9/building_a_highperformance_ai_setup_on_a_5000/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-15T16:47:22+00:00</published>
  </entry>
</feed>
