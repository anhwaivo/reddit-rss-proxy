<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-06-27T17:06:01+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1lkn5a8</id>
    <title>What’s the best user interface for AGI like?</title>
    <updated>2025-06-26T01:12:14+00:00</updated>
    <author>
      <name>/u/suvsuvsuv</name>
      <uri>https://old.reddit.com/user/suvsuvsuv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let's say we will achieve AGI tomorrow, can we feel it with the current shape of AI applications with chat UI? If not, what should it be like?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/suvsuvsuv"&gt; /u/suvsuvsuv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lkn5a8/whats_the_best_user_interface_for_agi_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lkn5a8/whats_the_best_user_interface_for_agi_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lkn5a8/whats_the_best_user_interface_for_agi_like/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T01:12:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkepfp</id>
    <title>Looking for Metrics, Reports, or Case Studies on Ollama in Enterprise Environments</title>
    <updated>2025-06-25T19:14:27+00:00</updated>
    <author>
      <name>/u/patitopower</name>
      <uri>https://old.reddit.com/user/patitopower</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi, does anyone know of any reliable reports or metrics on Ollama adoption in businesses? thanks for any insights or resources!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/patitopower"&gt; /u/patitopower &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lkepfp/looking_for_metrics_reports_or_case_studies_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lkepfp/looking_for_metrics_reports_or_case_studies_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lkepfp/looking_for_metrics_reports_or_case_studies_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-25T19:14:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkhizo</id>
    <title>Ollama won't listen to connections outside of localhost machine.</title>
    <updated>2025-06-25T21:05:03+00:00</updated>
    <author>
      <name>/u/Gamervote</name>
      <uri>https://old.reddit.com/user/Gamervote</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried editing the sudo systemctl edit ollama command to change the port that it listens on, to no avail. I'm running ollama on a ubuntu server. Pls help lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gamervote"&gt; /u/Gamervote &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lkhizo/ollama_wont_listen_to_connections_outside_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lkhizo/ollama_wont_listen_to_connections_outside_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lkhizo/ollama_wont_listen_to_connections_outside_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-25T21:05:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkaqlt</id>
    <title>🚀 Revamped My Dungeon AI GUI Project – Now with a Clean Interface &amp; Better Usability!</title>
    <updated>2025-06-25T16:44:38+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lkaqlt/revamped_my_dungeon_ai_gui_project_now_with_a/"&gt; &lt;img alt="🚀 Revamped My Dungeon AI GUI Project – Now with a Clean Interface &amp;amp; Better Usability!" src="https://external-preview.redd.it/NpzXevyc8hccQld5KSdg19C9gOjRKnxTSGT0NaYuRD8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=429f3f424c02aa8038222f2c0d7e58d883b099aa" title="🚀 Revamped My Dungeon AI GUI Project – Now with a Clean Interface &amp;amp; Better Usability!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/m7ca9bd1r39f1.gif"&gt;https://i.redd.it/m7ca9bd1r39f1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey folks!&lt;br /&gt; I just gave my old project &lt;a href="https://github.com/Laszlobeer/Dungeo_ai"&gt;Dungeo_ai&lt;/a&gt; a serious upgrade and wanted to share the improved version:&lt;br /&gt; 🔗 &lt;a href="https://github.com/Laszlobeer/Dungeo_ai_GUI"&gt;&lt;strong&gt;Dungeo_ai_GUI on GitHub&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a &lt;strong&gt;local, GUI-based Dungeon Master AI&lt;/strong&gt; designed to let you roleplay solo DnD-style adventures using your own LLM (like a local LLaMA model via Ollama). The original project was CLI-based and clunky, but now it’s been reworked with:&lt;/p&gt; &lt;p&gt;🧠 &lt;strong&gt;Improvements:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🖥️ &lt;strong&gt;User-friendly GUI&lt;/strong&gt; using &lt;code&gt;tkinter&lt;/code&gt;&lt;/li&gt; &lt;li&gt;🎮 More immersive roleplay support&lt;/li&gt; &lt;li&gt;💾 Easy save/load system for sessions&lt;/li&gt; &lt;li&gt;🛠️ Cleaner codebase and better modularity for community mods&lt;/li&gt; &lt;li&gt;🧩 Simple integration with local LLM APIs (e.g. Ollama, LM Studio)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🧪 Currently testing with local models like &lt;strong&gt;LLaMA 3 8B/13B&lt;/strong&gt;, and performance is smooth even on mid-range hardware.&lt;/p&gt; &lt;p&gt;If you’re into solo RPGs, interactive storytelling, or just want to tinker with AI-powered DMs, I’d love your feedback or contributions!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try it, break it, or fork it:&lt;/strong&gt;&lt;br /&gt; 👉 &lt;a href="https://github.com/Laszlobeer/Dungeo_ai_GUI"&gt;https://github.com/Laszlobeer/Dungeo_ai_GUI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy dungeon delving! 🐉&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lkaqlt/revamped_my_dungeon_ai_gui_project_now_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lkaqlt/revamped_my_dungeon_ai_gui_project_now_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lkaqlt/revamped_my_dungeon_ai_gui_project_now_with_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-25T16:44:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lks25n</id>
    <title>Bring your own LLM server</title>
    <updated>2025-06-26T05:30:48+00:00</updated>
    <author>
      <name>/u/numinouslymusing</name>
      <uri>https://old.reddit.com/user/numinouslymusing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So if you’re a hobby developer making an app you want to release for free to the internet, chances are you can’t just pay for the inference costs for users, so logic kind of dictates you make the app bring-your-own-key.&lt;/p&gt; &lt;p&gt;So while ideating along the lines of “how can I have users have free LLMs?” I thought of webllm, which is a very cool project, but a couple of drawbacks that made me want to find an alternate solution was the lack of support for the OpenAI ask, and lack of multimodal support.&lt;/p&gt; &lt;p&gt;Then I arrived at the idea of a “bring your own LLM server” model, where people can still use hosted, book providers, but people can also spin up local servers with ollama or llama cpp, expose the port over ngrok, and use that.&lt;/p&gt; &lt;p&gt;Idk this may sound redundant to some but I kinda just wanted to hear some other ideas/thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/numinouslymusing"&gt; /u/numinouslymusing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lks25n/bring_your_own_llm_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lks25n/bring_your_own_llm_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lks25n/bring_your_own_llm_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T05:30:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lku8ip</id>
    <title>Is there a 'ready-to-use' Linux distribution for running LLMs locally (like Ollama)?</title>
    <updated>2025-06-26T07:49:39+00:00</updated>
    <author>
      <name>/u/AreBee73</name>
      <uri>https://old.reddit.com/user/AreBee73</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, do you know of a Linux distribution specifically prepared to use ollama or other LMMs locally, therefore preconfigured and specific for this purpose?&lt;/p&gt; &lt;p&gt;In practice, provided already &amp;quot;ready to use&amp;quot; with only minimal settings to change.&lt;/p&gt; &lt;p&gt;A bit like there are specific distributions for privacy or other sectoral tasks.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AreBee73"&gt; /u/AreBee73 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lku8ip/is_there_a_readytouse_linux_distribution_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lku8ip/is_there_a_readytouse_linux_distribution_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lku8ip/is_there_a_readytouse_linux_distribution_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T07:49:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll4wfy</id>
    <title>Troll My First SaaS app</title>
    <updated>2025-06-26T16:40:12+00:00</updated>
    <author>
      <name>/u/Significant_Abroad36</name>
      <uri>https://old.reddit.com/user/Significant_Abroad36</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ll4wfy/troll_my_first_saas_app/"&gt; &lt;img alt="Troll My First SaaS app" src="https://external-preview.redd.it/MWFyMGU1cDV2YTlmMWde8gwunT_bYnWBdtlOsaKnzitVEHx3CN-s6EvsSOjA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7cea005975486b63a632859ece4b535ed73d4379" title="Troll My First SaaS app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys - I have built an app which creates a roadmap of chapters that you need to read to learn a given topic.&lt;/p&gt; &lt;p&gt;It is personalized, so chapters are created in runtime based on user's learning curve.&lt;/p&gt; &lt;p&gt;User has to pass each quiz to unlock the next chapter.&lt;/p&gt; &lt;p&gt;below is the video , check this out and tell me what you think and share some cool product recommendations.&lt;/p&gt; &lt;p&gt;Best reccomendations will get free access to the beta app ( + some GPU credits!!) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant_Abroad36"&gt; /u/Significant_Abroad36 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rj9b70p5va9f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ll4wfy/troll_my_first_saas_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ll4wfy/troll_my_first_saas_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T16:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll5kes</id>
    <title>I built an AI Compound Analyzer with a custom multi-agent backend (Agno/Python) and a TypeScript/React frontend.</title>
    <updated>2025-06-26T17:06:06+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ll5kes/i_built_an_ai_compound_analyzer_with_a_custom/"&gt; &lt;img alt="I built an AI Compound Analyzer with a custom multi-agent backend (Agno/Python) and a TypeScript/React frontend." src="https://external-preview.redd.it/NnNqYmV6aW96YTlmMeQY9vO3ByPnz_trCSlC0d709aWnAv-iGUW2bEz5TRNb.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f05559fd916dbbe8194251bf5b7a3f45d453330" title="I built an AI Compound Analyzer with a custom multi-agent backend (Agno/Python) and a TypeScript/React frontend." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been deep in a personal project building a larger &amp;quot;BioAI Platform,&amp;quot; and I'm excited to share the first major module. It's an AI Compound Analyzer that takes a chemical name, pulls its structure, and runs a full analysis for things like molecular properties and ADMET predictions (basically, how a drug might behave in the body).&lt;/p&gt; &lt;p&gt;The goal was to build a highly responsive, modern tool.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; TypeScript, React, Next.js, and framer-motion for the smooth animations.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; This is where it gets fun. I used &lt;strong&gt;Agno&lt;/strong&gt;, a lightweight Python framework, to build a multi-agent system that orchestrates the analysis. It's a faster, leaner alternative to some of the bigger agentic frameworks out there.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Communication:&lt;/strong&gt; I'm using Server-Sent Events (SSE) to stream the analysis results from the backend to the frontend in real-time, which is what makes the UI update live as it works.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's been a challenging but super rewarding project, especially getting the backend agents to communicate efficiently with the reactive frontend.&lt;/p&gt; &lt;p&gt;Would love to hear any thoughts on the architecture or if you have suggestions for other cool open-source tools to integrate!&lt;/p&gt; &lt;p&gt;🚀 P.S. I am looking for new roles , If you like my work and have any Opportunites in Computer Vision or LLM Domain do contact me&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;My Email:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=mailto%3Apavankunchalaofficial%40gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My GitHub Profile (for more projects):&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My Resume:&lt;/strong&gt; &lt;a href="https://drive.google.com/file/d/1LVMVgAPKGUJbnrfE09OLJ0MrEZlBccOT/view"&gt;https://drive.google.com/file/d/1LVMVgAPKGUJbnrfE09OLJ0MrEZlBccOT/view&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gvkxb6joza9f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ll5kes/i_built_an_ai_compound_analyzer_with_a_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ll5kes/i_built_an_ai_compound_analyzer_with_a_custom/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T17:06:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkp8bu</id>
    <title>Anyone using Ollama with browser plugins? We built something interesting.</title>
    <updated>2025-06-26T02:55:26+00:00</updated>
    <author>
      <name>/u/InfiniteJX</name>
      <uri>https://old.reddit.com/user/InfiniteJX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks — I’ve been working a lot with &lt;strong&gt;Ollama&lt;/strong&gt; lately and really love how smooth it runs locally.&lt;/p&gt; &lt;p&gt;As part of exploring real-world uses, we recently built a Chrome extension called &lt;a href="https://nativemind.app/?utm_source=rd&amp;amp;ref=rd"&gt;&lt;strong&gt;NativeMind&lt;/strong&gt;&lt;/a&gt;. It connects to your local Ollama instance and lets you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Summarize any webpage directly in a sidebar&lt;/li&gt; &lt;li&gt;Ask questions about the current page content&lt;/li&gt; &lt;li&gt;Do &lt;em&gt;local&lt;/em&gt; search across open tabs — no cloud needed, which I think is super cool&lt;/li&gt; &lt;li&gt;Plug-and-play with any model you’ve started in Ollama&lt;/li&gt; &lt;li&gt;Run fully on-device (no external calls, ever)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It’s open-source and works out of the box — just install and start chatting with the web like it’s a doc. I’ve been using it for reading research papers, articles, and documentation, and it’s honestly made browsing a lot more productive.&lt;/p&gt; &lt;p&gt;👉 GitHub: &lt;a href="https://github.com/NativeMindBrowser/NativeMindExtension"&gt;https://github.com/NativeMindBrowser/NativeMindExtension&lt;/a&gt;&lt;/p&gt; &lt;p&gt;👉 &lt;a href="https://chromewebstore.google.com/detail/nativemind-your-fully-pri/mgchaojnijgpemdfhpnbeejnppigfllj"&gt;Chrome Web Store&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear if anyone else here is exploring similar Ollama + browser workflows — or if you try this one out, happy to take feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InfiniteJX"&gt; /u/InfiniteJX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lkp8bu/anyone_using_ollama_with_browser_plugins_we_built/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lkp8bu/anyone_using_ollama_with_browser_plugins_we_built/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lkp8bu/anyone_using_ollama_with_browser_plugins_we_built/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T02:55:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lla5bn</id>
    <title>Homebrew install of Ollama 0.9.3 still has binary that reports as 0.9.0</title>
    <updated>2025-06-26T20:04:43+00:00</updated>
    <author>
      <name>/u/illkeepthatinmind</name>
      <uri>https://old.reddit.com/user/illkeepthatinmind</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else seeing this? Can't run the new Gemma model due to this. Already tried reinstalling and with cleared brew cache.&lt;/p&gt; &lt;p&gt;&lt;code&gt; brew install ollama Warning: Treating ollama as a formula. For the cask, use homebrew/cask/ollama-app or specify the --cask flag. To silence this message, use the \`--formula\` flag. ==&amp;gt; Downloading https://ghcr.io/v2/homebrew/core/ollama/manifests/0.9.3 ... ... ollama -v ollama version is 0.9.0 Warning: client version is 0.9.3&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/illkeepthatinmind"&gt; /u/illkeepthatinmind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lla5bn/homebrew_install_of_ollama_093_still_has_binary/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lla5bn/homebrew_install_of_ollama_093_still_has_binary/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lla5bn/homebrew_install_of_ollama_093_still_has_binary/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T20:04:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1llffm5</id>
    <title>Anyone running ollama models on windows and using claude code?</title>
    <updated>2025-06-26T23:48:59+00:00</updated>
    <author>
      <name>/u/Prophet_60091_</name>
      <uri>https://old.reddit.com/user/Prophet_60091_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(apologies if this question isn't a good fit for the sub)&lt;br /&gt; I'm trying to play around with writing some custom AI agents using different models running with ollama on my windows 11 desktop because I have an RTX 5080 GPU that I'm using to offload a lot of the work to. I am also trying to get claude code setup within my VSCode IDE so I can have it help me play around with writing code for the agents.&lt;/p&gt; &lt;p&gt;The problem I'm running into is that claude code isn't supported natively on windows and so I have to run it within WSL. I can connect to the distro from WSL, but I'm afraid I won't be able to run my scripts from within WSL and still have ollama offload the work onto my GPU. Do I need some fancy GPU passthrough setup for WSL? Are people just not using tools like claude code when working with ollama on PCs with powerful GPUs? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prophet_60091_"&gt; /u/Prophet_60091_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llffm5/anyone_running_ollama_models_on_windows_and_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llffm5/anyone_running_ollama_models_on_windows_and_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1llffm5/anyone_running_ollama_models_on_windows_and_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T23:48:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1llqljh</id>
    <title>Does this mean I'm poor 😂</title>
    <updated>2025-06-27T10:35:49+00:00</updated>
    <author>
      <name>/u/Kind_Ad_2866</name>
      <uri>https://old.reddit.com/user/Kind_Ad_2866</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1llqljh/does_this_mean_im_poor/"&gt; &lt;img alt="Does this mean I'm poor 😂" src="https://preview.redd.it/8p2fl4737g9f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fa9d72abe63e99966c81c7d7cf80971bd2c1d814" title="Does this mean I'm poor 😂" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kind_Ad_2866"&gt; /u/Kind_Ad_2866 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8p2fl4737g9f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llqljh/does_this_mean_im_poor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1llqljh/does_this_mean_im_poor/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T10:35:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lktb12</id>
    <title>I tested 10 LLMs locally on my MacBook Air M1 (8GB RAM!) – Here's what actually works-</title>
    <updated>2025-06-26T06:48:24+00:00</updated>
    <author>
      <name>/u/irodov4030</name>
      <uri>https://old.reddit.com/user/irodov4030</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lktb12/i_tested_10_llms_locally_on_my_macbook_air_m1_8gb/"&gt; &lt;img alt="I tested 10 LLMs locally on my MacBook Air M1 (8GB RAM!) – Here's what actually works-" src="https://external-preview.redd.it/te4YfuD8PP6HnzEPXIgrUZWitrs0nRz7rrJC6dhgl-g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6851992f1a218c5b31b036d8faf07e13da48567" title="I tested 10 LLMs locally on my MacBook Air M1 (8GB RAM!) – Here's what actually works-" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I went down the LLM rabbit hole trying to find the &lt;strong&gt;best local model&lt;/strong&gt; that runs &lt;em&gt;well&lt;/em&gt; on a humble MacBook Air M1 with just 8GB RAM.&lt;/p&gt; &lt;p&gt;My goal? &lt;strong&gt;Compare 10 models&lt;/strong&gt; across question generation, answering, and self-evaluation.&lt;/p&gt; &lt;p&gt;TL;DR: Some models were brilliant, others… not so much. One even took &lt;strong&gt;8 minutes&lt;/strong&gt; to write a question.&lt;/p&gt; &lt;p&gt;Here's the breakdown &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Models Tested&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mistral 7B&lt;/li&gt; &lt;li&gt;DeepSeek-R1 1.5B&lt;/li&gt; &lt;li&gt;Gemma3:1b&lt;/li&gt; &lt;li&gt;Gemma3:latest&lt;/li&gt; &lt;li&gt;Qwen3 1.7B&lt;/li&gt; &lt;li&gt;Qwen2.5-VL 3B&lt;/li&gt; &lt;li&gt;Qwen3 4B&lt;/li&gt; &lt;li&gt;LLaMA 3.2 1B&lt;/li&gt; &lt;li&gt;LLaMA 3.2 3B&lt;/li&gt; &lt;li&gt;LLaMA 3.1 8B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;(All models run with quantized versions, via: os.environ[&amp;quot;OLLAMA_CONTEXT_LENGTH&amp;quot;] = &amp;quot;4096&amp;quot; and os.environ[&amp;quot;OLLAMA_KV_CACHE_TYPE&amp;quot;] = &amp;quot;q4_0&amp;quot;)&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Each model:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Generated 1 question on 5 topics: &lt;em&gt;Math, Writing, Coding, Psychology, History&lt;/em&gt;&lt;/li&gt; &lt;li&gt;Answered all 50 questions (5 x 10)&lt;/li&gt; &lt;li&gt;Evaluated every answer (including their own)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So in total:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;50 questions&lt;/li&gt; &lt;li&gt;500 answers&lt;/li&gt; &lt;li&gt;4830 evaluations (Should be 5000; I evaluated less answers with qwen3:1.7b and qwen3:4b as they do not generate scores and take a lot of time&lt;strong&gt;)&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And I tracked:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;token generation speed (tokens/sec)&lt;/li&gt; &lt;li&gt;tokens created&lt;/li&gt; &lt;li&gt;time taken&lt;/li&gt; &lt;li&gt;scored all answers for quality&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question Generation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fastest: &lt;strong&gt;LLaMA 3.2 1B&lt;/strong&gt;, &lt;strong&gt;Gemma3:1b&lt;/strong&gt;, &lt;strong&gt;Qwen3 1.7B&lt;/strong&gt; (LLaMA 3.2 1B hit 82 tokens/sec, avg is ~40 tokens/sec (for english topic question it reached &lt;strong&gt;146 tokens/sec)&lt;/strong&gt; &lt;/li&gt; &lt;li&gt;Slowest: &lt;strong&gt;LLaMA 3.1 8B&lt;/strong&gt;, &lt;strong&gt;Qwen3 4B&lt;/strong&gt;, &lt;strong&gt;Mistral 7B&lt;/strong&gt; Qwen3 4B took &lt;strong&gt;486s&lt;/strong&gt; (8+ mins) to generate a single Math question! &lt;/li&gt; &lt;li&gt;Fun fact: deepseek-r1:1.5b, qwen3:4b and Qwen3:1.7B output &amp;lt;think&amp;gt; tags in questions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Answer Generation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fastest: &lt;strong&gt;Gemma3:1b&lt;/strong&gt;, &lt;strong&gt;LLaMA 3.2 1B&lt;/strong&gt; and &lt;strong&gt;DeepSeek-R1 1.5B&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;DeepSeek got faster answering &lt;em&gt;its own&lt;/em&gt; questions (80 tokens/s vs. avg 40 tokens/s)&lt;/li&gt; &lt;li&gt;Qwen3 4B generates &lt;strong&gt;2–3x more tokens&lt;/strong&gt; per answer&lt;/li&gt; &lt;li&gt;Slowest: llama3.1:8b, qwen3:4b and mistral:7b&lt;/li&gt; &lt;/ul&gt; &lt;p&gt; &lt;strong&gt;Evaluation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Best scorer: Gemma3:latest – consistent, numerical, no bias&lt;/li&gt; &lt;li&gt;Worst scorer: &lt;strong&gt;DeepSeek-R1 1.5B&lt;/strong&gt; – often skipped scores entirely&lt;/li&gt; &lt;li&gt;Bias detected: Many models &lt;strong&gt;rate their own answers higher&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;DeepSeek even evaluated some answers &lt;strong&gt;in Chinese&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Fun Observations&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Some models create &amp;lt;think&amp;gt; tags for questions, answers and even while evaluation as output&lt;/li&gt; &lt;li&gt;Score inflation is real: Mistral, Qwen3, and LLaMA 3.1 8B overrate themselves&lt;/li&gt; &lt;li&gt;Score formats vary wildly (text explanations vs. plain numbers)&lt;/li&gt; &lt;li&gt;Speed isn’t everything – some slower models gave much higher quality answers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Best Performers (My Picks)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;|| || |&lt;strong&gt;Task&lt;/strong&gt;|&lt;strong&gt;Best Model&lt;/strong&gt;|&lt;strong&gt;Why&lt;/strong&gt;| |Question Gen|LLaMA 3.2 1B|Fast &amp;amp; relevant| |Answer Gen|Gemma3:1b |Fast, accurate| |Evaluation|llama3.2:3b|Generates numerical scores and evaluations closest to the model average|&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Worst Surprises&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;|| || |&lt;strong&gt;Task&lt;/strong&gt;|&lt;strong&gt;Model&lt;/strong&gt;|&lt;strong&gt;Problem&lt;/strong&gt;| |Question Gen|Qwen3 4B|Took &lt;strong&gt;486s&lt;/strong&gt; to generate 1 question| |Answer Gen|LLaMA 3.1 8B|Slow | |Evaluation|DeepSeek-R1 1.5B|Inconsistent, skipped scores|&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Screenshots Galore&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I’m adding screenshots of:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Questions generation&lt;/li&gt; &lt;li&gt;Answer comparisons&lt;/li&gt; &lt;li&gt;Evaluation outputs&lt;/li&gt; &lt;li&gt;Token/sec charts (So stay tuned or ask if you want raw data!)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Takeaways&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You &lt;strong&gt;can&lt;/strong&gt; run decent LLMs locally on M1 Air (8GB) – if you pick the right ones&lt;/li&gt; &lt;li&gt;Model size ≠ performance. Bigger isn't always better.&lt;/li&gt; &lt;li&gt;Bias in self-evaluation is &lt;strong&gt;real&lt;/strong&gt; – and model behavior varies wildly&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Post questions if you have any, I will try to answer&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/irodov4030"&gt; /u/irodov4030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lktb12"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lktb12/i_tested_10_llms_locally_on_my_macbook_air_m1_8gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lktb12/i_tested_10_llms_locally_on_my_macbook_air_m1_8gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T06:48:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1llsh6p</id>
    <title>Document QA</title>
    <updated>2025-06-27T12:21:23+00:00</updated>
    <author>
      <name>/u/raghav-ai</name>
      <uri>https://old.reddit.com/user/raghav-ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have set of 10 manuals to be followed in a company , each manual is around 40-50 pages. Now , we need a chatbot appication which can answer based on this manuals. I tried RAG, but lot of hallucinations Answer can be from multiple documents and can be from mix of paras from differet pages ir even different manual. So in that case, if RAG gets wrong chunk, it hallucinates.&lt;/p&gt; &lt;p&gt;I need a complete offline solution.&lt;/p&gt; &lt;p&gt;I tried chatwithpdf sites , and ChatGPT on internet , it worked well.&lt;/p&gt; &lt;p&gt;But on offline solution, i am facing hard to achieve even 10% of that accuracy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/raghav-ai"&gt; /u/raghav-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llsh6p/document_qa/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llsh6p/document_qa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1llsh6p/document_qa/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T12:21:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll4us5</id>
    <title>Beautify Ollama</title>
    <updated>2025-06-26T16:38:25+00:00</updated>
    <author>
      <name>/u/falkon2112</name>
      <uri>https://old.reddit.com/user/falkon2112</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1ll4us5/video/5zt9ljutua9f1/player"&gt;https://reddit.com/link/1ll4us5/video/5zt9ljutua9f1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I got tired of the basic Ollama interfaces out there and decided to build something that looks like it belongs in 2025. Meet &lt;strong&gt;BeautifyOllama&lt;/strong&gt; - a modern web interface that makes chatting with your local AI models actually enjoyable.&lt;/p&gt; &lt;h1&gt;What it does:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Animated shine borders&lt;/strong&gt; that cycle through colors (because why not make AI conversations pretty?)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time streaming&lt;/strong&gt; responses that feel snappy&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dark/light themes&lt;/strong&gt; that follow your system preferences&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mobile-responsive&lt;/strong&gt; so you can chat with AI on the toilet (we've all been there)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Glassmorphism effects&lt;/strong&gt; and smooth animations everywhere&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Tech stack (for the nerds):&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Next.js 15 + React 19 (bleeding edge stuff)&lt;/li&gt; &lt;li&gt;TypeScript (because I like my code to not break)&lt;/li&gt; &lt;li&gt;TailwindCSS 4 (utility classes go brrr)&lt;/li&gt; &lt;li&gt;Framer Motion (for those buttery smooth animations)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Demo &amp;amp; Code:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Live demo:&lt;/strong&gt; &lt;a href="https://beautifyollama.vercel.app/"&gt;https://beautifyollama.vercel.app/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/falkon2/BeautifyOllama"&gt;https://github.com/falkon2/BeautifyOllama&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What's coming next:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;File uploads (drag &amp;amp; drop your docs)&lt;/li&gt; &lt;li&gt;Conversation history that doesn't disappear&lt;/li&gt; &lt;li&gt;Plugin system for extending functionality&lt;/li&gt; &lt;li&gt;Maybe a mobile app if people actually use this thing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup is stupid simple:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Have Ollama running (&lt;code&gt;ollama serve&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Clone the repo&lt;/li&gt; &lt;li&gt;&lt;code&gt;npm install &amp;amp;&amp;amp; npm run dev&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Profit&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I would appreciate any and all feedback as well as criticism.&lt;/p&gt; &lt;p&gt;The project is early-stage but functional. I'm actively working on it and would love feedback, contributions, or just general roasting of my code.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question for the community:&lt;/strong&gt; What features would you actually want in a local AI interface? I'm building this for real use,.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/falkon2112"&gt; /u/falkon2112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ll4us5/beautify_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ll4us5/beautify_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ll4us5/beautify_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T16:38:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1llv752</id>
    <title>Anyone else experiencing extreme slowness with Gemma 3n on Ollama?</title>
    <updated>2025-06-27T14:24:15+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded Genma3n FP16 off of Ollama’s official repository and I’m running it on an H100 and it’s running at like hot garbage (like 2 tokens/s). I’ve tried it on both 0.9.3 and pre-release of 0.9.4. Anymore else encountered this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llv752/anyone_else_experiencing_extreme_slowness_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llv752/anyone_else_experiencing_extreme_slowness_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1llv752/anyone_else_experiencing_extreme_slowness_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T14:24:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1llxeye</id>
    <title>[DEV] AgentTip – trigger your OpenAI assistants or Ollama models from any macOS app (one-time $4.99)</title>
    <updated>2025-06-27T15:54:02+00:00</updated>
    <author>
      <name>/u/Brazilgs</name>
      <uri>https://old.reddit.com/user/Brazilgs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1llxeye/dev_agenttip_trigger_your_openai_assistants_or/"&gt; &lt;img alt="[DEV] AgentTip – trigger your OpenAI assistants or Ollama models from any macOS app (one-time $4.99)" src="https://external-preview.redd.it/ejlhMXEwM3ZyaDlmMYYoLEsPwZFbst0w5wBXHUH9dFkt8rq8VyC4VGq0TReR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05e1dc68750abef1492e17db06338a1adc89de4d" title="[DEV] AgentTip – trigger your OpenAI assistants or Ollama models from any macOS app (one-time $4.99)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks 👋 I’m the dev behind AgentTip.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.agenttip.xyz/"&gt;https://www.agenttip.xyz/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Problem: jumping to a browser or separate window every time you want LLM kills flow.&lt;/p&gt; &lt;p&gt;Fix: type @idea brainstorm an onboarding flow, hit ⏎, and AgentTip swaps the trigger for the assistant’s reply—right where you were typing. No context-switch, no copy-paste.&lt;/p&gt; &lt;p&gt;• Instant trigger recognition – define @writer, @code, anything you like.&lt;/p&gt; &lt;p&gt;• Works system-wide – TextEdit → VS Code → Safari, you name it.&lt;/p&gt; &lt;p&gt;• Unlimited assistants – connect every OpenAI Assistant or Ollama model you’ve avaiable.&lt;/p&gt; &lt;p&gt;• Unlimited use – connect every Ollama model you’ve in your local machine. - TOTAL privacy, using Ollama, your data never goes online. &lt;/p&gt; &lt;p&gt;• Your own API key, stored in macOS Keychain – pay OpenAI directly; we never see your data.&lt;/p&gt; &lt;p&gt;• One-time purchase, $4.99 lifetime licence – no subscriptions.&lt;/p&gt; &lt;p&gt;Mac App Store: &lt;a href="https://apps.apple.com/app/agenttip/id6747261813?utm_source=reddit&amp;amp;utm_campaign=macapps_launch"&gt;https://apps.apple.com/app/agenttip/id6747261813?utm_source=reddit&amp;amp;utm_campaign=macapps_launch&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brazilgs"&gt; /u/Brazilgs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3qqih57vrh9f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llxeye/dev_agenttip_trigger_your_openai_assistants_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1llxeye/dev_agenttip_trigger_your_openai_assistants_or/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T15:54:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lly7a4</id>
    <title>Best models a macbook can support</title>
    <updated>2025-06-27T16:25:13+00:00</updated>
    <author>
      <name>/u/lrshaid</name>
      <uri>https://old.reddit.com/user/lrshaid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! &lt;/p&gt; &lt;p&gt;I'm doing my first baby steps in runnning LLMs locally. I have a M4 16gb macbook air. Based on your experience, what do you recommend to run? I mean, probably you can run a lot of stuff but with big waiting times. Nothing in particular, just want to read your experiences!&lt;/p&gt; &lt;p&gt;Thanks in advance :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lrshaid"&gt; /u/lrshaid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lly7a4/best_models_a_macbook_can_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lly7a4/best_models_a_macbook_can_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lly7a4/best_models_a_macbook_can_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T16:25:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lly9ii</id>
    <title>Recommend me the best model for coding</title>
    <updated>2025-06-27T16:27:34+00:00</updated>
    <author>
      <name>/u/mo7akh</name>
      <uri>https://old.reddit.com/user/mo7akh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running a beefy GTX 1650 4gb and a whopping 16gb of ram. Recommend me the best coding model for this hardware, and thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mo7akh"&gt; /u/mo7akh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lly9ii/recommend_me_the_best_model_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lly9ii/recommend_me_the_best_model_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lly9ii/recommend_me_the_best_model_for_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T16:27:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1llyj6o</id>
    <title>Issues with Tools via OW UI hitting Ollama via Tools/Filters</title>
    <updated>2025-06-27T16:38:07+00:00</updated>
    <author>
      <name>/u/matty990</name>
      <uri>https://old.reddit.com/user/matty990</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1llyj6o/issues_with_tools_via_ow_ui_hitting_ollama_via/"&gt; &lt;img alt="Issues with Tools via OW UI hitting Ollama via Tools/Filters" src="https://preview.redd.it/jhfkd85bzh9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c6e756d8c50683bf9aae2bae6a78a2006813640d" title="Issues with Tools via OW UI hitting Ollama via Tools/Filters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When using Open Web I have no issues with them speaking. It appears when trying to use a Memory Tool to connect it throws up 405s. &lt;/p&gt; &lt;p&gt;The network is all good as they are on the same docker stack. &lt;/p&gt; &lt;p&gt;Any advice would be amazing as this is the last step for me to get this fully setup. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matty990"&gt; /u/matty990 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jhfkd85bzh9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llyj6o/issues_with_tools_via_ow_ui_hitting_ollama_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1llyj6o/issues_with_tools_via_ow_ui_hitting_ollama_via/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T16:38:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1llst8n</id>
    <title>How do I force Ollama to exclusively use GPU</title>
    <updated>2025-06-27T12:37:50+00:00</updated>
    <author>
      <name>/u/RadiantPermission513</name>
      <uri>https://old.reddit.com/user/RadiantPermission513</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay so I have a bit of an interesting situation. The computer I have running my Ollama LLMs is kind of a potato, it's running an older Ryzen CPU I don't remember the model off the top of my head and 32gb DDR3 RAM. It was my old Proxmox server I have since upgraded. However I upgraded my GPU in my gaming rig a while back and have an Nvidia 3050 that wasn't being used. So I put the 3050 in the rig and decided to make a dedicated LLM server running Open Web UI on it as well. Yes I recognize I put a sports car engine in a potato. However the issue I am having is Ollama can decide to use the sports car engine which runs 8b models like a champ or the potato which locks up with 3b models. I regularly have to restart it and flip a coin which it'll use, if it decides to us the GPU it'll run great for a few days then decide to give Llama3.1 8b a good college try on the CPU and lock out once the CPU starts running at 450%. Is there a way to convince Ollama to only use GPU and forget about the CPU? It won't even try to offload, it's 100% one or the other.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RadiantPermission513"&gt; /u/RadiantPermission513 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llst8n/how_do_i_force_ollama_to_exclusively_use_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llst8n/how_do_i_force_ollama_to_exclusively_use_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1llst8n/how_do_i_force_ollama_to_exclusively_use_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T12:37:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1llt7gr</id>
    <title>gemma3n not working with pictures</title>
    <updated>2025-06-27T12:56:58+00:00</updated>
    <author>
      <name>/u/Fun_Librarian_7699</name>
      <uri>https://old.reddit.com/user/Fun_Librarian_7699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tested gemma3n and it's really fast, but I looks like ollama doesn't support images (yet). According to their &lt;a href="https://ollama.com/library/gemma3n"&gt;webseite&lt;/a&gt;, gemma3n should support images and also audio. I've never used a model that supports audio with ollama before, looking forward to trying it when it's working. By the way, I updated ollama today and am now using version 0.9.3.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;(base) PS C:\Users\andre&amp;gt; ollama run gemma3:12b-it-q4_K_M &amp;gt;&amp;gt;&amp;gt; Describe the picture in one sentence &amp;quot;C:\Users\andre\Desktop\picture.jpg&amp;quot; Added image 'C:\Users\andre\Desktop\picture.jpg' A fluffy, orange and white cat is sprawled out and relaxing on a colorful patterned blanket with its paws extended. &amp;gt;&amp;gt;&amp;gt; (base) PS C:\Users\andre&amp;gt; ollama run gemma3n:e4b-it-q8_0 &amp;gt;&amp;gt;&amp;gt; Describe the picture in one sentence &amp;quot;C:\Users\andre\Desktop\picture.jpg&amp;quot; I am unable to access local files or URLs, so I cannot describe the picture at the given file path. Therefore, I can't fulfill your request. To get a description, you would need to: 1. **Describe the picture to me:** Tell me what you see in the image. 2. **Use an image recognition service:** Upload the image to a service like Google Lens, Amazon Rekognition, or Clarifai, which can analyze the image and provide a description. &amp;gt;&amp;gt;&amp;gt; (base) PS C:\Users\andre&amp;gt; ollama -v ollama version is 0.9.3 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun_Librarian_7699"&gt; /u/Fun_Librarian_7699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llt7gr/gemma3n_not_working_with_pictures/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llt7gr/gemma3n_not_working_with_pictures/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1llt7gr/gemma3n_not_working_with_pictures/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T12:56:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1llwd01</id>
    <title>GPU Configuration for Macbook M3</title>
    <updated>2025-06-27T15:12:09+00:00</updated>
    <author>
      <name>/u/Initial-Ad751</name>
      <uri>https://old.reddit.com/user/Initial-Ad751</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, What’s the best Ollama setup config for a Macbook Air M3 with 16 GB RAM, 512 GB SSD? I want it to use the GPU but not sure if it’s using it. My use case is mostly VScode with Continue. Any particular suggestions for which model also to use best? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Initial-Ad751"&gt; /u/Initial-Ad751 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llwd01/gpu_configuration_for_macbook_m3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llwd01/gpu_configuration_for_macbook_m3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1llwd01/gpu_configuration_for_macbook_m3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T15:12:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll5h5p</id>
    <title>gemma3n is out</title>
    <updated>2025-06-26T17:02:40+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma 3n models are designed for efficient execution on everyday devices such as laptops, tablets or phones.&lt;/p&gt; &lt;p&gt;Gemma 3n models are designed for efficient execution on everyday devices such as laptops, tablets or phones. These models were trained with data in over 140 spoken languages.&lt;/p&gt; &lt;p&gt;Gemma 3n models use selective parameter activation technology to reduce resource requirements. This technique allows the models to operate at an effective size of 2B and 4B parameters, which is lower than the total number of parameters they contain.&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/gemma3n"&gt;https://ollama.com/library/gemma3n&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Upd: ollama 0.9.3 required&lt;/p&gt; &lt;p&gt;Upd2: official post &lt;a href="https://www.reddit.com/r/LocalLLaMA/s/0nLcE3wzA1"&gt;https://www.reddit.com/r/LocalLLaMA/s/0nLcE3wzA1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ll5h5p/gemma3n_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ll5h5p/gemma3n_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ll5h5p/gemma3n_is_out/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T17:02:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lls5os</id>
    <title>Arch-Router 1.5B - The world's fast and first LLM router that can align to your usage preferences.</title>
    <updated>2025-06-27T12:04:55+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lls5os/archrouter_15b_the_worlds_fast_and_first_llm/"&gt; &lt;img alt="Arch-Router 1.5B - The world's fast and first LLM router that can align to your usage preferences." src="https://preview.redd.it/7u00gzrxkg9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c39772460819d003fdebb4e72fe5e3ae7b1b9377" title="Arch-Router 1.5B - The world's fast and first LLM router that can align to your usage preferences." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited to share Arch-Router, our research and model for LLM routing. Routing to the right LLM is still an elusive problem, riddled with nuance and blindspots. For example:&lt;/p&gt; &lt;p&gt;“Embedding-based” (or simple intent-classifier) routers sound good on paper—label each prompt via embeddings as “support,” “SQL,” “math,” then hand it to the matching model—but real chats don’t stay in their lanes. Users bounce between topics, task boundaries blur, and any new feature means retraining the classifier. The result is brittle routing that can’t keep up with multi-turn conversations or fast-moving product scopes.&lt;/p&gt; &lt;p&gt;Performance-based routers swing the other way, picking models by benchmark or cost curves. They rack up points on MMLU or MT-Bench yet miss the human tests that matter in production: “Will Legal accept this clause?” “Does our support tone still feel right?” Because these decisions are subjective and domain-specific, benchmark-driven black-box routers often send the wrong model when it counts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Arch-Router skips both pitfalls by routing on&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;preferences you write in plain language&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt; Drop rules like “contract clauses → GPT-4o” or “quick travel tips → Gemini-Flash,” and our 1.5B auto-regressive router model maps prompt along with the context to your routing policies—no retraining, no sprawling rules that are encoded in if/else statements. Co-designed with Twilio and Atlassian, it adapts to intent drift, lets you swap in new models with a one-liner, and keeps routing logic in sync with the way you actually judge quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Specs&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tiny footprint&lt;/strong&gt; – 1.5 B params → runs on one modern GPU (or CPU while you play).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Plug-n-play&lt;/strong&gt; – points at any mix of LLM endpoints; adding models needs &lt;em&gt;zero&lt;/em&gt; retraining.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SOTA query-to-policy matching&lt;/strong&gt; – beats bigger closed models on conversational datasets.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost / latency smart&lt;/strong&gt; – push heavy stuff to premium models, everyday queries to the fast ones.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Exclusively available in Arch (the AI-native proxy for agents): &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;br /&gt; 🔗 Model + code: &lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;https://huggingface.co/katanemo/Arch-Router-1.5B&lt;/a&gt;&lt;br /&gt; 📄 Paper / longer read: &lt;a href="https://arxiv.org/abs/2506.16655"&gt;https://arxiv.org/abs/2506.16655&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7u00gzrxkg9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lls5os/archrouter_15b_the_worlds_fast_and_first_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lls5os/archrouter_15b_the_worlds_fast_and_first_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T12:04:55+00:00</published>
  </entry>
</feed>
