<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-24T06:25:15+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1i80brl</id>
    <title>My first every test drive of the deepseek-r1</title>
    <updated>2025-01-23T10:20:35+00:00</updated>
    <author>
      <name>/u/Kind_Ad_2866</name>
      <uri>https://old.reddit.com/user/Kind_Ad_2866</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i80brl/my_first_every_test_drive_of_the_deepseekr1/"&gt; &lt;img alt="My first every test drive of the deepseek-r1" src="https://b.thumbs.redditmedia.com/U7q8JfhCRJ9KmqGLoZ-z3_3bBBOkzIJsHlBoWdlNdgM.jpg" title="My first every test drive of the deepseek-r1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded DeepSeek-R1 last night and tested it on my Nvidia GeForce 4060 (8GB) paired with an Intel i7 and 64GB of RAM. My first impressions are really positive—it handles conversation and reasoning surprisingly well, especially for a locally running model. However, unlike other models such as LLaMA 3.3, I can’t seem to make it follow my instructions precisely. If you look at the screenshot, you’ll see that it doesn’t produce the exact output format I requested. I’m wondering if I’m missing a step or if this is just how the model behaves.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x67te5yd4qee1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1350e47660acbc968da44a05be040228799b0467"&gt;https://preview.redd.it/x67te5yd4qee1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1350e47660acbc968da44a05be040228799b0467&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/aq6ba6yd4qee1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1065763b1396f06a34b01f5f291e8af514999d3"&gt;https://preview.redd.it/aq6ba6yd4qee1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1065763b1396f06a34b01f5f291e8af514999d3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hh3oa7yd4qee1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9312866d88acd4a01c0e0006e98fdc7a32eb8d35"&gt;https://preview.redd.it/hh3oa7yd4qee1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9312866d88acd4a01c0e0006e98fdc7a32eb8d35&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kind_Ad_2866"&gt; /u/Kind_Ad_2866 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i80brl/my_first_every_test_drive_of_the_deepseekr1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i80brl/my_first_every_test_drive_of_the_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i80brl/my_first_every_test_drive_of_the_deepseekr1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T10:20:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i80oxr</id>
    <title>Which version of deepseek-r1 for coding on my M3 Pro MacBook?</title>
    <updated>2025-01-23T10:46:58+00:00</updated>
    <author>
      <name>/u/joyfulsparrow</name>
      <uri>https://old.reddit.com/user/joyfulsparrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've got a MacBook with these specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; M3 Pro&lt;/li&gt; &lt;li&gt; Memory: 36GB&lt;/li&gt; &lt;li&gt; macOS 15.2&lt;/li&gt; &lt;li&gt; Ollama&lt;/li&gt; &lt;li&gt; Ollamac app&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What's the best version of deepseek-rc1 for coding that I could use on the MacBook? &lt;a href="https://ollama.com/library/deepseek-r1"&gt;https://ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joyfulsparrow"&gt; /u/joyfulsparrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i80oxr/which_version_of_deepseekr1_for_coding_on_my_m3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i80oxr/which_version_of_deepseekr1_for_coding_on_my_m3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i80oxr/which_version_of_deepseekr1_for_coding_on_my_m3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T10:46:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i814ta</id>
    <title>Is it possible to run ollama if i have multiple computers?</title>
    <updated>2025-01-23T11:17:55+00:00</updated>
    <author>
      <name>/u/Muted_Membership9372</name>
      <uri>https://old.reddit.com/user/Muted_Membership9372</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m kind of new to this field, so I apologize for any ignorance on my part!&lt;br /&gt; Here’s the problem I’ve encountered:&lt;br /&gt; Suppose I have several devices with NVIDIA GeForce RTX 3060 Lite Hash Rate, and I’d like to run a model that requires more GPU VRAM than I have on a single GPU.&lt;br /&gt; So, my question is:&lt;br /&gt; Is it possible to use my devices (perhaps as a cluster) to run the model? And if so, will the workload be distributed equally across the GPUs?&lt;/p&gt; &lt;p&gt;I'd also be grateful to know if there's already implemented solutions out there, as I'd prefer not to reinvent the wheel&lt;/p&gt; &lt;p&gt;Thx in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Muted_Membership9372"&gt; /u/Muted_Membership9372 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i814ta/is_it_possible_to_run_ollama_if_i_have_multiple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i814ta/is_it_possible_to_run_ollama_if_i_have_multiple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i814ta/is_it_possible_to_run_ollama_if_i_have_multiple/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T11:17:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7as97</id>
    <title>The Chinese OBLITERATED OpenAI. A side-by-side comparison of DeepSeek R1 vs OpenAI O1 for Finance</title>
    <updated>2025-01-22T13:12:02+00:00</updated>
    <author>
      <name>/u/No-Definition-2886</name>
      <uri>https://old.reddit.com/user/No-Definition-2886</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7as97/the_chinese_obliterated_openai_a_sidebyside/"&gt; &lt;img alt="The Chinese OBLITERATED OpenAI. A side-by-side comparison of DeepSeek R1 vs OpenAI O1 for Finance" src="https://external-preview.redd.it/0d_JuM7Vh55nJ0hGva5m6LiE2ZjyLRmQI-3W0gsRh-4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b831df3cf7b173edcc1428ce4703b68ea01c270" title="The Chinese OBLITERATED OpenAI. A side-by-side comparison of DeepSeek R1 vs OpenAI O1 for Finance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Definition-2886"&gt; /u/No-Definition-2886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/p/93a1b4343a82"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7as97/the_chinese_obliterated_openai_a_sidebyside/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7as97/the_chinese_obliterated_openai_a_sidebyside/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T13:12:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7r6hf</id>
    <title>How to Turn off Thinking on ollama for DS-R1?</title>
    <updated>2025-01-23T00:54:11+00:00</updated>
    <author>
      <name>/u/PBlague</name>
      <uri>https://old.reddit.com/user/PBlague</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7r6hf/how_to_turn_off_thinking_on_ollama_for_dsr1/"&gt; &lt;img alt="How to Turn off Thinking on ollama for DS-R1?" src="https://a.thumbs.redditmedia.com/Xs-jZtPcBKuCW5kZ139NKMaiuaxV26kv1lwVYBQpU70.jpg" title="How to Turn off Thinking on ollama for DS-R1?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/o1yk2nfn5nee1.png?width=989&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5bfe41d31ef405a0d23da5d42fcbdc03cd00956f"&gt;DeepThink Button at the Bottom Left&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On the website you can turn on &amp;quot;DeepThink&amp;quot; if you so choose to, which helps make the model more powerful in reasoning and less powerful in actually holding the conversation.&lt;/p&gt; &lt;p&gt;Is there a way to turn off this thinking part in ollama too?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qc7szvbx5nee1.png?width=1405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fb98b4076d3cab51fbab5516c61bcb5962771766"&gt;&amp;lt;Think&amp;gt; &amp;lt;/Think&amp;gt; Patterns&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PBlague"&gt; /u/PBlague &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7r6hf/how_to_turn_off_thinking_on_ollama_for_dsr1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7r6hf/how_to_turn_off_thinking_on_ollama_for_dsr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7r6hf/how_to_turn_off_thinking_on_ollama_for_dsr1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T00:54:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8749h</id>
    <title>Problem launching Ollama Open-WebUI</title>
    <updated>2025-01-23T16:21:04+00:00</updated>
    <author>
      <name>/u/StuartMillington</name>
      <uri>https://old.reddit.com/user/StuartMillington</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to install and run Ollama and the Open-WebUI running in a VM on linux Ubuntu. I do NOT want to use Docker because it is an extra layer of complexity I can do without at this stage. My specifi problem is that although I have installed Ollama and the OpenWebUI, when I enter localhost:8080 into my browser, the response is that the localhost refused to connect. When I issue localhost:11434 the response is that Ollama is running. I have done the following:&lt;/p&gt; &lt;p&gt;Installed Ollama as per &lt;a href="https://itsfoss.com/ollama-setup-linux/"&gt;this link&lt;/a&gt; - Step #1 only. I have not tried to install any LLMs yet.&lt;/p&gt; &lt;p&gt;I installed the WebUI as per &lt;a href="https://docs.openwebui.com/getting-started/quick-start/"&gt;this link&lt;/a&gt; using the Python method onto a Linux machine. I have also update the application using Pip as per the same document. The problem comes when I try to launch the WebUI with localhost:8080 (for the Python installation). &lt;/p&gt; &lt;p&gt;Researching the problem on the internet suggests that there can be a problem if not all local interfaces are open to listening, as per &lt;a href="https://github.com/ollama/ollama/issues/703"&gt;this post&lt;/a&gt;. So I edited a service override file to add the following:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[Service] Environment=&amp;quot;OLLAMA_HOST=0.0.0.0&amp;quot; &amp;quot;OLLAMA_KEEP_ALIVE=-1&amp;quot; &amp;quot;OLLAMA_MAX_LOADED_MODELS=4&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;rather than edit the service daemon directly as per the advice by &lt;a href="https://github.com/nuaimat"&gt;nuaimat&lt;/a&gt;&lt;a href="https://github.com/ollama/ollama/issues/703#issuecomment-2325253842"&gt;on Sep 2, 2024&lt;/a&gt;. I then did the following:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo systemctl daemon-reload&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo service ollama restart&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo service ollama status&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The output of the last command is as follows:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama.service - Ollama Service&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Loaded: loaded (/etc/systemd/system/ollama.service; enabled; preset: enabled)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Drop-In: /etc/systemd/system/ollama.service.d&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;└─override.conf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Active: active (running) since Thu 2025-01-23 15:43:21 GMT; 33min ago&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Main PID: 1142 (ollama)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Tasks: 11 (limit: 23348)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Memory: 33.0M (peak: 33.5M)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;CPU: 271ms&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;CGroup: /system.slice/ollama.service&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;└─1142 /usr/local/bin/ollama serve&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jan 23 15:43:22 FRANK1 ollama[1142]: [GIN-debug] HEAD /api/version --&amp;gt;&lt;/code&gt; &lt;a href="http://github.com/ollama/ollama/server.(*Server"&gt;&lt;code&gt;github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2&lt;/code&gt;&lt;/a&gt;.GenerateRoutes.func2) &lt;code&gt;(5 handlers)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jan 23 15:43:22 FRANK1 ollama[1142]: time=2025-01-23T15:43:22.157Z level=INFO source=routes.go:1238 msg=&amp;quot;Listening on [::]:11434 (version 0.5.7)&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jan 23 15:43:22 FRANK1 ollama[1142]: time=2025-01-23T15:43:22.162Z level=INFO source=routes.go:1267 msg=&amp;quot;Dynamic LLM libraries&amp;quot; runners=&amp;quot;[cpu_avx2 cuda_v11_avx cuda_v12_avx rocm_avx cpu cpu&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jan 23 15:43:22 FRANK1 ollama[1142]: time=2025-01-23T15:43:22.165Z level=INFO source=gpu.go:226 msg=&amp;quot;looking for compatible GPUs&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jan 23 15:43:22 FRANK1 ollama[1142]: time=2025-01-23T15:43:22.212Z level=INFO source=gpu.go:392 msg=&amp;quot;no compatible GPUs were discovered&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jan 23 15:43:22 FRANK1 ollama[1142]: time=2025-01-23T15:43:22.212Z level=INFO source=types.go:131 msg=&amp;quot;inference compute&amp;quot; id=0 library=cpu variant=avx compute=&amp;quot;&amp;quot; driver=0.0 name=&amp;quot;&amp;quot; total=&amp;quot;1&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jan 23 15:52:11 FRANK1 ollama[1142]: [GIN] 2025/01/23 - 15:52:11 | 200 | 1.056082ms | ::1 | GET &amp;quot;/&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jan 23 15:52:11 FRANK1 ollama[1142]: [GIN] 2025/01/23 - 15:52:11 | 404 | 6.26µs | ::1 | GET &amp;quot;/favicon.ico&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jan 23 15:56:04 FRANK1 ollama[1142]: [GIN] 2025/01/23 - 15:56:04 | 200 | 50.966µs |&lt;/code&gt; &lt;a href="http://127.0.0.1"&gt;&lt;code&gt;127.0.0.1&lt;/code&gt;&lt;/a&gt; &lt;code&gt;| HEAD &amp;quot;/&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jan 23 15:56:04 FRANK1 ollama[1142]: [GIN] 2025/01/23 - 15:56:04 | 200 | 1.10902ms |&lt;/code&gt; &lt;a href="http://127.0.0.1"&gt;&lt;code&gt;127.0.0.1&lt;/code&gt;&lt;/a&gt; &lt;code&gt;| GET &amp;quot;/api/ps&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I feel that I am very close to getting this right, but there is something I am missing. Can anybody help, please. Regards, Stuart M&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StuartMillington"&gt; /u/StuartMillington &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8749h/problem_launching_ollama_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8749h/problem_launching_ollama_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8749h/problem_launching_ollama_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T16:21:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7ds9g</id>
    <title>It didn't even need to think to reply 😂 (deepseek-r1)</title>
    <updated>2025-01-22T15:33:52+00:00</updated>
    <author>
      <name>/u/EnoughVeterinarian90</name>
      <uri>https://old.reddit.com/user/EnoughVeterinarian90</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7ds9g/it_didnt_even_need_to_think_to_reply_deepseekr1/"&gt; &lt;img alt="It didn't even need to think to reply 😂 (deepseek-r1)" src="https://b.thumbs.redditmedia.com/bU5DA_X-RMAId5YJB4pzOMTQS9Hz0n0JNBOyXhHZi3E.jpg" title="It didn't even need to think to reply 😂 (deepseek-r1)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EnoughVeterinarian90"&gt; /u/EnoughVeterinarian90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i7ds9g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7ds9g/it_didnt_even_need_to_think_to_reply_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7ds9g/it_didnt_even_need_to_think_to_reply_deepseekr1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T15:33:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i87oru</id>
    <title>Struggling to run a RAQ application on my m1 mbp 16gb ram and 512ssd; model is ollama3.2 3b params</title>
    <updated>2025-01-23T16:45:03+00:00</updated>
    <author>
      <name>/u/Dasaboro</name>
      <uri>https://old.reddit.com/user/Dasaboro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using langflow since i'm new to python and every thing ai, but I can read in between the lines as I have a coding background&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dasaboro"&gt; /u/Dasaboro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i87oru/struggling_to_run_a_raq_application_on_my_m1_mbp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i87oru/struggling_to_run_a_raq_application_on_my_m1_mbp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i87oru/struggling_to_run_a_raq_application_on_my_m1_mbp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T16:45:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1i81q09</id>
    <title>Context with deepseek r1?</title>
    <updated>2025-01-23T11:57:37+00:00</updated>
    <author>
      <name>/u/joyfulsparrow</name>
      <uri>https://old.reddit.com/user/joyfulsparrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does deepseek r1 keep the context of what you're talking about, or is every question like new for it? For instance, if I ask a coding question, and then follow up with some bugs, it doesn't seem to remember the original question and answer. Is there a way to enable this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joyfulsparrow"&gt; /u/joyfulsparrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i81q09/context_with_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i81q09/context_with_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i81q09/context_with_deepseek_r1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T11:57:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i84th6</id>
    <title>Looking for a model that's good at recall/memory</title>
    <updated>2025-01-23T14:40:20+00:00</updated>
    <author>
      <name>/u/badaimbadjokes</name>
      <uri>https://old.reddit.com/user/badaimbadjokes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What I need most is a cognitive partner. It doesn't have to be all that smart, because I'm not all that smart. But what I REALLY want is for it to remember things. If I say, &amp;quot;Hey, what was that movie I added to my list the other day?&amp;quot; I want it to be able to say, &amp;quot;You wanted me to remember 'The Royal Tenenbaums.'&amp;quot; &lt;/p&gt; &lt;p&gt;Is there an ollama model that leans into this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/badaimbadjokes"&gt; /u/badaimbadjokes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i84th6/looking_for_a_model_thats_good_at_recallmemory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i84th6/looking_for_a_model_thats_good_at_recallmemory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i84th6/looking_for_a_model_thats_good_at_recallmemory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T14:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8c343</id>
    <title>How can I offload multiple models into ram instead of reloading from drive, if they do not all fit into vram?</title>
    <updated>2025-01-23T19:44:52+00:00</updated>
    <author>
      <name>/u/hawxxer</name>
      <uri>https://old.reddit.com/user/hawxxer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like the title say, I use multiple models &amp;quot;at the same time&amp;quot; switching between them. So I use one model for code completion and a bigger, different one, for code assist. Mostly there is another general conversation in the background. They don't fit all three into my vram, so I guess ollama is unloading the last active model, if i make a request on a different model, when they both wont fit into vram. Is there a way to store the current in use models in ram instead of reloading them from disk to speed up switching?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hawxxer"&gt; /u/hawxxer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8c343/how_can_i_offload_multiple_models_into_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8c343/how_can_i_offload_multiple_models_into_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8c343/how_can_i_offload_multiple_models_into_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T19:44:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i87mtt</id>
    <title>infinite download, ollama, deepseek models</title>
    <updated>2025-01-23T16:42:47+00:00</updated>
    <author>
      <name>/u/ext115</name>
      <uri>https://old.reddit.com/user/ext115</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can't download the language model with Ollama. While downloading the percentages go back and forth, the process takes hours until the session expires and it uses all my internet connections - cannot do anything with the internet. Do you have a similar problem?&lt;/p&gt; &lt;p&gt;ps&lt;br /&gt; i was only able to download smallest deepseek model deepseek-r1:1.5b &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ext115"&gt; /u/ext115 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i87mtt/infinite_download_ollama_deepseek_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i87mtt/infinite_download_ollama_deepseek_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i87mtt/infinite_download_ollama_deepseek_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T16:42:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8blli</id>
    <title>Issues running DeepSeek-R1</title>
    <updated>2025-01-23T19:24:30+00:00</updated>
    <author>
      <name>/u/branhama</name>
      <uri>https://old.reddit.com/user/branhama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So hearing all the good things about the model I wanted to try it out. Tried numerous version from 7b to 14b models and I get jibberish.&lt;/p&gt; &lt;p&gt;Usage: Development tasks within VSCode using Cline.&lt;/p&gt; &lt;p&gt;Request: Lets start a new project called ServerDocumentor. In this project begin a new powershell module which will be used to pull remote windows server information for documentation purposes.&lt;/p&gt; &lt;p&gt;LLM Response: First, I need to decide which programming language to use. Since the user didn't specify, I'll go with Python because it's widely used and straightforward for beginners.&lt;/p&gt; &lt;p&gt;Next, I should figure out where to save the file. The current working directory is C:/Users/abranham/Documents/testing, so I'll create the file there. Naming it hello_world.py makes sense because it's clear and follows common naming conventions.&lt;/p&gt; &lt;p&gt;Now, I need to write the code. In Python, printing &amp;quot;Hello World!&amp;quot; is simple. I'll use the print function: print(&amp;quot;Hello World!&amp;quot;). That should be the only line needed.&lt;/p&gt; &lt;p&gt;I don't have any open tabs in VSCode, so I'll create a new file. Using the create_file tool, I can specify the path and content. The path will&lt;/p&gt; &lt;p&gt;The system seems to be stuck on doing 2 tasks only, develop some python code or build a website. It never seems to follow anything I ask it to do.&lt;/p&gt; &lt;p&gt;I have ensured I am following the general guidlines for running at &lt;a href="https://github.com/deepseek-ai/DeepSeek-R1?tab=readme-ov-file#usage-recommendations"&gt;https://github.com/deepseek-ai/DeepSeek-R1?tab=readme-ov-file#usage-recommendations&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Any ideas? This is the first model I have run that acts like this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/branhama"&gt; /u/branhama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8blli/issues_running_deepseekr1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8blli/issues_running_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8blli/issues_running_deepseekr1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T19:24:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7yycc</id>
    <title>what can do now?</title>
    <updated>2025-01-23T08:33:23+00:00</updated>
    <author>
      <name>/u/Own-Perception-1574</name>
      <uri>https://old.reddit.com/user/Own-Perception-1574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7yycc/what_can_do_now/"&gt; &lt;img alt="what can do now?" src="https://preview.redd.it/y8xv9gwwfpee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f7701dbfaa233f2456f822ab6bf09b6cd5f6c5e" title="what can do now?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Perception-1574"&gt; /u/Own-Perception-1574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y8xv9gwwfpee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7yycc/what_can_do_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7yycc/what_can_do_now/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T08:33:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i87rs7</id>
    <title>Connect multiple Mac Minis for fast DeepSeek R1 performance</title>
    <updated>2025-01-23T16:48:30+00:00</updated>
    <author>
      <name>/u/Exciting-Syrup-1107</name>
      <uri>https://old.reddit.com/user/Exciting-Syrup-1107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there,&lt;/p&gt; &lt;p&gt;since using DeepSeek R1 I‘m baffled by the fact that I can use a nearly o1 level LLM on my local device. But it‘s just super slow. I have seen a YouTube video (i don‘t remember it exactly anymore) where a guy talks about connecting 2 or 3 Mac Minis to achieve a fast performing local DeepSeek R1.&lt;/p&gt; &lt;p&gt;My question now is: How can you do that? Are there certain tools that can „split“ the workload of ollama onto multiple devices? Are there online tools where you can rent fast computing space to run your own instance?&lt;/p&gt; &lt;p&gt;I‘m really curious about this topic and would love to learn more.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Exciting-Syrup-1107"&gt; /u/Exciting-Syrup-1107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i87rs7/connect_multiple_mac_minis_for_fast_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i87rs7/connect_multiple_mac_minis_for_fast_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i87rs7/connect_multiple_mac_minis_for_fast_deepseek_r1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T16:48:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7ygrp</id>
    <title>Good UI for DeepSeek R1</title>
    <updated>2025-01-23T07:56:24+00:00</updated>
    <author>
      <name>/u/AdAccomplished8942</name>
      <uri>https://old.reddit.com/user/AdAccomplished8942</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm running DeepSeek R1 (32b) on my Apple M3 MacBook Pro and it works pretty well, but I can only use it in Terminal because I haven't found a nice UI for it.&lt;/p&gt; &lt;p&gt;Msty looks really good and has great features, but it forgets previous prompts and answers. Basically every prompt is like a completely new chat, which makes many things impossible.&lt;/p&gt; &lt;p&gt;What are some nice UIs for DeepSeek R1 on ollama?&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdAccomplished8942"&gt; /u/AdAccomplished8942 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7ygrp/good_ui_for_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7ygrp/good_ui_for_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7ygrp/good_ui_for_deepseek_r1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T07:56:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7nqrj</id>
    <title>Run a fully local AI Search / RAG pipeline using Ollama with 4GB of memory and no GPU</title>
    <updated>2025-01-22T22:20:43+00:00</updated>
    <author>
      <name>/u/LeetTools</name>
      <uri>https://old.reddit.com/user/LeetTools</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, for people that want to run AI search and RAG pipelines locally, you can now build your local knowledge base with one line of command and everything runs locally with no docker or API key required. Repo is here: &lt;a href="https://github.com/leettools-dev/leettools"&gt;https://github.com/leettools-dev/leettools&lt;/a&gt;. The total memory usage is around 4GB with the Llama3.2 model: * llama3.2:latest 3.5 GB * nomic-embed-text:latest 370 MB * LeetTools: 350MB (Document pipeline backend with Python and DuckDB)&lt;/p&gt; &lt;p&gt;First, follow the instructions on &lt;a href="https://github.com/ollama/ollama"&gt;https://github.com/ollama/ollama&lt;/a&gt; to install the ollama program. Make sure the ollama program is running.&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;set up&lt;/h1&gt; &lt;p&gt;ollama pull llama3.2 ollama pull nomic-embed-text pip install leettools curl -fsSL -o .env.ollama &lt;a href="https://raw.githubusercontent.com/leettools-dev/leettools/refs/heads/main/env.ollama"&gt;https://raw.githubusercontent.com/leettools-dev/leettools/refs/heads/main/env.ollama&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;one command line to download a PDF and save it to the graphrag KB&lt;/h1&gt; &lt;p&gt;leet kb add-url -e .env.ollama -k graphrag -l info &lt;a href="https://arxiv.org/pdf/2501.09223"&gt;https://arxiv.org/pdf/2501.09223&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;now you query the local graphrag KB with questions&lt;/h1&gt; &lt;p&gt;leet flow -t answer -e .env.ollama -k graphrag -l info -p retriever_type=local -q &amp;quot;How does GraphRAG work?&amp;quot; ```&lt;/p&gt; &lt;p&gt;You can also add your local directory or files to the knowledge base using &lt;code&gt;leet kb add-local&lt;/code&gt; command.&lt;/p&gt; &lt;p&gt;For the above default setup, we are using * docling to convert PDF to markdown * chonkie as the chunker * nomic-embed-text as the embedding model * llama3.2 as the inference engine * Duckdb as the data storage include graph and vector&lt;/p&gt; &lt;p&gt;We think it might be helpful for some usage scenarios that require local deployment and resource limits. Questions or suggestions are welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeetTools"&gt; /u/LeetTools &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7nqrj/run_a_fully_local_ai_search_rag_pipeline_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7nqrj/run_a_fully_local_ai_search_rag_pipeline_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7nqrj/run_a_fully_local_ai_search_rag_pipeline_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T22:20:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8h7sd</id>
    <title>Param Count vs FP Precision</title>
    <updated>2025-01-23T23:23:25+00:00</updated>
    <author>
      <name>/u/PBlague</name>
      <uri>https://old.reddit.com/user/PBlague</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this is not an exact science and that models with the same amount of parameters and same floating point precision can have vastly different results based on the benchmark and how they've been trained and the model architecture. But I wanna know which one is gonna be generally better...&lt;/p&gt; &lt;p&gt;For example is the fp16 llama3.2 8b gonna be better than the q_2 llama3.2 70b? They have comparable amounts of data, somewhere around 25gbs.&lt;/p&gt; &lt;p&gt;I can't run the 70b with my current hardware and I doubt I will be able to run the fp16 8b either, but it seems more possible, so I was wondering if it's going to be comparable in accuracy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PBlague"&gt; /u/PBlague &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8h7sd/param_count_vs_fp_precision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8h7sd/param_count_vs_fp_precision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8h7sd/param_count_vs_fp_precision/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T23:23:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8f0cm</id>
    <title>I'm new to local LLMs but this is so inaccurate</title>
    <updated>2025-01-23T21:46:50+00:00</updated>
    <author>
      <name>/u/vsurresh</name>
      <uri>https://old.reddit.com/user/vsurresh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm new to running local LLMs and I'm only running the 8b version. I decided to try 'deepseek-r1:8b' and decided to ask a simple question. Looks like the training ended before the F1 championship was over but it says Lewis Hamilton has retired which is completely inaccurate. Is this type of response normal for locally running small LLMs? (screenshot attached) Thanks in advance&lt;/p&gt; &lt;p&gt;Edit - Adding the output&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; Who won 2024 F1 championship? &amp;lt;think&amp;gt; Okay, the user just asked, &amp;quot;Who won 2024 F1 championship?&amp;quot; Hmm, wait a minute. I remember that as of my last update in July 2024, the 2024 Formula 1 season was still ongoing. The races hadn't concluded yet, so it's not possible to determine the winner at that point. I should check if there have been any changes or updates since then. Let me think—did Lewis Hamilton win another championship? No, he retired from F1 in 2023. Max Verstappen won the 2023 championship but was involved in an incident at the 2024 Spanish Grand Prix where he crashed heavily. Wait, I'm getting confused because the user is asking about 2024, and my knowledge cutoff is July 2024. So, as of now, the championship is still being competed for, and the season hasn't ended yet. Therefore, the winner isn't determined until the end of the season. I should make sure to inform the user that the season is ongoing and the winner will be known after all races are completed. Also, I need to mention that Lewis Hamilton retired, so he's not competing anymore. It's important to provide accurate information without speculation since the championship isn't decided yet. &amp;lt;/think&amp;gt; As of my last update in July 2024, the 2024 Formula 1 season is still ongoing, and the champion has not yet been determined. The season concludes after all races are completed, so the winner will be known by the end of the year. Keep in mind that Lewis Hamilton retired from Formula 1 at the end of the 2023 season, so he is no longer competing. Max Verstappen won the 2023 championship but faced a challenging defense in 2024. The 2024 championship is still up for grabs, and it will be decided after the final race of the season. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vsurresh"&gt; /u/vsurresh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8f0cm/im_new_to_local_llms_but_this_is_so_inaccurate/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8f0cm/im_new_to_local_llms_but_this_is_so_inaccurate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8f0cm/im_new_to_local_llms_but_this_is_so_inaccurate/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T21:46:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8m6ls</id>
    <title>Llama 3.1 405B + 8x AMD Instinct Mi60 AI Server - Shockingly Good!</title>
    <updated>2025-01-24T03:26:28+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xzhz003o1vee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8m6ls/llama_31_405b_8x_amd_instinct_mi60_ai_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8m6ls/llama_31_405b_8x_amd_instinct_mi60_ai_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T03:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8k8u7</id>
    <title>I added ollama support for an open-source operator agent</title>
    <updated>2025-01-24T01:47:28+00:00</updated>
    <author>
      <name>/u/Swimming_Driver4974</name>
      <uri>https://old.reddit.com/user/Swimming_Driver4974</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created a similar Operator Agent framework like OpenAI's one, and made it open-source. Just added support for Ollama so it can be run completely locally using text and vision models. It's not perfect, but has potential: &lt;a href="https://github.com/GPT-Protocol/007-agent"&gt;https://github.com/GPT-Protocol/007-agent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Swimming_Driver4974"&gt; /u/Swimming_Driver4974 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8k8u7/i_added_ollama_support_for_an_opensource_operator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8k8u7/i_added_ollama_support_for_an_opensource_operator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8k8u7/i_added_ollama_support_for_an_opensource_operator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T01:47:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8lza7</id>
    <title>AI-Powered Bot for Automated Job Applications on LinkedIn</title>
    <updated>2025-01-24T03:15:40+00:00</updated>
    <author>
      <name>/u/Own-Perception-1574</name>
      <uri>https://old.reddit.com/user/Own-Perception-1574</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to develop an AI-powered bot capable of automating the job application process on LinkedIn. This bot would analyze job descriptions (JDs), customize the CV or resume based on the requirements, and apply to relevant positions automatically. Does a similar AI solution already exist in the market?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Perception-1574"&gt; /u/Own-Perception-1574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8lza7/aipowered_bot_for_automated_job_applications_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8lza7/aipowered_bot_for_automated_job_applications_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8lza7/aipowered_bot_for_automated_job_applications_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T03:15:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8fg88</id>
    <title>I Built an Open-Source RAG API for Docs, GitHub Issues and READMEs</title>
    <updated>2025-01-23T22:05:41+00:00</updated>
    <author>
      <name>/u/eleven-five</name>
      <uri>https://old.reddit.com/user/eleven-five</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been working on &lt;strong&gt;Ragpi&lt;/strong&gt;, an open-source AI assistant that builds knowledge bases from docs, GitHub Issues, and READMEs. It uses Redis Stack as a vector DB and leverages RAG to answer technical questions through an API.&lt;/p&gt; &lt;p&gt;Some things it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Creates knowledge bases from documentation websites, GitHub Issues, and READMEs&lt;/li&gt; &lt;li&gt;Uses hybrid search (semantic + keyword) for retrieval&lt;/li&gt; &lt;li&gt;Uses tool calling to dynamically search and retrieve relevant information during conversations&lt;/li&gt; &lt;li&gt;Works with OpenAI or Ollama&lt;/li&gt; &lt;li&gt;Provides a simple REST API for querying and managing sources&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Built with:&lt;/strong&gt; FastAPI, Redis Stack, and Celery.&lt;/p&gt; &lt;p&gt;It’s still a work in progress, but I’d love some feedback!&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/ragpi/ragpi"&gt;https://github.com/ragpi/ragpi&lt;/a&gt;&lt;br /&gt; API Reference: &lt;a href="https://docs.ragpi.io/"&gt;https://docs.ragpi.io&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eleven-five"&gt; /u/eleven-five &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8fg88/i_built_an_opensource_rag_api_for_docs_github/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8fg88/i_built_an_opensource_rag_api_for_docs_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8fg88/i_built_an_opensource_rag_api_for_docs_github/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T22:05:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8bhf3</id>
    <title>Upgraded!</title>
    <updated>2025-01-23T19:19:46+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i8bhf3/upgraded/"&gt; &lt;img alt="Upgraded!" src="https://preview.redd.it/8qdjrpbxmsee1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e72aeebb6bad9946b60f373d5e38bdfa97f84a8" title="Upgraded!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8qdjrpbxmsee1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8bhf3/upgraded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8bhf3/upgraded/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T19:19:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8n38z</id>
    <title>Coming soon: 100% Local Video Understanding Engine (an open-source project that can classify, caption, transcribe, and understand any video on your local device)</title>
    <updated>2025-01-24T04:17:00+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i8n38z/coming_soon_100_local_video_understanding_engine/"&gt; &lt;img alt="Coming soon: 100% Local Video Understanding Engine (an open-source project that can classify, caption, transcribe, and understand any video on your local device)" src="https://external-preview.redd.it/MDIwNzQ0ZzNidmVlMe21Biif0sGFU8GTsH3N7D_CJugYvIxsEVZ-nvrUed0U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8fa43786541e784dc36e99b0ae3b5e02c8a90ea9" title="Coming soon: 100% Local Video Understanding Engine (an open-source project that can classify, caption, transcribe, and understand any video on your local device)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4sh274g3bvee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8n38z/coming_soon_100_local_video_understanding_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8n38z/coming_soon_100_local_video_understanding_engine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T04:17:00+00:00</published>
  </entry>
</feed>
