<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-05-29T22:37:59+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1kwsf0v</id>
    <title>AI Presentation</title>
    <updated>2025-05-27T16:55:55+00:00</updated>
    <author>
      <name>/u/raghav-ai</name>
      <uri>https://old.reddit.com/user/raghav-ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any AI tool that can create ppt slides using ollama model, fully offline ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/raghav-ai"&gt; /u/raghav-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwsf0v/ai_presentation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwsf0v/ai_presentation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kwsf0v/ai_presentation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-27T16:55:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwzv9y</id>
    <title>Best type of computer to run</title>
    <updated>2025-05-27T21:46:47+00:00</updated>
    <author>
      <name>/u/MilaAmane</name>
      <uri>https://old.reddit.com/user/MilaAmane</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After tons of research, I decided I want to use a llama as my new ai. Do you do how much freedom there is?And it, in the fact that you don't have to have a monthly subscription. But I've heard you need to have a specific type of computer to be able to run specific features on it. And i'm trying to figure out which computer would be best suited. What I'm hoping to do with it is. I want to edit files to fix grammatical mistakes and writing stories with it similar to run to how chat gpt works that type of processor or something like that&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MilaAmane"&gt; /u/MilaAmane &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwzv9y/best_type_of_computer_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwzv9y/best_type_of_computer_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kwzv9y/best_type_of_computer_to_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-27T21:46:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxbq35</id>
    <title>LLM for text operations and manipulation</title>
    <updated>2025-05-28T08:16:42+00:00</updated>
    <author>
      <name>/u/Forward_Friend_2078</name>
      <uri>https://old.reddit.com/user/Forward_Friend_2078</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am building a solution that does string operations, simple math, intelligent conversion of unformatted dates, checking datatype of values in the variables.&lt;/p&gt; &lt;p&gt;What are some models that can be used for the above scenario?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Forward_Friend_2078"&gt; /u/Forward_Friend_2078 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxbq35/llm_for_text_operations_and_manipulation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxbq35/llm_for_text_operations_and_manipulation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kxbq35/llm_for_text_operations_and_manipulation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-28T08:16:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxc53y</id>
    <title>Ollama custom identifiers for models</title>
    <updated>2025-05-28T08:46:21+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HI,&lt;/p&gt; &lt;p&gt;I am using Ollama as a server for AI app.&lt;br /&gt; Is it possible to set a custom name for the model?&lt;br /&gt; If it s not, its a huge problem in different environments. If a local development uses for example 4B model, and production uses 32B model, the application has then different settings on dev and production. This is a problem when the configurations are in GIT and has to be transferred between environments. It would be really helpful to set a custom name for the model, and then both environments could set the same custom model name even they would actually run different models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxc53y/ollama_custom_identifiers_for_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxc53y/ollama_custom_identifiers_for_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kxc53y/ollama_custom_identifiers_for_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-28T08:46:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxa636</id>
    <title>Uncensored Chatbot Needed to run Locally</title>
    <updated>2025-05-28T06:31:46+00:00</updated>
    <author>
      <name>/u/Human_Celery_6637</name>
      <uri>https://old.reddit.com/user/Human_Celery_6637</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can anybody gimme a name of an uncensored LLM(no roleplay) for my M1 macbook air base (the models i know required 64gb ram and upwards and i dont have money to purchase another pc)&lt;/p&gt; &lt;p&gt;EDIT: GUYS I HAVE TRIED DOLPHIN-MISTRAL(NOT MIXTRAL) ITS NOT THAT UNCENSORED IF U COULD GIVE ME ONE MDEL FOR LOW END LAPTOPS ITS WOULD BE NICE&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Human_Celery_6637"&gt; /u/Human_Celery_6637 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxa636/uncensored_chatbot_needed_to_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxa636/uncensored_chatbot_needed_to_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kxa636/uncensored_chatbot_needed_to_run_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-28T06:31:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwm4cv</id>
    <title>D&amp;D Server</title>
    <updated>2025-05-27T12:34:55+00:00</updated>
    <author>
      <name>/u/barrulus</name>
      <uri>https://old.reddit.com/user/barrulus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So my son and I love to play D&amp;amp;D but have no one nearby who plays. Online play through D&amp;amp;d Beyond is possible but intimidating for him, so we practically never play. &lt;/p&gt; &lt;p&gt;Enter LLM’s!&lt;/p&gt; &lt;p&gt;This morning I opened up a chat with Gemma3 and gave it a simple prompt: “You are a Dungeon Master in a game of D&amp;amp;D. I am rogue halfling and [son] is chaotic wizard. We have just arrived at a harbour and walked into town, please treat this as a Session 0 style game”&lt;/p&gt; &lt;p&gt;We have been playing for hours now and having a great time! I am going to make this much more structured but what fun this is!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/barrulus"&gt; /u/barrulus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwm4cv/dd_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kwm4cv/dd_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kwm4cv/dd_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-27T12:34:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxgz0z</id>
    <title>Wanna create a workflow to read Engineering Drawing (pdf) and extract data in excel format</title>
    <updated>2025-05-28T13:22:49+00:00</updated>
    <author>
      <name>/u/Kohli01011</name>
      <uri>https://old.reddit.com/user/Kohli01011</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there..&lt;/p&gt; &lt;p&gt;I want to create a workflow using OCR, computer vision and recognition and llm to do feasibility analysis on those technical drawing.&lt;/p&gt; &lt;p&gt;Can any body help me in this ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kohli01011"&gt; /u/Kohli01011 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxgz0z/wanna_create_a_workflow_to_read_engineering/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxgz0z/wanna_create_a_workflow_to_read_engineering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kxgz0z/wanna_create_a_workflow_to_read_engineering/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-28T13:22:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxi0fs</id>
    <title>How do I get this kind of performance?</title>
    <updated>2025-05-28T14:07:31+00:00</updated>
    <author>
      <name>/u/jagauthier</name>
      <uri>https://old.reddit.com/user/jagauthier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kxi0fs/how_do_i_get_this_kind_of_performance/"&gt; &lt;img alt="How do I get this kind of performance?" src="https://external-preview.redd.it/Mt90tjYCKC9n53i1oiOKWNA0Qb4fIrcOqD1XYdCWeZ4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10e36df8b49d43cf090605824b61361d376a0972" title="How do I get this kind of performance?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 4x 3070 GPUs wtih 8G VRAM.&lt;/p&gt; &lt;p&gt;I've used this calculator:&lt;/p&gt; &lt;p&gt;&lt;a href="https://apxml.com/tools/vram-calculator"&gt;https://apxml.com/tools/vram-calculator&lt;/a&gt; &lt;/p&gt; &lt;p&gt;to calculate what it takes to run Gemma3:27B, the calculator gives me this info:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bvrb6suh3j3f1.png?width=389&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=72d94db6c773f1a6d182fa5946edb4e806063f57"&gt;https://preview.redd.it/bvrb6suh3j3f1.png?width=389&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=72d94db6c773f1a6d182fa5946edb4e806063f57&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However, after loading this model and running something simple &amp;quot;Give me a fun fact&amp;quot; Open-WebUI tells me my performance is this: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9yrw0w2o3j3f1.png?width=217&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddacfab15fa8197b8d58c7f36b1f34953758b91a"&gt;https://preview.redd.it/9yrw0w2o3j3f1.png?width=217&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddacfab15fa8197b8d58c7f36b1f34953758b91a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model is showing me this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;time=2025-05-28T13:52:25.923Z level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=63 layers.offload=62 layers.split=16,16,15,15 memory.available=&amp;quot;[7.5 GiB 7.5 GiB 7.5 GiB 7.5 GiB]&amp;quot; memory.gpu_overhead=&amp;quot;0 B&amp;quot; memory.required.full=&amp;quot;27.1 GiB&amp;quot; memory.required.partial=&amp;quot;24.3 GiB&amp;quot; memory.required.kv=&amp;quot;784.0 MiB&amp;quot; memory.required.allocations=&amp;quot;[6.2 GiB 6.2 GiB 5.9 GiB 5.9 GiB]&amp;quot; memory.weights.total=&amp;quot;15.4 GiB&amp;quot; memory.weights.repeating=&amp;quot;14.3 GiB&amp;quot; memory.weights.nonrepeating=&amp;quot;1.1 GiB&amp;quot; memory.graph.full=&amp;quot;1.6 GiB&amp;quot; memory.graph.partial=&amp;quot;1.6 GiB&amp;quot; projector.weights=&amp;quot;795.9 MiB&amp;quot; projector.graph=&amp;quot;1.0 GiB&amp;quot; time=2025-05-28T13:52:25.982Z level=INFO source=server.go:431 msg=&amp;quot;starting llama server&amp;quot; cmd=&amp;quot;/usr/bin/ollama runner --ollama-engine --model /root/.ollama/models/blobs/sha256-e796792eba26c4d3b04b0ac5adb01a453dd9ec2dfd83b6c59cbf6fe5f30b0f68 --ctx-size 2048 --batch-size 512 --n-gpu-layers 62 --threads 6 --parallel 1 --tensor-split 16,16,15,15 --port 37289&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And my GPU stats are:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;+-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 570.133.07 Driver Version: 570.133.07 CUDA Version: 12.8 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3070 Off | 00000000:03:00.0 Off | N/A | | 30% 33C P8 18W / 220W | 4459MiB / 8192MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 NVIDIA GeForce RTX 3070 Off | 00000000:04:00.0 Off | N/A | | 0% 45C P8 19W / 240W | 4293MiB / 8192MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 2 NVIDIA GeForce RTX 3070 Off | 00000000:07:00.0 Off | N/A | | 33% 34C P8 18W / 220W | 4053MiB / 8192MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 3 NVIDIA GeForce RTX 3070 Off | 00000000:09:00.0 On | N/A | | 0% 41C P8 13W / 220W | 4205MiB / 8192MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ +-----------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=========================================================================================| | 0 N/A N/A 2690348 C /usr/bin/ollama 4450MiB | | 1 N/A N/A 2690348 C /usr/bin/ollama 4284MiB | | 2 N/A N/A 2690348 C /usr/bin/ollama 4044MiB | | 3 N/A N/A 2690348 C /usr/bin/ollama 4190MiB | +-----------------------------------------------------------------------------------------+ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;One thing that seems interesting from the load messages is that maybe 1 layer isn't being loaded into VRAM, but I am not sure if that's what I am reading, and if so, why.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jagauthier"&gt; /u/jagauthier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxi0fs/how_do_i_get_this_kind_of_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxi0fs/how_do_i_get_this_kind_of_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kxi0fs/how_do_i_get_this_kind_of_performance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-28T14:07:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxg1tj</id>
    <title>Apparent difference between Local LLM responses when asking Terminal vs Python Script run in VS Code.</title>
    <updated>2025-05-28T12:40:26+00:00</updated>
    <author>
      <name>/u/iNick1</name>
      <uri>https://old.reddit.com/user/iNick1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So kind of new to all this but Ive been trying to develop a python script that: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Prompts user to select a word file&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Iterates through each word file to identify tables &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;converts each table to Markdown&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;prompts LLM for &amp;quot;Is this a Formulation card?&amp;quot; &amp;lt;Insert Markdown Table&amp;quot; Answer Yes or No. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Model used is deepseek-r1:1.5b which I recognise is not a great model but in testing using terminal appears to be okay. I wanted a model that could think a bit and explain its reasoning without setting my computer on fire. Other models, Gemma3, DeepseekR1:8b, Llama3 have all been tested but for now im just using deepseek-r1:1.5b installed via ollama. &lt;/p&gt; &lt;p&gt;So some really curious behaviour.. &lt;/p&gt; &lt;p&gt;When I ask directly in terminal: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;coding@Codings-MacBook-Pro ~ % ollama run deepseek-r1:1.5b &amp;gt;&amp;gt;&amp;gt; hi &amp;lt;think&amp;gt; &amp;lt;/think&amp;gt; Hello! How can I assist you today? 😊 &amp;gt;&amp;gt;&amp;gt; Review this Table and then answer is this a Formulation Card. Is this a formulation card? ... ... | Document control | Name and role | Date (dd-mmm-yyyy) | ... |:-----------------------|:-----------------------------------------------|:-----------------------------------------------| ... | Prepared by: | Nick | 17-Aug-2021 | ... | | Associate, EP Health, GSA | | ... | Reviewed by: | Nick | 18-Aug-2021 | ... | | Sr. Associate, EP-Health, GSA | | ... | Approved by: | Nick | 19-Aug-2021 | ... | | Manager, EP health, GSA | | ... | Document Version: | Final 1.0 | Final 1.0 | ... | Reason for Assessment: | To create a PSER for new TDS# 1000000 | To create a PSER for new TDS# 10000000 | ... | Additional Information | N/A | N/A | ... ... Please answer Yes or No. &amp;lt;think&amp;gt; Alright, let me see what the user is asking here. They provided this table with some data about document control details like who prepared it, when it was reviewed, and its version number. Then they asked if this is a Formulation Card and whether it's a formulation card. Hmm, so I need to figure out how to respond. First, I should understand what a Formulation Card typically includes. From what I remember, Formulation Cards are usually used in pharmaceutical settings, maybe for drug submissions or something similar. They often include information like the title, purpose, preparation details, dates, and any other relevant documents. Looking at the table provided, it seems to be about document control. It lists who prepared the document, when it was reviewed, approved by whom, version number, reason for assessment, and additional information. None of this looks like a Formulation Card. Maybe I should consider if there's another term that might relate here. Could &amp;quot;document control&amp;quot; be related to something else? Perhaps in project management or something similar? But even then, Formulation Cards are more about specific products or medications than document control. Also, the user mentioned a &amp;quot;PSER,&amp;quot; which could stand for Policy Statement on Resource Evaluation. That’s more about tracking resources and evaluations rather than formulating documents. I think it's safe to say that while there might be some overlap in terms like &amp;quot;document&amp;quot; and &amp;quot;control,&amp;quot; the specific structure of this table doesn't match what a Formulation Card entails. Therefore, I should respond with a No. &amp;lt;/think&amp;gt; No &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Pretty much a perfect response really. And this appears to be quite consistent having tried numerous times. &lt;/p&gt; &lt;p&gt;And then I try to call the same model via Python script. &lt;/p&gt; &lt;pre&gt;&lt;code&gt;Is this a Formulation Card? ... | Document control | Name and role | Date (dd-mmm-yyyy) | ... |:-----------------------|:-----------------------------------------------|:-----------------------------------------------| ... | Prepared by: | Nick | 17-Aug-2021 | ... | | Associate, EP Health, GSA | | ... | Reviewed by: | Nick | 18-Aug-2021 | ... | | Sr. Associate, EP-Health, GSA | | ... | Approved by: | Nick | 19-Aug-2021 | ... | | Manager, EP health, GSA | | ... | Document Version: | Final 1.0 | Final 1.0 | ... | Reason for Assessment: | To create a PSER for new TDS# 1000000 | To create a PSER for new TDS# 10000000 | ... | Additional Information | N/A | N/A | ... Please answer Yes or No. 🔎 Press Enter to ask the LLM using terminal... 🤖 Terminal LLM Response for Table 1: &amp;lt;think&amp;gt; Okay, so I have to figure out whether the given table is a Formulation Card. Let me think about this step by step. First, what's a Formulation Card? From what I remember, a Formulation Card usually contains information about chemical formulations. It often includes details like ingredients, their concentrations or amounts, and maybe some notes on how they were prepared or evaluated. The structure might be similar to the one provided in the table: headers for Document Control, Name and Role, Date, and sometimes additional information. Looking at the table, the columns are: 1. Document control 2. Name and role 3. Date (dd-mmm-yyyy) 4. Additional Information Hmm, that looks pretty much like a Formulation Card structure. The header rows are similar to those found in formulation cards, especially when there are multiple authors and dates involved. Let me check if the information makes sense. The document control probably refers to which document or report this is for. The name and role could indicate who prepared it, reviewed it, etc., like in a Formulation Card, you might have sections on Who Developed It, Who Reviewed It, etc. The date column seems straightforward—it's just when the document was created, so that makes sense for a Formulation Card. The additional information might be notes or other details beyond what's standard. In some cases, especially with more detailed or supplementary information, there could be an &amp;quot;Additional Information&amp;quot; section, which is present here. I should also consider if any of these are specific to a chemical formulation. Formulation Cards typically detail the chemicals involved in making a product, their quantities, and how they're formulated. The table seems to fit that description well—there's no mention of manufacturing processes or specific properties related to chemistry. Wait, but I'm not sure about the exact terminology here. Let me think: Formulation Card in some contexts refers to a document created when developing a chemical formulation for a product. It would include information on components, amounts, testing, etc. This table does have elements that match what you'd find in such a card. Another point is the structure: it's similar to the one I've seen before. The Date column and the additional info seem relevant for tracking changes or notes over time. That matches with Formulation Card structures that sometimes include versions or evaluation notes. I should also think about if this could be part of a recipe template instead. A recipe would have ingredients in specific quantities, but the dates might not be as important unless they're for batch preparation. However, since the table has both a Date column and Additional Information, it seems more like something designed for evaluation or assessment purposes rather than a standard recipe. Wait again, maybe it's a detailed instruction list. In that case, Formulation Card is used to specify all necessary ingredients, their amounts, and any specific notes on how they were prepared or tested. This table does have those aspects covered, just in a more condensed form with the structure of a Formulation Card. So, considering all these points, I think the table fits the structure and purpose of a Formulation Card. &amp;lt;/think&amp;gt; Yes, this table is indeed a Formulation Card. It follows the typical structure of a Formulation Card that includes Document Control, Name and Role, Date, and Additional Information. The content aligns with details like ingredient names, quantities, dates, and notes, fitting well within the framework of a chemical formulation document. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And Its Random and wrong in this case. If I ask it again I might be yes might be no. But terminal is pretty much always consistent. &lt;/p&gt; &lt;p&gt;Anyone have any idea whats going here? &lt;/p&gt; &lt;p&gt;Testing with larger models doesnt appear to improve this result. &lt;/p&gt; &lt;p&gt;Python Script: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;#!/usr/bin/env python3 import os import subprocess import tkinter as tk from tkinter import filedialog import pandas as pd from docx import Document # ───────────────────────────────────────────── # CONFIGURATION # ───────────────────────────────────────────── MODEL_NAME = &amp;quot;deepseek-r1:1.5b&amp;quot; OPEN_WORD = False # Now disabled by default # ───────────────────────────────────────────── # File picker # ───────────────────────────────────────────── def select_file(): root = tk.Tk() root.withdraw() file_path = filedialog.askopenfilename( title=&amp;quot;Select a Word document&amp;quot;, filetypes=[(&amp;quot;Word Documents&amp;quot;, &amp;quot;*.docx&amp;quot;)] ) if not file_path: print(&amp;quot;❌ No file selected. Exiting.&amp;quot;) exit() return file_path # ───────────────────────────────────────────── # Send prompt to terminal-based LLM using subprocess # ───────────────────────────────────────────── def call_llm_in_terminal(prompt): try: result = subprocess.run( [&amp;quot;ollama&amp;quot;, &amp;quot;run&amp;quot;, MODEL_NAME], input=prompt.encode(&amp;quot;utf-8&amp;quot;), stdout=subprocess.PIPE, stderr=subprocess.PIPE ) return result.stdout.decode(&amp;quot;utf-8&amp;quot;).strip() except Exception as e: return f&amp;quot;❌ Error calling LLM: {e}&amp;quot; # ───────────────────────────────────────────── # Main logic # ───────────────────────────────────────────── def main(): file_path = select_file() if OPEN_WORD: os.system(f&amp;quot;open '{file_path}'&amp;quot;) doc = Document(file_path) for i, table in enumerate(doc.tables): try: data = [[cell.text.strip() for cell in row.cells] for row in table.rows] df = pd.DataFrame(data) df = df.dropna(how='all').dropna(axis=1, how='all') if df.shape[0] &amp;lt; 2: continue df.columns = df.iloc[0] df = df[1:].reset_index(drop=True) markdown = df.to_markdown(index=False) prompt = f&amp;quot;&amp;quot;&amp;quot;Is this a Formulation Card? {markdown} Please answer Yes or No.&amp;quot;&amp;quot;&amp;quot; print(f&amp;quot;\n📝 Prompt for Table {i+1}:\n\n{prompt}\n&amp;quot;) input(&amp;quot;🔎 Press Enter to ask the LLM using terminal...&amp;quot;) result = call_llm_in_terminal(prompt) print(f&amp;quot;\n🤖 Terminal LLM Response for Table {i+1}:\n{result}\n&amp;quot;) input(&amp;quot;⏭️ Press Enter to continue to the next table...&amp;quot;) except Exception as e: print(f&amp;quot;⚠️ Error processing Table {i+1}: {e}&amp;quot;) # ───────────────────────────────────────────── # ENTRY POINT # ───────────────────────────────────────────── if __name__ == &amp;quot;__main__&amp;quot;: main() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Yes Script was built with Chat GPT im not a programmer. &lt;/p&gt; &lt;p&gt;Then as a last ditch attempt I explored to see if I could use AppleScript to directly use the terminal window and again the results started to be more consistent. &lt;/p&gt; &lt;p&gt;So there appears to be something that is different in how the script is asking the question to the model?&lt;/p&gt; &lt;p&gt;Thought this might be interesting for anyone doing something simialr as raises a slight alarm bell that the model may work but the script might cause an issue. Any thoughts appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iNick1"&gt; /u/iNick1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxg1tj/apparent_difference_between_local_llm_responses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxg1tj/apparent_difference_between_local_llm_responses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kxg1tj/apparent_difference_between_local_llm_responses/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-28T12:40:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxda05</id>
    <title>Local llm for small business request</title>
    <updated>2025-05-28T10:03:44+00:00</updated>
    <author>
      <name>/u/Ultra_running_fan</name>
      <uri>https://old.reddit.com/user/Ultra_running_fan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I run a small business and I'd like to automate some of the data processing to a llm and need it to be locally hosted due to data sharing issues etc. Would anyone be interested in contacting me directly to discuss working on this? I have very basic understanding of this so would need someone to guide and put together a system etc. we can discuss payment/price for time and whatever else etc. thanks in advance :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ultra_running_fan"&gt; /u/Ultra_running_fan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxda05/local_llm_for_small_business_request/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxda05/local_llm_for_small_business_request/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kxda05/local_llm_for_small_business_request/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-28T10:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxg442</id>
    <title>Bollama: simple ollama tui</title>
    <updated>2025-05-28T12:43:32+00:00</updated>
    <author>
      <name>/u/andrevdm_reddit</name>
      <uri>https://old.reddit.com/user/andrevdm_reddit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kxg442/bollama_simple_ollama_tui/"&gt; &lt;img alt="Bollama: simple ollama tui" src="https://preview.redd.it/sia4r29jqi3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=94dfb11fdff7f7130889860a2d7c2225f546a71a" title="Bollama: simple ollama tui" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TUI for Ollama – @Bollama@ – small, simple, maybe useful&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hey all – I made a little terminal UI for Ollama called &lt;strong&gt;Bollama&lt;/strong&gt;. It's nothing fancy and mostly built for myself to quickly test local models without needing to spin up a full UI or get lost in the CLI.&lt;/p&gt; &lt;p&gt;It supports chat, shows local models, show &amp;amp; stop running models.&lt;/p&gt; &lt;p&gt;If you're just trying to evaluate a few local models, it might come in handy.&lt;/p&gt; &lt;p&gt;⚠️ Not heavily supported, I'm not trying to compete with the bigger tools. It does what I need, and I figured maybe someone else might find it useful.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;🧪 What makes it different?&lt;/h3&gt; &lt;p&gt;Bollama is intentionally simple and aimed at quick evaluation of local models. I found other tools to be a bit heavy weigh or have the wrong focus for this. &lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;📦 Installation&lt;/h3&gt; &lt;p&gt;🛠️ There are &lt;a href="https://github.com/andrevdm/bollama/releases"&gt;prebuilt binaries for Linux and Windows&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/andrevdm/bollama"&gt;Github&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andrevdm_reddit"&gt; /u/andrevdm_reddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sia4r29jqi3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxg442/bollama_simple_ollama_tui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kxg442/bollama_simple_ollama_tui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-28T12:43:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxhj4v</id>
    <title>Connecting Ollama and Open WebUI in container to the internet?</title>
    <updated>2025-05-28T13:47:12+00:00</updated>
    <author>
      <name>/u/OriginalDiddi</name>
      <uri>https://old.reddit.com/user/OriginalDiddi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, Iam running a Ollama on my PC and a docker container with open webui. Open WebUI and Ollama are connected, so Iam using LLMs from Ollama in Open WebUI. &lt;/p&gt; &lt;p&gt;Now I want to connect Open WebUI to a certain website thats hosted in my network. How Iam going to do that and is it possible for Open WebUI or Ollama to read informations from the website?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OriginalDiddi"&gt; /u/OriginalDiddi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxhj4v/connecting_ollama_and_open_webui_in_container_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxhj4v/connecting_ollama_and_open_webui_in_container_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kxhj4v/connecting_ollama_and_open_webui_in_container_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-28T13:47:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ky9hs3</id>
    <title>I want to create a chatgpt like online service using opensource models, where to get started?</title>
    <updated>2025-05-29T12:26:53+00:00</updated>
    <author>
      <name>/u/abdojapan</name>
      <uri>https://old.reddit.com/user/abdojapan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am a computer engineer. I did some web apps even though that wasn't my main speciality, but I know how to create web apps mainly using express or PHP laravel and how to dockerize it.&lt;/p&gt; &lt;p&gt;I recently got into AI and I am fascinated with the potential. Now I want to create an online service like chatgpt with a fine tuned model for specific niche.&lt;/p&gt; &lt;p&gt;I know I can just use ollama and expose it publicly but I am sure there're a lot of nitty gritty stuff that some of you might hint at.&lt;/p&gt; &lt;p&gt;I will appreciate it if you can throw any ideas where to get started what are the challenges. Especially the following&lt;/p&gt; &lt;p&gt;- Which model's license allow for such use case?&lt;/p&gt; &lt;p&gt;- How to manage credits for users and integrate that with some payment either though appstore or something like paypal.&lt;/p&gt; &lt;p&gt;- Anything that might be uesful.&lt;/p&gt; &lt;p&gt;Thank you for advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdojapan"&gt; /u/abdojapan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ky9hs3/i_want_to_create_a_chatgpt_like_online_service/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ky9hs3/i_want_to_create_a_chatgpt_like_online_service/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ky9hs3/i_want_to_create_a_chatgpt_like_online_service/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T12:26:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxyi4x</id>
    <title>Which model do you recommend for M1 Pro, 32 GB Memory?</title>
    <updated>2025-05-29T01:32:06+00:00</updated>
    <author>
      <name>/u/more_muscle_aim</name>
      <uri>https://old.reddit.com/user/more_muscle_aim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello friends,&lt;/p&gt; &lt;p&gt;I’m new to LLM selection and was curious which is the appropriate model I can run to get the best results. &lt;/p&gt; &lt;p&gt;I’ll be mostly using the model for code generation / review, creating documentations, summarizing/generating MCQs/Indexing from PDF documents, etc.&lt;/p&gt; &lt;p&gt;I’m currently using gemma3:4b-iat-qat (randomly picked it. 😅). Not sure if it’s the best. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/more_muscle_aim"&gt; /u/more_muscle_aim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxyi4x/which_model_do_you_recommend_for_m1_pro_32_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxyi4x/which_model_do_you_recommend_for_m1_pro_32_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kxyi4x/which_model_do_you_recommend_for_m1_pro_32_gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T01:32:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kye015</id>
    <title>I need a model for adult SEO optimized content</title>
    <updated>2025-05-29T15:40:11+00:00</updated>
    <author>
      <name>/u/jimplementer</name>
      <uri>https://old.reddit.com/user/jimplementer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello.&lt;/p&gt; &lt;p&gt;I need a model who can write SEO-friendly descriptions for porn actors and categories for my adult video site.&lt;/p&gt; &lt;p&gt;Which model would you recommend?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jimplementer"&gt; /u/jimplementer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kye015/i_need_a_model_for_adult_seo_optimized_content/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kye015/i_need_a_model_for_adult_seo_optimized_content/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kye015/i_need_a_model_for_adult_seo_optimized_content/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T15:40:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxyav1</id>
    <title>Automate Your CSV Analysis with AI Agents – CrewAI + Ollama</title>
    <updated>2025-05-29T01:22:03+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kxyav1/automate_your_csv_analysis_with_ai_agents_crewai/"&gt; &lt;img alt="Automate Your CSV Analysis with AI Agents – CrewAI + Ollama" src="https://external-preview.redd.it/dmhwNXI2Y3RobTNmMV-krD7C_ZIFZ70xsLMOTqRPW4NjITnCLYEfMKf1vxnR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e66297828693b9b4a09c54595aa273083b9fabb" title="Automate Your CSV Analysis with AI Agents – CrewAI + Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever spent hours wrestling with messy CSVs and Excel sheets to find that one elusive insight? I just wrapped up a side project that might save you a ton of time:&lt;/p&gt; &lt;h1&gt;🚀 Automated Data Analysis with AI Agents&lt;/h1&gt; &lt;h1&gt;1️⃣ Effortless Data Ingestion&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Drop your customer-support ticket CSV into the pipeline&lt;/li&gt; &lt;li&gt;Agents spin up to parse, clean, and organize raw data&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2️⃣ Collaborative AI Agents at Work&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;🕵️‍♀️ Identify recurring issues &amp;amp; trending keywords&lt;/li&gt; &lt;li&gt;📈 Generate actionable insights on response times, ticket volumes, and more&lt;/li&gt; &lt;li&gt;💡 Propose concrete recommendations to boost customer satisfaction&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3️⃣ Polished, Shareable Reports&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Clean Markdown or PDF outputs&lt;/li&gt; &lt;li&gt;Charts, tables, and narrative summaries—ready to share with stakeholders&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🔧 Tech Stack Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mistral-Nemo&lt;/strong&gt; powering the NLP&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CrewAI&lt;/strong&gt; orchestrating parallel agents&lt;/li&gt; &lt;li&gt;&lt;strong&gt;100% open-source&lt;/strong&gt;, so you can fork and customize every step&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;👉 &lt;strong&gt;Check out the code &amp;amp; drop a&lt;/strong&gt; ⭐&lt;br /&gt; &lt;a href="https://github.com/Pavankunchala/LLM-Learn-PK/blob/main/AIAgent-CrewAi/customer_support/customer_support.py"&gt;https://github.com/Pavankunchala/LLM-Learn-PK/blob/main/AIAgent-CrewAi/customer_support/customer_support.py&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🚀 P.S. This project was a ton of fun, and I'm itching for my next AI challenge! If you or your team are doing innovative work in &lt;strong&gt;Computer Vision o&lt;/strong&gt;r LLMS and are looking for a passionate dev, I'd love to chat.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;My Email:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=mailto%3Apavankunchalaofficial%40gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My GitHub Profile (for more projects):&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My Resume:&lt;/strong&gt; &lt;a href="https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view"&gt;https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious to hear your thoughts, feedback, or feature ideas. What AI agent workflows do you wish existed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/q96916cthm3f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxyav1/automate_your_csv_analysis_with_ai_agents_crewai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kxyav1/automate_your_csv_analysis_with_ai_agents_crewai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T01:22:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxdddd</id>
    <title>We believe the future of AI is local, private, and personalized.</title>
    <updated>2025-05-28T10:09:55+00:00</updated>
    <author>
      <name>/u/NightShade4275</name>
      <uri>https://old.reddit.com/user/NightShade4275</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That’s why we built &lt;strong&gt;Cobolt&lt;/strong&gt; — a free cross-platform AI assistant that runs entirely on your device.&lt;/p&gt; &lt;p&gt;Cobolt represents our vision for the future of AI assistants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🔒 &lt;strong&gt;Privacy-first by design&lt;/strong&gt; — everything runs locally&lt;/li&gt; &lt;li&gt;🔧 &lt;strong&gt;Extensible&lt;/strong&gt; with our open Model Context Protocol (MCP)&lt;/li&gt; &lt;li&gt;⚙️ &lt;strong&gt;Powered by Ollama&lt;/strong&gt; for smooth performance&lt;/li&gt; &lt;li&gt;🧠 &lt;strong&gt;Personalized&lt;/strong&gt; without sending your data to the cloud&lt;/li&gt; &lt;li&gt;🤝 &lt;strong&gt;Built by the community&lt;/strong&gt;, for the community&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We're looking for contributors, testers, and fellow privacy advocates to join us in building the future of personal AI.&lt;/p&gt; &lt;p&gt;🤝 Contributions Welcome! 🌟 Star us on &lt;a href="https://github.com/platinum-hill/cobolt"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;📥 Try Cobolt on macOS or Windows or Linux. 🎉 &lt;a href="https://github.com/platinum-hill/cobolt?tab=readme-ov-file#getting-started"&gt;Get started here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let's build AI that serves you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NightShade4275"&gt; /u/NightShade4275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxdddd/we_believe_the_future_of_ai_is_local_private_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxdddd/we_believe_the_future_of_ai_is_local_private_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kxdddd/we_believe_the_future_of_ai_is_local_private_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-28T10:09:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxh2tv</id>
    <title>i got tired of the errors, so automated debugging using Ollama</title>
    <updated>2025-05-28T13:27:30+00:00</updated>
    <author>
      <name>/u/AntelopeEntire9191</name>
      <uri>https://old.reddit.com/user/AntelopeEntire9191</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kxh2tv/i_got_tired_of_the_errors_so_automated_debugging/"&gt; &lt;img alt="i got tired of the errors, so automated debugging using Ollama" src="https://external-preview.redd.it/bHI2dDhoeGd4aTNmMezr6njSDoFLFdrxC2JtmiXopTq_OV4L1dh4D_3UKGf5.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=68ebbe326e350760cd92cb52a81c12c6ce093c4a" title="i got tired of the errors, so automated debugging using Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got tired of debugging the same Python errors over and over, so I built a CLI the past 2 months that auto-fixes them with local LLMs&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR: Terminal errors → automatic fixes using your Ollama models + RAG across your entire codebase. 100% local&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You know when you see `&lt;code&gt;AttributeError\&lt;/code&gt;`for the 69th time? This catches those errors automatically and fixes them using:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Your local Ollama models (whatever you have downloaded)&lt;/li&gt; &lt;li&gt;RAG across your entire codebase for context&lt;/li&gt; &lt;li&gt;Everything stays on your machine&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Just integrated &lt;strong&gt;Claude 4&lt;/strong&gt; support aswell and it's genuinely scary good at debugging tbh&lt;/p&gt; &lt;p&gt;If you curious to see the implementation, its open source: &lt;a href="https://github.com/cloi-ai/cloi"&gt;https://github.com/cloi-ai/cloi&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AntelopeEntire9191"&gt; /u/AntelopeEntire9191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4x302fxgxi3f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kxh2tv/i_got_tired_of_the_errors_so_automated_debugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kxh2tv/i_got_tired_of_the_errors_so_automated_debugging/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-28T13:27:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyc8yg</id>
    <title>Online services that host ollama models?</title>
    <updated>2025-05-29T14:30:09+00:00</updated>
    <author>
      <name>/u/azimux</name>
      <uri>https://old.reddit.com/user/azimux</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey hey!&lt;/p&gt; &lt;p&gt;A recent upgrade of ollama results in my system rebooting if I use any models bigger than about 10GB in size. I'll probably try just rebuilding that whole machine to see if it alleviates the problem.&lt;/p&gt; &lt;p&gt;But made me realize... perhaps I should just pay for a service that hosts ollama models. This would allow me to access bigger models (I only have 24GB vram) and also save me time when upgrades go poorly.&lt;/p&gt; &lt;p&gt;Any recommendations for such a service?&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/azimux"&gt; /u/azimux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kyc8yg/online_services_that_host_ollama_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kyc8yg/online_services_that_host_ollama_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kyc8yg/online_services_that_host_ollama_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T14:30:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyctco</id>
    <title>Hackathon Idea : Build Your Own Internal Agent using C/ua</title>
    <updated>2025-05-29T14:52:52+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kyctco/hackathon_idea_build_your_own_internal_agent/"&gt; &lt;img alt="Hackathon Idea : Build Your Own Internal Agent using C/ua" src="https://external-preview.redd.it/eWVsN2g5MWlpcTNmMcuFUaVWr4ahtnKH-Dec5LgathfQnwmox07cDNY_yMlL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a69d0d0fcdbb7692436cea7eba2309f994bdd06a" title="Hackathon Idea : Build Your Own Internal Agent using C/ua" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Soon every employee will have their own AI agent handling the repetitive, mundane parts of their job, freeing them to focus on what they're uniquely good at. &lt;/p&gt; &lt;p&gt;Going through YC's recent Request for Startups, I am trying to build an internal agent builder for employees using c/ua.&lt;/p&gt; &lt;p&gt;C/ua provides a infrastructure to securely automate workflows using macOS and Linux containers on Apple Silicon.&lt;/p&gt; &lt;p&gt;We would try to make it work smoothly with everyday tools like your browser, IDE or Slack all while keeping permissions tight and handling sensitive data securely using the latest LLMs.&lt;/p&gt; &lt;p&gt;Github Link : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/of61wceiiq3f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kyctco/hackathon_idea_build_your_own_internal_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kyctco/hackathon_idea_build_your_own_internal_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T14:52:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1kydwb5</id>
    <title>What cool ways can u use your local llm?</title>
    <updated>2025-05-29T15:36:06+00:00</updated>
    <author>
      <name>/u/DOK10101</name>
      <uri>https://old.reddit.com/user/DOK10101</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DOK10101"&gt; /u/DOK10101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kydwb5/what_cool_ways_can_u_use_your_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kydwb5/what_cool_ways_can_u_use_your_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kydwb5/what_cool_ways_can_u_use_your_local_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T15:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyigpx</id>
    <title>Trying to read between the lines for Llama 4, how powerful of a machine is required?</title>
    <updated>2025-05-29T18:36:20+00:00</updated>
    <author>
      <name>/u/MrBlinko47</name>
      <uri>https://old.reddit.com/user/MrBlinko47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to understand if my computer can run Llama 4. I remember seeing a post about a rule of thumb for the amount of parameters to the amount of vram required.&lt;/p&gt; &lt;p&gt;Anyone have experience with Llama 4?&lt;/p&gt; &lt;p&gt;I have a 4080 Super so not sure if that is enough to power this model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrBlinko47"&gt; /u/MrBlinko47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kyigpx/trying_to_read_between_the_lines_for_llama_4_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kyigpx/trying_to_read_between_the_lines_for_llama_4_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kyigpx/trying_to_read_between_the_lines_for_llama_4_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T18:36:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ky965w</id>
    <title>GitHub - adeelahmad/mlx-grpo: 🧠 Train your own DeepSeek-R1 style reasoning model on Mac! First MLX implementation of GRPO - the breakthrough technique behind R1's o1-matching performance. Build mathematical reasoning AI without expensive RLHF. Apple Silicon optimized. 🚀</title>
    <updated>2025-05-29T12:10:09+00:00</updated>
    <author>
      <name>/u/adeelahmadch</name>
      <uri>https://old.reddit.com/user/adeelahmadch</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ky965w/github_adeelahmadmlxgrpo_train_your_own/"&gt; &lt;img alt="GitHub - adeelahmad/mlx-grpo: 🧠 Train your own DeepSeek-R1 style reasoning model on Mac! First MLX implementation of GRPO - the breakthrough technique behind R1's o1-matching performance. Build mathematical reasoning AI without expensive RLHF. Apple Silicon optimized. 🚀" src="https://external-preview.redd.it/4bujr2a9Rx-_7aFLAdHGHINXunCPwtFr2Yoq_Rriz8Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7b8787bb254f32fd8fbaf8bcac12c6686e69b828" title="GitHub - adeelahmad/mlx-grpo: 🧠 Train your own DeepSeek-R1 style reasoning model on Mac! First MLX implementation of GRPO - the breakthrough technique behind R1's o1-matching performance. Build mathematical reasoning AI without expensive RLHF. Apple Silicon optimized. 🚀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adeelahmadch"&gt; /u/adeelahmadch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/adeelahmad/mlx-grpo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ky965w/github_adeelahmadmlxgrpo_train_your_own/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ky965w/github_adeelahmadmlxgrpo_train_your_own/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T12:10:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyg2bb</id>
    <title>Dual 3090 Build for Inference Questions</title>
    <updated>2025-05-29T17:01:52+00:00</updated>
    <author>
      <name>/u/rhh4x0r</name>
      <uri>https://old.reddit.com/user/rhh4x0r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been scouring the posts here to figure out what might be the best build for local llm inference / homelab server.&lt;/p&gt; &lt;p&gt;I'm picking up 2 RTX 3090s, but I've got the rest of my build to make.&lt;/p&gt; &lt;p&gt;Budget around $1500 for the remaining components. What would you use?&lt;/p&gt; &lt;p&gt;I'm looking at a Ryen 7950, and know I should probably get a 1500W PSU just to be safe. What thoughts you have on processor/mobo/RAM here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rhh4x0r"&gt; /u/rhh4x0r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kyg2bb/dual_3090_build_for_inference_questions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kyg2bb/dual_3090_build_for_inference_questions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kyg2bb/dual_3090_build_for_inference_questions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T17:01:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1kymosf</id>
    <title>I built a local email summary dashboard</title>
    <updated>2025-05-29T21:25:25+00:00</updated>
    <author>
      <name>/u/vishruth555</name>
      <uri>https://old.reddit.com/user/vishruth555</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kymosf/i_built_a_local_email_summary_dashboard/"&gt; &lt;img alt="I built a local email summary dashboard" src="https://preview.redd.it/6p78z7ujgs3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=367bc299f4e2cf14d71a166dbe4e68f63942f8bf" title="I built a local email summary dashboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I often forget to check my emails, so I developed a tool that summarizes my inbox into a concise dashboard.&lt;/p&gt; &lt;p&gt;Features: • Runs locally using Ollama, Gemini api key can also be used for faster summaries at the cost of your privacy &lt;/p&gt; &lt;pre&gt;&lt;code&gt;• Summarizes Gmail inboxes into a clean, readable format • can be run in a container &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Check it out here: &lt;a href="https://github.com/vishruth555/mailBrief"&gt;https://github.com/vishruth555/mailBrief&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’d love to hear your feedback or suggestions for improvement!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vishruth555"&gt; /u/vishruth555 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6p78z7ujgs3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kymosf/i_built_a_local_email_summary_dashboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kymosf/i_built_a_local_email_summary_dashboard/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T21:25:25+00:00</published>
  </entry>
</feed>
