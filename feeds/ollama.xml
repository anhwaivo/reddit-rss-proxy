<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-05-09T13:29:58+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1kgdhjg</id>
    <title>Modelos de embedding para textos largos de ollama</title>
    <updated>2025-05-06T19:34:20+00:00</updated>
    <author>
      <name>/u/Effective_Budget7594</name>
      <uri>https://old.reddit.com/user/Effective_Budget7594</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for embedding templates for long texts. I've tried some but none fits the precision I need, I need precision but it can't take too long. It is for a chatbot to answer questions about the company, the product, the operation of the device, the instructions, the problems, the doubts and so on. Can you recommend one to me? Which one do you use? Do you have any tips for it to improve?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective_Budget7594"&gt; /u/Effective_Budget7594 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgdhjg/modelos_de_embedding_para_textos_largos_de_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgdhjg/modelos_de_embedding_para_textos_largos_de_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kgdhjg/modelos_de_embedding_para_textos_largos_de_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-06T19:34:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfqbcr</id>
    <title>Ollama-based Real-time AI Voice Chat at ~500ms Latency</title>
    <updated>2025-05-05T23:32:57+00:00</updated>
    <author>
      <name>/u/Lonligrin</name>
      <uri>https://old.reddit.com/user/Lonligrin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kfqbcr/ollamabased_realtime_ai_voice_chat_at_500ms/"&gt; &lt;img alt="Ollama-based Real-time AI Voice Chat at ~500ms Latency" src="https://external-preview.redd.it/FrSeQHAfH8ucYuqX6ERN969lQMmZVCPB5a9bZoQMkEk.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52de6f797c358b77b4905f13d71e5458e1d5d320" title="Ollama-based Real-time AI Voice Chat at ~500ms Latency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built RealtimeVoiceChat because I was frustrated with the latency in most voice AI interactions. This is an open-source (MIT license) system designed for real-time, local voice conversations with LLMs.&lt;/p&gt; &lt;p&gt;I wanted to get one step closer to natural conversation speed with a system that responses back with around 500ms latency. &lt;/p&gt; &lt;p&gt;Key aspects: Designed for local LLMs (Ollama primarily, OpenAI connector included). Interruptible conversation. Turn detection to avoid cutting the user off mid-thought. Dockerized setup available.&lt;/p&gt; &lt;p&gt;It requires a decent CUDA-enabled GPU for good performance due to the STT/TTS models.&lt;/p&gt; &lt;p&gt;Would love to hear your feedback on the approach, performance, potential optimizations, or any features you think are essential for a good local voice AI experience.&lt;/p&gt; &lt;p&gt;The code is here: &lt;a href="https://github.com/KoljaB/RealtimeVoiceChat"&gt;https://github.com/KoljaB/RealtimeVoiceChat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lonligrin"&gt; /u/Lonligrin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=HM_IQuuuPX8&amp;amp;si=R5zzcLV32SOOUCq7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kfqbcr/ollamabased_realtime_ai_voice_chat_at_500ms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kfqbcr/ollamabased_realtime_ai_voice_chat_at_500ms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-05T23:32:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgg89i</id>
    <title>I've created a Discord bot that connects to ollama to send prompts via discord messages</title>
    <updated>2025-05-06T21:26:09+00:00</updated>
    <author>
      <name>/u/Robots_Never_Die</name>
      <uri>https://old.reddit.com/user/Robots_Never_Die</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kgg89i/ive_created_a_discord_bot_that_connects_to_ollama/"&gt; &lt;img alt="I've created a Discord bot that connects to ollama to send prompts via discord messages" src="https://external-preview.redd.it/rERPtzmaFZvs8DJiIbD5lheGvo1fx56ZtArkhs27KlE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3c7ca00f890a60658b5aecc7a4146d0d68225fd" title="I've created a Discord bot that connects to ollama to send prompts via discord messages" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the first software I've developed and looking to share it. &lt;/p&gt; &lt;p&gt;Silas Blue is a versatile Discord bot powered by local AI models through Ollama. It allows you to bring powerful AI capabilities directly to your Discord server without relying on external API services, ensuring privacy and control over your data.&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local AI Processing&lt;/strong&gt;: Runs AI models locally through Ollama for privacy and control&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Model Support&lt;/strong&gt;: Compatible with various Ollama models (Gemma, Llama, etc.)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord Integration&lt;/strong&gt;: Seamless interaction within your server channels&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Server-Specific Configuration&lt;/strong&gt;: Customize settings per Discord server&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Permission Management&lt;/strong&gt;: Control who can use which features&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Automatic Restart Option&lt;/strong&gt;: Optional scheduled restarts for stability&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Paginated Responses&lt;/strong&gt;: Clean formatting for longer AI responses&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Terminal Control Interface&lt;/strong&gt;: Manage your bot settings via terminal commands&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simple Command Structure&lt;/strong&gt;: Interact using &lt;code&gt;!&lt;/code&gt; prefix or by tagging the bot&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Requirements&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Python 3&lt;/strong&gt;: &lt;a href="https://www.python.org/downloads"&gt;Download from python.org&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt;: &lt;a href="https://ollama.com/download"&gt;Download from ollama.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Python Libraries&lt;/strong&gt;: &lt;a href="http://Discord.py"&gt;Discord.py&lt;/a&gt;, aiohttp, asyncio, colorama&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord Developer Account&lt;/strong&gt;: You'll need to create an application in the &lt;a href="https://discord.com/developers/applications"&gt;Discord Developer Portal&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord Bot Token&lt;/strong&gt;: Generate a private token for your bot through the Developer Portal&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Detailed Setup Instructions&lt;/h1&gt; &lt;h1&gt;Installing Python and Required Libraries&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Install Python 3&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Visit &lt;a href="https://www.python.org/downloads"&gt;python.org/downloads&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Download the latest version for your operating system&lt;/li&gt; &lt;li&gt;During installation, &lt;strong&gt;make sure to check the box&lt;/strong&gt; &amp;quot;Add Python to PATH&amp;quot;&lt;/li&gt; &lt;li&gt;Complete the installation wizard&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Install Required Python Libraries&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Open a command prompt or terminal&lt;/li&gt; &lt;li&gt;For Windows (Run as Administrator):py -3 -m pip install -U &lt;a href="http://discord.py"&gt;discord.py&lt;/a&gt; aiohttp asyncio colorama&lt;/li&gt; &lt;li&gt;For macOS/Linux:python3 -m pip install -U &lt;a href="http://discord.py"&gt;discord.py&lt;/a&gt; aiohttp asyncio colorama&lt;/li&gt; &lt;li&gt;Wait for the installation to complete&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Setting Up Ollama and Models&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Install Ollama&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Visit &lt;a href="https://ollama.com/download"&gt;ollama.com/download&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Download and install the version for your operating system&lt;/li&gt; &lt;li&gt;Follow the installation prompts&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verify Ollama Installation&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Open a terminal or command prompt&lt;/li&gt; &lt;li&gt;Type: &lt;code&gt;ollama --version&lt;/code&gt;&lt;/li&gt; &lt;li&gt;You should see the version number displayed&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Start Ollama Service&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;In your terminal, run: &lt;code&gt;ollama serve&lt;/code&gt;&lt;/li&gt; &lt;li&gt;This starts the Ollama service in the background&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Download AI Models&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;In a new terminal window, download your preferred models:&lt;/li&gt; &lt;li&gt;For example: &lt;code&gt;ollama pull gemma3:1b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;You can find more models at &lt;a href="https://ollama.com/search"&gt;ollama.com/search&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Creating a Discord Bot&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Create a Discord Account&lt;/strong&gt; (skip if you already have one): &lt;ul&gt; &lt;li&gt;Visit &lt;a href="https://discord.com/register"&gt;discord.com/register&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Complete the registration process&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Access the Discord Developer Portal&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Go to &lt;a href="https://discord.com/developers/applications"&gt;discord.com/developers/applications&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Log in with your Discord account&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Create a New Application&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Click the &amp;quot;New Application&amp;quot; button in the top-right corner&lt;/li&gt; &lt;li&gt;Enter a name for your bot (e.g., &amp;quot;Silas Blue&amp;quot;)&lt;/li&gt; &lt;li&gt;Accept the terms and click &amp;quot;Create&amp;quot;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Configure Bot Settings&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;In the left sidebar, click &amp;quot;Bot&amp;quot;&lt;/li&gt; &lt;li&gt;Click &amp;quot;Add Bot&amp;quot; and confirm with &amp;quot;Yes, do it!&amp;quot;&lt;/li&gt; &lt;li&gt;Under the username section, you'll see your bot's profile&lt;/li&gt; &lt;li&gt;Toggle on these recommended settings: &lt;ul&gt; &lt;li&gt;&amp;quot;PUBLIC BOT&amp;quot; (if you want others to invite it)&lt;/li&gt; &lt;li&gt;&amp;quot;MESSAGE CONTENT INTENT&amp;quot; (required for the bot to read messages)&lt;/li&gt; &lt;li&gt;&amp;quot;PRESENCE INTENT&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;SERVER MEMBERS INTENT&amp;quot;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Get Your Bot Token&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;In the &amp;quot;Bot&amp;quot; section, click &amp;quot;Reset Token&amp;quot; and confirm&lt;/li&gt; &lt;li&gt;Copy the displayed token (this is your private bot token)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt;: Never share this token publicly - it grants control of your bot&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generate Invite Link&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;In the left sidebar, click &amp;quot;OAuth2&amp;quot; then &amp;quot;URL Generator&amp;quot;&lt;/li&gt; &lt;li&gt;Under &amp;quot;SCOPES&amp;quot;, select &amp;quot;bot&amp;quot;&lt;/li&gt; &lt;li&gt;Under &amp;quot;BOT PERMISSIONS&amp;quot;, select: &lt;ul&gt; &lt;li&gt;&amp;quot;Read Messages/View Channels&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Send Messages&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Embed Links&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Attach Files&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Read Message History&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Add Reactions&amp;quot;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Copy the generated URL from the bottom of the page&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Invite Bot to Your Server&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Paste the URL in your browser&lt;/li&gt; &lt;li&gt;Select your server from the dropdown&lt;/li&gt; &lt;li&gt;Click &amp;quot;Authorize&amp;quot; and complete any verification&lt;/li&gt; &lt;li&gt;Your bot will now appear in your server member list (likely offline until you run it)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Running Silas Blue&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Download Silas Blue&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Download and extract the &lt;a href="https://github.com/RBND/Silus-Blue/releases/latest"&gt;Silas Blue files&lt;/a&gt; to a folder on your computer&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Launch the Bot&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Open a terminal in the folder containing the bot files&lt;/li&gt; &lt;li&gt;To run with auto-restart: &lt;code&gt;python&lt;/code&gt; &lt;a href="http://starter.py"&gt;&lt;code&gt;starter.py&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;To run without auto-restart: &lt;code&gt;python&lt;/code&gt; &lt;a href="http://SilasBlue.py"&gt;&lt;code&gt;SilasBlue.py&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;First-Time Setup&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;When prompted, paste your Discord bot token&lt;/li&gt; &lt;li&gt;The bot will connect to Discord and display connection information&lt;/li&gt; &lt;li&gt;You'll see configuration information for any servers the bot has joined&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Using the Bot&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Interact with the bot in Discord using &lt;code&gt;!command&lt;/code&gt; or by tagging &lt;code&gt;@SilasBlue command&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Type &lt;code&gt;!help&lt;/code&gt; or &lt;code&gt;@SilasBlue help&lt;/code&gt; to see available commands&lt;/li&gt; &lt;li&gt;Use terminal commands for advanced configuration (type &lt;code&gt;Help&lt;/code&gt; in the terminal)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Terminal Commands&lt;/h1&gt; &lt;p&gt;Silas Blue offers a powerful terminal interface for configuration:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;help&lt;/code&gt; - Display all available commands&lt;/li&gt; &lt;li&gt;&lt;code&gt;servers&lt;/code&gt; - List all connected servers&lt;/li&gt; &lt;li&gt;&lt;code&gt;server &amp;lt;server_id&amp;gt;&lt;/code&gt; - View configuration for a specific server&lt;/li&gt; &lt;li&gt;&lt;code&gt;edit &amp;lt;server_id&amp;gt; &amp;lt;setting&amp;gt; &amp;lt;value&amp;gt;&lt;/code&gt; - Edit server settings&lt;/li&gt; &lt;li&gt;&lt;code&gt;permissions &amp;lt;server_id&amp;gt; &amp;lt;action&amp;gt; &amp;lt;permission_type&amp;gt;&lt;/code&gt; - Manage permissions&lt;/li&gt; &lt;li&gt;&lt;code&gt;token [new_token|show]&lt;/code&gt; - Change or view the Discord token&lt;/li&gt; &lt;li&gt;&lt;code&gt;restart&lt;/code&gt; - Restart the bot&lt;/li&gt; &lt;li&gt;&lt;code&gt;shutdown&lt;/code&gt; - Shut down the bot&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Keeping Your Bot Updated&lt;/h1&gt; &lt;p&gt;When updating to a new version of Silas Blue:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Keep your &lt;code&gt;bot_config.pkl&lt;/code&gt; and &lt;code&gt;token.txt&lt;/code&gt; files&lt;/li&gt; &lt;li&gt;Replace all other files with the new version&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Need Help?&lt;/h1&gt; &lt;p&gt;Contact RobotsNeverDie via &lt;a href="https://discord.com/users/296353246920835074"&gt;Discord&lt;/a&gt; (preferred) or &lt;a href="https://www.reddit.com/user/Robots_Never_Die/"&gt;Reddit&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Robots_Never_Die"&gt; /u/Robots_Never_Die &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/RBND/Silus-Blue"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgg89i/ive_created_a_discord_bot_that_connects_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kgg89i/ive_created_a_discord_bot_that_connects_to_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-06T21:26:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgoqll</id>
    <title>Best battery efficent ai model for i7 4710mq thinkpad</title>
    <updated>2025-05-07T04:26:33+00:00</updated>
    <author>
      <name>/u/dudewithaapetite</name>
      <uri>https://old.reddit.com/user/dudewithaapetite</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title, i just want a model i can use on the go that doesnt consume too much energy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dudewithaapetite"&gt; /u/dudewithaapetite &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgoqll/best_battery_efficent_ai_model_for_i7_4710mq/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgoqll/best_battery_efficent_ai_model_for_i7_4710mq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kgoqll/best_battery_efficent_ai_model_for_i7_4710mq/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-07T04:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kggwwo</id>
    <title>Would adding an RTX 3060 12GB improve my performance?</title>
    <updated>2025-05-06T21:55:08+00:00</updated>
    <author>
      <name>/u/Pauli1_Go</name>
      <uri>https://old.reddit.com/user/Pauli1_Go</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently have an RTX 4080. I tried running Gemma3:27b on it but ran into a VRAM limit and only got 5 t/s. When I added my old GTX 970 for the extra VRAM, it improved to 14 t/s. Is it worth buying an RTX 3060 12GB to run larger models? Or would the lower VRAM bandwidth of the 3060 slow it down to a point where it’s not worth the money? Would I expectedly get at least 30 t/s? Combined with my 4080, that would get me 28GB of VRAM. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pauli1_Go"&gt; /u/Pauli1_Go &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kggwwo/would_adding_an_rtx_3060_12gb_improve_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kggwwo/would_adding_an_rtx_3060_12gb_improve_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kggwwo/would_adding_an_rtx_3060_12gb_improve_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-06T21:55:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgbj9y</id>
    <title>How to get AI to "dig around" in a website?</title>
    <updated>2025-05-06T18:14:55+00:00</updated>
    <author>
      <name>/u/AggressiveSkirl1680</name>
      <uri>https://old.reddit.com/user/AggressiveSkirl1680</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running ollama and openwebui on linux--i'm new to it--and i was hoping to get some general direction on how to get it to go to a specific website and &amp;quot;dig around&amp;quot; and do research for me? Am I looking for an openwebui tool, or something else entirely? thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggressiveSkirl1680"&gt; /u/AggressiveSkirl1680 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgbj9y/how_to_get_ai_to_dig_around_in_a_website/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgbj9y/how_to_get_ai_to_dig_around_in_a_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kgbj9y/how_to_get_ai_to_dig_around_in_a_website/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-06T18:14:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgspw4</id>
    <title>Running multiple Ollama instances with different models on windows</title>
    <updated>2025-05-07T09:00:14+00:00</updated>
    <author>
      <name>/u/O2MINS</name>
      <uri>https://old.reddit.com/user/O2MINS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm setting up a system on Windows to run two instances of Ollama, each serving different models (Gemma3:12b and Llama3.2 3B) on separate ports. My machine specs are a 32-core AMD Epyc CPU and an NVIDIA A4000 GPU with 30GB VRAM (16GB dedicated, 14GB shared). I plan to dedicate this setup solely to hosting these models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Setting up Multiple Instances:&lt;/strong&gt; How can I run two Ollama instances, each serving a different model on distinct ports? What's the expected performance when both models run simultaneously on this setup?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Utilizing Full VRAM:&lt;/strong&gt; Currently, on my Task manager it shows 16GB dedicated VRAM and 14GB shared VRAM. How can I utilize the full 30GB VRAM capacity? Will the additional 14GB shared VRAM be automatically utilized when usage exceeds 16GB?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I appreciate any insights or experiences you can share on optimizing this setup for running AI models efficiently.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/O2MINS"&gt; /u/O2MINS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgspw4/running_multiple_ollama_instances_with_different/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgspw4/running_multiple_ollama_instances_with_different/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kgspw4/running_multiple_ollama_instances_with_different/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-07T09:00:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgrc0y</id>
    <title>Best Open-Source Model for Summarizing SQL Query Results – Currently Trying Qwen3 30B A3B</title>
    <updated>2025-05-07T07:16:49+00:00</updated>
    <author>
      <name>/u/Appropriate_Bus_989</name>
      <uri>https://old.reddit.com/user/Appropriate_Bus_989</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I’m using an open-source model to summarize SQL query results, aiming for speed and accuracy. Right now, I’m testing the &lt;strong&gt;Qwen3 30B A3B model&lt;/strong&gt;, but I’m open to suggestions for better options.&lt;/p&gt; &lt;p&gt;Requirements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fast and efficient for real-time processing&lt;/li&gt; &lt;li&gt;Accurate summaries&lt;/li&gt; &lt;li&gt;Open-source and scalable&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has anyone used Qwen3 30B A3B or any other models for this? Any recommendations would be helpful!&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Appropriate_Bus_989"&gt; /u/Appropriate_Bus_989 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgrc0y/best_opensource_model_for_summarizing_sql_query/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgrc0y/best_opensource_model_for_summarizing_sql_query/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kgrc0y/best_opensource_model_for_summarizing_sql_query/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-07T07:16:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgphel</id>
    <title>Arch 0.2.8 🚀 - Added support for bi-directional agent traffic, new local LLM for tools call, and more.</title>
    <updated>2025-05-07T05:12:27+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kgphel/arch_028_added_support_for_bidirectional_agent/"&gt; &lt;img alt="Arch 0.2.8 🚀 - Added support for bi-directional agent traffic, new local LLM for tools call, and more." src="https://preview.redd.it/3zyg9cb1maze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=401dcaa80e4d4af31e981751f573da1fcb07977a" title="Arch 0.2.8 🚀 - Added support for bi-directional agent traffic, new local LLM for tools call, and more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/katanemo/archgw"&gt;Arch&lt;/a&gt; is an AI-native proxy server for AI applications. It handles the pesky low-level work so that you can build agents faster with your framework of choice in any programming language and not have to repeat yourself.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's new in 0.2.8.&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Added support for bi-directional traffic as we work with Google to add support for A2A&lt;/li&gt; &lt;li&gt;Improved &lt;a href="https://huggingface.co/katanemo/Arch-Function-Chat-3B"&gt;Arch-Function-Chat 3B&lt;/a&gt; LLM for fast routing and common tool calling scenarios&lt;/li&gt; &lt;li&gt;Support for LLMs hosted on Groq&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Core Features&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;🚦 Routin&lt;/code&gt;g. Engineered with purpose-built &lt;a href="https://huggingface.co/collections/katanemo/arch-function-66f209a693ea8df14317ad68"&gt;LLMs&lt;/a&gt; for fast (&amp;lt;100ms) agent routing and hand-off&lt;/li&gt; &lt;li&gt;&lt;code&gt;⚡ Tools Use&lt;/code&gt;: For common agentic scenarios Arch clarifies prompts and makes tools calls&lt;/li&gt; &lt;li&gt;&lt;code&gt;⛨ Guardrails&lt;/code&gt;: Centrally configure and prevent harmful outcomes and enable safe interactions&lt;/li&gt; &lt;li&gt;&lt;code&gt;🔗 Access to LLM&lt;/code&gt;s: Centralize access and traffic to LLMs with smart retries&lt;/li&gt; &lt;li&gt;&lt;code&gt;🕵 Observabilit&lt;/code&gt;y: W3C compatible request tracing and LLM metrics&lt;/li&gt; &lt;li&gt;&lt;code&gt;🧱 Built on Envo&lt;/code&gt;y: Arch runs alongside app servers as a containerized process, and builds on top of &lt;a href="https://envoyproxy.io"&gt;Envoy's&lt;/a&gt; proven HTTP management and scalability features to handle ingress and egress traffic related to prompts and LLMs.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3zyg9cb1maze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgphel/arch_028_added_support_for_bidirectional_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kgphel/arch_028_added_support_for_bidirectional_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-07T05:12:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgxeo1</id>
    <title>Newbie question - Can any of these models search the web for new information ?</title>
    <updated>2025-05-07T13:30:18+00:00</updated>
    <author>
      <name>/u/Zealousideal-Heart83</name>
      <uri>https://old.reddit.com/user/Zealousideal-Heart83</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a newbie to llms. I am experimenting with some models just to get a feel of them to start with. It seems these models are unable to search for latest data from the internet (atleast Gemma3 models ?).&lt;/p&gt; &lt;p&gt;Is this the case for all of them ?&lt;/p&gt; &lt;p&gt;Chatgpt or Claude are able to search for latest information and do good research. I was hoping even if the quality of research/analysis is not as good as ChatGPT or Claude, these local LLMs should be atleast able to perform better than Google search. But it seems they only work off their snapshot data which is too bad.&lt;/p&gt; &lt;p&gt;I have 2 separate use cases that I am thinking of. 1. Code assistant 2. MCP integration for some existing API servers. (Kind of like AI agent)&lt;/p&gt; &lt;p&gt;I understand both are two different use cases and likely need two different models. What models would be a good fit for these use cases ? (I have 16GB VRAM at the moment, but I can may be try running on CPU if there is a good model that needs more RAM)&lt;/p&gt; &lt;p&gt;Edit: Another blocker seems to be that no model has a context memory ? ( I just tried several models in ollama and they themselves answered they don't have a context memory. Practically they seem to remember atmost 2 or 3 messages. This might be a bigger blocker for these open source models ?)&lt;/p&gt; &lt;p&gt;Update: Ok, so I had a complete misunderstanding because of the awesome ChatGPT/Claude front end. Basically LLM has no memory and is completely stateless. Moreover it cannot tun any tools by itself, nor can it do simple stuff like fetch something from internet. We have to do all these by ourselves. For ollama, openwebui does the history thing, but for data retrieval either from internet or elsewhere, we have to develop that logic ourselves and provide the retrieved data to LLM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Heart83"&gt; /u/Zealousideal-Heart83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgxeo1/newbie_question_can_any_of_these_models_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgxeo1/newbie_question_can_any_of_these_models_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kgxeo1/newbie_question_can_any_of_these_models_search/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-07T13:30:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgn65d</id>
    <title>ollama voice to text</title>
    <updated>2025-05-07T02:58:52+00:00</updated>
    <author>
      <name>/u/Adept_Maize_6213</name>
      <uri>https://old.reddit.com/user/Adept_Maize_6213</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What Ollama model will do voice to text best, and how good is it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adept_Maize_6213"&gt; /u/Adept_Maize_6213 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgn65d/ollama_voice_to_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgn65d/ollama_voice_to_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kgn65d/ollama_voice_to_text/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-07T02:58:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1kghugt</id>
    <title>Ollama + Open WebUI serving hundreds of users - any insight?</title>
    <updated>2025-05-06T22:35:30+00:00</updated>
    <author>
      <name>/u/cantcantdancer</name>
      <uri>https://old.reddit.com/user/cantcantdancer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m looking for insight or suggestions on how to approach this. &lt;/p&gt; &lt;p&gt;I want to build out an instance to serve a few hundred users, including roles and groups etc, ideally providing the “ChatGPT experience” via local LLM. &lt;/p&gt; &lt;p&gt;I assume someone has done this and I’m looking for insight on lessons learned, things you tried, things that worked/didnt work, maybe any right sizing experience you had regarding hardware/VM. &lt;/p&gt; &lt;p&gt;Or alternatively I guess if there is a better solution for this you would suggest?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cantcantdancer"&gt; /u/cantcantdancer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kghugt/ollama_open_webui_serving_hundreds_of_users_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kghugt/ollama_open_webui_serving_hundreds_of_users_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kghugt/ollama_open_webui_serving_hundreds_of_users_any/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-06T22:35:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgs97q</id>
    <title>Best (smaller) model for bigger context?</title>
    <updated>2025-05-07T08:25:14+00:00</updated>
    <author>
      <name>/u/warmpieFairy</name>
      <uri>https://old.reddit.com/user/warmpieFairy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, which is a good 4-5-6GB LLM that can understand bigger contexts? I tried gemma, llama3, deepseek r1, qwen2.5, they work kind of bad i also tried bigger ones like command r, but I think they consume too much VRAM cause they don t really answer my questions&lt;/p&gt; &lt;p&gt;Edit: thank you everyone for your recommendations! qwen3 and mistral-nemo were the best for my use case&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/warmpieFairy"&gt; /u/warmpieFairy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgs97q/best_smaller_model_for_bigger_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgs97q/best_smaller_model_for_bigger_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kgs97q/best_smaller_model_for_bigger_context/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-07T08:25:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1khn0hv</id>
    <title>AI powered crypto scalper analyst dashboard, looking for trader who helps assessing how good it is.</title>
    <updated>2025-05-08T10:45:49+00:00</updated>
    <author>
      <name>/u/TheBlackKnight2000BC</name>
      <uri>https://old.reddit.com/user/TheBlackKnight2000BC</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run a professional grade AI setup on AMD MI accellerators able to power largeLLMs&lt;br /&gt; a crypto analyst dashboard was built which consults AI in assessing crypto signals and detect patterns.&lt;br /&gt; the dashboard is now functional, accessing binance for data.&lt;br /&gt; Im looking for a professional trader, who has experience in high frequency trading, futures and patters, and is willing to assess the dashboard, try how good it is, and if it brings him value.&lt;/p&gt; &lt;p&gt;contact me&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheBlackKnight2000BC"&gt; /u/TheBlackKnight2000BC &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1khn0hv/ai_powered_crypto_scalper_analyst_dashboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1khn0hv/ai_powered_crypto_scalper_analyst_dashboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1khn0hv/ai_powered_crypto_scalper_analyst_dashboard/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-08T10:45:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh0fcu</id>
    <title>Hardware Advice for Running a Local 30B Model</title>
    <updated>2025-05-07T15:37:32+00:00</updated>
    <author>
      <name>/u/Quirky_Mess3651</name>
      <uri>https://old.reddit.com/user/Quirky_Mess3651</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! I'm in the process of setting up infrastructure for a business that will rely on a local LLM with around 30B parameters. We're looking to run inference locally (not training), and I'm trying to figure out the most practical hardware setup to support this.&lt;/p&gt; &lt;p&gt;I’m considering whether a single RTX 5090 would be sufficient, or if I’d be better off investing in enterprise-grade GPUs like the RTX 6000 Ada, or possibly a multi-GPU setup.&lt;/p&gt; &lt;p&gt;I’m trying to find the right balance between cost-effectiveness and smooth performance. It doesn't need to be ultra high-end, but it should run reliably and efficiently without major slowdowns. I’d love to hear from others with experience running 30B models locally—what's the cheapest setup you’d consider &lt;em&gt;viable&lt;/em&gt;?&lt;/p&gt; &lt;p&gt;Also, if we were to upgrade to a 60B parameter model down the line, what kind of hardware leap would that require? Would the same hardware scale, or are we looking at a whole different class of setup?&lt;/p&gt; &lt;p&gt;Appreciate any advice!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quirky_Mess3651"&gt; /u/Quirky_Mess3651 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kh0fcu/hardware_advice_for_running_a_local_30b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kh0fcu/hardware_advice_for_running_a_local_30b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kh0fcu/hardware_advice_for_running_a_local_30b_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-07T15:37:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgwapn</id>
    <title>Apple Silicon NPU / Ollama</title>
    <updated>2025-05-07T12:37:57+00:00</updated>
    <author>
      <name>/u/BoandlK</name>
      <uri>https://old.reddit.com/user/BoandlK</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;will it ever be possible to run a model like gemma3:12b on the Apple Silicon integrated NPUs (M1-4)?&lt;/p&gt; &lt;p&gt;Is an NPU even capable of running such a big LLM in theory?&lt;/p&gt; &lt;p&gt;Many thanks in advance.&lt;/p&gt; &lt;p&gt;Bastian&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoandlK"&gt; /u/BoandlK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgwapn/apple_silicon_npu_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgwapn/apple_silicon_npu_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kgwapn/apple_silicon_npu_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-07T12:37:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1khvz93</id>
    <title>How to make an ai give me the answer i want</title>
    <updated>2025-05-08T17:31:52+00:00</updated>
    <author>
      <name>/u/Scariingella</name>
      <uri>https://old.reddit.com/user/Scariingella</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So i just downloaded a model on ollama and im using anythingllm for the ui. im giving it this prompt so i can create flashcards from a text:&lt;br /&gt; for each page write me flash cards, the flash cards must be like this and without writing question, answer or the page and take the information only from the text that I send you below and format md:&lt;/p&gt; &lt;p&gt;# &amp;quot;question&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;answer&amp;quot;&lt;/p&gt; &lt;p&gt;# &amp;quot;question&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;answer&amp;quot;&lt;/p&gt; &lt;p&gt;text.......&lt;/p&gt; &lt;p&gt;when i run it on claude ai i get the flashcards done correctly but when i do the same prompt in ollama i get bad responded like not all the pages i sent him or not creating question and getting pages wrong and mixing information, what is the problem? im happy to give more context.&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/F13huTaa"&gt;https://pastebin.com/F13huTaa&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Scariingella"&gt; /u/Scariingella &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1khvz93/how_to_make_an_ai_give_me_the_answer_i_want/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1khvz93/how_to_make_an_ai_give_me_the_answer_i_want/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1khvz93/how_to_make_an_ai_give_me_the_answer_i_want/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-08T17:31:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1kht359</id>
    <title>Which Mac?</title>
    <updated>2025-05-08T15:34:34+00:00</updated>
    <author>
      <name>/u/jacob-indie</name>
      <uri>https://old.reddit.com/user/jacob-indie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What kind of (latest) Mac would you buy to run Ollama?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;best overall&lt;/li&gt; &lt;li&gt;best bang for buck - new?&lt;/li&gt; &lt;li&gt;best bang for buck - used?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My guess is it’s all about max ram, but is that true?&lt;/p&gt; &lt;p&gt;(I have lots of small local AI tasks and think about horizontal scaling)&lt;/p&gt; &lt;p&gt;(Bonus: if there is a superior PC option, maybe rack based… I may consider it; energy consumption is less of a concern thanks to lots of solar)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacob-indie"&gt; /u/jacob-indie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kht359/which_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kht359/which_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kht359/which_mac/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-08T15:34:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1khsrup</id>
    <title>Evaluating Models</title>
    <updated>2025-05-08T15:21:36+00:00</updated>
    <author>
      <name>/u/marketlurker</name>
      <uri>https://old.reddit.com/user/marketlurker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;with so many models available in &lt;strong&gt;Ollama&lt;/strong&gt;, I asked &lt;strong&gt;Gemini&lt;/strong&gt; the following, &amp;quot;can you contrast the top 15 models that ollama uses? I want to know what each is best at, why it is best and also any weaknesses. add mininum requirements for those models.&amp;quot; Here is what was returned.&lt;/p&gt; &lt;h1&gt;1. Falcon 40B&lt;/h1&gt; &lt;p&gt;· Best At: Multilingual capabilities and broad knowledge applications.&lt;/p&gt; &lt;p&gt;· Why: Large model with a comprehensive understanding of diverse languages.&lt;/p&gt; &lt;p&gt;· Weaknesses: Older architecture compared to newer models like Llama 3 or Mistral.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 40 GB&lt;/p&gt; &lt;h1&gt;2. Llama 3&lt;/h1&gt; &lt;p&gt;· Best At: Cutting-edge performance and efficiency.&lt;/p&gt; &lt;p&gt;· Why: Modern architecture that delivers superior results in various tasks.&lt;/p&gt; &lt;p&gt;· Weaknesses: May require more advanced hardware to operate optimally.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 30 GB&lt;/p&gt; &lt;h1&gt;3. Mistral&lt;/h1&gt; &lt;p&gt;· Best At: Speed and real-time applications.&lt;/p&gt; &lt;p&gt;· Why: Optimized for rapid inference and low latency.&lt;/p&gt; &lt;p&gt;· Weaknesses: Less suited for tasks requiring extensive knowledge bases.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 15 GB&lt;/p&gt; &lt;h1&gt;4. GPT-3&lt;/h1&gt; &lt;p&gt;· Best At: General-purpose tasks and conversational AI.&lt;/p&gt; &lt;p&gt;· Why: Extensive training data and versatile applications.&lt;/p&gt; &lt;p&gt;· Weaknesses: Larger model size can be a constraint in limited hardware environments.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 175 GB&lt;/p&gt; &lt;h1&gt;5. BERT&lt;/h1&gt; &lt;p&gt;· Best At: Natural language understanding.&lt;/p&gt; &lt;p&gt;· Why: Pre-trained on a vast corpus enabling deep comprehension capabilities.&lt;/p&gt; &lt;p&gt;· Weaknesses: Not optimized for generative tasks.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 11 GB&lt;/p&gt; &lt;h1&gt;6. RoBERTa&lt;/h1&gt; &lt;p&gt;· Best At: Enhanced natural language processing.&lt;/p&gt; &lt;p&gt;· Why: Fine-tuned improvements over BERT.&lt;/p&gt; &lt;p&gt;· Weaknesses: Similar to BERT, mainly focused on understanding rather than generation.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 12 GB&lt;/p&gt; &lt;h1&gt;7. XLNet&lt;/h1&gt; &lt;p&gt;· Best At: Contextual language modeling.&lt;/p&gt; &lt;p&gt;· Why: Autoregressive pre-training for better context handling.&lt;/p&gt; &lt;p&gt;· Weaknesses: Complex architecture that might require substantial computational power.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 14 GB&lt;/p&gt; &lt;h1&gt;8. T5&lt;/h1&gt; &lt;p&gt;· Best At: Text generation and transformation tasks.&lt;/p&gt; &lt;p&gt;· Why: Versatile model capable of handling multiple NLP tasks effectively.&lt;/p&gt; &lt;p&gt;· Weaknesses: Memory-intensive model requiring robust hardware.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 16 GB&lt;/p&gt; &lt;h1&gt;9. DistilBERT&lt;/h1&gt; &lt;p&gt;· Best At: Efficient NLP tasks with reduced model size.&lt;/p&gt; &lt;p&gt;· Why: Distilled version of BERT retaining performance with lesser computational demand.&lt;/p&gt; &lt;p&gt;· Weaknesses: Slightly less accurate compared to its larger counterpart.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 6 GB&lt;/p&gt; &lt;h1&gt;10. GPT-2&lt;/h1&gt; &lt;p&gt;· Best At: Generative text applications.&lt;/p&gt; &lt;p&gt;· Why: Well-known for its ability to produce coherent and contextually relevant text.&lt;/p&gt; &lt;p&gt;· Weaknesses: Lower performance compared to GPT-3.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 40 GB&lt;/p&gt; &lt;h1&gt;11. OpenAI Codex&lt;/h1&gt; &lt;p&gt;· Best At: Code generation and programming assistance.&lt;/p&gt; &lt;p&gt;· Why: Specifically trained on a diverse range of coding languages and tasks.&lt;/p&gt; &lt;p&gt;· Weaknesses: Limited in non-coding linguistic tasks.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 20 GB&lt;/p&gt; &lt;h1&gt;12. BART&lt;/h1&gt; &lt;p&gt;· Best At: Text summarization and generation.&lt;/p&gt; &lt;p&gt;· Why: Transformer-based model designed for sequence-to-sequence tasks.&lt;/p&gt; &lt;p&gt;· Weaknesses: Requires substantial computational resources.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 13 GB&lt;/p&gt; &lt;h1&gt;13. ALBERT&lt;/h1&gt; &lt;p&gt;· Best At: Efficient natural language understanding.&lt;/p&gt; &lt;p&gt;· Why: Lightweight model designed to mitigate BERT's limitations.&lt;/p&gt; &lt;p&gt;· Weaknesses: May have reduced performance in highly complex tasks.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 8 GB&lt;/p&gt; &lt;h1&gt;14. Electra&lt;/h1&gt; &lt;p&gt;· Best At: Pre-training efficiency.&lt;/p&gt; &lt;p&gt;· Why: Utilizes a novel approach to pre-training yielding high performance.&lt;/p&gt; &lt;p&gt;· Weaknesses: May require additional fine-tuning for specific tasks.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 10 GB&lt;/p&gt; &lt;h1&gt;15. GPT-Neo&lt;/h1&gt; &lt;p&gt;· Best At: Open-source generative modeling.&lt;/p&gt; &lt;p&gt;· Why: Provides flexibility and customization for various generative tasks.&lt;/p&gt; &lt;p&gt;· Weaknesses: Performance may vary compared to proprietary models.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 12 GB&lt;/p&gt; &lt;p&gt;I would love to hear the thoughts of any of you. I am looking to hear your experience and what you would change.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marketlurker"&gt; /u/marketlurker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1khsrup/evaluating_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1khsrup/evaluating_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1khsrup/evaluating_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-08T15:21:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ki8za7</id>
    <title>Save or auto launch parameter</title>
    <updated>2025-05-09T03:29:09+00:00</updated>
    <author>
      <name>/u/lavoie005</name>
      <uri>https://old.reddit.com/user/lavoie005</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, i want to change the parameter of the ollama llm or launch it before every request&lt;br /&gt; i want to set the num_gpu and num_ctx.&lt;br /&gt; i have check a couple of video put i dont have any idea how to do it.&lt;/p&gt; &lt;p&gt;Thanks for your help&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lavoie005"&gt; /u/lavoie005 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ki8za7/save_or_auto_launch_parameter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ki8za7/save_or_auto_launch_parameter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ki8za7/save_or_auto_launch_parameter/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-09T03:29:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ki2atk</id>
    <title>open source local AI debugger</title>
    <updated>2025-05-08T21:50:43+00:00</updated>
    <author>
      <name>/u/AntelopeEntire9191</name>
      <uri>https://old.reddit.com/user/AntelopeEntire9191</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Ollama community,&lt;/p&gt; &lt;p&gt;I’m Gabriel Cha and an incoming data science @ coluimbia and just wanted to share what I've been building past 2 weeks with my friend Min Kim.&lt;/p&gt; &lt;p&gt;cloi is a local debugging agent that runs in your terminal.&lt;/p&gt; &lt;p&gt;We made cloi because every AI coding tool wants API keys, subscriptions, and your entire codebase uploaded to their servers. cloi, however, runs entirely on your machine. No cloud, no API keys, no subscriptions, no data leaving your system.&lt;/p&gt; &lt;p&gt;The tech is simple: it captures your error context, spins up Ollama locally, generates targeted fixes, and - only with your explicit permission - applies patches to your files. You can swap to any Ollama model you've got installed.&lt;/p&gt; &lt;p&gt;Install Globaly: &lt;code&gt;$ npm install -g&lt;/code&gt; @cloi-ai&lt;code&gt;/cloi&lt;/code&gt;&lt;/p&gt; &lt;p&gt;cloi is open source &lt;a href="https://github.com/cloi-ai/cloi"&gt;https://github.com/cloi-ai/cloi&lt;/a&gt; [243 stars in under 7 days] We want to build something actually helpful and not just another garbage npm package, but if you feel as tho it is, drop the feedback and roast it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AntelopeEntire9191"&gt; /u/AntelopeEntire9191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cloi-ai.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ki2atk/open_source_local_ai_debugger/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ki2atk/open_source_local_ai_debugger/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-08T21:50:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kifr4n</id>
    <title>Simple Gradio Chat UI for Ollama and OpenRouter with Streaming Support</title>
    <updated>2025-05-09T11:04:37+00:00</updated>
    <author>
      <name>/u/Illustrious_Low_3411</name>
      <uri>https://old.reddit.com/user/Illustrious_Low_3411</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kifr4n/simple_gradio_chat_ui_for_ollama_and_openrouter/"&gt; &lt;img alt="Simple Gradio Chat UI for Ollama and OpenRouter with Streaming Support" src="https://preview.redd.it/ar5lsxxbmqze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fdba69ba828fe48be844a242a8947faeb41a1629" title="Simple Gradio Chat UI for Ollama and OpenRouter with Streaming Support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m new to LLMs and made a simple Gradio chat UI. It works with local models using Ollama and cloud models via OpenRouter. Has streaming too.&lt;br /&gt; Supports streaming too.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/gurmessa/llm-gradio-chat"&gt;https://github.com/gurmessa/llm-gradio-chat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious_Low_3411"&gt; /u/Illustrious_Low_3411 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ar5lsxxbmqze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kifr4n/simple_gradio_chat_ui_for_ollama_and_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kifr4n/simple_gradio_chat_ui_for_ollama_and_openrouter/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-09T11:04:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1khshnn</id>
    <title>Best way to run a model for local use? ~20 users at a time.</title>
    <updated>2025-05-08T15:10:06+00:00</updated>
    <author>
      <name>/u/Ttaywsenrak</name>
      <uri>https://old.reddit.com/user/Ttaywsenrak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is probably a question that has been asked before to some degree but here goes -&lt;/p&gt; &lt;p&gt;I am a high school comp-sci teacher, and I am looking to keep my kids as up to speed as possible by integrating AI into some of our projects next year. Mostly for simple things, but I think AI is one of the few things that excites students these days.&lt;/p&gt; &lt;p&gt;The trick is the relatively high cost of having enough tokens for this, and more importantly, the school district hates students having to have accounts for things, which is of course necessary for API keys (plus you have to be 18+ for most of the sign ups anyways).&lt;/p&gt; &lt;p&gt;Now, my classroom lab is pretty decent, all PCs could run a simple model no problem. But school IT has vetoed this because they don't have a way to log everything students ask, so they are worried about kids requesting how to make bombs etc. Compounding this is the fact that students can just download an uncensored model and do whatever they want.&lt;/p&gt; &lt;p&gt;Therefore, my potential requirements would be LAN API requests and logging. I don't necessarily need a GUI, though it would be a nice option as long as logging is available.&lt;/p&gt; &lt;p&gt;To be honest, I don't know a lot about running local LLMs yet, but I am a pretty quick study.&lt;/p&gt; &lt;p&gt;Thanks in advance for any help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ttaywsenrak"&gt; /u/Ttaywsenrak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1khshnn/best_way_to_run_a_model_for_local_use_20_users_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1khshnn/best_way_to_run_a_model_for_local_use_20_users_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1khshnn/best_way_to_run_a_model_for_local_use_20_users_at/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-08T15:10:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kicje6</id>
    <title>Can we choose what to offload to GPU?</title>
    <updated>2025-05-09T07:16:24+00:00</updated>
    <author>
      <name>/u/ACheshirov</name>
      <uri>https://old.reddit.com/user/ACheshirov</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I like Ollama because it gives me an easy way to integrate LLMs into my tools, but sometimes more advanced settings could be really beneficial.&lt;/p&gt; &lt;p&gt;So, I came across this reddit post &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ki7tg7/dont_offload_gguf_layers_offload_tensors_200_gen/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ki7tg7/dont_offload_gguf_layers_offload_tensors_200_gen/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This guy shows how we can get a 200%+ performance boost by offloading only the &amp;quot;right&amp;quot; layers to the GPU. Basically, when we can't fit the whole model into GPU VRAM, part of it has to run from the CPU and RAM. The key point is which parts go to the CPU and which ones to the GPU.&lt;/p&gt; &lt;p&gt;The idea is: let the GPU handle all possible tensors, but leave the GGUF layers on the CPU. That way, the GPU does the heavy lifting, and the whole thing runs more efficiently - you get more tokens per second for free. :) &lt;/p&gt; &lt;p&gt;At least, that's what I understood from his post.&lt;/p&gt; &lt;p&gt;So… is there a flag in Ollama that lets us do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ACheshirov"&gt; /u/ACheshirov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kicje6/can_we_choose_what_to_offload_to_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kicje6/can_we_choose_what_to_offload_to_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kicje6/can_we_choose_what_to_offload_to_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-09T07:16:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ki7x1s</id>
    <title>New very simple UI for Ollama</title>
    <updated>2025-05-09T02:30:13+00:00</updated>
    <author>
      <name>/u/rotgertesla</name>
      <uri>https://old.reddit.com/user/rotgertesla</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ki7x1s/new_very_simple_ui_for_ollama/"&gt; &lt;img alt="New very simple UI for Ollama" src="https://preview.redd.it/2xe9bpzq3oze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=530d0af0ac1c4e748f74446c02c26ecdc6ca03cb" title="New very simple UI for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created a very simple html UI for Ollama (single file).&lt;br /&gt; Probably the simplest UI you can find.&lt;/p&gt; &lt;p&gt;See github page here: &lt;a href="https://github.com/rotger/Simple-Ollama-Chatbot"&gt;https://github.com/rotger/Simple-Ollama-Chatbot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;support markdown, mathjax and code synthax highlighting&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rotgertesla"&gt; /u/rotgertesla &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2xe9bpzq3oze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ki7x1s/new_very_simple_ui_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ki7x1s/new_very_simple_ui_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-09T02:30:13+00:00</published>
  </entry>
</feed>
