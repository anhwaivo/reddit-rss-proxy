<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-22T13:13:46+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mujr2l</id>
    <title>Help me out in selecting a 'good' framework for AI Agents using local llm</title>
    <updated>2025-08-19T14:18:27+00:00</updated>
    <author>
      <name>/u/irodov4030</name>
      <uri>https://old.reddit.com/user/irodov4030</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am learning AI Agents.&lt;/p&gt; &lt;p&gt;I have hands on experience building small agents using OpenAI SDK, CrewAI, langchain and know about MCP.&lt;/p&gt; &lt;p&gt;I need complex flows, local LLMs using ollama, tool calling and memory.&lt;/p&gt; &lt;p&gt;I currently like CrewAI and langchain. I have been able to use local llm via ollama for all.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Please guide me on differentiating between these two and selecting 1 to deep dive&lt;/strong&gt; or if there are good alternatives.&lt;/p&gt; &lt;p&gt;My objective is to deep dive on 1 framework and build some side projects unrelated to any job.&lt;/p&gt; &lt;p&gt;I am a beginner so, can not and do no want to deep dive on all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/irodov4030"&gt; /u/irodov4030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mujr2l/help_me_out_in_selecting_a_good_framework_for_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mujr2l/help_me_out_in_selecting_a_good_framework_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mujr2l/help_me_out_in_selecting_a_good_framework_for_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-19T14:18:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1muip86</id>
    <title>Agentic Signal ‚Äì Visual AI Workflow Builder with Ollama Integration</title>
    <updated>2025-08-19T13:38:30+00:00</updated>
    <author>
      <name>/u/Code-Forge-Temple</name>
      <uri>https://old.reddit.com/user/Code-Forge-Temple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! I‚Äôve been working a few months now (except when I worked on &lt;a href="https://www.reddit.com/r/ollama/comments/1mgfcwe/my_aipowered_npcs_teach_sustainable_farming_with/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;LOCAL LLM NPC - The Gemma 3n Impact Challenge&lt;/a&gt;) on a project that integrates tightly with Ollama, and I thought the community might find it interesting and useful.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it is:&lt;/strong&gt;&lt;br /&gt; &lt;code&gt;Agentic Signal&lt;/code&gt; is a visual workflow automation platform that lets you build AI workflows using a drag-and-drop interface. Think of it as visual programming for AI agents and automation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it's useful for Ollama users:&lt;/strong&gt;&lt;br /&gt; - üîí Fully local ‚Äì runs on your local Ollama installation, no cloud needed&lt;br /&gt; - üé® Visual interface ‚Äì connect nodes instead of writing code&lt;br /&gt; - üõ†Ô∏è Tool calling ‚Äì AI agents can execute functions and access APIs&lt;br /&gt; - üìã Structured output ‚Äì JSON schema validation ensures reliable responses&lt;br /&gt; - üíæ Conversation memory ‚Äì maintains context across workflow runs&lt;br /&gt; - üìä Model management ‚Äì download, manage, and remove Ollama models from the UI&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example workflows you can build:&lt;/strong&gt;&lt;br /&gt; Email automation, calendar management, browser search automation, cloud storage integration, and more. All powered by your local Ollama models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;br /&gt; - &lt;a href="https://github.com/code-forge-temple/agentic-signal"&gt;GitHub Repository&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://www.youtube.com/watch?v=62zk8zE6UJI"&gt;Demo Video&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://code-forge-temple.github.io/agentic-signal/"&gt;Documentation &amp;amp; Examples&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;License:&lt;/strong&gt; AGPL v3 (open source) with commercial options available&lt;/p&gt; &lt;p&gt;I'd love feedback from anyone trying this with their Ollama setup, or ideas for new workflow types to support!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Code-Forge-Temple"&gt; /u/Code-Forge-Temple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1muip86/agentic_signal_visual_ai_workflow_builder_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1muip86/agentic_signal_visual_ai_workflow_builder_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1muip86/agentic_signal_visual_ai_workflow_builder_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-19T13:38:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv3jdh</id>
    <title>Getting started &amp; usecasea</title>
    <updated>2025-08-20T03:09:24+00:00</updated>
    <author>
      <name>/u/lpk86</name>
      <uri>https://old.reddit.com/user/lpk86</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi , I just installed Ollama on my local. &lt;/p&gt; &lt;p&gt;Currently just exploring some options to add some custom logic etc. I wanted the model to answer in specific way ex: if I give a Multiple choice question, it should map it to syllabus topic and then analyze each option and answer. But for every question I have to give prompts to do the same.. &lt;/p&gt; &lt;p&gt;And also, what are you using ollama for ? Or what are its typical use case ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lpk86"&gt; /u/lpk86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mv3jdh/getting_started_usecasea/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mv3jdh/getting_started_usecasea/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mv3jdh/getting_started_usecasea/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-20T03:09:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv20zs</id>
    <title>Tutorial about Templates for New Models (Modelfile)</title>
    <updated>2025-08-20T01:57:15+00:00</updated>
    <author>
      <name>/u/CarlosDelfino</name>
      <uri>https://old.reddit.com/user/CarlosDelfino</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote a tutorial in Portuguese. I don't think it'll be a problem, just use the page translator. The tutorial explains the main parameters, variables, and decision and control structures.&lt;/p&gt; &lt;p&gt;I think it's quite comprehensive and can help anyone who wants to customize a model for the first time.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arvoredossaberes.com.br/geral/como-usar-templates-do-ollama-para-criar-novos-modelos/"&gt;https://arvoredossaberes.com.br/geral/como-usar-templates-do-ollama-para-criar-novos-modelos/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CarlosDelfino"&gt; /u/CarlosDelfino &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mv20zs/tutorial_about_templates_for_new_models_modelfile/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mv20zs/tutorial_about_templates_for_new_models_modelfile/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mv20zs/tutorial_about_templates_for_new_models_modelfile/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-20T01:57:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mup1xu</id>
    <title>Is Ollama at risk of getting lost in its own complexity? A long-term user's perspective.</title>
    <updated>2025-08-19T17:29:41+00:00</updated>
    <author>
      <name>/u/Mulan20</name>
      <uri>https://old.reddit.com/user/Mulan20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a huge fan and an intensive user of Ollama. This tool has been incredibly helpful in my workflow, and I've integrated it into almost everything I do. I'm writing this to share some observations and concerns, hoping to spark a constructive discussion.&lt;/p&gt; &lt;p&gt;With every new update and the exciting new models being released, I can't shake the feeling that the project is moving in a direction that might compromise its core strengths. My primary concern is that Ollama, in its effort to evolve, might be getting lost in complexity.&lt;/p&gt; &lt;p&gt;This feeling is reminiscent of the evolution of tools like the Stable Diffusion WebUI, which started as a fast and straightforward interface but has since become heavier and, at times, limited in its functionality.&lt;/p&gt; &lt;p&gt;Here are a few specific points that I've noticed:&lt;/p&gt; &lt;p&gt;Performance and Resource Consumption: After recent updates, Ollama feels significantly more resource-intensive. I used to be able to run up to four scripts simultaneously that leveraged Ollama, but now I'm limited to a single script because an instance is already running. This has been a significant bottleneck for my productivity.&lt;/p&gt; &lt;p&gt;Model Quality Degradation: This is perhaps my biggest concern. Models running through the latest versions of Ollama don't seem to perform as well as they used to. Their ability to use functions seems diminished, and the overall quality of the responses feels lower. To validate this, I tried running the same models directly from Hugging Face and noticed a tangible difference in performance and output quality.&lt;/p&gt; &lt;p&gt;Growing Complexity vs. Simplicity: The initial appeal of Ollama was its brilliant simplicity and ease of use. However, with each update, it seems to be getting more complicated and &amp;quot;heavier.&amp;quot; The seamless experience that made me a dedicated user is slowly being eroded.&lt;/p&gt; &lt;p&gt;I truly hope that Ollama doesn't suffer the same fate as other successful open-source projects that eventually buckled under the weight of their own complexity. I have been, and will continue to be, a supporter and user of Ollama.&lt;/p&gt; &lt;p&gt;However, for the time being, these challenges are forcing me to look for alternative solutions for my production workflows. I wanted to share my experience to see if others feel the same way and to offer my perspective as a dedicated user who wants to see this project succeed in the long run.&lt;/p&gt; &lt;p&gt;Thank you for reading.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mulan20"&gt; /u/Mulan20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mup1xu/is_ollama_at_risk_of_getting_lost_in_its_own/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mup1xu/is_ollama_at_risk_of_getting_lost_in_its_own/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mup1xu/is_ollama_at_risk_of_getting_lost_in_its_own/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-19T17:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv81tr</id>
    <title>Agentic Signal is live on Product Hunt üöÄ (visual AI workflows + Ollama)</title>
    <updated>2025-08-20T07:23:27+00:00</updated>
    <author>
      <name>/u/Code-Forge-Temple</name>
      <uri>https://old.reddit.com/user/Code-Forge-Temple</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mv81tr/agentic_signal_is_live_on_product_hunt_visual_ai/"&gt; &lt;img alt="Agentic Signal is live on Product Hunt üöÄ (visual AI workflows + Ollama)" src="https://preview.redd.it/njzywejml4kf1.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e0f5a546dfa6e70eabb45ac916ef44f588f1e7a" title="Agentic Signal is live on Product Hunt üöÄ (visual AI workflows + Ollama)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just launched &lt;strong&gt;Agentic Signal&lt;/strong&gt; on Product Hunt!&lt;br /&gt; It‚Äôs a visual AI workflow builder with full &lt;strong&gt;Ollama&lt;/strong&gt; integration ‚Äî local, privacy‚Äëfirst, and extensible.&lt;/p&gt; &lt;p&gt;üëâ Check it out and share feedback: &lt;a href="https://www.producthunt.com/products/agentic-signal"&gt;https://www.producthunt.com/products/agentic-signal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs &amp;amp; intro video: &lt;a href="https://agentic-signal.com"&gt;https://agentic-signal.com&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=62zk8zE6UJI"&gt;https://www.youtube.com/watch?v=62zk8zE6UJI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/code-forge-temple/agentic-signal"&gt;https://github.com/code-forge-temple/agentic-signal&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Code-Forge-Temple"&gt; /u/Code-Forge-Temple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/njzywejml4kf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mv81tr/agentic_signal_is_live_on_product_hunt_visual_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mv81tr/agentic_signal_is_live_on_product_hunt_visual_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-20T07:23:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvotl6</id>
    <title>just a SE student asking for a recommendation PLS HELP I AM DROWNING</title>
    <updated>2025-08-20T19:33:11+00:00</updated>
    <author>
      <name>/u/ShelterSouth8142</name>
      <uri>https://old.reddit.com/user/ShelterSouth8142</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my internship, I noticed the company validates clients for loans via a ‚Äúcheck-the-box‚Äù process on documents. I built a fullstack webapp that parses these documents and uses an AI to deduce client suitability automatically.&lt;/p&gt; &lt;p&gt;Initially, I used Gemini via API, but the company firewall blocks it. My workaround is running an LLM through a Python script called in the Spring Boot backend. Everything works, except my personal PC has only 4GB VRAM and 16GB RAM.&lt;/p&gt; &lt;p&gt;I need a &lt;strong&gt;quantized, lightweight LLM&lt;/strong&gt; for testing. The final server will have better specs, but for now, it just needs to deduce simple text-based conditions. I‚Äôm new to this and would appreciate suggestions or advice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShelterSouth8142"&gt; /u/ShelterSouth8142 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvotl6/just_a_se_student_asking_for_a_recommendation_pls/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvotl6/just_a_se_student_asking_for_a_recommendation_pls/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mvotl6/just_a_se_student_asking_for_a_recommendation_pls/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-20T19:33:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv7sc0</id>
    <title>Qwen3-4B-Instruct-2507-GGUF template fixed</title>
    <updated>2025-08-20T07:07:18+00:00</updated>
    <author>
      <name>/u/Pjotrs</name>
      <uri>https://old.reddit.com/user/Pjotrs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Unsloth team uploaded templates to: &lt;a href="https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF"&gt;https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And how the model works out of box. Same should happen to the Thinking variant soon.&lt;/p&gt; &lt;p&gt;This model is amazing and having a drop-in working version is great.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pjotrs"&gt; /u/Pjotrs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mv7sc0/qwen34binstruct2507gguf_template_fixed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mv7sc0/qwen34binstruct2507gguf_template_fixed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mv7sc0/qwen34binstruct2507gguf_template_fixed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-20T07:07:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw2x28</id>
    <title>Need help picking LLM for sorting a book by speakers</title>
    <updated>2025-08-21T06:03:16+00:00</updated>
    <author>
      <name>/u/Only-Web-8543</name>
      <uri>https://old.reddit.com/user/Only-Web-8543</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, forgive my ignorance I am still learning. I am trying to find a model i can use to break down a book by speaker. I have ~100gb cpu ram thats usable (not vram too poor) so i need it to fit into that size and accuracy is a concern because i don't want the speaker to be mixed up or confused and get things wrong. I know ill probably have to break the book down into chapters because a 400 page book is probably too many tokens for most models but if there are any that can handle a 400 page book that would be great! If i have to go chapter by chapter which model would be best? I was looking at Qwen 3 32b instruct, LLama 3 34B, Minstral 30b, LLama scout 17B because it has a 1m token context window but from what i found that wont fit on 100gb but i could be wrong? and lastly I just saw that OpenAI released the oss models and was curious if those are any good? &lt;/p&gt; &lt;p&gt;Any advice is appreciated&lt;br /&gt; Thanks, &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Only-Web-8543"&gt; /u/Only-Web-8543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mw2x28/need_help_picking_llm_for_sorting_a_book_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mw2x28/need_help_picking_llm_for_sorting_a_book_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mw2x28/need_help_picking_llm_for_sorting_a_book_by/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T06:03:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvvxff</id>
    <title>How to optimize Ollama for continuous requests and avoid lost requests in the queue</title>
    <updated>2025-08-21T00:13:15+00:00</updated>
    <author>
      <name>/u/CarlosDelfino</name>
      <uri>https://old.reddit.com/user/CarlosDelfino</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm creating a question and answer dataset about all Wikipedia content. Everything is working, except that every 20 Wikipedia texts, ollama crashes, and I need to restart it. It returns to normal after the next 20 texts. I wrote the script so that it checks if there are many processes running and then waits for them to finish before adding another one, but it keeps crashing. I'm getting to the point where I need to run the script as root and set it to restart the service if it takes more than 5 minutes to free up the queue. I'm using a GTX 4070, which is unfortunately the best I can get right now. Does anyone have any suggestions for how ollama can better manage the request queue? I'm using the Granite3.3:8b model because it's the best I've found, with a large context window set to 1,000 tokens (40,000 total).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CarlosDelfino"&gt; /u/CarlosDelfino &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvvxff/how_to_optimize_ollama_for_continuous_requests/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvvxff/how_to_optimize_ollama_for_continuous_requests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mvvxff/how_to_optimize_ollama_for_continuous_requests/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T00:13:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvum1l</id>
    <title>Hardware &amp; LLM - Image Creation</title>
    <updated>2025-08-20T23:15:44+00:00</updated>
    <author>
      <name>/u/Illustrious-Hurry-59</name>
      <uri>https://old.reddit.com/user/Illustrious-Hurry-59</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi - I have recently started using text based models &amp;amp; I am amazed at what you can host locally using Ollama. I want to further play around with LLM but interested into taking it further into image/video generation.&lt;/p&gt; &lt;p&gt;I have the following rig config, can anyone suggest if this will be handle the image/video generation?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU: Ryzen 5 7600X&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU: NVIDIA¬Æ GeForce RTX‚Ñ¢ 5060 Ti 16GB&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory: 16 GB DDR5 DRAM 6000 MHz&lt;/strong&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also which model would be more suitable for my requirement &amp;amp; be compatible with the above hardware?&lt;/p&gt; &lt;p&gt;Thank you all in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Hurry-59"&gt; /u/Illustrious-Hurry-59 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvum1l/hardware_llm_image_creation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvum1l/hardware_llm_image_creation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mvum1l/hardware_llm_image_creation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-20T23:15:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvs4qa</id>
    <title>Had some beginner questions regarding how to use Ollama?</title>
    <updated>2025-08-20T21:35:46+00:00</updated>
    <author>
      <name>/u/RandomHuman1002</name>
      <uri>https://old.reddit.com/user/RandomHuman1002</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I am a beginner in trying to run AI locally had some questions regarding it.&lt;br /&gt; I want to run the AI on my laptop (13th gen i7-13650HX, 32GB RAM, RTX 4060 Laptop GPU) &lt;/p&gt; &lt;p&gt;1) Which AI model should I use I can see many of them on the ollama website like the new (gpt-oss, deepseek-r1, gemma3, qwen3 and llama3.1). Has anyone compared the pros and cons of each model?&lt;br /&gt; I can see that llama3.1 does not have thinking capabilities and gemma3 is the only vision model how does that affect the model that is running?&lt;/p&gt; &lt;p&gt;2) I am on a Windows machine so should I just use windows ollama or try to use Linux ollama using wsl (was recommended to do this)&lt;/p&gt; &lt;p&gt;3) Should I install openweb-ui and install ollama through that or just install ollama first?&lt;/p&gt; &lt;p&gt;Any other things I should keep in mind?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomHuman1002"&gt; /u/RandomHuman1002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvs4qa/had_some_beginner_questions_regarding_how_to_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvs4qa/had_some_beginner_questions_regarding_how_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mvs4qa/had_some_beginner_questions_regarding_how_to_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-20T21:35:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvrxsz</id>
    <title>Best model for my use case?</title>
    <updated>2025-08-20T21:28:23+00:00</updated>
    <author>
      <name>/u/guacgang</name>
      <uri>https://old.reddit.com/user/guacgang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am building an application where the model needs to make recommendations on rock climbing routes, including details about weather, difficulty, suggested gear, etc.&lt;/p&gt; &lt;p&gt;It also needs to be able to review videos that users/climbers upload and make suggestions on technique.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/guacgang"&gt; /u/guacgang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvrxsz/best_model_for_my_use_case/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvrxsz/best_model_for_my_use_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mvrxsz/best_model_for_my_use_case/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-20T21:28:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvyc7i</id>
    <title>Anyone using Ollama on a Windows Snapdragon Machine?</title>
    <updated>2025-08-21T02:04:36+00:00</updated>
    <author>
      <name>/u/Clipbeam</name>
      <uri>https://old.reddit.com/user/Clipbeam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious to see how well it performs... What models can you run on say the Surface laptop 15?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Clipbeam"&gt; /u/Clipbeam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvyc7i/anyone_using_ollama_on_a_windows_snapdragon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvyc7i/anyone_using_ollama_on_a_windows_snapdragon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mvyc7i/anyone_using_ollama_on_a_windows_snapdragon/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T02:04:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwbjwl</id>
    <title>Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp; Streamlit</title>
    <updated>2025-08-21T13:52:46+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mwbjwl/build_a_local_ai_agent_with_mcp_tools_using/"&gt; &lt;img alt="Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp;amp; Streamlit" src="https://external-preview.redd.it/rq8k6bkBVDqS3EaB-6PmZwrrp9mjAeoX2Tt37ubIdpg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76f91a095d5d6782424d54183f94d9fb060dd411" title="Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp;amp; Streamlit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/Baa-z7cum1g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwbjwl/build_a_local_ai_agent_with_mcp_tools_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mwbjwl/build_a_local_ai_agent_with_mcp_tools_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T13:52:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwilxb</id>
    <title>Are there best practices on how to use vanna with large databases and suboptimal table and columnnames?</title>
    <updated>2025-08-21T18:10:29+00:00</updated>
    <author>
      <name>/u/Private_Tank</name>
      <uri>https://old.reddit.com/user/Private_Tank</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Private_Tank"&gt; /u/Private_Tank &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LLMFrameworks/comments/1mwilh4/are_there_best_practices_on_how_to_use_vanna_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwilxb/are_there_best_practices_on_how_to_use_vanna_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mwilxb/are_there_best_practices_on_how_to_use_vanna_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T18:10:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwkg27</id>
    <title>Best model for text summarization</title>
    <updated>2025-08-21T19:18:57+00:00</updated>
    <author>
      <name>/u/Paleone123</name>
      <uri>https://old.reddit.com/user/Paleone123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need to create a fair number of presentations in a short time. I'm wondering what models will do best at at summarizing text into a series of headings and bullet points for me. It would also be nice if the model could output in markdown without me having to include a description of how basic markdown works in the context window. I'm much less concerned about tokens per second and much more about accuracy. I have 12gig of vram on my GPU, so 8b or 12b Q4 models are probably the limit of what I can run. I also have a ridiculous amount of ram, but I'm afraid ollama will crash out if I try to run a huge model on the CPU. Any advice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Paleone123"&gt; /u/Paleone123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwkg27/best_model_for_text_summarization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwkg27/best_model_for_text_summarization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mwkg27/best_model_for_text_summarization/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T19:18:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwf8jv</id>
    <title>Andrej Karpathy Software 3.0</title>
    <updated>2025-08-21T16:06:59+00:00</updated>
    <author>
      <name>/u/Brad_159</name>
      <uri>https://old.reddit.com/user/Brad_159</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mwf8jv/andrej_karpathy_software_30/"&gt; &lt;img alt="Andrej Karpathy Software 3.0" src="https://external-preview.redd.it/ItVjsy1WeovauKf8qvgECqUafdKTXMoXeunnlcMT0Gg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f6c058a6b915a0d391d295c52ba1ae54086852c" title="Andrej Karpathy Software 3.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That is almost what you can envision for the next five years. All the the applications and systems are going to be equipped with features that allow llms to call and operate.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brad_159"&gt; /u/Brad_159 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/LCEmiRjPEtQ?si=1G5_xVoBm7w-crbM"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwf8jv/andrej_karpathy_software_30/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mwf8jv/andrej_karpathy_software_30/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T16:06:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwhwx8</id>
    <title>Can LLMs Explain Their Reasoning? - Lecture Clip</title>
    <updated>2025-08-21T17:44:32+00:00</updated>
    <author>
      <name>/u/kushalgoenka</name>
      <uri>https://old.reddit.com/user/kushalgoenka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mwhwx8/can_llms_explain_their_reasoning_lecture_clip/"&gt; &lt;img alt="Can LLMs Explain Their Reasoning? - Lecture Clip" src="https://external-preview.redd.it/TEUGnG3_498JAZ2_OVIe6SduCtLk60U1nCPoQNdAxiw.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea3c8a53ee2ddd06374d9ca9c87ab9d321a362aa" title="Can LLMs Explain Their Reasoning? - Lecture Clip" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kushalgoenka"&gt; /u/kushalgoenka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/u2uNPzzZ45k"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwhwx8/can_llms_explain_their_reasoning_lecture_clip/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mwhwx8/can_llms_explain_their_reasoning_lecture_clip/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T17:44:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwe5e2</id>
    <title>gpt-oss provides correct date, but is sure that it is a different day of week</title>
    <updated>2025-08-21T15:27:44+00:00</updated>
    <author>
      <name>/u/JNKO266</name>
      <uri>https://old.reddit.com/user/JNKO266</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mwe5e2/gptoss_provides_correct_date_but_is_sure_that_it/"&gt; &lt;img alt="gpt-oss provides correct date, but is sure that it is a different day of week" src="https://preview.redd.it/8kc7a68b5ekf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b32860da5af726cc3f32e587b14cf7a290a43ff8" title="gpt-oss provides correct date, but is sure that it is a different day of week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been playing around with the new gpt-oss model while other models are downloading on a new machine, came onto this, which I thought was quite funny&lt;/p&gt; &lt;p&gt;‚ÄúUser claims today is Thursday August 21, 2025. That is obviously wrong: August 21, 2025 falls on Saturday.‚Äù&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JNKO266"&gt; /u/JNKO266 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8kc7a68b5ekf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwe5e2/gptoss_provides_correct_date_but_is_sure_that_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mwe5e2/gptoss_provides_correct_date_but_is_sure_that_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T15:27:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwsckl</id>
    <title>Ollama GUI is Electron based?</title>
    <updated>2025-08-22T00:42:59+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mwsckl/ollama_gui_is_electron_based/"&gt; &lt;img alt="Ollama GUI is Electron based?" src="https://preview.redd.it/yqq6arbawgkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39dc439c44365f27ef240c603f43fce30fd18572" title="Ollama GUI is Electron based?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Copilot chat on the ollama repo seems to think so but im hearing conflicting information&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yqq6arbawgkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwsckl/ollama_gui_is_electron_based/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mwsckl/ollama_gui_is_electron_based/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-22T00:42:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mx4m48</id>
    <title>Having issues when running two instances of Ollama, not sure if it even could really work</title>
    <updated>2025-08-22T12:03:52+00:00</updated>
    <author>
      <name>/u/thexdroid</name>
      <uri>https://old.reddit.com/user/thexdroid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For a specific test I installed 2 instances of Ollama on my computer, one on top of Windows, normal installation and a second of with linux WSL. For the WSL I've set a parameter to force it use CPU only, the intention was running 2 models at the same &amp;quot;time&amp;quot;.&lt;/p&gt; &lt;p&gt;What happens is the Ollama seems now to be attached to the wsl layer, what means that once I boot my computer Windows Ollama's GUI won't popup properly unless I start wsl. One more thing: I am sharing the model folder for both installations so I can download a model and it will be visible for both.&lt;/p&gt; &lt;p&gt;Should I revert and try to isolate the wsl version? Thanks for any idea.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thexdroid"&gt; /u/thexdroid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mx4m48/having_issues_when_running_two_instances_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mx4m48/having_issues_when_running_two_instances_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mx4m48/having_issues_when_running_two_instances_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-22T12:03:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwm9si</id>
    <title>Ollama Discord Rich Presence</title>
    <updated>2025-08-21T20:28:14+00:00</updated>
    <author>
      <name>/u/r00tkit_</name>
      <uri>https://old.reddit.com/user/r00tkit_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mwm9si/ollama_discord_rich_presence/"&gt; &lt;img alt="Ollama Discord Rich Presence" src="https://preview.redd.it/sf9eke8xmfkf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=068033aebc62dfffb22c97f765735aa5e6080ad0" title="Ollama Discord Rich Presence" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Made a Discord Rich Presence for Ollama - shows your current model + system specs&lt;/p&gt; &lt;p&gt;One-click install, works immediately. Thought you guys might like it!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/teodorgross/ollama-discord-presence"&gt;https://github.com/teodorgross/ollama-discord-presence&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/r00tkit_"&gt; /u/r00tkit_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sf9eke8xmfkf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwm9si/ollama_discord_rich_presence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mwm9si/ollama_discord_rich_presence/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T20:28:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwhmgk</id>
    <title>GLM-4.5 Air now running on Ollama, thanks to this kind soul (MichelRosselli)</title>
    <updated>2025-08-21T17:33:34+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You, sir or ma‚Äôam, are a friggin‚Äô LEGEND for posting working quants of GLM-4.5 Air on your Ollama repository &lt;a href="https://ollama.com/MichelRosselli/GLM-4.5-Air"&gt;https://ollama.com/MichelRosselli/GLM-4.5-Air&lt;/a&gt; even before any ‚Äúofficial‚Äù Ollama quants have been posted. Hats off to you! Note: According to the notes, the chat template is ‚Äúprovisional‚Äù, so tool calling doesn‚Äôt seem to be working at the moment and disabling thinking may not be supported either until the finalized chat template is added, but otherwise this thing is WAY COOL! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwhmgk/glm45_air_now_running_on_ollama_thanks_to_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mwhmgk/glm45_air_now_running_on_ollama_thanks_to_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mwhmgk/glm45_air_now_running_on_ollama_thanks_to_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T17:33:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mx0lix</id>
    <title>Local AI for students</title>
    <updated>2025-08-22T08:09:40+00:00</updated>
    <author>
      <name>/u/just-rundeer</name>
      <uri>https://old.reddit.com/user/just-rundeer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I‚Äôd like to give ~20 students access to a local AI system in class.&lt;/p&gt; &lt;p&gt;The main idea: build a simple RAG (retrieval-augmented generation) so they can look up rules/answers on their own when they don‚Äôt want to ask me.&lt;/p&gt; &lt;p&gt;Would a Beelink mini PC with 32GB RAM be enough to host a small LLM (7B‚Äì13B, quantized) plus a RAG index for ~20 simultaneous users?&lt;/p&gt; &lt;p&gt;Any experiences with performance under classroom conditions? Would you recommend Beelink or a small tower PC with GPU for more scalability?&lt;/p&gt; &lt;p&gt;Perfect would be if I could create something like Study and Learn mode but that will probably need GPU power then I am willing to spend. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/just-rundeer"&gt; /u/just-rundeer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mx0lix/local_ai_for_students/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mx0lix/local_ai_for_students/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mx0lix/local_ai_for_students/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-22T08:09:40+00:00</published>
  </entry>
</feed>
