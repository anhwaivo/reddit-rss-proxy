<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-02T12:55:47+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ifhrzl</id>
    <title>New to Ollama and LLMs, system RAM question.</title>
    <updated>2025-02-01T22:08:34+00:00</updated>
    <author>
      <name>/u/Tyr_Kukulkan</name>
      <uri>https://old.reddit.com/user/Tyr_Kukulkan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've recently started playing with local LLMs on my homelab. It is an old Threadripper 2950X with 128GB of RAM and a Vega 64 running Linux.&lt;/p&gt; &lt;p&gt;I've run 8B models which obviously load into VRAM fully and run quite quickly. They are functional but hallucinate a lot if you require too much detail or ask more niche questions.&lt;/p&gt; &lt;p&gt;I wanted to test larger condensed models which are better because they have more data. I tried 70B models which replies at about 1 word per second. The accuracy of results is significantly better but I'm severely limited by my hardware.&lt;/p&gt; &lt;p&gt;I've noticed that running the 70B it doesn't use any extra system RAM. I assume it is caching off the SSD instead and swapping with VRAM when required? That would explain part of why it is extremely slow.&lt;/p&gt; &lt;p&gt;Is there any way to force cache to system RAM? It'll still be limited by PCIe bandwidth of the GPU but it should be faster than reading off an SSD, right?&lt;/p&gt; &lt;p&gt;I'm not expecting miracles out of this old hardware, just trying to optimise performance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tyr_Kukulkan"&gt; /u/Tyr_Kukulkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifhrzl/new_to_ollama_and_llms_system_ram_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifhrzl/new_to_ollama_and_llms_system_ram_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifhrzl/new_to_ollama_and_llms_system_ram_question/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T22:08:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1if26o7</id>
    <title>I need an LLM that hallucinates a lot and generates garbage responses.</title>
    <updated>2025-02-01T08:43:17+00:00</updated>
    <author>
      <name>/u/dumbPotatoPot</name>
      <uri>https://old.reddit.com/user/dumbPotatoPot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I'm building a proof-of-concept to implement the &lt;a href="https://www.anthropic.com/research/building-effective-agents#:%7E:text=Workflow%3A%20Evaluator%2Doptimizer"&gt;evalutator-optimizer workflow&lt;/a&gt; in Java and need a primary model that hallucinates a lot for testing.&lt;/p&gt; &lt;p&gt;One approach would be to use a system prompt to specifically ask the LLM to produce irrelevant responses, but I can't do that currently.&lt;/p&gt; &lt;p&gt;Is there an LLM that's specifically available for such purpose? Also I've heard about increasing the context window to get quality responses, so in such case would decreasing the context window work? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dumbPotatoPot"&gt; /u/dumbPotatoPot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if26o7/i_need_an_llm_that_hallucinates_a_lot_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if26o7/i_need_an_llm_that_hallucinates_a_lot_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if26o7/i_need_an_llm_that_hallucinates_a_lot_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T08:43:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifkzs3</id>
    <title>Current - POV</title>
    <updated>2025-02-02T00:41:02+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifkzs3/current_pov/"&gt; &lt;img alt="Current - POV" src="https://preview.redd.it/0wm3v5pfgmge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56fded8a2e2b61f7e274b86376f34e730cfa9b79" title="Current - POV" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0wm3v5pfgmge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifkzs3/current_pov/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifkzs3/current_pov/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T00:41:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifb2pv</id>
    <title>Securing Ollama API endpoints for the web</title>
    <updated>2025-02-01T17:15:20+00:00</updated>
    <author>
      <name>/u/Historical_Visit7682</name>
      <uri>https://old.reddit.com/user/Historical_Visit7682</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you want to expose your Ollama API to anyone outside of your network, you should realize that Ollama does not support safeguarding the API with authentication tokens. For example, I wanted to run Ollama locally while hosting Open WebUI on my VPS. I utilized Cloudflare tunnels to connect my API to my domain, this provided no safety, anyone with the link could use Ollama freely. So I created this: &lt;a href="https://github.com/marcussaw123/ollamaSecure"&gt;project&lt;/a&gt;. It is essentially simply an express server piggybacking on the actual Ollama API, with the only difference being that it will check if the request contains an Authorization header with the necessary token.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Historical_Visit7682"&gt; /u/Historical_Visit7682 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifb2pv/securing_ollama_api_endpoints_for_the_web/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifb2pv/securing_ollama_api_endpoints_for_the_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifb2pv/securing_ollama_api_endpoints_for_the_web/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T17:15:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ietwx3</id>
    <title>Built my own discord bot using ollama and uh...</title>
    <updated>2025-02-01T00:29:41+00:00</updated>
    <author>
      <name>/u/GlitchPhoenix98</name>
      <uri>https://old.reddit.com/user/GlitchPhoenix98</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ietwx3/built_my_own_discord_bot_using_ollama_and_uh/"&gt; &lt;img alt="Built my own discord bot using ollama and uh..." src="https://preview.redd.it/xokxk8pu9fge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=902c2a44c4ab57ad1a35490d4766d3c979bf1edf" title="Built my own discord bot using ollama and uh..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GlitchPhoenix98"&gt; /u/GlitchPhoenix98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xokxk8pu9fge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ietwx3/built_my_own_discord_bot_using_ollama_and_uh/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ietwx3/built_my_own_discord_bot_using_ollama_and_uh/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T00:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifttud</id>
    <title>10 Clever Ways To Turn DeepSeek Into Your Side Hustle</title>
    <updated>2025-02-02T09:40:44+00:00</updated>
    <author>
      <name>/u/sveennn</name>
      <uri>https://old.reddit.com/user/sveennn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifttud/10_clever_ways_to_turn_deepseek_into_your_side/"&gt; &lt;img alt="10 Clever Ways To Turn DeepSeek Into Your Side Hustle" src="https://external-preview.redd.it/1Ry3CaKxw9SY-gw4E4GZHMpoQQwHVBg2LCmIIvj72B8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2aec056bf1179768fd89fd3616e733a4047d0aef" title="10 Clever Ways To Turn DeepSeek Into Your Side Hustle" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sveennn"&gt; /u/sveennn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@sveennn/10-clever-ways-to-turn-deepseek-into-your-side-hustle-bdc562fbec9c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifttud/10_clever_ways_to_turn_deepseek_into_your_side/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifttud/10_clever_ways_to_turn_deepseek_into_your_side/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T09:40:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1if9743</id>
    <title>I have 20K word documents, I need to add the data from them into mongodb.</title>
    <updated>2025-02-01T15:52:10+00:00</updated>
    <author>
      <name>/u/shaxadhere</name>
      <uri>https://old.reddit.com/user/shaxadhere</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Issue is that there is data inside them that needs to go in specific fields. Like there is a table at the start of the document 3 rows 3 columns, each cell contains some data and then there is a large body of text beneath the table that goes in one cell.&lt;/p&gt; &lt;p&gt;I tried picking data with regex but the table data changes like their format changes. so regex is not an option here i guess. &lt;/p&gt; &lt;p&gt;I'm thinking of hosting deepseek with ollama locally and write a nodejs script to all the work form.&lt;/p&gt; &lt;p&gt;So far I've scrapped the document and got the text in a variable from outside.&lt;/p&gt; &lt;p&gt;but when i pass the data to the deepseek server, it gives me a streaming response which causes a lots of problems, is there any way to just get the final response and not streaming one.&lt;/p&gt; &lt;p&gt;Or if you can suggest a better solution to this.&lt;/p&gt; &lt;p&gt;My clients prod database got deleted and this is the only option available, they have the content of their entire app inside a word document.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shaxadhere"&gt; /u/shaxadhere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if9743/i_have_20k_word_documents_i_need_to_add_the_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if9743/i_have_20k_word_documents_i_need_to_add_the_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if9743/i_have_20k_word_documents_i_need_to_add_the_data/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T15:52:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifp725</id>
    <title>What is Ollama and how to use it: a quick guide [part 1]</title>
    <updated>2025-02-02T04:28:17+00:00</updated>
    <author>
      <name>/u/geshan</name>
      <uri>https://old.reddit.com/user/geshan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifp725/what_is_ollama_and_how_to_use_it_a_quick_guide/"&gt; &lt;img alt="What is Ollama and how to use it: a quick guide [part 1]" src="https://external-preview.redd.it/8MWdJZUaawUAdrOKRXgnCZEz9o2FNzmFaI7Hu5xxzTQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3415e2df2b2072119555b4f9e941f458832579c" title="What is Ollama and how to use it: a quick guide [part 1]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geshan"&gt; /u/geshan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://geshan.com.np/blog/2025/02/what-is-ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifp725/what_is_ollama_and_how_to_use_it_a_quick_guide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifp725/what_is_ollama_and_how_to_use_it_a_quick_guide/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T04:28:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iflu0n</id>
    <title>Is there a load balancer for working with multiple instances of ollama?</title>
    <updated>2025-02-02T01:23:58+00:00</updated>
    <author>
      <name>/u/malformed-packet</name>
      <uri>https://old.reddit.com/user/malformed-packet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m experimenting with running multiple models at once to simulate a chat room. The problem is with using one ollama instance, it only processes one completion at a time. Is there a tool out there that sits on top of multiple instances and can load balance incoming queries &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/malformed-packet"&gt; /u/malformed-packet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iflu0n/is_there_a_load_balancer_for_working_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iflu0n/is_there_a_load_balancer_for_working_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iflu0n/is_there_a_load_balancer_for_working_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T01:23:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifqlp4</id>
    <title>Test if a model is any good?</title>
    <updated>2025-02-02T05:53:26+00:00</updated>
    <author>
      <name>/u/Kitchen-Purpose-6596</name>
      <uri>https://old.reddit.com/user/Kitchen-Purpose-6596</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are so many models to choose from. How do you test if a model is any good?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kitchen-Purpose-6596"&gt; /u/Kitchen-Purpose-6596 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifqlp4/test_if_a_model_is_any_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifqlp4/test_if_a_model_is_any_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifqlp4/test_if_a_model_is_any_good/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T05:53:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifj5qj</id>
    <title>New Experimental Agent Layer &amp; Reasoning Layer added to Notate v1.1.0. Now you can with any model locally reason and enable web search utilizing the Agent layer. More tools coming soon!</title>
    <updated>2025-02-01T23:12:16+00:00</updated>
    <author>
      <name>/u/Hairetsu</name>
      <uri>https://old.reddit.com/user/Hairetsu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifj5qj/new_experimental_agent_layer_reasoning_layer/"&gt; &lt;img alt="New Experimental Agent Layer &amp;amp; Reasoning Layer added to Notate v1.1.0. Now you can with any model locally reason and enable web search utilizing the Agent layer. More tools coming soon!" src="https://external-preview.redd.it/mcnU-d_DoLHew8RaPQUtHLP6gPiy9L1fZ85lNZzDPN4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=195f63b8ab536d1b6a94e52ac5e539c8a57f3eef" title="New Experimental Agent Layer &amp;amp; Reasoning Layer added to Notate v1.1.0. Now you can with any model locally reason and enable web search utilizing the Agent layer. More tools coming soon!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hairetsu"&gt; /u/Hairetsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/cntrlai/notate"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifj5qj/new_experimental_agent_layer_reasoning_layer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifj5qj/new_experimental_agent_layer_reasoning_layer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T23:12:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifhv4r</id>
    <title>Running DeepSeek on AWS</title>
    <updated>2025-02-01T22:12:33+00:00</updated>
    <author>
      <name>/u/immediate_a982</name>
      <uri>https://old.reddit.com/user/immediate_a982</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifhv4r/running_deepseek_on_aws/"&gt; &lt;img alt="Running DeepSeek on AWS" src="https://preview.redd.it/gzokp1bbqlge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a326aaf3ccff66f5454eaa09777e6616dfac0ef" title="Running DeepSeek on AWS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, anyone has tried to run DeepSeck on AWS or Azure? Any pointers you can share. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/immediate_a982"&gt; /u/immediate_a982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gzokp1bbqlge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifhv4r/running_deepseek_on_aws/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifhv4r/running_deepseek_on_aws/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T22:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ift60f</id>
    <title>Ollama not loaded into GPU in the WSL? does anyone know why?</title>
    <updated>2025-02-02T08:51:34+00:00</updated>
    <author>
      <name>/u/GTHell</name>
      <uri>https://old.reddit.com/user/GTHell</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I setup a new system with my 3090 and installed Arch linux into the WSL2. Upon installed cuda-toolkit both on Windows and WSL I got the GPU working. The torch.cuda does show that the cuda device is available. The nvidia-smi in the WSL also showing the information of the GPU but when I do ollama run or ollama serve I got a message saying &amp;quot;no cuda runners detected unabble to run on cuda GPU&amp;quot;.&lt;/p&gt; &lt;p&gt;Does anyone know how to bypass this? My gaming laptop with 4070 uses WSL for development as well and when loading the model it loaded into VRAM instead of RAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GTHell"&gt; /u/GTHell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ift60f/ollama_not_loaded_into_gpu_in_the_wsl_does_anyone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ift60f/ollama_not_loaded_into_gpu_in_the_wsl_does_anyone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ift60f/ollama_not_loaded_into_gpu_in_the_wsl_does_anyone/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T08:51:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifqrug</id>
    <title>Question--CPU vs GPU utilization</title>
    <updated>2025-02-02T06:03:34+00:00</updated>
    <author>
      <name>/u/w38122077</name>
      <uri>https://old.reddit.com/user/w38122077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all. I'm new to Ollama and I have a question about how / where it decides to execute.&lt;/p&gt; &lt;p&gt;Ubuntu in Proxmox:&lt;/p&gt; &lt;p&gt;Linux: Linux version 6.8.0-52-generic (buildd@lcy02-amd64-046) Build: (x86_64-linux-gnu-gcc-13 (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0, GNU ld (GNU Binutils for Ubuntu) 2.42)&lt;br /&gt; Release : 6.8.0-52-generic &lt;br /&gt; Version : #53-Ubuntu SMP PREEMPT_DYNAMIC Sat Jan 11 00:06:25 UTC 2025&lt;br /&gt; # of CPUs: 60&lt;br /&gt; Machine : x86_64&lt;/p&gt; &lt;p&gt;There are (2) Intel(R) Xeon(R) Gold 6226R CPUs or which I have 60 vCPUs configured through in &amp;quot;host&amp;quot; mode.&lt;/p&gt; &lt;p&gt;I have 84GB of DDR4 2933MHz RAM allocated.&lt;/p&gt; &lt;p&gt;I have (2) NVIDIA RTX 4000 SFF Ada GPUs passed through to the VM.&lt;/p&gt; &lt;p&gt;I am executing &amp;quot;ollama run deepseek-r1:70b&amp;quot;&lt;/p&gt; &lt;p&gt;My question is: &lt;/p&gt; &lt;p&gt;Why do my GPUs stay relatively idle while the CPU is spiked near 100% when responding to prompts?&lt;/p&gt; &lt;p&gt;ollama ps&lt;br /&gt; NAME ID SIZE PROCESSOR UNTIL &lt;br /&gt; deepseek-r1:70b 0c1615a8ca32 47 GB 14%/86% CPU/GPU Forever &lt;/p&gt; &lt;p&gt;So I know it's mostly loaded on the GPUs and I do see spikes of utilization. Is this normal?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w38122077"&gt; /u/w38122077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifqrug/questioncpu_vs_gpu_utilization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifqrug/questioncpu_vs_gpu_utilization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifqrug/questioncpu_vs_gpu_utilization/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T06:03:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifu0nn</id>
    <title>Is there a difference between ollama's model loader and langchain's model loader and ollama cli?</title>
    <updated>2025-02-02T09:55:09+00:00</updated>
    <author>
      <name>/u/No-Comfort3958</name>
      <uri>https://old.reddit.com/user/No-Comfort3958</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I generally use langchain' to load my ollama models, but I have been trying to work with llava, it seems dumb when I work with it in to describe an image in langchain. However when I tried same image with ollama cli, it responded far better. So does the model loader I am using actually affect the performance of the models? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Comfort3958"&gt; /u/No-Comfort3958 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifu0nn/is_there_a_difference_between_ollamas_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifu0nn/is_there_a_difference_between_ollamas_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifu0nn/is_there_a_difference_between_ollamas_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T09:55:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifub4k</id>
    <title>ollama install on macosx - formulae vs cask</title>
    <updated>2025-02-02T10:16:15+00:00</updated>
    <author>
      <name>/u/low_depo</name>
      <uri>https://old.reddit.com/user/low_depo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am using brew to install packages on macosx and there are two ways to install ollama:&lt;/p&gt; &lt;p&gt;- formulae - building&lt;/p&gt; &lt;p&gt;- cask - downloading app&lt;/p&gt; &lt;p&gt;I see that formulae is much more popular,&lt;/p&gt; &lt;p&gt;Are there any advantages to this solution being more popular?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/low_depo"&gt; /u/low_depo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifub4k/ollama_install_on_macosx_formulae_vs_cask/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifub4k/ollama_install_on_macosx_formulae_vs_cask/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifub4k/ollama_install_on_macosx_formulae_vs_cask/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T10:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifue87</id>
    <title>Why models don't use GPU?</title>
    <updated>2025-02-02T10:22:33+00:00</updated>
    <author>
      <name>/u/hn-mc</name>
      <uri>https://old.reddit.com/user/hn-mc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried to see what models do to computer resources in Task Manager, and what I've noticed is that while models run, CPU and RAM usage is very high, but the GPU isn't affected at all. Why is that?&lt;br /&gt; And I have a dedicated GPU with 4GB Nvidia GeForce 1050 Ti.&lt;/p&gt; &lt;p&gt;It's not used at all by models. Is there a way to configure Ollama in such a way, that it tries to use GPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hn-mc"&gt; /u/hn-mc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifue87/why_models_dont_use_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifue87/why_models_dont_use_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifue87/why_models_dont_use_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T10:22:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifv422</id>
    <title>Can someone clarify the subtypes of models (quantization, text vs instruct, etc.)?</title>
    <updated>2025-02-02T11:13:29+00:00</updated>
    <author>
      <name>/u/hn-mc</name>
      <uri>https://old.reddit.com/user/hn-mc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed that models come in many versions, but I'm a little confused about it.&lt;/p&gt; &lt;p&gt;First there are &amp;quot;instruct&amp;quot; models and &amp;quot;text&amp;quot; models? What's the difference?&lt;/p&gt; &lt;p&gt;Second, I know that quantization is a type of compression, and the bigger the model in gigabytes, the less compression, and therefore higher quality, but at cost of hardware demands and speed. I know this general principle. But I don't know what exactly these quantization types mean. For example, I've seen all these types of quantization:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;fp16&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q2_K&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q3_K_L&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q3_K_M&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q3_K_S&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q4_0&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q4_1&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q4_K_M&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q4_K_S&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q5_0&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q5_1&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q5_K_M&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q5_K_S&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q6_K&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q8_0&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;And they all come for TEXT models and INSTRUCT models?&lt;/p&gt; &lt;p&gt;How to make sense of all that mess?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hn-mc"&gt; /u/hn-mc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifv422/can_someone_clarify_the_subtypes_of_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifv422/can_someone_clarify_the_subtypes_of_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifv422/can_someone_clarify_the_subtypes_of_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T11:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifa93h</id>
    <title>DeepSeek R1 Hardware Requirements Explained</title>
    <updated>2025-02-01T16:39:48+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifa93h/deepseek_r1_hardware_requirements_explained/"&gt; &lt;img alt="DeepSeek R1 Hardware Requirements Explained" src="https://external-preview.redd.it/bavhQxeXV5pgAp-fBIVoF8XffcIw7GN5u11i9CCbtIY.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2a472a729ce975767359b47dc4788711949124d4" title="DeepSeek R1 Hardware Requirements Explained" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/5RhPZgDoglE"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifa93h/deepseek_r1_hardware_requirements_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifa93h/deepseek_r1_hardware_requirements_explained/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T16:39:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifqglv</id>
    <title>Using Local LLMS to read text images, pdf, excel, programming scripts and also output modified text files and programming scripts</title>
    <updated>2025-02-02T05:44:33+00:00</updated>
    <author>
      <name>/u/sheik_ali</name>
      <uri>https://old.reddit.com/user/sheik_ali</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifqglv/using_local_llms_to_read_text_images_pdf_excel/"&gt; &lt;img alt="Using Local LLMS to read text images, pdf, excel, programming scripts and also output modified text files and programming scripts" src="https://a.thumbs.redditmedia.com/xHY4X2cd7XDsA6zaZTJJ-cfhkKfmWIaRxyRRqrfz894.jpg" title="Using Local LLMS to read text images, pdf, excel, programming scripts and also output modified text files and programming scripts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Guys,&lt;/p&gt; &lt;p&gt;I would like to ask, I am using Ollama to run local LLMs, for now as yall might know since Ollama is run on a command prompt, one can only send text to the LLM for it to read and interpret. I understand there is a way to convert the files to a text for the LLMs to read and interpret, but what if the files I want to modify with the LLM's help is too big to convert to text to send in the command prompt? Is there a similiar function/feature to attach files to send in a prompt to the local LLMs just like in chatgpt prompt UI as seen in the attached image/GIF?&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/5m1yvitwynge1.gif"&gt;https://i.redd.it/5m1yvitwynge1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xfxsiegxynge1.png?width=288&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cfc2fa0ae1763d86bbaec4803d5128f84a548058"&gt;https://preview.redd.it/xfxsiegxynge1.png?width=288&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cfc2fa0ae1763d86bbaec4803d5128f84a548058&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sheik_ali"&gt; /u/sheik_ali &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifqglv/using_local_llms_to_read_text_images_pdf_excel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifqglv/using_local_llms_to_read_text_images_pdf_excel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifqglv/using_local_llms_to_read_text_images_pdf_excel/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T05:44:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifuzv3</id>
    <title>Introducing AIBench: Your Ultimate iPhone Tool for Mobile AI Performance Testing!</title>
    <updated>2025-02-02T11:05:36+00:00</updated>
    <author>
      <name>/u/Snoo_24581</name>
      <uri>https://old.reddit.com/user/Snoo_24581</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever wondered how your iPhone handles cutting-edge AI models? Meet &lt;strong&gt;AIBench&lt;/strong&gt; – a free, privacy-focused app that lets you test, analyze, and interact with popular large language models (LLMs) &lt;strong&gt;offline&lt;/strong&gt; on your device. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;🌟 Key Features:&lt;/strong&gt;&lt;br /&gt; - &lt;strong&gt;Cutting-Edge Models&lt;/strong&gt;: Run DeepSeek-R1 (distilled), Llama, Qwen, and more directly on your iPhone.&lt;br /&gt; - &lt;strong&gt;Pro-Level Analysis&lt;/strong&gt;: Monitor real-time metrics like inference speed, GPU usage, and memory consumption.&lt;br /&gt; - &lt;strong&gt;Built-In Benchmarks&lt;/strong&gt;: Test models across scenarios like translation, summarization, and text generation.&lt;br /&gt; - &lt;strong&gt;Customizable Settings&lt;/strong&gt;: Adjust token counts and sampling temperature for tailored experiments.&lt;br /&gt; - &lt;strong&gt;Data Visualization&lt;/strong&gt;: Track performance trends and system resource usage with intuitive graphs. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;🎯 Perfect For:&lt;/strong&gt;&lt;br /&gt; - &lt;strong&gt;Developers&lt;/strong&gt; optimizing mobile AI performance.&lt;br /&gt; - &lt;strong&gt;Researchers&lt;/strong&gt; needing portable evaluation tools.&lt;br /&gt; - &lt;strong&gt;Tech Enthusiasts&lt;/strong&gt; curious about on-device AI capabilities. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;🔒 Privacy First:&lt;/strong&gt; All computations happen locally – your data never leaves your device. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;📱 Compatibility:&lt;/strong&gt; Requires Metal support. Best experience on iPhone 15 Pro/newer models. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Download Now&lt;/strong&gt;: [AIBench on the App Store]() (link in comments) &lt;/p&gt; &lt;p&gt;Whether you’re tweaking models, benchmarking, or just geeking out over mobile AI, AIBench makes it effortless. Let’s push those iPhone GPUs to the limit! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snoo_24581"&gt; /u/Snoo_24581 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://apps.apple.com/cn/app/aibench-%E7%A7%BB%E5%8A%A8%E7%AB%AFai%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/id6741204584"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifuzv3/introducing_aibench_your_ultimate_iphone_tool_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifuzv3/introducing_aibench_your_ultimate_iphone_tool_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T11:05:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1iffzhn</id>
    <title>I created a web UI for Ollama that lets you talk to your models and manage them</title>
    <updated>2025-02-01T20:48:26+00:00</updated>
    <author>
      <name>/u/Ok_Promotion_9578</name>
      <uri>https://old.reddit.com/user/Ok_Promotion_9578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iffzhn/i_created_a_web_ui_for_ollama_that_lets_you_talk/"&gt; &lt;img alt="I created a web UI for Ollama that lets you talk to your models and manage them" src="https://external-preview.redd.it/MbCNCUfEvON9rMG6_Ug9gghhk5NKAhUN5fohRsQ5Kk0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bca5176e36c712849de99c8520c0e7c3eaa142c4" title="I created a web UI for Ollama that lets you talk to your models and manage them" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I believe that are some excellent solutions for this problem, but I wanted to take a stab at making my own and focusing on making it super light weight, minimalistic, and clean.&lt;/p&gt; &lt;p&gt;It is still in the early stages, but I'd love some feedback on what I've built so far and to hear about what you'd like to see in a solution like this! &lt;/p&gt; &lt;p&gt;I'm thinking about really cool things in the roadmap down the line, but wanted to start simple and involve the community.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/majicmaj/aloha"&gt;https://github.com/majicmaj/aloha&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xu3brq22blge1.png?width=750&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78b7bbe65ab50348fe373ed1e35d6e80e51e5650"&gt;https://preview.redd.it/xu3brq22blge1.png?width=750&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78b7bbe65ab50348fe373ed1e35d6e80e51e5650&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Promotion_9578"&gt; /u/Ok_Promotion_9578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iffzhn/i_created_a_web_ui_for_ollama_that_lets_you_talk/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iffzhn/i_created_a_web_ui_for_ollama_that_lets_you_talk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iffzhn/i_created_a_web_ui_for_ollama_that_lets_you_talk/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T20:48:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifkdid</id>
    <title>Run DeepSeek-R1 Locally with Ollama and Open-WebUI (Docker Compose)</title>
    <updated>2025-02-02T00:10:22+00:00</updated>
    <author>
      <name>/u/ntalekt</name>
      <uri>https://old.reddit.com/user/ntalekt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deploy DeepSeek-R1 on your local machine using Ollama and Open-WebUI with this Docker Compose setup. Perfect for those without GPU hardware who want to experiment with AI models.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clone the repo&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;code&gt;git clone&lt;/code&gt; &lt;a href="https://github.com/ntalekt/deepseek-r1-docker-compose.git"&gt;&lt;code&gt;https://github.com/ntalekt/deepseek-r1-docker-compose.git&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Start services&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;code&gt;docker compose up -d&lt;/code&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Access Web UI: &lt;a href="http://localhost:3000/"&gt;http://localhost:3000&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;CPU-only setup (no GPU required)&lt;/li&gt; &lt;li&gt;Automatic download of deepseek-r1:8b model&lt;/li&gt; &lt;li&gt;Easy installation and management with Docker Compose&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Requirements:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Docker Engine v20.10.10+&lt;/li&gt; &lt;li&gt;Docker Compose v2.20.0+&lt;/li&gt; &lt;li&gt;8GB RAM (16GB recommended)&lt;/li&gt; &lt;li&gt;20GB+ free disk space&lt;/li&gt; &lt;li&gt;Linux/macOS/WSL2&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Note:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;CPU inference will be slower than GPU-accelerated setups. Consider GPU hardware for production use.&lt;/li&gt; &lt;li&gt;License: MIT&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full repository: &lt;a href="https://github.com/ntalekt/deepseek-r1-docker-compose"&gt;https://github.com/ntalekt/deepseek-r1-docker-compose&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ntalekt"&gt; /u/ntalekt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifkdid/run_deepseekr1_locally_with_ollama_and_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifkdid/run_deepseekr1_locally_with_ollama_and_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifkdid/run_deepseekr1_locally_with_ollama_and_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T00:10:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifvbgp</id>
    <title>Can we really do something with deepseek-r1:1.5b?</title>
    <updated>2025-02-02T11:28:07+00:00</updated>
    <author>
      <name>/u/Inevitable-Judge2642</name>
      <uri>https://old.reddit.com/user/Inevitable-Judge2642</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifvbgp/can_we_really_do_something_with_deepseekr115b/"&gt; &lt;img alt="Can we really do something with deepseek-r1:1.5b?" src="https://external-preview.redd.it/eyfcbevjKo97vLjBfB0Fmj0NwFo3-R0O6txnn7zhQLY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb92958779590d19aea5be1f847281f6de982d04" title="Can we really do something with deepseek-r1:1.5b?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Judge2642"&gt; /u/Inevitable-Judge2642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://k33g.hashnode.dev/can-we-really-do-something-with-deepseek-r115b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifvbgp/can_we_really_do_something_with_deepseekr115b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifvbgp/can_we_really_do_something_with_deepseekr115b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T11:28:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1if4p38</id>
    <title>Been messing around with DeepSeek R1 + Ollama, and honestly, it's kinda wild how much you can do locally with free open-source tools. No cloud, no API keys, just your machine and some cool AI magic.</title>
    <updated>2025-02-01T11:50:04+00:00</updated>
    <author>
      <name>/u/hasan_py</name>
      <uri>https://old.reddit.com/user/hasan_py</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;Page-Assist Chrome Extension - &lt;a href="https://github.com/n4ze3m/page-assist"&gt;https://github.com/n4ze3m/page-assist&lt;/a&gt; (any model with any params)&lt;/li&gt; &lt;li&gt;Open Web-UI LLM Wrapper - &lt;a href="https://github.com/open-webui/open-webui"&gt;https://github.com/open-webui/open-webui&lt;/a&gt; (any model with any params)&lt;/li&gt; &lt;li&gt;Browser use – &lt;a href="https://github.com/browser-use/browser-use"&gt;https://github.com/browser-use/browser-use&lt;/a&gt; (deepseek r1:14b or more params) &lt;/li&gt; &lt;li&gt;Roo-Code (VS Code Extension) – &lt;a href="https://github.com/RooVetGit/Roo-Code"&gt;https://github.com/RooVetGit/Roo-Code&lt;/a&gt; (deepseek coder)&lt;/li&gt; &lt;li&gt;n8n – &lt;a href="https://github.com/n8n-io/n8n"&gt;https://github.com/n8n-io/n8n&lt;/a&gt; (any model with any params)&lt;/li&gt; &lt;li&gt;A simple RAG app: &lt;a href="https://github.com/hasan-py/chat-with-pdf-RAG"&gt;https://github.com/hasan-py/chat-with-pdf-RAG&lt;/a&gt; (deepseek r1:8b)&lt;/li&gt; &lt;li&gt;Ai assistant Chrome extension: &lt;a href="https://github.com/hasan-py/Ai-Assistant-Chrome-Extension"&gt;https://github.com/hasan-py/Ai-Assistant-Chrome-Extension&lt;/a&gt; (GPT, Gemini, Grok Api, Ollama added recently)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Full installation video: &lt;a href="https://youtu.be/hjg9kJs8al8?si=rillpsKpjONYMDYW"&gt;https://youtu.be/hjg9kJs8al8?si=rillpsKpjONYMDYW&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone exploring something else? Please share- it would be highly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hasan_py"&gt; /u/hasan_py &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if4p38/been_messing_around_with_deepseek_r1_ollama_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if4p38/been_messing_around_with_deepseek_r1_ollama_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if4p38/been_messing_around_with_deepseek_r1_ollama_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T11:50:04+00:00</published>
  </entry>
</feed>
