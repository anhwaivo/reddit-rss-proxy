<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-05T06:08:17+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1j324wb</id>
    <title>Proper local llm</title>
    <updated>2025-03-04T04:00:47+00:00</updated>
    <author>
      <name>/u/Daedric800</name>
      <uri>https://old.reddit.com/user/Daedric800</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys i need help to find local light weight Ilm that is specified and fine-tuned just for the task of coding, which means a model that is trained only for coding and nothing else which make it very light weight and small in size since it does not do chat, math, etc.. which makes it small in size yet powerful in coding like claude or deepseek models, i cant see why i havent came across a model like that yet, why are not people making a specific coding models, we are at 2025, so please if you have a model with these specs please do tell me, so i could use it for a proper coding tasks on my low end gpu locally or maybe someonw of you guys could train a simple unsloth model for just coding and upload it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daedric800"&gt; /u/Daedric800 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j324wb/proper_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j324wb/proper_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j324wb/proper_local_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T04:00:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j32eir</id>
    <title>Need help with an error message</title>
    <updated>2025-03-04T04:13:55+00:00</updated>
    <author>
      <name>/u/First_Handle_7722</name>
      <uri>https://old.reddit.com/user/First_Handle_7722</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m getting an error “model requires more system memory (747.4 MiB) than is available (694.8 MiB)” how can I fix it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/First_Handle_7722"&gt; /u/First_Handle_7722 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j32eir/need_help_with_an_error_message/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j32eir/need_help_with_an_error_message/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j32eir/need_help_with_an_error_message/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T04:13:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3550x</id>
    <title>Exam Questions Ai Saas</title>
    <updated>2025-03-04T07:04:55+00:00</updated>
    <author>
      <name>/u/Dry_Ingenuity_8009</name>
      <uri>https://old.reddit.com/user/Dry_Ingenuity_8009</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I working on make Saas that help teachers to put exam questions for high school In Egypt I have noticed that this process take from teachers a lot of time and money so the system is like this I ask the ai for like 100 low-level,mid-level or high-level questions for lets say physics subject so it has to give me exactly the 100 question with the chosen level or a group of levels so I want also this to be generated question not just retrieved from the knowledge base to prevent any copyright issues so what is the best technique to achieve this using fine tune only or rag or both of them and if there any one have the right way to do it please tell me &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Ingenuity_8009"&gt; /u/Dry_Ingenuity_8009 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3550x/exam_questions_ai_saas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3550x/exam_questions_ai_saas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3550x/exam_questions_ai_saas/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T07:04:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2ibxv</id>
    <title>Chat with my own PDF documents</title>
    <updated>2025-03-03T13:11:14+00:00</updated>
    <author>
      <name>/u/9elpi8</name>
      <uri>https://old.reddit.com/user/9elpi8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, as title says I would like to chat with my PDF documents. Which model would you recommend me to use? Best would be with multilanguage support. I have Nvidia 4060Ti 16GB.&lt;/p&gt; &lt;p&gt;My idea is make several threads inside AnythingLLM where I would have my receipts in other thread books related to engineering or some other learning stuff.&lt;/p&gt; &lt;p&gt;Thank you for your recommendation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/9elpi8"&gt; /u/9elpi8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2ibxv/chat_with_my_own_pdf_documents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2ibxv/chat_with_my_own_pdf_documents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2ibxv/chat_with_my_own_pdf_documents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T13:11:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j36zmn</id>
    <title>tool calls, how?</title>
    <updated>2025-03-04T09:26:22+00:00</updated>
    <author>
      <name>/u/Expensive-Award1965</name>
      <uri>https://old.reddit.com/user/Expensive-Award1965</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i apologize for this not well thought out post. i'm just frustrated, perhaps because i don't understand python, but i think mostly i have no idea how the thing actually calls the tools? i don't understand how it knows to call the functions, does it just come across a bit of the prompt and think oh i need to call this function so i can get that information? &lt;/p&gt; &lt;p&gt;is there like a way to call php lol? does anyone have a tool call that will then call php that i can use? &lt;/p&gt; &lt;p&gt;like the example has an array of tools right, but where does it call those tools from? where's the `get_current_weather` functions at? how do i define it?&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import ollama response = ollama.chat( model='llama3.1', messages=[{'role': 'user', 'content': 'What is the weather in Toronto?'}], # provide a weather checking tool to the model tools=[{ 'type': 'function', 'function': { 'name': 'get_current_weather', 'description': 'Get the current weather for a city', 'parameters': { 'type': 'object', 'properties': { 'city': { 'type': 'string', 'description': 'The name of the city', }, }, 'required': ['city'], }, }, }, ], ) print(response['message']['tool_calls'])import ollama &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expensive-Award1965"&gt; /u/Expensive-Award1965 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j36zmn/tool_calls_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j36zmn/tool_calls_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j36zmn/tool_calls_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T09:26:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1j383tj</id>
    <title>Looking for a Local AI Model Manager with API Proxy &amp; Web Interface</title>
    <updated>2025-03-04T10:48:41+00:00</updated>
    <author>
      <name>/u/Niutaokkul</name>
      <uri>https://old.reddit.com/user/Niutaokkul</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm looking for a self-hosted solution to manage AI models and monitor API usage, ideally with a web interface for easy administration.&lt;/p&gt; &lt;h1&gt;My needs:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I have an &lt;strong&gt;OpenAI API key&lt;/strong&gt; provided by my company, but I &lt;strong&gt;don't have access to usage stats&lt;/strong&gt; (requests made, tokens consumed).&lt;/li&gt; &lt;li&gt;I also want to &lt;strong&gt;run smaller local models&lt;/strong&gt; (like Ollama) for certain tasks without always relying on OpenAI.&lt;/li&gt; &lt;li&gt;Ideally, the platform should: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Host and serve local models&lt;/strong&gt; (e.g., Ollama)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Act as a proxy/API gateway&lt;/strong&gt; for OpenAI keys&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Log and track API usage&lt;/strong&gt; (requests, token counts, etc.)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Provide a web interface&lt;/strong&gt; to monitor activity and manage models easily&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I came across &lt;strong&gt;AI-Server by ServiceStack&lt;/strong&gt;, but it seems more like a client for interacting with models rather than a full-fledged management solution.&lt;/p&gt; &lt;p&gt;Is there any &lt;strong&gt;open-source or self-hosted&lt;/strong&gt; tool that fits these needs?&lt;/p&gt; &lt;p&gt;Thanks in advance for any recommendations!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Niutaokkul"&gt; /u/Niutaokkul &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j383tj/looking_for_a_local_ai_model_manager_with_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j383tj/looking_for_a_local_ai_model_manager_with_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j383tj/looking_for_a_local_ai_model_manager_with_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T10:48:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3d5uj</id>
    <title>How to setup local Hosted AI API for coded project?</title>
    <updated>2025-03-04T15:22:04+00:00</updated>
    <author>
      <name>/u/Shot-Negotiation5968</name>
      <uri>https://old.reddit.com/user/Shot-Negotiation5968</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have coded a project (AI Chat) in html and I installed Ollama llama2 locally. I want to request the AI with API on my coded project, Could you please help me how to do that? I found nothing on Youtube for this certain case Thank you &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shot-Negotiation5968"&gt; /u/Shot-Negotiation5968 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3d5uj/how_to_setup_local_hosted_ai_api_for_coded_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3d5uj/how_to_setup_local_hosted_ai_api_for_coded_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3d5uj/how_to_setup_local_hosted_ai_api_for_coded_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T15:22:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1j30893</id>
    <title>Qwen2.5 32b will start to put the tool calls in the content instead of the tool_calls</title>
    <updated>2025-03-04T02:20:23+00:00</updated>
    <author>
      <name>/u/nstevnc77</name>
      <uri>https://old.reddit.com/user/nstevnc77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I've been building a small application with Ollama for personal use that involves tool calling. I've been really impressed with Qwen2.5's ability to figure out when to do tool calls, which tools to use, and its overall reliability.&lt;/p&gt; &lt;p&gt;The only problem I've been running into is that Qwen2.5 will start putting its tool calls (JSON) in the content instead of the proper tool_calls part of the JSON. This is frustrating because it works so well otherwise.&lt;/p&gt; &lt;p&gt;It always seems to get the tool calls correct in the beginning, but about 20-40 messages in, it just starts putting the JSON in the content. Has anyone found a solution to this issue? I'm thinking that maybe because I'm saving those tool call messages in its list of messages or I'm adding &amp;quot;toolresult&amp;quot; responses that maybe it's getting confused?&lt;/p&gt; &lt;p&gt;Just wanted to see if anybody has had a similar experience!&lt;/p&gt; &lt;p&gt;Edit: I've tried llama models but they will ALWAYS call tools given the chance. Not very useful for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nstevnc77"&gt; /u/nstevnc77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j30893/qwen25_32b_will_start_to_put_the_tool_calls_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j30893/qwen25_32b_will_start_to_put_the_tool_calls_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j30893/qwen25_32b_will_start_to_put_the_tool_calls_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T02:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j39jx2</id>
    <title>Nvidia and AMD graphics card at the same time in ollama?</title>
    <updated>2025-03-04T12:22:32+00:00</updated>
    <author>
      <name>/u/Other_Button_3775</name>
      <uri>https://old.reddit.com/user/Other_Button_3775</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently running Ollama on an Ubuntu system with a Nvidia 3060ti and an AMD ROG RX580. I'm trying to set it up so that Ollama uses the 3060ti primarily and falls back to the RX580 if needed.&lt;/p&gt; &lt;p&gt;Has anyone had experience with this kind of setup? Is it even possible? Are there any specific configurations or settings I should be aware of to make sure both GPUs are utilized effectively?&lt;/p&gt; &lt;p&gt;Any help or insights would be greatly appreciated! Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Other_Button_3775"&gt; /u/Other_Button_3775 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j39jx2/nvidia_and_amd_graphics_card_at_the_same_time_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j39jx2/nvidia_and_amd_graphics_card_at_the_same_time_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j39jx2/nvidia_and_amd_graphics_card_at_the_same_time_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T12:22:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3hyxo</id>
    <title>Why we can't run vlm in ollama ? I want to run qwen2.5 VL 3b locally with ollama</title>
    <updated>2025-03-04T18:38:22+00:00</updated>
    <author>
      <name>/u/Kuggy1105</name>
      <uri>https://old.reddit.com/user/Kuggy1105</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kuggy1105"&gt; /u/Kuggy1105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3hyxo/why_we_cant_run_vlm_in_ollama_i_want_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3hyxo/why_we_cant_run_vlm_in_ollama_i_want_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3hyxo/why_we_cant_run_vlm_in_ollama_i_want_to_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T18:38:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1j35s3m</id>
    <title>The best small tools calls llm</title>
    <updated>2025-03-04T07:52:32+00:00</updated>
    <author>
      <name>/u/Ok-Masterpiece-0000</name>
      <uri>https://old.reddit.com/user/Ok-Masterpiece-0000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please people, I would like some help. I want to get the small open source llm like qwen2.5:3b or Mistral or some other to produce a correct tools, and even now to call the tools when they are available. HELP I tried everything but 00. Only big LLM like OpenAI one and other …&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Masterpiece-0000"&gt; /u/Ok-Masterpiece-0000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j35s3m/the_best_small_tools_calls_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j35s3m/the_best_small_tools_calls_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j35s3m/the_best_small_tools_calls_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T07:52:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3e54w</id>
    <title>Hardware recommendations</title>
    <updated>2025-03-04T16:04:17+00:00</updated>
    <author>
      <name>/u/Squirrel_daddy</name>
      <uri>https://old.reddit.com/user/Squirrel_daddy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working on a proof of concept AI rag application for a client. I have a budget of $5-$10k for hardware to use as a development and R&amp;amp;D setup. Does anyone have any recommendations as to what they would look at. I would love to run the largest mistal model i can, and I'm not concerned with hd storage in my budget number. Also I'm not opposed to used hardware nor am I really concerned with power efficiency. Just wanted peoples thoughts on best bang for buck options I may not of considered. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Squirrel_daddy"&gt; /u/Squirrel_daddy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3e54w/hardware_recommendations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3e54w/hardware_recommendations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3e54w/hardware_recommendations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T16:04:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2j8nt</id>
    <title>I open-sourced Klee today, an Ollama GUI designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities.</title>
    <updated>2025-03-03T13:58:46+00:00</updated>
    <author>
      <name>/u/w-zhong</name>
      <uri>https://old.reddit.com/user/w-zhong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j2j8nt/i_opensourced_klee_today_an_ollama_gui_designed/"&gt; &lt;img alt="I open-sourced Klee today, an Ollama GUI designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." src="https://preview.redd.it/e9n94wxjdhme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cea4c13efc5bd2a090237ef7c3fe7065ceb8f0d9" title="I open-sourced Klee today, an Ollama GUI designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w-zhong"&gt; /u/w-zhong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e9n94wxjdhme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2j8nt/i_opensourced_klee_today_an_ollama_gui_designed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2j8nt/i_opensourced_klee_today_an_ollama_gui_designed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T13:58:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3ashf</id>
    <title>Ollama on Windows isn't using my RTX 3090</title>
    <updated>2025-03-04T13:30:25+00:00</updated>
    <author>
      <name>/u/XALC1</name>
      <uri>https://old.reddit.com/user/XALC1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;As the title says my Windows 11 install isn't using my GPU but the CPU. I'm up to date on Windows and NVIDIA drivers. I'm not using docker. Could anyone help me troubleshoot?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XALC1"&gt; /u/XALC1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3ashf/ollama_on_windows_isnt_using_my_rtx_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3ashf/ollama_on_windows_isnt_using_my_rtx_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3ashf/ollama_on_windows_isnt_using_my_rtx_3090/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T13:30:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3pdcm</id>
    <title>Running LLM Training Examples + 8x AMD Instinct Mi60 Server + PYTORCH</title>
    <updated>2025-03-04T23:49:42+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1ob1zr4sfrme1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3pdcm/running_llm_training_examples_8x_amd_instinct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3pdcm/running_llm_training_examples_8x_amd_instinct/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T23:49:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j35t4t</id>
    <title>Recommendations for small but capable LLMs?</title>
    <updated>2025-03-04T07:54:48+00:00</updated>
    <author>
      <name>/u/Apart_Cause_6382</name>
      <uri>https://old.reddit.com/user/Apart_Cause_6382</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From what i understand, the smaller the number of parameters is, the faster the model is and the smaller is it's filesize, but the smaller amount of knowledge it has&lt;/p&gt; &lt;p&gt;I am searching for a very fast yet knowledgeful LLM, any recommendations? Thank you in advance for any comments&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apart_Cause_6382"&gt; /u/Apart_Cause_6382 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j35t4t/recommendations_for_small_but_capable_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j35t4t/recommendations_for_small_but_capable_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j35t4t/recommendations_for_small_but_capable_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T07:54:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3tash</id>
    <title>How to use local ai tools to help aid a beginner to make a video game?</title>
    <updated>2025-03-05T03:01:11+00:00</updated>
    <author>
      <name>/u/Game-Lover44</name>
      <uri>https://old.reddit.com/user/Game-Lover44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im a bit confused on how i should use ai to help in the process of making a game and what tools i can use along with models. People tell me to ask the ai or just do it, but that makes me more stumped.&lt;/p&gt; &lt;p&gt;What would you advise?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Game-Lover44"&gt; /u/Game-Lover44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3tash/how_to_use_local_ai_tools_to_help_aid_a_beginner/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3tash/how_to_use_local_ai_tools_to_help_aid_a_beginner/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3tash/how_to_use_local_ai_tools_to_help_aid_a_beginner/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T03:01:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3qsic</id>
    <title>Am I limited to 14b models on my AMD 7900xt?</title>
    <updated>2025-03-05T00:56:31+00:00</updated>
    <author>
      <name>/u/halfam</name>
      <uri>https://old.reddit.com/user/halfam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It has 20GB VRAM and I wish I had a 24GB card. What kind of models are best with the 20GB?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/halfam"&gt; /u/halfam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3qsic/am_i_limited_to_14b_models_on_my_amd_7900xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3qsic/am_i_limited_to_14b_models_on_my_amd_7900xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3qsic/am_i_limited_to_14b_models_on_my_amd_7900xt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T00:56:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3g7i1</id>
    <title>GPU vs. CPU: Deepseek R1 Distill Qwen</title>
    <updated>2025-03-04T17:27:49+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j3g7i1/gpu_vs_cpu_deepseek_r1_distill_qwen/"&gt; &lt;img alt="GPU vs. CPU: Deepseek R1 Distill Qwen" src="https://external-preview.redd.it/Q5jlmvwaxhZCXswmxVeacfiYbEM6IsziXkzDlS6KY3g.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=464630f84bff9a55c18a79fd3c34d59aa503a988" title="GPU vs. CPU: Deepseek R1 Distill Qwen" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/teWusSZoQ-M"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3g7i1/gpu_vs_cpu_deepseek_r1_distill_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3g7i1/gpu_vs_cpu_deepseek_r1_distill_qwen/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T17:27:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3uc08</id>
    <title>Anyone managed to use free LLM models in cursor editor?</title>
    <updated>2025-03-05T03:56:35+00:00</updated>
    <author>
      <name>/u/blnkslt</name>
      <uri>https://old.reddit.com/user/blnkslt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j3uc08/anyone_managed_to_use_free_llm_models_in_cursor/"&gt; &lt;img alt="Anyone managed to use free LLM models in cursor editor?" src="https://external-preview.redd.it/DVMPuS2zjSAzR8RNilSvelF0QhBxxZrn22PYYxcxQcw.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de4b5fc3ab79b856565c38b6eca8d6e9fa679e4b" title="Anyone managed to use free LLM models in cursor editor?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It used to be possible to make cursor to use your local model by prxying your localhost through something like ngrok. But when I followed &lt;a href="https://www.youtube.com/watch?v=Q_sy45XggeM"&gt;this tutorial&lt;/a&gt; to make use of my local Qwen model, I failed and got this error below. Seems like they have closed the loophole to cage you into their paid models. So I'm wondering if any one recently has been successful in using thier own free ai models with cursor? &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/we1f2wwhnsme1.png?width=1032&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2d7ccaff388b4528ecf36972c0e16285fea51e36"&gt;https://preview.redd.it/we1f2wwhnsme1.png?width=1032&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2d7ccaff388b4528ecf36972c0e16285fea51e36&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blnkslt"&gt; /u/blnkslt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3uc08/anyone_managed_to_use_free_llm_models_in_cursor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3uc08/anyone_managed_to_use_free_llm_models_in_cursor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3uc08/anyone_managed_to_use_free_llm_models_in_cursor/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T03:56:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1j38dim</id>
    <title>Tell me why NVIDIA isn't gatekeeping the future of AI for the wealthy with what ollama brings to every home of every family in the world.</title>
    <updated>2025-03-04T11:06:48+00:00</updated>
    <author>
      <name>/u/Low_Tune7301</name>
      <uri>https://old.reddit.com/user/Low_Tune7301</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR; 4090 shouldn't have been sunsetted, prices out a demographic of the entire world who can't afford the insane prices already and the new products while they're in a gold rush making $113b a year, jesus christ just make it all at this point as if you wouldn't scale everything up and sell to everyone.&lt;/p&gt; &lt;p&gt;I'm big mad lads, tell me why I'm wrong - it's incredible what tools like Ollama can bring to every single home in the world. Every human deserves to experience this revolution in their homes, ollama can support it and I think they're gate-keeping AI for the wealthy.&lt;/p&gt; &lt;p&gt;So we're in a gold rush where this company is the sole real winner of a race where every single human, store and commercial retailer is sitting on the edge of their seats for you to literally not even give new products but existing ones.&lt;/p&gt; &lt;p&gt;You can also literally have your cake and eat it too, you are the worlds leading innovator and driver for AI, you can sell cluster stacks of 50,000 H100's to big tech buddies, and you have every other human in the world playing at home crawling the web relentlessly - not even just for your new products but your old ones as well and people will love you for it forever.&lt;/p&gt; &lt;p&gt;You will be cemented in history for leading the hardware innovation that brought this experience to every home in the world, you have successfully changed the future.&lt;/p&gt; &lt;p&gt;Every product with 24GB+ of VRAM sold out everywhere, every new product you release sold at the drop of a hat - all over the entire world. You've won. You are the winner of capitalism.&lt;/p&gt; &lt;p&gt;So what does the winner receive? - A hundred and thirteen billion dollars in revenue just for last year alone.&lt;/p&gt; &lt;p&gt;So while the world is discovering fire again, the winner:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Sunsets the RTX 4090 - an incredibly performant product that could bring that revolution into every family home in the world at a reasonable price point. Ooh on that note &amp;gt;&lt;/li&gt; &lt;li&gt;NVIDIA is somehow magically undersupplied year on year yet keeps bringing out a new card and the cycle repeats. If every performant product is sold out across the entire world and you're big dog Jensen what do those meetings look like? Are people telling you they can't build enough factories? Did they get confused now after the first god knows how many you've already made? After three years those meetings are still just like nah sorry can't get it right?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It makes a lot of sense though if you've got an agenda. It doesn't make sense you wouldn't scale up and go all in we're in an insane new age you're driving and the world wants everything you could possibly offer.&lt;/p&gt; &lt;p&gt;I'm not even saying don't sell or whatever like run at capitalism big chief you've won I'm just saying sell to both sides because instead you choose to keep repeating the pattern of under-supplying and gatekeeping AI for the wealthy which is disgusting with what's possible with Ollama and tools.&lt;/p&gt; &lt;p&gt;I deliberately use the word choose because once is an accident, twice is coincidence, three times is a pattern.&lt;/p&gt; &lt;p&gt;But go on, keep promising the next big thing while under-supplying and gatekeeping this revolution, price an entire demographic of the world out and just keep making H100's for those clusters your big tech buddies want.&lt;/p&gt; &lt;p&gt;Look I'm just saying sell to both sides. The world is discovering fire again and every human in the world deserves to experience that, you can still have your cake and eat it too. Make everything.&lt;/p&gt; &lt;p&gt;What you might say to this is people can still use the products the big tech companies create and connect to commercial models which you're right but something about the fact they just commercialized hitting the enter key to the lower socio-economic demographics in the world just doesn't feel right to me but hey I never liked the pokies anyway.&lt;/p&gt; &lt;p&gt;So go, there's my ollama rant - tell me why i'm cooked for thinking a company who made $113.26 billion in revenue last year and god knows how much the years before shouldn't have figured it out at this point and isn't just gatekeeping AI which is disgusting in a world with what tools combined with ollama make possible for every family in the world.&lt;/p&gt; &lt;p&gt;God damn I hope the open source community distilling these models for cheaper hardware godspeed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Low_Tune7301"&gt; /u/Low_Tune7301 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j38dim/tell_me_why_nvidia_isnt_gatekeeping_the_future_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j38dim/tell_me_why_nvidia_isnt_gatekeeping_the_future_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j38dim/tell_me_why_nvidia_isnt_gatekeeping_the_future_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T11:06:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3smbl</id>
    <title>Recommendations for a 24GB VRAM card for running Ollama and mistral-small?</title>
    <updated>2025-03-05T02:26:32+00:00</updated>
    <author>
      <name>/u/THenrich</name>
      <uri>https://old.reddit.com/user/THenrich</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recommendations for a 24GB VRAM card for running Ollama and mistral-small?&lt;br /&gt; Preferably under $1000.&lt;/p&gt; &lt;p&gt;I might run larger models in the future. I am going to send thousands of prompts to Ollama so I need something performant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/THenrich"&gt; /u/THenrich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3smbl/recommendations_for_a_24gb_vram_card_for_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3smbl/recommendations_for_a_24gb_vram_card_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3smbl/recommendations_for_a_24gb_vram_card_for_running/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T02:26:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3riix</id>
    <title>OpenArc v1.0.1: openai endpoints, gradio dashboard with chat- get faster inference on intel CPUs, GPUs and NPUs</title>
    <updated>2025-03-05T01:31:25+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;My project, &lt;a href="https://github.com/SearchSavior/OpenArc"&gt;OpenArc&lt;/a&gt;, is an inference engine built with OpenVINO for leveraging hardware acceleration on Intel CPUs, GPUs and NPUs. Users can expect similar workflows to what's possible with Ollama, LM-Studio, Jan, OpenRouter, including a built in gradio chat, management dashboard and tools for working with Intel devices.&lt;/p&gt; &lt;p&gt;OpenArc is one of the first FOSS projects to offer a model agnostic serving engine taking full advantage of the OpenVINO runtime available from Transformers. Many other projects have support for OpenVINO as an extension but OpenArc features detailed documentation, GUI tools and discussion. Infer at the edge with text-based large language models with openai compatible endpoints tested with Gradio, OpenWebUI and SillyTavern. &lt;/p&gt; &lt;p&gt;Vision support is coming soon.&lt;/p&gt; &lt;p&gt;Since launch community support has been overwhelming; I even have a funding opportunity for OpenArc! For my first project that's pretty cool.&lt;/p&gt; &lt;p&gt;One thing we talked about was that OpenArc needs contributors who are excited about inference and getting good performance from their Intel devices.&lt;/p&gt; &lt;p&gt;Here's the ripcord:&lt;/p&gt; &lt;p&gt;An official &lt;a href="https://discord.gg/PnuTBVcr"&gt;Discord!&lt;/a&gt; - Best way to reach me. - If you are interested in contributing join the Discord!&lt;/p&gt; &lt;p&gt;Discussions on GitHub for:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SearchSavior/OpenArc/discussions/11"&gt;Linux Drivers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SearchSavior/OpenArc/discussions/12"&gt;Windows Drivers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SearchSavior/OpenArc/discussions/13"&gt;Environment Setup&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Instructions and models for testing out text generation for &lt;a href="https://github.com/SearchSavior/OpenArc/issues/14"&gt;NPU devices&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;A sister repo, &lt;a href="https://github.com/SearchSavior/OpenArcProjects"&gt;OpenArcProjects&lt;/a&gt;! - Share the things you build with OpenArc, OpenVINO, oneapi toolkit, IPEX-LLM and future tooling from Intel&lt;/p&gt; &lt;p&gt;Thanks for checking out OpenArc. I hope it ends up being a useful tool.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3riix/openarc_v101_openai_endpoints_gradio_dashboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3riix/openarc_v101_openai_endpoints_gradio_dashboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3riix/openarc_v101_openai_endpoints_gradio_dashboard/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-05T01:31:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3nwiw</id>
    <title>Generate an Entire Project from ONE Prompt</title>
    <updated>2025-03-04T22:44:22+00:00</updated>
    <author>
      <name>/u/No-Mulberry6961</name>
      <uri>https://old.reddit.com/user/No-Mulberry6961</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created an AI platform that allows a user to enter a single prompt with technical requirements and the LLM of choice thoroughly plans out and builds the entire thing nonstop until it is completely finished.&lt;/p&gt; &lt;p&gt;Here is a project it built last night using Claude 3.7, which took about 3 hours and has 214 files (can use any LLM, local, API, ollama etc…)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/justinlietz93/neuroca"&gt;https://github.com/justinlietz93/neuroca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m improving it every day as well and building an extension that locks into existing projects to finish them or add functionality&lt;/p&gt; &lt;p&gt;I have been asked to use my system to finish this project:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Recruitler/SortableTS"&gt;https://github.com/Recruitler/SortableTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If it is capable of doing that with a single prompt, then I can prove this legitimately is a novel and potentially breakthrough strategy for software development using AI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Mulberry6961"&gt; /u/No-Mulberry6961 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3nwiw/generate_an_entire_project_from_one_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3nwiw/generate_an_entire_project_from_one_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3nwiw/generate_an_entire_project_from_one_prompt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T22:44:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3fh7d</id>
    <title>Ollama-OCR</title>
    <updated>2025-03-04T16:58:47+00:00</updated>
    <author>
      <name>/u/imanoop7</name>
      <uri>https://old.reddit.com/user/imanoop7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I open-sourced &lt;strong&gt;Ollama-OCR&lt;/strong&gt; – an advanced &lt;strong&gt;OCR tool&lt;/strong&gt; powered by &lt;strong&gt;LLaVA 7B&lt;/strong&gt; and &lt;strong&gt;Llama 3.2 Vision&lt;/strong&gt; to extract text from images with high accuracy! 🚀&lt;/p&gt; &lt;p&gt;🔹 &lt;strong&gt;Features:&lt;/strong&gt;&lt;br /&gt; ✅ Supports &lt;strong&gt;Markdown, Plain Text, JSON, Structured, Key-Value Pairs&lt;/strong&gt;&lt;br /&gt; ✅ &lt;strong&gt;Batch processing&lt;/strong&gt; for handling multiple images efficiently&lt;br /&gt; ✅ Uses &lt;strong&gt;state-of-the-art vision-language models&lt;/strong&gt; for better OCR&lt;br /&gt; ✅ Ideal for &lt;strong&gt;document digitization, data extraction, and automation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Check it out &amp;amp; contribute! 🔗 &lt;a href="https://github.com/imanoop7/Ollama-OCR"&gt;GitHub: Ollama-OCR&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Details about Python Package -&lt;a href="https://medium.com/@mauryaanoop3/ollama-ocr-now-available-as-a-python-package-ff5e4240eb26"&gt; &lt;strong&gt;Guide&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thoughts? Feedback? Let’s discuss! 🔥&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/imanoop7"&gt; /u/imanoop7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3fh7d/ollamaocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j3fh7d/ollamaocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j3fh7d/ollamaocr/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-04T16:58:47+00:00</published>
  </entry>
</feed>
