<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-10T23:06:29+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ml66jw</id>
    <title>New Favorite Game: Convince GPT-OSS it Exists</title>
    <updated>2025-08-08T20:30:23+00:00</updated>
    <author>
      <name>/u/j_din</name>
      <uri>https://old.reddit.com/user/j_din</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spoiler alert: it can not FATHOM its own existence.&lt;/p&gt; &lt;p&gt;I have been trying for an hour to have the model even admit it's possible that OpenAI would release an open weight model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/bB0XE2Zv"&gt;https://pastebin.com/bB0XE2Zv&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please comment any successful attempts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/j_din"&gt; /u/j_din &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml66jw/new_favorite_game_convince_gptoss_it_exists/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml66jw/new_favorite_game_convince_gptoss_it_exists/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ml66jw/new_favorite_game_convince_gptoss_it_exists/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T20:30:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlt4zw</id>
    <title>XandAI-extension - Allow you to chat with your browser using ollama.</title>
    <updated>2025-08-09T16:07:50+00:00</updated>
    <author>
      <name>/u/Sea-Reception-2697</name>
      <uri>https://old.reddit.com/user/Sea-Reception-2697</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mlt4zw/xandaiextension_allow_you_to_chat_with_your/"&gt; &lt;img alt="XandAI-extension - Allow you to chat with your browser using ollama." src="https://external-preview.redd.it/bYRVS90ZKX1r4OqkPxy0a_PbC3IEsvLjY869Bkl6r1A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7564c949be66cb52cdfacd0f7c58a0c70de1044" title="XandAI-extension - Allow you to chat with your browser using ollama." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Reception-2697"&gt; /u/Sea-Reception-2697 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1mlszpz/xandaiextension_allow_you_to_chat_with_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlt4zw/xandaiextension_allow_you_to_chat_with_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlt4zw/xandaiextension_allow_you_to_chat_with_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T16:07:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml750r</id>
    <title>Local RAG with 97% smaller index and Claude Code‚Äìcompatible semantic search</title>
    <updated>2025-08-08T21:08:43+00:00</updated>
    <author>
      <name>/u/Lanky-District9096</name>
      <uri>https://old.reddit.com/user/Lanky-District9096</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôre building &lt;strong&gt;LEANN&lt;/strong&gt; at Berkeley Sky Lab ‚Äî a &lt;strong&gt;local vector index for RAG&lt;/strong&gt; that‚Äôs&lt;/p&gt; &lt;p&gt;üîí privacy-first&lt;/p&gt; &lt;p&gt;üì¶ 97% smaller&lt;/p&gt; &lt;p&gt;üß† fully compatible with &lt;strong&gt;Claude Code&lt;/strong&gt;, &lt;strong&gt;Ollama&lt;/strong&gt;, and &lt;strong&gt;GPT-OSS&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Run semantic search on your laptop ‚Äî fast, lightweight, and cloud-free.&lt;/p&gt; &lt;p&gt;üîó &lt;a href="https://github.com/yichuan-w/LEANN"&gt;https://github.com/yichuan-w/LEANN&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why does LEANN matter?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Because most vector DBs store &lt;em&gt;everything&lt;/em&gt; ‚Äî all embeddings, all graph structure ‚Äî which quickly balloons to 100+GB when you index emails, chat, and code.&lt;/p&gt; &lt;p&gt;But‚Ä¶ most queries only touch a tiny slice of the DB. So we asked:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why store every single embedding?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;LEANN introduces two ultra-lightweight backends:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;üîç Graph-only mode:&lt;/strong&gt; Stores &lt;em&gt;no embeddings&lt;/em&gt;, just a pruned HNSW graph.Recomputes embeddings on the fly using overlapping neighbors.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;üí° PQ+Rerank mode:&lt;/strong&gt; Compresses vectors with PQ, then &lt;em&gt;replaces&lt;/em&gt; heavy embedding storage with lightweight on-the-fly recomputation on the candidate set.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each offers a different tradeoff, but both aim for the same goal:&lt;/p&gt; &lt;p&gt;üß† &lt;strong&gt;Massive storage savings&lt;/strong&gt; with &lt;em&gt;no meaningful drop&lt;/em&gt; in recall.&lt;/p&gt; &lt;p&gt;üìù &lt;strong&gt;Note:&lt;/strong&gt; In modern RAG systems ‚Äî with long inputs and reasoning-heavy models ‚Äî &lt;strong&gt;generation&lt;/strong&gt;, not retrieval, is the bottleneck.&lt;/p&gt; &lt;p&gt;So even with slight retrieval latency increases, the end-to-end impact is &lt;strong&gt;~5% overhead&lt;/strong&gt; or less.&lt;/p&gt; &lt;p&gt;These give you blazing-fast semantic search over:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ üì® Apple Mail ‚Ä¢ üíæ Filesystem ‚Ä¢ üï∞Ô∏è Chrome / Chat history ‚Ä¢ üß† Codebase (Claude Code‚Äìcompatible) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;LEANN = your personal Jarvis, running locally.&lt;/p&gt; &lt;p&gt;üîó GitHub: &lt;a href="https://github.com/yichuan-w/LEANN"&gt;https://github.com/yichuan-w/LEANN&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üìÑ Paper: &lt;a href="https://arxiv.org/abs/2506.08276"&gt;https://arxiv.org/abs/2506.08276&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We‚Äôd love for you to try it out, give feedback, or ask questions on the repo!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lanky-District9096"&gt; /u/Lanky-District9096 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml750r/local_rag_with_97_smaller_index_and_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml750r/local_rag_with_97_smaller_index_and_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ml750r/local_rag_with_97_smaller_index_and_claude/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T21:08:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlzn7x</id>
    <title>Telling Ollama GUI to use GPU instead of CPU?</title>
    <updated>2025-08-09T20:38:46+00:00</updated>
    <author>
      <name>/u/Tinytitanic</name>
      <uri>https://old.reddit.com/user/Tinytitanic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a Ryzen 5700X and a RX 6800, both supported by Ollama. I used Ollama a few months ago and I noticed that it decided to use my GPU with high VRAM and GPU utilization but coming back to it to test the new GUI I noticed that it's instead using my CPU. Is there a way to tell it to use GPU instead? I feel like it was faster using the GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tinytitanic"&gt; /u/Tinytitanic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlzn7x/telling_ollama_gui_to_use_gpu_instead_of_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlzn7x/telling_ollama_gui_to_use_gpu_instead_of_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlzn7x/telling_ollama_gui_to_use_gpu_instead_of_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T20:38:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlgyct</id>
    <title>GPT-OSS 20b vs Qwen3-30B-A3B</title>
    <updated>2025-08-09T04:52:10+00:00</updated>
    <author>
      <name>/u/unofficialreddit0r</name>
      <uri>https://old.reddit.com/user/unofficialreddit0r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone used them in real life coding task? How do they compare?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialreddit0r"&gt; /u/unofficialreddit0r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlgyct/gptoss_20b_vs_qwen330ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlgyct/gptoss_20b_vs_qwen330ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlgyct/gptoss_20b_vs_qwen330ba3b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T04:52:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm1hs0</id>
    <title>user uploaded models</title>
    <updated>2025-08-09T21:58:41+00:00</updated>
    <author>
      <name>/u/Mediocre_Caramel_158</name>
      <uri>https://old.reddit.com/user/Mediocre_Caramel_158</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Where can I find user-uploaded models that are open and free to use by others on the Ollama server (ollama.com)? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mediocre_Caramel_158"&gt; /u/Mediocre_Caramel_158 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm1hs0/user_uploaded_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm1hs0/user_uploaded_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm1hs0/user_uploaded_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T21:58:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm0n7h</id>
    <title>Using a local LLM for proofreading on macOS?</title>
    <updated>2025-08-09T21:21:17+00:00</updated>
    <author>
      <name>/u/whooshingsounds</name>
      <uri>https://old.reddit.com/user/whooshingsounds</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to use a LLM to rephrase paragraphs of text, editing them for brevity, clarity, or to aim for a certain tone (formal, casual, businesslike‚Ä¶). I would also prefer not to upload these text to a server. Which local LLMs would be best suited for this? And would any Mac laptop be up to the task?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/whooshingsounds"&gt; /u/whooshingsounds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm0n7h/using_a_local_llm_for_proofreading_on_macos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm0n7h/using_a_local_llm_for_proofreading_on_macos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm0n7h/using_a_local_llm_for_proofreading_on_macos/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T21:21:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlxqtz</id>
    <title>Get a second GPU or go for a Mac Mini? (Qwen3-Coder30b)</title>
    <updated>2025-08-09T19:18:13+00:00</updated>
    <author>
      <name>/u/Manaberryio</name>
      <uri>https://old.reddit.com/user/Manaberryio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;I have a server machine I want to use for AI. I have an RX6800 (16GB VRAM) inside, along with a 13900KF and 128GB of ram. I've tried Qwen3-coder 30b 3ab but I cannot host it fully on my GPU with a bigger context than 16K. It's really slow and somehow unable to process debug request from Roo Code.&lt;/p&gt; &lt;p&gt;Will a second RX 6800 (around $300 used) would be helpful to do so? Or should I sell my stuff and get a Mac Mini M4 with at least 24GB of memory?&lt;/p&gt; &lt;p&gt;Thanks for helping&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Manaberryio"&gt; /u/Manaberryio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlxqtz/get_a_second_gpu_or_go_for_a_mac_mini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlxqtz/get_a_second_gpu_or_go_for_a_mac_mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlxqtz/get_a_second_gpu_or_go_for_a_mac_mini/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T19:18:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlwtkf</id>
    <title>Size of Gpt OSS</title>
    <updated>2025-08-09T18:40:05+00:00</updated>
    <author>
      <name>/u/Waakaari</name>
      <uri>https://old.reddit.com/user/Waakaari</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the size of gpt-oss-20b and gpt-oss-120b when I download from ollama? Is it the same from hugging face? &lt;/p&gt; &lt;p&gt;Is there any difference betwecn running gpt-oss models using ollama and using comfyui or setting it up any other way? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Waakaari"&gt; /u/Waakaari &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlwtkf/size_of_gpt_oss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlwtkf/size_of_gpt_oss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlwtkf/size_of_gpt_oss/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T18:40:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlz15z</id>
    <title>Are you good grok?</title>
    <updated>2025-08-09T20:12:41+00:00</updated>
    <author>
      <name>/u/Gandalfusmaximale</name>
      <uri>https://old.reddit.com/user/Gandalfusmaximale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mlz15z/are_you_good_grok/"&gt; &lt;img alt="Are you good grok?" src="https://preview.redd.it/lt6953j7x1if1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc8b58ce17e3bc02fb5fb414b4cf50dc3eb00ba4" title="Are you good grok?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else had this before ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gandalfusmaximale"&gt; /u/Gandalfusmaximale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lt6953j7x1if1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlz15z/are_you_good_grok/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlz15z/are_you_good_grok/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T20:12:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm7mp8</id>
    <title>Has anyone successfully run Ollama on a Jetson Orin Nano?</title>
    <updated>2025-08-10T02:56:49+00:00</updated>
    <author>
      <name>/u/matsyui_</name>
      <uri>https://old.reddit.com/user/matsyui_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm trying to turn my Jetson Orin Nano Super Developer Kit into a self-hosted AI API server running Ollama.&lt;/p&gt; &lt;p&gt;Has anyone here tried this, patched it, or gotten decent performance running Ollama on a Jetson device?&lt;/p&gt; &lt;p&gt;I‚Äôd love to hear about your setup, steps, or any pitfalls you ran into.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matsyui_"&gt; /u/matsyui_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm7mp8/has_anyone_successfully_run_ollama_on_a_jetson/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm7mp8/has_anyone_successfully_run_ollama_on_a_jetson/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm7mp8/has_anyone_successfully_run_ollama_on_a_jetson/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T02:56:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm9uds</id>
    <title>Need help with benchmarking for RAG + LLM running locally.</title>
    <updated>2025-08-10T04:56:32+00:00</updated>
    <author>
      <name>/u/irodov4030</name>
      <uri>https://old.reddit.com/user/irodov4030</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to benchmark RAG setup for multiple file formats like - doc, xls, csv, ppt, png etc.&lt;/p&gt; &lt;p&gt;Are there any benchmarks with which I can test accuracy/quality of answers across multiple file formats?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/irodov4030"&gt; /u/irodov4030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm9uds/need_help_with_benchmarking_for_rag_llm_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm9uds/need_help_with_benchmarking_for_rag_llm_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm9uds/need_help_with_benchmarking_for_rag_llm_running/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T04:56:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmf6xl</id>
    <title>Reasoning LLMs Explorer</title>
    <updated>2025-08-10T10:32:50+00:00</updated>
    <author>
      <name>/u/Boring_Rabbit2275</name>
      <uri>https://old.reddit.com/user/Boring_Rabbit2275</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a web page where a lot of information is compiled about Reasoning in LLMs (A tree of surveys, an atlas of definitions and a map of techniques in reasoning)&lt;/p&gt; &lt;p&gt;&lt;a href="https://azzedde.github.io/reasoning-explorer/"&gt;https://azzedde.github.io/reasoning-explorer/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Your insights ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Boring_Rabbit2275"&gt; /u/Boring_Rabbit2275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmf6xl/reasoning_llms_explorer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmf6xl/reasoning_llms_explorer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmf6xl/reasoning_llms_explorer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T10:32:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlxx5h</id>
    <title>LLMs are Stochastic Parrots - Interactive Visualization</title>
    <updated>2025-08-09T19:25:34+00:00</updated>
    <author>
      <name>/u/kushalgoenka</name>
      <uri>https://old.reddit.com/user/kushalgoenka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mlxx5h/llms_are_stochastic_parrots_interactive/"&gt; &lt;img alt="LLMs are Stochastic Parrots - Interactive Visualization" src="https://external-preview.redd.it/NvAI6Yum9O40l3qZlOeyOssVIs2oLgJwnoMTWT8Xzzg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64d63adc6a8fdef03b738bcffef859cc8986b0e8" title="LLMs are Stochastic Parrots - Interactive Visualization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kushalgoenka"&gt; /u/kushalgoenka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/6dn1kUwTFcc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlxx5h/llms_are_stochastic_parrots_interactive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlxx5h/llms_are_stochastic_parrots_interactive/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T19:25:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmmwcg</id>
    <title>Rookie question. Avoiding FOMO‚Ä¶</title>
    <updated>2025-08-10T16:25:44+00:00</updated>
    <author>
      <name>/u/Famous-Recognition62</name>
      <uri>https://old.reddit.com/user/Famous-Recognition62</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Famous-Recognition62"&gt; /u/Famous-Recognition62 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1mmmtlf/rookie_question_avoiding_fomo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmmwcg/rookie_question_avoiding_fomo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmmwcg/rookie_question_avoiding_fomo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T16:25:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmfhfa</id>
    <title>Fastest and best model for my really low spec hardware</title>
    <updated>2025-08-10T10:50:01+00:00</updated>
    <author>
      <name>/u/PurpleUser0000</name>
      <uri>https://old.reddit.com/user/PurpleUser0000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an I5-4th gen, ddr3 8GB ram 1600hz , no GPU ( IGPU ) &lt;/p&gt; &lt;p&gt;What's the best model I can go with here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PurpleUser0000"&gt; /u/PurpleUser0000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmfhfa/fastest_and_best_model_for_my_really_low_spec/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmfhfa/fastest_and_best_model_for_my_really_low_spec/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmfhfa/fastest_and_best_model_for_my_really_low_spec/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T10:50:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmhxiv</id>
    <title>Qwen 4B Instruct works beautifully ‚ù§Ô∏è</title>
    <updated>2025-08-10T12:58:52+00:00</updated>
    <author>
      <name>/u/krishnajeya</name>
      <uri>https://old.reddit.com/user/krishnajeya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mmhxiv/qwen_4b_instruct_works_beautifully/"&gt; &lt;img alt="Qwen 4B Instruct works beautifully ‚ù§Ô∏è" src="https://preview.redd.it/izyjc6vps6if1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c27f3d6f828cbb7921438248a37810117753209" title="Qwen 4B Instruct works beautifully ‚ù§Ô∏è" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/krishnajeya"&gt; /u/krishnajeya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/izyjc6vps6if1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmhxiv/qwen_4b_instruct_works_beautifully/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmhxiv/qwen_4b_instruct_works_beautifully/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T12:58:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm9zhv</id>
    <title>What should I do with my home ollama lab?</title>
    <updated>2025-08-10T05:04:27+00:00</updated>
    <author>
      <name>/u/onemorequickchange</name>
      <uri>https://old.reddit.com/user/onemorequickchange</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Setup an x99 with dual xeon 2699 v4s and dual 3090s, 256GB RAM, linux distro. Curious what people use this setup for? I just subscribed to Claude @ $100/month and have had chatGPT for teams $30/pers/month. Do people just use local to save money or are there legit use cases? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onemorequickchange"&gt; /u/onemorequickchange &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm9zhv/what_should_i_do_with_my_home_ollama_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm9zhv/what_should_i_do_with_my_home_ollama_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm9zhv/what_should_i_do_with_my_home_ollama_lab/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T05:04:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmtawi</id>
    <title>Noob here. Please Help me find the perfect model.</title>
    <updated>2025-08-10T20:32:41+00:00</updated>
    <author>
      <name>/u/Many-Kaleidoscope-72</name>
      <uri>https://old.reddit.com/user/Many-Kaleidoscope-72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone. I‚Äôm new to LLMs and ollama. But I‚Äôm interested and hyped. I am a programmer (IT technician) and I started dealing with local LLMs and vision based ais like stable diffusion. I invested into a Rog Zephyrus G16 2025 model with 64GB RAM, 5070ti (12GB), Core Ultra 9 285H model. As far as I know, this hardware lets me do light or medium AI work locally. I plan to learn AI development later on University. I will just start my first semester. That‚Äôs why I choose these specs. I have a 4070ti, 32GB RAM, i7 13700K desktop pc too. Obviously that‚Äôs faster but lacks immense VRAM for large models. I could remote onto the desktop from a weaker laptop, but I‚Äôd rather be able to do light / medium work locally, than to always remote onto the home PC.&lt;/p&gt; &lt;p&gt;Question is, what‚Äôs the absolute best AI model for text + vision, that can speak English and Hungarian (my native language) well. I know that there are runtime parameters for both ollama and the models, so maybe a bigger model is runable with the right tweaks? Currently gemma3 12B runs fast, knows Hungarian well. 27B runs very poorly. Even when changing model settings (I might set it up incorrectly).&lt;/p&gt; &lt;p&gt;So what are your recommendations, guys? Let me know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Many-Kaleidoscope-72"&gt; /u/Many-Kaleidoscope-72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmtawi/noob_here_please_help_me_find_the_perfect_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmtawi/noob_here_please_help_me_find_the_perfect_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmtawi/noob_here_please_help_me_find_the_perfect_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T20:32:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmnptg</id>
    <title>How to run multiple versions of same model?</title>
    <updated>2025-08-10T16:57:44+00:00</updated>
    <author>
      <name>/u/petr_bena</name>
      <uri>https://old.reddit.com/user/petr_bena</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Right now ollama always works only with latest version of a model, say Mistral:7b&lt;/p&gt; &lt;p&gt;These models get periodic updates. What if I wanted to retain version from 2024 and 2025 and be able to switch between them? Does ollama supports something like version tagging and maintaining multiple versions of same model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/petr_bena"&gt; /u/petr_bena &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmnptg/how_to_run_multiple_versions_of_same_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmnptg/how_to_run_multiple_versions_of_same_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmnptg/how_to_run_multiple_versions_of_same_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T16:57:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmr2n1</id>
    <title>is we can run gpt oss in 16gb vram ? why mine is offload to cpu im running in docker in ubuntu 22.04</title>
    <updated>2025-08-10T19:06:17+00:00</updated>
    <author>
      <name>/u/actuallytech</name>
      <uri>https://old.reddit.com/user/actuallytech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mmr2n1/is_we_can_run_gpt_oss_in_16gb_vram_why_mine_is/"&gt; &lt;img alt="is we can run gpt oss in 16gb vram ? why mine is offload to cpu im running in docker in ubuntu 22.04" src="https://a.thumbs.redditmedia.com/T8e2323zafFhBWk7siLTxyY4iplSS1jgitzzPLUcEX8.jpg" title="is we can run gpt oss in 16gb vram ? why mine is offload to cpu im running in docker in ubuntu 22.04" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;gpu used is rtx 5060ti 16gb vram&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/actuallytech"&gt; /u/actuallytech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mmr2n1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmr2n1/is_we_can_run_gpt_oss_in_16gb_vram_why_mine_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmr2n1/is_we_can_run_gpt_oss_in_16gb_vram_why_mine_is/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T19:06:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmrqgh</id>
    <title>How do I get vision models working in Ollama/LM Studio?</title>
    <updated>2025-08-10T19:31:37+00:00</updated>
    <author>
      <name>/u/avdsrj</name>
      <uri>https://old.reddit.com/user/avdsrj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I've been messing around with Ollama and LM Studio to run LLMs locally, and I'm hitting a wall with vision models.&lt;/p&gt; &lt;p&gt;So here's the deal - I know vision models need these &amp;quot;mmproj&amp;quot; files to actually see pictures, and everything works fine when I grab models straight from Ollama or LM Studio's repos. But the moment I try to use GGUF models from somewhere else (like Hugging Face), I'm completely lost on how to get the mmproj stuff working.&lt;/p&gt; &lt;p&gt;I've been googling this for way too long and honestly can't find a clear answer anywhere. It feels like there's some obvious step I'm missing.&lt;/p&gt; &lt;p&gt;Has anyone figured out how to manually add mmproj files to models? Like, is there a specific way to structure the Modelfile or some command I'm not seeing?&lt;/p&gt; &lt;p&gt;Would really appreciate if someone could point me in the right direction - this is driving me crazy!&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/avdsrj"&gt; /u/avdsrj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmrqgh/how_do_i_get_vision_models_working_in_ollamalm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmrqgh/how_do_i_get_vision_models_working_in_ollamalm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmrqgh/how_do_i_get_vision_models_working_in_ollamalm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T19:31:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmu24w</id>
    <title>What is the Best coding LLM for my system?</title>
    <updated>2025-08-10T21:02:54+00:00</updated>
    <author>
      <name>/u/tarsonis125</name>
      <uri>https://old.reddit.com/user/tarsonis125</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;rtx 3090 24vram + 96gig ram&lt;/p&gt; &lt;p&gt;What is the best local LLM to use on my system?&lt;br /&gt; Do some models do better then other at some tasks?&lt;br /&gt; I am trying out a bunch of them, but its hard for me to properly rate them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarsonis125"&gt; /u/tarsonis125 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmu24w/what_is_the_best_coding_llm_for_my_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmu24w/what_is_the_best_coding_llm_for_my_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmu24w/what_is_the_best_coding_llm_for_my_system/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T21:02:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmj279</id>
    <title>The first time I heard a sound coming from my MacBook was while using GPT-OSS 20B</title>
    <updated>2025-08-10T13:49:09+00:00</updated>
    <author>
      <name>/u/anakedsuperman</name>
      <uri>https://old.reddit.com/user/anakedsuperman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mmj279/the_first_time_i_heard_a_sound_coming_from_my/"&gt; &lt;img alt="The first time I heard a sound coming from my MacBook was while using GPT-OSS 20B" src="https://external-preview.redd.it/ZThhMDNvMmE1N2lmMTSXhaJcSNaMLiLEA491TTFq3lE-Ha0mGK07Lje4LN3h.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7372d0bd82781bd6388dff2cabe3eca39528ceca" title="The first time I heard a sound coming from my MacBook was while using GPT-OSS 20B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running GPT-OSS 20B on my MacBook M4 Max with 36GB RAM. I don't hear anything from other models, though, even with Devtra 140B.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anakedsuperman"&gt; /u/anakedsuperman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vyk9un2a57if1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmj279/the_first_time_i_heard_a_sound_coming_from_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmj279/the_first_time_i_heard_a_sound_coming_from_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T13:49:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm4ibk</id>
    <title>I ran OpenAI‚Äôs GPT-OSS 20B locally on a 16GB Mac with Ollama ‚Äî setup, gotchas, and mini demo</title>
    <updated>2025-08-10T00:19:29+00:00</updated>
    <author>
      <name>/u/Spirited-Wind6803</name>
      <uri>https://old.reddit.com/user/Spirited-Wind6803</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mm4ibk/i_ran_openais_gptoss_20b_locally_on_a_16gb_mac/"&gt; &lt;img alt="I ran OpenAI‚Äôs GPT-OSS 20B locally on a 16GB Mac with Ollama ‚Äî setup, gotchas, and mini demo" src="https://external-preview.redd.it/MHQ5M3V2aXQ0M2lmMf_ZwYHO2m1fMNCQy9M-9mV9J_Z510ikdbK6GDGwXk75.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=678ce1f3ab9b2ba28aeffdad56e26d3d77e4258f" title="I ran OpenAI‚Äôs GPT-OSS 20B locally on a 16GB Mac with Ollama ‚Äî setup, gotchas, and mini demo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all ‚Äî I‚Äôve been testing &lt;strong&gt;GPT-OSS 20B&lt;/strong&gt; locally using &lt;strong&gt;Ollama&lt;/strong&gt; and wanted to share a clean setup path, what worked, what didn‚Äôt, and a tiny QA demo. &lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Yes, 20B runs on a 16GB Mac&lt;/strong&gt; with Ollama. Do I have the patience? No, it took toooo long&lt;/li&gt; &lt;li&gt;Should you use 16GB to perform any other tasks such as coding, agent, RAG? No, not worth it - upgrade to 32GB maybe..maybe will give you more. I tried on A100 GPU and still did not meet my expectation &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spirited-Wind6803"&gt; /u/Spirited-Wind6803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qlifjvit43if1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm4ibk/i_ran_openais_gptoss_20b_locally_on_a_16gb_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm4ibk/i_ran_openais_gptoss_20b_locally_on_a_16gb_mac/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T00:19:29+00:00</published>
  </entry>
</feed>
