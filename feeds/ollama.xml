<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-07-16T09:11:36+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ly7bnc</id>
    <title>whats the best model for my use case?</title>
    <updated>2025-07-12T18:22:22+00:00</updated>
    <author>
      <name>/u/Witty_Mycologist_995</name>
      <uri>https://old.reddit.com/user/Witty_Mycologist_995</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;whats the fastest local ollama model, that has tool support.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Witty_Mycologist_995"&gt; /u/Witty_Mycologist_995 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ly7bnc/whats_the_best_model_for_my_use_case/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ly7bnc/whats_the_best_model_for_my_use_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ly7bnc/whats_the_best_model_for_my_use_case/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-12T18:22:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxzg14</id>
    <title>Github copilot with Ollama - need to sign in?</title>
    <updated>2025-07-12T12:38:01+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, now that Github copilot for Visual Studio Code supports Ollama, i consider using it instead of Continue. However, it seems like you can only get to the model switcher dialogue when you are signed into github?&lt;/p&gt; &lt;p&gt;Of course, i don't want to sign in to anything, that's why i want to use my local ollama instance in the 1st place!&lt;/p&gt; &lt;p&gt;Has anyone found a workaround to use Ollama with copilot without having to sign in?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxzg14/github_copilot_with_ollama_need_to_sign_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxzg14/github_copilot_with_ollama_need_to_sign_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lxzg14/github_copilot_with_ollama_need_to_sign_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-12T12:38:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyhl3h</id>
    <title>Socio especialista en N8N - Buscamos Socio</title>
    <updated>2025-07-13T02:19:08+00:00</updated>
    <author>
      <name>/u/BBCC37</name>
      <uri>https://old.reddit.com/user/BBCC37</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Somos un Grupo de 2 estudiantes de negocios (Socio 1 : Economía y Negocios Internacionales , Socio 2 Estudiante de Administración y Marketing)&lt;/p&gt; &lt;p&gt;Tenemos experiencia en impulsar negocios ya que tenemos un negocio de venta de automóviles en Perú, pero queremos incursionar en La creación de automatizaciones para empresas y hacer escalable el negocio, ya que es un nicho en crecimiento y creemos que es posible que con nuestra experiencia podamos hacer crecer la Startup que queremos crear para utilizar agentes de IA. &lt;/p&gt; &lt;p&gt;Buscamos un socio especialista en N8N en de su entorno para poder hacer escalable el negocio desde lo técnico ya que nosotros no encargaremos del desarrollo empresarial de la Startup con la búsqueda de financiamiento, planificación financiera y búsqueda de clientes a través de Marketing. &lt;/p&gt; &lt;p&gt;Lima - Perú &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BBCC37"&gt; /u/BBCC37 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyhl3h/socio_especialista_en_n8n_buscamos_socio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyhl3h/socio_especialista_en_n8n_buscamos_socio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyhl3h/socio_especialista_en_n8n_buscamos_socio/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T02:19:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxd4j0</id>
    <title>Two guys on a bus</title>
    <updated>2025-07-11T17:35:12+00:00</updated>
    <author>
      <name>/u/TodoLoQueCompartimos</name>
      <uri>https://old.reddit.com/user/TodoLoQueCompartimos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lxd4j0/two_guys_on_a_bus/"&gt; &lt;img alt="Two guys on a bus" src="https://preview.redd.it/kiuj2t0o6acf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=258cace7d27d200a281a0ce5f08e1ed7c163739d" title="Two guys on a bus" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TodoLoQueCompartimos"&gt; /u/TodoLoQueCompartimos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kiuj2t0o6acf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxd4j0/two_guys_on_a_bus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lxd4j0/two_guys_on_a_bus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-11T17:35:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxvdhu</id>
    <title>Henceforth …</title>
    <updated>2025-07-12T08:22:04+00:00</updated>
    <author>
      <name>/u/But-I-Am-a-Robot</name>
      <uri>https://old.reddit.com/user/But-I-Am-a-Robot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Overly joyous posters in this group shall be referred to as Ollama Lama Ding Dongs. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/But-I-Am-a-Robot"&gt; /u/But-I-Am-a-Robot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxvdhu/henceforth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxvdhu/henceforth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lxvdhu/henceforth/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-12T08:22:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyh7p2</id>
    <title>newbie on Ollama some issues with searxng</title>
    <updated>2025-07-13T01:59:15+00:00</updated>
    <author>
      <name>/u/ElTamales</name>
      <uri>https://old.reddit.com/user/ElTamales</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks!&lt;/p&gt; &lt;p&gt;I have a 4090 and I wanted to give a try to set some models to summarize news from time to time. &lt;/p&gt; &lt;p&gt;So I decided the safest way was to download the dockerized version of ollama + openwebui.&lt;/p&gt; &lt;p&gt;All was good on the first installation.&lt;/p&gt; &lt;p&gt;Problem? I was silly and forgot that all the models were downloaded into my main drive, which was a kinda small 1TB NVME which was already 90% full.&lt;/p&gt; &lt;p&gt;During this moment, the models were working fine.&lt;/p&gt; &lt;p&gt;So I decided to switch the storage to a much bigger place. Which started to give me some issues.&lt;/p&gt; &lt;p&gt;Since I did not want to make things complicated. I simply removed the images instead of packing them to Tar and then move them to the new disk.&lt;/p&gt; &lt;p&gt;So after making the changes. I redownloaded everything. Then I started to have problems.&lt;/p&gt; &lt;p&gt;The models (phi4) and others, seem to work fine using searxng hosted in a docker on my NAS.&lt;/p&gt; &lt;p&gt;Until I try to search sports content. (Ie soccer).&lt;/p&gt; &lt;p&gt;Upon doing this search, I suddenly will get a &lt;em&gt;&amp;quot;I'm sorry, but I don't have access to real-time data or events beyond my training cut-off in October 2023.&amp;quot;&lt;/em&gt; response over and over in different sports and stuff.&lt;/p&gt; &lt;p&gt;over the subsequent queries, it will repeat this similarly and starting to output incorrect data.&lt;/p&gt; &lt;p&gt;Yet it seems to have searched and found many correct websites where the content is.. and then inviting you to check the links instead of summarizing the data.&lt;/p&gt; &lt;p&gt;Am I doing something wrong?&lt;/p&gt; &lt;p&gt;The Specs:&lt;/p&gt; &lt;p&gt;Searxng : UNRAID Docker container in a NAS.&lt;/p&gt; &lt;p&gt;Running computer: 14900k 4090, 64GB of RAM 3HDDS, 3 NVMEs, 1 SSD.&lt;/p&gt; &lt;p&gt;software: Nobara42 (Fedora 42 core), Podman 1x ollama 1x openwui.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElTamales"&gt; /u/ElTamales &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyh7p2/newbie_on_ollama_some_issues_with_searxng/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyh7p2/newbie_on_ollama_some_issues_with_searxng/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyh7p2/newbie_on_ollama_some_issues_with_searxng/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T01:59:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyp2kk</id>
    <title>Ollama can't start - exit status 2</title>
    <updated>2025-07-13T09:55:24+00:00</updated>
    <author>
      <name>/u/Antoni_Nabzdyk</name>
      <uri>https://old.reddit.com/user/Antoni_Nabzdyk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys,&lt;/p&gt; &lt;p&gt;I'm a prrammer, and have used Ollama for some time now. Now, out of nowhere, my Ollama local installation on my VPS stopped working altogheter. Each respoinse was rejected with the 500 error. I didn't know what to do. I use Google's AIStudio for the fix, but fater 3 hours, I have enough. The AIis telling me that I might have hardware-compatibility issues, and that my hardware can't run those models. That's impossible! I used it for a few months. I did clean installs, but then my AI said that the real clue was buried deep in the journalctl -u ollama.service logs:&lt;/p&gt; &lt;p&gt;SIGILL: illegal instruction&lt;/p&gt; &lt;p&gt;This is my journal as of right now:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Jul 13 09:36:53 srv670432 ollama[490754]: time=2025-07-13T09:36:53.992Z level=ERROR source=sched.go:489 msg=&amp;quot;error loading llama server&amp;quot; error=&amp;quot;llama runner process has terminated: exit status 2&amp;quot; Jul 13 09:36:53 srv670432 ollama[490754]: [GIN] 2025/07/13 - 09:36:53 | 500 | 339.406703ms | 127.0.0.1 | POST &amp;quot;/api/generate&amp;quot; Jul 13 09:40:08 srv670432 ollama[490754]: [GIN] 2025/07/13 - 09:40:08 | 200 | 38.231µs | 127.0.0.1 | HEAD &amp;quot;/&amp;quot; Jul 13 09:40:08 srv670432 ollama[490754]: [GIN] 2025/07/13 - 09:40:08 | 200 | 22.95465ms | 127.0.0.1 | POST &amp;quot;/api/show&amp;quot; Jul 13 09:40:08 srv670432 ollama[490754]: time=2025-07-13T09:40:08.678Z level=INFO source=server.go:135 msg=&amp;quot;system memory&amp;quot; total=&amp;quot;7.8 GiB&amp;quot; free=&amp;quot;6.9 GiB&amp;quot; free_swap=&amp;quot;4.4 GiB&amp;quot; Jul 13 09:40:08 srv670432 ollama[490754]: time=2025-07-13T09:40:08.678Z level=WARN source=server.go:145 msg=&amp;quot;requested context size too large for model&amp;quot; num_ctx=8192 num_parallel=2 n_ctx_train=2048 Jul 13 09:40:08 srv670432 ollama[490754]: time=2025-07-13T09:40:08.678Z level=INFO source=server.go:175 msg=offload library=cpu layers.requested=-1 layers.model=23 layers.offload=0 layers.split=&amp;quot;&amp;quot; memory.available=&amp;quot;[6.9 GiB]&amp;quot; memory.gpu_overhead=&amp;quot;0 B&amp;quot; memory.required.full=&amp;quot;967.0 MiB&amp;quot; memory.required.partial=&amp;quot;0 B&amp;quot; memory.required.kv=&amp;quot;88.0 MiB&amp;quot; memory.required.allocations=&amp;quot;[967.0 MiB]&amp;quot; memory.weights.total=&amp;quot;571.4 MiB&amp;quot; memory.weights.repeating=&amp;quot;520.1 MiB&amp;quot; memory.weights.nonrepeating=&amp;quot;51.3 MiB&amp;quot; memory.graph.full=&amp;quot;280.0 MiB&amp;quot; memory.graph.partial=&amp;quot;278.3 MiB&amp;quot; Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-2af3b81862c6be03c769683af18efdadb2c33f60ff32ab6f83e42c043d6c7816 (version GGUF V3 (latest)) Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output. Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 0: general.architecture str = llama Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 1: general.name str = TinyLlama Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 2: llama.context_length u32 = 2048 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 3: llama.embedding_length u32 = 2048 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 4: llama.block_count u32 = 22 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 5: llama.feed_forward_length u32 = 5632 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 6: llama.rope.dimension_count u32 = 64 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 7: llama.attention.head_count u32 = 32 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 8: llama.attention.head_count_kv u32 = 4 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 9: llama.attention.layer_norm_rms_epsilon f32 = 0.000010 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 10: llama.rope.freq_base f32 = 10000.000000 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 11: general.file_type u32 = 2 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 12: tokenizer.ggml.model str = llama Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 13: tokenizer.ggml.tokens arr[str,32000] = [&amp;quot;&amp;lt;unk&amp;gt;&amp;quot;, &amp;quot;&amp;lt;s&amp;gt;&amp;quot;, &amp;quot;&amp;lt;/s&amp;gt;&amp;quot;, &amp;quot;&amp;lt;0x00&amp;gt;&amp;quot;, &amp;quot;&amp;lt;... Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 14: tokenizer.ggml.scores arr[f32,32000] = [0.000000, 0.000000, 0.000000, 0.0000... Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 15: tokenizer.ggml.token_type arr[i32,32000] = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ... Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 16: tokenizer.ggml.merges arr[str,61249] = [&amp;quot;▁ t&amp;quot;, &amp;quot;e r&amp;quot;, &amp;quot;i n&amp;quot;, &amp;quot;▁ a&amp;quot;, &amp;quot;e n... Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 17: tokenizer.ggml.bos_token_id u32 = 1 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 18: tokenizer.ggml.eos_token_id u32 = 2 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 19: tokenizer.ggml.unknown_token_id u32 = 0 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 20: tokenizer.ggml.padding_token_id u32 = 2 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 21: tokenizer.chat_template str = {% for message in messages %}\n{% if m... Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 22: general.quantization_version u32 = 2 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - type f32: 45 tensors Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - type q4_0: 155 tensors Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - type q6_K: 1 tensors Jul 13 09:40:08 srv670432 ollama[490754]: print_info: file format = GGUF V3 (latest) Jul 13 09:40:08 srv670432 ollama[490754]: print_info: file type = Q4_0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: file size = 606.53 MiB (4.63 BPW) Jul 13 09:40:08 srv670432 ollama[490754]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect Jul 13 09:40:08 srv670432 ollama[490754]: load: special tokens cache size = 3 Jul 13 09:40:08 srv670432 ollama[490754]: load: token to piece cache size = 0.1684 MB Jul 13 09:40:08 srv670432 ollama[490754]: print_info: arch = llama Jul 13 09:40:08 srv670432 ollama[490754]: print_info: vocab_only = 1 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: model type = ?B Jul 13 09:40:08 srv670432 ollama[490754]: print_info: model params = 1.10 B Jul 13 09:40:08 srv670432 ollama[490754]: print_info: general.name = TinyLlama Jul 13 09:40:08 srv670432 ollama[490754]: print_info: vocab type = SPM Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_vocab = 32000 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_merges = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: BOS token = 1 '&amp;lt;s&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: EOS token = 2 '&amp;lt;/s&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: UNK token = 0 '&amp;lt;unk&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: PAD token = 2 '&amp;lt;/s&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: LF token = 13 '&amp;lt;0x0A&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: EOG token = 2 '&amp;lt;/s&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: max token length = 48 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_load: vocab only - skipping tensors Jul 13 09:40:08 srv670432 ollama[490754]: time=2025-07-13T09:40:08.733Z level=INFO source=server.go:438 msg=&amp;quot;starting llama server&amp;quot; cmd=&amp;quot;/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-2af3b81862c6be03c769683af18efdadb2c33f60ff32ab6f83e42c043d6c7816 --ctx-size 4096 --batch-size 512 --threads 2 --no-mmap --parallel 2 --port 33555&amp;quot; Jul 13 09:40:08 srv670432 ollama[490754]: time=2025-07-13T09:40:08.734Z level=INFO source=sched.go:483 msg=&amp;quot;loaded runners&amp;quot; count=1 Jul 13 09:40:08 srv670432 ollama[490754]: time=2025-07-13T09:40:08.734Z level=INFO source=server.go:598 msg=&amp;quot;waiting for llama runner to start responding&amp;quot; Jul 13 09:40:08 srv670432 ollama[490754]: time=2025-07-13T09:40:08.735Z level=INFO source=server.go:632 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server not responding&amp;quot; Jul 13 09:40:08 srv670432 ollama[490754]: time=2025-07-13T09:40:08.758Z level=INFO source=runner.go:815 msg=&amp;quot;starting go runner&amp;quot; Jul 13 09:40:08 srv670432 ollama[490754]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so Jul 13 09:40:08 srv670432 ollama[490754]: time=2025-07-13T09:40:08.766Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc) Jul 13 09:40:08 srv670432 ollama[490754]: time=2025-07-13T09:40:08.766Z level=INFO source=runner.go:874 msg=&amp;quot;Server listening on 127.0.0.1:33555&amp;quot; Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-2af3b81862c6be03c769683af18efdadb2c33f60ff32ab6f83e42c043d6c7816 (version GGUF V3 (latest)) Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output. Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 0: general.architecture str = llama Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 1: general.name str = TinyLlama Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 2: llama.context_length u32 = 2048 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 3: llama.embedding_length u32 = 2048 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 4: llama.block_count u32 = 22 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 5: llama.feed_forward_length u32 = 5632 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 6: llama.rope.dimension_count u32 = 64 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 7: llama.attention.head_count u32 = 32 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 8: llama.attention.head_count_kv u32 = 4 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 9: llama.attention.layer_norm_rms_epsilon f32 = 0.000010 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 10: llama.rope.freq_base f32 = 10000.000000 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 11: general.file_type u32 = 2 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 12: tokenizer.ggml.model str = llama Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 13: tokenizer.ggml.tokens arr[str,32000] = [&amp;quot;&amp;lt;unk&amp;gt;&amp;quot;, &amp;quot;&amp;lt;s&amp;gt;&amp;quot;, &amp;quot;&amp;lt;/s&amp;gt;&amp;quot;, &amp;quot;&amp;lt;0x00&amp;gt;&amp;quot;, &amp;quot;&amp;lt;... Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 14: tokenizer.ggml.scores arr[f32,32000] = [0.000000, 0.000000, 0.000000, 0.0000... Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 15: tokenizer.ggml.token_type arr[i32,32000] = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ... Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 16: tokenizer.ggml.merges arr[str,61249] = [&amp;quot;▁ t&amp;quot;, &amp;quot;e r&amp;quot;, &amp;quot;i n&amp;quot;, &amp;quot;▁ a&amp;quot;, &amp;quot;e n... Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 17: tokenizer.ggml.bos_token_id u32 = 1 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 18: tokenizer.ggml.eos_token_id u32 = 2 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 19: tokenizer.ggml.unknown_token_id u32 = 0 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 20: tokenizer.ggml.padding_token_id u32 = 2 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 21: tokenizer.chat_template str = {% for message in messages %}\n{% if m... Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - kv 22: general.quantization_version u32 = 2 Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - type f32: 45 tensors Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - type q4_0: 155 tensors Jul 13 09:40:08 srv670432 ollama[490754]: llama_model_loader: - type q6_K: 1 tensors Jul 13 09:40:08 srv670432 ollama[490754]: print_info: file format = GGUF V3 (latest) Jul 13 09:40:08 srv670432 ollama[490754]: print_info: file type = Q4_0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: file size = 606.53 MiB (4.63 BPW) Jul 13 09:40:08 srv670432 ollama[490754]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect Jul 13 09:40:08 srv670432 ollama[490754]: load: special tokens cache size = 3 Jul 13 09:40:08 srv670432 ollama[490754]: load: token to piece cache size = 0.1684 MB Jul 13 09:40:08 srv670432 ollama[490754]: print_info: arch = llama Jul 13 09:40:08 srv670432 ollama[490754]: print_info: vocab_only = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_ctx_train = 2048 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_embd = 2048 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_layer = 22 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_head = 32 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_head_kv = 4 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_rot = 64 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_swa = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_swa_pattern = 1 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_embd_head_k = 64 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_embd_head_v = 64 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_gqa = 8 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_embd_k_gqa = 256 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_embd_v_gqa = 256 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: f_norm_eps = 0.0e+00 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: f_norm_rms_eps = 1.0e-05 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: f_clamp_kqv = 0.0e+00 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: f_max_alibi_bias = 0.0e+00 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: f_logit_scale = 0.0e+00 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: f_attn_scale = 0.0e+00 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_ff = 5632 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_expert = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_expert_used = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: causal attn = 1 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: pooling type = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: rope type = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: rope scaling = linear Jul 13 09:40:08 srv670432 ollama[490754]: print_info: freq_base_train = 10000.0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: freq_scale_train = 1 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_ctx_orig_yarn = 2048 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: rope_finetuned = unknown Jul 13 09:40:08 srv670432 ollama[490754]: print_info: ssm_d_conv = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: ssm_d_inner = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: ssm_d_state = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: ssm_dt_rank = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: ssm_dt_b_c_rms = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: model type = 1B Jul 13 09:40:08 srv670432 ollama[490754]: print_info: model params = 1.10 B Jul 13 09:40:08 srv670432 ollama[490754]: print_info: general.name = TinyLlama Jul 13 09:40:08 srv670432 ollama[490754]: print_info: vocab type = SPM Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_vocab = 32000 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: n_merges = 0 Jul 13 09:40:08 srv670432 ollama[490754]: print_info: BOS token = 1 '&amp;lt;s&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: EOS token = 2 '&amp;lt;/s&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: UNK token = 0 '&amp;lt;unk&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: PAD token = 2 '&amp;lt;/s&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: LF token = 13 '&amp;lt;0x0A&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: EOG token = 2 '&amp;lt;/s&amp;gt;' Jul 13 09:40:08 srv670432 ollama[490754]: print_info: max token length = 48 Jul 13 09:40:08 srv670432 ollama[490754]: load_tensors: loading model tensors, this can take a while... (mmap = false) Jul 13 09:40:08 srv670432 ollama[490754]: SIGILL: illegal instruction Jul 13 09:40:08 srv670432 ollama[490754]: PC=0x7f7803f1c5aa m=0 sigcode=2 Jul 13 09:40:08 srv670432 ollama[490754]: signal arrived during cgo execution &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I have no idea what to do next? My VPS has 8GB of RAM. After running this: root@srv670432:~# ollama run tinyllama &amp;quot;Hello, what's 2+2?&amp;quot;&lt;/p&gt; &lt;p&gt;Error: llama runner process has terminated: exit status 2&lt;/p&gt; &lt;p&gt;root@srv670432:~# &lt;/p&gt; &lt;pre&gt;&lt;code&gt;Jul 13 09:50:55 srv670432 ollama[490754]: [GIN] 2025/07/13 - 09:50:55 | 200 | 39.52µs | 127.0.0.1 | HEAD &amp;quot;/&amp;quot; Jul 13 09:50:55 srv670432 ollama[490754]: [GIN] 2025/07/13 - 09:50:55 | 200 | 39.553332ms | 127.0.0.1 | POST &amp;quot;/api/show&amp;quot; Jul 13 09:50:55 srv670432 ollama[490754]: time=2025-07-13T09:50:55.154Z level=INFO source=server.go:135 msg=&amp;quot;system memory&amp;quot; total=&amp;quot;7.8 GiB&amp;quot; free=&amp;quot;5.9 GiB&amp;quot; free_swap=&amp;quot;4.4 GiB&amp;quot; Jul 13 09:50:55 srv670432 ollama[490754]: time=2025-07-13T09:50:55.154Z level=WARN source=server.go:145 msg=&amp;quot;requested context size too large for model&amp;quot; num_ctx=8192 num_parallel=2 n_ctx_train=2048 Jul 13 09:50:55 srv670432 ollama[490754]: time=2025-07-13T09:50:55.155Z level=INFO source=server.go:175 msg=offload library=cpu layers.requested=-1 layers.model=23 layers.offload=0 layers.split=&amp;quot;&amp;quot; memory.available=&amp;quot;[5.9 GiB]&amp;quot; memory.gpu_overhead=&amp;quot;0 B&amp;quot; memory.required.full=&amp;quot;967.0 MiB&amp;quot; memory.required.partial=&amp;quot;0 B&amp;quot; memory.required.kv=&amp;quot;88.0 MiB&amp;quot; memory.required.allocations=&amp;quot;[967.0 MiB]&amp;quot; memory.weights.total=&amp;quot;571.4 MiB&amp;quot; memory.weights.repeating=&amp;quot;520.1 MiB&amp;quot; memory.weights.nonrepeating=&amp;quot;51.3 MiB&amp;quot; memory.graph.full=&amp;quot;280.0 MiB&amp;quot; memory.graph.partial=&amp;quot;278.3 MiB&amp;quot; Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-2af3b81862c6be03c769683af18efdadb2c33f60ff32ab6f83e42c043d6c7816 (version GGUF V3 (latest)) Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output. Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 0: general.architecture str = llama Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 1: general.name str = TinyLlama Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 2: llama.context_length u32 = 2048 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 3: llama.embedding_length u32 = 2048 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 4: llama.block_count u32 = 22 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 5: llama.feed_forward_length u32 = 5632 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 6: llama.rope.dimension_count u32 = 64 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 7: llama.attention.head_count u32 = 32 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 8: llama.attention.head_count_kv u32 = 4 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 9: llama.attention.layer_norm_rms_epsilon f32 = 0.000010 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 10: llama.rope.freq_base f32 = 10000.000000 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 11: general.file_type u32 = 2 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 12: tokenizer.ggml.model str = llama Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 13: tokenizer.ggml.tokens arr[str,32000] = [&amp;quot;&amp;lt;unk&amp;gt;&amp;quot;, &amp;quot;&amp;lt;s&amp;gt;&amp;quot;, &amp;quot;&amp;lt;/s&amp;gt;&amp;quot;, &amp;quot;&amp;lt;0x00&amp;gt;&amp;quot;, &amp;quot;&amp;lt;... Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 14: tokenizer.ggml.scores arr[f32,32000] = [0.000000, 0.000000, 0.000000, 0.0000... Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 15: tokenizer.ggml.token_type arr[i32,32000] = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ... Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 16: tokenizer.ggml.merges arr[str,61249] = [&amp;quot;▁ t&amp;quot;, &amp;quot;e r&amp;quot;, &amp;quot;i n&amp;quot;, &amp;quot;▁ a&amp;quot;, &amp;quot;e n... Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 17: tokenizer.ggml.bos_token_id u32 = 1 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 18: tokenizer.ggml.eos_token_id u32 = 2 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 19: tokenizer.ggml.unknown_token_id u32 = 0 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 20: tokenizer.ggml.padding_token_id u32 = 2 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 21: tokenizer.chat_template str = {% for message in messages %}\n{% if m... Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - kv 22: general.quantization_version u32 = 2 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - type f32: 45 tensors Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - type q4_0: 155 tensors Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: - type q6_K: 1 tensors Jul 13 09:50:55 srv670432 ollama[490754]: print_info: file format = GGUF V3 (latest) Jul 13 09:50:55 srv670432 ollama[490754]: print_info: file type = Q4_0 Jul 13 09:50:55 srv670432 ollama[490754]: print_info: file size = 606.53 MiB (4.63 BPW) Jul 13 09:50:55 srv670432 ollama[490754]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect Jul 13 09:50:55 srv670432 ollama[490754]: load: special tokens cache size = 3 Jul 13 09:50:55 srv670432 ollama[490754]: load: token to piece cache size = 0.1684 MB Jul 13 09:50:55 srv670432 ollama[490754]: print_info: arch = llama Jul 13 09:50:55 srv670432 ollama[490754]: print_info: vocab_only = 1 Jul 13 09:50:55 srv670432 ollama[490754]: print_info: model type = ?B Jul 13 09:50:55 srv670432 ollama[490754]: print_info: model params = 1.10 B Jul 13 09:50:55 srv670432 ollama[490754]: print_info: general.name = TinyLlama Jul 13 09:50:55 srv670432 ollama[490754]: print_info: vocab type = SPM Jul 13 09:50:55 srv670432 ollama[490754]: print_info: n_vocab = 32000 Jul 13 09:50:55 srv670432 ollama[490754]: print_info: n_merges = 0 Jul 13 09:50:55 srv670432 ollama[490754]: print_info: BOS token = 1 '&amp;lt;s&amp;gt;' Jul 13 09:50:55 srv670432 ollama[490754]: print_info: EOS token = 2 '&amp;lt;/s&amp;gt;' Jul 13 09:50:55 srv670432 ollama[490754]: print_info: UNK token = 0 '&amp;lt;unk&amp;gt;' Jul 13 09:50:55 srv670432 ollama[490754]: print_info: PAD token = 2 '&amp;lt;/s&amp;gt;' Jul 13 09:50:55 srv670432 ollama[490754]: print_info: LF token = 13 '&amp;lt;0x0A&amp;gt;' Jul 13 09:50:55 srv670432 ollama[490754]: print_info: EOG token = 2 '&amp;lt;/s&amp;gt;' Jul 13 09:50:55 srv670432 ollama[490754]: print_info: max token length = 48 Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_load: vocab only - skipping tensors Jul 13 09:50:55 srv670432 ollama[490754]: time=2025-07-13T09:50:55.214Z level=INFO source=server.go:438 msg=&amp;quot;starting llama server&amp;quot; cmd=&amp;quot;/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-2af3b81862c6be03c769683af18efdadb2c33f60ff32ab6f83e42c043d6c7816 --ctx-size 4096 --batch-size 512 --threads 2 --no-mmap --parallel 2 --port 35479&amp;quot; Jul 13 09:50:55 srv670432 ollama[490754]: time=2025-07-13T09:50:55.214Z level=INFO source=sched.go:483 msg=&amp;quot;loaded runners&amp;quot; count=1 Jul 13 09:50:55 srv670432 ollama[490754]: time=2025-07-13T09:50:55.214Z level=INFO source=server.go:598 msg=&amp;quot;waiting for llama runner to start responding&amp;quot; Jul 13 09:50:55 srv670432 ollama[490754]: time=2025-07-13T09:50:55.215Z level=INFO source=server.go:632 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server not responding&amp;quot; Jul 13 09:50:55 srv670432 ollama[490754]: time=2025-07-13T09:50:55.243Z level=INFO source=runner.go:815 msg=&amp;quot;starting go runner&amp;quot; Jul 13 09:50:55 srv670432 ollama[490754]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so Jul 13 09:50:55 srv670432 ollama[490754]: time=2025-07-13T09:50:55.267Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc) Jul 13 09:50:55 srv670432 ollama[490754]: time=2025-07-13T09:50:55.268Z level=INFO source=runner.go:874 msg=&amp;quot;Server listening on 127.0.0.1:35479&amp;quot; Jul 13 09:50:55 srv670432 ollama[490754]: llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-2af3b81862c6be03c769683af18efdadb2c33f60ff32ab6f83e42c043d6c7816 (version GGUF V3 (latest)) // de;ete some to keep post shorter Jul 13 09:50:55 srv670432 ollama[490754]: SIGILL: illegal instruction Jul 13 09:50:55 srv670432 ollama[490754]: PC=0x7f68f2ceb5aa m=3 sigcode=2 Jul 13 09:50:55 srv670432 ollama[490754]: signal arrived during cgo execution Jul 13 09:50:55 srv670432 ollama[490754]: instruction bytes: 0x62 0xf2 0xfd 0x8 0x7c 0xc0 0xc5 0xfa 0x7f 0x43 0x18 0x48 0x83 0xc4 0x8 0x5b Jul 13 09:50:55 srv670432 ollama[490754]: goroutine 5 gp=0xc000002000 m=3 mp=0xc000067008 [syscall]: Jul 13 09:50:55 srv670432 ollama[490754]: runtime.cgocall(0x55d03641b7c0, 0xc000070bb0) Jul 13 09:50:55 srv670432 ollama[490754]: runtime/cgocall.go:167 +0x4b fp=0xc000070b88 sp=0xc000070b50 pc=0x55d0357598cb Jul 13 09:50:55 srv670432 ollama[490754]: github.com/ollama/ollama/llama._Cfunc_llama_model_load_from_file(0x7f68ec000b70, {0x0, 0x0, 0x0, 0x1, 0x0, 0x0, 0x55d03641b030, 0xc000519890, 0x0, ...}) Jul 13 09:50:55 srv670432 ollama[490754]: _cgo_gotypes.go:815 // delete some lines here level=ERROR source=sched.go:489 msg=&amp;quot;error loading llama server&amp;quot; error=&amp;quot;llama runner process has terminated: exit status 2&amp;quot; Jul 13 09:50:55 srv670432 ollama[490754]: [GIN] 2025/07/13 - 09:50:55 | 500 | 370.079219ms | 127.0.0.1 | POST &amp;quot;/api/generate&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I have no idea what to do, guys. Sorry if this post is very long, but I have no clue as to what is happening - any help will be welcome!&lt;/p&gt; &lt;p&gt;Thanks,&lt;/p&gt; &lt;p&gt;Antoni&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Antoni_Nabzdyk"&gt; /u/Antoni_Nabzdyk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyp2kk/ollama_cant_start_exit_status_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyp2kk/ollama_cant_start_exit_status_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyp2kk/ollama_cant_start_exit_status_2/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T09:55:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxmtc1</id>
    <title>Thank you Ollama team! Observer AI launches tonight! 🚀 I built the local open-source screen-watching tool you guys asked for.</title>
    <updated>2025-07-12T00:21:19+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lxmtc1/thank_you_ollama_team_observer_ai_launches/"&gt; &lt;img alt="Thank you Ollama team! Observer AI launches tonight! 🚀 I built the local open-source screen-watching tool you guys asked for." src="https://external-preview.redd.it/OGRoeWJxNG82Y2NmMZhMJY7xahRuiOjw2oq-BMraDIRMdnw08UBcv5QQ2J3P.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76ef79c27e133311336216d3d1ce535a146b2fa5" title="Thank you Ollama team! Observer AI launches tonight! 🚀 I built the local open-source screen-watching tool you guys asked for." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; The open-source tool that lets local LLMs watch your screen launches tonight! Thanks to your feedback, it now has a &lt;strong&gt;1-command install (completely offline no certs to accept)&lt;/strong&gt;, supports &lt;strong&gt;any OpenAI-compatible API&lt;/strong&gt;, and has &lt;strong&gt;mobile support&lt;/strong&gt;. I'd love your feedback!&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;You guys are so amazing! After all the feedback from my last post, I'm very happy to announce that Observer AI is almost officially launched! I want to thank everyone for their encouragement and ideas.&lt;/p&gt; &lt;p&gt;For those who are new, Observer AI is a privacy-first, open-source tool to build your own micro-agents that watch your screen (or camera) and trigger simple actions, all running 100% locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's New in the last few days(Directly from your feedback!):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;✅ 1-Command 100% Local Install:&lt;/strong&gt; I made it super simple. Just run docker compose up --build and the entire stack runs locally. No certs to accept or &amp;quot;online activation&amp;quot; needed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;✅ Universal Model Support:&lt;/strong&gt; You're no longer limited to Ollama! You can now connect to &lt;strong&gt;any endpoint that uses the OpenAI v1/chat standard&lt;/strong&gt;. This includes local servers like LM Studio, Llama.cpp, and more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;✅ Mobile Support:&lt;/strong&gt; You can now use the app on your phone, using its camera and microphone as sensors. (Note: Mobile browsers don't support screen sharing).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My Roadmap:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I hope that I'm just getting started. Here's what I will focus on next:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Standalone Desktop App:&lt;/strong&gt; A 1-click installer for a native app experience. (With inference and everything!)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Telegram Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Slack Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent Sharing:&lt;/strong&gt; Easily share your creations with others via a simple link.&lt;/li&gt; &lt;li&gt;And much more!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Let's Build Together:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is a tool built for tinkerers, builders, and privacy advocates like you. Your feedback is crucial.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub (Please Star if you find it cool!):&lt;/strong&gt; &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;App Link (Try it in your browser no install!):&lt;/strong&gt; &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord (Join the community):&lt;/strong&gt; &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll be hanging out in the comments all day. Let me know what you think and what you'd like to see next. Thank you again!&lt;/p&gt; &lt;p&gt;PS. Sorry to everyone who &lt;/p&gt; &lt;p&gt;Cheers,&lt;br /&gt; Roy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ksu3dt4o6ccf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxmtc1/thank_you_ollama_team_observer_ai_launches/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lxmtc1/thank_you_ollama_team_observer_ai_launches/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-12T00:21:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly8ys0</id>
    <title>Anyone run Ollama on a gaming pc?</title>
    <updated>2025-07-12T19:31:50+00:00</updated>
    <author>
      <name>/u/pdawg17</name>
      <uri>https://old.reddit.com/user/pdawg17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know it's not ideal, but I just got a 5070ti and want to see how it does compared to my Mac Mini M4 with Ollama. The challenge is that I like having keep_alive at -1 (I use Ollama for Home Assistant so I ask it questions a lot), but that means when I play a game it cannot grab enough vram to run well.&lt;/p&gt; &lt;p&gt;Anyone use this setup and happy enough with it? Do you just shut down Ollama when playing then reload when done? Other options?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pdawg17"&gt; /u/pdawg17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ly8ys0/anyone_run_ollama_on_a_gaming_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ly8ys0/anyone_run_ollama_on_a_gaming_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ly8ys0/anyone_run_ollama_on_a_gaming_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-12T19:31:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lz0lv5</id>
    <title>Any front ends/GUIs that works in windows?</title>
    <updated>2025-07-13T18:53:01+00:00</updated>
    <author>
      <name>/u/Ancient-Asparagus837</name>
      <uri>https://old.reddit.com/user/Ancient-Asparagus837</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Any front ends/GUIs that works in windows natively?&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ancient-Asparagus837"&gt; /u/Ancient-Asparagus837 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lz0lv5/any_front_endsguis_that_works_in_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lz0lv5/any_front_endsguis_that_works_in_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lz0lv5/any_front_endsguis_that_works_in_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T18:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxzo4w</id>
    <title>I built a little CLI tool to do Ollama powered "deep" research from your terminal</title>
    <updated>2025-07-12T12:49:15+00:00</updated>
    <author>
      <name>/u/LightIn_</name>
      <uri>https://old.reddit.com/user/LightIn_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lxzo4w/i_built_a_little_cli_tool_to_do_ollama_powered/"&gt; &lt;img alt="I built a little CLI tool to do Ollama powered &amp;quot;deep&amp;quot; research from your terminal" src="https://preview.redd.it/aryz1e0hwfcf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70c17e0dfdca28b999b4bd63dd9bf81a030be45d" title="I built a little CLI tool to do Ollama powered &amp;quot;deep&amp;quot; research from your terminal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey,&lt;/p&gt; &lt;p&gt;I’ve been messing around with local LLMs lately (with Ollama) and… well, I ended up making a tiny CLI tool that tries to do “deep” research from your terminal.&lt;/p&gt; &lt;p&gt;It’s called &lt;strong&gt;deepsearch&lt;/strong&gt;. Basically you give it a question, and it tries to break it down into smaller sub-questions, search stuff on Wikipedia and DuckDuckGo, filter what seems relevant, summarize it all, and give you a final answer. Like… what a human would do, I guess.&lt;/p&gt; &lt;p&gt;Here’s the repo if you’re curious:&lt;br /&gt; &lt;a href="https://github.com/LightInn/deepsearch"&gt;https://github.com/LightInn/deepsearch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I don’t really know if this is &lt;em&gt;good&lt;/em&gt; (and even less if it's somewhat usefull :c ), just trying to glue something like this together. Honestly, it’s probably pretty rough, and I’m sure there are better ways to do what it does. But I thought it was a fun experiment and figured someone else might find it interesting too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LightIn_"&gt; /u/LightIn_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aryz1e0hwfcf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lxzo4w/i_built_a_little_cli_tool_to_do_ollama_powered/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lxzo4w/i_built_a_little_cli_tool_to_do_ollama_powered/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-12T12:49:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyp75q</id>
    <title>Has any rolled their own ollama farm? What is your hardware/software setup for your remote personal ollama server?</title>
    <updated>2025-07-13T10:03:24+00:00</updated>
    <author>
      <name>/u/RyanBThiesant</name>
      <uri>https://old.reddit.com/user/RyanBThiesant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am interested in reusing old tech to make a ollama server. I like the idea of buying a bunch of ps2s, mineral oil, fish tanks, batteries and solar panels.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RyanBThiesant"&gt; /u/RyanBThiesant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyp75q/has_any_rolled_their_own_ollama_farm_what_is_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyp75q/has_any_rolled_their_own_ollama_farm_what_is_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyp75q/has_any_rolled_their_own_ollama_farm_what_is_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T10:03:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lywmxq</id>
    <title>Customization</title>
    <updated>2025-07-13T16:11:54+00:00</updated>
    <author>
      <name>/u/BikeDazzling8818</name>
      <uri>https://old.reddit.com/user/BikeDazzling8818</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BikeDazzling8818"&gt; /u/BikeDazzling8818 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/OpenWebUI/comments/1lywmex/customization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lywmxq/customization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lywmxq/customization/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T16:11:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyu3pn</id>
    <title>Is there a good model for generating working mechanical designs?</title>
    <updated>2025-07-13T14:26:25+00:00</updated>
    <author>
      <name>/u/spookyclever</name>
      <uri>https://old.reddit.com/user/spookyclever</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m trying to design a gear system and it would be helpful if I could get a model that could translate my basic ideas to working systems that I could improve on in blender or solid works. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spookyclever"&gt; /u/spookyclever &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyu3pn/is_there_a_good_model_for_generating_working/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyu3pn/is_there_a_good_model_for_generating_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyu3pn/is_there_a_good_model_for_generating_working/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T14:26:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lz2mos</id>
    <title>AMD GPU</title>
    <updated>2025-07-13T20:14:48+00:00</updated>
    <author>
      <name>/u/neofita_</name>
      <uri>https://old.reddit.com/user/neofita_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys I made a mistake and bought GPU based on AMD…is there a lot of work to make different framework than Ollama work with my GPU? Or is there any way to make it work with AMD? Or O should just sell and buy Nvidia? 🙈&lt;/p&gt; &lt;p&gt;EDIT: you were all right. It took me 10minutes including downloading everything to make it work with AMD GPU&lt;/p&gt; &lt;p&gt;THANKS ALL! 💪🏿💪🏿&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neofita_"&gt; /u/neofita_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lz2mos/amd_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lz2mos/amd_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lz2mos/amd_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T20:14:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyx1xt</id>
    <title>Trying to get my Ollama model to run faster, is my solution a good one?</title>
    <updated>2025-07-13T16:29:03+00:00</updated>
    <author>
      <name>/u/Convillious</name>
      <uri>https://old.reddit.com/user/Convillious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m a bit confused on how memory storage within the LLM works but from what I’ve seen so far, it is common to pass in a system prompt with the user prompt for every chat that is sent to the LLM.&lt;/p&gt; &lt;p&gt;I have a slow computer and I need this to speed up so I had an idea. My project is a server hosting an LLM which a user can access with an API and receive a response.&lt;/p&gt; &lt;p&gt;Instead of sending a system prompt every time, would it speed things up if on server initialization, I send a system prompt that instructed the LLM on what it’s supposed to do. And then I stored this information using LangGraphs long term memory, and then whenever a user prompts my LLM it simply derives from its memory when answering?&lt;/p&gt; &lt;p&gt;Sorry if that sounds convoluted but I just figured cutting down on the total number of input tokens would speed things up. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Convillious"&gt; /u/Convillious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyx1xt/trying_to_get_my_ollama_model_to_run_faster_is_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyx1xt/trying_to_get_my_ollama_model_to_run_faster_is_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyx1xt/trying_to_get_my_ollama_model_to_run_faster_is_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T16:29:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyv7ge</id>
    <title>How I use Gemma 3 to help me reply my texts</title>
    <updated>2025-07-13T15:13:03+00:00</updated>
    <author>
      <name>/u/sean01-eth</name>
      <uri>https://old.reddit.com/user/sean01-eth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lyv7ge/how_i_use_gemma_3_to_help_me_reply_my_texts/"&gt; &lt;img alt="How I use Gemma 3 to help me reply my texts" src="https://external-preview.redd.it/rVejZdVoLwcawbSc5Q7BMTfnBvVftpV8Jx64l7lRtUY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c583a73ddcbd440fd51b76ffac1e785cc2ae281b" title="How I use Gemma 3 to help me reply my texts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sean01-eth"&gt; /u/sean01-eth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/48w6qb1mincf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyv7ge/how_i_use_gemma_3_to_help_me_reply_my_texts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyv7ge/how_i_use_gemma_3_to_help_me_reply_my_texts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T15:13:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyv9vm</id>
    <title>Ollama helping me study</title>
    <updated>2025-07-13T15:15:50+00:00</updated>
    <author>
      <name>/u/Economy_Cucumber_702</name>
      <uri>https://old.reddit.com/user/Economy_Cucumber_702</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lyv9vm/ollama_helping_me_study/"&gt; &lt;img alt="Ollama helping me study" src="https://b.thumbs.redditmedia.com/5mWC0Sa6yLocvhKCpyWdyHMzOonYz7tV1G0Gxwp49Bs.jpg" title="Ollama helping me study" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Economy_Cucumber_702"&gt; /u/Economy_Cucumber_702 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lyv9vm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyv9vm/ollama_helping_me_study/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyv9vm/ollama_helping_me_study/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T15:15:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyodnc</id>
    <title>Podcast generation app -- works with Ollama</title>
    <updated>2025-07-13T09:07:51+00:00</updated>
    <author>
      <name>/u/lfnovo</name>
      <uri>https://old.reddit.com/user/lfnovo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I've built a podcast generation app for people that use Notebook LM for this purpose and would lke some extra capabilities like Ollama support, 1-4 speakers, multiple generation profiles, other voice provider support, and enhanced control on the generation. It also handles extracting content from any file or URL to use in the casts.&lt;/p&gt; &lt;p&gt;It comes with all you need to run, plus a UI for you to create and manage your podcasts.&lt;/p&gt; &lt;p&gt;Community feedback is very welcome. I plan to maintain this actively as its used on another big project of ours.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lfnovo/podcast-creator"&gt;https://github.com/lfnovo/podcast-creator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are some examples of a [4 person debate](&lt;a href="https://soundcloud.com/lfnovo/situational-awareness-podcast"&gt;https://soundcloud.com/lfnovo/situational-awareness-podcast&lt;/a&gt;) and [single speaker lesson](&lt;a href="https://soundcloud.com/lfnovo/single-speaker-podcast-on-situational-awareness"&gt;https://soundcloud.com/lfnovo/single-speaker-podcast-on-situational-awareness&lt;/a&gt;) on the Situational Awareness paper. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lfnovo"&gt; /u/lfnovo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyodnc/podcast_generation_app_works_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lyodnc/podcast_generation_app_works_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lyodnc/podcast_generation_app_works_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-13T09:07:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzjle1</id>
    <title>Ollama retaining history?</title>
    <updated>2025-07-14T11:11:38+00:00</updated>
    <author>
      <name>/u/DimensionEnergy</name>
      <uri>https://old.reddit.com/user/DimensionEnergy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;so ive hosted ollama locally on my system on &lt;a href="http://localhost:11434/api/generate"&gt;http://localhost:11434/api/generate&lt;/a&gt; and was testing it out a bit and it seems that between separate fetch calls, ollama seems to be retaining some memory. &lt;/p&gt; &lt;p&gt;i don't understand why this would happen because as much as i have seen modern llms, they don't change their weights during inference. &lt;/p&gt; &lt;p&gt;Scenario:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;makes a query to ollama for topic 1 with a very specific keyword that i have created&lt;/li&gt; &lt;li&gt;makes another query to ollama for a topic that is similar to topic 1 but has a new keyword. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Turns out that the first keyword shows up in the second response aswell. Not always, but this shouldn't happen at all as much as i know&lt;/p&gt; &lt;p&gt;Is there something that i am missing?&lt;br /&gt; I checked the ollama/history file and it only contained prompts that i have made from the terminal using ollama run &amp;lt;model\_name&amp;gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DimensionEnergy"&gt; /u/DimensionEnergy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzjle1/ollama_retaining_history/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzjle1/ollama_retaining_history/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lzjle1/ollama_retaining_history/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-14T11:11:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzm1px</id>
    <title>Is it possible to generate images in open-webui about the generated text?</title>
    <updated>2025-07-14T13:12:45+00:00</updated>
    <author>
      <name>/u/assmaycsgoass</name>
      <uri>https://old.reddit.com/user/assmaycsgoass</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For ex. I ask the AI to write an intro for a story about a small village near a river, describing how it looks etc.&lt;/p&gt; &lt;p&gt;AI generates the text, and the image generation model uses that as a prompt and generates an image right below the paragraph in the window.&lt;/p&gt; &lt;p&gt;Is doing something like this possible? I use comfyui a lot but am a beginner here and was wondering if something like this can be done.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/assmaycsgoass"&gt; /u/assmaycsgoass &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzm1px/is_it_possible_to_generate_images_in_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzm1px/is_it_possible_to_generate_images_in_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lzm1px/is_it_possible_to_generate_images_in_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-14T13:12:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzmpnn</id>
    <title>With ROCm 7 expanding hardware compatibility and offering Windows support, will my 6700xt finally work natively on Windows?</title>
    <updated>2025-07-14T13:41:39+00:00</updated>
    <author>
      <name>/u/toast___ghost</name>
      <uri>https://old.reddit.com/user/toast___ghost</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Struggling to find a GPU compatibility list. Any one know or have a prediction?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/toast___ghost"&gt; /u/toast___ghost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzmpnn/with_rocm_7_expanding_hardware_compatibility_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzmpnn/with_rocm_7_expanding_hardware_compatibility_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lzmpnn/with_rocm_7_expanding_hardware_compatibility_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-14T13:41:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzxmfy</id>
    <title>How do I setup a research mode with ollama?</title>
    <updated>2025-07-14T20:29:56+00:00</updated>
    <author>
      <name>/u/MineDrumPE</name>
      <uri>https://old.reddit.com/user/MineDrumPE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want my local ai models to be able to search the web, is this possible locally? I've searched and haven't found any tutorials.&lt;/p&gt; &lt;p&gt;I want to be able to give ollama research access when I am accessing through webui and through n8n which will probably be 2 different setups I'm assuming?&lt;/p&gt; &lt;p&gt;Thanks for any help&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MineDrumPE"&gt; /u/MineDrumPE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzxmfy/how_do_i_setup_a_research_mode_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lzxmfy/how_do_i_setup_a_research_mode_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lzxmfy/how_do_i_setup_a_research_mode_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-14T20:29:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1m12wm4</id>
    <title>🚨 Docker container stuck on “Waiting for application startup” — Open WebUI won’t load in browser</title>
    <updated>2025-07-16T04:02:04+00:00</updated>
    <author>
      <name>/u/0nlyAxeman</name>
      <uri>https://old.reddit.com/user/0nlyAxeman</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/0nlyAxeman"&gt; /u/0nlyAxeman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1m12ij7/docker_container_stuck_on_waiting_for_application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m12wm4/docker_container_stuck_on_waiting_for_application/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m12wm4/docker_container_stuck_on_waiting_for_application/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T04:02:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0gzg2</id>
    <title>We built Explainable AI with pinpointed citations &amp; reasoning — works across PDFs, Excel, CSV, Docs &amp; more</title>
    <updated>2025-07-15T12:53:56+00:00</updated>
    <author>
      <name>/u/Effective-Ad2060</name>
      <uri>https://old.reddit.com/user/Effective-Ad2060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just added explainability to our RAG pipeline — the AI now shows &lt;strong&gt;pinpointed citations&lt;/strong&gt; down to the &lt;strong&gt;exact paragraph, table row, or cell&lt;/strong&gt; it used to generate its answer.&lt;/p&gt; &lt;p&gt;It doesn’t just name the source file but also &lt;strong&gt;highlights the exact text&lt;/strong&gt; and lets you &lt;strong&gt;jump directly to that part of the document&lt;/strong&gt;. This works across formats: PDFs, Excel, CSV, Word, PowerPoint, Markdown, and more.&lt;/p&gt; &lt;p&gt;It makes AI answers easy to &lt;strong&gt;trust and verify&lt;/strong&gt;, especially in messy or lengthy enterprise files. You also get insight into the &lt;strong&gt;reasoning&lt;/strong&gt; behind the answer.&lt;/p&gt; &lt;p&gt;It’s fully open-source: &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;br /&gt; Would love to hear your thoughts or feedback!&lt;/p&gt; &lt;p&gt;📹 Demo: &lt;a href="https://youtu.be/QWY_jtjRcCM"&gt;https://youtu.be/QWY_jtjRcCM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective-Ad2060"&gt; /u/Effective-Ad2060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m0gzg2/we_built_explainable_ai_with_pinpointed_citations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m0gzg2/we_built_explainable_ai_with_pinpointed_citations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m0gzg2/we_built_explainable_ai_with_pinpointed_citations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-15T12:53:56+00:00</published>
  </entry>
</feed>
