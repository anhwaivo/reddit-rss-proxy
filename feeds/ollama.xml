<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-31T16:37:59+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1idv02o</id>
    <title>Has anyone been using the base M4 Mac Mini as an Ollama server and want to share their mileage?</title>
    <updated>2025-01-30T19:20:09+00:00</updated>
    <author>
      <name>/u/_AdamWTF</name>
      <uri>https://old.reddit.com/user/_AdamWTF</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently have a home lab setup that spans a few different machines but I don’t currently have anything capable of running AI at decent speeds, I was going to Frankenstein together a machine using spare parts I have laying around, but getting a decent GPU with acceptable vram in the UK is so expensive right now.&lt;/p&gt; &lt;p&gt;This brings me to the Mac Mini M4, looking at just the base model and getting it for around £550, it seems like a lot of power for a decent price. I’m just curious how good the performance really is, I’m expecting with something like a 3b model of say llama 3.2 the response for 5k context tokens to be &amp;lt; 3 seconds to be suitable for me. &lt;/p&gt; &lt;p&gt;If anyone is willing to share their experiences and some examples of the kind of speeds you’re getting I’d really appreciate it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_AdamWTF"&gt; /u/_AdamWTF &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idv02o/has_anyone_been_using_the_base_m4_mac_mini_as_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idv02o/has_anyone_been_using_the_base_m4_mac_mini_as_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idv02o/has_anyone_been_using_the_base_m4_mac_mini_as_an/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T19:20:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1idxa19</id>
    <title>Fine tuning for small models</title>
    <updated>2025-01-30T20:55:48+00:00</updated>
    <author>
      <name>/u/jcrowe</name>
      <uri>https://old.reddit.com/user/jcrowe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an app that pulls specific information from a website. Right now I scrape the page, convert it to text and upload it to chatgpt to get a json file of data. Works great…&lt;/p&gt; &lt;p&gt;What I would like to do is switch over to a small LLM model (llama3.2:1b for example). But this model doesn’t return the results I want.&lt;/p&gt; &lt;p&gt;I would like to fine tune this model to make it work for my use case.&lt;/p&gt; &lt;p&gt;Can anyone recommend a tutorial for this or tell me I’m an idiot if it’s not the right way to use the technology?&lt;/p&gt; &lt;p&gt;ETA: to clarify, I don’t want to fine tune for the model to have the new information, I want to fine tune so that it knows how to deal with this information. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jcrowe"&gt; /u/jcrowe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idxa19/fine_tuning_for_small_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idxa19/fine_tuning_for_small_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idxa19/fine_tuning_for_small_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T20:55:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie5k9y</id>
    <title>Facing errors while installing deepseek r1 1.5b through ollama in windows</title>
    <updated>2025-01-31T03:21:10+00:00</updated>
    <author>
      <name>/u/AndreoBee100</name>
      <uri>https://old.reddit.com/user/AndreoBee100</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ie5k9y/facing_errors_while_installing_deepseek_r1_15b/"&gt; &lt;img alt="Facing errors while installing deepseek r1 1.5b through ollama in windows" src="https://preview.redd.it/dmv3g0miz8ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f773d3a58be867c97bec65c82e0a6ef252cea3e6" title="Facing errors while installing deepseek r1 1.5b through ollama in windows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AndreoBee100"&gt; /u/AndreoBee100 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dmv3g0miz8ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie5k9y/facing_errors_while_installing_deepseek_r1_15b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie5k9y/facing_errors_while_installing_deepseek_r1_15b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T03:21:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1idpjtj</id>
    <title>What would be the simplest way to train deepseek model on self hosted server.</title>
    <updated>2025-01-30T15:31:52+00:00</updated>
    <author>
      <name>/u/shaxadhere</name>
      <uri>https://old.reddit.com/user/shaxadhere</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to train model on specific set of data, I have the data in raw text and I have questions and answers from that raw text.&lt;/p&gt; &lt;p&gt;Is there any way I can train my model on all of that data.&lt;/p&gt; &lt;p&gt;Documents in total are 400,000 pages, and questions and answers are around 1 million+&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shaxadhere"&gt; /u/shaxadhere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idpjtj/what_would_be_the_simplest_way_to_train_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idpjtj/what_would_be_the_simplest_way_to_train_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idpjtj/what_would_be_the_simplest_way_to_train_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T15:31:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1idvw46</id>
    <title>Recommended deepseek model for coding tasks for an average PC?</title>
    <updated>2025-01-30T19:57:31+00:00</updated>
    <author>
      <name>/u/Upset_Hippo_5304</name>
      <uri>https://old.reddit.com/user/Upset_Hippo_5304</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Want to join the gang and try this stuff for coding tasks.&lt;/p&gt; &lt;p&gt;Py, dart, js, c#&lt;/p&gt; &lt;p&gt;32GB ram RTX 3070 Ryzen 5 5600x&lt;/p&gt; &lt;p&gt;Cheers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Upset_Hippo_5304"&gt; /u/Upset_Hippo_5304 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idvw46/recommended_deepseek_model_for_coding_tasks_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idvw46/recommended_deepseek_model_for_coding_tasks_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idvw46/recommended_deepseek_model_for_coding_tasks_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T19:57:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie6fzy</id>
    <title>Empty response for Deepseek Models</title>
    <updated>2025-01-31T04:09:36+00:00</updated>
    <author>
      <name>/u/TheHarinator</name>
      <uri>https://old.reddit.com/user/TheHarinator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone have a problem when trying to interact with the Deepseek-r1 models? The response always is empty for me. I have tried removing and re-pulling these models too. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheHarinator"&gt; /u/TheHarinator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie6fzy/empty_response_for_deepseek_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie6fzy/empty_response_for_deepseek_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie6fzy/empty_response_for_deepseek_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T04:09:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1iealuo</id>
    <title>why Ai Model requirement always show Memory instead GPU VRAM size?</title>
    <updated>2025-01-31T08:51:06+00:00</updated>
    <author>
      <name>/u/TheLastAirbender2025</name>
      <uri>https://old.reddit.com/user/TheLastAirbender2025</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, &lt;/p&gt; &lt;p&gt;Apologies in advance i am so new to Ai and bit confused here about term RAM mean. Does it mean memory in the GPU or Memory physically on the pc? I am windows user and my pc is bit old meaning 13 years at lest if not longer. &lt;/p&gt; &lt;p&gt;Example &lt;/p&gt; &lt;ul&gt; &lt;li&gt;7b models generally require at least 8GB of RAM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So when i installed a model onto my pc i see in system resources Ram hitting 9 GB and CPU % is like 30 but GPU is not even hitting 1% and 3D part show no activity. &lt;/p&gt; &lt;p&gt;I have Sparkle Intel Arc A380 ELF, 6GB GDDR6, Single Fan, SA380E-6G &lt;/p&gt; &lt;p&gt;Can someone explain this to me please &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLastAirbender2025"&gt; /u/TheLastAirbender2025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iealuo/why_ai_model_requirement_always_show_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iealuo/why_ai_model_requirement_always_show_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iealuo/why_ai_model_requirement_always_show_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T08:51:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1iebe80</id>
    <title>Deepseek unsecure?</title>
    <updated>2025-01-31T09:53:57+00:00</updated>
    <author>
      <name>/u/Emergency-Radish-696</name>
      <uri>https://old.reddit.com/user/Emergency-Radish-696</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My deepseek r1 will not do creative writing when I give it inputs with people's names or business. ( Writing sci-fi ) Is there an open unsecure model that isn't so bashful?( A deepseek model: unsecure?)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emergency-Radish-696"&gt; /u/Emergency-Radish-696 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iebe80/deepseek_unsecure/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iebe80/deepseek_unsecure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iebe80/deepseek_unsecure/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T09:53:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieam9t</id>
    <title>I can't see qwen2.5 max (the latest model) in ollama models list ? how can I run this locally ?</title>
    <updated>2025-01-31T08:52:01+00:00</updated>
    <author>
      <name>/u/AxelBlaze20850</name>
      <uri>https://old.reddit.com/user/AxelBlaze20850</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AxelBlaze20850"&gt; /u/AxelBlaze20850 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieam9t/i_cant_see_qwen25_max_the_latest_model_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieam9t/i_cant_see_qwen25_max_the_latest_model_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ieam9t/i_cant_see_qwen25_max_the_latest_model_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T08:52:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie42ld</id>
    <title>Deepseek-r1:8b is 4 times slower than llama3.2:8b on Ollama running locally</title>
    <updated>2025-01-31T02:03:05+00:00</updated>
    <author>
      <name>/u/PawanAgarwal</name>
      <uri>https://old.reddit.com/user/PawanAgarwal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am running both llama3.2:8b and deepseek-r1:8b locally on my mac. I noticed than latency per token for deepseek-r1:8b model is 4x llama3.2:8b. Since both are 8b versions, I was hoping latency would be similar. Anyone else also seeing that? Any configs needed for ollama for deepseek serving?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PawanAgarwal"&gt; /u/PawanAgarwal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie42ld/deepseekr18b_is_4_times_slower_than_llama328b_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie42ld/deepseekr18b_is_4_times_slower_than_llama328b_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie42ld/deepseekr18b_is_4_times_slower_than_llama328b_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T02:03:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1idh8ft</id>
    <title>Deepseek r1 671b on my local PC</title>
    <updated>2025-01-30T07:03:27+00:00</updated>
    <author>
      <name>/u/Geschirrtuch</name>
      <uri>https://old.reddit.com/user/Geschirrtuch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;Two days ago, I turned night into day, and in the end, I managed to get R1 running on my local PC. Yesterday, I uploaded a video on YouTube showing how I did it: &lt;a href="https://www.youtube.com/watch?v=O3Lk3xSkAdk"&gt;https://www.youtube.com/watch?v=O3Lk3xSkAdk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I don't post here often, so I'm not sure if sharing the link is okay—I hope it is.&lt;/p&gt; &lt;p&gt;The video is in German, but with subtitles, everyone should be able to understand it.&lt;br /&gt; Be careful if you want to try this yourself! ;)&lt;/p&gt; &lt;p&gt;Update:&lt;/p&gt; &lt;p&gt;For those who don't feel like watching the video: The &amp;quot;trick&amp;quot; was using Windows' pagefile. I set up three of them on three different SSDs, which gave me around 750GB of virtual memory in total.&lt;/p&gt; &lt;p&gt;Loading the model and answering a question took my PC about 90 minutes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Geschirrtuch"&gt; /u/Geschirrtuch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T07:03:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieby8e</id>
    <title>Is there a way to limit the number of concurrent downloads when pulling a model?</title>
    <updated>2025-01-31T10:35:17+00:00</updated>
    <author>
      <name>/u/MarinatedPickachu</name>
      <uri>https://old.reddit.com/user/MarinatedPickachu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm on windows and I get a lot of problems when pulling models, often the progress bar reverts to a lower number and many times i end up getting a &amp;quot;max retries exceeded&amp;quot; error completely cancelling the pull. Sometimes it succeeds, but rarely. I think it's caused by ollama creating many parallel downloads at once and cancelling them as soon as they stall for 5 seconds. Is there a way to limit the number of simultaneous downoads or to increase the timeout?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarinatedPickachu"&gt; /u/MarinatedPickachu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieby8e/is_there_a_way_to_limit_the_number_of_concurrent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieby8e/is_there_a_way_to_limit_the_number_of_concurrent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ieby8e/is_there_a_way_to_limit_the_number_of_concurrent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T10:35:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iecg2d</id>
    <title>What is wrong with the fp16 llama3.2 model?</title>
    <updated>2025-01-31T11:11:12+00:00</updated>
    <author>
      <name>/u/Octopus0nFire</name>
      <uri>https://old.reddit.com/user/Octopus0nFire</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iecg2d/what_is_wrong_with_the_fp16_llama32_model/"&gt; &lt;img alt="What is wrong with the fp16 llama3.2 model?" src="https://a.thumbs.redditmedia.com/X9t_AQnuJ6IunxNin7I8EXVXmN37kil_4Wm0LgjbQo8.jpg" title="What is wrong with the fp16 llama3.2 model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4pyucoo0bbge1.png?width=639&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67afdd30cb3efb720edb25967e88677f6168f6e3"&gt;https://preview.redd.it/4pyucoo0bbge1.png?width=639&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67afdd30cb3efb720edb25967e88677f6168f6e3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model with the tag 'latest' work as expexted, but the fp16 (and also the q8_0 llama 3.1 model) seem to be absolutely insane. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Octopus0nFire"&gt; /u/Octopus0nFire &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iecg2d/what_is_wrong_with_the_fp16_llama32_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iecg2d/what_is_wrong_with_the_fp16_llama32_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iecg2d/what_is_wrong_with_the_fp16_llama32_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T11:11:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1iecgob</id>
    <title>LLama2 absurd response</title>
    <updated>2025-01-31T11:12:29+00:00</updated>
    <author>
      <name>/u/ppadiya</name>
      <uri>https://old.reddit.com/user/ppadiya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iecgob/llama2_absurd_response/"&gt; &lt;img alt="LLama2 absurd response" src="https://b.thumbs.redditmedia.com/4MKGfDk6ED6EOXxngZ1hATaXUNpkIBZRADr4clqZfpE.jpg" title="LLama2 absurd response" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/cwaw8c13bbge1.png?width=1731&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a56793cfeaefc285ee5bb39163047556a21c49b7"&gt;https://preview.redd.it/cwaw8c13bbge1.png?width=1731&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a56793cfeaefc285ee5bb39163047556a21c49b7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What is happening here? I just loaded it and asked 'Hi, How are you' and it went on and on about learning Chinese. I had to interrupt it as it went on and on for over 5 mins.&lt;br /&gt; I then loaded it on open-webui and asked the same question. it again started a very long response on 'aspiring writers and authors' &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ppadiya"&gt; /u/ppadiya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iecgob/llama2_absurd_response/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iecgob/llama2_absurd_response/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iecgob/llama2_absurd_response/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T11:12:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iecleb</id>
    <title>Running DeepSeek R1 on my M4 Pro Mac mini with Ollama</title>
    <updated>2025-01-31T11:21:31+00:00</updated>
    <author>
      <name>/u/ope_poe</name>
      <uri>https://old.reddit.com/user/ope_poe</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ope_poe"&gt; /u/ope_poe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/macmini/comments/1idfuew/running_deepseek_r1_on_my_m4_pro_mac_mini_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iecleb/running_deepseek_r1_on_my_m4_pro_mac_mini_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iecleb/running_deepseek_r1_on_my_m4_pro_mac_mini_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T11:21:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1iedn22</id>
    <title>a 5B model? need some help ( cause of some words, not very nsfw but eh)</title>
    <updated>2025-01-31T12:26:52+00:00</updated>
    <author>
      <name>/u/Ardion63</name>
      <uri>https://old.reddit.com/user/Ardion63</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's ups guys, im first timer here in this reddit page but i need some help&lt;/p&gt; &lt;p&gt;i got a rtx 3060 6 gb vrm , i can use 7B models but the speed is a bit slow, especially longer sentences&lt;br /&gt; do you guys know any good 5B models?&lt;/p&gt; &lt;p&gt;Prefer Uncensored or abliterated ones cause i want to see how much &amp;quot;personality&amp;quot; an AI can get with its text / make it be my AI for the rest of my life, so a little good buddy AI just wont cut but oh well&lt;/p&gt; &lt;p&gt;i tried a bunch 7B models, they are alright but you know vram limited ( the rest of my laptop specs are much better yea) i am also trying the prithivMLmods/Triangulum-5B&lt;/p&gt; &lt;p&gt;but other then that i haven't seen any 5B ( 3B will make the AI impossible to get what i wanted sooo 5B is the sweet spot)&lt;/p&gt; &lt;p&gt;thanks!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ardion63"&gt; /u/Ardion63 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iedn22/a_5b_model_need_some_help_cause_of_some_words_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iedn22/a_5b_model_need_some_help_cause_of_some_words_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iedn22/a_5b_model_need_some_help_cause_of_some_words_not/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T12:26:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie1aui</id>
    <title>Does `ollama create` actually build a new model?</title>
    <updated>2025-01-30T23:50:04+00:00</updated>
    <author>
      <name>/u/homelab2946</name>
      <uri>https://old.reddit.com/user/homelab2946</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded a GGUF file and create a Modelfile to extend it. Then I run `ollama create model_name -f Modelfile`, a `model_name:latest` model is created and shows in `ollama list` 10 GB. The GGUF file is also around the same 10 GB. Does `ollama create` not just add instruction on a base model but actually acting more like `docker build`? Would it then be fine to remove the GGUF file after the build?&lt;/p&gt; &lt;p&gt;Another scenario is through a supported ollama model, like `llama3`. Does Ollama &amp;quot;build&amp;quot; a new image if I create a new Modelfile from `llama3`, so it takes double the storage?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/homelab2946"&gt; /u/homelab2946 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1aui/does_ollama_create_actually_build_a_new_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1aui/does_ollama_create_actually_build_a_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie1aui/does_ollama_create_actually_build_a_new_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T23:50:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iegqef</id>
    <title>AI models basics for newcomers</title>
    <updated>2025-01-31T15:03:14+00:00</updated>
    <author>
      <name>/u/Level_Fennel8071</name>
      <uri>https://old.reddit.com/user/Level_Fennel8071</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there any good place to underatand basic terms and specs for different models,and how it gonna affect the model usecases and hw requirements, terms like parameter size, quantization, model type, model format, context window....etc.&lt;/p&gt; &lt;p&gt;my goal is to run model that helps me studying, mostly summarizing books and docs, being able to chat with the model about some materials is plus&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Level_Fennel8071"&gt; /u/Level_Fennel8071 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegqef/ai_models_basics_for_newcomers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegqef/ai_models_basics_for_newcomers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iegqef/ai_models_basics_for_newcomers/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T15:03:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iedpso</id>
    <title>Is anyone else's Deepseek a bit chatty?</title>
    <updated>2025-01-31T12:31:06+00:00</updated>
    <author>
      <name>/u/Jimmy_drumstix</name>
      <uri>https://old.reddit.com/user/Jimmy_drumstix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've installed deepseek-r1:8b on my computer using ollama. I am asking it to provide shorter answers but it has verbal diarrhoea. Has anyone else experienced this?&lt;/p&gt; &lt;p&gt;Here's from the last two prompts I gave it:&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jimmy_drumstix"&gt; /u/Jimmy_drumstix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iedpso/is_anyone_elses_deepseek_a_bit_chatty/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iedpso/is_anyone_elses_deepseek_a_bit_chatty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iedpso/is_anyone_elses_deepseek_a_bit_chatty/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T12:31:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieeqcf</id>
    <title>What's going on with my download? keeps going backwards after it makes some progress?</title>
    <updated>2025-01-31T13:26:43+00:00</updated>
    <author>
      <name>/u/dusty_whale</name>
      <uri>https://old.reddit.com/user/dusty_whale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ieeqcf/whats_going_on_with_my_download_keeps_going/"&gt; &lt;img alt="What's going on with my download? keeps going backwards after it makes some progress?" src="https://external-preview.redd.it/MTljMHJtN2h6YmdlMfBoFUu8tfPjIjApOHTrjSrxAuMuJVPtuHQoaGzrhN1h.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5315b7f329164fa7d7289dc1353237595c527220" title="What's going on with my download? keeps going backwards after it makes some progress?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dusty_whale"&gt; /u/dusty_whale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rfoy8p7hzbge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieeqcf/whats_going_on_with_my_download_keeps_going/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ieeqcf/whats_going_on_with_my_download_keeps_going/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T13:26:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1iegog2</id>
    <title>Would x2 RTX 3060 12GB suffice advanced models like DeepSeek R1 14B or higher?</title>
    <updated>2025-01-31T15:00:54+00:00</updated>
    <author>
      <name>/u/GamerGuy95953</name>
      <uri>https://old.reddit.com/user/GamerGuy95953</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am determining if I should buy 2 RTX 3060 12GB (around $270 US each.) wondering if it’s even worth it over other options. Based on my research AI models perform better with lots of VRAM over clock speeds. &lt;/p&gt; &lt;p&gt;My server PC setup currently is a GTX 1050 and a Ryzen 7 2700X. I am planning to also upgrade the CPU to R9 something. It depending on the price for the R9 models. &lt;/p&gt; &lt;p&gt;Any suggestions are appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GamerGuy95953"&gt; /u/GamerGuy95953 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegog2/would_x2_rtx_3060_12gb_suffice_advanced_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegog2/would_x2_rtx_3060_12gb_suffice_advanced_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iegog2/would_x2_rtx_3060_12gb_suffice_advanced_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T15:00:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1idqxto</id>
    <title>Why Are All Local AI Models So Bad? No One Talks About This!</title>
    <updated>2025-01-30T16:31:59+00:00</updated>
    <author>
      <name>/u/NikkEvan</name>
      <uri>https://old.reddit.com/user/NikkEvan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with local AI models, even &amp;quot;high-end ones&amp;quot; like the recent DeepSeek-R1 32B, using Open WebUI.&lt;br /&gt; I expected them to be weaker than online models, but the gap is just ridiculous.&lt;br /&gt; Even for the simplest questions, they either fail, give nonsense answers, or completely misunderstand the input.&lt;/p&gt; &lt;p&gt;I’ve set the parameters and all the settings at the best i could, tried different setups, system prompts, and still , even after parsing a basic document just a few pages long, is a struggle.&lt;br /&gt; If it already fails here, how am I supposed to use it for hundreds of internal company documents?&lt;/p&gt; &lt;p&gt;The crazy part? No one talks about this!&lt;br /&gt; Instead, i see every video in youtube saying :&lt;br /&gt; &amp;quot;How to run locally (modelname) much better than chat-gpt&amp;quot;&lt;br /&gt; &amp;quot;Local Deepseek beats Chat-gpt&amp;quot;&lt;br /&gt; Than the question they ask to those local models are : How many 'R' are in the word Strawberry and the model answer: 2 ... lol &lt;/p&gt; &lt;p&gt;Why is the performance so bad, even on 32B models?&lt;/p&gt; &lt;p&gt;Why are there no proper guides to get the best out of local AI?&lt;br /&gt; Having a big hardware such as the Nvidia project DIGITS will make a big model work close to the online Chat-gpt 3 or 4 ? I see those has 175b parameters. &lt;/p&gt; &lt;p&gt;What are we missing?&lt;/p&gt; &lt;p&gt;I really want to make local AI work as close as the online models, even buying bigger and stronger hardware, but, right now, it just feels like a waste of time.&lt;br /&gt; Has anyone actually succeeded in making these models work well? If so, how? And , what do you intend for Working Well for a local Model ? &lt;/p&gt; &lt;p&gt;Let’s discuss this! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NikkEvan"&gt; /u/NikkEvan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T16:31:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iegbea</id>
    <title>Why is open-webui this size?</title>
    <updated>2025-01-31T14:43:53+00:00</updated>
    <author>
      <name>/u/DevuDixit</name>
      <uri>https://old.reddit.com/user/DevuDixit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iegbea/why_is_openwebui_this_size/"&gt; &lt;img alt="Why is open-webui this size?" src="https://external-preview.redd.it/Erly_C0iblWMXf_1Jomyy7GuVEMgel-rmrf80xwRk9I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa064b742fd7f951e2c00ee5d40558b26fb7d7d2" title="Why is open-webui this size?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/87a620piccge1.png?width=1102&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=824a6c45bcd69a837b8fe314f55546e743df3eee"&gt;https://preview.redd.it/87a620piccge1.png?width=1102&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=824a6c45bcd69a837b8fe314f55546e743df3eee&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/id1h4wgudcge1.png?width=1089&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f88f10529c414c7eaa7e3b463f062079af32aeb1"&gt;https://preview.redd.it/id1h4wgudcge1.png?width=1089&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f88f10529c414c7eaa7e3b463f062079af32aeb1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I was just trying to install and use the &lt;a href="https://github.com/open-webui/open-webui"&gt;open-webui&lt;/a&gt; for using the gui and I was curious about these files (marked by red) it is pulling. I have used some local web based guis in the past and all of them were under 100 MBs of size. So why is this comparatively bulky ? ( just curious ; )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DevuDixit"&gt; /u/DevuDixit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegbea/why_is_openwebui_this_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iegbea/why_is_openwebui_this_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iegbea/why_is_openwebui_this_size/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T14:43:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie210z</id>
    <title>Got Deepseek R1 1.5b running locally on Pixel 8 pro</title>
    <updated>2025-01-31T00:24:19+00:00</updated>
    <author>
      <name>/u/Teradyyne</name>
      <uri>https://old.reddit.com/user/Teradyyne</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"&gt; &lt;img alt="Got Deepseek R1 1.5b running locally on Pixel 8 pro" src="https://external-preview.redd.it/cTk0emwwZ3ozOGdlMYbCJM1MQLfOpw8fF1FnxkqAh7visCMP7lFjyVXppg7i.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=263215164e10efcd203b94bae4672c47e11c63d8" title="Got Deepseek R1 1.5b running locally on Pixel 8 pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Teradyyne"&gt; /u/Teradyyne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cha0fplz38ge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T00:24:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieb1za</id>
    <title>WARNING: Major Price Increase for Cursor’s Agentic Composer — 25x Hike</title>
    <updated>2025-01-31T09:26:56+00:00</updated>
    <author>
      <name>/u/pokemontra4321</name>
      <uri>https://old.reddit.com/user/pokemontra4321</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One of the best parts about cursor was agentic composer.&lt;/p&gt; &lt;p&gt;I’ve been using Cursor extensively for its Agentic Composer feature, where multiple tool calls (up to 25) within one composer message request used to count as a single “fast” request. Now, each tool call is billed separately, meaning quotas are used up at a much faster rate — effectively a 25x increase.&lt;/p&gt; &lt;p&gt;Now, you easily burn through your fast requests in a few days. Even the slow requests are not traffic based anymore, instead there are now hardcoded time limits. (Edit: I am using 0.45.7 - I had a timer countdown that seems to increase everytime. looked like exponential backoff mechanism kinda-thingy. But I could be wrong, it could be just traffic related.)&lt;/p&gt; &lt;p&gt;I loved how amazing cursor was. But 96% shrinkflation!!! What the actual fuck? This is bullshit.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;What are some good alternatives to cursor? I’ll compile a list here. (In no particular order)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/features/copilot"&gt;GithubCopilot&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.jetbrains.com/ai/"&gt;JetBrains with AI assistant&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://aws.amazon.com/q/developer/"&gt;Amazon Q Developer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://openai.com/index/introducing-canvas/?utm_source=chatgpt.com"&gt;ChatGPT Canvas&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.continue.dev/?utm_source=chatgpt.com"&gt;Continue with Ollama&lt;/a&gt; | &lt;a href="https://ollama.com/blog/continue-code-assistant"&gt;blog post from Ollama&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://codeium.com/windsurf"&gt;WindSurf&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.tabnine.com/"&gt;TabNine&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://codeium.com/"&gt;Codeium&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.trae.ai/"&gt;Trae&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://voideditor.com/"&gt;Void&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://sourcegraph.com/cody"&gt;Cody&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://zed.dev/"&gt;Zed&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://refact.ai"&gt;refact.ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.kite.com/blog/product/kite-is-saying-farewell/"&gt;Kite&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://aide.dev/"&gt;aide.dev&lt;/a&gt; | &lt;a href="https://aider.chat/"&gt;aider.chat&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://shelbula.dev/"&gt;Shelbula.dev&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;@Cursor - maybe add the US hosted deepseek R1 model in agent composer? I think that could help? but tbh, sonnet 3.5 v2 with agent composer was pretty darn solid for me!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pokemontra4321"&gt; /u/pokemontra4321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieb1za/warning_major_price_increase_for_cursors_agentic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ieb1za/warning_major_price_increase_for_cursors_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ieb1za/warning_major_price_increase_for_cursors_agentic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T09:26:56+00:00</published>
  </entry>
</feed>
