<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-04-07T03:41:49+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1js02qr</id>
    <title>Welcome to Infinite Oracle, a mystical Ollama client that channels boundless wisdom through an ethereal voice!</title>
    <updated>2025-04-05T10:01:15+00:00</updated>
    <author>
      <name>/u/WappyFlanker</name>
      <uri>https://old.reddit.com/user/WappyFlanker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1js02qr/welcome_to_infinite_oracle_a_mystical_ollama/"&gt; &lt;img alt="Welcome to Infinite Oracle, a mystical Ollama client that channels boundless wisdom through an ethereal voice!" src="https://external-preview.redd.it/K0kmjllRfd70adzKLvdhVOoCrgtDrXoi8JgMx0gxevc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc7b9ada9bdeb73795176a66b24f04c59448a616" title="Welcome to Infinite Oracle, a mystical Ollama client that channels boundless wisdom through an ethereal voice!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Greetings, Welcome to Infinite Oracle, a mystical application that channels boundless wisdom through an ethereal voice. This executable brings you cryptic, uplifting insights powered by Ollama, Coqui TTS and whisper-asr-webservice servers running locally!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WappyFlanker"&gt; /u/WappyFlanker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/aat440hz/Infinite_Oracle"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js02qr/welcome_to_infinite_oracle_a_mystical_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1js02qr/welcome_to_infinite_oracle_a_mystical_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T10:01:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1js1b0v</id>
    <title>Somehow Ollama has stopped using my GPU and I don't know why</title>
    <updated>2025-04-05T11:25:41+00:00</updated>
    <author>
      <name>/u/PFGSnoopy</name>
      <uri>https://old.reddit.com/user/PFGSnoopy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title said, Ollama doesn't utilize the GPU anymore and I have no idea why. I haven't changed anything. &lt;/p&gt; &lt;p&gt;My Ollama is running in a VM (Ubuntu 24.10) on a Proxmox Ve 8.3.5 with GPU pass-through (not as a vGPU). &lt;/p&gt; &lt;p&gt;I want to understand how this could happen and what I can do to prevent this from happening again (provided I can fix it in the first place).&lt;/p&gt; &lt;p&gt;Edit: to provide some more context. lspci inside the VM shows that the GPU (NVIDIA RTX2000 Ada Generation) is being recognised. So I would guess, it's not a case of broken GPU pass-through. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PFGSnoopy"&gt; /u/PFGSnoopy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js1b0v/somehow_ollama_has_stopped_using_my_gpu_and_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js1b0v/somehow_ollama_has_stopped_using_my_gpu_and_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1js1b0v/somehow_ollama_has_stopped_using_my_gpu_and_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T11:25:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jryl86</id>
    <title>MCP Servers using any LLM API and Local LLMs</title>
    <updated>2025-04-05T08:08:22+00:00</updated>
    <author>
      <name>/u/mehul_gupta1997</name>
      <uri>https://old.reddit.com/user/mehul_gupta1997</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jryl86/mcp_servers_using_any_llm_api_and_local_llms/"&gt; &lt;img alt="MCP Servers using any LLM API and Local LLMs" src="https://external-preview.redd.it/iiuYjTt5hi5en0DFmJ4Zb8-DLc00qDJtpewc3Kd7HY0.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=242eb6be14416a950f6bfe79e9deb75df1dcd369" title="MCP Servers using any LLM API and Local LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehul_gupta1997"&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/9Mml4ULLoF0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jryl86/mcp_servers_using_any_llm_api_and_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jryl86/mcp_servers_using_any_llm_api_and_local_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T08:08:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1js4ac5</id>
    <title>Can't set value for n_seq_max in ModelFile</title>
    <updated>2025-04-05T14:09:33+00:00</updated>
    <author>
      <name>/u/rsk_039</name>
      <uri>https://old.reddit.com/user/rsk_039</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys. I am new to editing ModelFiles in Ollama. I tried setting the value for n_seq_max to 1 (so that I can use full context window i believe), but ollama is giving me error &amp;quot;Couldn't set parameter: unknown parameter n_seq_max&amp;quot;. I tried with num_seq_max also, but same error returned. Any help with is greatly appreciated. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rsk_039"&gt; /u/rsk_039 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js4ac5/cant_set_value_for_n_seq_max_in_modelfile/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js4ac5/cant_set_value_for_n_seq_max_in_modelfile/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1js4ac5/cant_set_value_for_n_seq_max_in_modelfile/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T14:09:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1js4hds</id>
    <title>mcp_use lets you use MCPs with ollama LLMs</title>
    <updated>2025-04-05T14:19:10+00:00</updated>
    <author>
      <name>/u/Guilty-Effect-3771</name>
      <uri>https://old.reddit.com/user/Guilty-Effect-3771</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1js4hds/mcp_use_lets_you_use_mcps_with_ollama_llms/"&gt; &lt;img alt="mcp_use lets you use MCPs with ollama LLMs" src="https://external-preview.redd.it/0vfnV86EuiE-hK1Fi2yCNTpJyrIMbgnk_6FwE-6cfX0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f734f0f1a719b322fb7d76f47c2c6230dc892b44" title="mcp_use lets you use MCPs with ollama LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey lamas! &lt;/p&gt; &lt;p&gt;I do not have a lot of experience with Ollama but many people seemed interested in using MCPs from ollama models. I am not sure what your current flow is, but I think &lt;strong&gt;mcp-use&lt;/strong&gt; can be of help and some ollama users already are reaching out because it was useful to them!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;mcp-use&lt;/strong&gt; is a Python package that simplifies working with MCP. Born out of frustration with the desktop-app-only limitations of existing MCP tools, it provides a clean abstraction over the mcp connection management and server communication. It works with any langchain supported models that also support tool calling. &lt;/p&gt; &lt;p&gt;It is super easy to get started, you need:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;one of those MCPconfig JSONs - find many here: &lt;a href="https://github.com/punkpeye/awesome-mcp-servers"&gt;https://github.com/punkpeye/awesome-mcp-servers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;6 lines of code and you can have an agent use the MCP tools from python.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Like this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zhbxz4t3z0te1.png?width=1956&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fd7e284ec9f87945d43d336727ce3cf26df5136d"&gt;https://preview.redd.it/zhbxz4t3z0te1.png?width=1956&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fd7e284ec9f87945d43d336727ce3cf26df5136d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The structure is simple: an MCP client creates and manages the connection and instantiation (if needed) of the server and extracts the available tools. The MCPAgent reads the tools from the client, converts them into callable objects, gives access to them to an LLM, manages tool calls and responses.&lt;/p&gt; &lt;p&gt;It's very early-stage, and I'm sharing it here for feedback and contributions. If you're playing with MCP or building agents around it, I hope this makes your life easier.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/pietrozullo/mcp-use"&gt;https://github.com/pietrozullo/mcp-use&lt;/a&gt; Pipy: &lt;a href="https://pypi.org/project/mcp-use/"&gt;https://pypi.org/project/mcp-use/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs: &lt;a href="https://docs.mcp-use.io/introduction"&gt;https://docs.mcp-use.io/introduction&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install mcp-use &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Happy to answer questions or walk through examples!&lt;/p&gt; &lt;p&gt;Props: Name is clearly inspired by &lt;a href="https://browser-use.com/"&gt;browser_use&lt;/a&gt; an insane project by a friend of mine, following him closely I think I got brainwashed into naming everything mcp related _use.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Guilty-Effect-3771"&gt; /u/Guilty-Effect-3771 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js4hds/mcp_use_lets_you_use_mcps_with_ollama_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js4hds/mcp_use_lets_you_use_mcps_with_ollama_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1js4hds/mcp_use_lets_you_use_mcps_with_ollama_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T14:19:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1js78cr</id>
    <title>Model for Game Tips and Guide Bot</title>
    <updated>2025-04-05T16:24:23+00:00</updated>
    <author>
      <name>/u/PassionLuck</name>
      <uri>https://old.reddit.com/user/PassionLuck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to train an AI on the game Dark and Darker. The end goal is to be able to ask the AI tips on what gear to wear, skills, and perks with damage calculation. I have all of the math formulas for damage calculations.&lt;/p&gt; &lt;p&gt;Which model should I use for this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PassionLuck"&gt; /u/PassionLuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js78cr/model_for_game_tips_and_guide_bot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js78cr/model_for_game_tips_and_guide_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1js78cr/model_for_game_tips_and_guide_bot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T16:24:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsg26x</id>
    <title>What local LLM to choose</title>
    <updated>2025-04-05T22:58:47+00:00</updated>
    <author>
      <name>/u/rhawon</name>
      <uri>https://old.reddit.com/user/rhawon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I know this sounds like a noob question, but I'm a developer who wants to get familiarized with local LLMs. As a side project, I've been developing a mobile app and a backend for it, and this app needs a relatively smart LLM running together. Currently I use phi 3.5 (via ollama that runs on docker) but that's only for testing. phi is also on docker.&lt;/p&gt; &lt;p&gt;The PC spec:&lt;/p&gt; &lt;p&gt;- GPU: 2070 Super&lt;/p&gt; &lt;p&gt;- CPU: i5 8600k&lt;/p&gt; &lt;p&gt;- RAM: corsair 16gig ddr4 3000mhz cl15&lt;/p&gt; &lt;p&gt;What would be the smartest for this poor PC to run, and for me to get better results? Cannot say I'm very happy with phi thus far.&lt;/p&gt; &lt;p&gt;PS:&lt;br /&gt; Sorry, first time posting here, if I messed up some rules, happy to fix.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rhawon"&gt; /u/rhawon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsg26x/what_local_llm_to_choose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsg26x/what_local_llm_to_choose/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsg26x/what_local_llm_to_choose/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T22:58:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1js750i</id>
    <title>I built an AI Orchestrator that routes between local and cloud models based on real-time signals like battery, latency, and data sensitivity — and it's fully pluggable.</title>
    <updated>2025-04-05T16:20:15+00:00</updated>
    <author>
      <name>/u/Emotional-Evening-62</name>
      <uri>https://old.reddit.com/user/Emotional-Evening-62</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Been tinkering on this for a while — it’s a runtime orchestration layer that lets you:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Run AI models either on-device or in the cloud&lt;/li&gt; &lt;li&gt;Dynamically choose the best execution path (based on network, compute)&lt;/li&gt; &lt;li&gt;Plug in your own models (LLMs, vision, audio, whatever)&lt;/li&gt; &lt;li&gt;Built-in logging and fallback routing&lt;/li&gt; &lt;li&gt;Works with ONNX, TorchScript, and HTTP APIs (more coming)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Goal was to stop hardcoding execution logic and instead treat model routing like a smart decision system. Think traffic controller for AI workloads.&lt;/p&gt; &lt;p&gt;pip install oblix (mac only)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emotional-Evening-62"&gt; /u/Emotional-Evening-62 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js750i/i_built_an_ai_orchestrator_that_routes_between/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js750i/i_built_an_ai_orchestrator_that_routes_between/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1js750i/i_built_an_ai_orchestrator_that_routes_between/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T16:20:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jszy1p</id>
    <title>AI chatter with fans, OnlyFans chatter</title>
    <updated>2025-04-06T17:48:57+00:00</updated>
    <author>
      <name>/u/RiccardoPoli</name>
      <uri>https://old.reddit.com/user/RiccardoPoli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Context of my request:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I am the creator of an AI girl (with Stable Diffusion SDXL). Up until now, I have been manually chatting with fans on Fanvue.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I don't want to deal with answering fans, but I just want to create content, and do marketing. So I'm considering whether to pay a chatter, or whether to develop an AI LLama chatbot (I'm very interested in the second option).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I have little knowledge about LLamas, I don't know how to move, I'm asking here on this subreddit, because my request looks very specific and custom. I would like advices on what and how to do that. Specifically, I need an AI that is able to behave like the virtual girl with fans, so a fine-tuned model, which offers an online relationship experience. It must not be censored. It must be able to do normal conversations (like between 2 people in a relationship) but also roleplay, talk about sex, sexting, and other nsfw things.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Other specs:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It is very important to have a deep relationship with each fan, so the AI, as it writes to fans, must remember them, their preferences, their memories that they tell, their fears, their past experiences, and more. The AI's responses must be consistent and of quality with each individual fan. For example, if a fan likes to be called &amp;quot;pookie&amp;quot;, the AI ​​must remember to call the fan pookie. Chatgpt initially advised me to use json files, but I discovered that there is a system, with long-term and efficient memory, called RAG, but I have no idea how it works. Furthermore, the AI ​​must be able to send images to fans, and with context. For example, if a fan likes skirts, the AI ​​could send him a good morning &amp;quot;good morning pookie do you like this new skirt?&amp;quot; + attached image. The image is taken from a collection of pre-created images. Plus the AI should understand how to verify when fans send money, for example if a fan send money, the AI should recognize that and say thank you (thats just an example).&lt;/p&gt; &lt;p&gt;Another important thing is that the AI ​​must respond in the same way as I have responded to fans in the past, so its writing style must be the same as mine, with the same emotions and grammar, and emojis. And i honestly dont know how to achieve that, if i have to fine tune the model, or add to the model some txt or json file (the file contains a 3000 character text, explaining who is the AI girl, for example: im anastasia, coming from germany, im 23 years old, im studying at university, i love to ski and read horror books, i live with my mom, and more etc...)&lt;/p&gt; &lt;p&gt;My intention, is not to use this AI with Fanvue, but with telegram, simply becayse i gave a look to python Telegram API, and they look pretty simple to use.&lt;/p&gt; &lt;p&gt;I asked these things to chatgpt, and he suggested &lt;strong&gt;Mixtral 8x7b&lt;/strong&gt;, specifically the dolphin and other nsfw fine tuned model, + json/sql or &lt;strong&gt;RAG memory&lt;/strong&gt;, to memorize fans' info.&lt;/p&gt; &lt;p&gt;To resume, the AI must be unique, with a unique texting style, chat with multiple fans, remember stuff of each fans in long-term memory, send pictures, and understand when someone send money). The solution can be both a local LLama, or an external service, or both hybrid.&lt;/p&gt; &lt;p&gt;If anyone here, is into AI adult business, and AI girls, and understand my requests, feel free to exchange to contact me! :) &lt;/p&gt; &lt;p&gt;&lt;strong&gt;My computer power:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I have an RTX 3090 Ti, and 128GB of ram, i don't know if it's enough, but i can also rent online servers if needed with stronger gpus.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RiccardoPoli"&gt; /u/RiccardoPoli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jszy1p/ai_chatter_with_fans_onlyfans_chatter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jszy1p/ai_chatter_with_fans_onlyfans_chatter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jszy1p/ai_chatter_with_fans_onlyfans_chatter/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T17:48:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsbfp0</id>
    <title>What do you do with ollama?</title>
    <updated>2025-04-05T19:26:50+00:00</updated>
    <author>
      <name>/u/BlueTypes_</name>
      <uri>https://old.reddit.com/user/BlueTypes_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wondering what y'all do with machines and ollama.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlueTypes_"&gt; /u/BlueTypes_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsbfp0/what_do_you_do_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsbfp0/what_do_you_do_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsbfp0/what_do_you_do_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T19:26:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsqmmo</id>
    <title>Is it possible to make Ollama pretend to be ChatGPT?</title>
    <updated>2025-04-06T09:52:25+00:00</updated>
    <author>
      <name>/u/nahakubuilder</name>
      <uri>https://old.reddit.com/user/nahakubuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was thinking if there is possibility to reroute ChatGPT connections to Ollama.&lt;br /&gt; I have docker Ollama container, I have added Nginx to respond on `api.openai.com` + change my local DNS to point to it.&lt;br /&gt; I am coming to 2 issues.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;even with self signed certificate and added to linux the client is reporting it has invalid certificate. I think it is because of HTST, is it possible to make it to accept my self signed certificate for this public domain when is pointed locally?&lt;/li&gt; &lt;li&gt;I believe the API urls have different paths then ollama for openai. would be possible to change the paths, queries so it acts as openai? - with this one also I think is needed to mask the chatgpt models to some model what ollama supports too.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I am not sure if there is anything similar in work anywhere, as I Could not find it.&lt;/p&gt; &lt;p&gt;It would be nice if applications what force you to use public AI, would be possible to point to selfhosted ollama.&lt;/p&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;p&gt;For everyone responding. I am not looking for another GUI for ollama, I use Tabby.&lt;br /&gt; All I am looking for is to make Ollama ( Self hosted AI) to respond to queries what are meant for OpenAI.&lt;br /&gt; Reason for this is that many applications support only OpenAI, for example Bootstrap Studio.&lt;br /&gt; but if i can obfuscate ollama to act as open AI, all I need to make sure the &lt;a href="http://api.openai.com"&gt;api.openai.com&lt;/a&gt; is translated to Ollama instead of the real paid API.&lt;br /&gt; About cert, I already added the certificate to my PC and it still does not work.&lt;br /&gt; The calls are not in web browser but in apps, so certificated stored in local PC should be accepted.&lt;br /&gt; But as I Stated, the app complains about HSTS or something like that, or just says certificate invalid.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nahakubuilder"&gt; /u/nahakubuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsqmmo/is_it_possible_to_make_ollama_pretend_to_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsqmmo/is_it_possible_to_make_ollama_pretend_to_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsqmmo/is_it_possible_to_make_ollama_pretend_to_be/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T09:52:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1js2vzw</id>
    <title>Best uncensored ollama local model for erotic/porn prompt writing for image generation (sdxl, pony,...)</title>
    <updated>2025-04-05T12:59:34+00:00</updated>
    <author>
      <name>/u/Infinite-Stable-10</name>
      <uri>https://old.reddit.com/user/Infinite-Stable-10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I a trying to find a good local model, less than 15B, that can generate uncensored prompts (sdxl, pony) to feed my Comfy UI workflow for generation of erotic / porn images. Any recommendations? I used in past solar-pro 13B that work quite well fed with examples, but it is a little outdated. &lt;/p&gt; &lt;p&gt;If there is a good model that i 5B or 10B or around that is best so I can load it at the same time of my image generation model. &lt;/p&gt; &lt;p&gt;HELP &lt;/p&gt; &lt;p&gt;🙉&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Infinite-Stable-10"&gt; /u/Infinite-Stable-10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js2vzw/best_uncensored_ollama_local_model_for_eroticporn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js2vzw/best_uncensored_ollama_local_model_for_eroticporn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1js2vzw/best_uncensored_ollama_local_model_for_eroticporn/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T12:59:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1js9nkc</id>
    <title>I built an open source Computer-use framework that uses Local LLMs with Ollama</title>
    <updated>2025-04-05T18:09:51+00:00</updated>
    <author>
      <name>/u/sandropuppo</name>
      <uri>https://old.reddit.com/user/sandropuppo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1js9nkc/i_built_an_open_source_computeruse_framework_that/"&gt; &lt;img alt="I built an open source Computer-use framework that uses Local LLMs with Ollama" src="https://external-preview.redd.it/2AUUbeOoZ7agjDBXWHt091L224zZyg21bhTPn_iKBqY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=624a29b9725e7bb2871c8940c2c220a294d0d3e4" title="I built an open source Computer-use framework that uses Local LLMs with Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandropuppo"&gt; /u/sandropuppo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/trycua/cua"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1js9nkc/i_built_an_open_source_computeruse_framework_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1js9nkc/i_built_an_open_source_computeruse_framework_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T18:09:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsat5n</id>
    <title>llama 4</title>
    <updated>2025-04-05T18:59:58+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama4_omni/"&gt;https://www.llama.com/docs/model-cards-and-prompt-formats/llama4_omni/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When I can download it from Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsat5n/llama_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsat5n/llama_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsat5n/llama_4/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-05T18:59:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsu0sr</id>
    <title>Looking for a mistral 7B or equivalent that answer only in french</title>
    <updated>2025-04-06T13:23:34+00:00</updated>
    <author>
      <name>/u/maxorius13</name>
      <uri>https://old.reddit.com/user/maxorius13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;br /&gt; i found it pretty hard to ensure that mistral 7B would answer in french.&lt;br /&gt; Does any one know a model that will do the job ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxorius13"&gt; /u/maxorius13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsu0sr/looking_for_a_mistral_7b_or_equivalent_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsu0sr/looking_for_a_mistral_7b_or_equivalent_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsu0sr/looking_for_a_mistral_7b_or_equivalent_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T13:23:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt0gy0</id>
    <title>Project Title: SQL Chatbot with Ollama Integration</title>
    <updated>2025-04-06T18:11:08+00:00</updated>
    <author>
      <name>/u/ntnk1999</name>
      <uri>https://old.reddit.com/user/ntnk1999</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jt0gy0/project_title_sql_chatbot_with_ollama_integration/"&gt; &lt;img alt="Project Title: SQL Chatbot with Ollama Integration" src="https://external-preview.redd.it/8uTKcvd9x0EBLwRQcU7QYu6Rejuw6sRtgDwvDj_WT7M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eabfa2e060f7b47ddf9a6ca83e8c7e23cd5d7dd5" title="Project Title: SQL Chatbot with Ollama Integration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, can anybody tell me how to build this chatbot?&lt;/p&gt; &lt;p&gt;I don't have any coding experience—I'm just trying to build it for fun. I tried using Cursor and GitHub Copilot, but after some time, both started looping and generating incorrect code. They kept trying to fix it, but eventually, they seemed to forget what they were building.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ntnk1999"&gt; /u/ntnk1999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://g.co/gemini/share/1f18b52b44b2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jt0gy0/project_title_sql_chatbot_with_ollama_integration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jt0gy0/project_title_sql_chatbot_with_ollama_integration/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T18:11:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsvexq</id>
    <title>Janus pro 7b GGUF</title>
    <updated>2025-04-06T14:30:56+00:00</updated>
    <author>
      <name>/u/Inner-End7733</name>
      <uri>https://old.reddit.com/user/Inner-End7733</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Other posts seem to write the whole idea off in general without much thought. but theoretically you can run GGUF with Ollama, and there are GGUF versions of Janus pro on HF. Anyone done any experimetation with the applicable GGUF on HF? If so, how and to what degree of success? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inner-End7733"&gt; /u/Inner-End7733 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsvexq/janus_pro_7b_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsvexq/janus_pro_7b_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsvexq/janus_pro_7b_gguf/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T14:30:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsujp3</id>
    <title>M4 studio (M4 max 16 core CPU, 40 core GPU 128gb Ram) for LLM (local)</title>
    <updated>2025-04-06T13:49:53+00:00</updated>
    <author>
      <name>/u/Bahaal_1981</name>
      <uri>https://old.reddit.com/user/Bahaal_1981</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been experimenting with local LLMs (ollama) on an M1 Pro macbook (32GB ram) - so far OK but slowish. My desktop needs an upgrade and my use case is academic (assistance with programming with R / Shiny, perhaps some python, proofreading, generating new ideas / criticizing them, perhaps building a RAG to synthesise journal articles in .pdf). I am considering the M4 studio (M4 max, 16+40 - 128GB ram). Some of these tasks need to be done locally as in some use cases the data should not leave my device. I think the above config. should allow for comfortably running deepseek 70b, for example, next to other smaller models. (Other open source models?) And should be fairly futureproof (and allow to run some newer models locally (or quanizations). Any thoughts? Any suggestions for LLM models that would run well locally for the above tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bahaal_1981"&gt; /u/Bahaal_1981 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsujp3/m4_studio_m4_max_16_core_cpu_40_core_gpu_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsujp3/m4_studio_m4_max_16_core_cpu_40_core_gpu_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsujp3/m4_studio_m4_max_16_core_cpu_40_core_gpu_128gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T13:49:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jstwzw</id>
    <title>Smaller Gemma-3-27b QAT GGUF now available on Ollama</title>
    <updated>2025-04-06T13:18:01+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This smaller QAT gguf is faster than the one from Google, and it retains the same quality.&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama run JollyLlama/gemma-3-27b-it-q4_0_Small-QAT&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.ollama.com/JollyLlama/gemma-3-27b-it-q4_0_Small-QAT"&gt;https://www.ollama.com/JollyLlama/gemma-3-27b-it-q4_0_Small-QAT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ollama is having issues when importing Gemma3 ggufs; I have to edit the manifests manually to make the text part of this model work. The vision function doesn't work because Ollama doesn't support this projector.&lt;/p&gt; &lt;p&gt;Original creator of this smaller QAT gguf: &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jsq1so/smaller_gemma3_qat_versions_12b_in_8gb_and_27b_in/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jsq1so/smaller_gemma3_qat_versions_12b_in_8gb_and_27b_in/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jstwzw/smaller_gemma327b_qat_gguf_now_available_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jstwzw/smaller_gemma327b_qat_gguf_now_available_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jstwzw/smaller_gemma327b_qat_gguf_now_available_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T13:18:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsjhr5</id>
    <title>mistral-small:24b-3.1 finally on ollama!</title>
    <updated>2025-04-06T01:59:48+00:00</updated>
    <author>
      <name>/u/DominusVenturae</name>
      <uri>https://old.reddit.com/user/DominusVenturae</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jsjhr5/mistralsmall24b31_finally_on_ollama/"&gt; &lt;img alt="mistral-small:24b-3.1 finally on ollama!" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="mistral-small:24b-3.1 finally on ollama!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw the benchmark comparing it to Llama4 scout and remembered that when 3.0 24b came out it remained far down the list of &amp;quot;Newest Model&amp;quot; filter. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DominusVenturae"&gt; /u/DominusVenturae &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/library/mistral-small:24b-3.1-instruct-2503-q4_K_M"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsjhr5/mistralsmall24b31_finally_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsjhr5/mistralsmall24b31_finally_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T01:59:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt62o8</id>
    <title>Open-source Morphik MCP server for technical document search with Ollama client</title>
    <updated>2025-04-06T22:17:54+00:00</updated>
    <author>
      <name>/u/yes-no-maybe_idk</name>
      <uri>https://old.reddit.com/user/yes-no-maybe_idk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; - we built Morphik MCP to solve a common problem: finding specific information across scattered technical docs. We've experimented with GraphRAG, ColPali, contextual embeddings, and more. MCP emerged as the solution that unifies these approaches.&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multimodal search across text, diagrams, and videos&lt;/li&gt; &lt;li&gt;Natural language knowledge base management&lt;/li&gt; &lt;li&gt;Fully open-source with responsive support&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Integration with LibreChat and Open WebUI for Ollama users&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What sets MCP apart is its ability to return images (including diagrams) directly to the MCP client. Users have applied it to search over data ranging from blood tests to patents, and we use this daily with Cursor and Claude.&lt;/p&gt; &lt;p&gt;This makes Morphik MCP an excellent companion for your existing Ollama setup. &lt;/p&gt; &lt;p&gt;Give it a spin, and let us know what you think. &lt;/p&gt; &lt;p&gt;Link to our repo: &lt;a href="https://github.com/morphik-org/morphik-core"&gt;https://github.com/morphik-org/morphik-core&lt;/a&gt;, give it a star!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yes-no-maybe_idk"&gt; /u/yes-no-maybe_idk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jt62o8/opensource_morphik_mcp_server_for_technical/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jt62o8/opensource_morphik_mcp_server_for_technical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jt62o8/opensource_morphik_mcp_server_for_technical/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T22:17:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtau8m</id>
    <title>Help picking model</title>
    <updated>2025-04-07T02:26:14+00:00</updated>
    <author>
      <name>/u/Leather-Equipment256</name>
      <uri>https://old.reddit.com/user/Leather-Equipment256</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im using ollama to host a LLM that I use inside of obsidian to quiz me on notes and ask questions. Every model ive tried can’t really quiz me at all. What should I use my ollama is on a Rx 6750 xt 12gb vram and 5600+32gb@3800mhz ram. Ik ollama doesn’t have support for my gpu but im using a forked version that allows gpu acceleration while I wait for official support. So what model to use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leather-Equipment256"&gt; /u/Leather-Equipment256 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtau8m/help_picking_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jtau8m/help_picking_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jtau8m/help_picking_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T02:26:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt9o0s</id>
    <title>Looking for Collaborators to port and build an agent like manus in smolagents</title>
    <updated>2025-04-07T01:20:58+00:00</updated>
    <author>
      <name>/u/Character-Ad5001</name>
      <uri>https://old.reddit.com/user/Character-Ad5001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on this project for a while now and recently decided to build a UI for it. However, working with langchain and langgraph has been more of a challenge than expected — I’ve had to write a lot of custom solutions for vector stores, semantic chunking, persisting LangGraph with Drizzle, and more. After a lot of trial and error, I realized the simplest and most reliable way to run everything locally (without relying on external SaaS) is to stick with Python, using SQLite as the primary storage layer. While LangChain/LangGraph's JavaScript ecosystem does have solid integrations, they often tie into cloud services, which goes against the local-first goal of this project. I've experimented with almost every agentic library out there, including the newer lightweight ones, and in terms of support, stability, and future potential, smolagents seems like the best fit going forward. The vision for this project is to combine the best parts of various open source tools. Surprisingly, no current open source chat app implements full revision history — tools like LM Studio offer branching, but that’s a different UX model. Revision history needs a parent-child tree model, whereas branching is more like checkpointing (copy-paste). I'm also planning to integrate features like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SearchXNG in-chat search&lt;/li&gt; &lt;li&gt;CAPTCHA-free scraping via Playwright&lt;/li&gt; &lt;li&gt;NotebookLM-inspired source sidebar&lt;/li&gt; &lt;li&gt;Claude-style project handling&lt;/li&gt; &lt;li&gt;Toggleable manus type agent (like toggling on/off search/deepsearch from openai/grok)&lt;/li&gt; &lt;li&gt;And much more — thanks to incredible tools like zep, crawlforai, browser use, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to bring on some collaborators to help push this forward. If you're into LLMs, agentic workflows, and building local-first tools, hit me up! &lt;a href="https://github.com/mantrakp04/manusmcp"&gt;https://github.com/mantrakp04/manusmcp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Character-Ad5001"&gt; /u/Character-Ad5001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jt9o0s/looking_for_collaborators_to_port_and_build_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jt9o0s/looking_for_collaborators_to_port_and_build_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jt9o0s/looking_for_collaborators_to_port_and_build_an/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-07T01:20:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsljwk</id>
    <title>Github Copilot now supports Ollama and OpenRouter Models 🎉</title>
    <updated>2025-04-06T04:01:00+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jsljwk/github_copilot_now_supports_ollama_and_openrouter/"&gt; &lt;img alt="Github Copilot now supports Ollama and OpenRouter Models 🎉" src="https://b.thumbs.redditmedia.com/w6tm8jDvXneFlfBCf3lkx82SjxvqcNt_DTfDjlvvfmU.jpg" title="Github Copilot now supports Ollama and OpenRouter Models 🎉" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Huge W for programmers (and vibe coders) in the Local LLM community. Github Copilot now supports a much wider range of models from Ollama, OpenRouter, Gemini, and others.&lt;/p&gt; &lt;p&gt;To add your own models, click on &amp;quot;Manage Models&amp;quot; in the prompt field.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jsljwk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jsljwk/github_copilot_now_supports_ollama_and_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jsljwk/github_copilot_now_supports_ollama_and_openrouter/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T04:01:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt3ndd</id>
    <title>How do small models contain so much information?</title>
    <updated>2025-04-06T20:27:33+00:00</updated>
    <author>
      <name>/u/BallPythonTech</name>
      <uri>https://old.reddit.com/user/BallPythonTech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am amazed at how much data small models can re-create. For example, Gemma3:4b, I ask it to list the books of the Old Testament. It leaves some out listing only 35. &lt;/p&gt; &lt;p&gt;But how does it even store that? &lt;/p&gt; &lt;p&gt;List the books by Edgar Allen Poe, it gets most of them, same for Dr Seuss. Published years are often wrong but still. &lt;/p&gt; &lt;p&gt;List publications by Albert Einstein - mostly correct.&lt;/p&gt; &lt;p&gt;List elementary particles - it lists half of them, 17&lt;/p&gt; &lt;p&gt;So how in 3GB is it able to store so much information or is Ollama going out to the internet to get more data?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BallPythonTech"&gt; /u/BallPythonTech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jt3ndd/how_do_small_models_contain_so_much_information/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jt3ndd/how_do_small_models_contain_so_much_information/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jt3ndd/how_do_small_models_contain_so_much_information/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-06T20:27:33+00:00</published>
  </entry>
</feed>
