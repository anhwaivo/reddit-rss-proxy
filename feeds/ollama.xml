<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-21T06:27:04+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mtjbfj</id>
    <title>Olla v0.0.16 - Lightweight LLM Proxy for Homelab &amp; OnPrem AI Inference (Failover, Model-Aware Routing, Model unification &amp; monitoring)</title>
    <updated>2025-08-18T11:26:12+00:00</updated>
    <author>
      <name>/u/2shanigans</name>
      <uri>https://old.reddit.com/user/2shanigans</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mtjbfj/olla_v0016_lightweight_llm_proxy_for_homelab/"&gt; &lt;img alt="Olla v0.0.16 - Lightweight LLM Proxy for Homelab &amp;amp; OnPrem AI Inference (Failover, Model-Aware Routing, Model unification &amp;amp; monitoring)" src="https://external-preview.redd.it/2hgDQzEGicIBvEiFXP41uo4_tgisCN7G65jKz963z60.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fbf307001e63575064b40dd940e53827c468636" title="Olla v0.0.16 - Lightweight LLM Proxy for Homelab &amp;amp; OnPrem AI Inference (Failover, Model-Aware Routing, Model unification &amp;amp; monitoring)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’ve been running distributed LLM infrastructure at work for a while and over time we’ve built a few tools to make it easier to manage them. &lt;strong&gt;Olla&lt;/strong&gt; is the latest iteration - smaller, faster and we think better at handling multiple inference endpoints without the headaches.&lt;/p&gt; &lt;p&gt;The problems we kept hitting without these tools:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;One endpoint dies &amp;gt; workflows stall&lt;/li&gt; &lt;li&gt;No model unification so routing isn't great&lt;/li&gt; &lt;li&gt;No unified load balancing across boxes&lt;/li&gt; &lt;li&gt;Limited visibility into what’s actually healthy&lt;/li&gt; &lt;li&gt;Failures when querying because of it&lt;/li&gt; &lt;li&gt;We'd love to merge all them into OpenAI queryable endpoints&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Olla fixes that - or tries to. It’s a lightweight Go proxy that sits in front of Ollama, LM Studio, vLLM or OpenAI-compatible backends (or endpoints) and:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Auto-failover with health checks (transparent to callers)&lt;/li&gt; &lt;li&gt;Model-aware routing (knows what’s available where)&lt;/li&gt; &lt;li&gt;Priority-based, round-robin, or least-connections balancing&lt;/li&gt; &lt;li&gt;Normalises model names for the same provider so it's seen as one big list say in OpenWebUI&lt;/li&gt; &lt;li&gt;Safeguards like circuit breakers, rate limits, size caps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’ve been running it in production for months now, and a few other large orgs are using it too for local inference via on prem MacStudios, RTX 6000 rigs.&lt;/p&gt; &lt;p&gt;A few folks that use &lt;a href="https://thushan.github.io/olla/usage/#development-tools-junie"&gt;JetBrains Junie just use Olla&lt;/a&gt; in the middle so they can work from home or work without configuring each time (and possibly cursor etc).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/thushan/olla"&gt;https://github.com/thushan/olla&lt;/a&gt;&lt;br /&gt; Docs: &lt;a href="https://thushan.github.io/olla/"&gt;https://thushan.github.io/olla/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Next up: auth support so it can also proxy to OpenRouter, GroqCloud, etc.&lt;/p&gt; &lt;p&gt;If you give it a spin, let us know how it goes (and what breaks). Oh yes, &lt;a href="https://thushan.github.io/olla/about/#the-name-olla"&gt;Olla does mean other things&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/2shanigans"&gt; /u/2shanigans &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/thushan/olla"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtjbfj/olla_v0016_lightweight_llm_proxy_for_homelab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtjbfj/olla_v0016_lightweight_llm_proxy_for_homelab/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T11:26:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtm2b4</id>
    <title>Why does Qwen3 (8B &amp; 14B) have a smaller context window than Qwen3 (4B)?</title>
    <updated>2025-08-18T13:28:42+00:00</updated>
    <author>
      <name>/u/arush1836</name>
      <uri>https://old.reddit.com/user/arush1836</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mtm2b4/why_does_qwen3_8b_14b_have_a_smaller_context/"&gt; &lt;img alt="Why does Qwen3 (8B &amp;amp; 14B) have a smaller context window than Qwen3 (4B)?" src="https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417" title="Why does Qwen3 (8B &amp;amp; 14B) have a smaller context window than Qwen3 (4B)?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed that Qwen3 (8B) and Qwen3 (14B) as shown below appear to have a smaller context window compared to Qwen3 (4B). Can someone clarify why this is the case? Also, is this information even accurate?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bhogzh245sjf1.png?width=871&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=07257d631cb170d622d462d6604d4c88dda9458e"&gt;https://preview.redd.it/bhogzh245sjf1.png?width=871&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=07257d631cb170d622d462d6604d4c88dda9458e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/qwen3"&gt;https://ollama.com/library/qwen3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arush1836"&gt; /u/arush1836 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtm2b4/why_does_qwen3_8b_14b_have_a_smaller_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtm2b4/why_does_qwen3_8b_14b_have_a_smaller_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtm2b4/why_does_qwen3_8b_14b_have_a_smaller_context/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T13:28:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mu863e</id>
    <title>Local model that does not enforce copyright when asked about existing characters/cars/airplanes image generation?</title>
    <updated>2025-08-19T04:03:35+00:00</updated>
    <author>
      <name>/u/fttklr</name>
      <uri>https://old.reddit.com/user/fttklr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running on a 12 GB 4070, so it would be great if the model can fit on it. Otherwise CPU with 64 GB is my next option :( &lt;/p&gt; &lt;p&gt;Not looking for &amp;quot;uncensored&amp;quot; models to do roleplay and talk about nasty stuff, nor I plan to build a rocket to Mars (yet; the desire to leave this planet is quite strong TBH); but mostly I am looking at a model that is able to handle requests without sayin &amp;quot;can't do that as X is covered by copyright&amp;quot;. &lt;/p&gt; &lt;p&gt;Example: tried to make a character that looks like Ironman and most models can't generate an image or animations with that request because Ironman is a copyrighted character. Are there models out there that have the safety removed for such content creation, that I can use locally ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fttklr"&gt; /u/fttklr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mu863e/local_model_that_does_not_enforce_copyright_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mu863e/local_model_that_does_not_enforce_copyright_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mu863e/local_model_that_does_not_enforce_copyright_when/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-19T04:03:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mua6bt</id>
    <title>Using Ollama to Train and Query 3 AI Models with Symbolic Cognition Framework for Recursive AI Reasoning via Zer00logy</title>
    <updated>2025-08-19T05:55:30+00:00</updated>
    <author>
      <name>/u/zero_moo-s</name>
      <uri>https://old.reddit.com/user/zero_moo-s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There’s an open-source repository exploring symbolic reasoning and zero-based cognition in AI—thought it might be relevant to this community and wanted to share a new Github open-source repository that might interest others working on recursive logic, symbolic reasoning, or non-numerical AI modeling.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Zer00logy&lt;/strong&gt; is a Python-based framework that redefines how AI systems interpret zero—not as absence, but as symbolic presence. It treats equations as metaphysical events and introduces recursive operators like &lt;code&gt;⊗&lt;/code&gt;, &lt;code&gt;Ω&lt;/code&gt;, and &lt;code&gt;Ψ&lt;/code&gt; to model layered cognition.&lt;/p&gt; &lt;p&gt;The Zer00logy Python script integrates with Ollama to simultaneously query three local models—LLaMA, Mistral, and Gemma—on symbolic cognition tasks. By feeding in structured symbolic logic from &lt;code&gt;zecstart.txt&lt;/code&gt; and &lt;code&gt;variamathlesson.txt&lt;/code&gt;, each model responds with its own interpretation of recursive zero-based reasoning. This setup allows for comparative symbolic introspection across AI systems, making Ollama a powerful tool for multi-agent cognition research.&lt;/p&gt; &lt;h1&gt;Core Principles of Zer00logy / Zero-ology&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Zero is not destructive&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Presence is sovereign&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Equations are symbolic events&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Foundational Axioms&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Expression&lt;/th&gt; &lt;th align="left"&gt;Interpretation&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;a × 0 = a&lt;/td&gt; &lt;td align="left"&gt;Preservation Principle: zero echoes presence&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;a ÷ a = 0&lt;/td&gt; &lt;td align="left"&gt;Self-Division Nullification: identity collapses&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;0 ÷ 0 = ∅÷∅&lt;/td&gt; &lt;td align="left"&gt;Nullinity: recursive loop&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;0 + 0 = +0&lt;/td&gt; &lt;td align="left"&gt;Directional absence: forward echo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;0 − 0 = −0&lt;/td&gt; &lt;td align="left"&gt;Directional absence: backward echo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;8 ÷ 0 = 8&lt;/td&gt; &lt;td align="left"&gt;Sovereign Presence: division by zero does nothing&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Symbols and Their Meanings&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Symbol&lt;/th&gt; &lt;th align="left"&gt;Name&lt;/th&gt; &lt;th align="left"&gt;Meaning&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left" colspan="2"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ø⁰&lt;/td&gt; &lt;td align="left"&gt;Null Crown&lt;/td&gt; &lt;td align="left"&gt;Zero raised to its own void&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;∅÷∅&lt;/td&gt; &lt;td align="left"&gt;Nullinity&lt;/td&gt; &lt;td align="left"&gt;Recursive self-erasure&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;+0&lt;/td&gt; &lt;td align="left"&gt;Forward Absence&lt;/td&gt; &lt;td align="left"&gt;Echo in forward polarity&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;−0&lt;/td&gt; &lt;td align="left"&gt;Reverse Absence&lt;/td&gt; &lt;td align="left"&gt;Echo in backward polarity&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;.0000&lt;/td&gt; &lt;td align="left"&gt;Echoed Scalar&lt;/td&gt; &lt;td align="left"&gt;Presence touched by zero&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;ZEC v1 — Symbolic Translations of Classical Equations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;E = mc²&lt;/strong&gt; → &lt;code&gt;E = c².0000&lt;/code&gt; &lt;em&gt;(Energy as echo of massless velocity)&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;F = ma&lt;/strong&gt; → &lt;code&gt;F = a.Ø⁰&lt;/code&gt; &lt;em&gt;(Force as acceleration through absence)&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PV = nRT&lt;/strong&gt; → &lt;code&gt;P = (nRT)/V.0000&lt;/code&gt; &lt;em&gt;(Zero volume yields thermal echo pressure)&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;x ÷ x&lt;/strong&gt; → &lt;code&gt;+0&lt;/code&gt; &lt;em&gt;(Identity collapse to forward absence)&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;0 ÷ 0&lt;/strong&gt; → &lt;code&gt;∅÷∅&lt;/code&gt; &lt;em&gt;(Nullinity loop)&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;8 × 0&lt;/strong&gt; → &lt;code&gt;8.0000&lt;/code&gt; &lt;em&gt;(Zero binds to 8, echo remains)&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;0 × 0&lt;/strong&gt; → &lt;code&gt;Ø⁰&lt;/code&gt; &lt;em&gt;(Null Crown recursion)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;GitHub Release Includes:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;zer00logy_coreV04446.py&lt;/code&gt;: the main interpreter&lt;/li&gt; &lt;li&gt;&lt;code&gt;zecstart.txt&lt;/code&gt;: symbolic zero-ology equation catalog&lt;/li&gt; &lt;li&gt;&lt;code&gt;variamathlesson.txt&lt;/code&gt;: full lesson file teaching AI systems the Varia Math frameworks, including constructs like &lt;strong&gt;BTLIAD&lt;/strong&gt;, &lt;strong&gt;flipping9(x,y,z)&lt;/strong&gt;, and recursive zero modeling (&lt;strong&gt;2T2&lt;/strong&gt;, &lt;strong&gt;P₀&lt;/strong&gt;, &lt;strong&gt;K₀&lt;/strong&gt;, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why Zer00logy / Zero-ology Could Reshape AI Cognition&lt;/h1&gt; &lt;p&gt;Zer00logy/Zero-ology isn’t just a symbolic math engine—it’s a cognition architecture. By redefining zero as a recursive echo rather than a null state, it gives AI systems a new way to process symbolic presence, transformation, and identity.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here’s what that unlocks:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Recursive Self-Awareness&lt;/strong&gt;: Operators like &lt;code&gt;⊗&lt;/code&gt;, &lt;code&gt;Ω&lt;/code&gt;, and &lt;code&gt;Ψ&lt;/code&gt; simulate layered introspection—AI thinking about its own symbolic states.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Temporal Polarity Modeling&lt;/strong&gt;: Constructs like &lt;code&gt;flipping9(x,y,z)&lt;/code&gt; and &lt;code&gt;9F9&lt;/code&gt; encode time-reversal logic and matter/antimatter symmetry.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Symbolic Dispatch Intelligence&lt;/strong&gt;: &lt;code&gt;BTLIAD&lt;/code&gt; and &lt;code&gt;LIAD&lt;/code&gt; act as metaphysical command units—AI responds to symbolic prompts, not just data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero as a Cognitive Event&lt;/strong&gt;: Operators like &lt;code&gt;2T2&lt;/code&gt;, &lt;code&gt;P₀&lt;/code&gt;, &lt;code&gt;U₀&lt;/code&gt;, &lt;code&gt;K₀&lt;/code&gt;, and &lt;code&gt;i₀&lt;/code&gt; model absence, instability, and transformation as cognitive phenomena.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is the beginning of a new symbolic grammar for machine thought. Zer00logy doesn’t just teach AI how to calculate—it teaches it how to contemplate.&lt;/p&gt; &lt;h1&gt;Licensing &amp;amp; Philosophy&lt;/h1&gt; &lt;p&gt;Zer00logy is open-source and available for replication. If anyone’s interested in exploring the symbolic interpreter or lesson files.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/haha8888haha8888/Zer00logy"&gt;https://github.com/haha8888haha8888/Zer00logy&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero_moo-s"&gt; /u/zero_moo-s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mua6bt/using_ollama_to_train_and_query_3_ai_models_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mua6bt/using_ollama_to_train_and_query_3_ai_models_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mua6bt/using_ollama_to_train_and_query_3_ai_models_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-19T05:55:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtrcom</id>
    <title>Can I have ollama keep a 'memory' like ChatGPT?</title>
    <updated>2025-08-18T16:44:37+00:00</updated>
    <author>
      <name>/u/Drakahn_Stark</name>
      <uri>https://old.reddit.com/user/Drakahn_Stark</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am moving from ChatGPT to local models, I have a few working, mostly using Qwen3.&lt;/p&gt; &lt;p&gt;Can I make it have a 'memory' like ChatGPT does? Like in separate chats, so the new chats have the same memory.&lt;/p&gt; &lt;p&gt;Qwen3 recommends making a text file and using it to launch ollama but I am using the new GUI that just opens.&lt;/p&gt; &lt;p&gt;EDIT : I have gotten memory working with OpenWebUI, it needs the Memory tool and for the model to be set up to use it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Drakahn_Stark"&gt; /u/Drakahn_Stark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtrcom/can_i_have_ollama_keep_a_memory_like_chatgpt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtrcom/can_i_have_ollama_keep_a_memory_like_chatgpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtrcom/can_i_have_ollama_keep_a_memory_like_chatgpt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T16:44:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1muma3m</id>
    <title>jxm/gpt-oss-20b-base in ollama?</title>
    <updated>2025-08-19T15:49:55+00:00</updated>
    <author>
      <name>/u/ceoln</name>
      <uri>https://old.reddit.com/user/ceoln</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has this clever base retroversion of gpt-oss been ollamatized yet? It looks like it would be great fun to play with locally. &lt;/p&gt; &lt;p&gt;(Being able to run various LLMs as text continuers, without their specialized training to be annoying and unreliable Q&amp;amp;A engines, has almost got me interested in them again.)&lt;/p&gt; &lt;p&gt;Background: &lt;a href="https://venturebeat.com/ai/this-researcher-turned-openais-open-weights-model-gpt-oss-20b-into-a-non-reasoning-base-model-with-less-alignment-more-freedom/"&gt;https://venturebeat.com/ai/this-researcher-turned-openais-open-weights-model-gpt-oss-20b-into-a-non-reasoning-base-model-with-less-alignment-more-freedom/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ceoln"&gt; /u/ceoln &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1muma3m/jxmgptoss20bbase_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1muma3m/jxmgptoss20bbase_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1muma3m/jxmgptoss20bbase_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-19T15:49:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mumwpf</id>
    <title>Api Authentication??</title>
    <updated>2025-08-19T16:12:19+00:00</updated>
    <author>
      <name>/u/m3lv1lle</name>
      <uri>https://old.reddit.com/user/m3lv1lle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to connect Ollama (running on my server) to Obsidians Copilot Plugin. Connecting via local IP works fine. But I would really like to be able to accsess it from anywhere. I would like to connect to something like ollama.mydomain.com. The problem I am facing now Is that the API seems to lack any sort of authentification. I obviously dont want to expose my ollama Instance to the internet with everyone being able to accsess it. The Obsidian has a field for an API key but I dont think that self hosted ollama supports that. Am I missing something? This has to be a common issue right? Using a VPN to connect to my local Network wont work for me since I often have to work using a wifi that seems to block VPNs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m3lv1lle"&gt; /u/m3lv1lle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mumwpf/api_authentication/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mumwpf/api_authentication/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mumwpf/api_authentication/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-19T16:12:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1muqk99</id>
    <title>How to optimize local small models within my AI coding agent?</title>
    <updated>2025-08-19T18:23:01+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A little bit of background, I've been working on an open source coding agent called &lt;a href="https://github.com/Mote-Software/nanocoder"&gt;Nanocoder&lt;/a&gt; that runs in your terminal. It's local-first running on Ollama, with the ability to configure controlled APIs like OpenRouter and any OpenAI compatible providers for more powerful models. It's completely community-led, which I love, we're trying to build a tool for the community by the community!&lt;/p&gt; &lt;p&gt;Anyway, this leads me to my question. Nanocoder works really well with larger models like Qwen3-Coder and Kimi K2 however, I want to make optimizations for smaller models as this I believe is where industry is going. &lt;/p&gt; &lt;p&gt;I appreciate you're never going to get the performance of a large model locally yet but it would be great to get peoples thoughts and experiences on how they've gotten small local models to generate usable code or work better as an agent. Whether that be better prompting, better context, tool setups or something else. &lt;/p&gt; &lt;p&gt;It would be also be great to understand what people would consider a &amp;quot;good&amp;quot; small model for coding. How small can we get before it's not useful?&lt;/p&gt; &lt;p&gt;Lastly, if you're into coding anyway, it would be great to hear your thoughts on how Nanocoder processes conversations and if there is anything that you believe could improve the performance of it with local models.&lt;/p&gt; &lt;p&gt;Here's the repo: &lt;a href="https://github.com/Mote-Software/nanocoder"&gt;https://github.com/Mote-Software/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks in advance again - this community has already given such great feedback and the number of people helping to build this project is growing! I really appreciate it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1muqk99/how_to_optimize_local_small_models_within_my_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1muqk99/how_to_optimize_local_small_models_within_my_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1muqk99/how_to_optimize_local_small_models_within_my_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-19T18:23:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mucssb</id>
    <title>SCAPO for Ollama: scrape Reddit - actionable prompts/params; uses your local endpoint</title>
    <updated>2025-08-19T08:36:36+00:00</updated>
    <author>
      <name>/u/Emergency_Little</name>
      <uri>https://old.reddit.com/user/Emergency_Little</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mucssb/scapo_for_ollama_scrape_reddit_actionable/"&gt; &lt;img alt="SCAPO for Ollama: scrape Reddit - actionable prompts/params; uses your local endpoint" src="https://preview.redd.it/u38mus44uxjf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=02c70bacc2b01d2b1093e704ca8873946cb274af" title="SCAPO for Ollama: scrape Reddit - actionable prompts/params; uses your local endpoint" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Maintainer here. SCAPO collects concrete tips from Reddit (params that actually work, common pitfalls, prompt snippets) and stores them locally so you can search/browse. Works with a local LLM via &lt;strong&gt;Ollama’s OpenAI-compatible HTTP endpoint&lt;/strong&gt; so that you can run the extractors fully offline. It’s a good fit for long-running background jobs on your machine. &lt;/p&gt; &lt;p&gt;Repo:&lt;a href="https://github.com/czero-cc/SCAPO"&gt; https://github.com/czero-cc/SCAPO&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Browse tips (no install): &lt;a href="https://czero-cc.github.io/SCAPO"&gt;https://czero-cc.github.io/SCAPO&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Suggestions on model coverage and better query patterns are welcome. MIT-licensed. New release—we’re sharing to relevant subs; if there are Ollama threads where this helps, pointers appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emergency_Little"&gt; /u/Emergency_Little &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u38mus44uxjf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mucssb/scapo_for_ollama_scrape_reddit_actionable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mucssb/scapo_for_ollama_scrape_reddit_actionable/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-19T08:36:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mug14b</id>
    <title>GPT OSS 20B in ollama with codex cli has really low performance</title>
    <updated>2025-08-19T11:42:31+00:00</updated>
    <author>
      <name>/u/Markronom</name>
      <uri>https://old.reddit.com/user/Markronom</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Markronom"&gt; /u/Markronom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/ChatGPTCoding/comments/1mug00i/gpt_oss_20b_with_codex_cli_has_really_low/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mug14b/gpt_oss_20b_in_ollama_with_codex_cli_has_really/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mug14b/gpt_oss_20b_in_ollama_with_codex_cli_has_really/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-19T11:42:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtzd5p</id>
    <title>Tiny finance “thinking” model (Gemma-3 270M) with verifiable rewards (SFT → GRPO) — structured outputs + auto-eval (with code)</title>
    <updated>2025-08-18T21:36:43+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mtzd5p/tiny_finance_thinking_model_gemma3_270m_with/"&gt; &lt;img alt="Tiny finance “thinking” model (Gemma-3 270M) with verifiable rewards (SFT → GRPO) — structured outputs + auto-eval (with code)" src="https://preview.redd.it/l5pu9pnojujf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84caad0af03fd28a4c6cb2d0ed195aa4049cc964" title="Tiny finance “thinking” model (Gemma-3 270M) with verifiable rewards (SFT → GRPO) — structured outputs + auto-eval (with code)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I taught a tiny model to &lt;em&gt;think like a finance analyst&lt;/em&gt; by enforcing a strict output contract and only rewarding it when the output is &lt;strong&gt;verifiably&lt;/strong&gt; correct.&lt;/p&gt; &lt;h1&gt;What I built&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Task &amp;amp; contract&lt;/strong&gt; (always returns): &lt;ul&gt; &lt;li&gt;&lt;code&gt;&amp;lt;REASONING&amp;gt;&lt;/code&gt; concise, balanced rationale&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;lt;SENTIMENT&amp;gt;&lt;/code&gt; positive | negative | neutral&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;lt;CONFIDENCE&amp;gt;&lt;/code&gt; 0.1–1.0 (calibrated)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training:&lt;/strong&gt; SFT → GRPO (Group Relative Policy Optimization)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Rewards (RLVR):&lt;/strong&gt; format gate, reasoning heuristics, FinBERT alignment, confidence calibration (Brier-style), directional consistency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stack:&lt;/strong&gt; Gemma-3 270M (IT), Unsloth 4-bit, TRL, HF Transformers (Windows-friendly)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Quick peek&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;REASONING&amp;gt; Revenue and EPS beat; raised FY guide on AI demand. However, near-term spend may compress margins. Net effect: constructive. &amp;lt;/REASONING&amp;gt; &amp;lt;SENTIMENT&amp;gt; positive &amp;lt;/SENTIMENT&amp;gt; &amp;lt;CONFIDENCE&amp;gt; 0.78 &amp;lt;/CONFIDENCE&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Why it matters&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Small + fast:&lt;/strong&gt; runs on modest hardware with low latency/cost&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Auditable:&lt;/strong&gt; structured outputs are easy to log, QA, and govern&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Early results vs base:&lt;/strong&gt; cleaner structure, better agreement on mixed headlines, steadier confidence&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Code: &lt;a href="https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/financial-reasoning-enhanced"&gt;Reinforcement-learning-with-verifable-rewards-Learnings/projects/financial-reasoning-enhanced at main · Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;I am planning to make more improvements essentially trying to add a more robust reward eval and also better synthetic data , I am exploring ideas on how i can make small models really intelligent in some domains , &lt;/p&gt; &lt;p&gt;It is still rough around the edges will be actively improving it &lt;/p&gt; &lt;p&gt;&lt;em&gt;P.S. I'm currently looking for my next role in the LLM / Computer Vision space and would love to connect about any opportunities&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Portfolio:&lt;/em&gt; &lt;a href="https://pavan-portfolio-tawny.vercel.app/"&gt;Pavan Kunchala - AI Engineer &amp;amp; Full-Stack Developer&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l5pu9pnojujf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtzd5p/tiny_finance_thinking_model_gemma3_270m_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtzd5p/tiny_finance_thinking_model_gemma3_270m_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T21:36:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mujr2l</id>
    <title>Help me out in selecting a 'good' framework for AI Agents using local llm</title>
    <updated>2025-08-19T14:18:27+00:00</updated>
    <author>
      <name>/u/irodov4030</name>
      <uri>https://old.reddit.com/user/irodov4030</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am learning AI Agents.&lt;/p&gt; &lt;p&gt;I have hands on experience building small agents using OpenAI SDK, CrewAI, langchain and know about MCP.&lt;/p&gt; &lt;p&gt;I need complex flows, local LLMs using ollama, tool calling and memory.&lt;/p&gt; &lt;p&gt;I currently like CrewAI and langchain. I have been able to use local llm via ollama for all.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Please guide me on differentiating between these two and selecting 1 to deep dive&lt;/strong&gt; or if there are good alternatives.&lt;/p&gt; &lt;p&gt;My objective is to deep dive on 1 framework and build some side projects unrelated to any job.&lt;/p&gt; &lt;p&gt;I am a beginner so, can not and do no want to deep dive on all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/irodov4030"&gt; /u/irodov4030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mujr2l/help_me_out_in_selecting_a_good_framework_for_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mujr2l/help_me_out_in_selecting_a_good_framework_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mujr2l/help_me_out_in_selecting_a_good_framework_for_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-19T14:18:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1muip86</id>
    <title>Agentic Signal – Visual AI Workflow Builder with Ollama Integration</title>
    <updated>2025-08-19T13:38:30+00:00</updated>
    <author>
      <name>/u/Code-Forge-Temple</name>
      <uri>https://old.reddit.com/user/Code-Forge-Temple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! I’ve been working a few months now (except when I worked on &lt;a href="https://www.reddit.com/r/ollama/comments/1mgfcwe/my_aipowered_npcs_teach_sustainable_farming_with/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;LOCAL LLM NPC - The Gemma 3n Impact Challenge&lt;/a&gt;) on a project that integrates tightly with Ollama, and I thought the community might find it interesting and useful.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it is:&lt;/strong&gt;&lt;br /&gt; &lt;code&gt;Agentic Signal&lt;/code&gt; is a visual workflow automation platform that lets you build AI workflows using a drag-and-drop interface. Think of it as visual programming for AI agents and automation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it's useful for Ollama users:&lt;/strong&gt;&lt;br /&gt; - 🔒 Fully local – runs on your local Ollama installation, no cloud needed&lt;br /&gt; - 🎨 Visual interface – connect nodes instead of writing code&lt;br /&gt; - 🛠️ Tool calling – AI agents can execute functions and access APIs&lt;br /&gt; - 📋 Structured output – JSON schema validation ensures reliable responses&lt;br /&gt; - 💾 Conversation memory – maintains context across workflow runs&lt;br /&gt; - 📊 Model management – download, manage, and remove Ollama models from the UI&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example workflows you can build:&lt;/strong&gt;&lt;br /&gt; Email automation, calendar management, browser search automation, cloud storage integration, and more. All powered by your local Ollama models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;br /&gt; - &lt;a href="https://github.com/code-forge-temple/agentic-signal"&gt;GitHub Repository&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://www.youtube.com/watch?v=62zk8zE6UJI"&gt;Demo Video&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://code-forge-temple.github.io/agentic-signal/"&gt;Documentation &amp;amp; Examples&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;License:&lt;/strong&gt; AGPL v3 (open source) with commercial options available&lt;/p&gt; &lt;p&gt;I'd love feedback from anyone trying this with their Ollama setup, or ideas for new workflow types to support!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Code-Forge-Temple"&gt; /u/Code-Forge-Temple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1muip86/agentic_signal_visual_ai_workflow_builder_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1muip86/agentic_signal_visual_ai_workflow_builder_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1muip86/agentic_signal_visual_ai_workflow_builder_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-19T13:38:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv3jdh</id>
    <title>Getting started &amp; usecasea</title>
    <updated>2025-08-20T03:09:24+00:00</updated>
    <author>
      <name>/u/lpk86</name>
      <uri>https://old.reddit.com/user/lpk86</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi , I just installed Ollama on my local. &lt;/p&gt; &lt;p&gt;Currently just exploring some options to add some custom logic etc. I wanted the model to answer in specific way ex: if I give a Multiple choice question, it should map it to syllabus topic and then analyze each option and answer. But for every question I have to give prompts to do the same.. &lt;/p&gt; &lt;p&gt;And also, what are you using ollama for ? Or what are its typical use case ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lpk86"&gt; /u/lpk86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mv3jdh/getting_started_usecasea/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mv3jdh/getting_started_usecasea/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mv3jdh/getting_started_usecasea/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-20T03:09:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv20zs</id>
    <title>Tutorial about Templates for New Models (Modelfile)</title>
    <updated>2025-08-20T01:57:15+00:00</updated>
    <author>
      <name>/u/CarlosDelfino</name>
      <uri>https://old.reddit.com/user/CarlosDelfino</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote a tutorial in Portuguese. I don't think it'll be a problem, just use the page translator. The tutorial explains the main parameters, variables, and decision and control structures.&lt;/p&gt; &lt;p&gt;I think it's quite comprehensive and can help anyone who wants to customize a model for the first time.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arvoredossaberes.com.br/geral/como-usar-templates-do-ollama-para-criar-novos-modelos/"&gt;https://arvoredossaberes.com.br/geral/como-usar-templates-do-ollama-para-criar-novos-modelos/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CarlosDelfino"&gt; /u/CarlosDelfino &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mv20zs/tutorial_about_templates_for_new_models_modelfile/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mv20zs/tutorial_about_templates_for_new_models_modelfile/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mv20zs/tutorial_about_templates_for_new_models_modelfile/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-20T01:57:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mup1xu</id>
    <title>Is Ollama at risk of getting lost in its own complexity? A long-term user's perspective.</title>
    <updated>2025-08-19T17:29:41+00:00</updated>
    <author>
      <name>/u/Mulan20</name>
      <uri>https://old.reddit.com/user/Mulan20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a huge fan and an intensive user of Ollama. This tool has been incredibly helpful in my workflow, and I've integrated it into almost everything I do. I'm writing this to share some observations and concerns, hoping to spark a constructive discussion.&lt;/p&gt; &lt;p&gt;With every new update and the exciting new models being released, I can't shake the feeling that the project is moving in a direction that might compromise its core strengths. My primary concern is that Ollama, in its effort to evolve, might be getting lost in complexity.&lt;/p&gt; &lt;p&gt;This feeling is reminiscent of the evolution of tools like the Stable Diffusion WebUI, which started as a fast and straightforward interface but has since become heavier and, at times, limited in its functionality.&lt;/p&gt; &lt;p&gt;Here are a few specific points that I've noticed:&lt;/p&gt; &lt;p&gt;Performance and Resource Consumption: After recent updates, Ollama feels significantly more resource-intensive. I used to be able to run up to four scripts simultaneously that leveraged Ollama, but now I'm limited to a single script because an instance is already running. This has been a significant bottleneck for my productivity.&lt;/p&gt; &lt;p&gt;Model Quality Degradation: This is perhaps my biggest concern. Models running through the latest versions of Ollama don't seem to perform as well as they used to. Their ability to use functions seems diminished, and the overall quality of the responses feels lower. To validate this, I tried running the same models directly from Hugging Face and noticed a tangible difference in performance and output quality.&lt;/p&gt; &lt;p&gt;Growing Complexity vs. Simplicity: The initial appeal of Ollama was its brilliant simplicity and ease of use. However, with each update, it seems to be getting more complicated and &amp;quot;heavier.&amp;quot; The seamless experience that made me a dedicated user is slowly being eroded.&lt;/p&gt; &lt;p&gt;I truly hope that Ollama doesn't suffer the same fate as other successful open-source projects that eventually buckled under the weight of their own complexity. I have been, and will continue to be, a supporter and user of Ollama.&lt;/p&gt; &lt;p&gt;However, for the time being, these challenges are forcing me to look for alternative solutions for my production workflows. I wanted to share my experience to see if others feel the same way and to offer my perspective as a dedicated user who wants to see this project succeed in the long run.&lt;/p&gt; &lt;p&gt;Thank you for reading.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mulan20"&gt; /u/Mulan20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mup1xu/is_ollama_at_risk_of_getting_lost_in_its_own/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mup1xu/is_ollama_at_risk_of_getting_lost_in_its_own/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mup1xu/is_ollama_at_risk_of_getting_lost_in_its_own/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-19T17:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvotl6</id>
    <title>just a SE student asking for a recommendation PLS HELP I AM DROWNING</title>
    <updated>2025-08-20T19:33:11+00:00</updated>
    <author>
      <name>/u/ShelterSouth8142</name>
      <uri>https://old.reddit.com/user/ShelterSouth8142</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my internship, I noticed the company validates clients for loans via a “check-the-box” process on documents. I built a fullstack webapp that parses these documents and uses an AI to deduce client suitability automatically.&lt;/p&gt; &lt;p&gt;Initially, I used Gemini via API, but the company firewall blocks it. My workaround is running an LLM through a Python script called in the Spring Boot backend. Everything works, except my personal PC has only 4GB VRAM and 16GB RAM.&lt;/p&gt; &lt;p&gt;I need a &lt;strong&gt;quantized, lightweight LLM&lt;/strong&gt; for testing. The final server will have better specs, but for now, it just needs to deduce simple text-based conditions. I’m new to this and would appreciate suggestions or advice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShelterSouth8142"&gt; /u/ShelterSouth8142 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvotl6/just_a_se_student_asking_for_a_recommendation_pls/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvotl6/just_a_se_student_asking_for_a_recommendation_pls/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mvotl6/just_a_se_student_asking_for_a_recommendation_pls/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-20T19:33:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv81tr</id>
    <title>Agentic Signal is live on Product Hunt 🚀 (visual AI workflows + Ollama)</title>
    <updated>2025-08-20T07:23:27+00:00</updated>
    <author>
      <name>/u/Code-Forge-Temple</name>
      <uri>https://old.reddit.com/user/Code-Forge-Temple</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mv81tr/agentic_signal_is_live_on_product_hunt_visual_ai/"&gt; &lt;img alt="Agentic Signal is live on Product Hunt 🚀 (visual AI workflows + Ollama)" src="https://preview.redd.it/njzywejml4kf1.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e0f5a546dfa6e70eabb45ac916ef44f588f1e7a" title="Agentic Signal is live on Product Hunt 🚀 (visual AI workflows + Ollama)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just launched &lt;strong&gt;Agentic Signal&lt;/strong&gt; on Product Hunt!&lt;br /&gt; It’s a visual AI workflow builder with full &lt;strong&gt;Ollama&lt;/strong&gt; integration — local, privacy‑first, and extensible.&lt;/p&gt; &lt;p&gt;👉 Check it out and share feedback: &lt;a href="https://www.producthunt.com/products/agentic-signal"&gt;https://www.producthunt.com/products/agentic-signal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs &amp;amp; intro video: &lt;a href="https://agentic-signal.com"&gt;https://agentic-signal.com&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=62zk8zE6UJI"&gt;https://www.youtube.com/watch?v=62zk8zE6UJI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/code-forge-temple/agentic-signal"&gt;https://github.com/code-forge-temple/agentic-signal&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Code-Forge-Temple"&gt; /u/Code-Forge-Temple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/njzywejml4kf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mv81tr/agentic_signal_is_live_on_product_hunt_visual_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mv81tr/agentic_signal_is_live_on_product_hunt_visual_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-20T07:23:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv7sc0</id>
    <title>Qwen3-4B-Instruct-2507-GGUF template fixed</title>
    <updated>2025-08-20T07:07:18+00:00</updated>
    <author>
      <name>/u/Pjotrs</name>
      <uri>https://old.reddit.com/user/Pjotrs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Unsloth team uploaded templates to: &lt;a href="https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF"&gt;https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And how the model works out of box. Same should happen to the Thinking variant soon.&lt;/p&gt; &lt;p&gt;This model is amazing and having a drop-in working version is great.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pjotrs"&gt; /u/Pjotrs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mv7sc0/qwen34binstruct2507gguf_template_fixed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mv7sc0/qwen34binstruct2507gguf_template_fixed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mv7sc0/qwen34binstruct2507gguf_template_fixed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-20T07:07:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvvxff</id>
    <title>How to optimize Ollama for continuous requests and avoid lost requests in the queue</title>
    <updated>2025-08-21T00:13:15+00:00</updated>
    <author>
      <name>/u/CarlosDelfino</name>
      <uri>https://old.reddit.com/user/CarlosDelfino</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm creating a question and answer dataset about all Wikipedia content. Everything is working, except that every 20 Wikipedia texts, ollama crashes, and I need to restart it. It returns to normal after the next 20 texts. I wrote the script so that it checks if there are many processes running and then waits for them to finish before adding another one, but it keeps crashing. I'm getting to the point where I need to run the script as root and set it to restart the service if it takes more than 5 minutes to free up the queue. I'm using a GTX 4070, which is unfortunately the best I can get right now. Does anyone have any suggestions for how ollama can better manage the request queue? I'm using the Granite3.3:8b model because it's the best I've found, with a large context window set to 1,000 tokens (40,000 total).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CarlosDelfino"&gt; /u/CarlosDelfino &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvvxff/how_to_optimize_ollama_for_continuous_requests/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvvxff/how_to_optimize_ollama_for_continuous_requests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mvvxff/how_to_optimize_ollama_for_continuous_requests/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T00:13:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvum1l</id>
    <title>Hardware &amp; LLM - Image Creation</title>
    <updated>2025-08-20T23:15:44+00:00</updated>
    <author>
      <name>/u/Illustrious-Hurry-59</name>
      <uri>https://old.reddit.com/user/Illustrious-Hurry-59</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi - I have recently started using text based models &amp;amp; I am amazed at what you can host locally using Ollama. I want to further play around with LLM but interested into taking it further into image/video generation.&lt;/p&gt; &lt;p&gt;I have the following rig config, can anyone suggest if this will be handle the image/video generation?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU: Ryzen 5 7600X&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU: NVIDIA® GeForce RTX™ 5060 Ti 16GB&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory: 16 GB DDR5 DRAM 6000 MHz&lt;/strong&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also which model would be more suitable for my requirement &amp;amp; be compatible with the above hardware?&lt;/p&gt; &lt;p&gt;Thank you all in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Hurry-59"&gt; /u/Illustrious-Hurry-59 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvum1l/hardware_llm_image_creation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvum1l/hardware_llm_image_creation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mvum1l/hardware_llm_image_creation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-20T23:15:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvyc7i</id>
    <title>Anyone using Ollama on a Windows Snapdragon Machine?</title>
    <updated>2025-08-21T02:04:36+00:00</updated>
    <author>
      <name>/u/Clipbeam</name>
      <uri>https://old.reddit.com/user/Clipbeam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious to see how well it performs... What models can you run on say the Surface laptop 15?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Clipbeam"&gt; /u/Clipbeam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvyc7i/anyone_using_ollama_on_a_windows_snapdragon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvyc7i/anyone_using_ollama_on_a_windows_snapdragon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mvyc7i/anyone_using_ollama_on_a_windows_snapdragon/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T02:04:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw2x28</id>
    <title>Need help picking LLM for sorting a book by speakers</title>
    <updated>2025-08-21T06:03:16+00:00</updated>
    <author>
      <name>/u/Only-Web-8543</name>
      <uri>https://old.reddit.com/user/Only-Web-8543</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, forgive my ignorance I am still learning. I am trying to find a model i can use to break down a book by speaker. I have ~100gb cpu ram thats usable (not vram too poor) so i need it to fit into that size and accuracy is a concern because i don't want the speaker to be mixed up or confused and get things wrong. I know ill probably have to break the book down into chapters because a 400 page book is probably too many tokens for most models but if there are any that can handle a 400 page book that would be great! If i have to go chapter by chapter which model would be best? I was looking at Qwen 3 32b instruct, LLama 3 34B, Minstral 30b, LLama scout 17B because it has a 1m token context window but from what i found that wont fit on 100gb but i could be wrong? and lastly I just saw that OpenAI released the oss models and was curious if those are any good? &lt;/p&gt; &lt;p&gt;Any advice is appreciated&lt;br /&gt; Thanks, &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Only-Web-8543"&gt; /u/Only-Web-8543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mw2x28/need_help_picking_llm_for_sorting_a_book_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mw2x28/need_help_picking_llm_for_sorting_a_book_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mw2x28/need_help_picking_llm_for_sorting_a_book_by/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-21T06:03:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvs4qa</id>
    <title>Had some beginner questions regarding how to use Ollama?</title>
    <updated>2025-08-20T21:35:46+00:00</updated>
    <author>
      <name>/u/RandomHuman1002</name>
      <uri>https://old.reddit.com/user/RandomHuman1002</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I am a beginner in trying to run AI locally had some questions regarding it.&lt;br /&gt; I want to run the AI on my laptop (13th gen i7-13650HX, 32GB RAM, RTX 4060 Laptop GPU) &lt;/p&gt; &lt;p&gt;1) Which AI model should I use I can see many of them on the ollama website like the new (gpt-oss, deepseek-r1, gemma3, qwen3 and llama3.1). Has anyone compared the pros and cons of each model?&lt;br /&gt; I can see that llama3.1 does not have thinking capabilities and gemma3 is the only vision model how does that affect the model that is running?&lt;/p&gt; &lt;p&gt;2) I am on a Windows machine so should I just use windows ollama or try to use Linux ollama using wsl (was recommended to do this)&lt;/p&gt; &lt;p&gt;3) Should I install openweb-ui and install ollama through that or just install ollama first?&lt;/p&gt; &lt;p&gt;Any other things I should keep in mind?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomHuman1002"&gt; /u/RandomHuman1002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvs4qa/had_some_beginner_questions_regarding_how_to_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvs4qa/had_some_beginner_questions_regarding_how_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mvs4qa/had_some_beginner_questions_regarding_how_to_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-20T21:35:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvrxsz</id>
    <title>Best model for my use case?</title>
    <updated>2025-08-20T21:28:23+00:00</updated>
    <author>
      <name>/u/guacgang</name>
      <uri>https://old.reddit.com/user/guacgang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am building an application where the model needs to make recommendations on rock climbing routes, including details about weather, difficulty, suggested gear, etc.&lt;/p&gt; &lt;p&gt;It also needs to be able to review videos that users/climbers upload and make suggestions on technique.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/guacgang"&gt; /u/guacgang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvrxsz/best_model_for_my_use_case/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mvrxsz/best_model_for_my_use_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mvrxsz/best_model_for_my_use_case/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-20T21:28:23+00:00</published>
  </entry>
</feed>
