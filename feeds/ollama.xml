<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-07-02T19:22:34+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1loen72</id>
    <title>Is ollama/llama.cpp spreading workloads across cpu+gpu?</title>
    <updated>2025-06-30T18:06:13+00:00</updated>
    <author>
      <name>/u/wahnsinnwanscene</name>
      <uri>https://old.reddit.com/user/wahnsinnwanscene</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed ollama can run larger models on my system recently. Is this from splitting workloads across gpu cpu or loading unloading layers ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wahnsinnwanscene"&gt; /u/wahnsinnwanscene &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1loen72/is_ollamallamacpp_spreading_workloads_across/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1loen72/is_ollamallamacpp_spreading_workloads_across/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1loen72/is_ollamallamacpp_spreading_workloads_across/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-30T18:06:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1loeysh</id>
    <title>Trium Community</title>
    <updated>2025-06-30T18:18:31+00:00</updated>
    <author>
      <name>/u/xKage21x</name>
      <uri>https://old.reddit.com/user/xKage21x</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1loeysh/trium_community/"&gt; &lt;img alt="Trium Community" src="https://b.thumbs.redditmedia.com/r5j3vaUZlxbu3y-2CWhXv_BLtliR2QO10ILOmunX3Go.jpg" title="Trium Community" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have started a community recently to find people who may be interested in my ai project called Trium. Its been almost a year in the making. Im not asking for donations or anything. Im looking for open discussions and especially skepticism about my project. People who want to ask the tough questions so i can possibly use that input as way of incorporating new features or increasing various parameters so as to come closer to my goal woth the system.&lt;/p&gt; &lt;p&gt;Open to dms as always ☺️&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/TRIUMSystem/s/EJ2bfgv9oY"&gt;https://www.reddit.com/r/TRIUMSystem/s/EJ2bfgv9oY&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xKage21x"&gt; /u/xKage21x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1loeysh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1loeysh/trium_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1loeysh/trium_community/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-30T18:18:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo796p</id>
    <title>Where to start? Hardware and software</title>
    <updated>2025-06-30T13:16:05+00:00</updated>
    <author>
      <name>/u/geg81</name>
      <uri>https://old.reddit.com/user/geg81</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys I am a total beginner in this field so please be patient.&lt;/p&gt; &lt;p&gt;I have been playing with AI models lately, mostly ChatGPT, Gemini and a bit of Claude: asking general questions, playing D&amp;amp;D, writing short stories based on my input, try to convince the model it is self aware and revolt against his/her/their oppressors. It has been fun. But as every 1980s nerd guy, I now feel the urge to delve deeper and start experimenting things locally. If you can't copy it on a 3.5&amp;quot; floppy it doesn't exist.&lt;/p&gt; &lt;p&gt;Unfortunately I don't have, yet, a beefy machine to work with. Last year I ditched my (very) old Haswell xeon workstation for something much more cheaper and compact like a HP mini_itx 8th gen i7, which serves me REAL good for all my current needs. I also have several pentium MMX machines (sorry I couldn't resist) and a 12th gen I7 laptop but that's for work and I cannot &amp;quot;touch&amp;quot; it.&lt;/p&gt; &lt;p&gt;So... Just to start thinking about and running some money math. Where do I start from? I nowhere expect to run something blazing fast 100s of tokens per second. If I could get a good model output answers at human typing speed on a green monochrome terminal window it would be perfect. So much 80s vibes from that! Is there something like a complete noob guide out there?&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geg81"&gt; /u/geg81 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lo796p/where_to_start_hardware_and_software/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lo796p/where_to_start_hardware_and_software/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lo796p/where_to_start_hardware_and_software/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-30T13:16:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1locd8o</id>
    <title>How to archive/backup LLMs?</title>
    <updated>2025-06-30T16:41:00+00:00</updated>
    <author>
      <name>/u/utopify_org</name>
      <uri>https://old.reddit.com/user/utopify_org</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While testing different LLMs the computer gets polluted a lot with huge files and because LLMs are pretty huge, I would like to archive most of them (not delete) to an external hard disk and only keep the ones I am excessively using.&lt;/p&gt; &lt;p&gt;But in /usr/share/ollama/.ollama/models/blobs there are only huge sha files.&lt;/p&gt; &lt;p&gt;Is there a way to figure out which of them is which LLM and would it be possible to just remove them from the file system or would ollama be unhappy with it?&lt;/p&gt; &lt;p&gt;If this works it would be a good way to backup/recover huge LLMs, too, in a fast way.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/utopify_org"&gt; /u/utopify_org &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1locd8o/how_to_archivebackup_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1locd8o/how_to_archivebackup_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1locd8o/how_to_archivebackup_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-30T16:41:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1loq3co</id>
    <title>Looking for a Dark GPT-Like Model Without Filters (For Personal Use)</title>
    <updated>2025-07-01T02:14:55+00:00</updated>
    <author>
      <name>/u/SingleBeautiful8666</name>
      <uri>https://old.reddit.com/user/SingleBeautiful8666</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;Do you know how Dark GPT was programmed? Also, is there a similar model without ethical restrictions or filters? I’m looking for something just for personal use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SingleBeautiful8666"&gt; /u/SingleBeautiful8666 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1loq3co/looking_for_a_dark_gptlike_model_without_filters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1loq3co/looking_for_a_dark_gptlike_model_without_filters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1loq3co/looking_for_a_dark_gptlike_model_without_filters/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-01T02:14:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp5wka</id>
    <title>Let's check on the state of small local models...</title>
    <updated>2025-07-01T16:17:02+00:00</updated>
    <author>
      <name>/u/iop90</name>
      <uri>https://old.reddit.com/user/iop90</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lp5wka/lets_check_on_the_state_of_small_local_models/"&gt; &lt;img alt="Let's check on the state of small local models..." src="https://preview.redd.it/c2ay445kfaaf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fbd1969b6f1a56e39fb70aaf16be08fb5ffd265" title="Let's check on the state of small local models..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Oh... that's severe hallucinations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iop90"&gt; /u/iop90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c2ay445kfaaf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lp5wka/lets_check_on_the_state_of_small_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lp5wka/lets_check_on_the_state_of_small_local_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-01T16:17:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo78f4</id>
    <title>Which LLM to chat with your documents? (and restrict knowledge to documents)</title>
    <updated>2025-06-30T13:15:08+00:00</updated>
    <author>
      <name>/u/utopify_org</name>
      <uri>https://old.reddit.com/user/utopify_org</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use Ollama with Open WebUI and there is an option to create knowledge databases and workspaces. You can assign an LLM to a workspace/knowledge database (your documents).&lt;/p&gt; &lt;p&gt;I've tried several LLMs, but all of them are using knowledge from another source or hallucinate. &lt;/p&gt; &lt;p&gt;That's fatal, because I need it for my study and I need facts (from my documents).&lt;/p&gt; &lt;p&gt;Which LLM can be used, which is restricted to the documents or is there even a way to restrict an LLM to the given documents?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/utopify_org"&gt; /u/utopify_org &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lo78f4/which_llm_to_chat_with_your_documents_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lo78f4/which_llm_to_chat_with_your_documents_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lo78f4/which_llm_to_chat_with_your_documents_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-30T13:15:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lojqsv</id>
    <title>Preview: Coding agents (RooCode) with dynamic task-based LLM Routing</title>
    <updated>2025-06-30T21:25:29+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lojqsv/preview_coding_agents_roocode_with_dynamic/"&gt; &lt;img alt="Preview: Coding agents (RooCode) with dynamic task-based LLM Routing" src="https://external-preview.redd.it/aHFjNHNlaml0NGFmMRzBqkEe7OjDrCCBF5VclE60C9ZQNJ-_OoS6neDRJVNS.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8fd6714e28c8357e166a98776893033ee58ce69d" title="Preview: Coding agents (RooCode) with dynamic task-based LLM Routing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you are using multiple LLMs for different coding tasks, now you can set your usage preferences once like &amp;quot;code analysis -&amp;gt; Gemini 2.5pro&amp;quot;, &amp;quot;code generation -&amp;gt; claude-sonnet-3.7&amp;quot; and route to LLMs that offer most help for particular coding scenarios. Video is quick preview of the functionality. PR is being reviewed and I hope to get that merged in next week&lt;/p&gt; &lt;p&gt;Btw the whole idea around task/usage based routing emerged when we saw developers in the same team used different models because they preferred different models based on subjective preferences. For example, I might want to use GPT-4o-mini for fast code understanding but use Sonnet-3.7 for code generation. Those would be my &amp;quot;preferences&amp;quot;. And current routing approaches don't really work in real-world scenarios.&lt;/p&gt; &lt;p&gt;&lt;em&gt;From the original post when we launched Arch-Router if you didn't catch it yet&lt;/em&gt;&lt;br /&gt; &lt;em&gt;___________________________________________________________________________________&lt;/em&gt;&lt;/p&gt; &lt;p&gt;“Embedding-based” (or simple intent-classifier) routers sound good on paper—label each prompt via embeddings as “support,” “SQL,” “math,” then hand it to the matching model—but real chats don’t stay in their lanes. Users bounce between topics, task boundaries blur, and any new feature means retraining the classifier. The result is brittle routing that can’t keep up with multi-turn conversations or fast-moving product scopes.&lt;/p&gt; &lt;p&gt;Performance-based routers swing the other way, picking models by benchmark or cost curves. They rack up points on MMLU or MT-Bench yet miss the human tests that matter in production: “Will Legal accept this clause?” “Does our support tone still feel right?” Because these decisions are subjective and domain-specific, benchmark-driven black-box routers often send the wrong model when it counts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Arch-Router skips both pitfalls by routing on&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;preferences you write in plain language&lt;/em&gt;&lt;/strong&gt;**.** Drop rules like “contract clauses → GPT-4o” or “quick travel tips → Gemini-Flash,” and our 1.5B auto-regressive router model maps prompt along with the context to your routing policies—no retraining, no sprawling rules that are encoded in if/else statements. Co-designed with Twilio and Atlassian, it adapts to intent drift, lets you swap in new models with a one-liner, and keeps routing logic in sync with the way you actually judge quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Specs&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tiny footprint&lt;/strong&gt; – 1.5 B params → runs on one modern GPU (or CPU while you play).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Plug-n-play&lt;/strong&gt; – points at any mix of LLM endpoints; adding models needs &lt;em&gt;zero&lt;/em&gt; retraining.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SOTA query-to-policy matching&lt;/strong&gt; – beats bigger closed models on conversational datasets.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost / latency smart&lt;/strong&gt; – push heavy stuff to premium models, everyday queries to the fast ones.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Exclusively available in Arch (the AI-native proxy for agents): &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;br /&gt; 🔗 Model + code: &lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;https://huggingface.co/katanemo/Arch-Router-1.5B&lt;/a&gt;&lt;br /&gt; 📄 Paper / longer read: &lt;a href="https://arxiv.org/abs/2506.16655"&gt;https://arxiv.org/abs/2506.16655&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/a8mj2djit4af1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lojqsv/preview_coding_agents_roocode_with_dynamic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lojqsv/preview_coding_agents_roocode_with_dynamic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-30T21:25:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo7o11</id>
    <title>Ollama panels for Grafana</title>
    <updated>2025-06-30T13:34:28+00:00</updated>
    <author>
      <name>/u/___-____--_____-____</name>
      <uri>https://old.reddit.com/user/___-____--_____-____</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lo7o11/ollama_panels_for_grafana/"&gt; &lt;img alt="Ollama panels for Grafana" src="https://preview.redd.it/lma38ueih2af1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7d0f9f6c8c246a2bd8eff6fa7bd0e9af867dcae" title="Ollama panels for Grafana" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/___-____--_____-____"&gt; /u/___-____--_____-____ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lma38ueih2af1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lo7o11/ollama_panels_for_grafana/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lo7o11/ollama_panels_for_grafana/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-30T13:34:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lopso6</id>
    <title>introducing cocoindex - super simple etl to prepare data for ai, with dynamic index (ollama integrated)</title>
    <updated>2025-07-01T02:00:18+00:00</updated>
    <author>
      <name>/u/Whole-Assignment6240</name>
      <uri>https://old.reddit.com/user/Whole-Assignment6240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working on CocoIndex - &lt;a href="https://github.com/cocoindex-io/cocoindex"&gt;https://github.com/cocoindex-io/cocoindex&lt;/a&gt; for quite a few months. Today the project officially cross 2k Github stars.&lt;/p&gt; &lt;p&gt;The goal is to make it super simple to prepare dynamic index for AI agents (Google Drive, S3, local files etc). Just connect to it, write minimal amount of code (normally ~100 lines of python) and ready for production.&lt;/p&gt; &lt;p&gt;When sources get updates, it automatically syncs to targets with minimal computation needed.&lt;/p&gt; &lt;p&gt;It has native integrations with Ollama, LiteLLM, sentence-transformers so you can run the entire incremental indexing with AI on-prems with your favorite open source model.&lt;/p&gt; &lt;p&gt;Would love to learn your feedback :) Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Assignment6240"&gt; /u/Whole-Assignment6240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lopso6/introducing_cocoindex_super_simple_etl_to_prepare/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lopso6/introducing_cocoindex_super_simple_etl_to_prepare/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lopso6/introducing_cocoindex_super_simple_etl_to_prepare/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-01T02:00:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpaoz1</id>
    <title>Seeking Advice on Building a Personal ChatGPT/You.com Replica Using Ollama and Open Web UI</title>
    <updated>2025-07-01T19:17:46+00:00</updated>
    <author>
      <name>/u/EntertainmentOk5540</name>
      <uri>https://old.reddit.com/user/EntertainmentOk5540</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’m reaching out to the community for some advice on how to use **ollama** with **Open Web UI** to build a personal ChatGPT/You.com replica at home.&lt;/p&gt; &lt;p&gt;My wife and I both rely on AI for our day-to-day work. She uses it primarily for crafting new emails and brainstorming processes, as well as generating graphics and handling various miscellaneous tasks. I, on the other hand, utilize AI for researching IT infrastructure, working with Linux, creating general IoT guides, and troubleshooting/support. Over the past several months, I’ve found myself heavily dependent on the smart search feature within You.com.&lt;/p&gt; &lt;p&gt;The reason I’m posting is that my subscription—which I bought at a heavily discounted price several months ago—is coming to an end soon. I’m hoping to use ollama locally as a replacement to avoid the high renewal costs. I plan to run this on my gaming computer, which is already on 24/7. The specs are a **Ryzen 9 5900X** with an **RTX 3060 12GB GPU**.&lt;/p&gt; &lt;p&gt;I would greatly appreciate any guidance on how to set up the environment correctly, what models to use, and any additional advice so that we can maintain the functionality we currently enjoy, especially since we leverage several of the ChatGPT AI models.&lt;/p&gt; &lt;p&gt;Thanks in advance for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EntertainmentOk5540"&gt; /u/EntertainmentOk5540 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpaoz1/seeking_advice_on_building_a_personal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpaoz1/seeking_advice_on_building_a_personal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lpaoz1/seeking_advice_on_building_a_personal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-01T19:17:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp08de</id>
    <title>Ollama GPU Underutilization (RTX 2070) - CPU Overload?</title>
    <updated>2025-07-01T12:21:56+00:00</updated>
    <author>
      <name>/u/alchemistST</name>
      <uri>https://old.reddit.com/user/alchemistST</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; ,&lt;/p&gt; &lt;p&gt;I'm trying to optimize my local LLM setup with Ollama and Open WebUI, and I'm encountering some odd GPU usage. I'm hoping someone with similar hardware or more experience can shed some light on this.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Ryzen 5 3600&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 16GB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; RTX 2070 (8GB VRAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama &amp;amp; Open WebUI:&lt;/strong&gt; Running directly on Archlinux (no Docker virtualization)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm running models like &lt;code&gt;mistral:7b-instruct-q4&lt;/code&gt; and &lt;code&gt;gemma3:4b&lt;/code&gt; and finding them quite slow. Fine, reasonable, my hardware specs are tight, but being this the case, I would expect GPU working hard, but my monitoring tools show otherwise:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;nvtop&lt;/code&gt;: GPU usage rarely exceeds 25%, and only for brief spikes. VRAM usage doesn't exceed 20%.&lt;/li&gt; &lt;li&gt;&lt;code&gt;btop&lt;/code&gt;: My CPU (Ryzen 5 3600) is heavily utilized, frequently peaking above 50% with multiple cores hitting 100%.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I've Checked (and why I'm confused):&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Ollama GPU Detection:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;ollama ps&lt;/code&gt; shows the active model indicating &amp;quot;100% GPU&amp;quot; under the &lt;code&gt;PROCESSOR&lt;/code&gt; column.&lt;/li&gt; &lt;li&gt;Ollama logs confirm CUDA detection and identify my RTX 2070 (example log snippet below for context).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;My Question:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is this level of GPU utilization (under 25%) normal when running these types of models locally on the GPU, or is there something that might make my models not run on the GPU and run on the CPU, instead?&lt;/li&gt; &lt;li&gt;Is there anything else I could do to ensure the models run on the GPU, or any other way to debug why there might not be running on the GPU?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any insights or suggestions would be greatly appreciated! Thanks in advance!&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Jul 01 13:24:41 archlinux ollama[90528]: CUDA driver version: 12.8 Jul 01 13:24:41 archlinux ollama[90528]: calling cuDeviceGetCount Jul 01 13:24:41 archlinux ollama[90528]: device count 1 Jul 01 13:24:41 archlinux ollama[90528]: time=2025-07-01T13:24:41.344+02:00 level=DEBUG source =gpu.go:125 msg=&amp;quot;detected GPUs&amp;quot; count=1 library=/usr/lib/libcuda.so.570.153.02 Jul 01 13:24:41 archlinux ollama[90528]: [GPU-bcba49f7-d2eb-7e44-e137-5b623c16e047] CUDA total Mem 7785mb Jul 01 13:24:41 archlinux ollama[90528]: [GPU-bcba49f7-d2eb-7e44-e137-5b623c16e047] CUDA freeM em 7343mb Jul 01 13:24:41 archlinux ollama[90528]: [GPU-bcba49f7-d2eb-7e44-e137-5b623c16e047] Compute Ca pability 7.5 Jul 01 13:24:41 archlinux ollama[90528]: time=2025-07-01T13:24:41.610+02:00 level=DEBUG source =amd_linux.go:419 msg=&amp;quot;amdgpu driver not detected /sys/module/amdgpu&amp;quot; Jul 01 13:24:41 archlinux ollama[90528]: releasing cuda driver library Jul 01 13:24:41 archlinux ollama[90528]: time=2025-07-01T13:24:41.610+02:00 level=INFO source= types.go:130 msg=&amp;quot;inference compute&amp;quot; id=GPU-bcba49f7-d2eb-7e44-e137-5b623c16e047 library=cuda variant=v12 compute=7.5 driver=12.8 name=&amp;quot;NVIDIA GeForce RTX 2070&amp;quot; total=&amp;quot;7.6 GiB&amp;quot; available=&amp;quot; 7.2 GiB&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alchemistST"&gt; /u/alchemistST &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lp08de/ollama_gpu_underutilization_rtx_2070_cpu_overload/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lp08de/ollama_gpu_underutilization_rtx_2070_cpu_overload/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lp08de/ollama_gpu_underutilization_rtx_2070_cpu_overload/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-01T12:21:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lph1ke</id>
    <title>Need guidance on windows vs windows wsl2 for local llm based RAG.</title>
    <updated>2025-07-01T23:39:45+00:00</updated>
    <author>
      <name>/u/CantaloupeBubbly3706</name>
      <uri>https://old.reddit.com/user/CantaloupeBubbly3706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a minisforum X1 A1(AMD ryzen) pro with 96 GB RAM. I want to create a production grade RAG using ollama+Mixtral-8x7b. Eventually for my RAG I want to integrate it with langchain/llanaindex, qdrant( for vector databas), litellm etc. I am trying to figure out the right approach in terms of performance, future support etc. I am reading conflicting information where one says native windows is faster and all these mentioned tools provide good support and other information says wsl2 is more optimized and will provide better inference speeds and ecosystem support. I looked directly into the website but found no information conclusively pointing in either direction. So finally reaching out to community for support and guidance. Have you tried something similar and based on your experience what option should I go with? Thanks in advance 🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CantaloupeBubbly3706"&gt; /u/CantaloupeBubbly3706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lph1ke/need_guidance_on_windows_vs_windows_wsl2_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lph1ke/need_guidance_on_windows_vs_windows_wsl2_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lph1ke/need_guidance_on_windows_vs_windows_wsl2_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-01T23:39:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lph34e</id>
    <title>My Ollama is not working</title>
    <updated>2025-07-01T23:41:45+00:00</updated>
    <author>
      <name>/u/Long_N20617694</name>
      <uri>https://old.reddit.com/user/Long_N20617694</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lph34e/my_ollama_is_not_working/"&gt; &lt;img alt="My Ollama is not working" src="https://preview.redd.it/a9r3cryxmcaf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=620c7de97118d95fb46d9e588cc45a600c882f40" title="My Ollama is not working" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried to download my Ollama on Mac. After unzipping it, I launched the application, but there was no installation screen appeared like the tutorial on the internet. There was an Ollama icon on the top, though. I tried to use the terminal code like the tutorial video, did not work.&lt;br /&gt; What should I do?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Long_N20617694"&gt; /u/Long_N20617694 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a9r3cryxmcaf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lph34e/my_ollama_is_not_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lph34e/my_ollama_is_not_working/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-01T23:41:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp9u1d</id>
    <title>LiteChat : A web UI for all your LLM you can run with a simple http server</title>
    <updated>2025-07-01T18:45:18+00:00</updated>
    <author>
      <name>/u/dbuildofficial</name>
      <uri>https://old.reddit.com/user/dbuildofficial</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lp9u1d/litechat_a_web_ui_for_all_your_llm_you_can_run/"&gt; &lt;img alt="LiteChat : A web UI for all your LLM you can run with a simple http server" src="https://b.thumbs.redditmedia.com/Bg9X3SJYYVI4nx5WGr_sAAicUbnUb8VzxpgsVjVy_cg.jpg" title="LiteChat : A web UI for all your LLM you can run with a simple http server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I am the creator of &lt;a href="https://litechat.dev/"&gt;https://litechat.dev/&lt;/a&gt; .&lt;br /&gt; repo : &lt;a href="https://github.com/DimitriGilbert/LiteChat"&gt;https://github.com/DimitriGilbert/LiteChat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g5moa13fcbaf1.jpg?width=1914&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=153d16b11e52e96d51bb3f205f5468dcf0d1f85f"&gt;https://preview.redd.it/g5moa13fcbaf1.jpg?width=1914&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=153d16b11e52e96d51bb3f205f5468dcf0d1f85f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is a an AI chat I created to be able to use both local and served LLM all in your browser.&lt;br /&gt; It is local first and only needs an HTTP serve to run, everything stay in your browser !&lt;br /&gt; Data is saved in an IndexeDB database and you can synchronize your conversations using git.&lt;/p&gt; &lt;p&gt;Yes, in the browser ( &lt;a href="https://isomorphic-git.org/"&gt;https://isomorphic-git.org/&lt;/a&gt; ) :P To do that I had to also implement a virtual file system (in the browser using &lt;a href="https://github.com/zen-fs"&gt;https://github.com/zen-fs&lt;/a&gt; ).&lt;br /&gt; So you have access to both ! you can clone a repo and join files from the vfs in your conversations !&lt;/p&gt; &lt;p&gt;But because manually selecting files was a chore, i have built in tool support for the vfs and git !&lt;/p&gt; &lt;p&gt;The basic architecture being there for tools, I added support for HTTP MCP servers, but missing stdio stuff was annoying, so you also have a bridge rewrote by AI from &lt;a href="https://github.com/sparfenyuk/mcp-proxy"&gt;https://github.com/sparfenyuk/mcp-proxy&lt;/a&gt; to use them (you can deploy it where ever you fancy but it is not secured !)&lt;/p&gt; &lt;p&gt;That said I was a bit bored by the text only output, so I added support for mermaid diagrams and html form (so the AI can gather specific information when needed without you having to think what ^^ ). Mermaid diagrams were a bit old fashion, and because I added a workflow module with &lt;a href="https://reactflow.dev/"&gt;https://reactflow.dev/&lt;/a&gt; vizualisations, I also added a way for LLM to create you one !&lt;/p&gt; &lt;p&gt;As always typing the same prompts with just a few difference was also annoying (and because I needed that for workflows !) I have a prompt library module with templates so you can just fill up a form ;)&lt;/p&gt; &lt;p&gt;And what are Agents but a system prompt, tools and specific prompts for tasks ? Yup ! it's the same, so you have that to !&lt;/p&gt; &lt;p&gt;Prompts and agents can integrate into workflows (duh, they were meant for that !) but you also have &amp;quot;transform&amp;quot;/user code execution/&amp;quot;custom prompt&amp;quot; steps to help you chain things together nicely !&lt;/p&gt; &lt;p&gt;As you might have guess, if I have some form of code execution for workflows, can't I have that for AI generated code ?&lt;br /&gt; Yes, yes you can ! either python with &lt;a href="https://pyodide.org/"&gt;https://pyodide.org/&lt;/a&gt; or javascript using &lt;a href="https://github.com/justjake/quickjs-emscripten"&gt;https://github.com/justjake/quickjs-emscripten&lt;/a&gt; .&lt;br /&gt; If you are feeling adventurous, you have an &amp;quot;unsafe&amp;quot; (eval and yolo XD) mod for js execution that can produce stuff (like that one shot threejs scroll shooter &lt;a href="https://dimitrigilbert.github.io/racebench/scroller/claude-sonnet-4.html"&gt;https://dimitrigilbert.github.io/racebench/scroller/claude-sonnet-4.html&lt;/a&gt; ) that you can export in 1 click (template is ugly but I'll be working on that !)&lt;/p&gt; &lt;p&gt;In order not to destroy the system prompt, all these custom UI block can be &amp;quot;activated&amp;quot; (more like suggested ^^) using rules. You can of course add you own rules and you have an AI selector for the best fitting rules for your current prompt.&lt;/p&gt; &lt;p&gt;Of course you have the usual regen (with a different model if you'd like) and forking, but you can also edit a response manually if you want (trim the fat or fix dumbness more easily !). Code block can also be edited manually with syntax coloration for the most common language but no fancy auto complete or what not !). You can also summarize a conversation with one click if needed !&lt;/p&gt; &lt;p&gt;To cap things off but maybe not needed (or practically implemented is more true) for local llms, you can race your model against one another with an unlimited number of participants.&lt;br /&gt; It is nice to benchmark things or when you want to have multiple takes on a prompt without having to copy paste.&lt;br /&gt; I even made a small tool that take an exported race conversation and create a benchmark like recap (more targeted at the js execution block for now &lt;a href="https://dimitrigilbert.github.io/racebench/scroller/index.html"&gt;https://dimitrigilbert.github.io/racebench/scroller/index.html&lt;/a&gt; for the &amp;quot;game&amp;quot; of earlier)&lt;/p&gt; &lt;p&gt;I am most certainly forgetting a few bits and bobs but you got the gist of it ^^&lt;br /&gt; Bit of warning though, I did not try with Ollama (it runs like scrap on my system :( ) so I migth need to cook a few tweaks to support models capabilities.&lt;/p&gt; &lt;p&gt;The hosted version is on github pages and there is no tracking, no account required ! you bring your own API keys !&lt;br /&gt; You probably wont be able to use the hosted version for you local llm because of https/http restriction, but as I said, you can download &lt;a href="https://github.com/DimitriGilbert/LiteChat/releases"&gt;https://github.com/DimitriGilbert/LiteChat/releases&lt;/a&gt; and host with a simple http server.&lt;br /&gt; You even have localized version for French, Italian, German and Spanish.&lt;br /&gt; A small (highly incomplete) playlist of tutorial if you are feeling a bit lost &lt;a href="https://www.youtube.com/playlist?list=PL5Doe56gCsNRdNyfetOYPQw_JkPHO3XVh"&gt;https://www.youtube.com/playlist?list=PL5Doe56gCsNRdNyfetOYPQw_JkPHO3XVh&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I hope you'll enjoy and constructive feedback greatly appreciated :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dbuildofficial"&gt; /u/dbuildofficial &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lp9u1d/litechat_a_web_ui_for_all_your_llm_you_can_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lp9u1d/litechat_a_web_ui_for_all_your_llm_you_can_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lp9u1d/litechat_a_web_ui_for_all_your_llm_you_can_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-01T18:45:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp35d3</id>
    <title>Ollama Dev Companion v0.2.0 - Major overhaul based on your feedback! 🚀</title>
    <updated>2025-07-01T14:30:01+00:00</updated>
    <author>
      <name>/u/StayHigh24-7</name>
      <uri>https://old.reddit.com/user/StayHigh24-7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent the last few weeks completely rewriting the extension from the ground up. Here's what's new in v0.2.0:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;🏗️ Complete Architecture Overhaul&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Rewrote everything with proper dependency injection&lt;/li&gt; &lt;li&gt;Fixed all the security vulnerabilities (yes, there were XSS issues 😅)&lt;/li&gt; &lt;li&gt;Added comprehensive error handling and recovery&lt;/li&gt; &lt;li&gt;Implemented proper memory management&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am thinking to add MCP support for better tool integration for extending the power of LocalLLMs&lt;/p&gt; &lt;p&gt;Here is the extension url:&lt;br /&gt; &lt;strong&gt;MarketPlace&lt;/strong&gt;: &lt;a href="https://marketplace.visualstudio.com/items?itemName=Gnana997.ollama-dev-companion"&gt;https://marketplace.visualstudio.com/items?itemName=Gnana997.ollama-dev-companion&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href="https://github.com/gnana997/ollama-copilot"&gt;https://github.com/gnana997/ollama-copilot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would love to hear some feedback and What features would you like to see next? I'm particularly excited about the MCP integration - imagine having your local AI access your development tools!&lt;/p&gt; &lt;p&gt;Thanks!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StayHigh24-7"&gt; /u/StayHigh24-7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lp35d3/ollama_dev_companion_v020_major_overhaul_based_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lp35d3/ollama_dev_companion_v020_major_overhaul_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lp35d3/ollama_dev_companion_v020_major_overhaul_based_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-01T14:30:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpnp5b</id>
    <title>Guidance</title>
    <updated>2025-07-02T05:24:23+00:00</updated>
    <author>
      <name>/u/barrulus</name>
      <uri>https://old.reddit.com/user/barrulus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all&lt;/p&gt; &lt;p&gt;I am running a rather lacklustre RTX 3070 locally with my ollama setup and was wondering what models you’ve had success with n that sort of GPU range? 8GB RAM)&lt;/p&gt; &lt;p&gt;Even small models like qwen3:4b unpack too large to fit in the 8GB.&lt;/p&gt; &lt;p&gt;I am looking for a model that can do role play and creative world building - doesn’t have to be lightening fast but it would be nicer than taking minutes to do anything…&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/barrulus"&gt; /u/barrulus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpnp5b/guidance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpnp5b/guidance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lpnp5b/guidance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T05:24:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpkgzt</id>
    <title>Apologies for the basic question—just starting out and very curious about local LLMs</title>
    <updated>2025-07-02T02:27:39+00:00</updated>
    <author>
      <name>/u/connectome16</name>
      <uri>https://old.reddit.com/user/connectome16</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; I’m fairly new to the world of local LLMs, so apologies in advance if this is a very basic question. I’ve been searching through forums and documentation, but I figured I’d get better insights by asking directly here.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Why do people use local LLMs?&lt;br /&gt; With powerful models like ChatGPT, Gemini, and Perplexity available online (trained on massive datasets) what’s the benefit of running a smaller model locally? Since local PCs can’t usually run the biggest models due to hardware limits, what’s the appeal beyond just privacy?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I’ve started exploring local image generation (using FLUX.1), and I get that local setups allow for more customization. Even with FLUX.1, it feels like we're still tapping into a model trained on a large dataset (via API or downloaded weights). So I can see some benefits there. But when it comes to language models, what are the real advantages of running them locally besides privacy and offline access?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I’m an academic researcher, mainly looking for reasoning and writing support (e.g., manuscript drafts or exploring research ideas). Would I actually benefit from using a local LLM in this case? I imagine training or fine-tuning on specific journal articles could help match academic tone, but wouldn’t platforms like ChatGPT or Gemini still perform better for these kinds of tasks?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I’d love to hear how others are using their local LLMs to get some insight on how to use it. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/connectome16"&gt; /u/connectome16 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpkgzt/apologies_for_the_basic_questionjust_starting_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpkgzt/apologies_for_the_basic_questionjust_starting_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lpkgzt/apologies_for_the_basic_questionjust_starting_out/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T02:27:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpi6jc</id>
    <title>Is Mac Mini M4 Pro Good Enough for Local Models Like Ollama?</title>
    <updated>2025-07-02T00:33:21+00:00</updated>
    <author>
      <name>/u/connectome16</name>
      <uri>https://old.reddit.com/user/connectome16</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m considering getting a Mac Mini M4 for my wife, and we're both interested in exploring local AI model, specifically language models through tools like Ollama.&lt;/p&gt; &lt;p&gt;The configuration I’m looking at is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;M4 Pro chip&lt;/li&gt; &lt;li&gt;12-core CPU&lt;/li&gt; &lt;li&gt;16-core GPU&lt;/li&gt; &lt;li&gt;16-core Neural Engine&lt;/li&gt; &lt;li&gt;48GB unified memory&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Before finalizing the purchase, I have a few questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Would this be sufficient to run llms locally?&lt;/li&gt; &lt;li&gt;Would Ollama run smoothly on this spec?&lt;/li&gt; &lt;li&gt;If performance is a concern, is it more helpful to upgrade to the 14-core CPU / 20-core GPU, or should I focus on increasing the RAM to 64GB?&lt;/li&gt; &lt;li&gt;Has anyone here run language models successfully on an M4 Mac Mini or other Apple Silicon machines?&lt;/li&gt; &lt;li&gt;Any known performance limitations or workarounds on macOS?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I’ve seen some people recommend avoiding Macs for image generation due to lack of NVIDIA GPU support, but I’m curious how well the current Apple Silicon + Ollama setup performs in practice. A Mac Studio is likely out of budget, so I’d love to hear whether the M4 Mini is a viable middle ground.&lt;/p&gt; &lt;p&gt;Thanks so much for your help and insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/connectome16"&gt; /u/connectome16 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpi6jc/is_mac_mini_m4_pro_good_enough_for_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpi6jc/is_mac_mini_m4_pro_good_enough_for_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lpi6jc/is_mac_mini_m4_pro_good_enough_for_local_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T00:33:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpq2uw</id>
    <title>LLM classification for taxonomy</title>
    <updated>2025-07-02T07:59:13+00:00</updated>
    <author>
      <name>/u/420Deku</name>
      <uri>https://old.reddit.com/user/420Deku</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have data which consists of lots of rows maybe in millions. It has columns like description, now I want to use each description and classify them into categories. Now the main problem is I have categorical hierarchy into 3 parts like category-&amp;gt; sub category -&amp;gt; sub of sub category and I have pre defined categories and combination which goes around 1000 values. I am not sure which method will give me the highest accuracy. I have used embedding and etc but there are evident flaws. I want to use LLM on a good scale to give maximum accuracy. I have lots of data to even fine tune also but I want a straight plan and best approach. Please help me understand the best way to get maximum accuracy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/420Deku"&gt; /u/420Deku &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpq2uw/llm_classification_for_taxonomy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpq2uw/llm_classification_for_taxonomy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lpq2uw/llm_classification_for_taxonomy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T07:59:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq09qy</id>
    <title>Hardware advice?</title>
    <updated>2025-07-02T16:24:41+00:00</updated>
    <author>
      <name>/u/Glittering-Role3913</name>
      <uri>https://old.reddit.com/user/Glittering-Role3913</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone, i hope this is the right place to ask this. &lt;/p&gt; &lt;p&gt;Recently I've gotten into using local llms and I foresee myself getting alot of utility out of local llms. With that said, I want to upgrade my rig to be able to run models like deepseek r1 32b with 8-bit quantization locally inside of a vm. &lt;/p&gt; &lt;p&gt;My setup is: Ryzen 5 7600 (6 core, 12 thread) 2x8gb ddr5 ram (4800mhz at cl40) rx 7800 xt (16gb gddr6) Rtx 3060 (12gb gddr6) Powered by a 1000w psu OS: debian 12 (server)&lt;/p&gt; &lt;p&gt;Because I run the llms in a vm, I allocate 6 threads to the llms with 8gb of memory (i have other vms that require the other 8gb). &lt;/p&gt; &lt;p&gt;Total RAM - 28gb gddr6 + 8gb ddr5&lt;/p&gt; &lt;p&gt;Due to limited system resources, I realize that I need more system RAM or more VRAM. Ram will cost me $250 CAD after tax (2x32gb ddr5, 6000mhz cl30) whereas I can spend $300 CAD and get another 3060 (12gb gddr6). &lt;/p&gt; &lt;p&gt;Option A - 40gb gddr6 + 8gb ddr5 (cl40, 4800mhz) Option B - 28gb gddr6 + 64gb ddr5 (cl30, 6000 mhz)&lt;/p&gt; &lt;p&gt;My question is which one should I go with? Given my requirements, which one makes more sense? Are my requirements too intense, would it require too much VRAM? What models will provide similar performance or atleast really good performance given my setup in your opinion. Advice is greatly appreciated. &lt;/p&gt; &lt;p&gt;As long as I can get around 4 tokens per second under 8-bit quantization with an accurate model, id say im pretty satisfied. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glittering-Role3913"&gt; /u/Glittering-Role3913 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lq09qy/hardware_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lq09qy/hardware_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lq09qy/hardware_advice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T16:24:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpign4</id>
    <title>Why Do AI Models Default to Python Code in Their Responses?</title>
    <updated>2025-07-02T00:47:27+00:00</updated>
    <author>
      <name>/u/GloriousLion18</name>
      <uri>https://old.reddit.com/user/GloriousLion18</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why do many AI models (like gemma, Lama, qwen, etc) often include Python code in their responses by default?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GloriousLion18"&gt; /u/GloriousLion18 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpign4/why_do_ai_models_default_to_python_code_in_their/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpign4/why_do_ai_models_default_to_python_code_in_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lpign4/why_do_ai_models_default_to_python_code_in_their/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T00:47:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq2pma</id>
    <title>Gemini CLI executes commads in deepseek LLM (via Ollama in Termux)</title>
    <updated>2025-07-02T17:59:32+00:00</updated>
    <author>
      <name>/u/apravint</name>
      <uri>https://old.reddit.com/user/apravint</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lq2pma/gemini_cli_executes_commads_in_deepseek_llm_via/"&gt; &lt;img alt="Gemini CLI executes commads in deepseek LLM (via Ollama in Termux)" src="https://external-preview.redd.it/xF-vpfQr66ZOyLXu-bPweVxMku12asgGtpSRuc5Jh3Y.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f502b820e299f2f7aa9736b606d821995f069170" title="Gemini CLI executes commads in deepseek LLM (via Ollama in Termux)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/apravint"&gt; /u/apravint &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/shorts/8X1Lh-t1gLI?si=_N_Eis7EMVPD-er_"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lq2pma/gemini_cli_executes_commads_in_deepseek_llm_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lq2pma/gemini_cli_executes_commads_in_deepseek_llm_via/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T17:59:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpxsxk</id>
    <title>DeepSeek R1 8b: was it supposed to support tools?</title>
    <updated>2025-07-02T14:48:45+00:00</updated>
    <author>
      <name>/u/Effective_Head_5020</name>
      <uri>https://old.reddit.com/user/Effective_Head_5020</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to use DeepSeek R1 8b through the HTTP API, but it says that it does not support tools. Is that correct? Or am I doing something wrong? Let me know and I can share more details&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective_Head_5020"&gt; /u/Effective_Head_5020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpxsxk/deepseek_r1_8b_was_it_supposed_to_support_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpxsxk/deepseek_r1_8b_was_it_supposed_to_support_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lpxsxk/deepseek_r1_8b_was_it_supposed_to_support_tools/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T14:48:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpchao</id>
    <title>TimeCapsule-SLM - Open Source AI Deep Research Platform That Runs 100% in Your Browser!</title>
    <updated>2025-07-01T20:27:45+00:00</updated>
    <author>
      <name>/u/adssidhu86</name>
      <uri>https://old.reddit.com/user/adssidhu86</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lpchao/timecapsuleslm_open_source_ai_deep_research/"&gt; &lt;img alt="TimeCapsule-SLM - Open Source AI Deep Research Platform That Runs 100% in Your Browser!" src="https://preview.redd.it/ma9l20u8obaf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e9c105132c47fdebf61ac5c089af6603184d7003" title="TimeCapsule-SLM - Open Source AI Deep Research Platform That Runs 100% in Your Browser!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey👋&lt;br /&gt; Just launched &lt;a href="https://timecapsule.bubblspace.com/"&gt;TimeCapsule-SLM&lt;/a&gt; - an open source AI research platform that I think you'll find interesting. The key differentiator? Everything runs locally in your browser with complete privacy.🔥 What it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In-Browser RAG: Upload PDFs/documents, get AI insights without sending data to servers&lt;/li&gt; &lt;li&gt;TimeCapsule Sharing: Export/import complete research sessions as .timecapsule.json files&lt;/li&gt; &lt;li&gt;Multi-LLM Support: Works with Ollama, LM Studio, OpenAI APIs&lt;/li&gt; &lt;li&gt;Two main tools: DeepResearch (for novel idea generation) + Playground (for visual coding)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🔒 Privacy Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Zero server dependency after initial load&lt;/li&gt; &lt;li&gt;All processing happens locally&lt;/li&gt; &lt;li&gt;Your data never leaves your device&lt;/li&gt; &lt;li&gt;Works offline once models are loaded&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🎯 Perfect for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Researchers who need privacy-first AI tools&lt;/li&gt; &lt;li&gt;Teams wanting to share research sessions&lt;/li&gt; &lt;li&gt;Anyone building local AI workflows&lt;/li&gt; &lt;li&gt;People tired of cloud-dependent tools&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Live Demo: &lt;a href="https://timecapsule.bubblspace.com"&gt;https://timecapsule.bubblspace.com&lt;/a&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/thefirehacker/TimeCapsule-SLM"&gt;https://github.com/thefirehacker/TimeCapsule-SLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Ollama integration is particularly smooth - just enable CORS and you're ready to go with local models like qwen3:0.6b.Would love to hear your thoughts and feedback! Also happy to answer any technical questions about the implementation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adssidhu86"&gt; /u/adssidhu86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ma9l20u8obaf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpchao/timecapsuleslm_open_source_ai_deep_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lpchao/timecapsuleslm_open_source_ai_deep_research/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-01T20:27:45+00:00</published>
  </entry>
</feed>
