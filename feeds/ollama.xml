<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-04-02T08:25:23+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jo5rv9</id>
    <title>Menu bar Mac app?</title>
    <updated>2025-03-31T15:23:08+00:00</updated>
    <author>
      <name>/u/KenKaniffsmd</name>
      <uri>https://old.reddit.com/user/KenKaniffsmd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does there exist an Ollama UI where I can access and chat with the models I have downloaded from my menu bar?&lt;/p&gt; &lt;p&gt;I use chatbox right now which is nice, but I haven't been able to find any apps that do this, only with chatgtp.. Does anyone know if one exists? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KenKaniffsmd"&gt; /u/KenKaniffsmd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jo5rv9/menu_bar_mac_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jo5rv9/menu_bar_mac_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jo5rv9/menu_bar_mac_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-31T15:23:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jo9dpq</id>
    <title>NVIDIA RTX A4000 vs Quadro RTX 5000</title>
    <updated>2025-03-31T17:51:35+00:00</updated>
    <author>
      <name>/u/randomplebescite</name>
      <uri>https://old.reddit.com/user/randomplebescite</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomplebescite"&gt; /u/randomplebescite &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/buildapc/comments/1jo896e/nvidia_rtx_a4000_vs_quadro_rtx_5000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jo9dpq/nvidia_rtx_a4000_vs_quadro_rtx_5000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jo9dpq/nvidia_rtx_a4000_vs_quadro_rtx_5000/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-31T17:51:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnvnay</id>
    <title>Best local model which can process images and runs on 24GB GPU RAM?</title>
    <updated>2025-03-31T05:00:54+00:00</updated>
    <author>
      <name>/u/OkRide2660</name>
      <uri>https://old.reddit.com/user/OkRide2660</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to extend my local vibe voice model, so I can not just type with my voice, but also get nice LLM suggestions with my voice command and want to send the current screenshot as context.&lt;/p&gt; &lt;p&gt;I have a RTX 3090 and want to know what you consider the best ollama vision model which can run on this card (without being slow / swapping to system RAM etc). &lt;/p&gt; &lt;p&gt;Thank you! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OkRide2660"&gt; /u/OkRide2660 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnvnay/best_local_model_which_can_process_images_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnvnay/best_local_model_which_can_process_images_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jnvnay/best_local_model_which_can_process_images_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-31T05:00:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jojxzq</id>
    <title>App creation ai</title>
    <updated>2025-04-01T01:30:51+00:00</updated>
    <author>
      <name>/u/cuberhino</name>
      <uri>https://old.reddit.com/user/cuberhino</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anything I could use to create an app with ai? Like a web app or iOS app&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cuberhino"&gt; /u/cuberhino &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jojxzq/app_creation_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jojxzq/app_creation_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jojxzq/app_creation_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-01T01:30:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnrmd9</id>
    <title>I built an open-source NotebookLM alternative using Morphik</title>
    <updated>2025-03-31T01:07:54+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like using NoteBook LM, especially when I have a bunch of research papers I'm trying to extract insights from.&lt;/p&gt; &lt;p&gt;For example, if I'm implementing a new feature (like re-ranking) into Morphik, I like to create a notebook with some papers about it, and then compare those models with each other on different benchmarks.&lt;/p&gt; &lt;p&gt;I thought it would be cool to create a free, completely open-source version of it, so that I could use some private docs (like my journal!) and see if a NoteBook LM like system can help with that. I've found it to be insanely helpful, so I added a version of it onto the Morphik UI Component!&lt;/p&gt; &lt;p&gt;Try it out:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clone the repo at: &lt;a href="https://github.com/morphik-org/morphik-core"&gt;https://github.com/morphik-org/morphik-core&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Launch the UI component following instructions here: &lt;a href="https://docs.morphik.ai/using-morphik/morphik-ui"&gt;https://docs.morphik.ai/using-morphik/morphik-ui&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love to hear the &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; community's thoughts and feature requests!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnrmd9/i_built_an_opensource_notebooklm_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnrmd9/i_built_an_opensource_notebooklm_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jnrmd9/i_built_an_opensource_notebooklm_alternative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-31T01:07:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jon581</id>
    <title>Copying a Fine-Tuned Model to Another Machine</title>
    <updated>2025-04-01T04:23:12+00:00</updated>
    <author>
      <name>/u/BorisLovesMarishka</name>
      <uri>https://old.reddit.com/user/BorisLovesMarishka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I have been working on fine-tuning an Llama3 model and I want to share it with my colleagues to run on their own machines. What is the best way to send it to them? Would it just be to create a model file and send it? I would prefer not to send it up to Ollama for them to pull it down for themselves if possible. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BorisLovesMarishka"&gt; /u/BorisLovesMarishka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jon581/copying_a_finetuned_model_to_another_machine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jon581/copying_a_finetuned_model_to_another_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jon581/copying_a_finetuned_model_to_another_machine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-01T04:23:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jo46vx</id>
    <title>I built a voice assistant that types for me anywhere with context from screenshots</title>
    <updated>2025-03-31T14:14:31+00:00</updated>
    <author>
      <name>/u/OkRide2660</name>
      <uri>https://old.reddit.com/user/OkRide2660</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Simply hold a button and aks your question:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;your spoken text gets transcribed by a locally running whisper model&lt;/li&gt; &lt;li&gt;a screenshot is made &lt;/li&gt; &lt;li&gt;both is sent to an ollama model of your choice (defaults to Gemma3:27B)&lt;/li&gt; &lt;li&gt;the llm answer is typed into your keyboard&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So you can e. g. say 'reply to this email' and it sees the email and types your response.&lt;/p&gt; &lt;p&gt;Try it out and let me know what you think:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mpaepper/vibevoice"&gt;https://github.com/mpaepper/vibevoice&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OkRide2660"&gt; /u/OkRide2660 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jo46vx/i_built_a_voice_assistant_that_types_for_me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jo46vx/i_built_a_voice_assistant_that_types_for_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jo46vx/i_built_a_voice_assistant_that_types_for_me/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-31T14:14:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1johtit</id>
    <title>Responses are different</title>
    <updated>2025-03-31T23:45:48+00:00</updated>
    <author>
      <name>/u/Satoshi-Wasabi8520</name>
      <uri>https://old.reddit.com/user/Satoshi-Wasabi8520</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Responses is different using Ollama in console and Ollama models in open-webui. The response in console is straight forward and correct while in open-webui sometimes incorrect, same model, same prompt. Any idea?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Satoshi-Wasabi8520"&gt; /u/Satoshi-Wasabi8520 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1johtit/responses_are_different/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1johtit/responses_are_different/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1johtit/responses_are_different/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-31T23:45:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jox9zg</id>
    <title>Ollama python - How to use stream future with tools</title>
    <updated>2025-04-01T14:41:53+00:00</updated>
    <author>
      <name>/u/Icy-Comb6059</name>
      <uri>https://old.reddit.com/user/Icy-Comb6059</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. My current issue is my current code was not made for the intent of tools but now that I have to use it I am unable to recieve tool_calls from the output. If its not possible i am fine with using ollama without stream feature but would be really useful.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def communucateOllamaTools(systemPrompt, UserPrompt,model,tools,history = None): if history is None: history = [{'role': 'system', 'content': systemPrompt}] try: msgs = history msgs.append({'role': 'user', 'content': UserPrompt}) stream = chat( model=model, messages=msgs, stream=True, tools=tools # input tools as a list of tools ) outcome = &amp;quot;&amp;quot; for chunk in stream: print(chunk['message']['content'], end='', flush=True) outcome += chunk['message']['content'] msgs.append({'role': 'assistant', 'content': outcome}) return outcome, msgs except Exception as e: # error handling print(e) return e &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy-Comb6059"&gt; /u/Icy-Comb6059 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jox9zg/ollama_python_how_to_use_stream_future_with_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jox9zg/ollama_python_how_to_use_stream_future_with_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jox9zg/ollama_python_how_to_use_stream_future_with_tools/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-01T14:41:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp15t5</id>
    <title>Is my ollama using gpu on mac?</title>
    <updated>2025-04-01T17:18:32+00:00</updated>
    <author>
      <name>/u/Dear-Enthusiasm-9766</name>
      <uri>https://old.reddit.com/user/Dear-Enthusiasm-9766</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How do I know if my ollama is using my apple silicon gpu? If the llm is using cpu for inference then how do i change it to gpu. The mac I'm using has m2 chip.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Enthusiasm-9766"&gt; /u/Dear-Enthusiasm-9766 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jp15t5/is_my_ollama_using_gpu_on_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jp15t5/is_my_ollama_using_gpu_on_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jp15t5/is_my_ollama_using_gpu_on_mac/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-01T17:18:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jorrgn</id>
    <title>Are RDNA4 GPUs supported yet?</title>
    <updated>2025-04-01T09:51:21+00:00</updated>
    <author>
      <name>/u/T_Play</name>
      <uri>https://old.reddit.com/user/T_Play</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was wondering if Hardware Acceleration with RDNA4 GPUs (9070/9070 XT) is supported as of now. Because when I install ollama locally (Fedora 41) the installer states &amp;quot;AMD GPU ready&amp;quot; but when running a model, it clearly doesn't utilize my GPU&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/T_Play"&gt; /u/T_Play &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jorrgn/are_rdna4_gpus_supported_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jorrgn/are_rdna4_gpus_supported_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jorrgn/are_rdna4_gpus_supported_yet/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-01T09:51:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jo7bzj</id>
    <title>I want an LLM that responds with “I don’t know. How could I possibly do that or know that?” Instead of going into hallucinations</title>
    <updated>2025-03-31T16:28:09+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any recommendations? I tried a honest system prompt, but they are like hardwired to answer at any cost. &lt;/p&gt; &lt;p&gt;Reasoning ones are even worse.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jo7bzj/i_want_an_llm_that_responds_with_i_dont_know_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jo7bzj/i_want_an_llm_that_responds_with_i_dont_know_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jo7bzj/i_want_an_llm_that_responds_with_i_dont_know_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-31T16:28:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1josb2j</id>
    <title>New to Ollama, want to integrate it more but keep it portable.</title>
    <updated>2025-04-01T10:28:05+00:00</updated>
    <author>
      <name>/u/elkbond</name>
      <uri>https://old.reddit.com/user/elkbond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, due to work reasons I can’t install applications without approval. So I made a portable version of ollama and I am using llama 3.1 and Deepseek currently just to try out functionality.&lt;/p&gt; &lt;p&gt;I want to configure it to be more assistant-like, such as able to add things to my calendar. Remind me about things, just generally be an always on assistant for research and PA duties.&lt;/p&gt; &lt;p&gt;I don’t mind adding a few programs at home to achieve this, but the biggest issue is how much space these take up and the fact if I want to take my ‘PA’ to work I need to have it run from the drive only. So currently at work I am just command line-ing it, but at home I use MSTY.&lt;/p&gt; &lt;p&gt;Anyone else achieved anything like the above? Also I am average or below-average at python and coding in general. I can get about but use guides aalotttt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elkbond"&gt; /u/elkbond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1josb2j/new_to_ollama_want_to_integrate_it_more_but_keep/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1josb2j/new_to_ollama_want_to_integrate_it_more_but_keep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1josb2j/new_to_ollama_want_to_integrate_it_more_but_keep/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-01T10:28:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jovn1h</id>
    <title>Server Rack is coming together slowly but surely!</title>
    <updated>2025-04-01T13:31:05+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jovn1h/server_rack_is_coming_together_slowly_but_surely/"&gt; &lt;img alt="Server Rack is coming together slowly but surely!" src="https://preview.redd.it/orkd48uv48se1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fabaddb5591751181f445af71c42983676bd8b0a" title="Server Rack is coming together slowly but surely!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/orkd48uv48se1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jovn1h/server_rack_is_coming_together_slowly_but_surely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jovn1h/server_rack_is_coming_together_slowly_but_surely/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-01T13:31:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1joaaoj</id>
    <title>This project might be the most usable app for using models and image generation locally</title>
    <updated>2025-03-31T18:28:11+00:00</updated>
    <author>
      <name>/u/k1sh0r</name>
      <uri>https://old.reddit.com/user/k1sh0r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1joaaoj/this_project_might_be_the_most_usable_app_for/"&gt; &lt;img alt="This project might be the most usable app for using models and image generation locally" src="https://preview.redd.it/ypi8dajvi2se1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7fd295049f64e5b1474b5a2b5c643ee6a286c426" title="This project might be the most usable app for using models and image generation locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I came across this project called Clara in this subreddit few days ago, honestly it was so easy to setup and run. Previously I tried Open WebUI and it was too technical for me (as a non-tech person) to setup docker and all. I can see new improvements and in-app updates frequently. May be give it a try.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k1sh0r"&gt; /u/k1sh0r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ypi8dajvi2se1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1joaaoj/this_project_might_be_the_most_usable_app_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1joaaoj/this_project_might_be_the_most_usable_app_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-31T18:28:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jozu3w</id>
    <title>Is there a difference in performance and refinement of ollama api endpoints /api/chat and /v1/chat/completions</title>
    <updated>2025-04-01T16:25:26+00:00</updated>
    <author>
      <name>/u/binuuday</name>
      <uri>https://old.reddit.com/user/binuuday</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama supports the OpenAI API spec and the original Ollama spec (api/chat). In the open api spec, the chat completion example is &lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl http://localhost:11434/v1/chat/completions \ -H &amp;quot;Content-Type: application/json&amp;quot; \ -d '{ &amp;quot;model&amp;quot;: &amp;quot;qwen:14b&amp;quot;, &amp;quot;messages&amp;quot;: [ { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What is an apple&amp;quot; } ] }' curl http://localhost:11434/api/chat -d '{ &amp;quot;model&amp;quot;: &amp;quot;qwen:14b&amp;quot;, &amp;quot;stream&amp;quot;: false, &amp;quot;messages&amp;quot;: [ { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What is an apple&amp;quot; } ] }' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I am seeing that the /v1/chat/completions api always gives better refined output, in normal queries and when asking for programming queries.&lt;/p&gt; &lt;p&gt;Initially I thought the /v1/chat/completions is a wrapper around /api/chat. A quick code inspection on ollama repo, seems to indicate they have totally different pathways. &lt;/p&gt; &lt;p&gt;Does anyone have info on this. I checked the bug list on ollama repo, did not find anything of help. The documentation also does not indicate any refinements. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/binuuday"&gt; /u/binuuday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jozu3w/is_there_a_difference_in_performance_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jozu3w/is_there_a_difference_in_performance_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jozu3w/is_there_a_difference_in_performance_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-01T16:25:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp3rbz</id>
    <title>GenAI Job Roles</title>
    <updated>2025-04-01T19:01:23+00:00</updated>
    <author>
      <name>/u/Electrical-Button635</name>
      <uri>https://old.reddit.com/user/Electrical-Button635</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Good people of Reddit.&lt;/p&gt; &lt;p&gt;As i recently transitioning from a full stack dev (laravel LAMP stack) to GenAI role internal transition. &lt;/p&gt; &lt;p&gt;My main task is to integrate llms using frameworks like langchain and langraph. Llm Monitoring using langsmith.&lt;/p&gt; &lt;p&gt;Implementation of RAGs using ChromaDB to cover business specific usecases mainly to reduce hallucinations in responses. Still learning tho.&lt;/p&gt; &lt;p&gt;My next step is to learn langsmith for Agents and tool calling And learn &amp;quot;Fine-tuning a model&amp;quot; then gradually move to multi-modal implementations usecases such as images and stuff.&lt;/p&gt; &lt;p&gt;As it's been roughly 2months as of now i feel like I'm still majorly doing webdev but pipelining llm calls for smart saas.&lt;/p&gt; &lt;p&gt;I Mainly work in Django and fastAPI.&lt;/p&gt; &lt;p&gt;My motive is to switch for a proper genAi role in maybe 3-4 months.&lt;/p&gt; &lt;p&gt;People working in a genAi roles what's your actual day like means do you also deals with above topics or is it totally different story. Sorry i don't have much knowledge in this field I'm purely driven by passion here so i might sound naive.&lt;/p&gt; &lt;p&gt;I'll be glad if you could suggest what topics should i focus on and just some insights in this field I'll be forever grateful. Or maybe some great resources which can help me out here.&lt;/p&gt; &lt;p&gt;Thanks for your time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electrical-Button635"&gt; /u/Electrical-Button635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jp3rbz/genai_job_roles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jp3rbz/genai_job_roles/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jp3rbz/genai_job_roles/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-01T19:01:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp2po7</id>
    <title>What is the JSON Schema Format for Ollama feature in 0.6?</title>
    <updated>2025-04-01T18:19:56+00:00</updated>
    <author>
      <name>/u/XdtTransform</name>
      <uri>https://old.reddit.com/user/XdtTransform</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The &lt;a href="https://github.com/open-webui/open-webui/releases"&gt;release notes&lt;/a&gt; for 0.6 mention &amp;quot;JSON Schema Format for Ollama&amp;quot; as a new feature. &lt;/p&gt; &lt;p&gt;Specifically it says:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;JSON Schema Format for Ollama: Added support for defining the format using JSON schema in Ollama-compatible models, improving flexibility and validation of model outputs.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I've been providing the JSON schema since 0.5, and it always worked fine. Returning JSON in the exact format that I want. &lt;/p&gt; &lt;p&gt;What exactly is this new feature?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XdtTransform"&gt; /u/XdtTransform &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jp2po7/what_is_the_json_schema_format_for_ollama_feature/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jp2po7/what_is_the_json_schema_format_for_ollama_feature/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jp2po7/what_is_the_json_schema_format_for_ollama_feature/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-01T18:19:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpg0k2</id>
    <title>Someone stuck Ollama on a distro</title>
    <updated>2025-04-02T04:11:03+00:00</updated>
    <author>
      <name>/u/R0gueSch0lar</name>
      <uri>https://old.reddit.com/user/R0gueSch0lar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From what I can tell so far, theyve preconfigured a few apps and are going for out of the box functionality. I booted from a usb and had a VSCode knockoff generating code in seconds. &lt;a href="https://sourceforge.net/projects/pocketai/files/pocketai-2025.04.02-x64.iso/download"&gt;https://sourceforge.net/projects/pocketai/files/pocketai-2025.04.02-x64.iso/download&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R0gueSch0lar"&gt; /u/R0gueSch0lar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpg0k2/someone_stuck_ollama_on_a_distro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpg0k2/someone_stuck_ollama_on_a_distro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jpg0k2/someone_stuck_ollama_on_a_distro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T04:11:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp1q07</id>
    <title>Ollama parallel request tuning on M4 MacMini</title>
    <updated>2025-04-01T17:41:13+00:00</updated>
    <author>
      <name>/u/icbts</name>
      <uri>https://old.reddit.com/user/icbts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jp1q07/ollama_parallel_request_tuning_on_m4_macmini/"&gt; &lt;img alt="Ollama parallel request tuning on M4 MacMini" src="https://external-preview.redd.it/gIY3BxKRS1Rg5xMnJJyphiZvjRXHfeEXBsQzC2O1sSY.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3440b1ace8a83f475823d8301e4ba2f7d1082b0d" title="Ollama parallel request tuning on M4 MacMini" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In this video we tune Ollama's Parallel Request settings with several LLMs, if your model is somewhat small (7B and below), tuning towards 16 to 32 contexts will give you much better throughput performance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/icbts"&gt; /u/icbts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=hAHCQR-kD0U"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jp1q07/ollama_parallel_request_tuning_on_m4_macmini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jp1q07/ollama_parallel_request_tuning_on_m4_macmini/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-01T17:41:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jphiq2</id>
    <title>Server Help</title>
    <updated>2025-04-02T05:47:21+00:00</updated>
    <author>
      <name>/u/Far-Guarantee2097</name>
      <uri>https://old.reddit.com/user/Far-Guarantee2097</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to upload ollama's mistral model to my college server, but for some reason it isnt accepting the model path in my MacBook Pro.&lt;/p&gt; &lt;p&gt;I pulled the path of the models from my Finder and then used that, but it says the path doesn't exist. Can anyone let me know why this is happening or what else can I try?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Guarantee2097"&gt; /u/Far-Guarantee2097 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jphiq2/server_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jphiq2/server_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jphiq2/server_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T05:47:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpixcu</id>
    <title>(HELP) Building a RAG system</title>
    <updated>2025-04-02T07:28:23+00:00</updated>
    <author>
      <name>/u/Rambr1516</name>
      <uri>https://old.reddit.com/user/Rambr1516</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jpixcu/help_building_a_rag_system/"&gt; &lt;img alt="(HELP) Building a RAG system" src="https://b.thumbs.redditmedia.com/Z-bgtw2jkOM95ZHe1neHR0K4AGtjlhmQjsAxiV8kTKE.jpg" title="(HELP) Building a RAG system" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone - I need some help. I am a very beginner programmer with very VERY basic knowledge and I want to set up a RAG system with my obsidian vault (hundreds of markdown files totaling over 200k words) I also only have a machine with 16gb of ram (m1 pro macbook) but would love to use this RAG with local models and my open router integrations. &lt;/p&gt; &lt;p&gt;As I said I am a noob with programming, but absolutely not a noob with computer, I want this to be something I can learn and then update as time goes on, and especially update when I get a beefier system (MORE RAM). Ideally I would love to get on a call with someone, or just get a place to start learning. ChatGPT said something about chromaDB and LangChain but that is all greek to me. &lt;/p&gt; &lt;p&gt;Thank you so much in advance - if you are a pro at this shit lmk, im broke but a call would take time (like an hour or less) and time is money :) &lt;/p&gt; &lt;p&gt;have a good day&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b25wrgh1jdse1.png?width=278&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=114ac90de061b23d8deac469d69d55113ceda0f6"&gt;lots of words lol&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/khcyaqi2jdse1.png?width=566&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62607e7283571d3e255dbfdf8f9740350b3823d4"&gt;DISREGARD ATTACHMENTS - I only want MD files&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rambr1516"&gt; /u/Rambr1516 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpixcu/help_building_a_rag_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpixcu/help_building_a_rag_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jpixcu/help_building_a_rag_system/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T07:28:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpjntg</id>
    <title>I made an Ollama hub</title>
    <updated>2025-04-02T08:24:58+00:00</updated>
    <author>
      <name>/u/retoor42</name>
      <uri>https://old.reddit.com/user/retoor42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a hub where you can donate your Ollama instance so other people can use it. To use the shared servers you can just use the default Ollama api clients but set the host to &lt;a href="https://ollama.molodetz.nl"&gt;https://ollama.molodetz.nl&lt;/a&gt; and everything will work fine. It will pick a random server to execute your query on.&lt;/p&gt; &lt;p&gt;Availability provided by current server(s) online is here: &lt;a href="https://ollama.molodetz.nl/models"&gt;https://ollama.molodetz.nl/models&lt;/a&gt; &lt;/p&gt; &lt;p&gt;How to donate your own Ollama instance is here with code examples: &lt;a href="https://ollama.molodetz.nl"&gt;https://ollama.molodetz.nl&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It would be cool if we could make a free AI for everyone service! Together we share the load. &lt;/p&gt; &lt;p&gt;As you guys can see, designing / setting is not my best skill. Tips are welcome. I think the concept of this, is cool. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/retoor42"&gt; /u/retoor42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpjntg/i_made_an_ollama_hub/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jpjntg/i_made_an_ollama_hub/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jpjntg/i_made_an_ollama_hub/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T08:24:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1joyhf5</id>
    <title>I made this simple local RAG example using Langchain, ChromaDB &amp; Ollama</title>
    <updated>2025-04-01T15:31:16+00:00</updated>
    <author>
      <name>/u/yussufbyk</name>
      <uri>https://old.reddit.com/user/yussufbyk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made this after seeing that basically nobody on the internet have made a readable and clean code about this that was still working.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/yussufbiyik/langchain-chromadb-rag-example"&gt;https://github.com/yussufbiyik/langchain-chromadb-rag-example&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to contribute or test it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yussufbyk"&gt; /u/yussufbyk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1joyhf5/i_made_this_simple_local_rag_example_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1joyhf5/i_made_this_simple_local_rag_example_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1joyhf5/i_made_this_simple_local_rag_example_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-01T15:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jphngr</id>
    <title>How can I reduce hallucinations with ollama</title>
    <updated>2025-04-02T05:56:12+00:00</updated>
    <author>
      <name>/u/Pure-Caramel1216</name>
      <uri>https://old.reddit.com/user/Pure-Caramel1216</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to build an app using ollama api with the chat endpoint but the thing is it sometimes hallucinates a lot, how can make it so it does not hallucinatite (or hallucinates less)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pure-Caramel1216"&gt; /u/Pure-Caramel1216 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jphngr/how_can_i_reduce_hallucinations_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jphngr/how_can_i_reduce_hallucinations_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jphngr/how_can_i_reduce_hallucinations_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-02T05:56:12+00:00</published>
  </entry>
</feed>
