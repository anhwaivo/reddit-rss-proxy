<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-31T01:33:02+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1idlh6k</id>
    <title>local model for animated image</title>
    <updated>2025-01-30T12:05:16+00:00</updated>
    <author>
      <name>/u/24Gameplay_</name>
      <uri>https://old.reddit.com/user/24Gameplay_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm looking for a local LLM that can generate animated pictures similar to the style shown in the video below. If you have any recommendations, please let me know!&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=wn0IyvGBeUI"&gt;https://www.youtube.com/watch?v=wn0IyvGBeUI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/24Gameplay_"&gt; /u/24Gameplay_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idlh6k/local_model_for_animated_image/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idlh6k/local_model_for_animated_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idlh6k/local_model_for_animated_image/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T12:05:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1icyl2l</id>
    <title>Will there ever be uncensored self hosted AI?</title>
    <updated>2025-01-29T17:00:55+00:00</updated>
    <author>
      <name>/u/mshriver2</name>
      <uri>https://old.reddit.com/user/mshriver2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tried out Ollama today for the first time as I was excited at the possibility of having a fully uncensored and unrestricted AI that could answer any question. Unfortunately even self hosted it is just as censored as Chat-GPT or any other large AI model. Do you think we will ever have a fully open source completely unrestricted AI? I don't understand how a company gets to decide what code runs or doesn't run on my own hardware.&lt;/p&gt; &lt;p&gt;Apologies for the rant in advance.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I should be the one deciding what is &amp;quot;legal&amp;quot; or &amp;quot;ethical&amp;quot; not my computer.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model used for testing: DeepSeek-R1-Distill-Qwen-32B&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mshriver2"&gt; /u/mshriver2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icyl2l/will_there_ever_be_uncensored_self_hosted_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icyl2l/will_there_ever_be_uncensored_self_hosted_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icyl2l/will_there_ever_be_uncensored_self_hosted_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T17:00:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1idlr85</id>
    <title>R1 Reasoning Effort for the Open-Webui</title>
    <updated>2025-01-30T12:22:15+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1idflkk/r1_reasoning_effort_for_the_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idlr85/r1_reasoning_effort_for_the_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idlr85/r1_reasoning_effort_for_the_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T12:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1idm9ou</id>
    <title>Help needed - Creating Ollama mode from GGUF</title>
    <updated>2025-01-30T12:52:12+00:00</updated>
    <author>
      <name>/u/fall2</name>
      <uri>https://old.reddit.com/user/fall2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Im freshly new to Ollama. I wold like to be able to load the following model: &lt;a href="https://huggingface.co/TheBloke/firefly-mixtral-8x7b-GGUF"&gt;https://huggingface.co/TheBloke/firefly-mixtral-8x7b-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If I understand completely;&lt;/p&gt; &lt;p&gt;Step 1: I need a file named Modelfile(without extension. With the following content:&lt;/p&gt; &lt;p&gt;FROM &amp;quot;./firefly-mixtral-8x7b.Q2_K.gguf&amp;quot;&lt;/p&gt; &lt;p&gt;Step 2: After I need to run the following command prompt: lama create Mixtral -f Modelfile&lt;/p&gt; &lt;p&gt;Am I missing something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fall2"&gt; /u/fall2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idm9ou/help_needed_creating_ollama_mode_from_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idm9ou/help_needed_creating_ollama_mode_from_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idm9ou/help_needed_creating_ollama_mode_from_gguf/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T12:52:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ido2en</id>
    <title>Where to find Model system requirements ?</title>
    <updated>2025-01-30T14:23:41+00:00</updated>
    <author>
      <name>/u/RedditNoobie777</name>
      <uri>https://old.reddit.com/user/RedditNoobie777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I upgrade I GPU from 4GB VRAM to 16GB what models will I be able to run. what token and parameter number ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RedditNoobie777"&gt; /u/RedditNoobie777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ido2en/where_to_find_model_system_requirements/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ido2en/where_to_find_model_system_requirements/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ido2en/where_to_find_model_system_requirements/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T14:23:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1id92b2</id>
    <title>Cursor + Ollama -- Help a Blind Guy?</title>
    <updated>2025-01-30T00:18:10+00:00</updated>
    <author>
      <name>/u/mdizak</name>
      <uri>https://old.reddit.com/user/mdizak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Life decided to play a funny prank on me years ago and make me suddenly and totally blind. Can anyone quickly help here?&lt;/p&gt; &lt;p&gt;Looking to get Cursor IDE working with local install of Ollama. Within Cursor, I can go into Preferences -&amp;gt; Models screen and see the various textboxes for API keys, and can also see just under the OpenAI API Key textbox there's a link to override the API URL. I'm assuming this is what I want to flip the API endpoint to my local Ollama install.&lt;/p&gt; &lt;p&gt;For the life of me though, I'm unable to click on that link via screen reader, and posting on &lt;a href="/r/blind"&gt;/r/blind&lt;/a&gt; was of no help. So let's go with manual settings change route. &lt;/p&gt; &lt;p&gt;Found the settings file located at ~APP_CONFIG/Cursor/User/settings.json. On Linux Mint at least, this means at: ~/.config/Cursor/User/settings.json&lt;/p&gt; &lt;p&gt;Unfortunately, this file only lists modified settings instead of all settings. This is where I need help. &lt;/p&gt; &lt;p&gt;Could one of you kind souls that has Cursor running with local Ollama look in your settings.json file, and look for the settings that seem appropriate? Just need to send the API endpoint to Ollama's of &lt;a href="http://127.0.0.1:11434/api/generate"&gt;http://127.0.0.1:11434/api/generate&lt;/a&gt;, and put &amp;quot;deepseek-r1&amp;quot; as the model name. Looking through the setting names, it should be pretty obvious, plus any settings that looks like it's needed to enable / activate it.&lt;/p&gt; &lt;p&gt;If anyone is kind enough to reply with those setting names, would be greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mdizak"&gt; /u/mdizak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id92b2/cursor_ollama_help_a_blind_guy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id92b2/cursor_ollama_help_a_blind_guy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1id92b2/cursor_ollama_help_a_blind_guy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T00:18:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1idq0ou</id>
    <title>How do I set context size when using openai compatible API?</title>
    <updated>2025-01-30T15:52:48+00:00</updated>
    <author>
      <name>/u/sobolanul11</name>
      <uri>https://old.reddit.com/user/sobolanul11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using openai library (I already had it setup in the project, I just added another url). I added &lt;/p&gt; &lt;pre&gt;&lt;code&gt;options: { num_ctx: 16384 } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;to the request with no result &lt;/p&gt; &lt;p&gt;I also run the ollama with /set parameter num_ctx 8192, but with no luck&lt;/p&gt; &lt;p&gt;Every request is truncated at 2048, I see this in logs:&lt;br /&gt; &lt;code&gt;level=WARN source=runner.go:129 msg=&amp;quot;truncating input prompt&amp;quot; limit=2048 prompt=3246 keep=5 new=2048&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sobolanul11"&gt; /u/sobolanul11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idq0ou/how_do_i_set_context_size_when_using_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idq0ou/how_do_i_set_context_size_when_using_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idq0ou/how_do_i_set_context_size_when_using_openai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T15:52:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1idjwkh</id>
    <title>how to use Ollama with C++</title>
    <updated>2025-01-30T10:15:50+00:00</updated>
    <author>
      <name>/u/Reasonable-Falcon470</name>
      <uri>https://old.reddit.com/user/Reasonable-Falcon470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi i am bad with python i just cant i tried and i cant find any C++ instructions for Ollama&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable-Falcon470"&gt; /u/Reasonable-Falcon470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idjwkh/how_to_use_ollama_with_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idjwkh/how_to_use_ollama_with_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idjwkh/how_to_use_ollama_with_c/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T10:15:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1idf5ez</id>
    <title>How can I force Ollama to use the cpu instead of gpu?</title>
    <updated>2025-01-30T05:06:19+00:00</updated>
    <author>
      <name>/u/ResponsibleTruck4717</name>
      <uri>https://old.reddit.com/user/ResponsibleTruck4717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know I can force it use cpu instead of gpu when I'm using docker but I'm looking for solution without docker.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleTruck4717"&gt; /u/ResponsibleTruck4717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idf5ez/how_can_i_force_ollama_to_use_the_cpu_instead_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idf5ez/how_can_i_force_ollama_to_use_the_cpu_instead_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idf5ez/how_can_i_force_ollama_to_use_the_cpu_instead_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T05:06:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1idmtrv</id>
    <title>Deepseek on an RTX 4060 ti 8gb</title>
    <updated>2025-01-30T13:22:25+00:00</updated>
    <author>
      <name>/u/whymeimbusysleeping</name>
      <uri>https://old.reddit.com/user/whymeimbusysleeping</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi gang. Can I get a recommendation on which model to use that would suit my GPU? And how to install it?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/whymeimbusysleeping"&gt; /u/whymeimbusysleeping &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idmtrv/deepseek_on_an_rtx_4060_ti_8gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idmtrv/deepseek_on_an_rtx_4060_ti_8gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idmtrv/deepseek_on_an_rtx_4060_ti_8gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T13:22:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1idlbhw</id>
    <title>What's the best model that can be comfortably run on a normal PC?</title>
    <updated>2025-01-30T11:55:30+00:00</updated>
    <author>
      <name>/u/hn-mc</name>
      <uri>https://old.reddit.com/user/hn-mc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've installed llama3.2 as this is what Ollama itself recommend at the start, so I just installed the default option, but it seems kind of bad, especially in comparison to current popular models that we access online. Not only that, its skills in non-English languages are abysmal. In English, it's sort of decent, not that bad, but I tried it in Serbian, it doesn't even properly speak the language.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hn-mc"&gt; /u/hn-mc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idlbhw/whats_the_best_model_that_can_be_comfortably_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idlbhw/whats_the_best_model_that_can_be_comfortably_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idlbhw/whats_the_best_model_that_can_be_comfortably_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T11:55:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1id6y97</id>
    <title>I feel bad for the AI lol after seeing its chain of thought. 😭</title>
    <updated>2025-01-29T22:44:12+00:00</updated>
    <author>
      <name>/u/Tricky_Reflection_75</name>
      <uri>https://old.reddit.com/user/Tricky_Reflection_75</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1id6y97/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt; &lt;img alt="I feel bad for the AI lol after seeing its chain of thought. 😭" src="https://b.thumbs.redditmedia.com/enUEH3eKOb0e3umQhJmhNKG1mhLeM6dFReIv_8oKkzc.jpg" title="I feel bad for the AI lol after seeing its chain of thought. 😭" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/910r117yg0ge1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c699f0b744adc486372072f5a73072f6d893f97e"&gt;https://preview.redd.it/910r117yg0ge1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c699f0b744adc486372072f5a73072f6d893f97e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tricky_Reflection_75"&gt; /u/Tricky_Reflection_75 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id6y97/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id6y97/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1id6y97/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T22:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1idu6cm</id>
    <title>System String from previous instance wont go away</title>
    <updated>2025-01-30T18:46:30+00:00</updated>
    <author>
      <name>/u/Murphys_Project</name>
      <uri>https://old.reddit.com/user/Murphys_Project</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1idu6cm/system_string_from_previous_instance_wont_go_away/"&gt; &lt;img alt="System String from previous instance wont go away" src="https://a.thumbs.redditmedia.com/1tJzsgo66gwtkOMQQRXjstDacuZ80N9Ryz8EbFBG3V8.jpg" title="System String from previous instance wont go away" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I was messing around with ollama /set system &amp;lt;string&amp;gt; and made the deepseek-r1:7b model type code with ``` at the beginning and end of code blocks for implementation as a discord bot. I cleared the system string for the setting of ``` and ``` at the beginning and end of the code but the issue still persisted. It seems that when I /set system, it applied it globally to all models including the 12b and 8b model. I tried messing with the python library for asking the deepseek model a prompt via that api and the code it gives me still has ``` attached to it. I tried uninstalling and reinstalling ollama and the models to try to fix this and it still does not work. I even set the system parameters to remove ``` from code and it ignores it. When I uninstalled ollama, I stared at its file directories to make sure it was removed and it was, even the temp file. I genuinely don't know what up with my issue. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zx1rs1ynf6ge1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=285dc1c3b92b9a5d7189cfccfa5a7dffa9778f0c"&gt;https://preview.redd.it/zx1rs1ynf6ge1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=285dc1c3b92b9a5d7189cfccfa5a7dffa9778f0c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f06cpl0nf6ge1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9c101f7ca53c3cf8b18ce0d4fa6e9ff1c6722a0a"&gt;https://preview.redd.it/f06cpl0nf6ge1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9c101f7ca53c3cf8b18ce0d4fa6e9ff1c6722a0a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Murphys_Project"&gt; /u/Murphys_Project &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idu6cm/system_string_from_previous_instance_wont_go_away/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idu6cm/system_string_from_previous_instance_wont_go_away/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idu6cm/system_string_from_previous_instance_wont_go_away/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T18:46:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1idvl47</id>
    <title>Model for language learning?</title>
    <updated>2025-01-30T19:44:33+00:00</updated>
    <author>
      <name>/u/KiRa937</name>
      <uri>https://old.reddit.com/user/KiRa937</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m currently learning Japanese and I want to use AI in my studies. The most obvious way is to ask “explain word A in context of sentence B”, but I also thought maybe it is possible to analyze the material in my target language. Something like “here’s a bunch of texts, order them by grammar difficulty”. Or maybe “I’m interested in vocabulary of topic X, so order texts by level of use of said vocabulary”. So what model should I use for language learning? And for future reference what models would work for different languages if there’s no “one fits all” model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KiRa937"&gt; /u/KiRa937 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idvl47/model_for_language_learning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idvl47/model_for_language_learning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idvl47/model_for_language_learning/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T19:44:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1idsvfd</id>
    <title>Open WebUI not working after 0.5.6, "Network Problem" 400 Bad Request (Docker)</title>
    <updated>2025-01-30T17:52:08+00:00</updated>
    <author>
      <name>/u/ucffool</name>
      <uri>https://old.reddit.com/user/ucffool</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Even since I pulled down 0.5.6 or 0.5.7, the container runs and seems fine, but talking to Ollama or with Together.ai both fail. It can retrieve the list of models just fine, but it seems to be an issue with completions.&lt;/p&gt; &lt;p&gt;Anyone else having this problem? Do I need to blow it away and do a completely clean install, loading settings for exports?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;2025-01-30 10:50:03 INFO: connection rejected (400 Bad Request) 2025-01-30 10:50:03 INFO: connection closed &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ucffool"&gt; /u/ucffool &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idsvfd/open_webui_not_working_after_056_network_problem/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idsvfd/open_webui_not_working_after_056_network_problem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idsvfd/open_webui_not_working_after_056_network_problem/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T17:52:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie1jzx</id>
    <title>Remove &lt;think&gt; tags?</title>
    <updated>2025-01-31T00:01:59+00:00</updated>
    <author>
      <name>/u/midlivecrisis</name>
      <uri>https://old.reddit.com/user/midlivecrisis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apologies in advance if this has already been answered. Is there a way to force the smaller Deepseek models (7b, 14b running on Ollama) to NOT return the &amp;lt;think&amp;gt; tags and thinking content - and to only return the end response? Is there a parameter I missed where you can disable that content? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/midlivecrisis"&gt; /u/midlivecrisis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1jzx/remove_think_tags/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1jzx/remove_think_tags/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie1jzx/remove_think_tags/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T00:01:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie1s1h</id>
    <title>Running DeepSeek-R1-GGUF from Unsloth with Ollama</title>
    <updated>2025-01-31T00:12:35+00:00</updated>
    <author>
      <name>/u/WitcherSanek</name>
      <uri>https://old.reddit.com/user/WitcherSanek</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How to run this model on Windows with ollama &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-Q2%5C_K%5C_XL"&gt;https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-Q2\_K\_XL&lt;/a&gt;?&lt;/p&gt; &lt;p&gt;Huggingface has instruction &amp;quot;ollama run hf.co/unsloth/DeepSeek-R1-GGUF:Q2_K_XL&amp;quot; which leads to error &amp;quot;Error: pull model manifest: 400: The specified repository contains sharded GGUF. Ollama does not support this yet. Follow this issue for more info:&amp;quot;.&lt;/p&gt; &lt;p&gt;Downloading this files locally and running &amp;quot;llama-gguf-split --merge&amp;quot; leads to instant creation of empty file without any error or info message.&lt;/p&gt; &lt;p&gt;Manual concatenation with copy -b allows model to be imported, but &amp;quot;ollama run R1:latest&amp;quot; leads to error &amp;quot;Error: llama runner process has terminated: error loading model: invalid split file: D:\Ollama\blobs\sha256-311b7e2b72da29daffbac5e5f5df9353b1b3be9879d22d1dc498ece99529cfe5&amp;quot;.&lt;/p&gt; &lt;p&gt;What is wrong with my attempts? Am i missing something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WitcherSanek"&gt; /u/WitcherSanek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1s1h/running_deepseekr1gguf_from_unsloth_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1s1h/running_deepseekr1gguf_from_unsloth_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie1s1h/running_deepseekr1gguf_from_unsloth_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T00:12:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1idpjtj</id>
    <title>What would be the simplest way to train deepseek model on self hosted server.</title>
    <updated>2025-01-30T15:31:52+00:00</updated>
    <author>
      <name>/u/shaxadhere</name>
      <uri>https://old.reddit.com/user/shaxadhere</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to train model on specific set of data, I have the data in raw text and I have questions and answers from that raw text.&lt;/p&gt; &lt;p&gt;Is there any way I can train my model on all of that data.&lt;/p&gt; &lt;p&gt;Documents in total are 400,000 pages, and questions and answers are around 1 million+&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shaxadhere"&gt; /u/shaxadhere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idpjtj/what_would_be_the_simplest_way_to_train_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idpjtj/what_would_be_the_simplest_way_to_train_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idpjtj/what_would_be_the_simplest_way_to_train_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T15:31:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1idv02o</id>
    <title>Has anyone been using the base M4 Mac Mini as an Ollama server and want to share their mileage?</title>
    <updated>2025-01-30T19:20:09+00:00</updated>
    <author>
      <name>/u/_AdamWTF</name>
      <uri>https://old.reddit.com/user/_AdamWTF</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently have a home lab setup that spans a few different machines but I don’t currently have anything capable of running AI at decent speeds, I was going to Frankenstein together a machine using spare parts I have laying around, but getting a decent GPU with acceptable vram in the UK is so expensive right now.&lt;/p&gt; &lt;p&gt;This brings me to the Mac Mini M4, looking at just the base model and getting it for around £550, it seems like a lot of power for a decent price. I’m just curious how good the performance really is, I’m expecting with something like a 3b model of say llama 3.2 the response for 5k context tokens to be &amp;lt; 3 seconds to be suitable for me. &lt;/p&gt; &lt;p&gt;If anyone is willing to share their experiences and some examples of the kind of speeds you’re getting I’d really appreciate it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_AdamWTF"&gt; /u/_AdamWTF &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idv02o/has_anyone_been_using_the_base_m4_mac_mini_as_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idv02o/has_anyone_been_using_the_base_m4_mac_mini_as_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idv02o/has_anyone_been_using_the_base_m4_mac_mini_as_an/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T19:20:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1idxa19</id>
    <title>Fine tuning for small models</title>
    <updated>2025-01-30T20:55:48+00:00</updated>
    <author>
      <name>/u/jcrowe</name>
      <uri>https://old.reddit.com/user/jcrowe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an app that pulls specific information from a website. Right now I scrape the page, convert it to text and upload it to chatgpt to get a json file of data. Works great…&lt;/p&gt; &lt;p&gt;What I would like to do is switch over to a small LLM model (llama3.2:1b for example). But this model doesn’t return the results I want.&lt;/p&gt; &lt;p&gt;I would like to fine tune this model to make it work for my use case.&lt;/p&gt; &lt;p&gt;Can anyone recommend a tutorial for this or tell me I’m an idiot if it’s not the right way to use the technology?&lt;/p&gt; &lt;p&gt;ETA: to clarify, I don’t want to fine tune for the model to have the new information, I want to fine tune so that it knows how to deal with this information. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jcrowe"&gt; /u/jcrowe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idxa19/fine_tuning_for_small_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idxa19/fine_tuning_for_small_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idxa19/fine_tuning_for_small_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T20:55:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1idvw46</id>
    <title>Recommended deepseek model for coding tasks for an average PC?</title>
    <updated>2025-01-30T19:57:31+00:00</updated>
    <author>
      <name>/u/Upset_Hippo_5304</name>
      <uri>https://old.reddit.com/user/Upset_Hippo_5304</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Want to join the gang and try this stuff for coding tasks.&lt;/p&gt; &lt;p&gt;Py, dart, js, c#&lt;/p&gt; &lt;p&gt;32GB ram RTX 3070 Ryzen 5 5600x&lt;/p&gt; &lt;p&gt;Cheers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Upset_Hippo_5304"&gt; /u/Upset_Hippo_5304 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idvw46/recommended_deepseek_model_for_coding_tasks_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idvw46/recommended_deepseek_model_for_coding_tasks_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idvw46/recommended_deepseek_model_for_coding_tasks_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T19:57:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie210z</id>
    <title>Got Deepseek R1 1.5b running locally on Pixel 8 pro</title>
    <updated>2025-01-31T00:24:19+00:00</updated>
    <author>
      <name>/u/Teradyyne</name>
      <uri>https://old.reddit.com/user/Teradyyne</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"&gt; &lt;img alt="Got Deepseek R1 1.5b running locally on Pixel 8 pro" src="https://external-preview.redd.it/cTk0emwwZ3ozOGdlMYbCJM1MQLfOpw8fF1FnxkqAh7visCMP7lFjyVXppg7i.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=263215164e10efcd203b94bae4672c47e11c63d8" title="Got Deepseek R1 1.5b running locally on Pixel 8 pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Teradyyne"&gt; /u/Teradyyne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cha0fplz38ge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie210z/got_deepseek_r1_15b_running_locally_on_pixel_8_pro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-31T00:24:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie1aui</id>
    <title>Does `ollama create` actually build a new model?</title>
    <updated>2025-01-30T23:50:04+00:00</updated>
    <author>
      <name>/u/homelab2946</name>
      <uri>https://old.reddit.com/user/homelab2946</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded a GGUF file and create a Modelfile to extend it. Then I run `ollama create model_name -f Modelfile`, a `model_name:latest` model is created and shows in `ollama list` 10 GB. The GGUF file is also around the same 10 GB. Does `ollama create` not just add instruction on a base model but actually acting more like `docker build`? Would it then be fine to remove the GGUF file after the build?&lt;/p&gt; &lt;p&gt;Another scenario is through a supported ollama model, like `llama3`. Does Ollama &amp;quot;build&amp;quot; a new image if I create a new Modelfile from `llama3`, so it takes double the storage?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/homelab2946"&gt; /u/homelab2946 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1aui/does_ollama_create_actually_build_a_new_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ie1aui/does_ollama_create_actually_build_a_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ie1aui/does_ollama_create_actually_build_a_new_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T23:50:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1idh8ft</id>
    <title>Deepseek r1 671b on my local PC</title>
    <updated>2025-01-30T07:03:27+00:00</updated>
    <author>
      <name>/u/Geschirrtuch</name>
      <uri>https://old.reddit.com/user/Geschirrtuch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;Two days ago, I turned night into day, and in the end, I managed to get R1 running on my local PC. Yesterday, I uploaded a video on YouTube showing how I did it: &lt;a href="https://www.youtube.com/watch?v=O3Lk3xSkAdk"&gt;https://www.youtube.com/watch?v=O3Lk3xSkAdk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I don't post here often, so I'm not sure if sharing the link is okay—I hope it is.&lt;/p&gt; &lt;p&gt;The video is in German, but with subtitles, everyone should be able to understand it.&lt;br /&gt; Be careful if you want to try this yourself! ;)&lt;/p&gt; &lt;p&gt;Update:&lt;/p&gt; &lt;p&gt;For those who don't feel like watching the video: The &amp;quot;trick&amp;quot; was using Windows' pagefile. I set up three of them on three different SSDs, which gave me around 750GB of virtual memory in total.&lt;/p&gt; &lt;p&gt;Loading the model and answering a question took my PC about 90 minutes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Geschirrtuch"&gt; /u/Geschirrtuch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T07:03:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1idqxto</id>
    <title>Why Are All Local AI Models So Bad? No One Talks About This!</title>
    <updated>2025-01-30T16:31:59+00:00</updated>
    <author>
      <name>/u/NikkEvan</name>
      <uri>https://old.reddit.com/user/NikkEvan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with local AI models, even &amp;quot;high-end ones&amp;quot; like the recent DeepSeek-R1 32B, using Open WebUI.&lt;br /&gt; I expected them to be weaker than online models, but the gap is just ridiculous.&lt;br /&gt; Even for the simplest questions, they either fail, give nonsense answers, or completely misunderstand the input.&lt;/p&gt; &lt;p&gt;I’ve set the parameters and all the settings at the best i could, tried different setups, system prompts, and still , even after parsing a basic document just a few pages long, is a struggle.&lt;br /&gt; If it already fails here, how am I supposed to use it for hundreds of internal company documents?&lt;/p&gt; &lt;p&gt;The crazy part? No one talks about this!&lt;br /&gt; Instead, i see every video in youtube saying :&lt;br /&gt; &amp;quot;How to run locally (modelname) much better than chat-gpt&amp;quot;&lt;br /&gt; &amp;quot;Local Deepseek beats Chat-gpt&amp;quot;&lt;br /&gt; Than the question they ask to those local models are : How many 'R' are in the word Strawberry and the model answer: 2 ... lol &lt;/p&gt; &lt;p&gt;Why is the performance so bad, even on 32B models?&lt;/p&gt; &lt;p&gt;Why are there no proper guides to get the best out of local AI?&lt;br /&gt; Having a big hardware such as the Nvidia project DIGITS will make a big model work close to the online Chat-gpt 3 or 4 ? I see those has 175b parameters. &lt;/p&gt; &lt;p&gt;What are we missing?&lt;/p&gt; &lt;p&gt;I really want to make local AI work as close as the online models, even buying bigger and stronger hardware, but, right now, it just feels like a waste of time.&lt;br /&gt; Has anyone actually succeeded in making these models work well? If so, how? And , what do you intend for Working Well for a local Model ? &lt;/p&gt; &lt;p&gt;Let’s discuss this! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NikkEvan"&gt; /u/NikkEvan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T16:31:59+00:00</published>
  </entry>
</feed>
