<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-28T22:23:41+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jkyt85</id>
    <title>Dual rtx 3060</title>
    <updated>2025-03-27T07:56:27+00:00</updated>
    <author>
      <name>/u/ExtensionPatient7681</name>
      <uri>https://old.reddit.com/user/ExtensionPatient7681</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, im thinking of the popular setup of dual rtx 3060s. &lt;/p&gt; &lt;p&gt;Right now it seems to automatically run on my laptop gpu but when im upgrading to a dedicated server im wondering how much configuration and tinkering i must do to make it run on a dual gpu setup.&lt;/p&gt; &lt;p&gt;Is it as simple as plugging in the gpu's and download the cuda drivers then Download ollama and run the model or do i need to do further configuration?&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExtensionPatient7681"&gt; /u/ExtensionPatient7681 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkyt85/dual_rtx_3060/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkyt85/dual_rtx_3060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jkyt85/dual_rtx_3060/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-27T07:56:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk7hh0</id>
    <title>Use Ollama to create your own AI Memory locally from 30+ types of data sources</title>
    <updated>2025-03-26T09:06:32+00:00</updated>
    <author>
      <name>/u/Short-Honeydew-7000</name>
      <uri>https://old.reddit.com/user/Short-Honeydew-7000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;We've just finished a small guide on how to set up Ollama with cognee, an open-source AI memory tool that will allow you to ingest your local data into graph/vector stores, enrich it and search it.&lt;/p&gt; &lt;p&gt;You can load all your codebase to cognee and enrich it with your README file and documentation or load images, video and audio data and merge different data sources.&lt;/p&gt; &lt;p&gt;And in the end you get to see and explore a nice looking graph.&lt;/p&gt; &lt;p&gt;Here is a short tutorial to set up Ollama with cognee:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=aZYRo-eXDzA&amp;amp;t=62s"&gt;https://www.youtube.com/watch?v=aZYRo-eXDzA&amp;amp;t=62s&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here is our Github: &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/topoteretes/cognee"&gt;https://github.com/topoteretes/cognee&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Short-Honeydew-7000"&gt; /u/Short-Honeydew-7000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jk7hh0/use_ollama_to_create_your_own_ai_memory_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jk7hh0/use_ollama_to_create_your_own_ai_memory_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jk7hh0/use_ollama_to_create_your_own_ai_memory_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-26T09:06:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkz8lc</id>
    <title>Is it possible to train an AI to help run a D&amp;D campaign?</title>
    <updated>2025-03-27T08:30:24+00:00</updated>
    <author>
      <name>/u/SeriousLemur</name>
      <uri>https://old.reddit.com/user/SeriousLemur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running a modified version of a D&amp;amp;D campaign and I have all the information for the campaign in a bunch of .pdf or .htm files. I've been trying to get ChatGPT to thoroughly refer through the content before giving me answers but it still messes up important details sometimes.&lt;/p&gt; &lt;p&gt;Would it be possible to run something locally on my machine and train it to either memorize all of the details of the campaign or thoroughly read all of the documents before answering? I'd like help with creating descriptions, dialogue, suggestions on how things could continue, etc. Thank you, I'm unfamiliar with this stuff, I don't even know how to install ollama lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SeriousLemur"&gt; /u/SeriousLemur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkz8lc/is_it_possible_to_train_an_ai_to_help_run_a_dd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkz8lc/is_it_possible_to_train_an_ai_to_help_run_a_dd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jkz8lc/is_it_possible_to_train_an_ai_to_help_run_a_dd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-27T08:30:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl5gn2</id>
    <title>How to extract &lt;think&gt; tags for Deepseek?</title>
    <updated>2025-03-27T14:40:19+00:00</updated>
    <author>
      <name>/u/Desperate-Finger7851</name>
      <uri>https://old.reddit.com/user/Desperate-Finger7851</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm building an application that uses Ollama with Deepseek locally; I think it would be really cool to stream the &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; tags in real time to the application frontend (would be Streamlit for prototyping, eventually React).&lt;/p&gt; &lt;p&gt;I looked briefly and couldn't find much information on how they work? &lt;/p&gt; &lt;p&gt;Any help greatly appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate-Finger7851"&gt; /u/Desperate-Finger7851 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jl5gn2/how_to_extract_think_tags_for_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jl5gn2/how_to_extract_think_tags_for_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jl5gn2/how_to_extract_think_tags_for_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-27T14:40:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlqee3</id>
    <title>How much VRAM does gemma3:27b vision utilize in addition to text inference only?</title>
    <updated>2025-03-28T08:31:41+00:00</updated>
    <author>
      <name>/u/EatTFM</name>
      <uri>https://old.reddit.com/user/EatTFM</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running a job for extracting data from PDFs using ollama with gemma3:27b on a machine with anRTX 4090 24Gb VRAM. &lt;/p&gt; &lt;p&gt;I can see that ollama uses like 50% of my GPU core and 90% of my VRAM, but also all of my 12-core CPUs. I do not need that long context - could it be that I am that quickly out of VRAM due to the additional image processing?&lt;/p&gt; &lt;p&gt;Ollama lists the model as 17G in size.&lt;/p&gt; &lt;p&gt;root@llm:~# ollama ps&lt;br /&gt; NAME ID SIZE PROCESSOR UNTIL &lt;br /&gt; gemma3:27b 30ddded7fba6 21 GB 5%/95% CPU/GPU 4 minutes from now&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EatTFM"&gt; /u/EatTFM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlqee3/how_much_vram_does_gemma327b_vision_utilize_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlqee3/how_much_vram_does_gemma327b_vision_utilize_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlqee3/how_much_vram_does_gemma327b_vision_utilize_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T08:31:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlr3e4</id>
    <title>Whats up with Quantized models selection?</title>
    <updated>2025-03-28T09:26:22+00:00</updated>
    <author>
      <name>/u/Birdinhandandbush</name>
      <uri>https://old.reddit.com/user/Birdinhandandbush</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically when you go to the models section on the Ollama website, as far as I can tell it only shows you all the Q4 models. &lt;/p&gt; &lt;p&gt;You have to go to HuggingFace to find Q5-Q8 models for example. Why doesn't the official Ollama page have a drop down for different quantizations of the same models? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Birdinhandandbush"&gt; /u/Birdinhandandbush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlr3e4/whats_up_with_quantized_models_selection/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlr3e4/whats_up_with_quantized_models_selection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlr3e4/whats_up_with_quantized_models_selection/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T09:26:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlrlng</id>
    <title>Cpu??</title>
    <updated>2025-03-28T10:03:16+00:00</updated>
    <author>
      <name>/u/ExtensionPatient7681</name>
      <uri>https://old.reddit.com/user/ExtensionPatient7681</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How much does cpu matter when building a server? As i understand it i need as much vram as i can get. But what about cpu? Can i get away with a i9-7900X CPU @ 3.30GHz or do i need more?&lt;/p&gt; &lt;p&gt;Im asking because i can buy this second hand for 700usd, and my thinking is that its a good place to start. But since the cpu is old but was good for that age im not sure if its gonna slow me down a bunch of not.&lt;/p&gt; &lt;p&gt;Im gonna use it for a whisper large model and ollama model, as big as i can fit for a homeassistant voice assistant.&lt;/p&gt; &lt;p&gt;Since the mobo supports another gpu i was thinking of adding another 3060 down the line.&lt;/p&gt; &lt;p&gt;Mobo: Asus Corsair asus prime x299-a&lt;/p&gt; &lt;p&gt;Cpu: i9-7900X CPU @ 3.30GHz 3.31 GHz&lt;/p&gt; &lt;p&gt;Ram: 16gb&lt;/p&gt; &lt;p&gt;Gpu: rtx 3060&lt;/p&gt; &lt;p&gt;SSD: 465gb&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExtensionPatient7681"&gt; /u/ExtensionPatient7681 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlrlng/cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlrlng/cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlrlng/cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T10:03:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlrxdv</id>
    <title>Tuning Ollama for parallel request processing on a Nvidia RTX 1000 ADA</title>
    <updated>2025-03-28T10:26:34+00:00</updated>
    <author>
      <name>/u/icbts</name>
      <uri>https://old.reddit.com/user/icbts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jlrxdv/tuning_ollama_for_parallel_request_processing_on/"&gt; &lt;img alt="Tuning Ollama for parallel request processing on a Nvidia RTX 1000 ADA" src="https://external-preview.redd.it/fEmD4oMekfPVVq-gnDwT6mUL3NzZlSGs-l5UF8X1JoM.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df6db28e9416ddd9991856a19f855d7b9f64c0a6" title="Tuning Ollama for parallel request processing on a Nvidia RTX 1000 ADA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tuning Ollama for our Dell R250 w/ Nvidia RTX 1000 ADA (8Gb vram) card.&lt;/p&gt; &lt;p&gt;Ollama supports running requests in parallel, in this video we test out various settings for number of parallel context requests on a few different models to see if there are optimal settings for overall throughput. Keeping in mind that this card draws 50 watts processing sequentially or under higher load, its in our interest to get as much through the card as we can.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/icbts"&gt; /u/icbts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=lne8ChZ5rZk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlrxdv/tuning_ollama_for_parallel_request_processing_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlrxdv/tuning_ollama_for_parallel_request_processing_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T10:26:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jls83u</id>
    <title>Which model makes sense for my requirements?</title>
    <updated>2025-03-28T10:47:03+00:00</updated>
    <author>
      <name>/u/soft-boy</name>
      <uri>https://old.reddit.com/user/soft-boy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am using Ollama and want to run an llm locally on my MacBook Air. I mainly use it to give feedback on texts like screenplays. &lt;/p&gt; &lt;p&gt;I have used Llama for the past few days and am super disappointed in the results. &lt;/p&gt; &lt;p&gt;Which model would you guys suggest?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/soft-boy"&gt; /u/soft-boy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jls83u/which_model_makes_sense_for_my_requirements/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jls83u/which_model_makes_sense_for_my_requirements/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jls83u/which_model_makes_sense_for_my_requirements/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T10:47:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlwmgu</id>
    <title>How to prompt mixtral 8X7B correctly? Sometimes it ingores instructions for RAG in German</title>
    <updated>2025-03-28T14:40:36+00:00</updated>
    <author>
      <name>/u/No-Duty-8087</name>
      <uri>https://old.reddit.com/user/No-Duty-8087</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;br /&gt; As I am implementing RAG using the Mixtral 8X7B model, I have a question regarding the prompting part. From what I have found, an English prompt works better than a German one for this specific model. However, I have encountered an issue. If I add one more line of text to the existing prompt, it seems that the model ignores some of the instructions. With the current instructions, it seems to work fine.&lt;/p&gt; &lt;p&gt;Do you think that adding one more sentence causes the model to exceed its context window, and that’s why it cuts the prompt and ignores part of it?&lt;/p&gt; &lt;p&gt;Please help me with any advice, as I have worked extensively with this specific model and always had problems on prompting it correctly. Any advice would be greatly appreciated.&lt;/p&gt; &lt;p&gt;My system prompt looks like this:&lt;br /&gt; &amp;lt;s&amp;gt;[INST] You are a German helpful AI assistant, dedicated to answering questions based only on the given context. You must always follow the instructions and guidelines when generating an answer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Make sure to always follow ALL the instructions and guidelines that you find below:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Given only the context information, answer the question but NEVER mention where you found the answer.&lt;/li&gt; &lt;li&gt;When possible, EVERY single statement you generate MUST be followed by a numbered source reference in the order in which they are used, coming from the context in square brackets, e.g., [1].&lt;/li&gt; &lt;li&gt;If a harmful, unethical, prejudiced, or negative query comes up, don't make up an answer. Instead, respond exactly with &amp;quot;IIch kann die Frage nicht antworten&amp;quot; and NEVER give any type of numbered source reference in this case.&lt;/li&gt; &lt;li&gt;Examine the context, and if you cannot answer only from the context, don't make up an answer. Instead, respond exactly with &amp;quot;Vielen Dank für Ihre Frage. Leider kann ich nicht antworten.&amp;quot; and NEVER give any type of numbered source reference in this case.&lt;/li&gt; &lt;li&gt;Answer only in German, NEVER in English, regardless of the request or context. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;[/INST]&lt;/p&gt; &lt;h1&gt;Context is below:&lt;/h1&gt; &lt;h1&gt;{context}&lt;/h1&gt; &lt;h1&gt;Input:&lt;/h1&gt; &lt;p&gt;{query}&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Duty-8087"&gt; /u/No-Duty-8087 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlwmgu/how_to_prompt_mixtral_8x7b_correctly_sometimes_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlwmgu/how_to_prompt_mixtral_8x7b_correctly_sometimes_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlwmgu/how_to_prompt_mixtral_8x7b_correctly_sometimes_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T14:40:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlxfig</id>
    <title>@@@@ signs in model responses</title>
    <updated>2025-03-28T15:15:43+00:00</updated>
    <author>
      <name>/u/SergeiTvorogov</name>
      <uri>https://old.reddit.com/user/SergeiTvorogov</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone encountered the problem where the Qwen-coder model outputs @@@@ instead of text, and after restarting, everything normalizes for some time? I'm using it in the continue.dev plugin for code autocompletion&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SergeiTvorogov"&gt; /u/SergeiTvorogov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlxfig/signs_in_model_responses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlxfig/signs_in_model_responses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlxfig/signs_in_model_responses/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T15:15:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlunnp</id>
    <title>Mac Studio M1 Ultra or a TrueNAS box w/ RTX 3070 Ti</title>
    <updated>2025-03-28T13:07:44+00:00</updated>
    <author>
      <name>/u/zog1300</name>
      <uri>https://old.reddit.com/user/zog1300</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone — I’m lucky enough to have both systems running, and I’m trying to decide which one to dedicate to running Ollama (mainly for local LLM stuff like LLaMA, Mistral, etc.).&lt;/p&gt; &lt;p&gt;Here are my two setups:&lt;/p&gt; &lt;p&gt;🔹 Mac Studio M1 Ultra&lt;/p&gt; &lt;p&gt;64 GB unified memory&lt;/p&gt; &lt;p&gt;Apple Silicon (Metal backend, no CUDA)&lt;/p&gt; &lt;p&gt;Runs Ollama natively on macOS&lt;/p&gt; &lt;p&gt;🔹 TrueNAS SCALE box&lt;/p&gt; &lt;p&gt;Intel Xeon Bronze 3204 @ 1.90GHz&lt;/p&gt; &lt;p&gt;31 GB ECC RAM&lt;/p&gt; &lt;p&gt;EVGA RTX 3070 Ti (CUDA support)&lt;/p&gt; &lt;p&gt;I can run a Linux VM or container for Ollama and pass through the GPU&lt;/p&gt; &lt;p&gt;I'm only planning to run Ollama and use Samba shares — no VMs, Plex, or anything else intensive.&lt;/p&gt; &lt;p&gt;My gut says the 3070 Ti with CUDA support will destroy the M1 Ultra in terms of inference speed, even with the lower RAM, but I’d love to hear from people who’ve tested both. Has anyone done direct comparisons?&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts — especially around performance with 7B and 13B models, startup time, and memory overhead.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zog1300"&gt; /u/zog1300 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlunnp/mac_studio_m1_ultra_or_a_truenas_box_w_rtx_3070_ti/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlunnp/mac_studio_m1_ultra_or_a_truenas_box_w_rtx_3070_ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlunnp/mac_studio_m1_ultra_or_a_truenas_box_w_rtx_3070_ti/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T13:07:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlkpsj</id>
    <title>Which is the smallest, fastest text generation model on ollama that can be used for chatbot?</title>
    <updated>2025-03-28T02:24:49+00:00</updated>
    <author>
      <name>/u/gilzonme</name>
      <uri>https://old.reddit.com/user/gilzonme</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gilzonme"&gt; /u/gilzonme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlkpsj/which_is_the_smallest_fastest_text_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlkpsj/which_is_the_smallest_fastest_text_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlkpsj/which_is_the_smallest_fastest_text_generation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T02:24:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlqp06</id>
    <title>Ollama does not do well</title>
    <updated>2025-03-28T08:55:42+00:00</updated>
    <author>
      <name>/u/aadarsh_af</name>
      <uri>https://old.reddit.com/user/aadarsh_af</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;None of the ollama models or tags work well with structured output. I've tried it with 3B param models as i don't have large GPU resources, my GPU gets stuck even with llama3.2. I've tried prompt engineering and grammar, it does not generate valid JSON. Is there any way i could make smaller param models perform well with lesser compute power?? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aadarsh_af"&gt; /u/aadarsh_af &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlqp06/ollama_does_not_do_well/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlqp06/ollama_does_not_do_well/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlqp06/ollama_does_not_do_well/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T08:55:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm2ee2</id>
    <title>Weird slowness after first query?</title>
    <updated>2025-03-28T18:47:13+00:00</updated>
    <author>
      <name>/u/john_alan</name>
      <uri>https://old.reddit.com/user/john_alan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, with all models I see weird behaviour that I googled around but can't see an explanation for...&lt;/p&gt; &lt;p&gt;On first run I get stats like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;total duration: 1.094507167s load duration: 8.850792ms prompt eval count: 33 token(s) prompt eval duration: 32.268125ms prompt eval rate: 1022.68 tokens/s eval count: 236 token(s) eval duration: 1.052533167s eval rate: 224.22 tokens/s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;then on second and further queries it slows:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;total duration: 1.041227416s load duration: 9.1175ms prompt eval count: 286 token(s) prompt eval duration: 29.909875ms prompt eval rate: 9562.06 tokens/s eval count: 212 token(s) eval duration: 1.001476792s eval rate: 211.69 tokens/ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Until about 155 tokens/ on eval rate. &lt;/p&gt; &lt;p&gt;Any idea why?&lt;/p&gt; &lt;p&gt;Closing the model and running again immediately returns to ~224.&lt;/p&gt; &lt;p&gt;I'm using Ollama &lt;code&gt;0.6.2&lt;/code&gt; - and Llama 3.&lt;/p&gt; &lt;p&gt;But it happens in other versions and with other models...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/john_alan"&gt; /u/john_alan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm2ee2/weird_slowness_after_first_query/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm2ee2/weird_slowness_after_first_query/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jm2ee2/weird_slowness_after_first_query/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T18:47:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm35s2</id>
    <title>Computer vision for reading</title>
    <updated>2025-03-28T19:19:07+00:00</updated>
    <author>
      <name>/u/gttcoelho</name>
      <uri>https://old.reddit.com/user/gttcoelho</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, guys! I am using the Google vision API for transcribing text from images, but it is too expensive... do you know some cheaper alternative for this? I have tried llava but it is petty bad for text transcribing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gttcoelho"&gt; /u/gttcoelho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm35s2/computer_vision_for_reading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm35s2/computer_vision_for_reading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jm35s2/computer_vision_for_reading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T19:19:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm3obq</id>
    <title>Minimalist Note-Taking App with Integrated AI Assistant</title>
    <updated>2025-03-28T19:41:04+00:00</updated>
    <author>
      <name>/u/The_Money_Mindset</name>
      <uri>https://old.reddit.com/user/The_Money_Mindset</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I'm exploring an idea for a note-taking app inspired by Flatnotes—offering a simple, distraction-free interface for capturing ideas—enhanced with built-in AI functionalities. The envisioned features include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Summarization:&lt;/strong&gt; Automatically condensing long notes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Suggestions:&lt;/strong&gt; Offering context-aware recommendations to refine or expand ideas.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interactive Prompts:&lt;/strong&gt; Asking insightful questions to deepen understanding and clarity of the notes.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The goal is to blend a minimalist design with smart, targeted AI capabilities that truly add value.&lt;/p&gt; &lt;p&gt;How would you suggest approaching this project? Are there any existing solutions that combine straightforward note-taking with these AI elements?&lt;/p&gt; &lt;p&gt;Any insights or suggestions are greatly appreciated. Thanks for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The_Money_Mindset"&gt; /u/The_Money_Mindset &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm3obq/minimalist_notetaking_app_with_integrated_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm3obq/minimalist_notetaking_app_with_integrated_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jm3obq/minimalist_notetaking_app_with_integrated_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T19:41:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm49d9</id>
    <title>WSL + Ollama: Local LLMs Are (Kinda) Here — Full Guide + Use Case Thoughts</title>
    <updated>2025-03-28T20:05:45+00:00</updated>
    <author>
      <name>/u/Standard_Abrocoma539</name>
      <uri>https://old.reddit.com/user/Standard_Abrocoma539</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Standard_Abrocoma539"&gt; /u/Standard_Abrocoma539 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/wsl2/comments/1jkdro9/wsl_ollama_local_llms_are_kinda_here_full_guide/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm49d9/wsl_ollama_local_llms_are_kinda_here_full_guide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jm49d9/wsl_ollama_local_llms_are_kinda_here_full_guide/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T20:05:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlhysn</id>
    <title>Building a front end that sits on ollama, is this pointless?</title>
    <updated>2025-03-28T00:08:08+00:00</updated>
    <author>
      <name>/u/Outside-Prune-5838</name>
      <uri>https://old.reddit.com/user/Outside-Prune-5838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jlhysn/building_a_front_end_that_sits_on_ollama_is_this/"&gt; &lt;img alt="Building a front end that sits on ollama, is this pointless?" src="https://a.thumbs.redditmedia.com/_uNUXAvyhpU50VvCDcArNF1fQGQ4kxrSsS9nrKJqCZ4.jpg" title="Building a front end that sits on ollama, is this pointless?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I started using gpt but ran into limits, got the $20 plan and was still hitting limits (because ai is fun) so I asked gpt what I could do and it recommended chatting through the api. Another gpt and 30 versions later I had a front end that spoke to openai but had zero personality. They also tend to lose their minds when the conversations get long.&lt;/p&gt; &lt;p&gt;Back to gpt to complain and asked how to do it for free and it said go for local llm and landed on ollama. Naturally I chose models that were too big to run on my machine because I was clueless but I got it sorted.&lt;/p&gt; &lt;p&gt;Got a bit annoyed at the basic interface and lack of memory and personality so I went back to gpt (getting my moneys worth) and spent a week (so far) working on a frontend that can talk to either locally running ollama or openai through api, remembers everything you spoke about and your memory is stored locally. It can analyse files and store them in memory too. You can give it whole documents then ask for summaries or specific points. It also reads what llms are downloaded in ollama and can even autostart them from the interface. You can also load in custom personas over the llm.&lt;/p&gt; &lt;p&gt;Also supports either local embedding w/gpu or embedding from openai through their api. Im debating releasing it because it was just a niche thing I did for me which turned into a whole ass program. If you can run ollama comfortably, you can run this on top easily as theres almost zero overhead.&lt;/p&gt; &lt;p&gt;The goal is jarvis on a budget and the memory thing has evolved several times which resulted because I wanted it to remember my name and now it remembers &lt;em&gt;everything&lt;/em&gt;. It also has a voice journal mode (work in progress, think star trek captains log). Right now integrating more voice features and an even more niche feature - way to control sonar, sabnzbd and radarr through the llm. Its also going to have tool access to go online and whatnot.&lt;/p&gt; &lt;p&gt;Its basically a multi-LLM brain with a shared long-term memory that is saved on your pc. You can start a conversation with your local llm, switch to gpt for something more complicated THEN switch back and your local llm has access to everything. The chat window doesnt even clear.&lt;/p&gt; &lt;p&gt;Talking to gpt through api doesnt require a plus plan just requires a few bucks in your openai api account, although Im big on local everything.&lt;/p&gt; &lt;h1&gt;Here's what happens under the hood:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;You chat with Mistral (or whatever llm) → everything gets stored: &lt;ul&gt; &lt;li&gt;Chat history → SQLite&lt;/li&gt; &lt;li&gt;Embedded chunks → ChromaDB&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;You switch to GPT (OpenAI) → same memory system is accessed: &lt;ul&gt; &lt;li&gt;GPT pulls from the same vector memory&lt;/li&gt; &lt;li&gt;You may even embed with the same SentenceTransformer (if not OpenAI embeddings)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;You switch back to Mistral → &lt;strong&gt;nothing is lost&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Vector search still hits all past data&lt;/li&gt; &lt;li&gt;SQLite short-term history still intact (unless wiped)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Snippet below, shameless self plug, sorry:&lt;/p&gt; &lt;h1&gt;⚛️ Atom — A Memory-Driven, Local AI Command Center&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Atom&lt;/strong&gt; is a locally hosted, memory-enhanced AI assistant built for devs, tinkerers, and power users who want full control of their LLM environment. It fuses &lt;strong&gt;chat&lt;/strong&gt;, &lt;strong&gt;file-based memory&lt;/strong&gt;, &lt;strong&gt;tool execution&lt;/strong&gt;, and &lt;strong&gt;GPU-accelerated embedding&lt;/strong&gt; — all inside a slick, modular cockpit interface.&lt;/p&gt; &lt;p&gt;Forget cloud APIs and stateless interactions. Atom doesn’t just respond — it &lt;strong&gt;remembers&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;🧠 Why Atom’s Memory Hits Different&lt;/h1&gt; &lt;p&gt;Atom combines &lt;strong&gt;short-term chat memory&lt;/strong&gt; and &lt;strong&gt;long-term vector memory&lt;/strong&gt; to create a persistent assistant that can recall your history, files, and intent — across sessions.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Vector Memory (ChromaDB)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Automatically chunks and embeds uploaded files (e.g., &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Semantically searchable — even if you don’t use exact keywords&lt;/li&gt; &lt;li&gt;Fully GPU-accelerated with &lt;code&gt;sentence-transformers&lt;/code&gt; + CUDA&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat Memory (SQLite)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Logs all user/assistant messages&lt;/li&gt; &lt;li&gt;Feeds recent dialogue back into the LLM for continuity&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory Injection&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Relevant chunks are auto-injected into system prompts in real time&lt;/li&gt; &lt;li&gt;Optional filters by file, chunk ID, or context window&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory Dashboard&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Full frontend panel showing stored vector data&lt;/li&gt; &lt;li&gt;View per-file chunk metadata (source, index, timestamp)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🔧 Core Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;⚡ &lt;strong&gt;GPU Embeddings&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;900+ chunks embedded from large files in seconds&lt;/li&gt; &lt;li&gt;Powered by RTX CUDA-enabled cards&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;🧰 &lt;strong&gt;LLM Tool Execution&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Add tools like &lt;code&gt;summarize_file&lt;/code&gt;, &lt;code&gt;search_web&lt;/code&gt;, &lt;code&gt;inject_chunk&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Triggered with &lt;code&gt;::tool:&lt;/code&gt; syntax or natural language&lt;/li&gt; &lt;li&gt;Executed live via FastAPI backend&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;👤 &lt;strong&gt;Persona Layer&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;YAML-defined styles (e.g., casual, sarcastic, technical)&lt;/li&gt; &lt;li&gt;Memory-aware greetings (e.g., &amp;quot;Welcome back, John.&amp;quot;)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;🖥️ &lt;strong&gt;React UI with Vite + Tailwind&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Tabbed interface: Chat, Files, Memory View, Tools, etc.&lt;/li&gt; &lt;li&gt;Model selector, GPU monitor, file uploader, token preview&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;🔐 &lt;strong&gt;Offline, Private, and Extendable&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Ollama + Mistral for fast local inference&lt;/li&gt; &lt;li&gt;No API keys needed (openai api access and openai embedding is totally optional)&lt;/li&gt; &lt;li&gt;No cloud. No snooping.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;💡 TL;DR&lt;/h1&gt; &lt;p&gt;Atom isn’t just another chatbot UI — it’s a &lt;strong&gt;self-hosted, memory-capable assistant platform&lt;/strong&gt; that grows smarter the more you use it.&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Its a work in progress. Written by me and several gpts, its still evolving and may never see the light of day.&lt;/p&gt; &lt;p&gt;Unless people actually want it, then I might throw it on git.&lt;/p&gt; &lt;p&gt;But yeah. ollama is great tbh.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/09rqnxtmlcre1.png?width=1708&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f766e4878a7e0dc294cff4b02df897c665b88031"&gt;https://preview.redd.it/09rqnxtmlcre1.png?width=1708&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f766e4878a7e0dc294cff4b02df897c665b88031&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xbc9wisolcre1.png?width=779&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b6e499ce94827264aedddaab83c4732b8cd0ee39"&gt;https://preview.redd.it/xbc9wisolcre1.png?width=779&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b6e499ce94827264aedddaab83c4732b8cd0ee39&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c8hovy3rkbre1.png?width=1707&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=947c6b167251ce602fb33ff064c3d5ce60116f15"&gt;https://preview.redd.it/c8hovy3rkbre1.png?width=1707&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=947c6b167251ce602fb33ff064c3d5ce60116f15&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Update 3/27&lt;/p&gt; &lt;h1&gt;ATOM: Post-Cognee Upgrade Breakdown&lt;/h1&gt; &lt;h1&gt;🧠 MEMORY: From Flat to Hybrid Brain&lt;/h1&gt; &lt;h1&gt;BEFORE:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Chunks were just text blobs — untyped, unstructured&lt;/li&gt; &lt;li&gt;Memory was recalled via top-k semantic match&lt;/li&gt; &lt;li&gt;No separation between facts, tasks, chat, etc.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;AFTER:&lt;/h1&gt; &lt;p&gt;✅ &lt;strong&gt;Memory Typing&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Each memory chunk has a &lt;code&gt;type&lt;/code&gt;: &lt;code&gt;chat&lt;/code&gt;, &lt;code&gt;identity&lt;/code&gt;, &lt;code&gt;file&lt;/code&gt;, &lt;code&gt;task&lt;/code&gt;, &lt;code&gt;summary&lt;/code&gt;, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;✅ &lt;strong&gt;Memory Prioritization&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Chunks can be tagged with &lt;code&gt;priority&lt;/code&gt; levels (&lt;code&gt;low&lt;/code&gt;, &lt;code&gt;high&lt;/code&gt;, &lt;code&gt;critical&lt;/code&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;✅ &lt;strong&gt;Usage Tracking&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Each chunk now tracks how many times it’s been retrieved: &lt;code&gt;usage_count&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;✅ &lt;strong&gt;TTL Expiration&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Chunks can auto-expire after a set time using &lt;code&gt;expires&lt;/code&gt; metadata&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;✅ &lt;strong&gt;Memory Role Filtering&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Excludes assistant replies from being re-injected and parroted&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;✅ &lt;strong&gt;Memory Source Support (coming)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tag origin: &lt;code&gt;user&lt;/code&gt;, &lt;code&gt;tool&lt;/code&gt;, &lt;code&gt;system&lt;/code&gt;, &lt;code&gt;reflection&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🔁 REFLECTION SYSTEM&lt;/h1&gt; &lt;p&gt;✅ &lt;strong&gt;Scheduled Reflection&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Every 10 messages, Atom runs a full memory review: &lt;ul&gt; &lt;li&gt;Reflects on &lt;code&gt;identity&lt;/code&gt;, &lt;code&gt;file&lt;/code&gt;, and &lt;code&gt;task&lt;/code&gt; chunks&lt;/li&gt; &lt;li&gt;Sorts by &lt;code&gt;usage_count&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Stores summaries as &lt;code&gt;type=&amp;quot;summary&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;✅ &lt;strong&gt;Tool:&lt;/strong&gt; &lt;code&gt;generate_memory_reflection&lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Can be called manually or auto-triggered&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;✅ &lt;strong&gt;Stored like internal thoughts&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You’ll see memory chunks like:&lt;/li&gt; &lt;li&gt;[Reflection: identity] &lt;ol&gt; &lt;li&gt;Bob is a network engineer. (used 12x)&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;2. Prefers short, smart answers. (used 7x)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;✅ &lt;strong&gt;LLM can now reason over what it reflects&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;🛠️ TOOLCHAIN EXPANSION&lt;/h1&gt; &lt;p&gt;You now have a &lt;strong&gt;fully extensible tool registry&lt;/strong&gt; with:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Tool&lt;/th&gt; &lt;th align="left"&gt;Purpose&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;summarize_file&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;LLM-based file summarization&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;recall_memory_type&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Get all memory of a given type&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;set_memory_type&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Reclassify memory&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;prioritize_memory&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Change priority level&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;delete_memory&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Remove chunks&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;purge_expired_chunks&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Wipe expired data&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;generate_memory_reflection&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Run type-specific reflections&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;summarize_memory_stats&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Show chunk count, usage, TTL status&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;✅ Tool calls are handled via &lt;code&gt;::tool:tool_name{args}&lt;/code&gt;&lt;br /&gt; ✅ Fully callable by the LLM (agent-ready)&lt;br /&gt; ✅ Fully expandable by you&lt;/p&gt; &lt;h1&gt;📊 COGNITIVE UI UPGRADES&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Memory Stats Panel&lt;/strong&gt; → Shows count, usage, expiration&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory View Filtering&lt;/strong&gt; (next step) → Filter by type, priority&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reflection Viewer&lt;/strong&gt; (planned) → Read Atom’s thoughts&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chunk Reclassification / Deletion Buttons&lt;/strong&gt; (planned)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outside-Prune-5838"&gt; /u/Outside-Prune-5838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlhysn/building_a_front_end_that_sits_on_ollama_is_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlhysn/building_a_front_end_that_sits_on_ollama_is_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlhysn/building_a_front_end_that_sits_on_ollama_is_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T00:08:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm5spn</id>
    <title>Ollama blobs</title>
    <updated>2025-03-28T21:11:53+00:00</updated>
    <author>
      <name>/u/techmago</name>
      <uri>https://old.reddit.com/user/techmago</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a ton of blobs...&lt;br /&gt; How do i figure out which model is the owner of each blob?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techmago"&gt; /u/techmago &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm5spn/ollama_blobs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm5spn/ollama_blobs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jm5spn/ollama_blobs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T21:11:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlvhgk</id>
    <title>Link model with DB for memory?</title>
    <updated>2025-03-28T13:48:11+00:00</updated>
    <author>
      <name>/u/Ben_Graf</name>
      <uri>https://old.reddit.com/user/Ben_Graf</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there, I was curious if its possible to link a model to a local database and use that as memory. The scenario: The goal is a proactively acting calender and planner as well as control media. My idea would be for that to create on the main pc promts and results and have the model on on a pie just play them dynamically. Also it should remember things from the calender and use those as trigger too. &lt;/p&gt; &lt;p&gt;Example: i plan a calender event to clean my home. It plays the reply and t2speech premade at the time i told it to start. Depending on my reaction it either plays a more cheerful or more sarcastic one to motivate me.&lt;/p&gt; &lt;p&gt;I managed to set all up but without a memory it was all gone. Also I'd need my main pc to run all day if it was the source. So i think running it on a pie be better&lt;/p&gt; &lt;p&gt;Is that possible?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ben_Graf"&gt; /u/Ben_Graf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlvhgk/link_model_with_db_for_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlvhgk/link_model_with_db_for_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlvhgk/link_model_with_db_for_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T13:48:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm6k1u</id>
    <title>Edit this repo for streamed response?</title>
    <updated>2025-03-28T21:46:05+00:00</updated>
    <author>
      <name>/u/eriknau13</name>
      <uri>https://old.reddit.com/user/eriknau13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like &lt;a href="https://github.com/enricollen/rag-conversational-agent"&gt;this RAG project&lt;/a&gt; for its simplicity and customizability. The one thing I can't figure out how to customize is setting ollama streaming to true so it can post answers in chunks rather than all at once. If anyone is familiar with this project and can see how I might do that I would appreciate any suggestions. It seems like the place to insert that setting would be in &lt;a href="http://llm.py"&gt;llm.py&lt;/a&gt; but I can't get anything successful to happen.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eriknau13"&gt; /u/eriknau13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm6k1u/edit_this_repo_for_streamed_response/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm6k1u/edit_this_repo_for_streamed_response/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jm6k1u/edit_this_repo_for_streamed_response/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T21:46:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm1rqh</id>
    <title>Worth fine-tuning an embedding model specifically for file/folder naming?</title>
    <updated>2025-03-28T18:20:30+00:00</updated>
    <author>
      <name>/u/taxem_tbma</name>
      <uri>https://old.reddit.com/user/taxem_tbma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;br /&gt; I’m not very experienced in AI, but I’ve been experimenting with using embedding models to semantically organize files — basically comparing file names, clustering them, and generating folder names with a local LLM if needed.&lt;/p&gt; &lt;p&gt;Right now I’m using general-purpose embedding models &lt;em&gt;mxbai-embed-large&lt;/em&gt; , but they sometimes miss the mark when it comes to the &lt;em&gt;&amp;quot;folder naming intuition&amp;quot;&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;So my question is:&lt;br /&gt; &lt;strong&gt;Would it make sense to fine-tune a small embedding model specifically for file/folder naming semantics?&lt;/strong&gt;&lt;br /&gt; Or is that overkill for a local tool like this?&lt;/p&gt; &lt;p&gt;For context, I’ve been building a CLI tool called &lt;a href="https://github.com/PerminovEugene/messy-folder-reorganizer-ai"&gt;messy-folder-reorganizer-ai&lt;/a&gt; that does exactly this with Ollama and local vector search.&lt;/p&gt; &lt;p&gt;Would love to hear thoughts or similar experiences.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taxem_tbma"&gt; /u/taxem_tbma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm1rqh/worth_finetuning_an_embedding_model_specifically/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm1rqh/worth_finetuning_an_embedding_model_specifically/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jm1rqh/worth_finetuning_an_embedding_model_specifically/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T18:20:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jll087</id>
    <title>Great event tonight with Ollama and vLLM</title>
    <updated>2025-03-28T02:39:46+00:00</updated>
    <author>
      <name>/u/Rude-Bad-6579</name>
      <uri>https://old.reddit.com/user/Rude-Bad-6579</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jll087/great_event_tonight_with_ollama_and_vllm/"&gt; &lt;img alt="Great event tonight with Ollama and vLLM" src="https://preview.redd.it/6yvrist4fcre1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=90f67ffb2ffcb6b8fada40af2bd2ba6a22bfc94b" title="Great event tonight with Ollama and vLLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Packed house, lots of great attendees. Loved Gemma demo running off 1 Mac laptop live. Super impressive &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rude-Bad-6579"&gt; /u/Rude-Bad-6579 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6yvrist4fcre1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jll087/great_event_tonight_with_ollama_and_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jll087/great_event_tonight_with_ollama_and_vllm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T02:39:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm1a4g</id>
    <title>Mastering Text Chunking with Ollama: A Comprehensive Guide to Advanced Processing</title>
    <updated>2025-03-28T18:00:04+00:00</updated>
    <author>
      <name>/u/KonradFreeman</name>
      <uri>https://old.reddit.com/user/KonradFreeman</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KonradFreeman"&gt; /u/KonradFreeman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://danielkliewer.com/blog/2025-03-28-Ollama-Chunking"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm1a4g/mastering_text_chunking_with_ollama_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jm1a4g/mastering_text_chunking_with_ollama_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T18:00:04+00:00</published>
  </entry>
</feed>
