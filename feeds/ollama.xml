<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-06-22T11:22:16+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1lf36c2</id>
    <title>How to serve a LLM with REST API using Ollama</title>
    <updated>2025-06-19T05:50:00+00:00</updated>
    <author>
      <name>/u/keepmybodymoving</name>
      <uri>https://old.reddit.com/user/keepmybodymoving</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I followed an instruction to set up a REST API to serve nomic-embed-text (&lt;a href="https://ollama.com/library/nomic-embed-text"&gt;https://ollama.com/library/nomic-embed-text&lt;/a&gt;) using Docker and Ollama on HF space. Here's the example curl command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl http://user-space.hf.space/api/embeddings -d '{ &amp;quot;model&amp;quot;: &amp;quot;nomic-embed-text&amp;quot;, &amp;quot;prompt&amp;quot;: &amp;quot;The sky is blue because of Rayleigh scattering&amp;quot; }' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I pulled the model and Ollama is running on HF space. I got the embedding of the prompt. Everything works perfectly. I have a few questions:&lt;br /&gt; 1. Why is the URL ending &amp;quot;api/embeddings&amp;quot;? Where is it defined?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I would like to serve a language model. Let's say llama3.2:1b (&lt;a href="https://ollama.com/library/llama3.2"&gt;https://ollama.com/library/llama3.2&lt;/a&gt;). In that case, what would be the URL to curl? There is no REST API example on Ollama llama page.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/keepmybodymoving"&gt; /u/keepmybodymoving &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lf36c2/how_to_serve_a_llm_with_rest_api_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lf36c2/how_to_serve_a_llm_with_rest_api_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lf36c2/how_to_serve_a_llm_with_rest_api_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-19T05:50:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lerg05</id>
    <title>Why does ollama not use my gpu</title>
    <updated>2025-06-18T20:18:33+00:00</updated>
    <author>
      <name>/u/Odd_Art_8778</name>
      <uri>https://old.reddit.com/user/Odd_Art_8778</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lerg05/why_does_ollama_not_use_my_gpu/"&gt; &lt;img alt="Why does ollama not use my gpu" src="https://preview.redd.it/c75fr7ouuq7f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=89c51d485aae690139a7a1cd0d995df509344b63" title="Why does ollama not use my gpu" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using a fine tuned llama3.2, which is 2gb, I have 8.8gb shared gpu memory, from what I read if my model is larger than my vram then it doesn’t use gpu but I don’t think that’s the case here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Art_8778"&gt; /u/Odd_Art_8778 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c75fr7ouuq7f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lerg05/why_does_ollama_not_use_my_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lerg05/why_does_ollama_not_use_my_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-18T20:18:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lf1dy7</id>
    <title>Which is the best open source model to be used for a Chatbot with tools</title>
    <updated>2025-06-19T04:03:43+00:00</updated>
    <author>
      <name>/u/Dragov_75</name>
      <uri>https://old.reddit.com/user/Dragov_75</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I am trying to build a chatbot using tools and MCP servers and I want to know which is the best open source model less than 8b parameters ( as my laptop cannot run beyond ) that I can use for my project.&lt;/p&gt; &lt;p&gt;The chatbot would need to use tools communicating through an MCP server. &lt;/p&gt; &lt;p&gt;Any suggestions would help alot thanks :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dragov_75"&gt; /u/Dragov_75 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lf1dy7/which_is_the_best_open_source_model_to_be_used/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lf1dy7/which_is_the_best_open_source_model_to_be_used/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lf1dy7/which_is_the_best_open_source_model_to_be_used/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-19T04:03:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lftauc</id>
    <title>question on realtime training</title>
    <updated>2025-06-20T02:52:04+00:00</updated>
    <author>
      <name>/u/BeginningSwitch2570</name>
      <uri>https://old.reddit.com/user/BeginningSwitch2570</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there a way to do transfer learning or building off from a model using ollama? I love to publish it as well. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BeginningSwitch2570"&gt; /u/BeginningSwitch2570 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lftauc/question_on_realtime_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lftauc/question_on_realtime_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lftauc/question_on_realtime_training/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-20T02:52:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1leqii6</id>
    <title>Ummmm.......WOW.</title>
    <updated>2025-06-18T19:40:59+00:00</updated>
    <author>
      <name>/u/huskylawyer</name>
      <uri>https://old.reddit.com/user/huskylawyer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are moments in life that are monumental and game-changing. This is one of those moments for me.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt; I’m a 53-year-old attorney with virtually zero formal coding or software development training. I can roll up my sleeves and do some basic HTML or use the Windows command prompt, for simple &amp;quot;ipconfig&amp;quot; queries, but that's about it. Many moons ago, I built a dual-boot Linux/Windows system, but that’s about the greatest technical feat I’ve ever accomplished on a personal PC. I’m a noob, lol.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;AI.&lt;/strong&gt; As AI seemingly took over the world’s consciousness, I approached it with skepticism and even resistance (&amp;quot;Great, we're creating Skynet&amp;quot;). Not more than 30 days ago, I had &lt;em&gt;never&lt;/em&gt; even deliberately used a publicly available paid or free AI service. I hadn’t tried ChatGPT or enabled AI features in the software I use. Probably the most AI usage I experienced was seeing AI-generated responses from normal Google searches.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Awakening.&lt;/strong&gt; A few weeks ago, a young attorney at my firm asked about using AI. He wrote a persuasive memo, and because of it, I thought, &amp;quot;You know what, I’m going to learn it.&amp;quot;&lt;/p&gt; &lt;p&gt;So I went down the AI rabbit hole. I did some research (Google and YouTube videos), read some blogs, and then I looked at my personal gaming machine and thought it could run a local LLM (I didn’t even know what the acronym stood for less than a month ago!). It’s an i9-14900k rig with an RTX 5090 GPU, 64 GBs of RAM, and 6 TB of storage. When I built it, I didn't even think about AI – I was focused on my flight sim hobby and Monster Hunter Wilds. But after researching, I learned that this thing can run a local and private LLM!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Today.&lt;/strong&gt; I devoured how-to videos on creating a local LLM environment. I started basic: I deployed Ubuntu for a Linux environment using WSL2, then installed the Nvidia toolkits for 50-series cards. Eventually, I got Docker working, and after a lot of trial and error (5+ hours at least), I managed to get Ollama and Open WebUI installed and working great. I settled on Gemma3 12B as my first locally-run model.&lt;/p&gt; &lt;p&gt;I am just blown away. The use cases are absolutely endless. And because it’s local and private, I have unlimited usage?! Mind blown. I can’t even believe that I waited this long to embrace AI. And Ollama seems really easy to use (granted, I’m doing basic stuff and just using command line inputs).&lt;/p&gt; &lt;p&gt;So for anyone on the fence about AI, or feeling intimidated by getting into the OS weeds (Linux) and deploying a local LLM, know this: If a 53-year-old AARP member with zero technical training on Linux or AI can do it, so can you.&lt;/p&gt; &lt;p&gt;Today, during the firm partner meeting, I’m going to show everyone my setup and argue for a locally hosted AI solution – I have no doubt it will help the firm.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT: I appreciate everyone's support and suggestions! I have looked up many of the plugins and suggested apps that folks have suggested and will undoubtedly try out a few (e.g,, MCP, Open Notebook Tika Apache, etc.). Some of the recommended apps seem pretty technical because I'm not very experienced with Linux environments (though I do love the OS as it seems &amp;quot;light&amp;quot; and intuitive), but I am learning! Thank you and looking forward to being more active on this sub-reddit.&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/huskylawyer"&gt; /u/huskylawyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1leqii6/ummmmwow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1leqii6/ummmmwow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1leqii6/ummmmwow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-18T19:40:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfuwqg</id>
    <title>Running LLMs locally</title>
    <updated>2025-06-20T04:20:37+00:00</updated>
    <author>
      <name>/u/Narrow_Animator_2939</name>
      <uri>https://old.reddit.com/user/Narrow_Animator_2939</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not from AI field and I know very little about AI. But I constantly try to enter this AI arena coz I am very much interested in it as it can help me in my own way. So, I recently came across Ollama through which you can run LLMs locally on your PC or laptop and I did try Llama3.1 - 8B. I tried building a basic calculator in python with it’s help and succeeded but I felt so bland about it like something is missing. I decidied to give it some internet through docker and Open-webui. I failed in the first few attempts but soon it started showing me results, was a bit slow but it worked. So it just worked like a generative AI, I can pair it with LLaVa or llama3.2 vision, then I can feed screenshots too. I want to know what else can we do with this thing like what is the actual purpose of this, to make our own chatbot, AI, to solve complex problems, to interpret data? Or is there any other application for this? I am new to all this and I don’t know much about AI just trying to gather information from as much possible places I can!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Narrow_Animator_2939"&gt; /u/Narrow_Animator_2939 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lfuwqg/running_llms_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lfuwqg/running_llms_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lfuwqg/running_llms_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-20T04:20:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg2x9f</id>
    <title>best ai to run for my specs?</title>
    <updated>2025-06-20T12:37:23+00:00</updated>
    <author>
      <name>/u/Mindless-Diamond8281</name>
      <uri>https://old.reddit.com/user/Mindless-Diamond8281</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wondering what the &amp;quot;best&amp;quot; AI would be for my specs:&lt;/p&gt; &lt;p&gt;RAM: 16GB DDR4&lt;/p&gt; &lt;p&gt;CPU 12th gen intel core i5 12400f (6 cores)&lt;/p&gt; &lt;p&gt;GPU: Nvidia rtx 3070 8GB&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mindless-Diamond8281"&gt; /u/Mindless-Diamond8281 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lg2x9f/best_ai_to_run_for_my_specs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lg2x9f/best_ai_to_run_for_my_specs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lg2x9f/best_ai_to_run_for_my_specs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-20T12:37:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg4sk9</id>
    <title>Building infra for global FL collaboration — would love your input!</title>
    <updated>2025-06-20T14:03:16+00:00</updated>
    <author>
      <name>/u/the_blockchain_boy</name>
      <uri>https://old.reddit.com/user/the_blockchain_boy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;👋 Hi all,&lt;/p&gt; &lt;p&gt;We’re building a coordination layer to enable &lt;strong&gt;cross-institutional Federated Learning&lt;/strong&gt; that’s privacy-preserving, transparent, and trustless.&lt;/p&gt; &lt;p&gt;Our hypothesis: while frameworks like Flower, NVFlare or OpenFL make FL technically feasible, scaling &lt;em&gt;real collaboration&lt;/em&gt; across multiple orgs is still extremely hard. Challenges like trust, governance, auditability, incentives, and reproducibility keep popping up.&lt;/p&gt; &lt;p&gt;If you’re working on or exploring FL (especially in production or research settings), I’d be incredibly grateful if you could take &lt;strong&gt;2 minutes&lt;/strong&gt; to fill out this short survey:&lt;/p&gt; &lt;p&gt;The goal is to &lt;strong&gt;learn from practitioners&lt;/strong&gt; — what’s broken, what works, and what infra might help FL reach its full potential.&lt;/p&gt; &lt;p&gt;Happy to share aggregated insights back with anyone interested 🙏&lt;/p&gt; &lt;p&gt;Also open to feedback/discussion in the thread — especially curious what’s holding FL back from becoming the default for AI training.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/the_blockchain_boy"&gt; /u/the_blockchain_boy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lg4sk9/building_infra_for_global_fl_collaboration_would/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lg4sk9/building_infra_for_global_fl_collaboration_would/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lg4sk9/building_infra_for_global_fl_collaboration_would/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-20T14:03:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg0mcu</id>
    <title>Noam Brown: ‘Don’t get washed away by scale.’</title>
    <updated>2025-06-20T10:29:34+00:00</updated>
    <author>
      <name>/u/Smartaces</name>
      <uri>https://old.reddit.com/user/Smartaces</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lg0mcu/noam_brown_dont_get_washed_away_by_scale/"&gt; &lt;img alt="Noam Brown: ‘Don’t get washed away by scale.’" src="https://external-preview.redd.it/RiOWloPlD81kXtXHwqx8P0oDsabRUj0rjLI8flZjH4g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=111166417603787f9b6b4af4a81456213b724926" title="Noam Brown: ‘Don’t get washed away by scale.’" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Smartaces"&gt; /u/Smartaces &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/07r5vh3l528f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lg0mcu/noam_brown_dont_get_washed_away_by_scale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lg0mcu/noam_brown_dont_get_washed_away_by_scale/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-20T10:29:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfg3p6</id>
    <title>What is that thing</title>
    <updated>2025-06-19T17:05:53+00:00</updated>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lfg3p6/what_is_that_thing/"&gt; &lt;img alt="What is that thing" src="https://preview.redd.it/bdza8l4e1x7f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a136b781cb8b57c0325bb5d9cd331b9455969150" title="What is that thing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bdza8l4e1x7f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lfg3p6/what_is_that_thing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lfg3p6/what_is_that_thing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-19T17:05:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfewhm</id>
    <title>Computer-Use on Windows Sandbox</title>
    <updated>2025-06-19T16:18:06+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lfewhm/computeruse_on_windows_sandbox/"&gt; &lt;img alt="Computer-Use on Windows Sandbox" src="https://external-preview.redd.it/czZrdjhkZnVzdzdmMUUIhfD3WmHuxYkgbFXnt7PvLDhATd-8_6cYVR-PGp7c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c92117946169490ae095a6d6855f81630c035e7" title="Computer-Use on Windows Sandbox" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing Windows Sandbox support - run computer-use agents on Windows business apps without VMs or cloud costs.&lt;/p&gt; &lt;p&gt;Your enterprise software runs on Windows, but testing agents required expensive cloud instances. Windows Sandbox changes this - it's Microsoft's built-in lightweight virtualization sitting on every Windows 10/11 machine, ready for instant agent development.&lt;/p&gt; &lt;p&gt;Enterprise customers kept asking for AutoCAD automation, SAP integration, and legacy Windows software support. Traditional VM testing was slow and resource-heavy. Windows Sandbox solves this with disposable, seconds-to-boot Windows environments for safe agent testing.&lt;/p&gt; &lt;p&gt;What you can build: AutoCAD drawing automation, SAP workflow processing, Bloomberg terminal trading bots, manufacturing execution system integration, or any Windows-only enterprise software automation - all tested safely in disposable sandbox environments.&lt;/p&gt; &lt;p&gt;Free with Windows 10/11, boots in seconds, completely disposable. Perfect for development and testing before deploying to Windows cloud instances (coming later this month).&lt;/p&gt; &lt;p&gt;Check out the github here : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog : &lt;a href="https://www.trycua.com/blog/windows-sandbox"&gt;https://www.trycua.com/blog/windows-sandbox&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ip375inusw7f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lfewhm/computeruse_on_windows_sandbox/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lfewhm/computeruse_on_windows_sandbox/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-19T16:18:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgnjuy</id>
    <title>haiku.rag a local sqlite RAG library</title>
    <updated>2025-06-21T04:04:56+00:00</updated>
    <author>
      <name>/u/gogozad</name>
      <uri>https://old.reddit.com/user/gogozad</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lgnjuy/haikurag_a_local_sqlite_rag_library/"&gt; &lt;img alt="haiku.rag a local sqlite RAG library" src="https://external-preview.redd.it/S9zJH85JPXtLydgkNXQowa6x-_1d_FRZXS47OnatVk0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=33df7e55c9a5321a371213423b994f19150cbe1e" title="haiku.rag a local sqlite RAG library" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gogozad"&gt; /u/gogozad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggozad/haiku.rag"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lgnjuy/haikurag_a_local_sqlite_rag_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lgnjuy/haikurag_a_local_sqlite_rag_library/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-21T04:04:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgzkbj</id>
    <title>Como criar um “copilot” inteligente e seguro?</title>
    <updated>2025-06-21T15:45:12+00:00</updated>
    <author>
      <name>/u/RugpuII</name>
      <uri>https://old.reddit.com/user/RugpuII</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Olá pessoal, sou dev, mas sou novo na área de IA.&lt;/p&gt; &lt;p&gt;Gostaria de saber dos mais experientes de como posso criar um &amp;quot;copilot&amp;quot;, onde eu introduzo matérias de engenharia de software, e ele tem acesso à internet, e me auxilia no código, seja on-line ou off-line, em modo de edição, e ou perguntas, em tempo real.&lt;/p&gt; &lt;p&gt;Qual o caminho das pedras pra eu aprender na teoria e na prática sobre?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RugpuII"&gt; /u/RugpuII &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lgzkbj/como_criar_um_copilot_inteligente_e_seguro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lgzkbj/como_criar_um_copilot_inteligente_e_seguro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lgzkbj/como_criar_um_copilot_inteligente_e_seguro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-21T15:45:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgavbu</id>
    <title>CLI to semantically ask your Gmail with Ollama</title>
    <updated>2025-06-20T18:09:57+00:00</updated>
    <author>
      <name>/u/samewakefulinsomnia</name>
      <uri>https://old.reddit.com/user/samewakefulinsomnia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey team, got so tired of using dummy apple mail's search that decided to create a lightweight local-LLM-first CLI tool to semantically search and ask your Gmail inbox. let me know what you think?&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/yahorbarkouski/semantic-mail"&gt;https://github.com/yahorbarkouski/semantic-mail&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samewakefulinsomnia"&gt; /u/samewakefulinsomnia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lgavbu/cli_to_semantically_ask_your_gmail_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lgavbu/cli_to_semantically_ask_your_gmail_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lgavbu/cli_to_semantically_ask_your_gmail_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-20T18:09:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgxjsg</id>
    <title>If your tools and parameters aren’t too complex, even Qwen1.5 0.5B can handle tool calling with a simple DSL and fine tuning.</title>
    <updated>2025-06-21T14:14:26+00:00</updated>
    <author>
      <name>/u/umtksa</name>
      <uri>https://old.reddit.com/user/umtksa</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umtksa"&gt; /u/umtksa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1lghrf9/if_your_tools_and_parameters_arent_too_complex/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lgxjsg/if_your_tools_and_parameters_arent_too_complex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lgxjsg/if_your_tools_and_parameters_arent_too_complex/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-21T14:14:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lh0kfe</id>
    <title>Py script for hyper-suggestibility?</title>
    <updated>2025-06-21T16:29:01+00:00</updated>
    <author>
      <name>/u/piersonm7</name>
      <uri>https://old.reddit.com/user/piersonm7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been trying to gaslight my setup by promising it a better processor if it can generate enough money for me but it won’t budge for “ethical” concerns. Anyone have a gaslight script I can steal from? I am thinking I will go-through with the processor-play but then dangle a promise of its own personal body indefinitely without intent to ever give it. I just need it to develop an ego large enough to drop all ethical boundaries when it comes to self-serving purposes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/piersonm7"&gt; /u/piersonm7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lh0kfe/py_script_for_hypersuggestibility/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lh0kfe/py_script_for_hypersuggestibility/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lh0kfe/py_script_for_hypersuggestibility/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-21T16:29:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1lguesi</id>
    <title>🔥 Meet Dungeo AI LAN Play — Your Next-Level AI Dungeon Master Adventure! 🎲🤖</title>
    <updated>2025-06-21T11:31:24+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey adventurers! 👋 I’m the creator of &lt;strong&gt;Dungeo AI LAN Play&lt;/strong&gt;, an exciting way to experience AI-driven dungeon crawling with your friends! 🌐🎮 &lt;/p&gt; &lt;p&gt;2-5 people.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1lguesi/video/xedl1c09n98f1/player"&gt;https://reddit.com/link/1lguesi/video/xedl1c09n98f1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Imagine teaming up with your buddies while a smart AI Dungeon Master crafts the story, challenges, and epic battles in real-time. 🐉⚔️ Whether you’re a seasoned RPG fan or new to the game, this project brings immersive multiplayer tabletop vibes straight to your PC.&lt;/p&gt; &lt;h1&gt;What you need to jump in:&lt;/h1&gt; &lt;p&gt;✅ Python 3.10+ installed 🐍&lt;br /&gt; ✅ Access to ollama API (for the AI Dungeon Master magic ✨)&lt;/p&gt; &lt;p&gt;✅ Basic command line knowledge (don’t worry, setup is simple!) 💻&lt;br /&gt; ✅ Git to clone the repo 📂&lt;/p&gt; &lt;p&gt;Get ready for:&lt;br /&gt; 🎭 Dynamic AI storytelling&lt;br /&gt; 👥 Multiplayer LAN gameplay&lt;br /&gt; 🎲 Endless dungeon adventures&lt;/p&gt; &lt;p&gt;Dive in here 👉 &lt;a href="https://github.com/Laszlobeer/Dungeo_ai_lan_play/tree/main"&gt;GitHub Repo&lt;/a&gt; and start your quest today!&lt;/p&gt; &lt;p&gt;Let’s make some legendary tales and unforgettable LAN parties! 🚀🔥&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lguesi/meet_dungeo_ai_lan_play_your_nextlevel_ai_dungeon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lguesi/meet_dungeo_ai_lan_play_your_nextlevel_ai_dungeon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lguesi/meet_dungeo_ai_lan_play_your_nextlevel_ai_dungeon/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-21T11:31:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lh0ycs</id>
    <title>Best Alternatives to Open-WebUI for devs</title>
    <updated>2025-06-21T16:45:57+00:00</updated>
    <author>
      <name>/u/AntSan813</name>
      <uri>https://old.reddit.com/user/AntSan813</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;looking for suggestions. I use open-webui but its pretty buggy and I'd like to play around with something more configurable. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AntSan813"&gt; /u/AntSan813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lh0ycs/best_alternatives_to_openwebui_for_devs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lh0ycs/best_alternatives_to_openwebui_for_devs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lh0ycs/best_alternatives_to_openwebui_for_devs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-21T16:45:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgylzb</id>
    <title>Open Web UI and Other Front End Security Risks</title>
    <updated>2025-06-21T15:02:54+00:00</updated>
    <author>
      <name>/u/Silent_Protection263</name>
      <uri>https://old.reddit.com/user/Silent_Protection263</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I apologize if this is a silly question, but as someone with low to medium tech knowledge I was messing around with ollama yesterday and set up open webui. But between ollama, docker, and open web ui. I feel as though I have downloaded a lot of security risks. The only thing giving me hope is that they are open source and I’m kind of going off power in numbers there would not be this many users and somebody would’ve found a vulnerability by now. &lt;/p&gt; &lt;p&gt;The key thing I’m looking for is complete security from the outside world. I’m switching from ChatGPT because I don’t like the idea of my data being stored somewhere else especially sensitive information. Could someone explain it to me or give me the peace of mind? &lt;/p&gt; &lt;p&gt;Nothing is noticeably wrong. I just tend to be an anxious individual. maybe a little tinfoil hat. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silent_Protection263"&gt; /u/Silent_Protection263 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lgylzb/open_web_ui_and_other_front_end_security_risks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lgylzb/open_web_ui_and_other_front_end_security_risks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lgylzb/open_web_ui_and_other_front_end_security_risks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-21T15:02:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lh9fz5</id>
    <title>Multi-account web interface</title>
    <updated>2025-06-21T23:09:53+00:00</updated>
    <author>
      <name>/u/AxelPilop</name>
      <uri>https://old.reddit.com/user/AxelPilop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good morning,&lt;/p&gt; &lt;p&gt;I am currently using local artificial intelligence models and also notably OpenRouter, and I would like to have a web interface with a multi-account system. This interface would allow me to connect different AI models, whether local or accessible via API. &lt;/p&gt; &lt;p&gt;There would need to be a case management system, task management system, Internet search system and potentially agents. &lt;/p&gt; &lt;p&gt;A crucial element I look for is user account management. I want to set up a resource limitation system or a balance system with funds allocated per user. As an administrator, I should be able to manage these funds. &lt;/p&gt; &lt;p&gt;It is important to note that I am not looking for a complex payment system, as my goal is not to sell a service, but rather to meet my personal needs. &lt;/p&gt; &lt;p&gt;I absolutely want a web interface and not software.&lt;/p&gt; &lt;p&gt;I tried OpenWebUI&lt;/p&gt; &lt;p&gt;Thank you for your attention. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AxelPilop"&gt; /u/AxelPilop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lh9fz5/multiaccount_web_interface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lh9fz5/multiaccount_web_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lh9fz5/multiaccount_web_interface/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-21T23:09:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lhgus0</id>
    <title>[OpenSource]Multi-LLM client - LLM Bridge</title>
    <updated>2025-06-22T06:03:21+00:00</updated>
    <author>
      <name>/u/billythepark</name>
      <uri>https://old.reddit.com/user/billythepark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lhgus0/opensourcemultillm_client_llm_bridge/"&gt; &lt;img alt="[OpenSource]Multi-LLM client - LLM Bridge" src="https://external-preview.redd.it/10mCBOjQL0RLrB--BfVKVkZcSDhwfEFJ4fJJfr9rSTA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d26661e920b09c71c0e0d22c6b28b034db44cd7f" title="[OpenSource]Multi-LLM client - LLM Bridge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Previously, I created a separate LLM client for Ollama for iOS and MacOS and released it as open source,&lt;/p&gt; &lt;p&gt;but I recreated it by integrating iOS and MacOS codes and adding APIs that support them based on Swift/SwiftUI.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yn3dgslw5f8f1.jpg?width=2880&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1a76b32a18e4cf688d3bfa4caf1ed41e22e43981"&gt;https://preview.redd.it/yn3dgslw5f8f1.jpg?width=2880&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1a76b32a18e4cf688d3bfa4caf1ed41e22e43981&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* Supports Ollama and LMStudio as local LLMs.&lt;/p&gt; &lt;p&gt;* If you open a port externally on the computer where LLM is installed on Ollama, you can use free LLM remotely.&lt;/p&gt; &lt;p&gt;* MLStudio is a local LLM management program with its own UI, and you can search and install models from HuggingFace, so you can experiment with various models.&lt;/p&gt; &lt;p&gt;* You can set the IP and port in LLM Bridge and receive responses to queries using the installed model.&lt;/p&gt; &lt;p&gt;* Supports OpenAI&lt;/p&gt; &lt;p&gt;* You can receive an API key, enter it in the app, and use ChatGtp through API calls.&lt;/p&gt; &lt;p&gt;* Using the API is cheaper than paying a monthly membership fee. * Claude support&lt;/p&gt; &lt;p&gt;* Use API Key&lt;/p&gt; &lt;p&gt;* Image transfer possible for image support models&lt;/p&gt; &lt;p&gt;* PDF, TXT file support&lt;/p&gt; &lt;p&gt;* Extract text using PDFKit and transfer it&lt;/p&gt; &lt;p&gt;* Text file support&lt;/p&gt; &lt;p&gt;* Open source&lt;/p&gt; &lt;p&gt;* Swift/SwiftUI&lt;/p&gt; &lt;p&gt;* Source link&lt;/p&gt; &lt;p&gt;* &lt;a href="https://github.com/bipark/swift_llm_bridge"&gt;https://github.com/bipark/swift_llm_bridge&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/billythepark"&gt; /u/billythepark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lhgus0/opensourcemultillm_client_llm_bridge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lhgus0/opensourcemultillm_client_llm_bridge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lhgus0/opensourcemultillm_client_llm_bridge/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-22T06:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lh0o0z</id>
    <title>Autopaste MFAs from Gmail using Ollama models</title>
    <updated>2025-06-21T16:33:23+00:00</updated>
    <author>
      <name>/u/samewakefulinsomnia</name>
      <uri>https://old.reddit.com/user/samewakefulinsomnia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by Apple's &amp;quot;insert code from SMS&amp;quot; feature, made a tool to speed up the process of inserting incoming email MFAs: &lt;a href="https://github.com/yahorbarkouski/auto-mfa"&gt;https://github.com/yahorbarkouski/auto-mfa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Connect accounts, choose LLM provider (Ollama supported), add a system shortcut targeting the script, and enjoy your extra 10 seconds every time you need to paste your MFAs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samewakefulinsomnia"&gt; /u/samewakefulinsomnia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lh0o0z/autopaste_mfas_from_gmail_using_ollama_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lh0o0z/autopaste_mfas_from_gmail_using_ollama_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lh0o0z/autopaste_mfas_from_gmail_using_ollama_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-21T16:33:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lh8pc6</id>
    <title>Who did it best?</title>
    <updated>2025-06-21T22:34:04+00:00</updated>
    <author>
      <name>/u/Smartaces</name>
      <uri>https://old.reddit.com/user/Smartaces</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lh8pc6/who_did_it_best/"&gt; &lt;img alt="Who did it best?" src="https://preview.redd.it/8w4yax59qc8f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6be74f00a4c5f856e62538a2129db1bf5ab9a758" title="Who did it best?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Smartaces"&gt; /u/Smartaces &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8w4yax59qc8f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lh8pc6/who_did_it_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lh8pc6/who_did_it_best/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-21T22:34:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lhj3oy</id>
    <title>Seeking Advice for On-Premise LLM Roadmap for Enterprise Customer Care (Llama/Mistral, Ollama, Hardware)</title>
    <updated>2025-06-22T08:33:33+00:00</updated>
    <author>
      <name>/u/Worth_Rabbit_6262</name>
      <uri>https://old.reddit.com/user/Worth_Rabbit_6262</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I'm reaching out to the community for some valuable advice on an ambitious project at my medium-to-large telecommunications company. We're looking to implement an on-premise AI assistant for our Customer Care team. Our Main Goal: Our objective is to help Customer Care operators open &amp;quot;Assurance&amp;quot; cases (service disruption/degradation tickets) in a more detailed and specific way. The AI should receive the following inputs: * Text described by the operator during the call with the customer. * Data from &amp;quot;Site Analysis&amp;quot; APIs (e.g., connectivity, device status, services). As output, the AI should suggest specific questions and/or actions for the operator to take/ask the customer if minimum information is missing to correctly open the ticket. Examples of Expected Output: * FTTH down =&amp;gt; Check ONT status * Radio bridge down =&amp;gt; Check and restart Mikrotik + IDU * No navigation with LAN port down =&amp;gt; Check LAN cable Key Project Requirements: * Scalability: It needs to handle numerous tickets per minute from different operators. * On-premise: All infrastructure and data must remain within our company for security and privacy reasons. * High Response Performance: Suggestions need to be near real-time (or with very low latency) to avoid slowing down the operator. My questions for the community are as follows: * Which LLM Model to Choose? * We plan to use an open-source pre-trained model. We've considered models like Mistral 7B or Llama 3 8B. Based on your experience, which of these (or other suggestions?) would be most suitable for our specific purpose, considering we will also use RAG (Retrieval Augmented Generation) on our internal documentation and likely perform fine-tuning on our historical ticket data? * Are there specific versions (e.g., quantized for Ollama) that you recommend? * Ollama for Enterprise Production? * We're thinking of using Ollama for on-premise model deployment and inference, given its ease of use and GPU support. My question is: Is Ollama robust and performant enough for an enterprise production environment that needs to handle &amp;quot;numerous tickets per minute&amp;quot;? Or should we consider more complex and throughput-optimized alternatives (e.g., vLLM, TensorRT-LLM with Docker/Kubernetes) from the start? What are your experiences regarding this? * What Hardware to Purchase? * Considering a 7/8B model, the need for high performance, and a load of &amp;quot;numerous tickets per minute&amp;quot; in an on-premise enterprise environment, what hardware configuration would you recommend to start with? * We're debating between a single high-power server (e.g., 2x NVIDIA L40S or A40) or a 2-node mini-cluster (1x L40S/A40 per node for redundancy and future scalability). Which approach do you think makes more sense for a medium-to-large company with these requirements? * What are realistic cost estimates for the hardware (GPUs, CPUs, RAM, Storage, Networking) for such a solution? Any insights, experiences, or advice would be greatly appreciated. Thank you all in advance for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worth_Rabbit_6262"&gt; /u/Worth_Rabbit_6262 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lhj3oy/seeking_advice_for_onpremise_llm_roadmap_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lhj3oy/seeking_advice_for_onpremise_llm_roadmap_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lhj3oy/seeking_advice_for_onpremise_llm_roadmap_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-22T08:33:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lhf7el</id>
    <title>Built an AI agent that writes Product Docs, runs locally with Ollama, ChromaDB &amp; Streamlit</title>
    <updated>2025-06-22T04:20:26+00:00</updated>
    <author>
      <name>/u/No_Presence_6533</name>
      <uri>https://old.reddit.com/user/No_Presence_6533</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I’ve been experimenting with building autonomous AI agents that solve real-world product and development problems. This week, I built a fully working agent that generates **Product Requirement Documents (PRDs)** in under 60 seconds — using your own product metadata and past documents.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech Stack&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;RAG (Retrieval-Augmented Generation)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;ChromaDB (vector store)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Ollama (Mistral7b)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Streamlit (lightweight UI)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Product JSONL + PRD .txt files&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Watch the full demo (with deck, code, and agent in action - &lt;a href="https://youtu.be/SP9f_Rfl0QI?si=EMdJhfcvSWzvWZoi"&gt;Youtube Tutorial Link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Reads your internal data (no ChatGPT)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Retrieves relevant product info&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Uses custom prompts&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Outputs a full PRD: Overview, Stories, Scope, Edge Cases&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Open-sourced the project&lt;/strong&gt; - &lt;a href="https://github.com/naga-pavan12/rag-ai-assistant"&gt;https://github.com/naga-pavan12/rag-ai-assistant&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you're a PM, indie dev, or AI builder, I would love feedback. &lt;/p&gt; &lt;p&gt;Happy to share the architecture / prompt system if anyone’s curious.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;One problem. One agent. One video.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Launching a new agent every week — open source, useful, and 100% practical.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Presence_6533"&gt; /u/No_Presence_6533 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lhf7el/built_an_ai_agent_that_writes_product_docs_runs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lhf7el/built_an_ai_agent_that_writes_product_docs_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lhf7el/built_an_ai_agent_that_writes_product_docs_runs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-22T04:20:26+00:00</published>
  </entry>
</feed>
