<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-24T12:26:16+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1i8c343</id>
    <title>How can I offload multiple models into ram instead of reloading from drive, if they do not all fit into vram?</title>
    <updated>2025-01-23T19:44:52+00:00</updated>
    <author>
      <name>/u/hawxxer</name>
      <uri>https://old.reddit.com/user/hawxxer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like the title say, I use multiple models &amp;quot;at the same time&amp;quot; switching between them. So I use one model for code completion and a bigger, different one, for code assist. Mostly there is another general conversation in the background. They don't fit all three into my vram, so I guess ollama is unloading the last active model, if i make a request on a different model, when they both wont fit into vram. Is there a way to store the current in use models in ram instead of reloading them from disk to speed up switching?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hawxxer"&gt; /u/hawxxer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8c343/how_can_i_offload_multiple_models_into_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8c343/how_can_i_offload_multiple_models_into_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8c343/how_can_i_offload_multiple_models_into_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T19:44:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i87mtt</id>
    <title>infinite download, ollama, deepseek models</title>
    <updated>2025-01-23T16:42:47+00:00</updated>
    <author>
      <name>/u/ext115</name>
      <uri>https://old.reddit.com/user/ext115</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can't download the language model with Ollama. While downloading the percentages go back and forth, the process takes hours until the session expires and it uses all my internet connections - cannot do anything with the internet. Do you have a similar problem?&lt;/p&gt; &lt;p&gt;ps&lt;br /&gt; i was only able to download smallest deepseek model deepseek-r1:1.5b &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ext115"&gt; /u/ext115 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i87mtt/infinite_download_ollama_deepseek_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i87mtt/infinite_download_ollama_deepseek_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i87mtt/infinite_download_ollama_deepseek_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T16:42:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8blli</id>
    <title>Issues running DeepSeek-R1</title>
    <updated>2025-01-23T19:24:30+00:00</updated>
    <author>
      <name>/u/branhama</name>
      <uri>https://old.reddit.com/user/branhama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So hearing all the good things about the model I wanted to try it out. Tried numerous version from 7b to 14b models and I get jibberish.&lt;/p&gt; &lt;p&gt;Usage: Development tasks within VSCode using Cline.&lt;/p&gt; &lt;p&gt;Request: Lets start a new project called ServerDocumentor. In this project begin a new powershell module which will be used to pull remote windows server information for documentation purposes.&lt;/p&gt; &lt;p&gt;LLM Response: First, I need to decide which programming language to use. Since the user didn't specify, I'll go with Python because it's widely used and straightforward for beginners.&lt;/p&gt; &lt;p&gt;Next, I should figure out where to save the file. The current working directory is C:/Users/abranham/Documents/testing, so I'll create the file there. Naming it hello_world.py makes sense because it's clear and follows common naming conventions.&lt;/p&gt; &lt;p&gt;Now, I need to write the code. In Python, printing &amp;quot;Hello World!&amp;quot; is simple. I'll use the print function: print(&amp;quot;Hello World!&amp;quot;). That should be the only line needed.&lt;/p&gt; &lt;p&gt;I don't have any open tabs in VSCode, so I'll create a new file. Using the create_file tool, I can specify the path and content. The path will&lt;/p&gt; &lt;p&gt;The system seems to be stuck on doing 2 tasks only, develop some python code or build a website. It never seems to follow anything I ask it to do.&lt;/p&gt; &lt;p&gt;I have ensured I am following the general guidlines for running at &lt;a href="https://github.com/deepseek-ai/DeepSeek-R1?tab=readme-ov-file#usage-recommendations"&gt;https://github.com/deepseek-ai/DeepSeek-R1?tab=readme-ov-file#usage-recommendations&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Any ideas? This is the first model I have run that acts like this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/branhama"&gt; /u/branhama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8blli/issues_running_deepseekr1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8blli/issues_running_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8blli/issues_running_deepseekr1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T19:24:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1i87rs7</id>
    <title>Connect multiple Mac Minis for fast DeepSeek R1 performance</title>
    <updated>2025-01-23T16:48:30+00:00</updated>
    <author>
      <name>/u/Exciting-Syrup-1107</name>
      <uri>https://old.reddit.com/user/Exciting-Syrup-1107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there,&lt;/p&gt; &lt;p&gt;since using DeepSeek R1 I‘m baffled by the fact that I can use a nearly o1 level LLM on my local device. But it‘s just super slow. I have seen a YouTube video (i don‘t remember it exactly anymore) where a guy talks about connecting 2 or 3 Mac Minis to achieve a fast performing local DeepSeek R1.&lt;/p&gt; &lt;p&gt;My question now is: How can you do that? Are there certain tools that can „split“ the workload of ollama onto multiple devices? Are there online tools where you can rent fast computing space to run your own instance?&lt;/p&gt; &lt;p&gt;I‘m really curious about this topic and would love to learn more.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Exciting-Syrup-1107"&gt; /u/Exciting-Syrup-1107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i87rs7/connect_multiple_mac_minis_for_fast_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i87rs7/connect_multiple_mac_minis_for_fast_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i87rs7/connect_multiple_mac_minis_for_fast_deepseek_r1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T16:48:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7ygrp</id>
    <title>Good UI for DeepSeek R1</title>
    <updated>2025-01-23T07:56:24+00:00</updated>
    <author>
      <name>/u/AdAccomplished8942</name>
      <uri>https://old.reddit.com/user/AdAccomplished8942</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm running DeepSeek R1 (32b) on my Apple M3 MacBook Pro and it works pretty well, but I can only use it in Terminal because I haven't found a nice UI for it.&lt;/p&gt; &lt;p&gt;Msty looks really good and has great features, but it forgets previous prompts and answers. Basically every prompt is like a completely new chat, which makes many things impossible.&lt;/p&gt; &lt;p&gt;What are some nice UIs for DeepSeek R1 on ollama?&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdAccomplished8942"&gt; /u/AdAccomplished8942 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7ygrp/good_ui_for_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7ygrp/good_ui_for_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7ygrp/good_ui_for_deepseek_r1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T07:56:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7yycc</id>
    <title>what can do now?</title>
    <updated>2025-01-23T08:33:23+00:00</updated>
    <author>
      <name>/u/Own-Perception-1574</name>
      <uri>https://old.reddit.com/user/Own-Perception-1574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7yycc/what_can_do_now/"&gt; &lt;img alt="what can do now?" src="https://preview.redd.it/y8xv9gwwfpee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f7701dbfaa233f2456f822ab6bf09b6cd5f6c5e" title="what can do now?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Perception-1574"&gt; /u/Own-Perception-1574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y8xv9gwwfpee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7yycc/what_can_do_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7yycc/what_can_do_now/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T08:33:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7nqrj</id>
    <title>Run a fully local AI Search / RAG pipeline using Ollama with 4GB of memory and no GPU</title>
    <updated>2025-01-22T22:20:43+00:00</updated>
    <author>
      <name>/u/LeetTools</name>
      <uri>https://old.reddit.com/user/LeetTools</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, for people that want to run AI search and RAG pipelines locally, you can now build your local knowledge base with one line of command and everything runs locally with no docker or API key required. Repo is here: &lt;a href="https://github.com/leettools-dev/leettools"&gt;https://github.com/leettools-dev/leettools&lt;/a&gt;. The total memory usage is around 4GB with the Llama3.2 model: * llama3.2:latest 3.5 GB * nomic-embed-text:latest 370 MB * LeetTools: 350MB (Document pipeline backend with Python and DuckDB)&lt;/p&gt; &lt;p&gt;First, follow the instructions on &lt;a href="https://github.com/ollama/ollama"&gt;https://github.com/ollama/ollama&lt;/a&gt; to install the ollama program. Make sure the ollama program is running.&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;set up&lt;/h1&gt; &lt;p&gt;ollama pull llama3.2 ollama pull nomic-embed-text pip install leettools curl -fsSL -o .env.ollama &lt;a href="https://raw.githubusercontent.com/leettools-dev/leettools/refs/heads/main/env.ollama"&gt;https://raw.githubusercontent.com/leettools-dev/leettools/refs/heads/main/env.ollama&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;one command line to download a PDF and save it to the graphrag KB&lt;/h1&gt; &lt;p&gt;leet kb add-url -e .env.ollama -k graphrag -l info &lt;a href="https://arxiv.org/pdf/2501.09223"&gt;https://arxiv.org/pdf/2501.09223&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;now you query the local graphrag KB with questions&lt;/h1&gt; &lt;p&gt;leet flow -t answer -e .env.ollama -k graphrag -l info -p retriever_type=local -q &amp;quot;How does GraphRAG work?&amp;quot; ```&lt;/p&gt; &lt;p&gt;You can also add your local directory or files to the knowledge base using &lt;code&gt;leet kb add-local&lt;/code&gt; command.&lt;/p&gt; &lt;p&gt;For the above default setup, we are using * docling to convert PDF to markdown * chonkie as the chunker * nomic-embed-text as the embedding model * llama3.2 as the inference engine * Duckdb as the data storage include graph and vector&lt;/p&gt; &lt;p&gt;We think it might be helpful for some usage scenarios that require local deployment and resource limits. Questions or suggestions are welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeetTools"&gt; /u/LeetTools &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7nqrj/run_a_fully_local_ai_search_rag_pipeline_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7nqrj/run_a_fully_local_ai_search_rag_pipeline_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7nqrj/run_a_fully_local_ai_search_rag_pipeline_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T22:20:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8pjrm</id>
    <title>ollama on mac via dmg install or brew install</title>
    <updated>2025-01-24T06:48:51+00:00</updated>
    <author>
      <name>/u/discoveringnature12</name>
      <uri>https://old.reddit.com/user/discoveringnature12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone is there a difference between installing ollama using the Mac DMG or we brew install. The only difference I see is if I install the app using DMG it always keeps running the server in the background, with brow installer, I have to explicitly start the server. Any other major differences?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/discoveringnature12"&gt; /u/discoveringnature12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8pjrm/ollama_on_mac_via_dmg_install_or_brew_install/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8pjrm/ollama_on_mac_via_dmg_install_or_brew_install/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8pjrm/ollama_on_mac_via_dmg_install_or_brew_install/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T06:48:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8h7sd</id>
    <title>Param Count vs FP Precision</title>
    <updated>2025-01-23T23:23:25+00:00</updated>
    <author>
      <name>/u/PBlague</name>
      <uri>https://old.reddit.com/user/PBlague</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this is not an exact science and that models with the same amount of parameters and same floating point precision can have vastly different results based on the benchmark and how they've been trained and the model architecture. But I wanna know which one is gonna be generally better...&lt;/p&gt; &lt;p&gt;For example is the fp16 llama3.2 8b gonna be better than the q_2 llama3.2 70b? They have comparable amounts of data, somewhere around 25gbs.&lt;/p&gt; &lt;p&gt;I can't run the 70b with my current hardware and I doubt I will be able to run the fp16 8b either, but it seems more possible, so I was wondering if it's going to be comparable in accuracy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PBlague"&gt; /u/PBlague &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8h7sd/param_count_vs_fp_precision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8h7sd/param_count_vs_fp_precision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8h7sd/param_count_vs_fp_precision/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T23:23:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8qav3</id>
    <title>A list of all the top Open Source Chat UI for ollama/any LLM in general. (community edition)</title>
    <updated>2025-01-24T07:43:57+00:00</updated>
    <author>
      <name>/u/VisibleLawfulness246</name>
      <uri>https://old.reddit.com/user/VisibleLawfulness246</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's my list right now&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open Web UI&lt;/li&gt; &lt;li&gt;LibreChat&lt;/li&gt; &lt;li&gt;anythingLLM&lt;/li&gt; &lt;li&gt;GPT4all&lt;/li&gt; &lt;li&gt;oobabooga&lt;/li&gt; &lt;li&gt;verba&lt;/li&gt; &lt;li&gt;dify&lt;/li&gt; &lt;li&gt;SillyTavern&lt;/li&gt; &lt;li&gt;Danswer&lt;/li&gt; &lt;li&gt;Lobe Ui&lt;/li&gt; &lt;li&gt;hugging face chat-Ui&lt;/li&gt; &lt;li&gt;kobold Cpp/ for from llama cpp&lt;/li&gt; &lt;li&gt;private gpt&lt;/li&gt; &lt;li&gt;serge chat&lt;/li&gt; &lt;li&gt;JanHQ&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What am I missing from this list? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VisibleLawfulness246"&gt; /u/VisibleLawfulness246 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8qav3/a_list_of_all_the_top_open_source_chat_ui_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8qav3/a_list_of_all_the_top_open_source_chat_ui_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8qav3/a_list_of_all_the_top_open_source_chat_ui_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T07:43:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8re2x</id>
    <title>Ghif: llm-friendly GitHub issue fetcher</title>
    <updated>2025-01-24T09:08:18+00:00</updated>
    <author>
      <name>/u/kiwami_zamurai</name>
      <uri>https://old.reddit.com/user/kiwami_zamurai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/kiwamizamurai/ghif"&gt;https://github.com/kiwamizamurai/ghif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I developed a way to bring repository issues to a local environment, hoping it might make AI development a bit more enjoyable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kiwami_zamurai"&gt; /u/kiwami_zamurai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8re2x/ghif_llmfriendly_github_issue_fetcher/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8re2x/ghif_llmfriendly_github_issue_fetcher/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8re2x/ghif_llmfriendly_github_issue_fetcher/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T09:08:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8f0cm</id>
    <title>I'm new to local LLMs but this is so inaccurate</title>
    <updated>2025-01-23T21:46:50+00:00</updated>
    <author>
      <name>/u/vsurresh</name>
      <uri>https://old.reddit.com/user/vsurresh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm new to running local LLMs and I'm only running the 8b version. I decided to try 'deepseek-r1:8b' and decided to ask a simple question. Looks like the training ended before the F1 championship was over but it says Lewis Hamilton has retired which is completely inaccurate. Is this type of response normal for locally running small LLMs? (screenshot attached) Thanks in advance&lt;/p&gt; &lt;p&gt;Edit - Adding the output&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; Who won 2024 F1 championship? &amp;lt;think&amp;gt; Okay, the user just asked, &amp;quot;Who won 2024 F1 championship?&amp;quot; Hmm, wait a minute. I remember that as of my last update in July 2024, the 2024 Formula 1 season was still ongoing. The races hadn't concluded yet, so it's not possible to determine the winner at that point. I should check if there have been any changes or updates since then. Let me think—did Lewis Hamilton win another championship? No, he retired from F1 in 2023. Max Verstappen won the 2023 championship but was involved in an incident at the 2024 Spanish Grand Prix where he crashed heavily. Wait, I'm getting confused because the user is asking about 2024, and my knowledge cutoff is July 2024. So, as of now, the championship is still being competed for, and the season hasn't ended yet. Therefore, the winner isn't determined until the end of the season. I should make sure to inform the user that the season is ongoing and the winner will be known after all races are completed. Also, I need to mention that Lewis Hamilton retired, so he's not competing anymore. It's important to provide accurate information without speculation since the championship isn't decided yet. &amp;lt;/think&amp;gt; As of my last update in July 2024, the 2024 Formula 1 season is still ongoing, and the champion has not yet been determined. The season concludes after all races are completed, so the winner will be known by the end of the year. Keep in mind that Lewis Hamilton retired from Formula 1 at the end of the 2023 season, so he is no longer competing. Max Verstappen won the 2023 championship but faced a challenging defense in 2024. The 2024 championship is still up for grabs, and it will be decided after the final race of the season. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vsurresh"&gt; /u/vsurresh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8f0cm/im_new_to_local_llms_but_this_is_so_inaccurate/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8f0cm/im_new_to_local_llms_but_this_is_so_inaccurate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8f0cm/im_new_to_local_llms_but_this_is_so_inaccurate/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T21:46:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8spym</id>
    <title>Recovering lost model files due to forced windows update</title>
    <updated>2025-01-24T10:48:26+00:00</updated>
    <author>
      <name>/u/justquacks</name>
      <uri>https://old.reddit.com/user/justquacks</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i8spym/recovering_lost_model_files_due_to_forced_windows/"&gt; &lt;img alt="Recovering lost model files due to forced windows update" src="https://b.thumbs.redditmedia.com/froMZXhP8yqyX2g1t-7Ts1xdg_M6eJB3_wSLVYePBKs.jpg" title="Recovering lost model files due to forced windows update" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just spent 14 hours downloading a 400 gigabyte model.&lt;/p&gt; &lt;p&gt;I got about 60% of the way through when windows update force restarted the computer. &lt;/p&gt; &lt;p&gt;Sometimes, ollama has end of file errors or the connection times out. Many times my downloads were able to be resumed and there were zero issues.&lt;/p&gt; &lt;p&gt;I dont know what caused this, but ALL of the files from that download are gone. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Does anyone know of any recovery software that doesnt overwrite data? Software that just makes data available again?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As far as I know, recuva is absolutely NOT the tool for this-&lt;/p&gt; &lt;p&gt;I was writing this post when I was again prompted to update windows on the server pc- I cannot express how full of rage I am after losing 14 hours of download.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/far7dj7p7xee1.png?width=714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=662c07ebe2c446dc655fc71c9e8b68191ae11f1e"&gt;https://preview.redd.it/far7dj7p7xee1.png?width=714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=662c07ebe2c446dc655fc71c9e8b68191ae11f1e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/justquacks"&gt; /u/justquacks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8spym/recovering_lost_model_files_due_to_forced_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8spym/recovering_lost_model_files_due_to_forced_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8spym/recovering_lost_model_files_due_to_forced_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T10:48:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8trrd</id>
    <title>ollama local models disappeared on my linux machine</title>
    <updated>2025-01-24T11:59:43+00:00</updated>
    <author>
      <name>/u/wo-tatatatatata</name>
      <uri>https://old.reddit.com/user/wo-tatatatatata</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;~/.ollama only has a 'history' file, ollama models are nowhere under /, other than the app binary, here is the result after running: ollama list&lt;/p&gt; &lt;p&gt;NAME ID SIZE MODIFIED&lt;/p&gt; &lt;p&gt;llama3.3:latest a6eb4748fd29 42 GB 3 weeks ago&lt;/p&gt; &lt;p&gt;incept5/llama3.1-claude:latest 4ba850d59c62 4.7 GB 3 weeks ago&lt;/p&gt; &lt;p&gt;codellama:13b-instruct 9f438cb9cd58 7.4 GB 3 weeks ago&lt;/p&gt; &lt;p&gt;mistral:latest f974a74358d6 4.1 GB 3 weeks ago&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wo-tatatatatata"&gt; /u/wo-tatatatatata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8trrd/ollama_local_models_disappeared_on_my_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8trrd/ollama_local_models_disappeared_on_my_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8trrd/ollama_local_models_disappeared_on_my_linux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T11:59:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8r971</id>
    <title>I want to try to replicate the RAG functionality similar to LM Studio using open source tools. Any ideas on where to start?</title>
    <updated>2025-01-24T08:57:53+00:00</updated>
    <author>
      <name>/u/ikmalsaid</name>
      <uri>https://old.reddit.com/user/ikmalsaid</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ikmalsaid"&gt; /u/ikmalsaid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8r971/i_want_to_try_to_replicate_the_rag_functionality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8r971/i_want_to_try_to_replicate_the_rag_functionality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8r971/i_want_to_try_to_replicate_the_rag_functionality/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T08:57:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8lza7</id>
    <title>AI-Powered Bot for Automated Job Applications on LinkedIn</title>
    <updated>2025-01-24T03:15:40+00:00</updated>
    <author>
      <name>/u/Own-Perception-1574</name>
      <uri>https://old.reddit.com/user/Own-Perception-1574</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to develop an AI-powered bot capable of automating the job application process on LinkedIn. This bot would analyze job descriptions (JDs), customize the CV or resume based on the requirements, and apply to relevant positions automatically. Does a similar AI solution already exist in the market?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Perception-1574"&gt; /u/Own-Perception-1574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8lza7/aipowered_bot_for_automated_job_applications_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8lza7/aipowered_bot_for_automated_job_applications_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8lza7/aipowered_bot_for_automated_job_applications_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T03:15:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8k8u7</id>
    <title>I added ollama support for an open-source operator agent</title>
    <updated>2025-01-24T01:47:28+00:00</updated>
    <author>
      <name>/u/Swimming_Driver4974</name>
      <uri>https://old.reddit.com/user/Swimming_Driver4974</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created a similar Operator Agent framework like OpenAI's one, and made it open-source. Just added support for Ollama so it can be run completely locally using text and vision models. It's not perfect, but has potential: &lt;a href="https://github.com/GPT-Protocol/007-agent"&gt;https://github.com/GPT-Protocol/007-agent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Swimming_Driver4974"&gt; /u/Swimming_Driver4974 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8k8u7/i_added_ollama_support_for_an_opensource_operator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8k8u7/i_added_ollama_support_for_an_opensource_operator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8k8u7/i_added_ollama_support_for_an_opensource_operator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T01:47:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8m6ls</id>
    <title>Llama 3.1 405B + 8x AMD Instinct Mi60 AI Server - Shockingly Good!</title>
    <updated>2025-01-24T03:26:28+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xzhz003o1vee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8m6ls/llama_31_405b_8x_amd_instinct_mi60_ai_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8m6ls/llama_31_405b_8x_amd_instinct_mi60_ai_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T03:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8fg88</id>
    <title>I Built an Open-Source RAG API for Docs, GitHub Issues and READMEs</title>
    <updated>2025-01-23T22:05:41+00:00</updated>
    <author>
      <name>/u/eleven-five</name>
      <uri>https://old.reddit.com/user/eleven-five</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been working on &lt;strong&gt;Ragpi&lt;/strong&gt;, an open-source AI assistant that builds knowledge bases from docs, GitHub Issues, and READMEs. It uses Redis Stack as a vector DB and leverages RAG to answer technical questions through an API.&lt;/p&gt; &lt;p&gt;Some things it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Creates knowledge bases from documentation websites, GitHub Issues, and READMEs&lt;/li&gt; &lt;li&gt;Uses hybrid search (semantic + keyword) for retrieval&lt;/li&gt; &lt;li&gt;Uses tool calling to dynamically search and retrieve relevant information during conversations&lt;/li&gt; &lt;li&gt;Works with OpenAI or Ollama&lt;/li&gt; &lt;li&gt;Provides a simple REST API for querying and managing sources&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Built with:&lt;/strong&gt; FastAPI, Redis Stack, and Celery.&lt;/p&gt; &lt;p&gt;It’s still a work in progress, but I’d love some feedback!&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/ragpi/ragpi"&gt;https://github.com/ragpi/ragpi&lt;/a&gt;&lt;br /&gt; API Reference: &lt;a href="https://docs.ragpi.io/"&gt;https://docs.ragpi.io&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eleven-five"&gt; /u/eleven-five &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8fg88/i_built_an_opensource_rag_api_for_docs_github/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8fg88/i_built_an_opensource_rag_api_for_docs_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8fg88/i_built_an_opensource_rag_api_for_docs_github/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T22:05:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8r79t</id>
    <title>How to run models that are not on ollama website? Especially uncensored ones</title>
    <updated>2025-01-24T08:53:40+00:00</updated>
    <author>
      <name>/u/discoveringnature12</name>
      <uri>https://old.reddit.com/user/discoveringnature12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;trying to figure out how to run models not listed &lt;a href="https://ollama.com/library"&gt;https://ollama.com/library&lt;/a&gt;. Want to run uncensored models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/discoveringnature12"&gt; /u/discoveringnature12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8r79t/how_to_run_models_that_are_not_on_ollama_website/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8r79t/how_to_run_models_that_are_not_on_ollama_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8r79t/how_to_run_models_that_are_not_on_ollama_website/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T08:53:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8qckb</id>
    <title>List of top Open Source Chat UI for ollama/any LLM in general. (community edition)</title>
    <updated>2025-01-24T07:47:35+00:00</updated>
    <author>
      <name>/u/VisibleLawfulness246</name>
      <uri>https://old.reddit.com/user/VisibleLawfulness246</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey community, I am trying to compile a list of all the open-source ChatGPT UI. Here is the list from my research. Let's make this thread helpful. tell me- what do you use? and what are the pros and cons along with alternatives your tool of choice.&lt;/p&gt; &lt;p&gt;personally I'm a big fan of Open WebUI but I'm looking to try out what all is new in the community,&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open WebUI&lt;/li&gt; &lt;li&gt;LibreChat&lt;/li&gt; &lt;li&gt;anythingLLM&lt;/li&gt; &lt;li&gt;GPT4all&lt;/li&gt; &lt;li&gt;oobabooga&lt;/li&gt; &lt;li&gt;verba&lt;/li&gt; &lt;li&gt;dify&lt;/li&gt; &lt;li&gt;SillyTavern&lt;/li&gt; &lt;li&gt;Danswer&lt;/li&gt; &lt;li&gt;Lobe Ui&lt;/li&gt; &lt;li&gt;hugging face chat-Ui&lt;/li&gt; &lt;li&gt;kobold Cpp/ for from llama cpp&lt;/li&gt; &lt;li&gt;private gpt&lt;/li&gt; &lt;li&gt;serge chat&lt;/li&gt; &lt;li&gt;JanHQ&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What am I missing from this list? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VisibleLawfulness246"&gt; /u/VisibleLawfulness246 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8qckb/list_of_top_open_source_chat_ui_for_ollamaany_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8qckb/list_of_top_open_source_chat_ui_for_ollamaany_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8qckb/list_of_top_open_source_chat_ui_for_ollamaany_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T07:47:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8q0gk</id>
    <title>LLM website that lets you use any model and pay as you go. I can’t remember what it’s called.</title>
    <updated>2025-01-24T07:22:08+00:00</updated>
    <author>
      <name>/u/opelly</name>
      <uri>https://old.reddit.com/user/opelly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The title says it all. I came across a website a while ago that has a ChatGPT like interface, but lets you use all kinds of different models, and pay by the token. I specifically remember a leaderboard that ranks the models by popularity and even showed the number of tokens that each model had generated among all users on the website. I can’t find it for the life of me. Please let me know what site this is if it rings a bell. Thank you!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/opelly"&gt; /u/opelly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8q0gk/llm_website_that_lets_you_use_any_model_and_pay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8q0gk/llm_website_that_lets_you_use_any_model_and_pay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8q0gk/llm_website_that_lets_you_use_any_model_and_pay/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T07:22:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8bhf3</id>
    <title>Upgraded!</title>
    <updated>2025-01-23T19:19:46+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i8bhf3/upgraded/"&gt; &lt;img alt="Upgraded!" src="https://preview.redd.it/8qdjrpbxmsee1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e72aeebb6bad9946b60f373d5e38bdfa97f84a8" title="Upgraded!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8qdjrpbxmsee1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8bhf3/upgraded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8bhf3/upgraded/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T19:19:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8qav7</id>
    <title>A list of all the top Open Source Chat UI for ollama/any LLM in general. (community edition)</title>
    <updated>2025-01-24T07:43:58+00:00</updated>
    <author>
      <name>/u/VisibleLawfulness246</name>
      <uri>https://old.reddit.com/user/VisibleLawfulness246</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i8qav7/a_list_of_all_the_top_open_source_chat_ui_for/"&gt; &lt;img alt="A list of all the top Open Source Chat UI for ollama/any LLM in general. (community edition)" src="https://b.thumbs.redditmedia.com/2mlU8Ofw8DVHHHmMnu33kSfWqc1LdBv2Whegj1zwhkY.jpg" title="A list of all the top Open Source Chat UI for ollama/any LLM in general. (community edition)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's my list right now&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open Web UI&lt;/li&gt; &lt;li&gt;LibreChat&lt;/li&gt; &lt;li&gt;anythingLLM&lt;/li&gt; &lt;li&gt;GPT4all&lt;/li&gt; &lt;li&gt;oobabooga&lt;/li&gt; &lt;li&gt;verba&lt;/li&gt; &lt;li&gt;dify&lt;/li&gt; &lt;li&gt;SillyTavern&lt;/li&gt; &lt;li&gt;Danswer&lt;/li&gt; &lt;li&gt;Lobe Ui&lt;/li&gt; &lt;li&gt;hugging face chat-Ui&lt;/li&gt; &lt;li&gt;kobold Cpp/ for from llama cpp&lt;/li&gt; &lt;li&gt;private gpt&lt;/li&gt; &lt;li&gt;serge chat&lt;/li&gt; &lt;li&gt;JanHQ&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What am I missing from this list?&lt;/p&gt; &lt;p&gt;adding this image from my research&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VisibleLawfulness246"&gt; /u/VisibleLawfulness246 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8qav7/a_list_of_all_the_top_open_source_chat_ui_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8qav7/a_list_of_all_the_top_open_source_chat_ui_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8qav7/a_list_of_all_the_top_open_source_chat_ui_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T07:43:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8n38z</id>
    <title>Coming soon: 100% Local Video Understanding Engine (an open-source project that can classify, caption, transcribe, and understand any video on your local device)</title>
    <updated>2025-01-24T04:17:00+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i8n38z/coming_soon_100_local_video_understanding_engine/"&gt; &lt;img alt="Coming soon: 100% Local Video Understanding Engine (an open-source project that can classify, caption, transcribe, and understand any video on your local device)" src="https://external-preview.redd.it/MDIwNzQ0ZzNidmVlMe21Biif0sGFU8GTsH3N7D_CJugYvIxsEVZ-nvrUed0U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8fa43786541e784dc36e99b0ae3b5e02c8a90ea9" title="Coming soon: 100% Local Video Understanding Engine (an open-source project that can classify, caption, transcribe, and understand any video on your local device)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4sh274g3bvee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8n38z/coming_soon_100_local_video_understanding_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8n38z/coming_soon_100_local_video_understanding_engine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T04:17:00+00:00</published>
  </entry>
</feed>
