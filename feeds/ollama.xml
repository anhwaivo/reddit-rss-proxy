<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-04-17T19:48:50+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1k0aff3</id>
    <title>Persistent Local Memory for Your Models</title>
    <updated>2025-04-16T03:01:46+00:00</updated>
    <author>
      <name>/u/GVDub2</name>
      <uri>https://old.reddit.com/user/GVDub2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just updated my PanAI Seed Node project with a nice little sub-project that provides local memory with analysis and reflection for your local models, doing embedding to a Qdrant database and allowing semantic analysis , reflection and even a little dreaming. It’s all at &lt;a href="https://github.com/GVDub/panai-seed-node"&gt;https://github.com/GVDub/panai-seed-node&lt;/a&gt; as an optional part of the project (which I’m not able to work on as quickly as I’d like or I think it deserves). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GVDub2"&gt; /u/GVDub2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0aff3/persistent_local_memory_for_your_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0aff3/persistent_local_memory_for_your_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0aff3/persistent_local_memory_for_your_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T03:01:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0hyc9</id>
    <title>Any kind of digital assistant Android App with Ollama compatibility?</title>
    <updated>2025-04-16T11:21:09+00:00</updated>
    <author>
      <name>/u/Final_Wheel_7486</name>
      <uri>https://old.reddit.com/user/Final_Wheel_7486</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello to you all,&lt;/p&gt; &lt;p&gt;as you may or may not know, Android provides the capability for apps to register as &amp;quot;digital assistants&amp;quot;, allowing them to be pulled up by swiping from a corner or, sometimes, pressing and holding the power button. Gemini, for example, uses this API.&lt;/p&gt; &lt;p&gt;Is there any kind of open-source digital assistant app that's as accessible as well, but instead using Ollama or something locally/self-hosted?&lt;/p&gt; &lt;p&gt;It would take the usability and helpfulness of self hosted AI to a new level for me.&lt;/p&gt; &lt;p&gt;Greets!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Final_Wheel_7486"&gt; /u/Final_Wheel_7486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0hyc9/any_kind_of_digital_assistant_android_app_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0hyc9/any_kind_of_digital_assistant_android_app_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0hyc9/any_kind_of_digital_assistant_android_app_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T11:21:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0l14e</id>
    <title>Is there a model around the size of Gemma3:4B that is better than Gemma3:4B for questions such as "Give me a tip about vim"? I want to run it once a day in Conky for daily tips.</title>
    <updated>2025-04-16T13:55:47+00:00</updated>
    <author>
      <name>/u/___nutthead___</name>
      <uri>https://old.reddit.com/user/___nutthead___</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The small binary size + its generic nature makes me think it probably doesn't know much about vim, but I could be wrong.&lt;/p&gt; &lt;p&gt;Anyway, any alternatives that you think I should give a go, but not much larger than Gemma3:4B?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/___nutthead___"&gt; /u/___nutthead___ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0l14e/is_there_a_model_around_the_size_of_gemma34b_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0l14e/is_there_a_model_around_the_size_of_gemma34b_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0l14e/is_there_a_model_around_the_size_of_gemma34b_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T13:55:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k17oa0</id>
    <title>Neutral LLMs - Are Truly Objective Models Possible?</title>
    <updated>2025-04-17T08:17:02+00:00</updated>
    <author>
      <name>/u/Sascha1887</name>
      <uri>https://old.reddit.com/user/Sascha1887</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been diving deep into Ollama lately and it’s fantastic for experimenting with different LLMs locally. However, I'm&lt;/p&gt; &lt;p&gt;increasingly concerned about the inherent biases present in many of these models. It seems a lot are trained on&lt;/p&gt; &lt;p&gt;datasets rife with ideological viewpoints, leading to responses that feel… well, “woke.”&lt;/p&gt; &lt;p&gt;I'm wondering if anyone else has had a similar experience, or if anyone’s managed to find Ollama models (or models&lt;/p&gt; &lt;p&gt;easily integrated with Ollama) that prioritize factual accuracy and logical reasoning *above* all else.&lt;/p&gt; &lt;p&gt;Essentially, are there any models that genuinely strive for neutrality and avoid injecting subjective opinions or&lt;/p&gt; &lt;p&gt;perspectives into their answers?&lt;/p&gt; &lt;p&gt;I'm looking for models that would reliably stick to verifiable facts and sound reasoning, regardless of the&lt;/p&gt; &lt;p&gt;prompt. I’m specifically interested in seeing if there are any that haven’t been explicitly fine-tuned for&lt;/p&gt; &lt;p&gt;engaging in conversations about social justice or political issues.&lt;/p&gt; &lt;p&gt;I've tried some of the more popular models, and while they're impressive, they often lean into a certain&lt;/p&gt; &lt;p&gt;narrative.&lt;/p&gt; &lt;p&gt;Anyone working with Ollama find any models that lean towards pure logic and data? Any recommendations or&lt;/p&gt; &lt;p&gt;approaches for training a model on a truly neutral dataset?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sascha1887"&gt; /u/Sascha1887 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k17oa0/neutral_llms_are_truly_objective_models_possible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k17oa0/neutral_llms_are_truly_objective_models_possible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k17oa0/neutral_llms_are_truly_objective_models_possible/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T08:17:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0slhs</id>
    <title>check local/cloud orchestration -- fully open source</title>
    <updated>2025-04-16T19:06:52+00:00</updated>
    <author>
      <name>/u/Emotional-Evening-62</name>
      <uri>https://old.reddit.com/user/Emotional-Evening-62</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a video that orchestrates between local/cloud models; Fully open source, would love to hear from community:&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/j0dOVWWzBrE?si=lHISeYU992irM-7p"&gt;https://youtu.be/j0dOVWWzBrE?si=lHISeYU992irM-7p&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emotional-Evening-62"&gt; /u/Emotional-Evening-62 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0slhs/check_localcloud_orchestration_fully_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0slhs/check_localcloud_orchestration_fully_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0slhs/check_localcloud_orchestration_fully_open_source/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T19:06:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0iwng</id>
    <title>LLM's too supportive?</title>
    <updated>2025-04-16T12:14:02+00:00</updated>
    <author>
      <name>/u/Hedwig2222</name>
      <uri>https://old.reddit.com/user/Hedwig2222</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;hopefully what I'm asking makes sense and wasn't too sure on how to title this. But for example with ChatGPT and other big ones like Gemini. I've noticed that pretty much everything you try to talk about with it, it usually always is very supportive of you. Regardless of the topic. They seem very overly supportive most of the time. Where as a regular person would be more realistic as in they would be more neutral and realistic about the situation/topic you're discussing. For example ChatGPT is often overly supportive and optimistic I think. Like if you were to talk about a bad job interview, and maybe you havent heard back from them when you expected to, ChatGPT would still be very supportive and overly optimistic that you still have a chance etc. Where as a real person, a close friend or family member could be like &amp;quot;Yeah...sorry bud, looks like you effed up that interview, better start applying to more jobs&amp;quot;&lt;/p&gt; &lt;p&gt;Am I making sense? It seems the big LLMs' like ChatGPT and Google gemini are programmed in this way to be ultra supportive and optimistic for you rather than realistic. Which I find annoying because I sometimes feel that I'm not getting a truthful answer on the topic or situation shared. I've found even the uncensored ones can be like this also.&lt;/p&gt; &lt;p&gt;Is this just a limitation of todays LLM's? They will either be overly supportive and optimistic for you regardless of the facts, alternatively if not programmed like this they would be the opposite and just not useful at all lol. Or are there actually decent Models out there that are more realistic on a personal level when discussing topics and situations with them where they won't always be supportive and optimistic just because, but they will be more realistic as in, agreeing you're a bit screwed in said situation such as the above bad interview example and not being overly optimistic you still have a chance etc and instead be more like.. yeah, you screwed up, better start looking for new jobs lol. I assume it would be an uncensored model? But which one do you guys find is the best for a more realistic conversation on life and things?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hedwig2222"&gt; /u/Hedwig2222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0iwng/llms_too_supportive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0iwng/llms_too_supportive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0iwng/llms_too_supportive/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T12:14:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0fn6y</id>
    <title>How do you finetune a model?</title>
    <updated>2025-04-16T08:44:57+00:00</updated>
    <author>
      <name>/u/ChikyScaresYou</name>
      <uri>https://old.reddit.com/user/ChikyScaresYou</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm still pretty new to this topic, but I've seen that some of fhe LLMs i'm running are fine tunned to specifix topics. There are, however, other topics where I havent found anything fine tunned to it. So, how do people fine tune LLMs? Does it rewuire too much processing power? Is it even worth it? &lt;/p&gt; &lt;p&gt;And how do you make an LLM &amp;quot;learn&amp;quot; a large text like a novel?&lt;/p&gt; &lt;p&gt;I'm asking becausey current method uses very small chunks in a chromadb database, but it seems that the &amp;quot;material&amp;quot; the LLM retrieves is minuscule in comparison to the entire novel. I thought the LLM would have access to the entire novel now that it's in a database, but it doesnt seem to be the case. Also, still unsure how RAG works, as it seems that it's basicallt creating a database of the documents as well, which turns out to have the same issue....&lt;/p&gt; &lt;p&gt;o, I was thinking, could I finetune an LLM to know everything that happens in the novel and be able to answer any question about it, regardless of how detailed? And, in addition, I'd like to make an LLM fine tuned with military and police knowledge in attack and defense for factchecking. I'd like to know how to do that, or if that's the wrong approach, if you could point me in the right direction and share resources, i'd appreciate it, thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChikyScaresYou"&gt; /u/ChikyScaresYou &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0fn6y/how_do_you_finetune_a_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0fn6y/how_do_you_finetune_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0fn6y/how_do_you_finetune_a_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T08:44:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0rj64</id>
    <title>Morphik just hit 1k stars - Thank you!</title>
    <updated>2025-04-16T18:23:26+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;I'm grateful and happy to announce that our repository, &lt;a href="https://github.com/morphik-org/morphik-core"&gt;Morphik&lt;/a&gt;, just hit 1k stars! This really wouldn't have been possible without the support of the &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; community, and I'm just writing this post to say thanks :) &lt;/p&gt; &lt;p&gt;As another thank you, we want to help solve your most difficult, annoying, expensive, or time consuming problems with documents and multimodal data. Reply to this post with your most pressing issues - eg. &amp;quot;I have x PDFs and I'm trying to get structured information out of them&amp;quot;, or &amp;quot;I have a 1000 files of game footage, and I want to cut highlights featuring player y&amp;quot;, etc. We'll have a feature or implementation that fixes that up within a week :)&lt;/p&gt; &lt;p&gt;Thanks again! &lt;/p&gt; &lt;p&gt;Sending love from SF &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0rj64/morphik_just_hit_1k_stars_thank_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0rj64/morphik_just_hit_1k_stars_thank_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0rj64/morphik_just_hit_1k_stars_thank_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T18:23:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1k180f7</id>
    <title>Looking LLM that is uncensored and unbias</title>
    <updated>2025-04-17T08:42:36+00:00</updated>
    <author>
      <name>/u/Beautiful_Emu8026</name>
      <uri>https://old.reddit.com/user/Beautiful_Emu8026</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried Dolphin. I literally want to find a model for example if I wanted it to cuss me down with swear words it will do it. It's even censored while offline is there any cracked models by any chance?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beautiful_Emu8026"&gt; /u/Beautiful_Emu8026 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k180f7/looking_llm_that_is_uncensored_and_unbias/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k180f7/looking_llm_that_is_uncensored_and_unbias/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k180f7/looking_llm_that_is_uncensored_and_unbias/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T08:42:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1k18xk0</id>
    <title>What am I doing wrong? tried downloading .DEB for ollama web ui and this happened</title>
    <updated>2025-04-17T09:51:30+00:00</updated>
    <author>
      <name>/u/ThrowRAmyuser</name>
      <uri>https://old.reddit.com/user/ThrowRAmyuser</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k18xk0/what_am_i_doing_wrong_tried_downloading_deb_for/"&gt; &lt;img alt="What am I doing wrong? tried downloading .DEB for ollama web ui and this happened" src="https://preview.redd.it/f4rlyysfadve1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d92fffa18c3c7fa6f9b6b629158a34c9d5c43941" title="What am I doing wrong? tried downloading .DEB for ollama web ui and this happened" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThrowRAmyuser"&gt; /u/ThrowRAmyuser &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f4rlyysfadve1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k18xk0/what_am_i_doing_wrong_tried_downloading_deb_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k18xk0/what_am_i_doing_wrong_tried_downloading_deb_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T09:51:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0xy2g</id>
    <title>4xMi300a Server + QwQ-32B-Q8</title>
    <updated>2025-04-16T22:55:18+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rvs71fcai6ve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0xy2g/4xmi300a_server_qwq32bq8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0xy2g/4xmi300a_server_qwq32bq8/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T22:55:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0z2gu</id>
    <title>6x vLLM | 6x 32B Models | 2 Node 16x GPU Cluster | Sustains 140+ Tokens/s = 5X Increase!</title>
    <updated>2025-04-16T23:48:33+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cwxw17crw9ve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0z2gu/6x_vllm_6x_32b_models_2_node_16x_gpu_cluster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0z2gu/6x_vllm_6x_32b_models_2_node_16x_gpu_cluster/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T23:48:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1k18loo</id>
    <title>Exploring the Architecture of Large Language Models</title>
    <updated>2025-04-17T09:27:00+00:00</updated>
    <author>
      <name>/u/Veerans</name>
      <uri>https://old.reddit.com/user/Veerans</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k18loo/exploring_the_architecture_of_large_language/"&gt; &lt;img alt="Exploring the Architecture of Large Language Models" src="https://external-preview.redd.it/eWCdGn_OOImPwXpBY0yyeWbAruAzVKGbD3JyTdQFD0M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a5d2a52bf78f8677c1bb5d510a19fb4ce17172b2" title="Exploring the Architecture of Large Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Veerans"&gt; /u/Veerans &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://bigdataanalyticsnews.com/exploring-architecture-of-large-language-models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k18loo/exploring_the_architecture_of_large_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k18loo/exploring_the_architecture_of_large_language/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T09:27:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0wgyv</id>
    <title>Made this text replacement tool using Ollama and shell scripting [LINUX ONLY]</title>
    <updated>2025-04-16T21:49:05+00:00</updated>
    <author>
      <name>/u/Sweaty_Advance1172</name>
      <uri>https://old.reddit.com/user/Sweaty_Advance1172</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k0wgyv/made_this_text_replacement_tool_using_ollama_and/"&gt; &lt;img alt="Made this text replacement tool using Ollama and shell scripting [LINUX ONLY]" src="https://external-preview.redd.it/NTRieHZrMGlwOXZlMVqD_vMUP0hgQsuXG-MfaXkaE4rdI5NsUCZLb2_L6nm-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b18307393ff2f81f2ebb017da903955d50245dbb" title="Made this text replacement tool using Ollama and shell scripting [LINUX ONLY]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last week I installed Grammarly on my laptop, and they had this one feature where you could select the entire text, and then it will rewrite the whole thing with improved grammar, but only 3 such replacements were possible every day.&lt;/p&gt; &lt;p&gt;This got me wondering, can I do it using LLMs and some shell scripting, and so Betterwrite was born.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sweaty_Advance1172"&gt; /u/Sweaty_Advance1172 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2n23rb0ip9ve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0wgyv/made_this_text_replacement_tool_using_ollama_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0wgyv/made_this_text_replacement_tool_using_ollama_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T21:49:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0ndas</id>
    <title>OSS SDK to automate your Windows computer in JS or Python. 100x faster and cheaper than OpenAI Operator or Anthropic Computer Use</title>
    <updated>2025-04-16T15:34:54+00:00</updated>
    <author>
      <name>/u/louis3195</name>
      <uri>https://old.reddit.com/user/louis3195</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k0ndas/oss_sdk_to_automate_your_windows_computer_in_js/"&gt; &lt;img alt="OSS SDK to automate your Windows computer in JS or Python. 100x faster and cheaper than OpenAI Operator or Anthropic Computer Use" src="https://external-preview.redd.it/cm1jaWR2d2J1N3ZlMeJLErIi_ot-KBDYl6VMuMkZ36iBX_i-T6LEtxagz-fp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08f39be6dc1ae2191a10e26bf1e08175886e9bfd" title="OSS SDK to automate your Windows computer in JS or Python. 100x faster and cheaper than OpenAI Operator or Anthropic Computer Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;yo all, i've been working on an OSS SDK that uses OS-level APIs to provide a Playwright-like easy DX to control your computer in python, TS, or anything else, &lt;/p&gt; &lt;p&gt;making it 100x faster than vision approach used by OpenAI and Anthropic while being model agnostic, compatible with ollama/OSS model or even gemini etc.&lt;/p&gt; &lt;p&gt;would love your thoughts, feedback, or any tinkering with ollama 🙏 &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mediar-ai/terminator"&gt;https://github.com/mediar-ai/terminator&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/louis3195"&gt; /u/louis3195 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8x51dtwbu7ve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0ndas/oss_sdk_to_automate_your_windows_computer_in_js/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0ndas/oss_sdk_to_automate_your_windows_computer_in_js/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T15:34:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1k136t3</id>
    <title>Run Ollama Language Models in Chrome – Quick 2-Minute Setup</title>
    <updated>2025-04-17T03:21:40+00:00</updated>
    <author>
      <name>/u/AdOdd4004</name>
      <uri>https://old.reddit.com/user/AdOdd4004</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k136t3/run_ollama_language_models_in_chrome_quick/"&gt; &lt;img alt="Run Ollama Language Models in Chrome – Quick 2-Minute Setup" src="https://external-preview.redd.it/CvgaoZ16vSWsjmHKDBUmAYXRYRtxpchfqPBKqOs_oBc.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=945a3fa00d0a598497191286a0a563e8b5ede630" title="Run Ollama Language Models in Chrome – Quick 2-Minute Setup" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just came across this Chrome extension that lets you run local LLMs (like Ollama models) &lt;strong&gt;directly inside Chrome&lt;/strong&gt; — plus it supports APIs like Gemini and OpenRouter too.&lt;/p&gt; &lt;p&gt;Super lightweight and took me under 2 mins to set up. I liked it enough to throw together a quick video demo if anyone’s curious:&lt;/p&gt; &lt;p&gt;📹 &lt;a href="https://youtu.be/vejRMXLk6V0"&gt;https://youtu.be/vejRMXLk6V0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Might be useful if you just want to mess around with LLMs without leaving Chrome. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Bonus:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It can also allow you to chat with your web pages and uploaded documents.&lt;/li&gt; &lt;li&gt;It also allows you to add web search without the need for API keys!&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdOdd4004"&gt; /u/AdOdd4004 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/vejRMXLk6V0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k136t3/run_ollama_language_models_in_chrome_quick/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k136t3/run_ollama_language_models_in_chrome_quick/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T03:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0g8fp</id>
    <title>No API keys, no cloud. Just local AI + tools that actually work. Too much to ask?</title>
    <updated>2025-04-16T09:28:15+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k0g8fp/no_api_keys_no_cloud_just_local_ai_tools_that/"&gt; &lt;img alt="No API keys, no cloud. Just local AI + tools that actually work. Too much to ask?" src="https://external-preview.redd.it/Q3SKDCrADGVEsTRTkQ-Dz8wknf8WPBvAR2OOxzRDVdY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5677a7a9cee67696561f40dacb232612cdd9b8cd" title="No API keys, no cloud. Just local AI + tools that actually work. Too much to ask?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It’s been about a month since I first posted Clara here.&lt;/p&gt; &lt;p&gt;Clara is a local-first AI assistant — think of it like ChatGPT, but fully private and running on your own machine using Ollama.&lt;/p&gt; &lt;p&gt;Since the initial release, I’ve had a small group of users try it out, and I’ve pushed several updates based on real usage and feedback.&lt;/p&gt; &lt;p&gt;The biggest update is that &lt;strong&gt;Clara now comes with n8n built-in&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;That means you can now build and run your own tools directly inside the assistant — no setup needed, no external services. Just open Clara and start automating.&lt;/p&gt; &lt;p&gt;With the n8n integration, Clara can now do more than chat. You can use it to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Check your emails&lt;/li&gt; &lt;li&gt;Manage your calendar&lt;/li&gt; &lt;li&gt;Call APIs&lt;/li&gt; &lt;li&gt;Run scheduled tasks&lt;/li&gt; &lt;li&gt;Process webhooks&lt;/li&gt; &lt;li&gt;Connect to databases&lt;/li&gt; &lt;li&gt;And anything else you can wire up using n8n’s visual flow builder&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The assistant can trigger these workflows directly — so you can talk to Clara and ask it to do real tasks, using tools that run entirely on your device.&lt;/p&gt; &lt;p&gt;Everything happens locally. No data goes out, no accounts, no cloud dependency.&lt;/p&gt; &lt;p&gt;If you're someone who wants full control of your AI and automation setup, this might be something worth trying.&lt;/p&gt; &lt;p&gt;You can check out the project here:&lt;br /&gt; GitHub: &lt;a href="https://github.com/badboysm890/ClaraVerse"&gt;https://github.com/badboysm890/ClaraVerse&lt;/a&gt;&lt;br /&gt; Web version (Ollama required): &lt;a href="https://clara.badboysm890.in"&gt;https://clara.badboysm890.in&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks to everyone who's been trying it and sending feedback. Still improving things — more updates soon.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; I'm aware of great projects like OpenWebUI and LibreChat. Clara takes a slightly different approach — focusing on reducing dependencies, offering a native desktop app, and making the overall experience more user-friendly so that more people can easily get started with local AI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/badboysm890/ClaraVerse"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0g8fp/no_api_keys_no_cloud_just_local_ai_tools_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0g8fp/no_api_keys_no_cloud_just_local_ai_tools_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T09:28:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k16pd2</id>
    <title>Ollama reloads model at every prompt. Why and how to fix?</title>
    <updated>2025-04-17T07:05:25+00:00</updated>
    <author>
      <name>/u/lillemets</name>
      <uri>https://old.reddit.com/user/lillemets</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k16pd2/ollama_reloads_model_at_every_prompt_why_and_how/"&gt; &lt;img alt="Ollama reloads model at every prompt. Why and how to fix?" src="https://preview.redd.it/mrfncbmlgcve1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65951996d3e999ee4dd878eeb0da16bf863c65e3" title="Ollama reloads model at every prompt. Why and how to fix?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lillemets"&gt; /u/lillemets &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mrfncbmlgcve1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k16pd2/ollama_reloads_model_at_every_prompt_why_and_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k16pd2/ollama_reloads_model_at_every_prompt_why_and_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T07:05:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1dalp</id>
    <title>Running Large Concept Models</title>
    <updated>2025-04-17T13:50:59+00:00</updated>
    <author>
      <name>/u/tshawkins</name>
      <uri>https://old.reddit.com/user/tshawkins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anybody know if there is a tool like ollama for running LCMs (large concept models)&lt;/p&gt; &lt;p&gt;These differer from LLMs because they are models built with concepts extracted from texts. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tshawkins"&gt; /u/tshawkins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1dalp/running_large_concept_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1dalp/running_large_concept_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k1dalp/running_large_concept_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T13:50:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1kl3k</id>
    <title>Blue screen error when using Ollama</title>
    <updated>2025-04-17T18:52:50+00:00</updated>
    <author>
      <name>/u/Ms_Ivyyblack</name>
      <uri>https://old.reddit.com/user/Ms_Ivyyblack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k1kl3k/blue_screen_error_when_using_ollama/"&gt; &lt;img alt="Blue screen error when using Ollama" src="https://b.thumbs.redditmedia.com/DXYPpNdEr8G_VPlZXpsFF59VQTtY0bo9a7U84Rrtxkw.jpg" title="Blue screen error when using Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;my pc is fairly new, upgraded to 4070 super, and ram is 32, I don't run large models, max is 21b (works great before), but I use 12b mostly and using sillytavern to connect api, I've used Ollama months before it never gave me the error so I'm not sure if the issue from the app or pc itself, everything is up-to-date so far.&lt;/p&gt; &lt;p&gt;everytime i use ollama it gives me blue screen with same settings I used before. I tried koboldcpp and heavy stress test on my pc, everything works fine under pressure. i use brave browser, if that helps.&lt;/p&gt; &lt;p&gt;any support will be appreciated&lt;/p&gt; &lt;p&gt;this example of the error (I took image from google) :&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mgvolwezxfve1.png?width=498&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5943e3c75ef1eaf9fa43510cc93c8d705b5a2855"&gt;https://preview.redd.it/mgvolwezxfve1.png?width=498&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5943e3c75ef1eaf9fa43510cc93c8d705b5a2855&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ms_Ivyyblack"&gt; /u/Ms_Ivyyblack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1kl3k/blue_screen_error_when_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1kl3k/blue_screen_error_when_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k1kl3k/blue_screen_error_when_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T18:52:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1f0j0</id>
    <title>Mac mini M4(10‑core CPU, 10‑core GPU, 32 GB unified RAM, 256 GB SSD) vs. Mac Studio M4 Max (16‑core CPU, 40‑core GPU, 64 GB unified RAM, 512 GB SSD) – is the extra $1.7 k worth it?</title>
    <updated>2025-04-17T15:03:53+00:00</updated>
    <author>
      <name>/u/msahil515</name>
      <uri>https://old.reddit.com/user/msahil515</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m torn between keeping my &lt;strong&gt;Mac mini M4&lt;/strong&gt; (10‑core CPU, 10‑core GPU, 32 GB unified RAM, 256 GB SSD) or stepping up to a &lt;strong&gt;Mac Studio M4 Max&lt;/strong&gt; (16‑core CPU, 40‑core GPU, 64 GB unified RAM, 512 GB SSD). The Studio is about $1,700 more up front, and if I stick with the mini I’d still need to shell out roughly $300 for a Thunderbolt SSD upgrade, so the true delta is about $1,300 to $1,400.&lt;/p&gt; &lt;p&gt;I plan to run some medium‑sized &lt;strong&gt;Ollama&lt;/strong&gt; models locally, and on paper the extra RAM and GPU cores in the Studio could help. But if most of my heavy lifting lives on API calls and I only fire up local models occasionally, the mini and SSD might serve just fine until the next chip generation.&lt;/p&gt; &lt;p&gt;I’d love to hear your thoughts on which option makes more sense.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/msahil515"&gt; /u/msahil515 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1f0j0/mac_mini_m410core_cpu_10core_gpu_32_gb_unified/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1f0j0/mac_mini_m410core_cpu_10core_gpu_32_gb_unified/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k1f0j0/mac_mini_m410core_cpu_10core_gpu_32_gb_unified/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T15:03:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1k1l3</id>
    <title>Using Ollama and MCP</title>
    <updated>2025-04-17T18:30:07+00:00</updated>
    <author>
      <name>/u/myronsnila</name>
      <uri>https://old.reddit.com/user/myronsnila</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone had success using an Ollama model such as Llama 3.1 to call mcp servers? I’m using the 5ire app in Windows and I can’t get it to call the mcp server such as the time system mcp server. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/myronsnila"&gt; /u/myronsnila &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1k1l3/using_ollama_and_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1k1l3/using_ollama_and_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k1k1l3/using_ollama_and_mcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T18:30:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1k15c1h</id>
    <title>Siliv - MacOS Silicon VRAM App but free</title>
    <updated>2025-04-17T05:31:01+00:00</updated>
    <author>
      <name>/u/_Sub01_</name>
      <uri>https://old.reddit.com/user/_Sub01_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k15c1h/siliv_macos_silicon_vram_app_but_free/"&gt; &lt;img alt="Siliv - MacOS Silicon VRAM App but free" src="https://external-preview.redd.it/4MHiqj_yO9p5oQ-MlPydYGxTPuwjc5W-_vK2lUPKUVw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b4eb3dab8435639fdd3a636ebc28c381a095c48" title="Siliv - MacOS Silicon VRAM App but free" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw a specific post 8-9 hrs ago about a paid vram app which could be set in a simple few commands. However, I've decided to speed code one to make it open sourced! 😉&lt;/p&gt; &lt;p&gt;Here's the repo so go check it out!&lt;br /&gt; &lt;a href="https://github.com/PaulShiLi/Siliv"&gt;https://github.com/PaulShiLi/Siliv&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/si51zwqxzbve1.png?width=254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f2d7a4d209c3728b82376654a6f5cdf19914cb32"&gt;https://preview.redd.it/si51zwqxzbve1.png?width=254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f2d7a4d209c3728b82376654a6f5cdf19914cb32&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Created a &lt;a href="https://www.reddit.com/r/macapps/comments/1k1ldnc/siliv_tweak_your_apple_silicon_vram_allocation"&gt;reddit post&lt;/a&gt; on &lt;a href="/r/macapps"&gt;r/macapps&lt;/a&gt; so people can find this app more easily in the future!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_Sub01_"&gt; /u/_Sub01_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k15c1h/siliv_macos_silicon_vram_app_but_free/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k15c1h/siliv_macos_silicon_vram_app_but_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k15c1h/siliv_macos_silicon_vram_app_but_free/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T05:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1inmf</id>
    <title>RENTAHAL: An open source Web GUI for Ollama with AI Worker Node Orchestration</title>
    <updated>2025-04-17T17:33:36+00:00</updated>
    <author>
      <name>/u/CHEVISION</name>
      <uri>https://old.reddit.com/user/CHEVISION</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/jimpames/rentahal"&gt;https://github.com/jimpames/rentahal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I welcome you to explore RENTAHAL - a new paradigm in AI Orchestration.&lt;/p&gt; &lt;p&gt;It's simple to run and simple to use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CHEVISION"&gt; /u/CHEVISION &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1inmf/rentahal_an_open_source_web_gui_for_ollama_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1inmf/rentahal_an_open_source_web_gui_for_ollama_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k1inmf/rentahal_an_open_source_web_gui_for_ollama_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T17:33:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1attv</id>
    <title>I tested all four Gemma 3 models on Ollama - Here's what I learned about their capabilities</title>
    <updated>2025-04-17T11:50:36+00:00</updated>
    <author>
      <name>/u/AnomanderRake_</name>
      <uri>https://old.reddit.com/user/AnomanderRake_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been playing with Google's new Gemma 3 models on Ollama and wanted to share some interesting findings for anyone considering which version to use. I tested the 1B, 4B, 12B, and 27B parameter models across logic puzzles, image recognition, and code generation tasks [&lt;a href="https://github.com/zazencodes/zazencodes-season-2/tree/main/src/gemma3-ollama"&gt;Source Code&lt;/a&gt;]&lt;/p&gt; &lt;p&gt;Here's some of my takeaways:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Models struggle with silly things&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Simple tricks like negation and spatial reasoning trip up even the 27B model sometimes&lt;/li&gt; &lt;li&gt;Smaller Gemma 3 models have a really hard time counting things (the 4B model went into an infinite loop while trying to count how many L's are in LOLLAPALOOZA)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Visual recognition varied significantly&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The 1B model is text-only (no image capabilities) but it will hallucinate as if it can read images when prompting with Ollama&lt;/li&gt; &lt;li&gt;All multimodal models struggled to understand historical images, e.g. Mayan glyphs and Japanese playing cards&lt;/li&gt; &lt;li&gt;The 27B model correctly identified Mexico City's Roma Norte neighborhood while smaller models couldn't&lt;/li&gt; &lt;li&gt;Visual humor recognition was nearly non-existent across all models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Code generation scaled with model size&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1B ran like a breeze and produced runnable code (although very rough)&lt;/li&gt; &lt;li&gt;The 4B models put a lot more stress on my system but ran pretty fast&lt;/li&gt; &lt;li&gt;The 12B model created the most visually appealing design but it runs too slow for real-world use&lt;/li&gt; &lt;li&gt;Only the 27B model worked properly with Cline (automatically created the file) however was painfully slow&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you're curious about memory usage, I was able to run all models in parallel and stay within a 48GB limit, with the model sizes ranging from 800MB (1B) to 17GB (27B).&lt;/p&gt; &lt;p&gt;For those interested in seeing the full tests in action, I made a &lt;a href="https://youtu.be/RiaCdQszjgA"&gt;detailed video breakdown&lt;/a&gt; of the comparisons I described above:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=RiaCdQszjgA"&gt;https://www.youtube.com/watch?v=RiaCdQszjgA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What has your experience been with Gemma 3 models? I'm particularly interested in what people think of the 4B model—as it seems to be a sweet spot right now in terms of size and performance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnomanderRake_"&gt; /u/AnomanderRake_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1attv/i_tested_all_four_gemma_3_models_on_ollama_heres/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1attv/i_tested_all_four_gemma_3_models_on_ollama_heres/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k1attv/i_tested_all_four_gemma_3_models_on_ollama_heres/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T11:50:36+00:00</published>
  </entry>
</feed>
