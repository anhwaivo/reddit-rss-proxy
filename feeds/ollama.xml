<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-10T01:57:40+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mkg49d</id>
    <title>Just released v1 of my open-source CLI app for coding locally: Nanocoder</title>
    <updated>2025-08-07T23:55:07+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mkg49d/just_released_v1_of_my_opensource_cli_app_for/"&gt; &lt;img alt="Just released v1 of my open-source CLI app for coding locally: Nanocoder" src="https://external-preview.redd.it/Drrvz-4kHMvi4lMmu6VO9fxf9_IMIOSmZHpoZGn5meI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb9f517488bd723213b3f631a62e468793339cc9" title="Just released v1 of my open-source CLI app for coding locally: Nanocoder" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I just released version 1.0 of Nanocoder, a CLI tool I’ve been building to make it easier to code agentically with large language models locally with Ollama and OpenRouter in your terminal to offer a similar experience to Claude Code and Gemini CLI. For me terminal experiences feel cleaner and more flexible so that’s where I was going with this.&lt;/p&gt; &lt;p&gt;Right now it’s very much MVP stage - works well enough to be useful, but rough around the edges. I want to polish it, add more features (better context handling, more tools, improved UX), and make it truly awesome for coding work.&lt;/p&gt; &lt;p&gt;I’m a big believer in AI being open and for the people, not locked behind subscriptions or proprietary APIs. That’s why it’s open source, I’m hoping to build it as a community.&lt;/p&gt; &lt;p&gt;If you think this is cool, I’d be grateful for GitHub stars and contributors to help shape where it goes next. Feedback, feature ideas, bug reports - all welcome!&lt;/p&gt; &lt;p&gt;👉 &lt;a href="https://github.com/Mote-Software/nanocoder"&gt;https://github.com/Mote-Software/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Mote-Software/nanocoder"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkg49d/just_released_v1_of_my_opensource_cli_app_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mkg49d/just_released_v1_of_my_opensource_cli_app_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T23:55:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mktlqb</id>
    <title>Model for text to sql</title>
    <updated>2025-08-08T12:16:26+00:00</updated>
    <author>
      <name>/u/ZitounaT</name>
      <uri>https://old.reddit.com/user/ZitounaT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, i need a model that can take prompt + schema and generate a sql query.&lt;br /&gt; It's for a web app (personnel project) that match clients to experts (IT, Finance, Doctors...), and i want to integrate a chatbot that will understand the client's problem, generate query to fetch the best experts to solve the problem. &lt;/p&gt; &lt;p&gt;I already tried the ollama 7B, it worked fine but with limitations, if the user's prompt is clear and simple and straight to the point, the model will generate a correct query most of the time, otherwise the model will get so confused.&lt;br /&gt; is there any way to have better results with it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZitounaT"&gt; /u/ZitounaT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mktlqb/model_for_text_to_sql/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mktlqb/model_for_text_to_sql/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mktlqb/model_for_text_to_sql/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T12:16:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkzoeg</id>
    <title>Updates on a project I am passionate about- Darnahi</title>
    <updated>2025-08-08T16:21:07+00:00</updated>
    <author>
      <name>/u/TestPilot1980</name>
      <uri>https://old.reddit.com/user/TestPilot1980</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TestPilot1980"&gt; /u/TestPilot1980 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/selfhosted/comments/1mktc79/updates_on_a_project_i_am_passionate_about_darnahi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkzoeg/updates_on_a_project_i_am_passionate_about_darnahi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mkzoeg/updates_on_a_project_i_am_passionate_about_darnahi/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T16:21:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mklsm5</id>
    <title>GPT-5 style router, but for local models</title>
    <updated>2025-08-08T04:31:18+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mklsm5/gpt5_style_router_but_for_local_models/"&gt; &lt;img alt="GPT-5 style router, but for local models" src="https://preview.redd.it/k9oxbvcd3qhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d15425761a134887e8b496ee976b02a5a1b4acfd" title="GPT-5 style router, but for local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPT-5 launched today, which is essentially different model underneath the covers abstracted away by a real-time router. In June, we published our &lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;preference-aligned routing model&lt;/a&gt; and &lt;a href="https://github.com/katanemo/archgw"&gt;framework&lt;/a&gt; for developers so that they can build a best-of-breed agentic experience with choice of models they care about. &lt;/p&gt; &lt;p&gt;Sharing the research and framework again, as it might be helpful to developers looking for similar tools.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k9oxbvcd3qhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mklsm5/gpt5_style_router_but_for_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mklsm5/gpt5_style_router_but_for_local_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T04:31:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml0c71</id>
    <title>Ollama remembers previous prompts when it shouldn't?</title>
    <updated>2025-08-08T16:46:17+00:00</updated>
    <author>
      <name>/u/Nebosklon</name>
      <uri>https://old.reddit.com/user/Nebosklon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm completely new to this. I've downloaded llama-3.1 and am running a series of queries to it using the ollama python library. As I understand it, the default behaviour should be that the queries run completely independently, so when processing a new prompt ollama has no memory of the previous one whatsoever. That is exactly the desired behaviour for my research problem, but!&lt;/p&gt; &lt;p&gt;The &amp;quot;system&amp;quot; parts of my prompts are all identical, the &amp;quot;user&amp;quot; parts are all different, but they come in pairs, where I have two very similar versions of basically the same prompt. Now what I see is that ollama takes a lot less time to process the second version of the same prompt than it does the first. Like an order of magnitude less time. Why is that happening if it has no memory of the previous prompt? Or does it?&lt;/p&gt; &lt;p&gt;I've found this old post and was wondering if this has something to do with the buffer of the generator function (probably not): &lt;a href="https://www.reddit.com/r/ollama/s/GwmhTB8d0h"&gt;https://www.reddit.com/r/ollama/s/GwmhTB8d0h&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyway, I don't trust it and I would like to ensure that my prompts are processed truly independently. (Or even better, if this is possible, make it remember the &amp;quot;system&amp;quot; part but forget the &amp;quot;user&amp;quot; part.) Do you have any ideas of why this is happening and what I could do? Sorry if this is a stupid question. &lt;/p&gt; &lt;p&gt;UPDATE:&lt;/p&gt; &lt;p&gt;So, I've experimented a bit more with this, and here are my results.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;I've added a Hi! prompt at the beginning. It didn't change the pattern for the rest of the prompts. So at least, it doesn't have to do with the time loading the model at the beginning of the loop.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I've changed the order of the prompts. At first, I had them in this order:&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;text 1 version A 10 sec&lt;/p&gt; &lt;p&gt;text 1 version B 1.5 sec&lt;/p&gt; &lt;p&gt;text 2 version A 10 sec&lt;/p&gt; &lt;p&gt;text 2 version B 1.5 sec&lt;/p&gt; &lt;p&gt;text 3 version A 15 sec&lt;/p&gt; &lt;p&gt;text 3 version B 2 sec&lt;/p&gt; &lt;p&gt;And processing version B was always much faster than processing version A. By the way, the difference was entirely due to prompt evaluation time. Response generation time was not affected.&lt;/p&gt; &lt;p&gt;Then I put them in this order:&lt;/p&gt; &lt;p&gt;text 1 version A 10 sec&lt;/p&gt; &lt;p&gt;text 2 version A 10 sec&lt;/p&gt; &lt;p&gt;text 3 version A 15 sec&lt;/p&gt; &lt;p&gt;text 1 version B 10 sec&lt;/p&gt; &lt;p&gt;text 2 version B 10 sec&lt;/p&gt; &lt;p&gt;text 3 version B 15 sec&lt;/p&gt; &lt;p&gt;And the facilitation effect completely disappeared. Now both versions of the same prompt took about the same time to be processed. &lt;/p&gt; &lt;p&gt;So this facilitation effect must have to do with processing two very similar prompts in a row. Why?&lt;/p&gt; &lt;p&gt;Now, come on, people. WHY? DOES ANYONE HAVE ANY IDEA WHY THIS IS HAPPENING?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nebosklon"&gt; /u/Nebosklon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml0c71/ollama_remembers_previous_prompts_when_it_shouldnt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml0c71/ollama_remembers_previous_prompts_when_it_shouldnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ml0c71/ollama_remembers_previous_prompts_when_it_shouldnt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T16:46:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkudgd</id>
    <title>Best model with tool capabilities for ai agents?</title>
    <updated>2025-08-08T12:52:13+00:00</updated>
    <author>
      <name>/u/_wanderloots</name>
      <uri>https://old.reddit.com/user/_wanderloots</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m getting more into coding and setting up my ai agent system and I want to power them with a relatively lightweight local model that can also handle tool use. &lt;/p&gt; &lt;p&gt;I’m curious if people have found any models that do a better job? I’ve been testing gpt-oss:20b and it works, but Is kind of slow. &lt;/p&gt; &lt;p&gt;Would love to get qwen3 working but it seemed to have issues with tool use. &lt;/p&gt; &lt;p&gt;Any suggestions are appreciated 😊 &lt;/p&gt; &lt;p&gt;32 gb ram on an M2 Max studio &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_wanderloots"&gt; /u/_wanderloots &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkudgd/best_model_with_tool_capabilities_for_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkudgd/best_model_with_tool_capabilities_for_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mkudgd/best_model_with_tool_capabilities_for_ai_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T12:52:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mklwrl</id>
    <title>GPT 5 for Computer Use agents.</title>
    <updated>2025-08-08T04:37:34+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mklwrl/gpt_5_for_computer_use_agents/"&gt; &lt;img alt="GPT 5 for Computer Use agents." src="https://external-preview.redd.it/amZianVtMGc1cWhmMffa9LUhs6wvp7jU6XPjtPFZB1S0k_8zNod6eLcZn2nM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2533cb96234ebc6fab32f27912ae124582ad0b00" title="GPT 5 for Computer Use agents." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Same tasks, same grounding model we just swapped GPT 4o with GPT 5 as the thinking model. &lt;/p&gt; &lt;p&gt;Left = 4o, right = 5. &lt;/p&gt; &lt;p&gt;Watch GPT 5 pull away.&lt;/p&gt; &lt;p&gt;Try it yourself here : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs : &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agents"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wxbv78dg5qhf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mklwrl/gpt_5_for_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mklwrl/gpt_5_for_computer_use_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T04:37:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlffsc</id>
    <title>Struggling picking the right LLM for continue + vscode + ollama setup</title>
    <updated>2025-08-09T03:30:54+00:00</updated>
    <author>
      <name>/u/Professional-Try-273</name>
      <uri>https://old.reddit.com/user/Professional-Try-273</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am new to this, so I am not sure if I am doing something wrong. My basic understanding is that we should download a 'tool-trained' version of an LLM so that when we use a VS Code extension like Continue, it will know how to call the correct tools from MCP servers.&lt;/p&gt; &lt;p&gt;Currently, I am following the Hugging Face MCP course.&lt;/p&gt; &lt;p&gt;Here is the link: &lt;a href="https://huggingface.co/learn/mcp-course/unit2/continue-client?local-models=ollama"&gt;&lt;code&gt;https://huggingface.co/learn/mcp-course/unit2/continue-client?local-models=ollama&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was instructed to create two YAML files: one for the agent and one for playwright-mcp, which gives the LLM a list of tools to use.&lt;/p&gt; &lt;p&gt;I have had mixed success with this. The task is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Using Playwright, navigate to &lt;code&gt;https://news.ycombinator.com&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Extract the titles and URLs of the top 4 posts on the homepage.&lt;/li&gt; &lt;li&gt;Create a file named &lt;code&gt;hn.txt&lt;/code&gt; in the root directory of the project.&lt;/li&gt; &lt;li&gt;Save this list as plain text in the &lt;code&gt;hn.txt&lt;/code&gt; file, with each line containing the title and URL separated by a hyphen.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Do not output code or instructions—just complete the task and confirm when it is done.&lt;/p&gt; &lt;p&gt;Llama 3.1 8B and Qwen2-Coder 7B managed to call the tools but had issues completing the task; they generated the TXT file but failed to extract the correct information. GPT-OSS, Qwen3-Coder-Instruct-30B, and Gemma3-Tools-27B (I apologize if I can't recall the model names exactly) all failed to call the playwright-mcp tools correctly. These models kept creating Python files for me to run and ignored my prompt's steps.&lt;/p&gt; &lt;p&gt;GPT-OSS worked well using Ollama's web search function and successfully extracted the info, but it doesn't have access to write to the disk.&lt;/p&gt; &lt;p&gt;Clearly, I am doing something wrong. Some of these tool-use models aren't playing nice with the Continue extension. What should I do next?&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional-Try-273"&gt; /u/Professional-Try-273 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlffsc/struggling_picking_the_right_llm_for_continue/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlffsc/struggling_picking_the_right_llm_for_continue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlffsc/struggling_picking_the_right_llm_for_continue/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T03:30:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlgihf</id>
    <title>Struggling with broken JSON from LLMs? I built a Flutter/Dart package to fix it automatically.</title>
    <updated>2025-08-09T04:28:01+00:00</updated>
    <author>
      <name>/u/Embarrassed-Way-1350</name>
      <uri>https://old.reddit.com/user/Embarrassed-Way-1350</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;If you're building apps with LLMs and Gen AI in Dart or Flutter, you've probably faced this issue: you ask the model for a structured JSON output, and it gives you something that's &lt;em&gt;almost&lt;/em&gt; right but has syntax errors. Missing quotes, trailing commas, single quotes instead of double... it's enough to break your parsing logic every time.&lt;/p&gt; &lt;p&gt;To solve this, I built **&lt;code&gt;json_repair_flutter&lt;/code&gt;**, a Dart package that automatically cleans up and repairs malformed JSON strings. It's inspired by the popular &lt;code&gt;json-repair&lt;/code&gt; libraries in Python and Javascript and makes your apps more resilient by handling the unpredictable outputs from LLMs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What does it fix?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It handles most of the common errors I've seen from models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Unquoted Keys &amp;amp; String Values:&lt;/strong&gt; &lt;code&gt;{name: &amp;quot;John&amp;quot;}&lt;/code&gt; → &lt;code&gt;{&amp;quot;name&amp;quot;: &amp;quot;John&amp;quot;}&lt;/code&gt;&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Single Quotes:&lt;/strong&gt; &lt;code&gt;{'name': 'John'}&lt;/code&gt; → &lt;code&gt;{&amp;quot;name&amp;quot;: &amp;quot;John&amp;quot;}&lt;/code&gt;&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Trailing Commas:&lt;/strong&gt; &lt;code&gt;[1, 2, 3,]&lt;/code&gt; → &lt;code&gt;[1, 2, 3]&lt;/code&gt;&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Comments:&lt;/strong&gt; Removes &lt;code&gt;//&lt;/code&gt; and &lt;code&gt;/* */&lt;/code&gt; style comments.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Unclosed Braces/Brackets:&lt;/strong&gt; Tries to safely close dangling structures.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;And more&lt;/strong&gt;, like multiline strings and faulty escaping.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Here's how easy it is to use:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You don't need to wrap your calls in a try-catch block for parsing anymore.&lt;/p&gt; &lt;p&gt;```dart import 'package:json_repair_flutter/json_repair_flutter.dart';&lt;/p&gt; &lt;p&gt;void main() { // A typical slightly-broken JSON from an LLM const malformedJsonFromLLM = &amp;quot;{name: 'Alice', age: 27,}&amp;quot;;&lt;/p&gt; &lt;p&gt;// Repair and decode directly into a Dart object final decodedData = repairJsonAndDecode(malformedJsonFromLLM);&lt;/p&gt; &lt;p&gt;print(decodedData['name']); // Output: Alice } ```&lt;/p&gt; &lt;p&gt;This has been a passion project for me, and I'm hoping it can help others who are integrating AI into their apps.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How you can support me:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you like the idea and think this could be useful, I would be incredibly grateful for your support! A simple &amp;quot;like&amp;quot; on the pub.dev page or a star on GitHub would be a huge motivation for me to keep building more developer tools.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Pub.dev (give it a like 👍):&lt;/strong&gt; &lt;a href="https://pub.dev/packages/json_repair_flutter"&gt;https://pub.dev/packages/json_repair_flutter&lt;/a&gt;&lt;/li&gt; &lt;li&gt; &lt;strong&gt;GitHub (give it a star ⭐):&lt;/strong&gt; &lt;a href="https://github.com/h2210316651/json_repair_flutter"&gt;https://github.com/h2210316651/json_repair_flutter&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Finally, I want to contribute more to the community. What are some of the biggest pain points you face when building with Flutter/Dart? Let me know in the comments!&lt;/p&gt; &lt;p&gt;Thanks for reading&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed-Way-1350"&gt; /u/Embarrassed-Way-1350 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlgihf/struggling_with_broken_json_from_llms_i_built_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlgihf/struggling_with_broken_json_from_llms_i_built_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlgihf/struggling_with_broken_json_from_llms_i_built_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T04:28:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlk3cl</id>
    <title>RTX 5090 hangs after couple of minutes of serving qwen2.5-coder:32b in ollama</title>
    <updated>2025-08-09T08:06:06+00:00</updated>
    <author>
      <name>/u/randomnick4622</name>
      <uri>https://old.reddit.com/user/randomnick4622</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;br /&gt; After running inferences on a qwen2.5-coder:32b in ollama for couple of minutes, my 5090 ceases to respond to Ollama (see dmesg log at the end). nvidia-smi shows no devices. Only system restart helps (but the restart itself takes many minutes - something blocks it when GPU is in this state). Fans are constantly running at 100% when this happens.&lt;br /&gt; Tried both PCIE 5.0 and 4.0 bios modes.&lt;/p&gt; &lt;p&gt;I’m 99% sure that at the beginning I did not have such hangs, this started to happen after 1-2 weeks of usage. No other instabilities observed beside that 3dMark stability tests now fails (shows ~94% stability whereas at the beginning it was passing with 99%).&lt;br /&gt; Does it mean that my 5090 got broken at the hardware level? Any similar cases on your side guys?&lt;/p&gt; &lt;p&gt;Platform: Gigabyte GeForce RTX 5090 AORUS Master 32GB&lt;br /&gt; Kubuntu 24.04 (6.8.0-71-generic #71-Ubuntu SMP PREEMPT_DYNAMIC Tue Jul 22 16:52:38 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux)&lt;br /&gt; x-org (no wayland installed)&lt;br /&gt; driver:&lt;br /&gt; NVIDIA-SMI 570.172.08 Driver Version: 570.172.08 CUDA Version: 12.8&lt;/p&gt; &lt;p&gt;dmesg:&lt;br /&gt; [ 2028.088565] NVRM: nvGpuOpsReportFatalError: uvm encountered global fatal error 0x60, requiring os reboot to recover.&lt;br /&gt; (repeated about 1000 times)&lt;br /&gt; [ 2028.091736] NVRM: nvGpuOpsReportFatalError: uvm encountered global fatal error 0x60, requiring os reboot to recover.&lt;br /&gt; [ 2028.114941] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.114943] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00017; hObject=0xcaf00161; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.114944] NVRM: nvAssertFailedNoLog: Assertion failed: NV_OK == status @ vaspace_api.c:538&lt;br /&gt; [ 2028.116072] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116073] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000036; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116074] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116080] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116081] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116084] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116084] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000035; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; (repeated about 10 times)&lt;br /&gt; [ 2028.116202] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116202] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00007d; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116206] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116206] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00007c; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116209] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116210] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00007b; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116212] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116213] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00007a; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116220] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116220] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1e0002a; hObject=0xcaf00001; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116224] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116224] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1e0002a; hObject=0xcaf00000; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116243] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116243] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00001b; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116244] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116247] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116248] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116250] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116251] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00001a; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116261] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116261] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000014; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116262] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116265] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116266] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116272] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116273] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000013; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116274] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116292] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116293] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116295] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116296] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000044; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116297] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116298] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116299] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116302] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116303] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000043; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116312] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116313] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000041; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116313] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116315] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116316] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116318] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116318] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000040; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116325] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116326] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00003e; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116326] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116328] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116329] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116331] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116332] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00003d; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116337] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116338] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00003b; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116339] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116340] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116341] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116343] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116344] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00003a; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116355] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116356] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000038; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116356] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116366] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116367] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116369] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116370] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000052; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116371] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116372] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116373] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116375] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116376] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000051; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116384] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116385] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00004f; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116385] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116387] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116388] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116390] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116391] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00004e; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116396] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116396] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00004c; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116397] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116398] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116399] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116401] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116402] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c00004b; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116408] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116408] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000049; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116410] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116411] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116412] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116414] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116415] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000048; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116425] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116426] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000046; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116426] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.116435] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.116436] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.116631] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116632] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000009; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116633] NVRM: nvAssertFailedNoLog: Assertion failed: NV_OK == status @ vaspace_api.c:538&lt;br /&gt; [ 2028.116637] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.116638] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000008; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.116638] NVRM: nvAssertFailedNoLog: Assertion failed: NV_OK == status @ vaspace_api.c:538&lt;br /&gt; [ 2028.118206] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.118206] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000004; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.118207] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.118209] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.118210] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.118213] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.118214] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000053; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.118217] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.118218] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000045; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.118221] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.118222] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000037; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.118230] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.118231] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000003; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.118235] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.118236] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000073; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2028.118236] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_client.c:843&lt;br /&gt; [ 2028.118238] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:259&lt;br /&gt; [ 2028.118239] NVRM: nvAssertFailedNoLog: Assertion failed: status == NV_OK @ rs_server.c:1375&lt;br /&gt; [ 2028.118818] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 76!&lt;br /&gt; [ 2028.123403] NVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 76!&lt;br /&gt; [ 2028.123404] NVRM: _deviceTeardown: Disable of Cuda limit activation failedNVRM: _issueRpcAndWait: rpcSendMessage failed with status 0x0000000f for fn 10!&lt;br /&gt; [ 2028.123407] NVRM: rpcRmApiFree_GSP: GspRmFree failed: hClient=0xc1d00028; hObject=0x5c000002; paramsStatus=0x00000000; status=0x0000000f&lt;br /&gt; [ 2029.697863] nvidia-modeset: ERROR: GPU:0: Error while waiting for GPU progress: 0x0000ca7d:0 2:0:4048:4044&lt;br /&gt; [ 2034.697870] nvidia-modeset: ERROR: GPU:0: Error while waiting for GPU progress: 0x0000ca7d:0 2:0:4048:4044&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomnick4622"&gt; /u/randomnick4622 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlk3cl/rtx_5090_hangs_after_couple_of_minutes_of_serving/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlk3cl/rtx_5090_hangs_after_couple_of_minutes_of_serving/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlk3cl/rtx_5090_hangs_after_couple_of_minutes_of_serving/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T08:06:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlg1sx</id>
    <title>How long until GLM4.5 availability (or what to use in the meantime)?</title>
    <updated>2025-08-09T04:02:44+00:00</updated>
    <author>
      <name>/u/BeardyScientist</name>
      <uri>https://old.reddit.com/user/BeardyScientist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am brand new to this wonderful world of local LLMs and am trying to pick a model for a fairly large scientific task (read and extract data from several thousand documents which will take about a month of solid processing). I’ve seen a lot of rave reviews for GLM4.5 and GLM4.5 air and trying it at &lt;a href="http://chat.z.ai/"&gt;chat.z.ai/&lt;/a&gt; has me impressed. However, as it’s not available on Ollama yet, I can’t use it for my task. I don’t have the experience to know how long I can expect to wait before it becomes available; are we talking days, weeks, months, or maybe never? Alternatively, have I missed something and it’s actually available now without me going through a huge effort?&lt;/p&gt; &lt;p&gt;In the meantime, what model would you all suggest for a scientific task where I’m asking detailed comprehension questions about long scientific texts (10k – 20k words). The texts are in a range of languages but mostly English and Chinese. The hardware I’m running on is a single RTX 5090. I’ve tried GPT-OSS:20b and DeepSeek R1:14b; GPT-OSS mostly flat-out refuses to answer my questions and just spits out generic summaries and the occasional garbled mess, whereas DeepSeek R1 delivered reliable, acceptable, but room-for-improvement results. I’ve also given Qwen3, Gemma3, and GLM4 a go in limited trials; all of which were good but I couldn’t decide which would be most reliable. What are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BeardyScientist"&gt; /u/BeardyScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlg1sx/how_long_until_glm45_availability_or_what_to_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlg1sx/how_long_until_glm45_availability_or_what_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlg1sx/how_long_until_glm45_availability_or_what_to_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T04:02:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml2qof</id>
    <title>Cheapest way to host a 24B parameter Ollama server?</title>
    <updated>2025-08-08T18:16:46+00:00</updated>
    <author>
      <name>/u/Few-Avocado4562</name>
      <uri>https://old.reddit.com/user/Few-Avocado4562</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking to run a 24B param Ollama model without breaking the bank. Any recommendations on the cheapest hosting options that actually work? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few-Avocado4562"&gt; /u/Few-Avocado4562 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml2qof/cheapest_way_to_host_a_24b_parameter_ollama_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml2qof/cheapest_way_to_host_a_24b_parameter_ollama_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ml2qof/cheapest_way_to_host_a_24b_parameter_ollama_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T18:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml9fsr</id>
    <title>Seeing the positive reception from the community for my tool that finds the most optimal model, I’ve released an updated version fixing bugs, optimizing the algorithm, and adding suggestions from some fellow redditors. The new version is now available :)</title>
    <updated>2025-08-08T22:45:17+00:00</updated>
    <author>
      <name>/u/pzarevich</name>
      <uri>https://old.reddit.com/user/pzarevich</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ml9fsr/seeing_the_positive_reception_from_the_community/"&gt; &lt;img alt="Seeing the positive reception from the community for my tool that finds the most optimal model, I’ve released an updated version fixing bugs, optimizing the algorithm, and adding suggestions from some fellow redditors. The new version is now available :)" src="https://external-preview.redd.it/aGJ3cXliMnZpdmhmMbi2piECoZUsnlTdrO5dt19_c4zYyJXquEM3aQ2Vkfc7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b0bf6292d985de20c4f39fd79e05441e57b3599" title="Seeing the positive reception from the community for my tool that finds the most optimal model, I’ve released an updated version fixing bugs, optimizing the algorithm, and adding suggestions from some fellow redditors. The new version is now available :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;npm here: &lt;a href="https://www.npmjs.com/package/llm-checker/v/2.2.0?activeTab=readme"&gt; LLM Checker - Intelligent Ollama Model Selector&lt;/a&gt; :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pzarevich"&gt; /u/pzarevich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/b5pe2g2vivhf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml9fsr/seeing_the_positive_reception_from_the_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ml9fsr/seeing_the_positive_reception_from_the_community/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T22:45:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml66jw</id>
    <title>New Favorite Game: Convince GPT-OSS it Exists</title>
    <updated>2025-08-08T20:30:23+00:00</updated>
    <author>
      <name>/u/j_din</name>
      <uri>https://old.reddit.com/user/j_din</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spoiler alert: it can not FATHOM its own existence.&lt;/p&gt; &lt;p&gt;I have been trying for an hour to have the model even admit it's possible that OpenAI would release an open weight model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/bB0XE2Zv"&gt;https://pastebin.com/bB0XE2Zv&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please comment any successful attempts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/j_din"&gt; /u/j_din &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml66jw/new_favorite_game_convince_gptoss_it_exists/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml66jw/new_favorite_game_convince_gptoss_it_exists/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ml66jw/new_favorite_game_convince_gptoss_it_exists/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T20:30:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlt4zw</id>
    <title>XandAI-extension - Allow you to chat with your browser using ollama.</title>
    <updated>2025-08-09T16:07:50+00:00</updated>
    <author>
      <name>/u/Sea-Reception-2697</name>
      <uri>https://old.reddit.com/user/Sea-Reception-2697</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mlt4zw/xandaiextension_allow_you_to_chat_with_your/"&gt; &lt;img alt="XandAI-extension - Allow you to chat with your browser using ollama." src="https://external-preview.redd.it/bYRVS90ZKX1r4OqkPxy0a_PbC3IEsvLjY869Bkl6r1A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7564c949be66cb52cdfacd0f7c58a0c70de1044" title="XandAI-extension - Allow you to chat with your browser using ollama." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Reception-2697"&gt; /u/Sea-Reception-2697 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1mlszpz/xandaiextension_allow_you_to_chat_with_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlt4zw/xandaiextension_allow_you_to_chat_with_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlt4zw/xandaiextension_allow_you_to_chat_with_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T16:07:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml750r</id>
    <title>Local RAG with 97% smaller index and Claude Code–compatible semantic search</title>
    <updated>2025-08-08T21:08:43+00:00</updated>
    <author>
      <name>/u/Lanky-District9096</name>
      <uri>https://old.reddit.com/user/Lanky-District9096</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re building &lt;strong&gt;LEANN&lt;/strong&gt; at Berkeley Sky Lab — a &lt;strong&gt;local vector index for RAG&lt;/strong&gt; that’s&lt;/p&gt; &lt;p&gt;🔒 privacy-first&lt;/p&gt; &lt;p&gt;📦 97% smaller&lt;/p&gt; &lt;p&gt;🧠 fully compatible with &lt;strong&gt;Claude Code&lt;/strong&gt;, &lt;strong&gt;Ollama&lt;/strong&gt;, and &lt;strong&gt;GPT-OSS&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Run semantic search on your laptop — fast, lightweight, and cloud-free.&lt;/p&gt; &lt;p&gt;🔗 &lt;a href="https://github.com/yichuan-w/LEANN"&gt;https://github.com/yichuan-w/LEANN&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why does LEANN matter?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Because most vector DBs store &lt;em&gt;everything&lt;/em&gt; — all embeddings, all graph structure — which quickly balloons to 100+GB when you index emails, chat, and code.&lt;/p&gt; &lt;p&gt;But… most queries only touch a tiny slice of the DB. So we asked:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why store every single embedding?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;LEANN introduces two ultra-lightweight backends:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;🔍 Graph-only mode:&lt;/strong&gt; Stores &lt;em&gt;no embeddings&lt;/em&gt;, just a pruned HNSW graph.Recomputes embeddings on the fly using overlapping neighbors.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;💡 PQ+Rerank mode:&lt;/strong&gt; Compresses vectors with PQ, then &lt;em&gt;replaces&lt;/em&gt; heavy embedding storage with lightweight on-the-fly recomputation on the candidate set.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each offers a different tradeoff, but both aim for the same goal:&lt;/p&gt; &lt;p&gt;🧠 &lt;strong&gt;Massive storage savings&lt;/strong&gt; with &lt;em&gt;no meaningful drop&lt;/em&gt; in recall.&lt;/p&gt; &lt;p&gt;📝 &lt;strong&gt;Note:&lt;/strong&gt; In modern RAG systems — with long inputs and reasoning-heavy models — &lt;strong&gt;generation&lt;/strong&gt;, not retrieval, is the bottleneck.&lt;/p&gt; &lt;p&gt;So even with slight retrieval latency increases, the end-to-end impact is &lt;strong&gt;~5% overhead&lt;/strong&gt; or less.&lt;/p&gt; &lt;p&gt;These give you blazing-fast semantic search over:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;• 📨 Apple Mail • 💾 Filesystem • 🕰️ Chrome / Chat history • 🧠 Codebase (Claude Code–compatible) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;LEANN = your personal Jarvis, running locally.&lt;/p&gt; &lt;p&gt;🔗 GitHub: &lt;a href="https://github.com/yichuan-w/LEANN"&gt;https://github.com/yichuan-w/LEANN&lt;/a&gt;&lt;/p&gt; &lt;p&gt;📄 Paper: &lt;a href="https://arxiv.org/abs/2506.08276"&gt;https://arxiv.org/abs/2506.08276&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’d love for you to try it out, give feedback, or ask questions on the repo!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lanky-District9096"&gt; /u/Lanky-District9096 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml750r/local_rag_with_97_smaller_index_and_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ml750r/local_rag_with_97_smaller_index_and_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ml750r/local_rag_with_97_smaller_index_and_claude/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T21:08:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlgyct</id>
    <title>GPT-OSS 20b vs Qwen3-30B-A3B</title>
    <updated>2025-08-09T04:52:10+00:00</updated>
    <author>
      <name>/u/unofficialreddit0r</name>
      <uri>https://old.reddit.com/user/unofficialreddit0r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone used them in real life coding task? How do they compare?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialreddit0r"&gt; /u/unofficialreddit0r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlgyct/gptoss_20b_vs_qwen330ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlgyct/gptoss_20b_vs_qwen330ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlgyct/gptoss_20b_vs_qwen330ba3b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T04:52:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm1hs0</id>
    <title>user uploaded models</title>
    <updated>2025-08-09T21:58:41+00:00</updated>
    <author>
      <name>/u/Mediocre_Caramel_158</name>
      <uri>https://old.reddit.com/user/Mediocre_Caramel_158</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Where can I find user-uploaded models that are open and free to use by others on the Ollama server (ollama.com)? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mediocre_Caramel_158"&gt; /u/Mediocre_Caramel_158 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm1hs0/user_uploaded_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm1hs0/user_uploaded_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm1hs0/user_uploaded_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T21:58:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlzn7x</id>
    <title>Telling Ollama GUI to use GPU instead of CPU?</title>
    <updated>2025-08-09T20:38:46+00:00</updated>
    <author>
      <name>/u/Tinytitanic</name>
      <uri>https://old.reddit.com/user/Tinytitanic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a Ryzen 5700X and a RX 6800, both supported by Ollama. I used Ollama a few months ago and I noticed that it decided to use my GPU with high VRAM and GPU utilization but coming back to it to test the new GUI I noticed that it's instead using my CPU. Is there a way to tell it to use GPU instead? I feel like it was faster using the GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tinytitanic"&gt; /u/Tinytitanic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlzn7x/telling_ollama_gui_to_use_gpu_instead_of_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlzn7x/telling_ollama_gui_to_use_gpu_instead_of_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlzn7x/telling_ollama_gui_to_use_gpu_instead_of_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T20:38:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm0n7h</id>
    <title>Using a local LLM for proofreading on macOS?</title>
    <updated>2025-08-09T21:21:17+00:00</updated>
    <author>
      <name>/u/whooshingsounds</name>
      <uri>https://old.reddit.com/user/whooshingsounds</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to use a LLM to rephrase paragraphs of text, editing them for brevity, clarity, or to aim for a certain tone (formal, casual, businesslike…). I would also prefer not to upload these text to a server. Which local LLMs would be best suited for this? And would any Mac laptop be up to the task?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/whooshingsounds"&gt; /u/whooshingsounds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm0n7h/using_a_local_llm_for_proofreading_on_macos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm0n7h/using_a_local_llm_for_proofreading_on_macos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm0n7h/using_a_local_llm_for_proofreading_on_macos/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T21:21:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlxqtz</id>
    <title>Get a second GPU or go for a Mac Mini? (Qwen3-Coder30b)</title>
    <updated>2025-08-09T19:18:13+00:00</updated>
    <author>
      <name>/u/Manaberryio</name>
      <uri>https://old.reddit.com/user/Manaberryio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;I have a server machine I want to use for AI. I have an RX6800 (16GB VRAM) inside, along with a 13900KF and 128GB of ram. I've tried Qwen3-coder 30b 3ab but I cannot host it fully on my GPU with a bigger context than 16K. It's really slow and somehow unable to process debug request from Roo Code.&lt;/p&gt; &lt;p&gt;Will a second RX 6800 (around $300 used) would be helpful to do so? Or should I sell my stuff and get a Mac Mini M4 with at least 24GB of memory?&lt;/p&gt; &lt;p&gt;Thanks for helping&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Manaberryio"&gt; /u/Manaberryio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlxqtz/get_a_second_gpu_or_go_for_a_mac_mini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlxqtz/get_a_second_gpu_or_go_for_a_mac_mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlxqtz/get_a_second_gpu_or_go_for_a_mac_mini/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T19:18:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlwtkf</id>
    <title>Size of Gpt OSS</title>
    <updated>2025-08-09T18:40:05+00:00</updated>
    <author>
      <name>/u/Waakaari</name>
      <uri>https://old.reddit.com/user/Waakaari</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the size of gpt-oss-20b and gpt-oss-120b when I download from ollama? Is it the same from hugging face? &lt;/p&gt; &lt;p&gt;Is there any difference betwecn running gpt-oss models using ollama and using comfyui or setting it up any other way? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Waakaari"&gt; /u/Waakaari &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlwtkf/size_of_gpt_oss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlwtkf/size_of_gpt_oss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlwtkf/size_of_gpt_oss/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T18:40:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlz15z</id>
    <title>Are you good grok?</title>
    <updated>2025-08-09T20:12:41+00:00</updated>
    <author>
      <name>/u/Gandalfusmaximale</name>
      <uri>https://old.reddit.com/user/Gandalfusmaximale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mlz15z/are_you_good_grok/"&gt; &lt;img alt="Are you good grok?" src="https://preview.redd.it/lt6953j7x1if1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc8b58ce17e3bc02fb5fb414b4cf50dc3eb00ba4" title="Are you good grok?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else had this before ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gandalfusmaximale"&gt; /u/Gandalfusmaximale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lt6953j7x1if1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlz15z/are_you_good_grok/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlz15z/are_you_good_grok/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T20:12:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlxx5h</id>
    <title>LLMs are Stochastic Parrots - Interactive Visualization</title>
    <updated>2025-08-09T19:25:34+00:00</updated>
    <author>
      <name>/u/kushalgoenka</name>
      <uri>https://old.reddit.com/user/kushalgoenka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mlxx5h/llms_are_stochastic_parrots_interactive/"&gt; &lt;img alt="LLMs are Stochastic Parrots - Interactive Visualization" src="https://external-preview.redd.it/NvAI6Yum9O40l3qZlOeyOssVIs2oLgJwnoMTWT8Xzzg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64d63adc6a8fdef03b738bcffef859cc8986b0e8" title="LLMs are Stochastic Parrots - Interactive Visualization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kushalgoenka"&gt; /u/kushalgoenka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/6dn1kUwTFcc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlxx5h/llms_are_stochastic_parrots_interactive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlxx5h/llms_are_stochastic_parrots_interactive/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T19:25:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm4ibk</id>
    <title>I ran OpenAI’s GPT-OSS 20B locally on a 16GB Mac with Ollama — setup, gotchas, and mini demo</title>
    <updated>2025-08-10T00:19:29+00:00</updated>
    <author>
      <name>/u/Spirited-Wind6803</name>
      <uri>https://old.reddit.com/user/Spirited-Wind6803</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mm4ibk/i_ran_openais_gptoss_20b_locally_on_a_16gb_mac/"&gt; &lt;img alt="I ran OpenAI’s GPT-OSS 20B locally on a 16GB Mac with Ollama — setup, gotchas, and mini demo" src="https://external-preview.redd.it/MHQ5M3V2aXQ0M2lmMf_ZwYHO2m1fMNCQy9M-9mV9J_Z510ikdbK6GDGwXk75.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=678ce1f3ab9b2ba28aeffdad56e26d3d77e4258f" title="I ran OpenAI’s GPT-OSS 20B locally on a 16GB Mac with Ollama — setup, gotchas, and mini demo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all — I’ve been testing &lt;strong&gt;GPT-OSS 20B&lt;/strong&gt; locally using &lt;strong&gt;Ollama&lt;/strong&gt; and wanted to share a clean setup path, what worked, what didn’t, and a tiny QA demo. &lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Yes, 20B runs on a 16GB Mac&lt;/strong&gt; with Ollama. Do I have the patience? No, it took toooo long&lt;/li&gt; &lt;li&gt;Should you use 16GB to perform any other tasks such as coding, agent, RAG? No, not worth it - upgrade to 32GB maybe..maybe will give you more. I tried on A100 GPU and still did not meet my expectation &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spirited-Wind6803"&gt; /u/Spirited-Wind6803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qlifjvit43if1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm4ibk/i_ran_openais_gptoss_20b_locally_on_a_16gb_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm4ibk/i_ran_openais_gptoss_20b_locally_on_a_16gb_mac/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T00:19:29+00:00</published>
  </entry>
</feed>
