<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-28T12:48:18+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jjnzhf</id>
    <title>Cheapest Serverless Coding LLM or API</title>
    <updated>2025-03-25T16:48:58+00:00</updated>
    <author>
      <name>/u/juan_berger</name>
      <uri>https://old.reddit.com/user/juan_berger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the CHEAPEST serverless option to run an llm for coding (at least as good as qwen 32b).&lt;/p&gt; &lt;p&gt;Basically asking what is the cheapest way to use an llm through an api, not the web ui.&lt;/p&gt; &lt;p&gt;Open to ideas like: - Official APIs (if they are cheap) - Serverless (Modal, Lambda, etc...) - Spot GPU instance running ollama - Renting (Vast AI &amp;amp; Similar) - Services like Google Cloud Run&lt;/p&gt; &lt;p&gt;Basically curious what options people have tried.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juan_berger"&gt; /u/juan_berger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjnzhf/cheapest_serverless_coding_llm_or_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjnzhf/cheapest_serverless_coding_llm_or_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jjnzhf/cheapest_serverless_coding_llm_or_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-25T16:48:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk9fab</id>
    <title>Ollama *always* summarizes a local text file</title>
    <updated>2025-03-26T11:22:33+00:00</updated>
    <author>
      <name>/u/ozaarmat</name>
      <uri>https://old.reddit.com/user/ozaarmat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OS : MacOS 15.3.2&lt;br /&gt; ollama : installed locally and as python module&lt;br /&gt; models : llama2, mistral&lt;br /&gt; language : python3&lt;br /&gt; issue : no matter what I prompt, the output is always a summary of the local text file. &lt;/p&gt; &lt;p&gt;I'd appreciate some tips if anyone has encountered this issue. &lt;/p&gt; &lt;p&gt;CLI PROMPT 1&lt;br /&gt; $python3 &lt;a href="http://promptfile2.py"&gt;promptfile2.py&lt;/a&gt; cinq_semaines.txt &amp;quot;Count the words in this text file&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt; The prompt is read correctly&lt;br /&gt; &amp;quot;Sending prompt: Count the number of words and characters in this file. &amp;quot; but&lt;br /&gt; &amp;gt;&amp;gt; I get a summary of the text file, irrespective of which model is selected (llama2 or mistral)&lt;/p&gt; &lt;p&gt;CLI PROMPT 2&lt;br /&gt; $ollama run mistral &amp;quot;Do not summarize. Return only the total number of words in this text as an integer, nothing else: Hello world, this is a test.&amp;quot;&lt;br /&gt; &amp;gt;&amp;gt; 15&lt;br /&gt; &amp;gt;&amp;gt; direct prompt returns the correct result. Counting words is for testing purposes, I know there are other ways to count words. &lt;/p&gt; &lt;p&gt;** ollama/mistral is able to understand the instruction when called directly, but not via the script.&lt;br /&gt; ** My text file is in French, but llama2 or mistral read it and give me a nice summary in English.&lt;br /&gt; ** I tried ollama.chat() and ollama.generate() &lt;/p&gt; &lt;p&gt;Code :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import ollama import os import sys # Check command-line arguments if len(sys.argv) &amp;lt; 2 or len(sys.argv) &amp;gt; 3: print(&amp;quot;Usage: python3 promptfileX.py &amp;lt;filename.txt&amp;gt; [prompt]&amp;quot;) print(&amp;quot; If no prompt is provided, defaults to 'Summarize'&amp;quot;) sys.exit(1) filename = sys.argv[1] prompt = sys.argv[2] # Check file validity if not filename.endswith(&amp;quot;.txt&amp;quot;) or not os.path.isfile(filename): print(&amp;quot;Error: Please provide a valid .txt file&amp;quot;) sys.exit(1) # Read the file def read_text_file(file_path): try: with open(file_path, 'r', encoding='utf-8') as file: return file.read() except Exception as e: return f&amp;quot;Error reading file: {str(e)}&amp;quot; # Use ollama.generate() def query_ollama_generate(content, prompt): full_prompt = f&amp;quot;{prompt}\n\n---\n\n{content}&amp;quot; print(f&amp;quot;Sending prompt: {prompt[:60]}...&amp;quot;) try: response = ollama.generate( model='mistral', # or 'mistral', whichever you want prompt=full_prompt ) return response['response'] except Exception as e: return f&amp;quot;Error from Ollama: {str(e)}&amp;quot; # Main content = read_text_file(filename) if &amp;quot;Error&amp;quot; in content: print(content) sys.exit(1) result = query_ollama_generate(content, prompt) print(&amp;quot;Ollama response:&amp;quot;) print(result) import ollama import os import sys # Check command-line arguments if len(sys.argv) &amp;lt; 2 or len(sys.argv) &amp;gt; 3: print(&amp;quot;Usage: python3 promptfileX.py &amp;lt;filename.txt&amp;gt; [prompt]&amp;quot;) print(&amp;quot; If no prompt is provided, defaults to 'Summarize'&amp;quot;) sys.exit(1) filename = sys.argv[1] prompt = sys.argv[2] # Check file validity if not filename.endswith(&amp;quot;.txt&amp;quot;) or not os.path.isfile(filename): print(&amp;quot;Error: Please provide a valid .txt file&amp;quot;) sys.exit(1) # Read the file def read_text_file(file_path): try: with open(file_path, 'r', encoding='utf-8') as file: return file.read() except Exception as e: return f&amp;quot;Error reading file: {str(e)}&amp;quot; # Use ollama.generate() def query_ollama_generate(content, prompt): full_prompt = f&amp;quot;{prompt}\n\n---\n\n{content}&amp;quot; print(f&amp;quot;Sending prompt: {prompt[:60]}...&amp;quot;) try: response = ollama.generate( model='mistral', # or 'mistral', whichever you want prompt=full_prompt ) return response['response'] except Exception as e: return f&amp;quot;Error from Ollama: {str(e)}&amp;quot; # Main content = read_text_file(filename) if &amp;quot;Error&amp;quot; in content: print(content) sys.exit(1) result = query_ollama_generate(content, prompt) print(&amp;quot;Ollama response:&amp;quot;) print(result) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ozaarmat"&gt; /u/ozaarmat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jk9fab/ollama_always_summarizes_a_local_text_file/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jk9fab/ollama_always_summarizes_a_local_text_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jk9fab/ollama_always_summarizes_a_local_text_file/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-26T11:22:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jju2jy</id>
    <title>Create Your Personal AI Knowledge Assistant - No Coding Needed</title>
    <updated>2025-03-25T20:54:42+00:00</updated>
    <author>
      <name>/u/PeterHash</name>
      <uri>https://old.reddit.com/user/PeterHash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've just published a guide on building a personal AI assistant using Open WebUI that works with your own documents.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What You Can Do:&lt;/strong&gt; - Answer questions from personal notes - Search through research PDFs - Extract insights from web content - Keep all data private on your own machine&lt;/p&gt; &lt;p&gt;My tutorial walks you through: - Setting up a knowledge base - Creating a research companion - Lots of tips and trick for getting precise answers - All without any programming&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Might be helpful for:&lt;/strong&gt; - Students organizing research - Professionals managing information - Anyone wanting smarter document interactions&lt;/p&gt; &lt;p&gt;Upcoming articles will cover more advanced AI techniques like function calling and multi-agent systems.&lt;/p&gt; &lt;p&gt;Curious what knowledge base you're thinking of creating. Drop a comment!&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@hautel.alex2000/open-webui-tutorial-supercharging-your-local-ai-with-rag-and-custom-knowledge-bases-334d272c8c40"&gt;Open WebUI tutorial — Supercharge Your Local AI with RAG and Custom Knowledge Bases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PeterHash"&gt; /u/PeterHash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jju2jy/create_your_personal_ai_knowledge_assistant_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jju2jy/create_your_personal_ai_knowledge_assistant_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jju2jy/create_your_personal_ai_knowledge_assistant_no/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-25T20:54:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjsc2b</id>
    <title>I got Ollama working on my 9070xt - here's how (Windows)</title>
    <updated>2025-03-25T19:44:49+00:00</updated>
    <author>
      <name>/u/DegenerativePoop</name>
      <uri>https://old.reddit.com/user/DegenerativePoop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was struggling to get the official image of Ollama to work with my new 9070xt. It doesn't appear to natively support it yet. I was browsing and found &lt;a href="https://github.com/likelovewant/ollama-for-amd/releases/tag/v0.6.3"&gt;Ollama-For-AMD&lt;/a&gt;. I installed that version, and downloaded the &lt;a href="https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU/releases/tag/v0.6.2.4"&gt;ROCmLibs for 6.2.4&lt;/a&gt; (it would be the rocm gfx1201 file).&lt;/p&gt; &lt;p&gt;Find the &lt;code&gt;rocblas.dll&lt;/code&gt; file and the &lt;code&gt;rocblas/library&lt;/code&gt; folder within the Ollama installation folder (usually located at &lt;code&gt;C:\Users\usrname\AppData\Local\Programs\Ollama\lib\ollama\rocm&lt;/code&gt;). I am not sure where it is in linux, at least not until I get home and check)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Delete&lt;/strong&gt; the existing &lt;code&gt;rocblas/library&lt;/code&gt; folder.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Replace&lt;/strong&gt; it with the correct ROCm libraries.&lt;/li&gt; &lt;li&gt;Also replace the rocblas.dll file with the downloaded one&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That's it! It's working for me, and it works pretty well!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DegenerativePoop"&gt; /u/DegenerativePoop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjsc2b/i_got_ollama_working_on_my_9070xt_heres_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjsc2b/i_got_ollama_working_on_my_9070xt_heres_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jjsc2b/i_got_ollama_working_on_my_9070xt_heres_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-25T19:44:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkfsmr</id>
    <title>GPU Not Recognized in Ollama Running in LXC (Host: pve) – "cuda driver library init failure: 999" Error</title>
    <updated>2025-03-26T16:23:21+00:00</updated>
    <author>
      <name>/u/lowriskcork</name>
      <uri>https://old.reddit.com/user/lowriskcork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I’m encountering a persistent issue trying to enable GPU acceleration with Ollama within an LXC container on my host system. Although my host detects the GPU via PCI (and the appropriate kernel driver is in use), Ollama inside the container cannot initialize CUDA and falls back to CPU inference with the following error:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;unknown error initializing cuda driver library /usr/lib/x86_64-linux-gnu/nvidia/current/libcuda.so.535.216.01: cuda driver library init failure: 999. see https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md for more information &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Below I’ve included the diagnostic information I’ve gathered both from the container and the host.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Inside the Container:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;CUDA Library and NVIDIA Directory:&lt;/strong&gt;&lt;em&gt;Output snippet from the container:&lt;/em&gt;ls -l /lib/x86_64-linux-gnu/libcuda.so* ls -l /usr/lib/x86_64-linux-gnu/nvidia/current/ lrwxrwxrwx 1 root root 34 Mar 26 16:17 /lib/x86_64-linux-gnu/libcuda.so.535.216.01 -&amp;gt; /lib/x86_64-linux-gnu/libcuda.so.1 ... &lt;/li&gt; &lt;li&gt;&lt;strong&gt;LD_LIBRARY_PATH:&lt;/strong&gt;&lt;em&gt;Output:&lt;/em&gt;echo $LD_LIBRARY_PATH /usr/lib/x86_64-linux-gnu/nvidia/current:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/nvidia/current:/usr/lib/x86_64-linux-gnu: &lt;/li&gt; &lt;li&gt;&lt;strong&gt;NVIDIA GPU Details:&lt;/strong&gt;&lt;em&gt;Output from container:&lt;/em&gt;nvidia-smi Wed Mar 26 16:20:09 2025 +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 535.216.01 Driver Version: 535.216.01 CUDA Version: 12.2 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | |=========================================+======================+======================| | 0 Quadro P2000 On | 00000000:C1:00.0 Off | N/A | +-----------------------------------------+----------------------+----------------------+ &lt;/li&gt; &lt;li&gt;&lt;strong&gt;CUDA Compiler Version:&lt;/strong&gt;&lt;em&gt;Output snippet:&lt;/em&gt;nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Cuda compilation tools, release 11.8, V11.8.89 &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kernel Information:&lt;/strong&gt;&lt;em&gt;Output:&lt;/em&gt;uname -a Linux GPU 6.8.12-9-pve #1 SMP PREEMPT_DYNAMIC PMX 6.8.12-9 (2025-03-16T19:18Z) x86_64 GNU/Linux &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamic Linker Cache for CUDA:&lt;/strong&gt;&lt;em&gt;Output snippet:&lt;/em&gt;ldconfig -p | grep cuda libcuda.so.1 (libc6,x86-64) =&amp;gt; /lib/x86_64-linux-gnu/libcuda.so.1 &lt;a href="http://libcuda.so"&gt;libcuda.so&lt;/a&gt; (libc6,x86-64) =&amp;gt; /lib/x86_64-linux-gnu/libcuda.so &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Logs:&lt;/strong&gt;&lt;em&gt;Key Log Lines:&lt;/em&gt;ollama serve time=2025-03-26T16:20:41.525Z level=WARN source=gpu.go:605 msg=&amp;quot;unknown error initializing cuda driver library /usr/lib/x86_64-linux-gnu/nvidia/current/libcuda.so.535.216.01: cuda driver library init failure: 999...&amp;quot; time=2025-03-26T16:20:41.593Z level=INFO source=gpu.go:377 msg=&amp;quot;no compatible GPUs were discovered&amp;quot; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Container Environment Variables:&lt;/strong&gt;&lt;em&gt;Snippet of the output:&lt;/em&gt;cat /proc/1/environ | tr '\0' '\n' TERM=linux container=lxc &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;On the Host Machine:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I also gathered some details from the host, running on Proxmox Virtual Environment (pve):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Kernel Version and OS Info:&lt;/strong&gt;&lt;em&gt;Output:&lt;/em&gt;uname -a Linux pve 6.8.12-9-pve #1 SMP PREEMPT_DYNAMIC PMX 6.8.12-9 (2025-03-16T19:18Z) x86_64 &lt;/li&gt; &lt;li&gt;&lt;strong&gt;nvidia-smi:&lt;/strong&gt;When I ran &lt;code&gt;nvidia-smi&lt;/code&gt; on the host, I received:However, the GPU is visible via PCI later.-bash: nvidia-smi: command not found &lt;/li&gt; &lt;li&gt;&lt;strong&gt;PCI Device Listing:&lt;/strong&gt;&lt;em&gt;Output:&lt;/em&gt;lspci -nnk | grep -i nvidia c1:00.0 VGA compatible controller [0300]: NVIDIA Corporation GP106GL [Quadro P2000] [10de:1c30] (rev a1) Kernel driver in use: nvidia Kernel modules: nvidia c1:00.1 Audio device [0403]: NVIDIA Corporation GP106 High Definition Audio Controller [10de:10f1] (rev a1) &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Host Dynamic Linker Cache:&lt;/strong&gt;&lt;em&gt;Output snippet:&lt;/em&gt;ldconfig -p | grep cuda libcuda.so.1 (libc6,x86-64) =&amp;gt; /lib/x86_64-linux-gnu/libcuda.so.1 &lt;a href="http://libcuda.so"&gt;libcuda.so&lt;/a&gt; (libc6,x86-64) =&amp;gt; /lib/x86_64-linux-gnu/libcuda.so &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The Issue &amp;amp; My Questions:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Issue:&lt;/strong&gt; Despite detailed configuration inside the container, Ollama fails to initialize the CUDA driver (error 999) and falls back to CPU, even though the GPU is visible and the symlink adjustments seem correct.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Questions:&lt;/strong&gt; &lt;ol&gt; &lt;li&gt;Are there any known compatibility issues with Ollama, the specific NVIDIA driver/CUDA version, and running inside an LXC container?&lt;/li&gt; &lt;li&gt;Is there additional host-side configuration (perhaps re: GPU passthrough or container privileges) that I should check?&lt;/li&gt; &lt;li&gt;Should I provide or adjust any further details from the host (like installing or running &lt;code&gt;nvidia-smi&lt;/code&gt; on the host) to help diagnose this?&lt;/li&gt; &lt;li&gt;Are there additional debugging steps to force Ollama to successfully initialize the CUDA driver?&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any help or insights would be greatly appreciated. I’m happy to provide further logs or configuration details if needed.&lt;/p&gt; &lt;p&gt;Thanks in advance for your assistance!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Additional Note:&lt;/strong&gt;&lt;br /&gt; If anyone has suggestions for ensuring that the host’s NVIDIA tools (like &lt;code&gt;nvidia-smi&lt;/code&gt;) are available for deeper diagnostics from inside the host environment, please let me know.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lowriskcork"&gt; /u/lowriskcork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkfsmr/gpu_not_recognized_in_ollama_running_in_lxc_host/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkfsmr/gpu_not_recognized_in_ollama_running_in_lxc_host/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jkfsmr/gpu_not_recognized_in_ollama_running_in_lxc_host/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-26T16:23:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkgp0g</id>
    <title>Hardware Recommendations</title>
    <updated>2025-03-26T17:00:36+00:00</updated>
    <author>
      <name>/u/CorpusculantCortex</name>
      <uri>https://old.reddit.com/user/CorpusculantCortex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just that, I am looking for recommendations for what to prioritize hardware wise.&lt;/p&gt; &lt;p&gt;I am far overdue for a computer upgrade, current system: I7 9700kf 32gb ram RTX 2070 &lt;/p&gt; &lt;p&gt;And i have been thinking something like: I9 14900k 64g ddr5 RTX 5070TI (if ever available)&lt;/p&gt; &lt;p&gt;That was what I was thinking, but have gotten into the world of ollama relatively recently, specifically trying to host my own llm to drive my project goose ai agent. I tried a half dozen models on my current system, but as you can imagine they are either painfully slow, or painfully inadequate. So I am looking to upgrade with that as a dream, but it may be way out of reach.. the leader board for tool calling is topped by watt-tool 70B but i can't see how i could afford to run that with any efficiency. I also want to do more light /medium model training, but not llms really, I'm a data analyst/scientist/engineer and would be leveraging for optimization of work tasks. But I think anything that can handle a decent ollama instance can manage my needs there&lt;/p&gt; &lt;p&gt;The overall goal is to use this all for work tasks that I really can't send certain data offside. And or the sheer volume of frequency would make it prohibitive to go pay model.&lt;/p&gt; &lt;p&gt;Anyway my budget is ~$2000 USD and I don't have the bandwidth or trust to run down used parts right now.&lt;/p&gt; &lt;p&gt;What are your recommendations for what I should prioritize. I am very not up on the state of the art but am trying to get there quickly. Any special installations and approaches that I should learn about are also helpful! Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CorpusculantCortex"&gt; /u/CorpusculantCortex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkgp0g/hardware_recommendations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkgp0g/hardware_recommendations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jkgp0g/hardware_recommendations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-26T17:00:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk36h6</id>
    <title>Best small model to run without a gpu? (For coding and questions)</title>
    <updated>2025-03-26T04:00:00+00:00</updated>
    <author>
      <name>/u/Game-Lover44</name>
      <uri>https://old.reddit.com/user/Game-Lover44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a pretty good desktop but i want to test the limits of a laptop i have that im not sure what to do with but i want to be more productive on the go.&lt;/p&gt; &lt;p&gt;said laptop has 16 ram ddr4, 2 threads and 4 cores (intel i5 that is old), around 200 gb ssd, its a Lenovo ThinkPad T470 and it is possible i may have got something wrong.&lt;/p&gt; &lt;p&gt;would i be better of using a online ai, i just find myself in alot of places that dont have wifi for my laptop such as a waiting room.&lt;/p&gt; &lt;p&gt;i havent found a good small model yet and there no way im running anything big on this laptop.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Game-Lover44"&gt; /u/Game-Lover44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jk36h6/best_small_model_to_run_without_a_gpu_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jk36h6/best_small_model_to_run_without_a_gpu_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jk36h6/best_small_model_to_run_without_a_gpu_for_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-26T04:00:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkhtxd</id>
    <title>changelog for https://ollama.com/library/gemma3 ?</title>
    <updated>2025-03-26T17:46:03+00:00</updated>
    <author>
      <name>/u/caetydid</name>
      <uri>https://old.reddit.com/user/caetydid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw gemma3 got updated yesterday - is there a way to see changelogs for ollama model library updates?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/caetydid"&gt; /u/caetydid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkhtxd/changelog_for_httpsollamacomlibrarygemma3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkhtxd/changelog_for_httpsollamacomlibrarygemma3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jkhtxd/changelog_for_httpsollamacomlibrarygemma3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-26T17:46:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjtyrq</id>
    <title>Create Your Personal AI Knowledge Assistant - No Coding Needed</title>
    <updated>2025-03-25T20:50:15+00:00</updated>
    <author>
      <name>/u/PeterHash</name>
      <uri>https://old.reddit.com/user/PeterHash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've just published a guide on building a personal AI assistant using Open WebUI that works with your own documents.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What You Can Do:&lt;/strong&gt; - Answer questions from personal notes - Search through research PDFs - Extract insights from web content - Keep all data private on your own machine&lt;/p&gt; &lt;p&gt;My tutorial walks you through: - Setting up a knowledge base - Creating a research companion - Lots of tips and trick for getting precise answers - All without any programming&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Might be helpful for:&lt;/strong&gt; - Students organizing research - Professionals managing information - Anyone wanting smarter document interactions&lt;/p&gt; &lt;p&gt;Upcoming articles will cover more advanced AI techniques like function calling and multi-agent systems.&lt;/p&gt; &lt;p&gt;Curious what knowledge base you're thinking of creating. Drop a comment!&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@hautel.alex2000/open-webui-tutorial-supercharging-your-local-ai-with-rag-and-custom-knowledge-bases-334d272c8c40"&gt;Open WebUI tutorial — Supercharge Your Local AI with RAG and Custom Knowledge Bases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PeterHash"&gt; /u/PeterHash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjtyrq/create_your_personal_ai_knowledge_assistant_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjtyrq/create_your_personal_ai_knowledge_assistant_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jjtyrq/create_your_personal_ai_knowledge_assistant_no/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-25T20:50:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkjttu</id>
    <title>Problems Using Vision Models</title>
    <updated>2025-03-26T19:07:28+00:00</updated>
    <author>
      <name>/u/GhostInThePudding</name>
      <uri>https://old.reddit.com/user/GhostInThePudding</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else having trouble with vision models from either Ollama or Huggingface? Gemma3 works fine, but I tried about 8 variants of it that are meant to be uncensored/abliterated and none of them work. For example:&lt;br /&gt; &lt;a href="https://ollama.com/huihui_ai/gemma3-abliterated"&gt;https://ollama.com/huihui_ai/gemma3-abliterated&lt;/a&gt;&lt;br /&gt; &lt;a href="https://ollama.com/nidumai/nidum-gemma-3-27b-instruct-uncensored"&gt;https://ollama.com/nidumai/nidum-gemma-3-27b-instruct-uncensored&lt;/a&gt;&lt;br /&gt; Both claim to support vision, and they run and work normally, but if you try and add an image, it simply doesn't add the image and will answers questions about the image with pure hallucinations.&lt;/p&gt; &lt;p&gt;I also tried a bunch from Huggingface, I got the GGUF version but they give errors when running. I've got plenty of Huggingface models running before, but the vision ones seem to require multiple files, but even when I create a model to load the files, I get various errors.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GhostInThePudding"&gt; /u/GhostInThePudding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkjttu/problems_using_vision_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkjttu/problems_using_vision_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jkjttu/problems_using_vision_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-26T19:07:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkyt85</id>
    <title>Dual rtx 3060</title>
    <updated>2025-03-27T07:56:27+00:00</updated>
    <author>
      <name>/u/ExtensionPatient7681</name>
      <uri>https://old.reddit.com/user/ExtensionPatient7681</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, im thinking of the popular setup of dual rtx 3060s. &lt;/p&gt; &lt;p&gt;Right now it seems to automatically run on my laptop gpu but when im upgrading to a dedicated server im wondering how much configuration and tinkering i must do to make it run on a dual gpu setup.&lt;/p&gt; &lt;p&gt;Is it as simple as plugging in the gpu's and download the cuda drivers then Download ollama and run the model or do i need to do further configuration?&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExtensionPatient7681"&gt; /u/ExtensionPatient7681 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkyt85/dual_rtx_3060/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkyt85/dual_rtx_3060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jkyt85/dual_rtx_3060/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-27T07:56:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkth2i</id>
    <title>Has anybody gotten anything useful out of Exaone 32b?</title>
    <updated>2025-03-27T02:17:01+00:00</updated>
    <author>
      <name>/u/GVDub2</name>
      <uri>https://old.reddit.com/user/GVDub2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Installed it today, asked it to evaluate a short Python script to update restart policy on Docker containers, and it spent 10 minutes thinking, starting to seriously hallucinate halfway through. DeepSeekR1:32b (distill of Qwen2.5) thought of 45 seconds, and spit out improved streamlined code. I find it hard to believe the charts with with Ollama model that claim Exaone is all that. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GVDub2"&gt; /u/GVDub2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkth2i/has_anybody_gotten_anything_useful_out_of_exaone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkth2i/has_anybody_gotten_anything_useful_out_of_exaone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jkth2i/has_anybody_gotten_anything_useful_out_of_exaone/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-27T02:17:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk7hh0</id>
    <title>Use Ollama to create your own AI Memory locally from 30+ types of data sources</title>
    <updated>2025-03-26T09:06:32+00:00</updated>
    <author>
      <name>/u/Short-Honeydew-7000</name>
      <uri>https://old.reddit.com/user/Short-Honeydew-7000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;We've just finished a small guide on how to set up Ollama with cognee, an open-source AI memory tool that will allow you to ingest your local data into graph/vector stores, enrich it and search it.&lt;/p&gt; &lt;p&gt;You can load all your codebase to cognee and enrich it with your README file and documentation or load images, video and audio data and merge different data sources.&lt;/p&gt; &lt;p&gt;And in the end you get to see and explore a nice looking graph.&lt;/p&gt; &lt;p&gt;Here is a short tutorial to set up Ollama with cognee:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=aZYRo-eXDzA&amp;amp;t=62s"&gt;https://www.youtube.com/watch?v=aZYRo-eXDzA&amp;amp;t=62s&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here is our Github: &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/topoteretes/cognee"&gt;https://github.com/topoteretes/cognee&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Short-Honeydew-7000"&gt; /u/Short-Honeydew-7000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jk7hh0/use_ollama_to_create_your_own_ai_memory_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jk7hh0/use_ollama_to_create_your_own_ai_memory_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jk7hh0/use_ollama_to_create_your_own_ai_memory_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-26T09:06:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkz8lc</id>
    <title>Is it possible to train an AI to help run a D&amp;D campaign?</title>
    <updated>2025-03-27T08:30:24+00:00</updated>
    <author>
      <name>/u/SeriousLemur</name>
      <uri>https://old.reddit.com/user/SeriousLemur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running a modified version of a D&amp;amp;D campaign and I have all the information for the campaign in a bunch of .pdf or .htm files. I've been trying to get ChatGPT to thoroughly refer through the content before giving me answers but it still messes up important details sometimes.&lt;/p&gt; &lt;p&gt;Would it be possible to run something locally on my machine and train it to either memorize all of the details of the campaign or thoroughly read all of the documents before answering? I'd like help with creating descriptions, dialogue, suggestions on how things could continue, etc. Thank you, I'm unfamiliar with this stuff, I don't even know how to install ollama lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SeriousLemur"&gt; /u/SeriousLemur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkz8lc/is_it_possible_to_train_an_ai_to_help_run_a_dd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkz8lc/is_it_possible_to_train_an_ai_to_help_run_a_dd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jkz8lc/is_it_possible_to_train_an_ai_to_help_run_a_dd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-27T08:30:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl5gn2</id>
    <title>How to extract &lt;think&gt; tags for Deepseek?</title>
    <updated>2025-03-27T14:40:19+00:00</updated>
    <author>
      <name>/u/Desperate-Finger7851</name>
      <uri>https://old.reddit.com/user/Desperate-Finger7851</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm building an application that uses Ollama with Deepseek locally; I think it would be really cool to stream the &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; tags in real time to the application frontend (would be Streamlit for prototyping, eventually React).&lt;/p&gt; &lt;p&gt;I looked briefly and couldn't find much information on how they work? &lt;/p&gt; &lt;p&gt;Any help greatly appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate-Finger7851"&gt; /u/Desperate-Finger7851 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jl5gn2/how_to_extract_think_tags_for_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jl5gn2/how_to_extract_think_tags_for_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jl5gn2/how_to_extract_think_tags_for_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-27T14:40:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlqee3</id>
    <title>How much VRAM does gemma3:27b vision utilize in addition to text inference only?</title>
    <updated>2025-03-28T08:31:41+00:00</updated>
    <author>
      <name>/u/EatTFM</name>
      <uri>https://old.reddit.com/user/EatTFM</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running a job for extracting data from PDFs using ollama with gemma3:27b on a machine with anRTX 4090 24Gb VRAM. &lt;/p&gt; &lt;p&gt;I can see that ollama uses like 50% of my GPU core and 90% of my VRAM, but also all of my 12-core CPUs. I do not need that long context - could it be that I am that quickly out of VRAM due to the additional image processing?&lt;/p&gt; &lt;p&gt;Ollama lists the model as 17G in size.&lt;/p&gt; &lt;p&gt;root@llm:~# ollama ps&lt;br /&gt; NAME ID SIZE PROCESSOR UNTIL &lt;br /&gt; gemma3:27b 30ddded7fba6 21 GB 5%/95% CPU/GPU 4 minutes from now&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EatTFM"&gt; /u/EatTFM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlqee3/how_much_vram_does_gemma327b_vision_utilize_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlqee3/how_much_vram_does_gemma327b_vision_utilize_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlqee3/how_much_vram_does_gemma327b_vision_utilize_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T08:31:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlr3e4</id>
    <title>Whats up with Quantized models selection?</title>
    <updated>2025-03-28T09:26:22+00:00</updated>
    <author>
      <name>/u/Birdinhandandbush</name>
      <uri>https://old.reddit.com/user/Birdinhandandbush</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically when you go to the models section on the Ollama website, as far as I can tell it only shows you all the Q4 models. &lt;/p&gt; &lt;p&gt;You have to go to HuggingFace to find Q5-Q8 models for example. Why doesn't the official Ollama page have a drop down for different quantizations of the same models? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Birdinhandandbush"&gt; /u/Birdinhandandbush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlr3e4/whats_up_with_quantized_models_selection/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlr3e4/whats_up_with_quantized_models_selection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlr3e4/whats_up_with_quantized_models_selection/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T09:26:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlrlng</id>
    <title>Cpu??</title>
    <updated>2025-03-28T10:03:16+00:00</updated>
    <author>
      <name>/u/ExtensionPatient7681</name>
      <uri>https://old.reddit.com/user/ExtensionPatient7681</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How much does cpu matter when building a server? As i understand it i need as much vram as i can get. But what about cpu? Can i get away with a i9-7900X CPU @ 3.30GHz or do i need more?&lt;/p&gt; &lt;p&gt;Im asking because i can buy this second hand for 700usd, and my thinking is that its a good place to start. But since the cpu is old but was good for that age im not sure if its gonna slow me down a bunch of not.&lt;/p&gt; &lt;p&gt;Im gonna use it for a whisper large model and ollama model, as big as i can fit for a homeassistant voice assistant.&lt;/p&gt; &lt;p&gt;Since the mobo supports another gpu i was thinking of adding another 3060 down the line.&lt;/p&gt; &lt;p&gt;Mobo: Asus Corsair asus prime x299-a&lt;/p&gt; &lt;p&gt;Cpu: i9-7900X CPU @ 3.30GHz 3.31 GHz&lt;/p&gt; &lt;p&gt;Ram: 16gb&lt;/p&gt; &lt;p&gt;Gpu: rtx 3060&lt;/p&gt; &lt;p&gt;SSD: 465gb&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExtensionPatient7681"&gt; /u/ExtensionPatient7681 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlrlng/cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlrlng/cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlrlng/cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T10:03:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlrxdv</id>
    <title>Tuning Ollama for parallel request processing on a Nvidia RTX 1000 ADA</title>
    <updated>2025-03-28T10:26:34+00:00</updated>
    <author>
      <name>/u/icbts</name>
      <uri>https://old.reddit.com/user/icbts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jlrxdv/tuning_ollama_for_parallel_request_processing_on/"&gt; &lt;img alt="Tuning Ollama for parallel request processing on a Nvidia RTX 1000 ADA" src="https://external-preview.redd.it/fEmD4oMekfPVVq-gnDwT6mUL3NzZlSGs-l5UF8X1JoM.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df6db28e9416ddd9991856a19f855d7b9f64c0a6" title="Tuning Ollama for parallel request processing on a Nvidia RTX 1000 ADA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tuning Ollama for our Dell R250 w/ Nvidia RTX 1000 ADA (8Gb vram) card.&lt;/p&gt; &lt;p&gt;Ollama supports running requests in parallel, in this video we test out various settings for number of parallel context requests on a few different models to see if there are optimal settings for overall throughput. Keeping in mind that this card draws 50 watts processing sequentially or under higher load, its in our interest to get as much through the card as we can.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/icbts"&gt; /u/icbts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=lne8ChZ5rZk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlrxdv/tuning_ollama_for_parallel_request_processing_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlrxdv/tuning_ollama_for_parallel_request_processing_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T10:26:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jls83u</id>
    <title>Which model makes sense for my requirements?</title>
    <updated>2025-03-28T10:47:03+00:00</updated>
    <author>
      <name>/u/soft-boy</name>
      <uri>https://old.reddit.com/user/soft-boy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am using Ollama and want to run an llm locally on my MacBook Air. I mainly use it to give feedback on texts like screenplays. &lt;/p&gt; &lt;p&gt;I have used Llama for the past few days and am super disappointed in the results. &lt;/p&gt; &lt;p&gt;Which model would you guys suggest?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/soft-boy"&gt; /u/soft-boy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jls83u/which_model_makes_sense_for_my_requirements/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jls83u/which_model_makes_sense_for_my_requirements/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jls83u/which_model_makes_sense_for_my_requirements/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T10:47:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlmc0d</id>
    <title>Resume Tailor - an AI-powered tool that helps job seekers customize their resumes for specific positions! 💼</title>
    <updated>2025-03-28T03:50:54+00:00</updated>
    <author>
      <name>/u/Maleficent-Penalty50</name>
      <uri>https://old.reddit.com/user/Maleficent-Penalty50</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jlmc0d/resume_tailor_an_aipowered_tool_that_helps_job/"&gt; &lt;img alt="Resume Tailor - an AI-powered tool that helps job seekers customize their resumes for specific positions! 💼" src="https://external-preview.redd.it/cTJnaXcwb3NyY3JlMcOCn68Ug8MSclXcs9Aau_EFApnS4CTmSgUVZy2v_uPC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d67b3a7496d6954ccf612a3941dd8120038a2aa" title="Resume Tailor - an AI-powered tool that helps job seekers customize their resumes for specific positions! 💼" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent-Penalty50"&gt; /u/Maleficent-Penalty50 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/npa680rsrcre1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlmc0d/resume_tailor_an_aipowered_tool_that_helps_job/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlmc0d/resume_tailor_an_aipowered_tool_that_helps_job/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T03:50:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlqp06</id>
    <title>Ollama does not do well</title>
    <updated>2025-03-28T08:55:42+00:00</updated>
    <author>
      <name>/u/aadarsh_af</name>
      <uri>https://old.reddit.com/user/aadarsh_af</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;None of the ollama models or tags work well with structured output. I've tried it with 3B param models as i don't have large GPU resources, my GPU gets stuck even with llama3.2. I've tried prompt engineering and grammar, it does not generate valid JSON. Is there any way i could make smaller param models perform well with lesser compute power?? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aadarsh_af"&gt; /u/aadarsh_af &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlqp06/ollama_does_not_do_well/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlqp06/ollama_does_not_do_well/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlqp06/ollama_does_not_do_well/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T08:55:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlkpsj</id>
    <title>Which is the smallest, fastest text generation model on ollama that can be used for chatbot?</title>
    <updated>2025-03-28T02:24:49+00:00</updated>
    <author>
      <name>/u/gilzonme</name>
      <uri>https://old.reddit.com/user/gilzonme</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gilzonme"&gt; /u/gilzonme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlkpsj/which_is_the_smallest_fastest_text_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlkpsj/which_is_the_smallest_fastest_text_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlkpsj/which_is_the_smallest_fastest_text_generation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T02:24:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlhysn</id>
    <title>Building a front end that sits on ollama, is this pointless?</title>
    <updated>2025-03-28T00:08:08+00:00</updated>
    <author>
      <name>/u/Outside-Prune-5838</name>
      <uri>https://old.reddit.com/user/Outside-Prune-5838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jlhysn/building_a_front_end_that_sits_on_ollama_is_this/"&gt; &lt;img alt="Building a front end that sits on ollama, is this pointless?" src="https://a.thumbs.redditmedia.com/_uNUXAvyhpU50VvCDcArNF1fQGQ4kxrSsS9nrKJqCZ4.jpg" title="Building a front end that sits on ollama, is this pointless?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I started using gpt but ran into limits, got the $20 plan and was still hitting limits (because ai is fun) so I asked gpt what I could do and it recommended chatting through the api. Another gpt and 30 versions later I had a front end that spoke to openai but had zero personality. They also tend to lose their minds when the conversations get long.&lt;/p&gt; &lt;p&gt;Back to gpt to complain and asked how to do it for free and it said go for local llm and landed on ollama. Naturally I chose models that were too big to run on my machine because I was clueless but I got it sorted.&lt;/p&gt; &lt;p&gt;Got a bit annoyed at the basic interface and lack of memory and personality so I went back to gpt (getting my moneys worth) and spent a week (so far) working on a frontend that can talk to either locally running ollama or openai through api, remembers everything you spoke about and your memory is stored locally. It can analyse files and store them in memory too. You can give it whole documents then ask for summaries or specific points. It also reads what llms are downloaded in ollama and can even autostart them from the interface. You can also load in custom personas over the llm.&lt;/p&gt; &lt;p&gt;Also supports either local embedding w/gpu or embedding from openai through their api. Im debating releasing it because it was just a niche thing I did for me which turned into a whole ass program. If you can run ollama comfortably, you can run this on top easily as theres almost zero overhead.&lt;/p&gt; &lt;p&gt;The goal is jarvis on a budget and the memory thing has evolved several times which resulted because I wanted it to remember my name and now it remembers &lt;em&gt;everything&lt;/em&gt;. It also has a voice journal mode (work in progress, think star trek captains log). Right now integrating more voice features and an even more niche feature - way to control sonar, sabnzbd and radarr through the llm. Its also going to have tool access to go online and whatnot.&lt;/p&gt; &lt;p&gt;Its basically a multi-LLM brain with a shared long-term memory that is saved on your pc. You can start a conversation with your local llm, switch to gpt for something more complicated THEN switch back and your local llm has access to everything. The chat window doesnt even clear.&lt;/p&gt; &lt;p&gt;Talking to gpt through api doesnt require a plus plan just requires a few bucks in your openai api account, although Im big on local everything.&lt;/p&gt; &lt;h1&gt;Here's what happens under the hood:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;You chat with Mistral (or whatever llm) → everything gets stored: &lt;ul&gt; &lt;li&gt;Chat history → SQLite&lt;/li&gt; &lt;li&gt;Embedded chunks → ChromaDB&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;You switch to GPT (OpenAI) → same memory system is accessed: &lt;ul&gt; &lt;li&gt;GPT pulls from the same vector memory&lt;/li&gt; &lt;li&gt;You may even embed with the same SentenceTransformer (if not OpenAI embeddings)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;You switch back to Mistral → &lt;strong&gt;nothing is lost&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Vector search still hits all past data&lt;/li&gt; &lt;li&gt;SQLite short-term history still intact (unless wiped)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Snippet below, shameless self plug, sorry:&lt;/p&gt; &lt;h1&gt;⚛️ Atom — A Memory-Driven, Local AI Command Center&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Atom&lt;/strong&gt; is a locally hosted, memory-enhanced AI assistant built for devs, tinkerers, and power users who want full control of their LLM environment. It fuses &lt;strong&gt;chat&lt;/strong&gt;, &lt;strong&gt;file-based memory&lt;/strong&gt;, &lt;strong&gt;tool execution&lt;/strong&gt;, and &lt;strong&gt;GPU-accelerated embedding&lt;/strong&gt; — all inside a slick, modular cockpit interface.&lt;/p&gt; &lt;p&gt;Forget cloud APIs and stateless interactions. Atom doesn’t just respond — it &lt;strong&gt;remembers&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;🧠 Why Atom’s Memory Hits Different&lt;/h1&gt; &lt;p&gt;Atom combines &lt;strong&gt;short-term chat memory&lt;/strong&gt; and &lt;strong&gt;long-term vector memory&lt;/strong&gt; to create a persistent assistant that can recall your history, files, and intent — across sessions.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Vector Memory (ChromaDB)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Automatically chunks and embeds uploaded files (e.g., &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Semantically searchable — even if you don’t use exact keywords&lt;/li&gt; &lt;li&gt;Fully GPU-accelerated with &lt;code&gt;sentence-transformers&lt;/code&gt; + CUDA&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat Memory (SQLite)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Logs all user/assistant messages&lt;/li&gt; &lt;li&gt;Feeds recent dialogue back into the LLM for continuity&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory Injection&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Relevant chunks are auto-injected into system prompts in real time&lt;/li&gt; &lt;li&gt;Optional filters by file, chunk ID, or context window&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory Dashboard&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Full frontend panel showing stored vector data&lt;/li&gt; &lt;li&gt;View per-file chunk metadata (source, index, timestamp)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🔧 Core Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;⚡ &lt;strong&gt;GPU Embeddings&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;900+ chunks embedded from large files in seconds&lt;/li&gt; &lt;li&gt;Powered by RTX CUDA-enabled cards&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;🧰 &lt;strong&gt;LLM Tool Execution&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Add tools like &lt;code&gt;summarize_file&lt;/code&gt;, &lt;code&gt;search_web&lt;/code&gt;, &lt;code&gt;inject_chunk&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Triggered with &lt;code&gt;::tool:&lt;/code&gt; syntax or natural language&lt;/li&gt; &lt;li&gt;Executed live via FastAPI backend&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;👤 &lt;strong&gt;Persona Layer&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;YAML-defined styles (e.g., casual, sarcastic, technical)&lt;/li&gt; &lt;li&gt;Memory-aware greetings (e.g., &amp;quot;Welcome back, John.&amp;quot;)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;🖥️ &lt;strong&gt;React UI with Vite + Tailwind&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Tabbed interface: Chat, Files, Memory View, Tools, etc.&lt;/li&gt; &lt;li&gt;Model selector, GPU monitor, file uploader, token preview&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;🔐 &lt;strong&gt;Offline, Private, and Extendable&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Ollama + Mistral for fast local inference&lt;/li&gt; &lt;li&gt;No API keys needed (openai api access and openai embedding is totally optional)&lt;/li&gt; &lt;li&gt;No cloud. No snooping.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;💡 TL;DR&lt;/h1&gt; &lt;p&gt;Atom isn’t just another chatbot UI — it’s a &lt;strong&gt;self-hosted, memory-capable assistant platform&lt;/strong&gt; that grows smarter the more you use it.&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Its a work in progress. Written by me and several gpts, its still evolving and may never see the light of day.&lt;/p&gt; &lt;p&gt;Unless people actually want it, then I might throw it on git.&lt;/p&gt; &lt;p&gt;But yeah. ollama is great tbh.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/09rqnxtmlcre1.png?width=1708&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f766e4878a7e0dc294cff4b02df897c665b88031"&gt;https://preview.redd.it/09rqnxtmlcre1.png?width=1708&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f766e4878a7e0dc294cff4b02df897c665b88031&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xbc9wisolcre1.png?width=779&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b6e499ce94827264aedddaab83c4732b8cd0ee39"&gt;https://preview.redd.it/xbc9wisolcre1.png?width=779&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b6e499ce94827264aedddaab83c4732b8cd0ee39&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c8hovy3rkbre1.png?width=1707&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=947c6b167251ce602fb33ff064c3d5ce60116f15"&gt;https://preview.redd.it/c8hovy3rkbre1.png?width=1707&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=947c6b167251ce602fb33ff064c3d5ce60116f15&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Update 3/27&lt;/p&gt; &lt;h1&gt;ATOM: Post-Cognee Upgrade Breakdown&lt;/h1&gt; &lt;h1&gt;🧠 MEMORY: From Flat to Hybrid Brain&lt;/h1&gt; &lt;h1&gt;BEFORE:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Chunks were just text blobs — untyped, unstructured&lt;/li&gt; &lt;li&gt;Memory was recalled via top-k semantic match&lt;/li&gt; &lt;li&gt;No separation between facts, tasks, chat, etc.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;AFTER:&lt;/h1&gt; &lt;p&gt;✅ &lt;strong&gt;Memory Typing&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Each memory chunk has a &lt;code&gt;type&lt;/code&gt;: &lt;code&gt;chat&lt;/code&gt;, &lt;code&gt;identity&lt;/code&gt;, &lt;code&gt;file&lt;/code&gt;, &lt;code&gt;task&lt;/code&gt;, &lt;code&gt;summary&lt;/code&gt;, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;✅ &lt;strong&gt;Memory Prioritization&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Chunks can be tagged with &lt;code&gt;priority&lt;/code&gt; levels (&lt;code&gt;low&lt;/code&gt;, &lt;code&gt;high&lt;/code&gt;, &lt;code&gt;critical&lt;/code&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;✅ &lt;strong&gt;Usage Tracking&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Each chunk now tracks how many times it’s been retrieved: &lt;code&gt;usage_count&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;✅ &lt;strong&gt;TTL Expiration&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Chunks can auto-expire after a set time using &lt;code&gt;expires&lt;/code&gt; metadata&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;✅ &lt;strong&gt;Memory Role Filtering&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Excludes assistant replies from being re-injected and parroted&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;✅ &lt;strong&gt;Memory Source Support (coming)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tag origin: &lt;code&gt;user&lt;/code&gt;, &lt;code&gt;tool&lt;/code&gt;, &lt;code&gt;system&lt;/code&gt;, &lt;code&gt;reflection&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🔁 REFLECTION SYSTEM&lt;/h1&gt; &lt;p&gt;✅ &lt;strong&gt;Scheduled Reflection&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Every 10 messages, Atom runs a full memory review: &lt;ul&gt; &lt;li&gt;Reflects on &lt;code&gt;identity&lt;/code&gt;, &lt;code&gt;file&lt;/code&gt;, and &lt;code&gt;task&lt;/code&gt; chunks&lt;/li&gt; &lt;li&gt;Sorts by &lt;code&gt;usage_count&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Stores summaries as &lt;code&gt;type=&amp;quot;summary&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;✅ &lt;strong&gt;Tool:&lt;/strong&gt; &lt;code&gt;generate_memory_reflection&lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Can be called manually or auto-triggered&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;✅ &lt;strong&gt;Stored like internal thoughts&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You’ll see memory chunks like:&lt;/li&gt; &lt;li&gt;[Reflection: identity] &lt;ol&gt; &lt;li&gt;Bob is a network engineer. (used 12x)&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;2. Prefers short, smart answers. (used 7x)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;✅ &lt;strong&gt;LLM can now reason over what it reflects&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;🛠️ TOOLCHAIN EXPANSION&lt;/h1&gt; &lt;p&gt;You now have a &lt;strong&gt;fully extensible tool registry&lt;/strong&gt; with:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Tool&lt;/th&gt; &lt;th align="left"&gt;Purpose&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;summarize_file&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;LLM-based file summarization&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;recall_memory_type&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Get all memory of a given type&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;set_memory_type&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Reclassify memory&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;prioritize_memory&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Change priority level&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;delete_memory&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Remove chunks&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;purge_expired_chunks&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Wipe expired data&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;generate_memory_reflection&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Run type-specific reflections&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;summarize_memory_stats&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Show chunk count, usage, TTL status&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;✅ Tool calls are handled via &lt;code&gt;::tool:tool_name{args}&lt;/code&gt;&lt;br /&gt; ✅ Fully callable by the LLM (agent-ready)&lt;br /&gt; ✅ Fully expandable by you&lt;/p&gt; &lt;h1&gt;📊 COGNITIVE UI UPGRADES&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Memory Stats Panel&lt;/strong&gt; → Shows count, usage, expiration&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory View Filtering&lt;/strong&gt; (next step) → Filter by type, priority&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reflection Viewer&lt;/strong&gt; (planned) → Read Atom’s thoughts&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chunk Reclassification / Deletion Buttons&lt;/strong&gt; (planned)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outside-Prune-5838"&gt; /u/Outside-Prune-5838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlhysn/building_a_front_end_that_sits_on_ollama_is_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jlhysn/building_a_front_end_that_sits_on_ollama_is_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jlhysn/building_a_front_end_that_sits_on_ollama_is_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T00:08:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jll087</id>
    <title>Great event tonight with Ollama and vLLM</title>
    <updated>2025-03-28T02:39:46+00:00</updated>
    <author>
      <name>/u/Rude-Bad-6579</name>
      <uri>https://old.reddit.com/user/Rude-Bad-6579</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jll087/great_event_tonight_with_ollama_and_vllm/"&gt; &lt;img alt="Great event tonight with Ollama and vLLM" src="https://preview.redd.it/6yvrist4fcre1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=90f67ffb2ffcb6b8fada40af2bd2ba6a22bfc94b" title="Great event tonight with Ollama and vLLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Packed house, lots of great attendees. Loved Gemma demo running off 1 Mac laptop live. Super impressive &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rude-Bad-6579"&gt; /u/Rude-Bad-6579 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6yvrist4fcre1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jll087/great_event_tonight_with_ollama_and_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jll087/great_event_tonight_with_ollama_and_vllm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T02:39:46+00:00</published>
  </entry>
</feed>
