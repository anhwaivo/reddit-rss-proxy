<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-03T16:38:49+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1j1kig3</id>
    <title>Harry Potter: Tom Riddle's Diary Using Ollama/Llama3.2</title>
    <updated>2025-03-02T06:25:05+00:00</updated>
    <author>
      <name>/u/learn_And_</name>
      <uri>https://old.reddit.com/user/learn_And_</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/learn_And_"&gt; /u/learn_And_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/iwl7xvg9z7me1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1kig3/harry_potter_tom_riddles_diary_using_ollamallama32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1kig3/harry_potter_tom_riddles_diary_using_ollamallama32/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T06:25:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1kfjs</id>
    <title>Is it possible to change where Ollama installs itself?</title>
    <updated>2025-03-02T06:19:51+00:00</updated>
    <author>
      <name>/u/DuelShockX</name>
      <uri>https://old.reddit.com/user/DuelShockX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As title says, I have a C drive and a D drive and I like to install everything on the D drive since it's the bigger one but Ollama doesn't seem to be giving me a choice in the matter when I try to install. Am I missing something or is it optionless in that regard?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DuelShockX"&gt; /u/DuelShockX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1kfjs/is_it_possible_to_change_where_ollama_installs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1kfjs/is_it_possible_to_change_where_ollama_installs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1kfjs/is_it_possible_to_change_where_ollama_installs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T06:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1i2sq</id>
    <title>How to run multiple instances of same model</title>
    <updated>2025-03-02T03:57:20+00:00</updated>
    <author>
      <name>/u/Arwin_06</name>
      <uri>https://old.reddit.com/user/Arwin_06</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I have two rtx3060 each 12gb ram. I am running llama3.2 model which uses only 4gb vram. How can I run multiple instances of llama3.2 instead of running 1 llama3.2 I planning to run a total of 6 llama3.2 in my gpu. This is because I am hosting the model locally, if request increase the wait time is increasing so if I host multiple instances I can distribute the load. Please help me &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arwin_06"&gt; /u/Arwin_06 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1i2sq/how_to_run_multiple_instances_of_same_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1i2sq/how_to_run_multiple_instances_of_same_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1i2sq/how_to_run_multiple_instances_of_same_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T03:57:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1oze3</id>
    <title>1-2.0b llms practial use cases</title>
    <updated>2025-03-02T11:38:06+00:00</updated>
    <author>
      <name>/u/pencilline</name>
      <uri>https://old.reddit.com/user/pencilline</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;due to hardware limitations, i use anything within 1-2b llms (deepseek-r1:1.5b and qwen:1.8b) what can i use these models for that is practical?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pencilline"&gt; /u/pencilline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1oze3/120b_llms_practial_use_cases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1oze3/120b_llms_practial_use_cases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1oze3/120b_llms_practial_use_cases/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T11:38:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1wljt</id>
    <title>A proper coding LLM</title>
    <updated>2025-03-02T17:45:15+00:00</updated>
    <author>
      <name>/u/Daedric800</name>
      <uri>https://old.reddit.com/user/Daedric800</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys i need help to find local light weight Ilm that is specified and fine-tuned just for the task of coding, which means a model that is trained only for coding and nothing else which make it very light weight and small in size since it does not do chat, math, etc.. which makes it small in size yet powerful in coding like claude or deepseek models, i cant see why i havent came across a model like that yet, why are not people making a specific coding models, we are at 2025, so please if you have a model with these specs please do tell me, so i could use it for a proper coding tasks on my low end gpu locally &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daedric800"&gt; /u/Daedric800 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1wljt/a_proper_coding_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1wljt/a_proper_coding_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1wljt/a_proper_coding_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T17:45:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2bwbb</id>
    <title>$100 Worth of Deepseek R1 API Credits for Just $20</title>
    <updated>2025-03-03T05:52:07+00:00</updated>
    <author>
      <name>/u/babarich-id</name>
      <uri>https://old.reddit.com/user/babarich-id</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/5zc9bs3f7ame1.png?width=1805&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44d3421a0f3fa0774815d68c670fda43b9c66b64"&gt;https://preview.redd.it/5zc9bs3f7ame1.png?width=1805&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44d3421a0f3fa0774815d68c670fda43b9c66b64&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone, this might sound like a scam, but I'm really serious about wanting to sell this because I need the money right now.&lt;/p&gt; &lt;p&gt;I’m selling 7 pre-registered Kluster AI accounts, each for just $20. Each account comes loaded with a $100 free credit—much better than the current new user offer, which only provides $5 in credit for new signups.&lt;/p&gt; &lt;p&gt;You will get access to the models provided by the Kluster AI :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DeepSeek-R1&lt;/li&gt; &lt;li&gt;Meta-Llama-3.1-8B-Instruct-Turbo&lt;/li&gt; &lt;li&gt;Meta-Llama-3.3-70B-Instruct-Turbo&lt;/li&gt; &lt;li&gt;Meta-Llama-3.1-405B-Instruct-Turbo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;every buyer will receive my personal WhatsApp contact for any support or questions you might have.&lt;/p&gt; &lt;p&gt;DM me if you're interested in grabbing one of these accounts!&lt;/p&gt; &lt;p&gt;Note :&lt;/p&gt; &lt;p&gt;New users must purchase at least $10 in credits to upgrade to the Standard tier and unlock priority processing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/babarich-id"&gt; /u/babarich-id &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2bwbb/100_worth_of_deepseek_r1_api_credits_for_just_20/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2bwbb/100_worth_of_deepseek_r1_api_credits_for_just_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2bwbb/100_worth_of_deepseek_r1_api_credits_for_just_20/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T05:52:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1qzba</id>
    <title>Looking to Contribute to LLM &amp; AI Agent Projects – Willing to Learn &amp; Help!</title>
    <updated>2025-03-02T13:35:45+00:00</updated>
    <author>
      <name>/u/Superb_Practice_4544</name>
      <uri>https://old.reddit.com/user/Superb_Practice_4544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’m eager to contribute to an LLM or AI agent project to deepen my understanding and gain hands-on experience.&lt;/p&gt; &lt;p&gt;I have an intermediate understanding of machine learning and AI concepts, including architectures like Transformers. As a fresher, I was an active member of my college's AI club, where I worked on multiple projects involving scratch training and fine-tuning. I have hands-on experience building Retrieval-Augmented Generation (RAG) pipelines and chatbot applications using LangChain.&lt;/p&gt; &lt;p&gt;I don’t expect compensation—just looking for an opportunity to collaborate, contribute, and grow. If you're working on something cool and could use an extra pair of hands, let’s connect!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Superb_Practice_4544"&gt; /u/Superb_Practice_4544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1qzba/looking_to_contribute_to_llm_ai_agent_projects/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1qzba/looking_to_contribute_to_llm_ai_agent_projects/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1qzba/looking_to_contribute_to_llm_ai_agent_projects/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T13:35:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1q26x</id>
    <title>Avoid placeholders</title>
    <updated>2025-03-02T12:45:42+00:00</updated>
    <author>
      <name>/u/samftijazwaro</name>
      <uri>https://old.reddit.com/user/samftijazwaro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No matter what prompt I use, no matter what system prompt I give, deepseek 14b and qwen-coder14b ALWAYS use placeholder text.&lt;/p&gt; &lt;p&gt;I want to be asked &amp;quot;what is the path to the file, what is your username, what is the URL?&amp;quot; and then once it has the information, provide complete terminal commands.&lt;/p&gt; &lt;p&gt;I just cannot get it to work. Meanwhile, I have 0 such issues with Grok 3/ChatGPT. Is it simply a limitation of weaker models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samftijazwaro"&gt; /u/samftijazwaro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1q26x/avoid_placeholders/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1q26x/avoid_placeholders/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1q26x/avoid_placeholders/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T12:45:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1w6j5</id>
    <title>Feedback Required! on Reasoning Model Trained/finetuned using GRPO</title>
    <updated>2025-03-02T17:28:04+00:00</updated>
    <author>
      <name>/u/adeelahmadch</name>
      <uri>https://old.reddit.com/user/adeelahmadch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I continued the training of the LLAMA 3.2 3B quantized version on my mac book using a custom written GRPO based Agent in Gym Env using MLX. I have not finished the training on all episodes but keen to get some feedback from the community.&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/adeelahmad/ReasonableLLAMA-Jr-3b"&gt;https://ollama.com/adeelahmad/ReasonableLLAMA-Jr-3b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please feel free to let me know how bad it is :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adeelahmadch"&gt; /u/adeelahmadch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1w6j5/feedback_required_on_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1w6j5/feedback_required_on_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1w6j5/feedback_required_on_reasoning_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T17:28:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1n84y</id>
    <title>Experiment Reddit + small local LLM</title>
    <updated>2025-03-02T09:36:28+00:00</updated>
    <author>
      <name>/u/raul3820</name>
      <uri>https://old.reddit.com/user/raul3820</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to test the possibility of filtering content with small local models, just reading the text multiple times, filtering few things at a time. In this case I use &lt;code&gt;mistral-small:24b&lt;/code&gt;&lt;/p&gt; &lt;p&gt;To test the idea, I made a reddit account &lt;a href="/u/osoconfesoso007"&gt;u/osoconfesoso007&lt;/a&gt; that receives stories and publishes them anonimously.&lt;/p&gt; &lt;p&gt;It's supposed to filter out personal data and only publish interesting stories. I want to test if the filters are reliable, so feel free to poke at it.&lt;/p&gt; &lt;p&gt;It's open source: &lt;a href="https://github.com/raul3820/oso"&gt;github&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/raul3820"&gt; /u/raul3820 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1n84y/experiment_reddit_small_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1n84y/experiment_reddit_small_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1n84y/experiment_reddit_small_local_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T09:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1yh4j</id>
    <title>Suggestions for a coding model for a MacBook M4 Pro 24gb</title>
    <updated>2025-03-02T19:02:03+00:00</updated>
    <author>
      <name>/u/PLCLINK</name>
      <uri>https://old.reddit.com/user/PLCLINK</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Would be pleased to hear your suggestions or experiences. Thanks in advance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PLCLINK"&gt; /u/PLCLINK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1yh4j/suggestions_for_a_coding_model_for_a_macbook_m4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1yh4j/suggestions_for_a_coding_model_for_a_macbook_m4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1yh4j/suggestions_for_a_coding_model_for_a_macbook_m4/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T19:02:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j21vzv</id>
    <title>What's the difference between Ollama and LMstudio for hosting?</title>
    <updated>2025-03-02T21:24:36+00:00</updated>
    <author>
      <name>/u/DuelShockX</name>
      <uri>https://old.reddit.com/user/DuelShockX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm still trying to get Ollama downloaded on my D drive instead of C drive so I've only experienced LMstudio so far. Anyone here can tell me what's the difference between the two? Does Ollama offer a way to connect the models to the internetfor real-time data?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DuelShockX"&gt; /u/DuelShockX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j21vzv/whats_the_difference_between_ollama_and_lmstudio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j21vzv/whats_the_difference_between_ollama_and_lmstudio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j21vzv/whats_the_difference_between_ollama_and_lmstudio/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T21:24:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1ree1</id>
    <title>WASImancer, an MCP server with SSE transport, powered by WebAssembly</title>
    <updated>2025-03-02T13:57:20+00:00</updated>
    <author>
      <name>/u/Inevitable-Judge2642</name>
      <uri>https://old.reddit.com/user/Inevitable-Judge2642</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j1ree1/wasimancer_an_mcp_server_with_sse_transport/"&gt; &lt;img alt="WASImancer, an MCP server with SSE transport, powered by WebAssembly" src="https://external-preview.redd.it/1x8HVEC_omAWU9QM9VtDofQPOgkR7qQwNIfTXFBoRX4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=852f2ce1959ef1e202cbadd2d9dcfc833d460148" title="WASImancer, an MCP server with SSE transport, powered by WebAssembly" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Judge2642"&gt; /u/Inevitable-Judge2642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://k33g.hashnode.dev/wasimancer-an-mcp-server-with-sse-transport-powered-by-webassembly"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1ree1/wasimancer_an_mcp_server_with_sse_transport/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1ree1/wasimancer_an_mcp_server_with_sse_transport/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T13:57:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1h4mz</id>
    <title>What do you actually use local LLM's for?</title>
    <updated>2025-03-02T03:04:07+00:00</updated>
    <author>
      <name>/u/ivkemilioner</name>
      <uri>https://old.reddit.com/user/ivkemilioner</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivkemilioner"&gt; /u/ivkemilioner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1h4mz/what_do_you_actually_use_local_llms_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1h4mz/what_do_you_actually_use_local_llms_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1h4mz/what_do_you_actually_use_local_llms_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T03:04:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j24poa</id>
    <title>I did some poking, but didn't see a lot of info. Ollama and graphics.</title>
    <updated>2025-03-02T23:28:02+00:00</updated>
    <author>
      <name>/u/Ravenseye</name>
      <uri>https://old.reddit.com/user/Ravenseye</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a pipeline for getting image generation llms to work under the ollama umbrella?&lt;/p&gt; &lt;p&gt;Can they be run offline as well?&lt;/p&gt; &lt;p&gt;Thank you in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ravenseye"&gt; /u/Ravenseye &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j24poa/i_did_some_poking_but_didnt_see_a_lot_of_info/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j24poa/i_did_some_poking_but_didnt_see_a_lot_of_info/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j24poa/i_did_some_poking_but_didnt_see_a_lot_of_info/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T23:28:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1e5k8</id>
    <title>For Mac users, Ollama is getting MLX support!</title>
    <updated>2025-03-02T00:29:13+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama has officially started work on MLX support! For those who don't know, this is huge for anyone running models locally on their Mac. MLX is designed to fully utilize Apple's unified memory and GPU. Expect faster, more efficient LLM training, execution and inference speeds.&lt;/p&gt; &lt;p&gt;You can watch the progress here:&lt;br /&gt; &lt;a href="https://github.com/ollama/ollama/pull/9118"&gt;https://github.com/ollama/ollama/pull/9118&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Development is still early but you can now pull it down and run it yourself by running the following (as mentioned in the PR)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cmake -S . -B build cmake --build build -j go build . OLLAMA_NEW_ENGINE=1 OLLAMA_BACKEND=mlx ollama serve &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let me know your thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1e5k8/for_mac_users_ollama_is_getting_mlx_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j1e5k8/for_mac_users_ollama_is_getting_mlx_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j1e5k8/for_mac_users_ollama_is_getting_mlx_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T00:29:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2cc8o</id>
    <title>Small Models for android</title>
    <updated>2025-03-03T06:20:43+00:00</updated>
    <author>
      <name>/u/Loveandfucklife</name>
      <uri>https://old.reddit.com/user/Loveandfucklife</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can someone suggest &amp;lt;6GB or &amp;lt;8GB models to run on android ?&lt;/p&gt; &lt;p&gt;Condition - 1. For general purpose QnA or Infobased 2. Knowledge cut of date near 2024 3. Unfiltered or Uncensored&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loveandfucklife"&gt; /u/Loveandfucklife &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2cc8o/small_models_for_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2cc8o/small_models_for_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2cc8o/small_models_for_android/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T06:20:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2fygi</id>
    <title>Accessing an LLM Across the home network</title>
    <updated>2025-03-03T10:45:11+00:00</updated>
    <author>
      <name>/u/Birdinhandandbush</name>
      <uri>https://old.reddit.com/user/Birdinhandandbush</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sure this is obvious to some, but wondering what I need to do myself. &lt;/p&gt; &lt;p&gt;If I have an LLM running on my home system, running the Ollama Serve command, can I access the local LLM on my tablet in another room for example. &lt;/p&gt; &lt;p&gt;In the future I was hoping to have a desktop/server setup in one room with the LLM running, and that I could connect with my other laptop or tablets as needed. &lt;/p&gt; &lt;p&gt;Any advice or feedback appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Birdinhandandbush"&gt; /u/Birdinhandandbush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2fygi/accessing_an_llm_across_the_home_network/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2fygi/accessing_an_llm_across_the_home_network/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2fygi/accessing_an_llm_across_the_home_network/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T10:45:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2hw55</id>
    <title>I just downloaded cuda. How should I now be able to give ollama access to the power of my gpu?</title>
    <updated>2025-03-03T12:47:47+00:00</updated>
    <author>
      <name>/u/Dalar42</name>
      <uri>https://old.reddit.com/user/Dalar42</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dalar42"&gt; /u/Dalar42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2hw55/i_just_downloaded_cuda_how_should_i_now_be_able/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2hw55/i_just_downloaded_cuda_how_should_i_now_be_able/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2hw55/i_just_downloaded_cuda_how_should_i_now_be_able/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T12:47:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j20l5m</id>
    <title>ANY UPDATES ON THE APPLE SILICON (M1,M2,M3,M4) CRITICAL FLAW?</title>
    <updated>2025-03-02T20:29:57+00:00</updated>
    <author>
      <name>/u/fremenmuaddib</name>
      <uri>https://old.reddit.com/user/fremenmuaddib</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone have some news about this issue? I have 2 thunderbolt SSD drives connected to my MacMini M4 Pro 64GB, and this is still a huge source of troubles for me, with continuous and unpredictable resets of the machine while I'm using mlx models, as you can read here: &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/neobundy/Deep-Dive-Into-AI-With-MLX-PyTorch/blob/main/NOTES_ON_METAL_BUGS.md"&gt;NOTES ON METAL BUGS by neobundy&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Neobundy is a smart Korean guy who wrote 3 technical books on MLX, hundreds of web articles and tutorials, and even developed two stable diffusion apps that use different SD models on apple silicon. He was one of the most prominent supporter of the architecture, but after discovering and reporting the critical issue with the M chips, Apple ignored his requests for an entire year, until he finally announced his decision to abandon any R&amp;amp;D work on the Apple Silicon since he now believes that Apple does not have any plan to address the issue.&lt;/p&gt; &lt;p&gt;I don't understand. Is Apple going to admit the design flaws in the M processors and start working on a software fix or on a improved hardware architecture?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fremenmuaddib"&gt; /u/fremenmuaddib &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j20l5m/any_updates_on_the_apple_silicon_m1m2m3m4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j20l5m/any_updates_on_the_apple_silicon_m1m2m3m4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j20l5m/any_updates_on_the_apple_silicon_m1m2m3m4/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-02T20:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2kz8u</id>
    <title>Ollama models for translation</title>
    <updated>2025-03-03T15:19:02+00:00</updated>
    <author>
      <name>/u/Zalupik98</name>
      <uri>https://old.reddit.com/user/Zalupik98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was trying using DeepSeek model to translate English text to Norwegian but it works terribly, is there any models which would work better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zalupik98"&gt; /u/Zalupik98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2kz8u/ollama_models_for_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2kz8u/ollama_models_for_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2kz8u/ollama_models_for_translation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T15:19:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2i2bv</id>
    <title>Best open source models to try out different RAG techniques.</title>
    <updated>2025-03-03T12:57:14+00:00</updated>
    <author>
      <name>/u/Superb_Practice_4544</name>
      <uri>https://old.reddit.com/user/Superb_Practice_4544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hii all, hope you guys are doing great. Recently I am learning about RAGs and want to try out different RAG techniques and their differences.&lt;/p&gt; &lt;p&gt;Which 7b parameter model works best for RAG use case ? Also need suggestion on best Open source embedding models. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Superb_Practice_4544"&gt; /u/Superb_Practice_4544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2i2bv/best_open_source_models_to_try_out_different_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2i2bv/best_open_source_models_to_try_out_different_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2i2bv/best_open_source_models_to_try_out_different_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T12:57:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2ibxv</id>
    <title>Chat with my own PDF documents</title>
    <updated>2025-03-03T13:11:14+00:00</updated>
    <author>
      <name>/u/9elpi8</name>
      <uri>https://old.reddit.com/user/9elpi8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, as title says I would like to chat with my PDF documents. Which model would you recommend me to use? Best would be with multilanguage support. I have Nvidia 4060Ti 16GB.&lt;/p&gt; &lt;p&gt;My idea is make several threads inside AnythingLLM where I would have my receipts in other thread books related to engineering or some other learning stuff.&lt;/p&gt; &lt;p&gt;Thank you for your recommendation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/9elpi8"&gt; /u/9elpi8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2ibxv/chat_with_my_own_pdf_documents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2ibxv/chat_with_my_own_pdf_documents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2ibxv/chat_with_my_own_pdf_documents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T13:11:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j26bbr</id>
    <title>Impressed with how well Ollama runs on the RasPi, this is Granite3.1 MoE</title>
    <updated>2025-03-03T00:45:13+00:00</updated>
    <author>
      <name>/u/RasPiBuilder</name>
      <uri>https://old.reddit.com/user/RasPiBuilder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j26bbr/impressed_with_how_well_ollama_runs_on_the_raspi/"&gt; &lt;img alt="Impressed with how well Ollama runs on the RasPi, this is Granite3.1 MoE" src="https://preview.redd.it/u1578jgzfdme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70291d2ca9084c471397d78898f453ae38d57293" title="Impressed with how well Ollama runs on the RasPi, this is Granite3.1 MoE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RasPiBuilder"&gt; /u/RasPiBuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u1578jgzfdme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j26bbr/impressed_with_how_well_ollama_runs_on_the_raspi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j26bbr/impressed_with_how_well_ollama_runs_on_the_raspi/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T00:45:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2j8nt</id>
    <title>I open-sourced Klee today, an Ollama GUI designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities.</title>
    <updated>2025-03-03T13:58:46+00:00</updated>
    <author>
      <name>/u/w-zhong</name>
      <uri>https://old.reddit.com/user/w-zhong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j2j8nt/i_opensourced_klee_today_an_ollama_gui_designed/"&gt; &lt;img alt="I open-sourced Klee today, an Ollama GUI designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." src="https://preview.redd.it/e9n94wxjdhme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cea4c13efc5bd2a090237ef7c3fe7065ceb8f0d9" title="I open-sourced Klee today, an Ollama GUI designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w-zhong"&gt; /u/w-zhong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e9n94wxjdhme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j2j8nt/i_opensourced_klee_today_an_ollama_gui_designed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j2j8nt/i_opensourced_klee_today_an_ollama_gui_designed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-03T13:58:46+00:00</published>
  </entry>
</feed>
