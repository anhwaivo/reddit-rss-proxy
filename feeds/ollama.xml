<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-29T04:25:10+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1n1cn1m</id>
    <title>FREE Local AI Meeting Note-Taker - Hyprnote - Obsidian - Ollama</title>
    <updated>2025-08-27T09:47:54+00:00</updated>
    <author>
      <name>/u/NoobMLDude</name>
      <uri>https://old.reddit.com/user/NoobMLDude</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoobMLDude"&gt; /u/NoobMLDude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1n0zcx2/free_local_ai_meeting_notetaker_hyprnote_obsidian/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1cn1m/free_local_ai_meeting_notetaker_hyprnote_obsidian/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1cn1m/free_local_ai_meeting_notetaker_hyprnote_obsidian/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T09:47:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1dap9</id>
    <title>I get "request timed out after 60 seconds" in vs code for ollama</title>
    <updated>2025-08-27T10:26:13+00:00</updated>
    <author>
      <name>/u/Aggressive_Mix_4258</name>
      <uri>https://old.reddit.com/user/Aggressive_Mix_4258</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n1dap9/i_get_request_timed_out_after_60_seconds_in_vs/"&gt; &lt;img alt="I get &amp;quot;request timed out after 60 seconds&amp;quot; in vs code for ollama" src="https://a.thumbs.redditmedia.com/P0rDQj51g3s7qBZ4Kt9OaflTMujf3qzv_CAGeyVssZ0.jpg" title="I get &amp;quot;request timed out after 60 seconds&amp;quot; in vs code for ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys, I have installed ollama and vs code and then installed Cline and Continue. Ollama is working very well but when I try to use it in Cline or Continue, I get &amp;quot;request timed out after 60 seconds&amp;quot; error in Cline and an error as you can see in the screenshot. Everything is done as these videos: &lt;a href="https://www.youtube.com/watch?v=aM0sS5TIaVI"&gt;https://www.youtube.com/watch?v=aM0sS5TIaVI&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=P5YXTTS8OFk"&gt;https://www.youtube.com/watch?v=P5YXTTS8OFk&lt;/a&gt; Then why doesn't it work for me? please keep in mind that I can use &lt;a href="http://openrouter.ai"&gt;openrouter.ai&lt;/a&gt; services via API key and without any problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aggressive_Mix_4258"&gt; /u/Aggressive_Mix_4258 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n1dap9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1dap9/i_get_request_timed_out_after_60_seconds_in_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1dap9/i_get_request_timed_out_after_60_seconds_in_vs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T10:26:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1n189ws</id>
    <title>Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler</title>
    <updated>2025-08-27T05:07:34+00:00</updated>
    <author>
      <name>/u/Fluid-Engineering769</name>
      <uri>https://old.reddit.com/user/Fluid-Engineering769</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n189ws/websitecrawler_extract_data_from_websites_in_llm/"&gt; &lt;img alt="Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler" src="https://external-preview.redd.it/okdIR3Y4TwJ-9MFnT63LQUQFrkGy2DIAhPYXAsFmGGA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f2cd37c695d3768b0d4a384bc1e0b870ed8b00c" title="Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluid-Engineering769"&gt; /u/Fluid-Engineering769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/pc8544/Website-Crawler"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n189ws/websitecrawler_extract_data_from_websites_in_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n189ws/websitecrawler_extract_data_from_websites_in_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T05:07:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1b74d</id>
    <title>Ollama app parameters?</title>
    <updated>2025-08-27T08:12:54+00:00</updated>
    <author>
      <name>/u/Dylan31245</name>
      <uri>https://old.reddit.com/user/Dylan31245</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i installed the ollama app and installed qwen3:8b. While the model runs theres a lot of repetition and it tends to think infinitely. Whenever i go to settings however, the only visible option is context size. I like the app more than running in terminal, so is there any way to change the parameters in the app? Sorry if this is in documentations! OS is windows 10.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dylan31245"&gt; /u/Dylan31245 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1b74d/ollama_app_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1b74d/ollama_app_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1b74d/ollama_app_parameters/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T08:12:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1hh1o</id>
    <title>Quadro K2200 4g with Gemma3 (3.3G)</title>
    <updated>2025-08-27T13:45:09+00:00</updated>
    <author>
      <name>/u/Mother-Ad4153</name>
      <uri>https://old.reddit.com/user/Mother-Ad4153</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;Is it okay to run Gemma3 (3.3G) on Quadro K2200 4g?&lt;/p&gt; &lt;p&gt;I've asked Gemini. It told me it's not okay.&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mother-Ad4153"&gt; /u/Mother-Ad4153 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1hh1o/quadro_k2200_4g_with_gemma3_33g/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1hh1o/quadro_k2200_4g_with_gemma3_33g/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1hh1o/quadro_k2200_4g_with_gemma3_33g/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T13:45:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1nuyw</id>
    <title>Ollama loads model always to CPU when called from application</title>
    <updated>2025-08-27T17:45:11+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have nvidia GPU 32GB vram and Ubuntu 24.04 which runs inside a VM.&lt;br /&gt; When the VM is rebooted and a app calls ollama, it load gemma3 12b to CPU.&lt;br /&gt; When the VM is rebooted, and I write in command line: Ollama run...the model is loaded to GPU.&lt;br /&gt; Whats the issue? User permissions etc? Why there are no clear instructions how to set the environment in the ollama.service? &lt;/p&gt; &lt;p&gt;&lt;code&gt;[Service]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Environment=&amp;quot;OLLAMA_HOST=0.0.0.0:11434&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Environment=&amp;quot;OLLAMA_KEEP_ALIVE=2200&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Environment=&amp;quot;OLLAMA_MAX_LOADED_MODELS=2&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Environment=&amp;quot;OLLAMA_NUM_PARALLEL=2&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Environment=&amp;quot;OLLAMA_MAX_QUEUE=512&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1nuyw/ollama_loads_model_always_to_cpu_when_called_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1nuyw/ollama_loads_model_always_to_cpu_when_called_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1nuyw/ollama_loads_model_always_to_cpu_when_called_from/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T17:45:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1hyr5</id>
    <title>Building a local Ai PC</title>
    <updated>2025-08-27T14:04:19+00:00</updated>
    <author>
      <name>/u/KCCarpenter5739</name>
      <uri>https://old.reddit.com/user/KCCarpenter5739</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Advice needed: I’m looking at micro center, building my own pc. I’m thinking of using Ryzen 9 cpu, Msi pro x870e-p wifi mobo, Corsair 32gb ram (128gb total), Samsung pro 4 tb nvme, liquid cooling aio, 1300w Psu, LIAN li O11D XL case.&lt;/p&gt; &lt;p&gt;GPU is where I’m getting stuck, the mobo has 3 slots (yes I know the secondary slots are bottlenecked), I’m thinking of running a 5060 TI 16gb primary, 3060 rtx for offloading and my old 1070ti for offloading more. Is this a good setup? Am I completely wrong? Never built custom before&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KCCarpenter5739"&gt; /u/KCCarpenter5739 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1hyr5/building_a_local_ai_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1hyr5/building_a_local_ai_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1hyr5/building_a_local_ai_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T14:04:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1n18h8q</id>
    <title>GPT-OSS Web Search</title>
    <updated>2025-08-27T05:19:42+00:00</updated>
    <author>
      <name>/u/No-Engineering3583</name>
      <uri>https://old.reddit.com/user/No-Engineering3583</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The updates and blog posts about gpt-oss support and Ollama v0.11 mention web search support: &amp;quot;Ollama is providing a built-in web search that can be optionally enabled to augment the model with the latest information&amp;quot;&lt;/p&gt; &lt;p&gt;How is this being provided? How is it enabled/disabled? Is it only in the Ollama app or is it available when using the CLI or python libraries to access the model hosted on a local Ollama instance?&lt;/p&gt; &lt;p&gt;EDIT for clarity: I am aware there are other ways to do this, I've even coded personal solutions. My inquiry is about how a feature they semi-announced works, if it is available, and how to use it. I would like to be able to compare it against other solutions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Engineering3583"&gt; /u/No-Engineering3583 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n18h8q/gptoss_web_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n18h8q/gptoss_web_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n18h8q/gptoss_web_search/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T05:19:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1mg7v</id>
    <title>Tool calls keep ending up as responses</title>
    <updated>2025-08-27T16:53:07+00:00</updated>
    <author>
      <name>/u/thewiirocks</name>
      <uri>https://old.reddit.com/user/thewiirocks</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n1mg7v/tool_calls_keep_ending_up_as_responses/"&gt; &lt;img alt="Tool calls keep ending up as responses" src="https://preview.redd.it/rtfarn7pcllf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=228e5a9c14dd8871eb8431600e3a1762b39ccd05" title="Tool calls keep ending up as responses" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've given llama3.2 a tool to run reports using an OLAP schema. When the LLM triggers the tool call, everything works well. The problem I'm having is that the tool call is often ending up as a regular response rather than a tool call. &lt;/p&gt; &lt;p&gt;Here is the exact response text:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;model&amp;quot;: &amp;quot;llama3.2&amp;quot;, &amp;quot;created_at&amp;quot;: &amp;quot;2025-08-27T16:48:54.552815Z&amp;quot;, &amp;quot;message&amp;quot;: { &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;{\&amp;quot;name\&amp;quot;: \&amp;quot;generateReport\&amp;quot;, \&amp;quot;parameters\&amp;quot;: {\&amp;quot;arg0\&amp;quot;: \&amp;quot;[\\\&amp;quot;Franchise Name\\\&amp;quot;, \\\&amp;quot;Product Name\\\&amp;quot;]\&amp;quot;, \&amp;quot;arg1\&amp;quot;: \&amp;quot;[\\\&amp;quot;Units Sold\\\&amp;quot;, \\\&amp;quot;Total Sale \\$\\\&amp;quot;]\&amp;quot;}}&amp;quot; }, &amp;quot;done&amp;quot;: false } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is becoming a huge frustration to reliable operation. I could try and intercept these situations, but that feels like a bit of a hack. (Which I supposed describes a lot of LLM interactions. 😅)&lt;/p&gt; &lt;p&gt;Does anyone know why this is happening and how to resolve? Or do you just intercept the call yourself?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thewiirocks"&gt; /u/thewiirocks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rtfarn7pcllf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1mg7v/tool_calls_keep_ending_up_as_responses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1mg7v/tool_calls_keep_ending_up_as_responses/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T16:53:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1yfeu</id>
    <title>Questions about Agents</title>
    <updated>2025-08-28T00:50:13+00:00</updated>
    <author>
      <name>/u/Street_Equivalent_45</name>
      <uri>https://old.reddit.com/user/Street_Equivalent_45</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Fellow ai experts. &lt;/p&gt; &lt;p&gt;I am currently making agent using Ollama in local agent with langchains Because of costs😂 Is there anyways to make agent better not using chatgpt or claudes or having no coat issues? I know maybe impossible but I really know what you guys think&lt;/p&gt; &lt;p&gt;Thanks for reading my comments&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Street_Equivalent_45"&gt; /u/Street_Equivalent_45 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1yfeu/questions_about_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1yfeu/questions_about_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1yfeu/questions_about_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T00:50:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1rc2y</id>
    <title>llama.ui - minimal, privacy focused chat interface</title>
    <updated>2025-08-27T19:55:53+00:00</updated>
    <author>
      <name>/u/COBECT</name>
      <uri>https://old.reddit.com/user/COBECT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n1rc2y/llamaui_minimal_privacy_focused_chat_interface/"&gt; &lt;img alt="llama.ui - minimal, privacy focused chat interface" src="https://preview.redd.it/6g2icqwi96lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93145b5e6ac2c5f127d14e540cb4261819454a6b" title="llama.ui - minimal, privacy focused chat interface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/COBECT"&gt; /u/COBECT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6g2icqwi96lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1rc2y/llamaui_minimal_privacy_focused_chat_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1rc2y/llamaui_minimal_privacy_focused_chat_interface/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T19:55:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n25rhd</id>
    <title>Ollama help!</title>
    <updated>2025-08-28T07:27:46+00:00</updated>
    <author>
      <name>/u/IndependentBug490</name>
      <uri>https://old.reddit.com/user/IndependentBug490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n25rhd/ollama_help/"&gt; &lt;img alt="Ollama help!" src="https://b.thumbs.redditmedia.com/PwS1yOmD3a8ondnY40EzM0_SuNAdB78gHX0Oa5mdvbw.jpg" title="Ollama help!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im making a workflow in n8n using the ollama chat model, i used it as an alternative to google gemini chat model. But it keeps in erroring and the output is fetch failed. Im self host, and im only using chatgpt to help me. im a totally beginner when it comes to n8n, hopefully anyone can help me with this. thank you. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ypp0m2h1qplf1.png?width=1382&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9f53630bf8fcbcb53f4ac0f979039b75f9d1196c"&gt;https://preview.redd.it/ypp0m2h1qplf1.png?width=1382&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9f53630bf8fcbcb53f4ac0f979039b75f9d1196c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IndependentBug490"&gt; /u/IndependentBug490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n25rhd/ollama_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n25rhd/ollama_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n25rhd/ollama_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T07:27:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1xjnb</id>
    <title>Agentic: Your 3B local model becomes a thoughtful research partner.</title>
    <updated>2025-08-28T00:09:22+00:00</updated>
    <author>
      <name>/u/Thin_Beat_9072</name>
      <uri>https://old.reddit.com/user/Thin_Beat_9072</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n1xjnb/agentic_your_3b_local_model_becomes_a_thoughtful/"&gt; &lt;img alt="Agentic: Your 3B local model becomes a thoughtful research partner." src="https://preview.redd.it/915rf37hpmlf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=7d625aa9b27cf3e56d522a230d24e40702faf46e" title="Agentic: Your 3B local model becomes a thoughtful research partner." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thin_Beat_9072"&gt; /u/Thin_Beat_9072 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/915rf37hpmlf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1xjnb/agentic_your_3b_local_model_becomes_a_thoughtful/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1xjnb/agentic_your_3b_local_model_becomes_a_thoughtful/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T00:09:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n27qcn</id>
    <title>Qwen3 rbit rl finetuned for stromger reasoning</title>
    <updated>2025-08-28T09:37:04+00:00</updated>
    <author>
      <name>/u/adeelahmadch</name>
      <uri>https://old.reddit.com/user/adeelahmadch</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adeelahmadch"&gt; /u/adeelahmadch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1n27p5g/qwen3_rbit_rl_finetuned_for_stromger_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n27qcn/qwen3_rbit_rl_finetuned_for_stromger_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n27qcn/qwen3_rbit_rl_finetuned_for_stromger_reasoning/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T09:37:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1m9fk</id>
    <title>Pair a vision grounding model with a reasoning LLM with Cua</title>
    <updated>2025-08-27T16:46:06+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n1m9fk/pair_a_vision_grounding_model_with_a_reasoning/"&gt; &lt;img alt="Pair a vision grounding model with a reasoning LLM with Cua" src="https://external-preview.redd.it/dzUycDIzdHFjbGxmMbzuqtV-zsPSC2s-Lu_18m-UGy8cX2XwaXvrFiOhDTxh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d3f7d72bb82103be882d2f660f7910185af8eb8" title="Pair a vision grounding model with a reasoning LLM with Cua" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cua just shipped v0.4 of the Cua Agent framework with Composite Agents - you can now pair a vision/grounding model with a reasoning LLM using a simple modelA+modelB syntax. Best clicks + best plans.&lt;/p&gt; &lt;p&gt;The problem: every GUI model speaks a different dialect. • some want pixel coordinates • others want percentages • a few spit out cursed tokens like &amp;lt;|loc095|&amp;gt;&lt;/p&gt; &lt;p&gt;We built a universal interface that works the same across Anthropic, OpenAI, Hugging Face, etc.:&lt;/p&gt; &lt;p&gt;agent = ComputerAgent( model=&amp;quot;anthropic/claude-3-5-sonnet-20241022&amp;quot;, tools=[computer] )&lt;/p&gt; &lt;p&gt;But here’s the fun part: you can combine models by specialization. Grounding model (sees + clicks) + Planning model (reasons + decides) →&lt;/p&gt; &lt;p&gt;agent = ComputerAgent( model=&amp;quot;huggingface-local/HelloKKMe/GTA1-7B+openai/gpt-4o&amp;quot;, tools=[computer] )&lt;/p&gt; &lt;p&gt;This gives GUI skills to models that were never built for computer use. One handles the eyes/hands, the other the brain. Think driver + navigator working together.&lt;/p&gt; &lt;p&gt;Two specialists beat one generalist. We’ve got a ready-to-run notebook demo - curious what combos you all will try.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog : &lt;a href="https://www.trycua.com/blog/composite-agents"&gt;https://www.trycua.com/blog/composite-agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7io8lg1rcllf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1m9fk/pair_a_vision_grounding_model_with_a_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1m9fk/pair_a_vision_grounding_model_with_a_reasoning/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T16:46:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2exp5</id>
    <title>Ollama having server timeout issues (on Mac), need help</title>
    <updated>2025-08-28T15:11:59+00:00</updated>
    <author>
      <name>/u/solarsflare</name>
      <uri>https://old.reddit.com/user/solarsflare</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My Ollama isn't starting, it just shows a blank screen. I checked through the terminal and it keeps server timing out. I tried an alternative way to get it to run by directly making Ollama run through the terminal, but whenever i tried to select a model and send a message, it wouldn't do anything. It'd continue to time out. I tried restarting and re-installing Ollama multiple times. I even restarted my Macbook. It's an M4 with plenty of RAM and space. Currently on the 26 beta. Does anyone know how to fix this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/solarsflare"&gt; /u/solarsflare &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2exp5/ollama_having_server_timeout_issues_on_mac_need/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2exp5/ollama_having_server_timeout_issues_on_mac_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n2exp5/ollama_having_server_timeout_issues_on_mac_need/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T15:11:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2eqc1</id>
    <title>Coquette Mobile - Android App, Ollama with Agentic Properties - desktop control.</title>
    <updated>2025-08-28T15:04:29+00:00</updated>
    <author>
      <name>/u/Fimeg</name>
      <uri>https://old.reddit.com/user/Fimeg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n2eqc1/coquette_mobile_android_app_ollama_with_agentic/"&gt; &lt;img alt="Coquette Mobile - Android App, Ollama with Agentic Properties - desktop control." src="https://b.thumbs.redditmedia.com/5JVeogvyCM6U8FWg9rpAnrZ9CgUGteXfiawa1DSs9Do.jpg" title="Coquette Mobile - Android App, Ollama with Agentic Properties - desktop control." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am quite hesitant to tell anyone about my projects for fear of having to work on them xD&lt;/p&gt; &lt;p&gt;&lt;a href="https://GitHub.com/Fimeg/CoquetteMobile"&gt;https://GitHub.com/Fimeg/CoquetteMobile&lt;/a&gt; is agentic AI on android. I built this running on a GTX 1070ti, Jan 4b model, and a Pixel 3. I would love to do this post a bit more justice but I am on mobile today. I see so many of you are creating amazing agentic desktop tools - and I want them EVERYWHERE.&lt;/p&gt; &lt;p&gt;This is just a proof of concept. I want more people working on this for FREE for us all... Fork it, give suggestions. Data Sovereignty is key for our futures.&lt;/p&gt; &lt;p&gt;Extracted from Github: ⚠️ DEVELOPMENT SOFTWARE - HIGHLY EXPERIMENTAL ⚠️&lt;/p&gt; &lt;p&gt;Your data. Your models. Your control.&lt;/p&gt; &lt;p&gt;Transform mobile AI from a black box into a transparent, user-controlled system. This isn't just another AI app - it's a demonstration of how technology should work: empowering users without extracting their data, putting complete control in your hands, and maintaining full transparency in every operation.&lt;/p&gt; &lt;p&gt;No data harvesting. No cloud dependencies. No hidden algorithms. Just pure, transparent AI assistance that you own and control. Just a privacy-first Android AI assistant built on principles of data sovereignty and technological autonomy. Features complete operational transparency - because you deserve to understand exactly how your AI assistant works.&lt;/p&gt; &lt;p&gt;🚧 Current Status: Active Development&lt;/p&gt; &lt;pre&gt;&lt;code&gt;This software is in early development and contains bugs Features may not work as expected Updates will be frequent and may introduce breaking changes Use at your own risk - not ready for production use &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;🔐 SECURITY WARNING: HID Device Control&lt;/p&gt; &lt;pre&gt;&lt;code&gt;This app can inject keyboard/mouse commands into connected computers Supports DuckyScript and similar automation protocols Can execute arbitrary commands on target systems Use only on systems you own or have explicit permission to control Malicious use is prohibited - for legitimate automation only &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fimeg"&gt; /u/Fimeg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n2eqc1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2eqc1/coquette_mobile_android_app_ollama_with_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n2eqc1/coquette_mobile_android_app_ollama_with_agentic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T15:04:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2f3mp</id>
    <title>Evaluate any computer-use agent with HUD + OSWorld-Verified</title>
    <updated>2025-08-28T15:17:50+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We integrated Cua with HUD so you can run OSWorld-Verified and other computer-/browser-use benchmarks at scale.&lt;/p&gt; &lt;p&gt;Different runners and logs made results hard to compare. Cua × HUD gives you a consistent runner, reliable traces, and comparable metrics across setups.&lt;/p&gt; &lt;p&gt;Bring your stack (OpenAI, Anthropic, Hugging Face) — or Composite Agents (grounder + planner) from Day 3. Pick the dataset and keep the same workflow.&lt;/p&gt; &lt;p&gt;See the notebook for the code: run OSWorld-Verified (~369 tasks) by XLang Labs to benchmark on real desktop apps (Chrome, LibreOffice, VS Code, GIMP).&lt;/p&gt; &lt;p&gt;Heading to Hack the North? Enter our on-site computer-use agent track — the top OSWorld-Verified score earns a guaranteed interview with a YC partner in the next batch.&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://www.trycua.com/blog/hud-agent-evals"&gt;https://www.trycua.com/blog/hud-agent-evals&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs: &lt;a href="https://docs.trycua.com/docs/agent-sdk/integrations/hud"&gt;https://docs.trycua.com/docs/agent-sdk/integrations/hud&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Notebook: &lt;a href="https://github.com/trycua/cua/blob/main/notebooks/eval_osworld.ipynb"&gt;https://github.com/trycua/cua/blob/main/notebooks/eval_osworld.ipynb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2f3mp/evaluate_any_computeruse_agent_with_hud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2f3mp/evaluate_any_computeruse_agent_with_hud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n2f3mp/evaluate_any_computeruse_agent_with_hud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T15:17:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1tfj4</id>
    <title>Ollama + PostgreSQL: Your Local LLM Can Now Query Production Databases</title>
    <updated>2025-08-27T21:17:07+00:00</updated>
    <author>
      <name>/u/Sea-Assignment6371</name>
      <uri>https://old.reddit.com/user/Sea-Assignment6371</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n1tfj4/ollama_postgresql_your_local_llm_can_now_query/"&gt; &lt;img alt="Ollama + PostgreSQL: Your Local LLM Can Now Query Production Databases" src="https://external-preview.redd.it/d2huZzF2ZHZvbWxmMblmm4Un8wPzClWvKIqbCd-O0tnIDr4HTaJ8aTVXM2nL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fdb7439b91fc4e3bf69069c2c0e5c06b049adfcd" title="Ollama + PostgreSQL: Your Local LLM Can Now Query Production Databases" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt;! Quick update - DataKit now lets you query PostgreSQL databases with Ollama's help.&lt;/p&gt; &lt;p&gt;Well the best part: Your data/schema NEVER goes to OpenAI/Claude. Your local LLM generates the SQL just by looking at the schema of the file.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What this enables:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;• &amp;quot;Show me all users who signed up last month but haven't made a purchase&amp;quot;&lt;/p&gt; &lt;p&gt;• &amp;quot;Find orders with unusual patterns&amp;quot;&lt;/p&gt; &lt;p&gt;• &amp;quot;Generate a cohort analysis query&amp;quot;&lt;/p&gt; &lt;p&gt;All happens locally. Ollama writes the SQL, DuckDB executes it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Run: `OLLAMA_ORIGINS=&amp;quot;&lt;a href="https://datakit.page"&gt;https://datakit.page&lt;/a&gt;&amp;quot; ollama serve`&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Connect your PostgreSQL&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Ask questions in plain English&lt;/p&gt; &lt;p&gt;Try it at &lt;a href="http://datakit.page"&gt;datakit.page&lt;/a&gt; - would love feedback on what models work best for SQL generation!&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Assignment6371"&gt; /u/Sea-Assignment6371 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9qyoordvomlf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1tfj4/ollama_postgresql_your_local_llm_can_now_query/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1tfj4/ollama_postgresql_your_local_llm_can_now_query/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T21:17:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1v1uw</id>
    <title>Just released version 1.4 of Nanocoder built in Ink - such an epic framework for CLI applications!</title>
    <updated>2025-08-27T22:21:00+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n1v1uw/just_released_version_14_of_nanocoder_built_in/"&gt; &lt;img alt="Just released version 1.4 of Nanocoder built in Ink - such an epic framework for CLI applications!" src="https://preview.redd.it/vkqp6dgi0nlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7c9616bc2e60e27283ed7ab441d6f9aa60448d1" title="Just released version 1.4 of Nanocoder built in Ink - such an epic framework for CLI applications!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don’t know why I didn’t build the previous versions of Nanocoder from the start in Ink, it has been so powerful in building a beautiful next-gen version of my open source coding agent.&lt;/p&gt; &lt;p&gt;It helps create some incredible UIs around the terminal and is pretty much pick up and go if you’re already fluent in React. The only challenge has been getting to the UI to scale when you resize the terminal window - any tips let me know!&lt;/p&gt; &lt;p&gt;We’re almost on 100 stars on GitHub which I know is small but I really believe in the philosophies behind this small community! It would make my day to get it there!&lt;/p&gt; &lt;p&gt;All contributors and feedback welcome - people have been so amazing already! I’m trying to get people involved to build a piece of software that is owned and pushed by the community - not big tech companies! 😄&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Link&lt;/strong&gt;: &lt;a href="https://github.com/Mote-Software/nanocoder"&gt;https://github.com/Mote-Software/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discord Link to Get Involved&lt;/strong&gt;: &lt;a href="https://discord.gg/ktPDV6rekE"&gt;https://discord.gg/ktPDV6rekE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vkqp6dgi0nlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n1v1uw/just_released_version_14_of_nanocoder_built_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n1v1uw/just_released_version_14_of_nanocoder_built_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-27T22:21:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ugfs</id>
    <title>Spring AI Playground — Self-hosted Web UI with Ollama, RAG and MCP tools</title>
    <updated>2025-08-29T01:35:22+00:00</updated>
    <author>
      <name>/u/kr-jmlab</name>
      <uri>https://old.reddit.com/user/kr-jmlab</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n2ugfs/spring_ai_playground_selfhosted_web_ui_with/"&gt; &lt;img alt="Spring AI Playground — Self-hosted Web UI with Ollama, RAG and MCP tools" src="https://external-preview.redd.it/Tdp9Yr11guCxzcug45e2Jg_hVSbeQUV2eb5zstdOZ5Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cbdafe1ed4b7e420dcb8f2e46a256d6f18757dd0" title="Spring AI Playground — Self-hosted Web UI with Ollama, RAG and MCP tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built &lt;strong&gt;Spring AI Playground&lt;/strong&gt;, a self-hosted web UI for experimenting with &lt;strong&gt;Ollama, RAG workflows, and MCP tools&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Uses &lt;strong&gt;Ollama as the default backend&lt;/strong&gt; — no API keys needed&lt;/li&gt; &lt;li&gt;Upload docs → chunk → embed → run similarity search with vector DBs (Pinecone, Milvus, PGVector, Weaviate, etc.)&lt;/li&gt; &lt;li&gt;Visual &lt;strong&gt;MCP Playground&lt;/strong&gt;: connect tools via HTTP/STDIO/SSE, inspect metadata, tweak args, and call them from chat&lt;/li&gt; &lt;li&gt;Can also swap to OpenAI, Anthropic, Google, etc. if you want&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I built this because wiring up RAG + tool integrations in Java always felt slow and repetitive. Now I can spin things up quickly in a browser UI, fully local.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/JM-Lab/spring-ai-playground"&gt;https://github.com/JM-Lab/spring-ai-playground&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear how this community is using Ollama for RAG today, and what features you’d like to see added.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kr-jmlab"&gt; /u/kr-jmlab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/JM-Lab/spring-ai-playground"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2ugfs/spring_ai_playground_selfhosted_web_ui_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n2ugfs/spring_ai_playground_selfhosted_web_ui_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-29T01:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2f514</id>
    <title>Private AI by Proton</title>
    <updated>2025-08-28T15:19:18+00:00</updated>
    <author>
      <name>/u/naperwind</name>
      <uri>https://old.reddit.com/user/naperwind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n2f514/private_ai_by_proton/"&gt; &lt;img alt="Private AI by Proton" src="https://external-preview.redd.it/qwgh9izd63uNNKC7KnYNGMWh58yq5A1iL3wjdvP3PgM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2cd0508e37d8af974808c811adb21f6ed8bbd68c" title="Private AI by Proton" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anybody tried? Can it be put on Ollama? Thank you in advance for your thoughts. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/naperwind"&gt; /u/naperwind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://proton.me/blog/lumo-1-1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2f514/private_ai_by_proton/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n2f514/private_ai_by_proton/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T15:19:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2pspb</id>
    <title>ollama-lancache (like caching games for a lan party but models instead!)</title>
    <updated>2025-08-28T22:07:13+00:00</updated>
    <author>
      <name>/u/jjasghar</name>
      <uri>https://old.reddit.com/user/jjasghar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I'd like to announce I created &lt;code&gt;ollama-lancache&lt;/code&gt;. Basically, it's a way to share out the &amp;quot;blobs&amp;quot; for ollama models from one (laptop) computer to a bunch of attendee machines. &lt;/p&gt; &lt;p&gt;So, if you have a say conference with Wi-Fi, and it takes hours to download your models, you can use this app that sits beside your already downloaded models and will install them to the correct location for Windows/Mac/Linux. &lt;/p&gt; &lt;p&gt;There's even a &amp;quot;downloads&amp;quot; directory, so you can have specific versions of Ollama or any additional downloads for leveraging models.&lt;/p&gt; &lt;p&gt;Conference wifi has always been a problem. This is a small Go application that leverages something already on your laptop, and ideally will allow you to get your attendees to leverage your tech sooner rather than later.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jjasghar/ollama-lancache"&gt;https://github.com/jjasghar/ollama-lancache&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jjasghar"&gt; /u/jjasghar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2pspb/ollamalancache_like_caching_games_for_a_lan_party/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2pspb/ollamalancache_like_caching_games_for_a_lan_party/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n2pspb/ollamalancache_like_caching_games_for_a_lan_party/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T22:07:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2c5mw</id>
    <title>I’ve Debugged 100+ RAG/LLM Pipelines. These 16 Bugs Always Come Back. (70 days, 800 stars)</title>
    <updated>2025-08-28T13:25:01+00:00</updated>
    <author>
      <name>/u/onestardao</name>
      <uri>https://old.reddit.com/user/onestardao</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i used to think RAG was mostly “pick better embeddings, tune chunk size, choose a faster vector db.” then production happened.&lt;/p&gt; &lt;p&gt;what i thought&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;switch cosine to dot, increase chunk length, rerun.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;try another vector store and RPS goes up, so answers should improve.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;hybrid retrieval must be strictly better than a single retriever.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;what really happened&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;high similarity with wrong meaning. facts exist in the corpus but never surface.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;answers look right while citations silently drift to the wrong section.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;first call after deploy fails because secrets are not ready.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;hybrid sometimes performs worse than a single strong retriever with a clean contract.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;after 100+ pipelines across ollama stacks, the same patterns kept returning. none of this was random. they were structural failure modes. so i wrote them down as a Problem Map with 16 reproducible slots, each with a permanent fix. examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;No.5 embedding ≠ semantic. high similarity, wrong meaning.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No.8 retrieval traceability. answer looks fine, citations do not align to the exact offsets.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No.14 bootstrap ordering. first call after deploy crashes or uses stale env because infra is not warmed.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No.15 deployment deadlock. retriever or merge waits forever on an index that is still building.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;i shared the map and the community response was surprisingly strong. 70 days, 800 stars, and even the tesseract.js author starred it. more important than stars though, the map made bugs stop repeating. once a slot is fixed structurally, it stays fixed.&lt;/p&gt; &lt;p&gt;👉 Problem Map, 16 failure modes with fixes (link above)&lt;/p&gt; &lt;h2&gt;a concrete ollama workflow you can try in 60 seconds&lt;/h2&gt; &lt;p&gt;open a fresh ollama chat with your model. paste this diagnostic prompt as is:&lt;/p&gt; &lt;p&gt;You are a RAG pipeline auditor. Classify the current failure into the Problem Map slots (No.5 embedding≠semantic, No.8 retrieval traceability, No.14 bootstrap ordering, No.15 deployment deadlock, or other). Return a short JSON plan with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;slot&amp;quot;: &amp;quot;No.x&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;why&amp;quot;: one-sentence symptom match&lt;/li&gt; &lt;li&gt;&amp;quot;checks&amp;quot;: ordered steps I can run now&lt;/li&gt; &lt;li&gt;&amp;quot;fix&amp;quot;: the minimal structural change&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Rules:&lt;/p&gt; &lt;p&gt;1) enforce cite-then-explain. if citations or offsets are missing, fail fast and say &amp;quot;add traceability contract&amp;quot;.&lt;/p&gt; &lt;p&gt;2) if coverage &amp;lt; 0.70 or alignment is inconsistent across 3 paraphrases, flag &amp;quot;needs retriever repair&amp;quot;.&lt;/p&gt; &lt;p&gt;3) do not change my infra. propose guardrails I can add at the text and contract layer.&lt;/p&gt; &lt;p&gt;Keep it terse and auditable.&lt;/p&gt; &lt;p&gt;now ask your real question, or paste a failing trace. the model should classify into one of the slots and return a tiny, checkable plan.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;minimal guardrails you can add today&lt;/h2&gt; &lt;p&gt;acceptance targets&lt;/p&gt; &lt;ul&gt; &lt;li&gt;coverage for the target section ≥ 0.70&lt;/li&gt; &lt;li&gt;enforce cite then explain&lt;/li&gt; &lt;li&gt;stop on missing fields: snippet_id, section_id, source_url, offsets, tokens&lt;/li&gt; &lt;li&gt;flag instability if the answer flips across 3 paraphrases with identical inputs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;bootstrap fence&lt;/p&gt; &lt;ul&gt; &lt;li&gt;before any retrieval or generation, assert env and secrets are present. if not, short circuit with a wait and a capped retry counter. this prevents No.14.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;traceability contract&lt;/p&gt; &lt;ul&gt; &lt;li&gt;require snippet level ids and offsets. reject answers that cannot point back to the exact span. this prevents No.8 from hiding for weeks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;retriever sanity&lt;/p&gt; &lt;ul&gt; &lt;li&gt;verify the analyzer and normalization used to write the index matches the one used in retrieval. a mismatch often masquerades as No.5.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;single writer&lt;/p&gt; &lt;ul&gt; &lt;li&gt;queue or mutex all index writes. many “random” 500s are actually No.15 race conditions.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;why this matters to ollama users&lt;/h2&gt; &lt;p&gt;ollama gives you control and speed. the failure modes above sneak in precisely when you move fast. if you keep a short checklist and a common language for the bugs, you do not waste cycles arguing about tools. you fix the structure once, and it stays fixed.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;global fix map, work in progress&lt;/h2&gt; &lt;p&gt;problem map 1.0 is the foundation. i am now drafting a global fix map that spans ollama, langchain, llamaindex, qdrant, weaviate, milvus, and common automation stacks. same idea, one level broader. minimal recipes, clean guardrails, no infra rewrites.&lt;/p&gt; &lt;p&gt;what would you want included besides “common bugs + fixes”?&lt;/p&gt; &lt;p&gt;metrics you actually check, copy paste recipes, deployment checklists, or something else you wish you had before prod went live? ( will be launched soon) &lt;/p&gt; &lt;p&gt;🫡 Thank you in advance &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onestardao"&gt; /u/onestardao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/onestardao/WFGY/tree/main/ProblemMap/README.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2c5mw/ive_debugged_100_ragllm_pipelines_these_16_bugs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n2c5mw/ive_debugged_100_ragllm_pipelines_these_16_bugs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T13:25:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2omfl</id>
    <title>[Guide + Code] Fine-Tuning a Vision-Language Model on a Single GPU (Yes, With Code)</title>
    <updated>2025-08-28T21:19:23+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n2omfl/guide_code_finetuning_a_visionlanguage_model_on_a/"&gt; &lt;img alt="[Guide + Code] Fine-Tuning a Vision-Language Model on a Single GPU (Yes, With Code)" src="https://preview.redd.it/086kei6dutlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da7b7c3f635056e44d3dedb5a3e573006bf0f4bb" title="[Guide + Code] Fine-Tuning a Vision-Language Model on a Single GPU (Yes, With Code)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote a step-by-step guide (with code) on how to fine-tune SmolVLM-256M-Instruct using Hugging Face TRL + PEFT. It covers lazy dataset streaming (no OOM), LoRA/DoRA explained simply, ChartQA for verifiable evaluation, and how to deploy via vLLM. Runs fine on a single consumer GPU like a 3060/4070.&lt;/p&gt; &lt;p&gt;Guide: &lt;a href="https://pavankunchalapk.medium.com/the-definitive-guide-to-fine-tuning-a-vision-language-model-on-a-single-gpu-with-code-79f7aa914fc6?utm_source=chatgpt.com"&gt;https://pavankunchalapk.medium.com/the-definitive-guide-to-fine-tuning-a-vision-language-model-on-a-single-gpu-with-code-79f7aa914fc6&lt;/a&gt;&lt;br /&gt; Code: &lt;a href="https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/vllm-fine-tuning-smolvlm?utm_source=chatgpt.com"&gt;https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/vllm-fine-tuning-smolvlm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also — I’m open to roles! Hands-on with real-time pose estimation, LLMs, and deep learning architectures. Resume: &lt;a href="https://pavan-portfolio-tawny.vercel.app/"&gt;https://pavan-portfolio-tawny.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/086kei6dutlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n2omfl/guide_code_finetuning_a_visionlanguage_model_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n2omfl/guide_code_finetuning_a_visionlanguage_model_on_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-28T21:19:23+00:00</published>
  </entry>
</feed>
