<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-18T23:48:47+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jdd7k9</id>
    <title>I built a VM for AI agents supporting local models with Ollama</title>
    <updated>2025-03-17T13:55:49+00:00</updated>
    <author>
      <name>/u/sandropuppo</name>
      <uri>https://old.reddit.com/user/sandropuppo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jdd7k9/i_built_a_vm_for_ai_agents_supporting_local/"&gt; &lt;img alt="I built a VM for AI agents supporting local models with Ollama" src="https://external-preview.redd.it/4R-oKdOmCmX_i9_20c366T82clUbCUIJRVPgtgZpfGo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=510a70ca5c4eba0d1e8f90da819b81b800e0e4fe" title="I built a VM for AI agents supporting local models with Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandropuppo"&gt; /u/sandropuppo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/trycua/computer"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdd7k9/i_built_a_vm_for_ai_agents_supporting_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdd7k9/i_built_a_vm_for_ai_agents_supporting_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T13:55:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd40r2</id>
    <title>GPT 4.5 System Prompt Preamble</title>
    <updated>2025-03-17T03:55:48+00:00</updated>
    <author>
      <name>/u/foomanchu89</name>
      <uri>https://old.reddit.com/user/foomanchu89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jd40r2/gpt_45_system_prompt_preamble/"&gt; &lt;img alt="GPT 4.5 System Prompt Preamble" src="https://preview.redd.it/e84gtswqa6pe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a69962b66b61145ec819b5d83e10aa81d6a0b0a6" title="GPT 4.5 System Prompt Preamble" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty cool but buried in the docs&lt;/p&gt; &lt;p&gt;&lt;a href="https://platform.openai.com/docs/guides/prompt-engineering#tactic-use-delimiters-to-clearly-indicate-distinct-parts-of-the-input"&gt;https://platform.openai.com/docs/guides/prompt-engineering#tactic-use-delimiters-to-clearly-indicate-distinct-parts-of-the-input&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I bet it works on open source models too&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foomanchu89"&gt; /u/foomanchu89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e84gtswqa6pe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd40r2/gpt_45_system_prompt_preamble/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jd40r2/gpt_45_system_prompt_preamble/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T03:55:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdeuqb</id>
    <title>Vision support for the gemma-3-12b-it-GGUF:Q4_K_M of unsloth and lmstudio-community not working</title>
    <updated>2025-03-17T15:07:24+00:00</updated>
    <author>
      <name>/u/_digito</name>
      <uri>https://old.reddit.com/user/_digito</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I have been testing the gemma-3-12b-it-GGUF:Q4_K_M model with Ollama and Open-webui and when I tried to get the text from an image either with the unsloth and lmstudio-community versions I got an error on Ollama logs:&lt;/p&gt; &lt;p&gt;msg=&amp;quot;llm predict error: Failed to create new sequence: failed to process inputs: this model is missing data required for image input&amp;quot;&lt;/p&gt; &lt;p&gt;If I use the gemma3:12b from the Ollama repository it works as expected and it gives the text more or less as expected. I'm using the recommended configurations for the temperature = 1.0, top_k = 64, top_p = 0.95, min_p = 0.0 for all the models I tested. Also the context size was the same to all, 8192 token.&lt;/p&gt; &lt;p&gt;From the HF pages the information is that the models are image-text-to-text, so I expected them to work like the gemma3:12b from Ollama repository. Any ideas?&lt;/p&gt; &lt;p&gt;Thank you. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_digito"&gt; /u/_digito &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdeuqb/vision_support_for_the_gemma312bitggufq4_k_m_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdeuqb/vision_support_for_the_gemma312bitggufq4_k_m_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdeuqb/vision_support_for_the_gemma312bitggufq4_k_m_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T15:07:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdt4am</id>
    <title>Getting any model to avoid certain words</title>
    <updated>2025-03-18T01:02:31+00:00</updated>
    <author>
      <name>/u/jamboman_</name>
      <uri>https://old.reddit.com/user/jamboman_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can you help me?&lt;/p&gt; &lt;p&gt;I've tried several different methods, different interfaces (using msty right now), and several different models.&lt;/p&gt; &lt;p&gt;I want to have a list of words that I don't want to be used when a model is replying to me.&lt;/p&gt; &lt;p&gt;Have any of you had success with this? It's been a nightmare, and seems so simple compared to other things I've been able to do.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jamboman_"&gt; /u/jamboman_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdt4am/getting_any_model_to_avoid_certain_words/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdt4am/getting_any_model_to_avoid_certain_words/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdt4am/getting_any_model_to_avoid_certain_words/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T01:02:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdkevi</id>
    <title>Creating Gemma 3 from GGUF with mmproj not working.</title>
    <updated>2025-03-17T18:49:33+00:00</updated>
    <author>
      <name>/u/Admirable-Star7088</name>
      <uri>https://old.reddit.com/user/Admirable-Star7088</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;EDIT: Solved, read comment to this post.&lt;/h1&gt; &lt;p&gt;When I was going to download Gemma 3 for Ollama, I could not find a Q5_K_M version. This is my favorite quant because it's the smallest quant possible with no noticeable quality loss (in my experience).&lt;/p&gt; &lt;p&gt;So, instead of downloading, I was doing some quick research how to convert my own GGUF file (&lt;strong&gt;google_gemma-3-12b-it-Q5_K_M.gguf&lt;/strong&gt;) and my mmproj file (&lt;strong&gt;mmproj-google_gemma-3-12b-it-f32.gguf&lt;/strong&gt;) to a format that I can run in Ollama. (these GGUFs are downloaded from &lt;a href="https://huggingface.co/bartowski/google_gemma-3-12b-it-GGUF"&gt;Bartowski&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;After successfully converting, the model works fine at first and it responds to text, but when I send it an image and ask it to describe it, it won't respond. I assume there is some problem with the mmproj file? Here is my Modelfile:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;FROM ./google_gemma-3-12b-it-Q5_K_M.gguf FROM ./mmproj-google_gemma-3-12b-it-f32.gguf PARAMETER temperature 1 PARAMETER top_k 64 PARAMETER top_p 0.95 PARAMETER min_p 0.0 PARAMETER num_ctx 8192 PARAMETER stop &amp;quot;&amp;lt;end_of_turn&amp;gt;&amp;quot; TEMPLATE &amp;quot;&amp;quot;&amp;quot; {{- range $i, $_ := .Messages }} {{- $last := eq (len (slice $.Messages $i)) 1 }} {{- if or (eq .Role &amp;quot;user&amp;quot;) (eq .Role &amp;quot;system&amp;quot;) }}&amp;lt;start_of_turn&amp;gt;user {{ .Content }}&amp;lt;end_of_turn&amp;gt; {{ if $last }}&amp;lt;start_of_turn&amp;gt;model {{ end }} {{- else if eq .Role &amp;quot;assistant&amp;quot; }}&amp;lt;start_of_turn&amp;gt;model {{ .Content }}{{ if not $last }}&amp;lt;end_of_turn&amp;gt; {{ end }} {{- end }} {{- end }} &amp;quot;&amp;quot;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm an amateur with Ollama, I have probably just made a silly mistake or missed some step. Thanks in advance to anyone who can help out!&lt;/p&gt; &lt;p&gt;p.s, I'm using Open WebUI as front-end.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable-Star7088"&gt; /u/Admirable-Star7088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdkevi/creating_gemma_3_from_gguf_with_mmproj_not_working/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdkevi/creating_gemma_3_from_gguf_with_mmproj_not_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdkevi/creating_gemma_3_from_gguf_with_mmproj_not_working/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T18:49:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd4op1</id>
    <title>Clara: Browser based Local AI Chat, ImageGen with simple custom Agent builder.</title>
    <updated>2025-03-17T04:36:21+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jd4op1/clara_browser_based_local_ai_chat_imagegen_with/"&gt; &lt;img alt="Clara: Browser based Local AI Chat, ImageGen with simple custom Agent builder." src="https://external-preview.redd.it/662kDyxmF0rz11JzhX_fLISnvTFW9w3CE6gaa8Ta4wg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abda2fd3ebfe247a77cb433fc9a2bcd0dcc638b1" title="Clara: Browser based Local AI Chat, ImageGen with simple custom Agent builder." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey devs,&lt;/p&gt; &lt;p&gt;I built Clara because I wanted a simple, lightweight AI assistant that runs entirely on my own machine. Most AI tools depend on cloud services, track usage, or require heavy setups—Clara is different. It connects directly to Ollama for LLMs and ComfyUI for Stable Diffusion image generation, with zero external dependencies.&lt;/p&gt; &lt;p&gt;No docker, no backend, just ollama and clara installed on the pc is enough.&lt;/p&gt; &lt;p&gt;🔗 Repo: &lt;a href="https://rgithub.com/badboysm890/ClaraVerse"&gt;https://rgithub.com/badboysm890/ClaraVerse&lt;/a&gt; 💻 Download the app: &lt;a href="https://github.com/badboysm890/ClaraVerse/releases/tag/v0.2.0"&gt;https://github.com/badboysm890/ClaraVerse/releases/tag/v0.2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Why Clara? 1. Runs Locally – No cloud, no API calls, fully private. 2. All the data is stored in IndexDB 3. Fast &amp;amp; Lightweight – I love open web UI but now its too big for my machine 4. Agent Builder – Create simple AI agents and convert them into apps. 5. ComfyUI Integration – Generate images with Stable Diffusion models. 6. Custom Model Support – Works with any Ollama-compatible LLM. 7. Built-in Image Gallery – Just added is so i can have all the images generated in one place&lt;/p&gt; &lt;p&gt;💡 Need Help! I don’t have a Windows machine, so if anyone can help with building and testing the Windows version, I’d really appreciate it! Let me know if you’re interested.&lt;/p&gt; &lt;p&gt;Would love to hear your feedback if you try it! 🚀&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/badboysm890/ClaraVerse"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd4op1/clara_browser_based_local_ai_chat_imagegen_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jd4op1/clara_browser_based_local_ai_chat_imagegen_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T04:36:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdf5fp</id>
    <title>X299 i9 7980XE SKYLAKEX-CASCADEX CPU ONLY LLM PERFORMANCE BENCHMARK</title>
    <updated>2025-03-17T15:19:44+00:00</updated>
    <author>
      <name>/u/bharlesm</name>
      <uri>https://old.reddit.com/user/bharlesm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jdf5fp/x299_i9_7980xe_skylakexcascadex_cpu_only_llm/"&gt; &lt;img alt="X299 i9 7980XE SKYLAKEX-CASCADEX CPU ONLY LLM PERFORMANCE BENCHMARK" src="https://preview.redd.it/xlivwqroo9pe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f076d64564d470e199f8365d70518819a046b47b" title="X299 i9 7980XE SKYLAKEX-CASCADEX CPU ONLY LLM PERFORMANCE BENCHMARK" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bharlesm"&gt; /u/bharlesm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xlivwqroo9pe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdf5fp/x299_i9_7980xe_skylakexcascadex_cpu_only_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdf5fp/x299_i9_7980xe_skylakexcascadex_cpu_only_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T15:19:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdj98c</id>
    <title>Is worth it to buy 128gb ram + tesla k80?</title>
    <updated>2025-03-17T18:03:41+00:00</updated>
    <author>
      <name>/u/Dangerous_Pineapple1</name>
      <uri>https://old.reddit.com/user/Dangerous_Pineapple1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, I’m new to AI. I’m planning to buy 128GB of RAM and a Tesla K80 for my Dell R730xd (with an Intel Xeon E5-2640 v4). The doubt I have is about what models I could run with this setup, since I’m not finding much information&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous_Pineapple1"&gt; /u/Dangerous_Pineapple1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdj98c/is_worth_it_to_buy_128gb_ram_tesla_k80/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdj98c/is_worth_it_to_buy_128gb_ram_tesla_k80/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdj98c/is_worth_it_to_buy_128gb_ram_tesla_k80/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T18:03:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdr0bv</id>
    <title>Old Trusty!</title>
    <updated>2025-03-17T23:22:58+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jdr0bv/old_trusty/"&gt; &lt;img alt="Old Trusty!" src="https://preview.redd.it/56mrtc9lcbpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2189cfb531f3a6f13473cbf43f90235d3da5e32a" title="Old Trusty!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/56mrtc9lcbpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdr0bv/old_trusty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdr0bv/old_trusty/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T23:22:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdpta0</id>
    <title>Best models on a MacBook Pro M3 w/ 18GB of RAM in 2025?</title>
    <updated>2025-03-17T22:30:10+00:00</updated>
    <author>
      <name>/u/vegantiger</name>
      <uri>https://old.reddit.com/user/vegantiger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been playing with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;llama3:8b&lt;/li&gt; &lt;li&gt;gemma3:4b&lt;/li&gt; &lt;li&gt;deepseek-r1:7b&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So far llama3 seems to be the best all around, and anything bigger I've tried is so slow that it's unusable…&lt;/p&gt; &lt;p&gt;Are there any other models that run acceptably fast on this kind of setup that I should check out? I'm especially looking for coding stuff, as well as transcriptions and translations English → French.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vegantiger"&gt; /u/vegantiger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdpta0/best_models_on_a_macbook_pro_m3_w_18gb_of_ram_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdpta0/best_models_on_a_macbook_pro_m3_w_18gb_of_ram_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdpta0/best_models_on_a_macbook_pro_m3_w_18gb_of_ram_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T22:30:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jduw95</id>
    <title>Light-R1-32B-FP16 + 8xMi50 Server + vLLM</title>
    <updated>2025-03-18T02:31:18+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0oakixfd0dpe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jduw95/lightr132bfp16_8xmi50_server_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jduw95/lightr132bfp16_8xmi50_server_vllm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T02:31:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdnsik</id>
    <title>Why does AI gives better result</title>
    <updated>2025-03-17T21:04:57+00:00</updated>
    <author>
      <name>/u/OkConsideration2734</name>
      <uri>https://old.reddit.com/user/OkConsideration2734</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have started using Ollama since yesterday and i am a little surprised because LLM looks like they are giving way better results in theirs originals websites/apps. Perharps, is there a way to change that and make my LLMS in Ollama give more accurate results ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OkConsideration2734"&gt; /u/OkConsideration2734 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdnsik/why_does_ai_gives_better_result/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdnsik/why_does_ai_gives_better_result/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdnsik/why_does_ai_gives_better_result/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T21:04:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1je04qv</id>
    <title>Is it just me or is LG's EXAONE 2.4b crazy good?</title>
    <updated>2025-03-18T08:20:27+00:00</updated>
    <author>
      <name>/u/Nathamuni</name>
      <uri>https://old.reddit.com/user/Nathamuni</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nathamuni"&gt; /u/Nathamuni &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1jduff4/is_it_just_me_or_is_lgs_exaone_24b_crazy_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1je04qv/is_it_just_me_or_is_lgs_exaone_24b_crazy_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1je04qv/is_it_just_me_or_is_lgs_exaone_24b_crazy_good/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T08:20:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1je7s1l</id>
    <title>Trying to make my own llama 3 model and getting this: Error: no Modelfile or safetensors files found</title>
    <updated>2025-03-18T15:34:45+00:00</updated>
    <author>
      <name>/u/josuk8</name>
      <uri>https://old.reddit.com/user/josuk8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On windows, installed ollama, cmd ollama get llama 3, created a text file with no .txt at the end with vs code with;&lt;/p&gt; &lt;p&gt;&amp;quot;FROM llama3&lt;/p&gt; &lt;p&gt;SYSTEM *instructions and personality*&amp;quot; &lt;/p&gt; &lt;p&gt;Thats just called &amp;quot;name-llama3&amp;quot; and placed it into C:\Users\&amp;quot;user&amp;quot;\OneDrive\Documents\AiStuff\CustomModels and the .ollama file is in C:\Users\&amp;quot;user&amp;quot;\.ollama, anyone know how to fix this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/josuk8"&gt; /u/josuk8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1je7s1l/trying_to_make_my_own_llama_3_model_and_getting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1je7s1l/trying_to_make_my_own_llama_3_model_and_getting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1je7s1l/trying_to_make_my_own_llama_3_model_and_getting/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T15:34:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1je418g</id>
    <title>Fine tuning a I’ll with technical documents and manuals</title>
    <updated>2025-03-18T12:42:59+00:00</updated>
    <author>
      <name>/u/WarbossTodd</name>
      <uri>https://old.reddit.com/user/WarbossTodd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I’m trying to create a AI bot where we can ask simple questions like what’s the default IP of a device or what does the yellow status light mean based on information that’s contained in technical manuals (pdf) and possibly some excel spreadsheets. &lt;/p&gt; &lt;p&gt;What’s the best way to accomplish this? I have ollama, llama3 and OpenWeb up and running in a Windows 11 box. If I can prove this is a viable path forward as a support and research tool O will be able to expand it significantly. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WarbossTodd"&gt; /u/WarbossTodd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1je418g/fine_tuning_a_ill_with_technical_documents_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1je418g/fine_tuning_a_ill_with_technical_documents_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1je418g/fine_tuning_a_ill_with_technical_documents_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T12:42:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdb4zm</id>
    <title>I created a text editor that integrates with Ollama.</title>
    <updated>2025-03-17T12:11:22+00:00</updated>
    <author>
      <name>/u/lehen01</name>
      <uri>https://old.reddit.com/user/lehen01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jdb4zm/i_created_a_text_editor_that_integrates_with/"&gt; &lt;img alt="I created a text editor that integrates with Ollama." src="https://external-preview.redd.it/eXl1OWhjb3lxOHBlMdocuIzrYi9v6aPpAc1tPXmrtMByoLVp6lH-YWI3ILPo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e15c458e95cf8ceed96405eec2a1539ec81fecf" title="I created a text editor that integrates with Ollama." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working for a couple of years on a project I just launched.&lt;/p&gt; &lt;p&gt;It is a text editor that doesn't force you to send your notes to the cloud and integrates with Ollama to add AI prompts.&lt;/p&gt; &lt;p&gt;If you need a place to create your ideas and don't want to worry about who is spying on you, you'll love this app =]. Looks like Notion, but focused on privacy and offline usage (with better UI, in my opinion hahaha).&lt;/p&gt; &lt;p&gt;Website: &lt;a href="https://writeopia.io/"&gt;writeopia.io&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Writeopia/Writeopia"&gt;https://github.com/Writeopia/Writeopia&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My future plans:&lt;/p&gt; &lt;p&gt;- Finish the signature of Windows app and post it.&lt;/p&gt; &lt;p&gt;- Android/iOS apps.&lt;/p&gt; &lt;p&gt;- Meetings summary. (Drag and drop a video, you get the summary).&lt;/p&gt; &lt;p&gt;- Semantic search.&lt;/p&gt; &lt;p&gt;- AI generates a small presentation based on your document.&lt;/p&gt; &lt;p&gt;- Text summary.&lt;/p&gt; &lt;p&gt;- Backend that can be self-hosted.&lt;/p&gt; &lt;p&gt;I would love the community feedback about the project. Feel free to reach out with questions or issues, you can use this thread or send me a DM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lehen01"&gt; /u/lehen01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i5bcrdoyq8pe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdb4zm/i_created_a_text_editor_that_integrates_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdb4zm/i_created_a_text_editor_that_integrates_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T12:11:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jebgq3</id>
    <title>Open weights model that supports function calling?</title>
    <updated>2025-03-18T18:05:13+00:00</updated>
    <author>
      <name>/u/boxabirds</name>
      <uri>https://old.reddit.com/user/boxabirds</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all I'm doing some local agent work and it really slams the LLMs. I keep getting 429s from Claude and Gemini. So I thought I'd use my local 4090 / 24GB rig as the LLM. But I'm having a devil of a time finding an open weights LLM that works. &lt;/p&gt; &lt;p&gt;I tried llama3.2:3b, gemma3:27b, phi4 all to no avail -- they all returned &amp;quot;function calling not supported&amp;quot; &lt;/p&gt; &lt;p&gt;then I tried phi4-mini and this random stuff came out &lt;/p&gt; &lt;p&gt;Ollama 0.6.2 is what I'm using. &lt;/p&gt; &lt;p&gt;Here's a sample script I wrote to test it and ph4-mini output -- maybe it's wrong? Because it certainly produces gobbledegook (that ollama setup otherwise works fine). &lt;/p&gt; &lt;p&gt;output --&lt;/p&gt; &lt;pre&gt;&lt;code&gt; Initial model response: { &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot; Bob is called a function which… goes on forever … I blocks and should switch between brackets \&amp;quot; has created this mark as Y. &amp;quot; } Model response (no function call): Bob is called a function which …&amp;quot;,&amp;quot; The following marks a number indicates that the previous indices can be generated at random, I blocks and should switch between brackets &amp;quot; has created this mark as Y. ``` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;import js&lt;/p&gt; &lt;pre&gt;&lt;code&gt;on import requests from datetime import datetime # Custom Ollama base URL OLLAMA_BASE_URL = &amp;quot;http://gruntus:11434/v1&amp;quot; # Function to call Ollama API directly def ollama_chat(model, messages, tools=None, tool_choice=None): url = f&amp;quot;{OLLAMA_BASE_URL}/chat/completions&amp;quot; payload = { &amp;quot;model&amp;quot;: model, &amp;quot;messages&amp;quot;: messages } if tools: payload[&amp;quot;tools&amp;quot;] = tools if tool_choice: payload[&amp;quot;tool_choice&amp;quot;] = tool_choice response = requests.post(url, json=payload) return response.json() # Define a simple function schema function_schema = { &amp;quot;type&amp;quot;: &amp;quot;function&amp;quot;, &amp;quot;function&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;get_weather&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Get the current weather in a given location&amp;quot;, &amp;quot;parameters&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;, &amp;quot;properties&amp;quot;: { &amp;quot;location&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;The city and state, e.g. San Francisco, CA&amp;quot; }, &amp;quot;unit&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;enum&amp;quot;: [&amp;quot;celsius&amp;quot;, &amp;quot;fahrenheit&amp;quot;], &amp;quot;description&amp;quot;: &amp;quot;The temperature unit to use&amp;quot; } }, &amp;quot;required&amp;quot;: [&amp;quot;location&amp;quot;] } } } # Mock function to simulate getting weather data def get_weather(location, unit=&amp;quot;celsius&amp;quot;): # In a real application, this would call a weather API mock_temps = {&amp;quot;New York&amp;quot;: 22, &amp;quot;San Francisco&amp;quot;: 18, &amp;quot;Miami&amp;quot;: 30} temp = mock_temps.get(location, 25) if unit == &amp;quot;fahrenheit&amp;quot;: temp = (temp * 9/5) + 32 return { &amp;quot;location&amp;quot;: location, &amp;quot;temperature&amp;quot;: temp, &amp;quot;unit&amp;quot;: unit, &amp;quot;condition&amp;quot;: &amp;quot;sunny&amp;quot;, &amp;quot;timestamp&amp;quot;: datetime.now().isoformat() } # Create a conversation messages = [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What's the weather like in New York right now?&amp;quot;}] # Call the model with function calling response = ollama_chat( model=&amp;quot;phi4-mini&amp;quot;, messages=messages, tools=[function_schema], tool_choice=&amp;quot;auto&amp;quot; ) # Extract the message from the response model_message = response.get(&amp;quot;choices&amp;quot;, [{}])[0].get(&amp;quot;message&amp;quot;, {}) # Add the response to the conversation messages.append(model_message) print(&amp;quot;Initial model response:&amp;quot;) print(json.dumps(model_message, indent=2)) # Check if the model wants to call a function if model_message.get(&amp;quot;tool_calls&amp;quot;): for tool_call in model_message[&amp;quot;tool_calls&amp;quot;]: function_name = tool_call[&amp;quot;function&amp;quot;][&amp;quot;name&amp;quot;] function_args = json.loads(tool_call[&amp;quot;function&amp;quot;][&amp;quot;arguments&amp;quot;]) print(f&amp;quot;\nModel is calling function: {function_name}&amp;quot;) print(f&amp;quot;With arguments: {function_args}&amp;quot;) # Execute the function if function_name == &amp;quot;get_weather&amp;quot;: result = get_weather( location=function_args.get(&amp;quot;location&amp;quot;), unit=function_args.get(&amp;quot;unit&amp;quot;, &amp;quot;celsius&amp;quot;) ) # Add the function result to the conversation messages.append({ &amp;quot;role&amp;quot;: &amp;quot;tool&amp;quot;, &amp;quot;tool_call_id&amp;quot;: tool_call[&amp;quot;id&amp;quot;], &amp;quot;name&amp;quot;: function_name, &amp;quot;content&amp;quot;: json.dumps(result) }) # Get the final response from the model final_response = ollama_chat( model=&amp;quot;phi4-mini&amp;quot;, messages=messages ) final_message = final_response.get(&amp;quot;choices&amp;quot;, [{}])[0].get(&amp;quot;message&amp;quot;, {}) print(&amp;quot;\nFinal response:&amp;quot;) print(final_message.get(&amp;quot;content&amp;quot;, &amp;quot;No response content&amp;quot;)) else: print(&amp;quot;\nModel response (no function call):&amp;quot;) print(model_message.get(&amp;quot;content&amp;quot;, &amp;quot;No response content&amp;quot;)) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;``` &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boxabirds"&gt; /u/boxabirds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jebgq3/open_weights_model_that_supports_function_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jebgq3/open_weights_model_that_supports_function_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jebgq3/open_weights_model_that_supports_function_calling/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T18:05:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1je8faj</id>
    <title>Why is there such a performance difference between Olama CLI and Olama Python?</title>
    <updated>2025-03-18T16:01:52+00:00</updated>
    <author>
      <name>/u/Upbeat-Teacher-2306</name>
      <uri>https://old.reddit.com/user/Upbeat-Teacher-2306</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried a lot of models in my laptop with ollama cli. Some of them with good inference speed , but when I use ollama in my python code with the same models , the inference speed is too slow!!! WHY? There are some way to accelerate this inference time in python? Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Upbeat-Teacher-2306"&gt; /u/Upbeat-Teacher-2306 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1je8faj/why_is_there_such_a_performance_difference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1je8faj/why_is_there_such_a_performance_difference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1je8faj/why_is_there_such_a_performance_difference/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T16:01:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jegywj</id>
    <title>Ollama and Gemma3</title>
    <updated>2025-03-18T21:50:03+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Installed latest Ollama, 0.6.1&lt;/p&gt; &lt;p&gt;Trying to run any Gemma3, and gettings this:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama run gemma3:27b&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Error: Post &amp;quot;http://127.0.0.1:11434/api/generate&amp;quot;: EOF&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Any other model, llama3.3, aya,mistral,deepseek works! &lt;/p&gt; &lt;p&gt;What is the problem here, why Gemma3 does not work but all others do?&lt;/p&gt; &lt;p&gt;I have 2x 7900 XTX. Loads of RAM and CPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jegywj/ollama_and_gemma3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jegywj/ollama_and_gemma3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jegywj/ollama_and_gemma3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T21:50:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jecyck</id>
    <title>Swapping from Chatgpt to ollama</title>
    <updated>2025-03-18T19:05:37+00:00</updated>
    <author>
      <name>/u/Pirate_dolphin</name>
      <uri>https://old.reddit.com/user/Pirate_dolphin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working with &lt;a href="https://github.com/BRlkl/AGI-Samantha"&gt;AGI Samantha &lt;/a&gt;and it's working fine. I had to make some tweaks but its visual, self prompting and can now take my terminal or speech input. It has a locally recorded short term memory, long term memory and a subconcious. &lt;/p&gt; &lt;p&gt;When I convert this to ollama the model is repeating these inputs back to me, rather than taking them internally and acting with them.&lt;/p&gt; &lt;p&gt;Any suggestions on how this could be done? I'm thinking about changing the model file instead of leaving them in the script&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pirate_dolphin"&gt; /u/Pirate_dolphin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jecyck/swapping_from_chatgpt_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jecyck/swapping_from_chatgpt_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jecyck/swapping_from_chatgpt_to_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T19:05:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jegfg8</id>
    <title>LLM Req's for Goose on RTX 2070</title>
    <updated>2025-03-18T21:26:30+00:00</updated>
    <author>
      <name>/u/CorpusculantCortex</name>
      <uri>https://old.reddit.com/user/CorpusculantCortex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to get a bare bones functional instance of Goose running on my system. I haven't upgraded in a few years and am holding out for 5070ti stock to come in (hahaha).. Anyway, I tried mistral 7B because of the size, it is snappy, but it didn't trigger any tools, just endlessly told me there were tools available. I am currently trying qwq, but dear lord it is doggish and not especially accurate either, so I am left wait forever just to give basic instruction. Is there anything I can mount on 8gb VRAM that will at least marginally get me moving while I consider my upgrade plans? &lt;/p&gt; &lt;p&gt;I was spoiled by the beta of Manus, but the session and context limits are killing me, even if I had a dogshit slow instance running local that I can run all day at a fraction of the efficiency would make me happier. Plus, I ultimately would like to use my current system to offload low weight tasks in a cluster if at all possible.&lt;/p&gt; &lt;p&gt;I mostly do python scripting, automations, data analysis.&lt;/p&gt; &lt;p&gt;Am I a fool with absurd dreams? Just kidding I would love any and all suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CorpusculantCortex"&gt; /u/CorpusculantCortex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jegfg8/llm_reqs_for_goose_on_rtx_2070/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jegfg8/llm_reqs_for_goose_on_rtx_2070/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jegfg8/llm_reqs_for_goose_on_rtx_2070/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T21:26:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1je1k10</id>
    <title>Mistral Small 3.1</title>
    <updated>2025-03-18T10:10:50+00:00</updated>
    <author>
      <name>/u/laurentbourrelly</name>
      <uri>https://old.reddit.com/user/laurentbourrelly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you are looking for a small model, Mistral is an interesting option. Unfortunately, like all small models, it hallucinates a lot.&lt;/p&gt; &lt;p&gt;The new Mistral just came out and looks promising &lt;a href="https://mistral.ai/news/mistral-small-3-1"&gt;https://mistral.ai/news/mistral-small-3-1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/laurentbourrelly"&gt; /u/laurentbourrelly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1je1k10/mistral_small_31/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1je1k10/mistral_small_31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1je1k10/mistral_small_31/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T10:10:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jein5y</id>
    <title>Local Agents with Full Machine Access!!!</title>
    <updated>2025-03-18T23:02:52+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Ollama community!&lt;/p&gt; &lt;p&gt;I've just released a major new feature for Observer AI that I think many of you will find interesting: &lt;strong&gt;full Jupyter Server integration&lt;/strong&gt; with Python code execution capabilities!&lt;/p&gt; &lt;h1&gt;What this means:&lt;/h1&gt; &lt;p&gt;Observer AI can now connect to your existing Jupyter server, allowing you to execute Python code directly on your machine with agent's responses! This creates a complete perception-action loop where agents can:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Observe your screen (via OCR or screenshots with vision models)&lt;/li&gt; &lt;li&gt;Process what they see with LLMs running locally through Ollama&lt;/li&gt; &lt;li&gt;Execute Python code to perform actions on your system!!&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Potential use cases:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Data processing: Agents that monitor spreadsheets and write files!&lt;/li&gt; &lt;li&gt;Automation tools: Create personalized workflow automations triggered by screen content&lt;/li&gt; &lt;li&gt;Screen regulation: Watches for violent content and shuts down the computer! hahaha&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Looking for feedback:&lt;/h1&gt; &lt;p&gt;As fellow Ollama users who are comfortable with technical setups, I'd love your thoughts on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What kinds of agents would you build with Python execution capabilities? (I can help you through discord or dm's)&lt;/li&gt; &lt;li&gt;Any security considerations you'd want to see addressed? (everything is local so no RCE yet i think)&lt;/li&gt; &lt;li&gt;Feature requests or improvements to the Jupyter integration?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Observer AI remains 100% open source and local-first - try it at &lt;a href="https://app.observer-ai.com"&gt;https://app.observer-ai.com&lt;/a&gt; or check out the code at &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for all the support and feedback so far!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jein5y/local_agents_with_full_machine_access/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jein5y/local_agents_with_full_machine_access/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jein5y/local_agents_with_full_machine_access/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T23:02:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jebv52</id>
    <title>PrivateLLMLens updated (zero web server single page HTML file)</title>
    <updated>2025-03-18T18:21:26+00:00</updated>
    <author>
      <name>/u/Specialist_Laugh_231</name>
      <uri>https://old.reddit.com/user/Specialist_Laugh_231</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jebv52/privatellmlens_updated_zero_web_server_single/"&gt; &lt;img alt="PrivateLLMLens updated (zero web server single page HTML file)" src="https://preview.redd.it/l183ie98phpe1.gif?width=640&amp;amp;crop=smart&amp;amp;s=ca6e8f3b06dc7665ea9c1444981d972afbdcc6b7" title="PrivateLLMLens updated (zero web server single page HTML file)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist_Laugh_231"&gt; /u/Specialist_Laugh_231 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l183ie98phpe1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jebv52/privatellmlens_updated_zero_web_server_single/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jebv52/privatellmlens_updated_zero_web_server_single/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T18:21:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1je5u77</id>
    <title>Example running Gemma 3 locally for OCR using Ollama</title>
    <updated>2025-03-18T14:09:48+00:00</updated>
    <author>
      <name>/u/Elegant-Army-8888</name>
      <uri>https://old.reddit.com/user/Elegant-Army-8888</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google DeepMind has been cooking lately, while everyone has been focusing on the Gemini 2.0 Flash native image generation release, Gemma 3 is really a nifty little tool for developers&lt;/p&gt; &lt;p&gt;I build this demo python app in a couple of hours with Claude 3.7 in &lt;a href="/u/cursor_ai"&gt;u/cursor_ai&lt;/a&gt; showcasing that.&lt;br /&gt; The app uses Streamlit for the UI, Ollama as the backend running Gemma 3 vision locally, PIL for image processing, and pdf2image for PDF support.&lt;/p&gt; &lt;p&gt;And I can run it all locally on my 3 year old Macbook Pro. Takes about 30 seconds per image, but that's ok by me. If you have more than 32 gb of memory, and an RTX or M4 i'm sure it's even faster.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/adspiceprospice/localOCR"&gt;https://github.com/adspiceprospice/localOCR&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Elegant-Army-8888"&gt; /u/Elegant-Army-8888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1je5u77/example_running_gemma_3_locally_for_ocr_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1je5u77/example_running_gemma_3_locally_for_ocr_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1je5u77/example_running_gemma_3_locally_for_ocr_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T14:09:48+00:00</published>
  </entry>
</feed>
