<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-07-19T11:23:08+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1m0gzg2</id>
    <title>We built Explainable AI with pinpointed citations &amp; reasoning — works across PDFs, Excel, CSV, Docs &amp; more</title>
    <updated>2025-07-15T12:53:56+00:00</updated>
    <author>
      <name>/u/Effective-Ad2060</name>
      <uri>https://old.reddit.com/user/Effective-Ad2060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just added explainability to our RAG pipeline — the AI now shows &lt;strong&gt;pinpointed citations&lt;/strong&gt; down to the &lt;strong&gt;exact paragraph, table row, or cell&lt;/strong&gt; it used to generate its answer.&lt;/p&gt; &lt;p&gt;It doesn’t just name the source file but also &lt;strong&gt;highlights the exact text&lt;/strong&gt; and lets you &lt;strong&gt;jump directly to that part of the document&lt;/strong&gt;. This works across formats: PDFs, Excel, CSV, Word, PowerPoint, Markdown, and more.&lt;/p&gt; &lt;p&gt;It makes AI answers easy to &lt;strong&gt;trust and verify&lt;/strong&gt;, especially in messy or lengthy enterprise files. You also get insight into the &lt;strong&gt;reasoning&lt;/strong&gt; behind the answer.&lt;/p&gt; &lt;p&gt;It’s fully open-source: &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;br /&gt; Would love to hear your thoughts or feedback!&lt;/p&gt; &lt;p&gt;📹 Demo: &lt;a href="https://youtu.be/QWY_jtjRcCM"&gt;https://youtu.be/QWY_jtjRcCM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective-Ad2060"&gt; /u/Effective-Ad2060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m0gzg2/we_built_explainable_ai_with_pinpointed_citations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m0gzg2/we_built_explainable_ai_with_pinpointed_citations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m0gzg2/we_built_explainable_ai_with_pinpointed_citations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-15T12:53:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1e4di</id>
    <title>HELP - How to get the llm to write and read to txt files on linux.</title>
    <updated>2025-07-16T14:31:43+00:00</updated>
    <author>
      <name>/u/TheStronkFemboy</name>
      <uri>https://old.reddit.com/user/TheStronkFemboy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have created a modified version of mistral-nemo:12b, to talk to my friends in my discord server. i managed to get her to send messages in the server, but id like for her to write and read from a text file for long term memory. Thanks in Advanced! :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheStronkFemboy"&gt; /u/TheStronkFemboy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1e4di/help_how_to_get_the_llm_to_write_and_read_to_txt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1e4di/help_how_to_get_the_llm_to_write_and_read_to_txt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m1e4di/help_how_to_get_the_llm_to_write_and_read_to_txt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T14:31:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1im63</id>
    <title>Dreaming Bard - lightweight self-hosted writing assistant for novels using external LLMs (R&amp;D project)</title>
    <updated>2025-07-16T17:19:51+00:00</updated>
    <author>
      <name>/u/leshiy-urban</name>
      <uri>https://old.reddit.com/user/leshiy-urban</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/leshiy-urban"&gt; /u/leshiy-urban &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/selfhosted/comments/1m1iljp/dreaming_bard_lightweight_selfhosted_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1im63/dreaming_bard_lightweight_selfhosted_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m1im63/dreaming_bard_lightweight_selfhosted_writing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T17:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1jj4l</id>
    <title>Recommend hardware for my use case?</title>
    <updated>2025-07-16T17:54:12+00:00</updated>
    <author>
      <name>/u/-how-about-69-</name>
      <uri>https://old.reddit.com/user/-how-about-69-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: My model right now is about 60gb. Uses a context window of 1million tokens. &lt;/p&gt; &lt;p&gt;I’m curious what kind of hardware should I look to upgrade to? I’d like something that is also future proofed a bit as I continue to tinker with the model and it gets more demanding. &lt;/p&gt; &lt;p&gt;I was thinking of either a Mac Studio with 512gb of ram or the Ryzen 395 max with 128gb but I’m open to other suggestions or recommendations. &lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;p&gt;Full context:&lt;/p&gt; &lt;p&gt;So my use case is a bit more extreme than most people. &lt;/p&gt; &lt;p&gt;I am a fan fic writer as a hobby. I have written 6 fan fiction books in my life. Each around 100-200k words. I have built a whole fictional universe for my characters. This is something I really enjoy but I actually hate the writing part of it. This is actually why I never publish anything for money and write under a fictional name as I have never been proud of my books. &lt;/p&gt; &lt;p&gt;Making fictional outlines is super fun for me but creative writing is my weak point and frankly just unenjoyable to me. &lt;/p&gt; &lt;p&gt;I’ve been training an AI model from Ollama on my previous works and all my outlines. I want to use this model to help me refine my prior works to improve the writing and use it for turning my unwritten outlines into full novels. &lt;/p&gt; &lt;p&gt;I know there’s paid software out there to do this but having used them I felt they produced a product that was no better than my meager skills. I want to actually produce a product that I would be proud to put my name on. &lt;/p&gt; &lt;p&gt;I did test my model and was actually very happy with the result. It’s not perfect but It’s much better than the paid models online but it took about 4 weeks to produce a single response which consisted of 1 chapter or about 1500 tokens. &lt;/p&gt; &lt;p&gt;I’d like to reduce that response time into hours if possible. &lt;/p&gt; &lt;p&gt;My model right now is about 60gb. Uses a context window of 1million tokens. &lt;/p&gt; &lt;p&gt;My rig has 64gb of ram and a 1080ti w/11gb. I also have an old 4tb mechanical hdd as paging for windows otherwise ollama would complain I didn’t have enough memory. &lt;/p&gt; &lt;p&gt;I’m curious what kind of hardware should I look to upgrade to? &lt;/p&gt; &lt;p&gt;I was thinking of either a Mac Studio with 512gb of ram or the Ryzen 395 max with 128gb but I’m open to other suggestions or recommendations. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-how-about-69-"&gt; /u/-how-about-69- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1jj4l/recommend_hardware_for_my_use_case/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1jj4l/recommend_hardware_for_my_use_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m1jj4l/recommend_hardware_for_my_use_case/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T17:54:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1qxwf</id>
    <title>Which model would perform well for code auto-completion on my setup?</title>
    <updated>2025-07-16T22:42:24+00:00</updated>
    <author>
      <name>/u/SubstantialAdvisor37</name>
      <uri>https://old.reddit.com/user/SubstantialAdvisor37</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m using 3 x Quadro RTX 4000 GPUs (8GB each). I tested the Qwen2.5 Coder 14B, but it's a bit too slow. The 7B model runs fast, but I’m wondering if there’s a good middle ground—something faster than the 14B but potentially more capable than the 7B.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SubstantialAdvisor37"&gt; /u/SubstantialAdvisor37 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1qxwf/which_model_would_perform_well_for_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1qxwf/which_model_would_perform_well_for_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m1qxwf/which_model_would_perform_well_for_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T22:42:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1p2nk</id>
    <title>Ideal Ollama Setup Suggestions needed</title>
    <updated>2025-07-16T21:26:11+00:00</updated>
    <author>
      <name>/u/MUKE-13</name>
      <uri>https://old.reddit.com/user/MUKE-13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi. a novice local-LLM practiser here. i need help setting up ollama (again).&lt;/p&gt; &lt;p&gt;Some background for reference. I had installed it before and played around a bit with some LLM models (gemma3 mainly). I ran a WSL setup with Ollama and Open WEB-UI over a docker container inside WSL. I talked back and forth with gemma, which suggested i install the whole thing with python, as that would be more flexible in case i wanted to start using more advanced things like MCP and Databases (which i totally dont know how to do btw) but i thought, well ok, might give it a shot. I might learn the most by doing it wrong. soon enough, i must have did so, because my open Web-UI stopped working completely, i couldnt pull any new models and the ones installed wouldnt run anymore.&lt;br /&gt; Long story short, i tried uninstalling everything and installing it with docker desktop again but that only made things worse. I thought to myself alright happens and freshly installed windows from scratch because honestly i gave up on fixing the error/s.&lt;br /&gt; Now i would like to ask you guys, what would you suggest? Is it really that much of a difference, if i install it via python or wsl or docker desktop? what are the con's of the different setup-variations, apart from the rather difficult setup procedure for python (bear with me please, im not well versed in that area at all)&lt;br /&gt; I'm happy for any suggestions and help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MUKE-13"&gt; /u/MUKE-13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1p2nk/ideal_ollama_setup_suggestions_needed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1p2nk/ideal_ollama_setup_suggestions_needed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m1p2nk/ideal_ollama_setup_suggestions_needed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T21:26:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1mjip</id>
    <title>Limit gpu usage on MacOs</title>
    <updated>2025-07-16T19:46:42+00:00</updated>
    <author>
      <name>/u/fossa04_</name>
      <uri>https://old.reddit.com/user/fossa04_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I just bought a M3 MacBook Air with 24GB of memory and I wanted to test Ollama.&lt;/p&gt; &lt;p&gt;The problem is that when I submit a prompt the gpu usage goes to 100% and the laptop really hot, there some setting to limit the usage of gpu on ollama? I don't mind if it will be slower, I just want to make it usable.&lt;/p&gt; &lt;p&gt;Bonus question: is it normal that deepseek r1 14B occupy only 1.6GB of memory from activity monitor, am I missing something?&lt;/p&gt; &lt;p&gt;Thank you all! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fossa04_"&gt; /u/fossa04_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1mjip/limit_gpu_usage_on_macos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1mjip/limit_gpu_usage_on_macos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m1mjip/limit_gpu_usage_on_macos/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T19:46:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1eqlg</id>
    <title>5060TI 16GB or 5070 12GB which one is better to run ai model in ollama</title>
    <updated>2025-07-16T14:55:42+00:00</updated>
    <author>
      <name>/u/actuallytech</name>
      <uri>https://old.reddit.com/user/actuallytech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i just confused to buy 5060ti 16gb vram or 5070 12gb the diffrence is 4 gb in vram , 5070 have more cuda cores but if i cant load ai models there no point having good perfomance &lt;/p&gt; &lt;p&gt;i think i can run gemma3:27b and other models if i have 16gb vram &lt;/p&gt; &lt;p&gt;btw im new into running ai model i guess anyone can help me&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/actuallytech"&gt; /u/actuallytech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1eqlg/5060ti_16gb_or_5070_12gb_which_one_is_better_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1eqlg/5060ti_16gb_or_5070_12gb_which_one_is_better_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m1eqlg/5060ti_16gb_or_5070_12gb_which_one_is_better_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T14:55:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1munw</id>
    <title>How can I access open web ui from across the home network?</title>
    <updated>2025-07-16T19:59:00+00:00</updated>
    <author>
      <name>/u/Doge_gameing</name>
      <uri>https://old.reddit.com/user/Doge_gameing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've finished setting up Ollama and open webui on my home server, but I can't figure out how to use the open web ui from my other devices. I could not use Docker because the server is running Windows Server 2019, so I had to do a Python install of it. im just looking for any solution to use the open webui on my other devices&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Doge_gameing"&gt; /u/Doge_gameing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1munw/how_can_i_access_open_web_ui_from_across_the_home/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1munw/how_can_i_access_open_web_ui_from_across_the_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m1munw/how_can_i_access_open_web_ui_from_across_the_home/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T19:59:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1phub</id>
    <title>Shortcut to inject your desktop UI into AI context window with Ollama</title>
    <updated>2025-07-16T21:43:07+00:00</updated>
    <author>
      <name>/u/louis3195</name>
      <uri>https://old.reddit.com/user/louis3195</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m1phub/shortcut_to_inject_your_desktop_ui_into_ai/"&gt; &lt;img alt="Shortcut to inject your desktop UI into AI context window with Ollama" src="https://external-preview.redd.it/aXFhY3lpZzMxYmRmMRv1DRecgjO_4o9uwyVMt6D0M3tL8nnnWa1A7lkOWT7i.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e54fbf7745b45f915f917cd4df2b0f81718964ba" title="Shortcut to inject your desktop UI into AI context window with Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;git clone https://github.com/mediar-ai/terminator.git cd terminator/terminator-mcp-agent/examples/terminator-ai-summarizer cargo build --release --bin terminator-ai-summarizer # basic UI-dump mode (no AI summarization) ./target/release/terminator-ai-summarizer \--model ollama/gemma-1b \--system-prompt &amp;quot;Summarize this UI tree&amp;quot; \--hotkey &amp;quot;ctrl+alt+j&amp;quot; # AI summarization ./target/release/terminator-ai-summarizer \--model ollama/gemma-3b \--system-prompt &amp;quot;You are a UI assistant.&amp;quot; \--hotkey &amp;quot;ctrl+alt+j&amp;quot; \--ai-mode &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;How it works&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Use cases&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;- Copy paste your whole WhatsApp to clipboard and chat with the content&lt;br /&gt; - Same for Telegram&lt;br /&gt; - Other apps / website where cmd/ctrl A does not work or screenshot does not fit in viewport&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/louis3195"&gt; /u/louis3195 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ap37gjg31bdf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1phub/shortcut_to_inject_your_desktop_ui_into_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m1phub/shortcut_to_inject_your_desktop_ui_into_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T21:43:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2531i</id>
    <title>Struggling with structured data extraction from scanned receipts</title>
    <updated>2025-07-17T11:30:10+00:00</updated>
    <author>
      <name>/u/Easy_Letterhead5466</name>
      <uri>https://old.reddit.com/user/Easy_Letterhead5466</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I’m working on a project to extract structured data (like company name, date, total, address) from scanned receipts and forms using models like Donut ocr or layoutlmv3. I’ve prepared my dataset in a prompt format and trained Donut on it, but during evaluation I often get wrong predictions. I’m wondering if this is due to tokenizer issues, formatting, or small dataset size. Has anyone faced similar problems with Donut or other imagetotext models? I’d also appreciate suggestions on better models or techniques for extracting data from scanned documents or noisy PDFs without using bounding boxes. Thanks! The dataset is SROIE one from kaggle&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Easy_Letterhead5466"&gt; /u/Easy_Letterhead5466 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2531i/struggling_with_structured_data_extraction_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2531i/struggling_with_structured_data_extraction_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m2531i/struggling_with_structured_data_extraction_from/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-17T11:30:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1n2pt</id>
    <title>recommend me an embedding model</title>
    <updated>2025-07-16T20:07:37+00:00</updated>
    <author>
      <name>/u/why_not_my_email</name>
      <uri>https://old.reddit.com/user/why_not_my_email</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm an academic, and over the years I've amassed a library of about 13,000 PDFs of journal articles and books. Over the past few days I put together a basic semantic search app where I can start with a sentence or paragraph (from something I'm writing) and find 10-15 items from my library (as potential sources/citations). &lt;/p&gt; &lt;p&gt;Since this is my first time working with document embeddings, I went with &lt;code&gt;snowflake-arctic-embed2&lt;/code&gt; primarily because it has a relatively long 8k context window. A typical journal article in my field is 8-10k words, and of course books are much longer. &lt;/p&gt; &lt;p&gt;I've found some recommendations to &amp;quot;choose an embedding model based on your use case,&amp;quot; but no actual discussion of &lt;em&gt;which&lt;/em&gt; models work well for different kinds of use cases. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/why_not_my_email"&gt; /u/why_not_my_email &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1n2pt/recommend_me_an_embedding_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m1n2pt/recommend_me_an_embedding_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m1n2pt/recommend_me_an_embedding_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-16T20:07:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2fv7a</id>
    <title>Locally Running AI model with Intel GPU</title>
    <updated>2025-07-17T18:50:10+00:00</updated>
    <author>
      <name>/u/dragonknight-18</name>
      <uri>https://old.reddit.com/user/dragonknight-18</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an intel arc graphics card and ai - npu , powered with intel core ultra 7-155H processor, with 16gb ram (though that this would be useful for doing ai work but i am regretting my deicision , i could have easily bought a gaming laptop with this money). Pls pls pls it would be so much better if anyone could help&lt;br /&gt; But when running an ai model locally using ollama, it neither uses gpu nor npu , can someone else suggest any other service platform like ollama, where we can locally download and run ai model efficiently, as i want to train small 1b model with a .csv file .&lt;br /&gt; Or can anyone also suggest any other ways where i can use gpu, (i am an undergrad student).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dragonknight-18"&gt; /u/dragonknight-18 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2fv7a/locally_running_ai_model_with_intel_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2fv7a/locally_running_ai_model_with_intel_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m2fv7a/locally_running_ai_model_with_intel_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-17T18:50:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2djlq</id>
    <title>MedGemma 27b (multimodal version) vision capability seems to not work with Ollama 0.9.7 pre-release rc1. Anyone else encountering this?</title>
    <updated>2025-07-17T17:22:05+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried Unsloth’s Q_8 of MedGemma 27b (multimodal version) &lt;a href="https://huggingface.co/unsloth/medgemma-27b-it-GGUF"&gt;https://huggingface.co/unsloth/medgemma-27b-it-GGUF&lt;/a&gt; under Ollama 0.9.7rc1 using Open WebUI 0.6.16 and I get no response from the model upon sending an image to it with a prompt. Text prompts seem to work just fine, but no luck with images. “Vision” checkbox is checked in the model page on Open WebUI and an “Ollama show” command shows image support for the model. My Gemma3 models seem to work fine with images just fine, but not MedGemma. what’s going on? &lt;/p&gt; &lt;p&gt;Has anyone else encountered the same issue? If so, did you resolve it? How? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2djlq/medgemma_27b_multimodal_version_vision_capability/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2djlq/medgemma_27b_multimodal_version_vision_capability/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m2djlq/medgemma_27b_multimodal_version_vision_capability/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-17T17:22:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2tiy1</id>
    <title>Gaming Desktop is Overkill?</title>
    <updated>2025-07-18T05:01:59+00:00</updated>
    <author>
      <name>/u/MrJaver</name>
      <uri>https://old.reddit.com/user/MrJaver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanna have an AI for coding (java backend, react frontend) inside Jetbrains IDE. I pay for a license but the cloud AI quota is very small but don't feel like paying as AI doesn't do all that much, just convenience for debugging, plus it's kinda slow going to/from the network. Jetbrains recently added local ollama support, so I wanna give it a try but I don't know what I'm doing. I got:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;2019 16&amp;quot; macbook pro 2.4 GHz 8-Core Intel Core i9/AMD Radeon Pro 5500M 4 GB/32 GB 2667 MHz DDR4&lt;/li&gt; &lt;li&gt;A gaming desktop with 32gb ram ddr4, i7 12 gen, RTX 3060ti, about 100gb m.2 pcie3 and 600gb HDD&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I tried running deepseek-r1:8b on my MacBook and it was unacceptably slow, printing &amp;quot;thinking&amp;quot; steps and then replying. Guess I don't care that it's thinking out loud but it took like a whole minute to reply to &amp;quot;hello&amp;quot;. I didn't see much GPU processing usage, just GPU memory, maybe I need to configure something?&lt;/p&gt; &lt;p&gt;I could try to use some lightweight model but then I don't want the model to give me wrong answers, does that matter at all for coding? I read there are models curated for coding, I'll try some...&lt;/p&gt; &lt;p&gt;Another idea is that I have this gaming desktop standing around, I could start it up and run a model on there, is that overkill for what I need? Also, not much high-speed storage there, although I can buy another ssd if it's worth the trouble. Not sure how I can connect my MacBook to PC, they are both connected to wifi, I can also try ethernet/usb cord - does that matter?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrJaver"&gt; /u/MrJaver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2tiy1/gaming_desktop_is_overkill/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2tiy1/gaming_desktop_is_overkill/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m2tiy1/gaming_desktop_is_overkill/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-18T05:01:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2tvkv</id>
    <title>Does ollama still not support Radeon 6600 GPU</title>
    <updated>2025-07-18T05:22:05+00:00</updated>
    <author>
      <name>/u/Comfortable-Fan4865</name>
      <uri>https://old.reddit.com/user/Comfortable-Fan4865</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am just getting started with downloading and integrating my first AI, but it does not use my Radeon 6600 GPU and is very slow because of it. Does ollama still not support it, or am I just dumb and don't know what i'm doing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Fan4865"&gt; /u/Comfortable-Fan4865 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2tvkv/does_ollama_still_not_support_radeon_6600_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2tvkv/does_ollama_still_not_support_radeon_6600_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m2tvkv/does_ollama_still_not_support_radeon_6600_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-18T05:22:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2em7l</id>
    <title>Ollama + open webui + excel</title>
    <updated>2025-07-17T18:02:39+00:00</updated>
    <author>
      <name>/u/Ordinary_Mention3655</name>
      <uri>https://old.reddit.com/user/Ordinary_Mention3655</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, new to ollama. I attached an excel file on webui and gave a prompt for it to analyze the data and generate the output, but it keeps saying it is not able to access the file. Any idea what I am doing wrong in this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ordinary_Mention3655"&gt; /u/Ordinary_Mention3655 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2em7l/ollama_open_webui_excel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2em7l/ollama_open_webui_excel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m2em7l/ollama_open_webui_excel/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-17T18:02:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2uj1f</id>
    <title>Spy Search CLI supports Ollama</title>
    <updated>2025-07-18T06:00:59+00:00</updated>
    <author>
      <name>/u/jasonhon2013</name>
      <uri>https://old.reddit.com/user/jasonhon2013</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really want to say thank you to the Ollama community! I just released my second open-source project, which is native (and originally designed for Ollama). The idea is to replace the Gemini CLI with lightning speed. Similar to the previous spy search, this open-source project will be really quick if you are using Mistral models! I hope you enjoy it. Once again, thank you so much for your support. I just can't reach this level without Ollama's support! (Yeah, give me an upvote or stars if you love this idea!)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/JasonHonKL/spy-search-cli"&gt;https://github.com/JasonHonKL/spy-search-cli&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jasonhon2013"&gt; /u/jasonhon2013 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2uj1f/spy_search_cli_supports_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2uj1f/spy_search_cli_supports_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m2uj1f/spy_search_cli_supports_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-18T06:00:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2qnfk</id>
    <title>RouteGPT - the chrome extension for chatgpt that means no more pedaling to the model selector (powered by Ollama and Arch-Router LLM)</title>
    <updated>2025-07-18T02:31:30+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m2qnfk/routegpt_the_chrome_extension_for_chatgpt_that/"&gt; &lt;img alt="RouteGPT - the chrome extension for chatgpt that means no more pedaling to the model selector (powered by Ollama and Arch-Router LLM)" src="https://external-preview.redd.it/ZmNqcDI1MjN4aWRmMXNi_tiJEhKI4dMkHPB85mAr78mWhl9BNIxK-HEB1dv0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2a13dec5570f74069b49dd90d50a8dc0e6318b7" title="RouteGPT - the chrome extension for chatgpt that means no more pedaling to the model selector (powered by Ollama and Arch-Router LLM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;f you are a ChatGPT pro user like me, you are probably frustrated and tired of pedaling to the model selector drop down to pick a model, prompt that model and then repeat that cycle all over again. Well that pedaling goes away with RouteGPT.&lt;/p&gt; &lt;p&gt;RouteGPT is a Chrome extension for &lt;a href="http://chatgpt.com"&gt;chatgpt.com&lt;/a&gt; that automatically selects the right OpenAI model for your prompt based on preferences you define. For example: “creative novel writing, story ideas, imaginative prose” → GPT-4o, or “critical analysis, deep insights, and market research ” → o3 &lt;/p&gt; &lt;p&gt;Instead of switching models manually, RouteGPT handles it for you — like automatic transmission for your ChatGPT experience.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Extension link&lt;/strong&gt; : &lt;a href="https://chromewebstore.google.com/search/RouteGPT"&gt;https://chromewebstore.google.com/search/RouteGPT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;P.S:&lt;/strong&gt; The extension is an experiment - I &lt;em&gt;vibe coded&lt;/em&gt; it in 7 days - and a means to demonstrate some of our technology. My hope is to be helpful to those who might benefit from this, and drive a discussion about the science and infrastructure work underneath that could enable the most ambitious teams to move faster in building great agents&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;: &lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;https://huggingface.co/katanemo/Arch-Router-1.5B&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href="https://arxiv.org/abs/2506.16655"&gt;https://arxiv.org/abs/2506.16655&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h8o8m223xidf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m2qnfk/routegpt_the_chrome_extension_for_chatgpt_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m2qnfk/routegpt_the_chrome_extension_for_chatgpt_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-18T02:31:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1m346zm</id>
    <title>Built Ollamaton - Universal MCP Client for Ollama (CLI/API/GUI)</title>
    <updated>2025-07-18T14:43:28+00:00</updated>
    <author>
      <name>/u/inventorado</name>
      <uri>https://old.reddit.com/user/inventorado</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m346zm/built_ollamaton_universal_mcp_client_for_ollama/"&gt; &lt;img alt="Built Ollamaton - Universal MCP Client for Ollama (CLI/API/GUI)" src="https://external-preview.redd.it/SuDxJUkGbblEMhWNwAdUMq9vE644u0P-tro1ImMeakI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c757484d6433d24eb9d362ceb1f0d3237f882db4" title="Built Ollamaton - Universal MCP Client for Ollama (CLI/API/GUI)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inventorado"&gt; /u/inventorado &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/mcp/comments/1m332dj/built_ollamaton_universal_mcp_client_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m346zm/built_ollamaton_universal_mcp_client_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m346zm/built_ollamaton_universal_mcp_client_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-18T14:43:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3htty</id>
    <title>Nvidia GTX-1080Ti 11GB Vram</title>
    <updated>2025-07-18T23:52:35+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran into problems when I replace the &lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1070.c2840"&gt;GTX-1070&lt;/a&gt; with&lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877"&gt; GTX 1080Ti&lt;/a&gt;. NVTOP would show about 7GB of VRAM usage. So I had to adjust the num_gpu value to 63. Nice improvement.&lt;/p&gt; &lt;p&gt;These my steps:&lt;/p&gt; &lt;p&gt;&lt;code&gt;time ollama run --verbose gemma3:12b-it-qat&lt;/code&gt;&lt;br /&gt; &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;/set parameter num_gpu 63&lt;/code&gt;&lt;br /&gt; &lt;code&gt;Set parameter 'num_gpu' to '63'&lt;/code&gt;&lt;br /&gt; &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;/save mygemma3&lt;/code&gt;&lt;br /&gt; Created new model 'mygemma3'&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;NAME&lt;/th&gt; &lt;th align="left"&gt;eval rate&lt;/th&gt; &lt;th align="left"&gt;prompt eval rate&lt;/th&gt; &lt;th align="left"&gt;total duration&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3:12b-it-qat&lt;/td&gt; &lt;td align="left"&gt;6.69&lt;/td&gt; &lt;td align="left"&gt;118.6&lt;/td&gt; &lt;td align="left"&gt;3m2.831s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mygemma3:latest&lt;/td&gt; &lt;td align="left"&gt;24.74&lt;/td&gt; &lt;td align="left"&gt;349.2&lt;/td&gt; &lt;td align="left"&gt;0m38.677s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Here are a few other models:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;NAME&lt;/th&gt; &lt;th align="left"&gt;eval rate&lt;/th&gt; &lt;th align="left"&gt;prompt eval rate&lt;/th&gt; &lt;th align="left"&gt;total duration&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek-r1:14b&lt;/td&gt; &lt;td align="left"&gt;22.72&lt;/td&gt; &lt;td align="left"&gt;51.83&lt;/td&gt; &lt;td align="left"&gt;34.07208103&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mygemma3:latest&lt;/td&gt; &lt;td align="left"&gt;23.97&lt;/td&gt; &lt;td align="left"&gt;321.68&lt;/td&gt; &lt;td align="left"&gt;47.22412009&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3:12b&lt;/td&gt; &lt;td align="left"&gt;16.84&lt;/td&gt; &lt;td align="left"&gt;96.54&lt;/td&gt; &lt;td align="left"&gt;1m20.845913225&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3:12b-it-qat&lt;/td&gt; &lt;td align="left"&gt;13.33&lt;/td&gt; &lt;td align="left"&gt;159.54&lt;/td&gt; &lt;td align="left"&gt;1m36.518625216&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3:27b&lt;/td&gt; &lt;td align="left"&gt;3.65&lt;/td&gt; &lt;td align="left"&gt;9.49&lt;/td&gt; &lt;td align="left"&gt;7m30.344502487&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3n:e2b-it-q8_0&lt;/td&gt; &lt;td align="left"&gt;45.95&lt;/td&gt; &lt;td align="left"&gt;183.27&lt;/td&gt; &lt;td align="left"&gt;30.09576316&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite3.1-moe:3b-instruct-q8_0&lt;/td&gt; &lt;td align="left"&gt;88.46&lt;/td&gt; &lt;td align="left"&gt;546.45&lt;/td&gt; &lt;td align="left"&gt;8.24215104&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama3.1:8b&lt;/td&gt; &lt;td align="left"&gt;38.29&lt;/td&gt; &lt;td align="left"&gt;174.13&lt;/td&gt; &lt;td align="left"&gt;16.73243012&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;minicpm-v:8b&lt;/td&gt; &lt;td align="left"&gt;37.67&lt;/td&gt; &lt;td align="left"&gt;188.41&lt;/td&gt; &lt;td align="left"&gt;4.663153513&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mistral:7b-instruct-v0.2-q5_K_M&lt;/td&gt; &lt;td align="left"&gt;40.33&lt;/td&gt; &lt;td align="left"&gt;176.14&lt;/td&gt; &lt;td align="left"&gt;5.90872581&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmo2:13b&lt;/td&gt; &lt;td align="left"&gt;12.18&lt;/td&gt; &lt;td align="left"&gt;107.56&lt;/td&gt; &lt;td align="left"&gt;26.67653928&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;phi4:14b&lt;/td&gt; &lt;td align="left"&gt;23.56&lt;/td&gt; &lt;td align="left"&gt;116.84&lt;/td&gt; &lt;td align="left"&gt;16.40753603&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3:14b&lt;/td&gt; &lt;td align="left"&gt;22.66&lt;/td&gt; &lt;td align="left"&gt;156.32&lt;/td&gt; &lt;td align="left"&gt;36.78135622&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I had each model create a CSV format from the ollama --verbose output and the following models failed.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;FAILED:&lt;/p&gt; &lt;p&gt;minicpm-v:8b &lt;/p&gt; &lt;p&gt;olmo2:13b &lt;/p&gt; &lt;p&gt;granite3.1-moe:3b-instruct-q8_0 &lt;/p&gt; &lt;p&gt;mistral:7b-instruct-v0.2-q5_K_M &lt;/p&gt; &lt;p&gt;gemma3n:e2b-it-q8_0&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I cut GPU total power from 250 to 188 using: &lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo nvidia-smi -i 0 -pl 188&lt;/code&gt; &lt;/p&gt; &lt;p&gt;Resulted in 'eval rate' &lt;/p&gt; &lt;p&gt;250 watts=24.7 &lt;/p&gt; &lt;p&gt;188 watts=23.6&lt;/p&gt; &lt;p&gt;Not much of a hit to drop 25% power usage. I also tested the bare minimum of 125 watts but that resulted in a 25% reduction in eval rate. Still that makes running several cards viable. &lt;/p&gt; &lt;p&gt;I have a more in depth review on my &lt;a href="https://tabletuser.blogspot.com/2025/07/gtx-1080ti-optimized-for-ollama.html"&gt;blog&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3htty/nvidia_gtx1080ti_11gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3htty/nvidia_gtx1080ti_11gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m3htty/nvidia_gtx1080ti_11gb_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-18T23:52:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3pz4z</id>
    <title>LANGCHAIN + DEEPSEEK OLLAMA = LONG WAIT AND RANDOM BLOB</title>
    <updated>2025-07-19T07:12:38+00:00</updated>
    <author>
      <name>/u/ComedianObjective572</name>
      <uri>https://old.reddit.com/user/ComedianObjective572</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m3pz4z/langchain_deepseek_ollama_long_wait_and_random/"&gt; &lt;img alt="LANGCHAIN + DEEPSEEK OLLAMA = LONG WAIT AND RANDOM BLOB" src="https://preview.redd.it/xcvoam7w6sdf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3dda6a9c735044f73516c2ec142e84a3bae64fd7" title="LANGCHAIN + DEEPSEEK OLLAMA = LONG WAIT AND RANDOM BLOB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there! I currently built an AI Agent for Business needs. However, I tried DeepSeek for LLM and it was a long wait and a random Blob. Is it just me or does this happen to you?&lt;/p&gt; &lt;p&gt;P.S. Prefered Model is Qwen3 and Code Qwen 2.5. I just want to explore if there are better models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComedianObjective572"&gt; /u/ComedianObjective572 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xcvoam7w6sdf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3pz4z/langchain_deepseek_ollama_long_wait_and_random/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m3pz4z/langchain_deepseek_ollama_long_wait_and_random/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-19T07:12:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3qa4o</id>
    <title>Simple way to run ollama on an air gapped Server?</title>
    <updated>2025-07-19T07:32:32+00:00</updated>
    <author>
      <name>/u/Toeeni</name>
      <uri>https://old.reddit.com/user/Toeeni</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Guys,&lt;/p&gt; &lt;p&gt;what is the simplest way to run ollama on an air gapped Server? I don't find any solutions yet to just download ollama and a llm and transfer it to the server to run it there.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Toeeni"&gt; /u/Toeeni &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3qa4o/simple_way_to_run_ollama_on_an_air_gapped_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3qa4o/simple_way_to_run_ollama_on_an_air_gapped_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m3qa4o/simple_way_to_run_ollama_on_an_air_gapped_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-19T07:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3jon2</id>
    <title>Meet "Z840 Pascal" | My ugly old z840 stuffed with cheap Pascal cards from Ebay, running llama4:scout @ 5 tokens/second</title>
    <updated>2025-07-19T01:22:39+00:00</updated>
    <author>
      <name>/u/Wooden_Push_4137</name>
      <uri>https://old.reddit.com/user/Wooden_Push_4137</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m3jon2/meet_z840_pascal_my_ugly_old_z840_stuffed_with/"&gt; &lt;img alt="Meet &amp;quot;Z840 Pascal&amp;quot; | My ugly old z840 stuffed with cheap Pascal cards from Ebay, running llama4:scout @ 5 tokens/second" src="https://b.thumbs.redditmedia.com/1ygDUJCHEercPbRDEvI4Bko-akX2AP05oi_4mTcTMPg.jpg" title="Meet &amp;quot;Z840 Pascal&amp;quot; | My ugly old z840 stuffed with cheap Pascal cards from Ebay, running llama4:scout @ 5 tokens/second" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do I know how to have a Friday night, or what?!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fadmmyilgqdf1.png?width=1244&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23a6fb59a1ea448ad2e44beb0def5c7127921b28"&gt;https://preview.redd.it/fadmmyilgqdf1.png?width=1244&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23a6fb59a1ea448ad2e44beb0def5c7127921b28&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8hedwqaveqdf1.png?width=1320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa3ef4d2d7467bcdee27ce3e6ea6da803b00f723"&gt;https://preview.redd.it/8hedwqaveqdf1.png?width=1320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa3ef4d2d7467bcdee27ce3e6ea6da803b00f723&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ovvb5tytfqdf1.png?width=1660&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3198b36dc403a9f3eec6af5682ba854d5e056e29"&gt;https://preview.redd.it/ovvb5tytfqdf1.png?width=1660&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3198b36dc403a9f3eec6af5682ba854d5e056e29&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d9hzgu7yfqdf1.png?width=1662&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=195b3b0613bfb987aaaabe7e81c2cf3995091715"&gt;https://preview.redd.it/d9hzgu7yfqdf1.png?width=1662&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=195b3b0613bfb987aaaabe7e81c2cf3995091715&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden_Push_4137"&gt; /u/Wooden_Push_4137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3jon2/meet_z840_pascal_my_ugly_old_z840_stuffed_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3jon2/meet_z840_pascal_my_ugly_old_z840_stuffed_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m3jon2/meet_z840_pascal_my_ugly_old_z840_stuffed_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-19T01:22:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3g64z</id>
    <title>Hate my PM Job so I Tried to Automate it with a Custom CUA Agent</title>
    <updated>2025-07-18T22:38:12+00:00</updated>
    <author>
      <name>/u/Defiant-Plan-1393</name>
      <uri>https://old.reddit.com/user/Defiant-Plan-1393</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Rather than using one of the traceable, available tools, I decided to make my own computer use and MCP agent, SOFIA (Sort of Functional Interactive Agent), for ollama and openai to try and automate my job by hosting it on my VPN. The tech probably just isn't there yet, but I came up with an agent that can successfully navigate apps on my desktop.&lt;/p&gt; &lt;p&gt;You can see the github: &lt;a href="https://github.com/akim42003/SOFIA"&gt;https://github.com/akim42003/SOFIA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The CUA architecture uses a custom omniparser layer and filter to get positional information about the desktop, which ensures almost perfect accuracy for mouse manipulation without damaging the context. It is reasonable effective using mistral-small3.1:24b, but is obviously much slower and less accurate than using GPT. I did notice that embedding the thought process into the modelfile made a big difference in the agents ability to breakdown tasks and execute tools sequentially.&lt;/p&gt; &lt;p&gt;I do genuinely use this tool as an email and calendar assistant.&lt;/p&gt; &lt;p&gt;It also contains a desktop, hastily put together version of cluely I made for fun. I would love to discuss this project and any similar experiences other people have had.&lt;/p&gt; &lt;p&gt;As a side note if anyone wants to get me out of PM hell by hiring me as a SWE that would be great!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Defiant-Plan-1393"&gt; /u/Defiant-Plan-1393 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3g64z/hate_my_pm_job_so_i_tried_to_automate_it_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m3g64z/hate_my_pm_job_so_i_tried_to_automate_it_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m3g64z/hate_my_pm_job_so_i_tried_to_automate_it_with_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-18T22:38:12+00:00</published>
  </entry>
</feed>
