<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-05-19T19:48:56+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1kox9ew</id>
    <title>Model Recommendations</title>
    <updated>2025-05-17T16:38:51+00:00</updated>
    <author>
      <name>/u/TheMicrosoftMan</name>
      <uri>https://old.reddit.com/user/TheMicrosoftMan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have two main devices that I can use to run local AI models on. The first of those devices is my Surface Pro 11 with a Snapdragon X Elite chip. The other one is an old surface book 2 with an Nvidia 1060 GPU. Which one is better for running AI models with Ollama on? Does the Nvidia 1000-series support Cuda? What are the best models for each device? Is there a way to have the computer remain idle until a request is sent to it so it is not constantly sucking power?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheMicrosoftMan"&gt; /u/TheMicrosoftMan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kox9ew/model_recommendations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kox9ew/model_recommendations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kox9ew/model_recommendations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-17T16:38:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kork29</id>
    <title>MULTI MODAL VIDEO RAG PROJECT</title>
    <updated>2025-05-17T12:14:39+00:00</updated>
    <author>
      <name>/u/Pez_99</name>
      <uri>https://old.reddit.com/user/Pez_99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to build a multimodal RAG application specifically for videos. The core idea is to leverage the visual content of videos, essentially the individual frames, which are just images, to extract and utilize the information they contain. These frames can present various forms of data such as: • On screen text • Diagrams and charts • Images of objects or scenes&lt;/p&gt; &lt;p&gt;My understanding is that everything in a video can essentially be broken down into two primary formats: text and images. • Audio can be converted into text using speech to text models. • Frames are images that may contain embedded text or visual context.&lt;/p&gt; &lt;p&gt;So, the system should primarily focus on these two modalities: text and images.&lt;/p&gt; &lt;p&gt;Here’s what I envision building: 1. Extract and store all textual information present in each frame. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;If a frame lacks text, the system should still be able to understand the visual context. Maybe using a Vision Language Model (VLM).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Maintain contextual continuity across neighboring frames, since the meaning of one frame may heavily rely on the preceding or succeeding frames.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Apply the same principle to audio: segment transcripts based on sentence boundaries and associate them with the relevant sequence of frames (this seems less challenging, as it’s mostly about syncing text with visuals).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Generate image captions for frames to add an extra layer of context and understanding. (Using CLIP or something)&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;To be honest, I’m still figuring out the details and would appreciate guidance on how to approach this effectively.&lt;/p&gt; &lt;p&gt;What I want from this Video RAG application:&lt;/p&gt; &lt;p&gt;I want the system to be able to answer user queries about a video, even if the video contains ambiguous or sparse information. For example:&lt;/p&gt; &lt;p&gt;• Provide a summary of the quarterly sales chart. • What were the main points discussed by the trainer in this video • List all the policies mentioned throughout the video.&lt;/p&gt; &lt;p&gt;Note: I’m not trying to build the kind of advanced video RAG that understands a video purely from visual context alone, such as a silent video of someone tying a tie, where the system infers the steps without any textual or audio cues. That’s beyond the current scope.&lt;/p&gt; &lt;p&gt;The three main scenarios I want to address: 1. Videos with both transcription and audio 2. Videos with visuals and audio, but no pre existing transcription (We can use models like Whisper to transcribe the audio) 3. Videos with no transcription or audio (These could have background music or be completely silent, requiring visual only understanding)&lt;/p&gt; &lt;p&gt;Please help me refine this idea further or guide me on the right tools, architectures, and strategies to implement such a system effectively. Any other approach or anything that I missing. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pez_99"&gt; /u/Pez_99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kork29/multi_modal_video_rag_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kork29/multi_modal_video_rag_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kork29/multi_modal_video_rag_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-17T12:14:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1kor7be</id>
    <title>macOS Application for Ollama - macLlama</title>
    <updated>2025-05-17T11:55:38+00:00</updated>
    <author>
      <name>/u/gogimandoo</name>
      <uri>https://old.reddit.com/user/gogimandoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kor7be/macos_application_for_ollama_macllama/"&gt; &lt;img alt="macOS Application for Ollama - macLlama" src="https://preview.redd.it/77q61hsxzb1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b28e30f101d196edd3641446170d2d020bfdcda" title="macOS Application for Ollama - macLlama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;macLlama is a native macOS application providing a graphical user interface for the Ollama command-line tool. This application facilitates model management and interaction with local language models.&lt;/p&gt; &lt;p&gt;Features include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A dedicated interface for interacting with language models.&lt;/li&gt; &lt;li&gt;Open-source development and availability.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The application is developed using SwiftUI.&lt;/p&gt; &lt;p&gt;Release information are available at: &lt;a href="https://github.com/hellotunamayo/macLlama/releases"&gt;https://github.com/hellotunamayo/macLlama/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repository: &lt;a href="https://github.com/hellotunamayo/macLlama"&gt;https://github.com/hellotunamayo/macLlama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The application is in early development, and feedback is greatly appreciated to guide future enhancements. Please submit suggestions and bug reports via the GitHub repository.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gogimandoo"&gt; /u/gogimandoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/77q61hsxzb1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kor7be/macos_application_for_ollama_macllama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kor7be/macos_application_for_ollama_macllama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-17T11:55:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kps6mh</id>
    <title>Moving Ai platforms</title>
    <updated>2025-05-18T19:27:18+00:00</updated>
    <author>
      <name>/u/Unknown-Developments</name>
      <uri>https://old.reddit.com/user/Unknown-Developments</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey peeps,&lt;/p&gt; &lt;p&gt;I have been using ChatGPT mostly and recently found its physical limitations that OpenAi have bound to it and started migrating to an Ollama model. My question is, how can I move the personality of my ChatGPT custom GPT through to an Ollama model of choice? My logics system in the custom GPT is highly advanced due to the philosophical models I ran through it.&lt;/p&gt; &lt;p&gt;Can anyone assist in merging my custom GPT's personality with a local Ai model from Ollama? ChatGPT has been assisting with the migration but there are so many incorrect resources on the Web it struggles to give correct directions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unknown-Developments"&gt; /u/Unknown-Developments &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kps6mh/moving_ai_platforms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kps6mh/moving_ai_platforms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kps6mh/moving_ai_platforms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-18T19:27:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1kov3n4</id>
    <title>Photoshop using Local Computer Use agents.</title>
    <updated>2025-05-17T15:04:21+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kov3n4/photoshop_using_local_computer_use_agents/"&gt; &lt;img alt="Photoshop using Local Computer Use agents." src="https://external-preview.redd.it/YnhuZThpNm14YzFmMRXG3T7XldLhbQ1-zZgDOchlvuRH1Qq3ebSvcq1i84vf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70e12542867145b258ac69c65700c043640b8337" title="Photoshop using Local Computer Use agents." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Photoshop using c/ua.&lt;/p&gt; &lt;p&gt;No code. Just a user prompt, picking models and a Docker, and the right agent loop.&lt;/p&gt; &lt;p&gt;A glimpse at the more managed experience c/ua is building to lower the barrier for casual vibe-coders.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/up7g8ydmxc1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kov3n4/photoshop_using_local_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kov3n4/photoshop_using_local_computer_use_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-17T15:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1kp2akd</id>
    <title>Offline real-time voice conversations with custom chatbots</title>
    <updated>2025-05-17T20:19:22+00:00</updated>
    <author>
      <name>/u/w00fl35</name>
      <uri>https://old.reddit.com/user/w00fl35</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kp2akd/offline_realtime_voice_conversations_with_custom/"&gt; &lt;img alt="Offline real-time voice conversations with custom chatbots" src="https://external-preview.redd.it/bW5pZHczd3RoZTFmMatKx9EjWmqye1H1Sfl0oBQ-Ipj7s6y-PHQT0KQMotHc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88790253488a75fc0d85a1a296293c7ac9f639cd" title="Offline real-time voice conversations with custom chatbots" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w00fl35"&gt; /u/w00fl35 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/q36s4h6ohe1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kp2akd/offline_realtime_voice_conversations_with_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kp2akd/offline_realtime_voice_conversations_with_custom/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-17T20:19:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpi3vz</id>
    <title>Sentiment Analysis - hit and miss when it comes to results</title>
    <updated>2025-05-18T11:47:58+00:00</updated>
    <author>
      <name>/u/Banana5kin</name>
      <uri>https://old.reddit.com/user/Banana5kin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else using (or trying to use) Ollama to perform Sentiment Analysis?&lt;/p&gt; &lt;p&gt;I thought I'd give it a test drive, but results are inconsistent, failure to run through the dataset, incorrect analysis and 100% correct analysis all within a 1/2 dozen runs. To eliminate any potential issues with the text for analysis I ran it through a n8n code node to remove an punctuation, uppercase to lower &amp;amp; remove any white space. I have used Gemma3:1b which hits all 3 inconsistencies (more often failing) and ALIENTELLIGENCE/sentimentanalyzer which produces 100% results when it runs without error.&lt;/p&gt; &lt;p&gt;For clarity ollama is being called by the n8n sentiment analysis node using the standard system prompt as supplied by the node.&lt;/p&gt; &lt;p&gt;*edit - openai and anthropic both work flawlessly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Banana5kin"&gt; /u/Banana5kin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpi3vz/sentiment_analysis_hit_and_miss_when_it_comes_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpi3vz/sentiment_analysis_hit_and_miss_when_it_comes_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kpi3vz/sentiment_analysis_hit_and_miss_when_it_comes_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-18T11:47:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpw33v</id>
    <title>Beginner exploring local AI models for screen-reading and interactive task automation</title>
    <updated>2025-05-18T22:18:03+00:00</updated>
    <author>
      <name>/u/dark_side_o0o</name>
      <uri>https://old.reddit.com/user/dark_side_o0o</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;br /&gt; I'm completely new to local AI models and automation. I run a small digital store, and I'm trying to build a system that can handle repeated order-based tasks without manual input.&lt;/p&gt; &lt;p&gt;I'm considering using a local AI model (like LLaMA via Ollama or similar) not just to &lt;strong&gt;read what's on the screen&lt;/strong&gt;, but also to &lt;strong&gt;interact with the interface&lt;/strong&gt; — like logging into an account, selecting options, and completing a purchase or submission process.&lt;/p&gt; &lt;p&gt;The workflow I'm imagining looks like this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Detect new order (via database or webhook)&lt;/li&gt; &lt;li&gt;Launch a browser (with optional extensions)&lt;/li&gt; &lt;li&gt;Read screen content or interface status (with some form of vision model or screen parser)&lt;/li&gt; &lt;li&gt;Log in using provided credentials&lt;/li&gt; &lt;li&gt;Navigate to a specific section, choose options (like product amount), and proceed to checkout&lt;/li&gt; &lt;li&gt;Possibly handle CAPTCHAs using an external API&lt;/li&gt; &lt;li&gt;Complete the task and clean the browser session&lt;/li&gt; &lt;li&gt;Repeat for the next order&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’d love to know if there are existing tools or agents that support this kind of real-time interaction — especially ones that can be controlled locally, work offline if needed, and are beginner-friendly to configure.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dark_side_o0o"&gt; /u/dark_side_o0o &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpw33v/beginner_exploring_local_ai_models_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpw33v/beginner_exploring_local_ai_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kpw33v/beginner_exploring_local_ai_models_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-18T22:18:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kp8t1v</id>
    <title>I built an AI-powered Food &amp; Nutrition Tracker that analyzes meals from photos! Planning to open-source it</title>
    <updated>2025-05-18T01:42:57+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kp8t1v/i_built_an_aipowered_food_nutrition_tracker_that/"&gt; &lt;img alt="I built an AI-powered Food &amp;amp; Nutrition Tracker that analyzes meals from photos! Planning to open-source it" src="https://external-preview.redd.it/a3hkZzZ2MzczZzFmMevpjUkJAH29ctL9GGNTRuXbe-uU1nbp5uR8WvjIiEr4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61e785d23d7de38d4e10e32eae081497ca1c2912" title="I built an AI-powered Food &amp;amp; Nutrition Tracker that analyzes meals from photos! Planning to open-source it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;/p&gt; &lt;p&gt;Been working on this Diet &amp;amp; Nutrition tracking app and wanted to share a quick demo of its current state. The core idea is to make food logging as painless as possible.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features so far:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI Meal Analysis:&lt;/strong&gt; You can upload an image of your food, and the AI tries to identify it and provide nutritional estimates (calories, protein, carbs, fat).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Manual Logging &amp;amp; Edits:&lt;/strong&gt; Of course, you can add/edit entries manually.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Daily Nutrition Overview:&lt;/strong&gt; Tracks calories against goals, macro distribution.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Water Intake:&lt;/strong&gt; Simple water tracking.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Weekly Stats &amp;amp; Streaks:&lt;/strong&gt; To keep motivation up.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm really excited about the AI integration. It's still a work in progress, but the goal is to streamline the most tedious part of tracking.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code Status:&lt;/strong&gt; I'm planning to clean up the codebase and open-source it on GitHub in the near future! For now, if you're interested in other AI/LLM related projects and learning resources I've put together, you can check out my &amp;quot;LLM-Learn-PK&amp;quot; repo:&lt;br /&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2FPavankunchala%2FLLM-Learn-PK"&gt;https://github.com/Pavankunchala/LLM-Learn-PK&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; On a related note, I'm actively looking for new opportunities in Computer Vision and LLM engineering. If your team is hiring or you know of any openings, I'd be grateful if you'd reach out!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Email:&lt;/strong&gt; [&lt;a href="mailto:pavankunchalaofficial@gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;](mailto:&lt;a href="mailto:pavankunchalaofficial@gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My other projects on GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Resume:&lt;/strong&gt; &lt;a href="https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view"&gt;https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/tcdgsv373g1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kp8t1v/i_built_an_aipowered_food_nutrition_tracker_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kp8t1v/i_built_an_aipowered_food_nutrition_tracker_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-18T01:42:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpp1jd</id>
    <title>OLLAMA_NEW_ENGINE</title>
    <updated>2025-05-18T17:14:04+00:00</updated>
    <author>
      <name>/u/planetf1a</name>
      <uri>https://old.reddit.com/user/planetf1a</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This seems initially targetted at running new visual models.&lt;/p&gt; &lt;p&gt;Is there feature parity for other model types vs llamacp? For example running models like granite3.3, qwen3 ~8b on a mac m1 ? Any info on relative performance?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/planetf1a"&gt; /u/planetf1a &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpp1jd/ollama_new_engine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpp1jd/ollama_new_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kpp1jd/ollama_new_engine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-18T17:14:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpo4aw</id>
    <title>Contribution to ollama-python: decorators, helper functions and simplified creation tool</title>
    <updated>2025-05-18T16:34:31+00:00</updated>
    <author>
      <name>/u/chavomodder</name>
      <uri>https://old.reddit.com/user/chavomodder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kpo4aw/contribution_to_ollamapython_decorators_helper/"&gt; &lt;img alt="Contribution to ollama-python: decorators, helper functions and simplified creation tool" src="https://external-preview.redd.it/0WHEreexf2DJMw78A-6XfudwOUYNJRPPM2H2EZ2R2b8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6bbde65c5b5324258d0882eae80d38a008f28e7e" title="Contribution to ollama-python: decorators, helper functions and simplified creation tool" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! (This post was written in Portuguese)&lt;/p&gt; &lt;p&gt;I made a commit to ollama-python with the aim of making it easier to create and use custom tools. You can now use simple decorators to register functions:&lt;/p&gt; &lt;p&gt;@ollama_tool – for synchronous functions&lt;/p&gt; &lt;p&gt;@ollama_async_tool – for asynchronous functions&lt;/p&gt; &lt;p&gt;I also added auxiliary functions to make organizing and using the tools easier:&lt;/p&gt; &lt;p&gt;get_tools() – returns all registered tools&lt;/p&gt; &lt;p&gt;get_tools_name() – dictionary with the name of the tools and their respective functions&lt;/p&gt; &lt;p&gt;get_name_async_tools() – list of asynchronous tool names&lt;/p&gt; &lt;p&gt;Additionally, I created a new function called create_function_tool, which allows you to create tools in a similar way to manual, but without worrying about the JSON structure. Just pass the Python parameters like: (tool_name, description, parameter_list, required_parameters)&lt;/p&gt; &lt;p&gt;Now, to work with the tools, the flow is very simple:&lt;/p&gt; &lt;h1&gt;Returns the functions that are with the decorators&lt;/h1&gt; &lt;p&gt;tools = get_tools()&lt;/p&gt; &lt;h1&gt;dictionary with all functions using decorators (as already used)&lt;/h1&gt; &lt;p&gt;available_functions = get_tools_name() &lt;/p&gt; &lt;h1&gt;returns the names of asynchronous functions&lt;/h1&gt; &lt;p&gt;async_available_functions = get_name_async_tools() &lt;/p&gt; &lt;p&gt;And in the code, you can use an if to check if the function is asynchronous (based on the list of async_available_functions) and use await or asyncio.run() as necessary.&lt;/p&gt; &lt;p&gt;These changes help reduce the boilerplate and make development with the library more practical.&lt;/p&gt; &lt;p&gt;Anyone who wants to take a look or suggest something, follow:&lt;/p&gt; &lt;p&gt;Commit link: [ &lt;a href="https://github.com/ollama/ollama-python/pull/516"&gt;https://github.com/ollama/ollama-python/pull/516&lt;/a&gt; ]&lt;/p&gt; &lt;p&gt;My repository link:&lt;/p&gt; &lt;p&gt;[ &lt;a href="https://github.com/caua1503/ollama-python/tree/main"&gt;https://github.com/caua1503/ollama-python/tree/main&lt;/a&gt; ]&lt;/p&gt; &lt;p&gt;Observation:&lt;/p&gt; &lt;p&gt;I was already using this in my real project and decided to share it. &lt;/p&gt; &lt;p&gt;I'm an experienced Python dev, but this is my first time working with decorators and I decided to do this in the simplest way possible, I hope to help the community, I know defining global lists, maybe it's not the best way to do this but I haven't found another way&lt;/p&gt; &lt;p&gt;In addition to langchain being complicated and changing everything with each update, I couldn't use it with ollama models, so I went to the Ollama Python library&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chavomodder"&gt; /u/chavomodder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ollama/ollama-python/pull/516"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpo4aw/contribution_to_ollamapython_decorators_helper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kpo4aw/contribution_to_ollamapython_decorators_helper/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-18T16:34:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq5l3i</id>
    <title>MacBook Air M4 24 vs 32G RAM - any difference for ollama?</title>
    <updated>2025-05-19T07:16:07+00:00</updated>
    <author>
      <name>/u/dar_mach</name>
      <uri>https://old.reddit.com/user/dar_mach</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As in the topic, I'm about to get Macbook air M4, options for me are 24 or 32 G of ram. Will it make any difference in terms of running a bigger model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dar_mach"&gt; /u/dar_mach &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kq5l3i/macbook_air_m4_24_vs_32g_ram_any_difference_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kq5l3i/macbook_air_m4_24_vs_32g_ram_any_difference_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kq5l3i/macbook_air_m4_24_vs_32g_ram_any_difference_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T07:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq5w0r</id>
    <title>Hardware Configuration AI Systems</title>
    <updated>2025-05-19T07:37:31+00:00</updated>
    <author>
      <name>/u/OriginalDiddi</name>
      <uri>https://old.reddit.com/user/OriginalDiddi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kq5w0r/hardware_configuration_ai_systems/"&gt; &lt;img alt="Hardware Configuration AI Systems" src="https://b.thumbs.redditmedia.com/vaGsONk_TxcwL5Ur9A9xDsKL9UuVTso3NJ2jX8KO1DU.jpg" title="Hardware Configuration AI Systems" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, so I asked AI for an Configuration to setup a Server to power on-premise AI Systems.&lt;/p&gt; &lt;p&gt;This is what it came up with:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5aio6w3jzo1f1.png?width=733&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f702909a76275301cd0bf49111584594d3e4e50"&gt;https://preview.redd.it/5aio6w3jzo1f1.png?width=733&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f702909a76275301cd0bf49111584594d3e4e50&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Is this somewhat accurate or a total mess? Any recomendations on an AI Setup?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OriginalDiddi"&gt; /u/OriginalDiddi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kq5w0r/hardware_configuration_ai_systems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kq5w0r/hardware_configuration_ai_systems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kq5w0r/hardware_configuration_ai_systems/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T07:37:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq26ow</id>
    <title>ollama not utilising GPU?</title>
    <updated>2025-05-19T03:37:16+00:00</updated>
    <author>
      <name>/u/thelegend27al</name>
      <uri>https://old.reddit.com/user/thelegend27al</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kq26ow/ollama_not_utilising_gpu/"&gt; &lt;img alt="ollama not utilising GPU?" src="https://a.thumbs.redditmedia.com/QWOdEOFTvonvihn2meJ54GHKHgtErbB498Fy2OdNBU4.jpg" title="ollama not utilising GPU?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have installed ROCm, is this normal to see, or is my CPU running inference instead? When I type in a prompt my GPU usage spikes to max for a few seconds then only my CPU seems to be running at max utilisation. Thanks!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jimtwy0xsn1f1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8d4876d70b8b5ec741e6d6701e2a928657593ea"&gt;https://preview.redd.it/jimtwy0xsn1f1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8d4876d70b8b5ec741e6d6701e2a928657593ea&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thelegend27al"&gt; /u/thelegend27al &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kq26ow/ollama_not_utilising_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kq26ow/ollama_not_utilising_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kq26ow/ollama_not_utilising_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T03:37:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpzatm</id>
    <title>GPU utilized only on api/generate endpoint and not on api/chat endpoint</title>
    <updated>2025-05-19T00:59:30+00:00</updated>
    <author>
      <name>/u/Commanderdrag</name>
      <uri>https://old.reddit.com/user/Commanderdrag</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am new to using ollama, not new to programming, and I have having some trouble getting gemma3 to utilize my gpu when using chat api. I can see that the GPU is utilized when I run the model from the commandline, which uses the generate endpoint. However when I use the python ollama package and call the same gemma3 model using the chat() function, which uses the chat api endpoint, I see no load on my gpu and the response takes significantly longer. Reading the server logs nothing jumps out as super important, in fact the debug logs for both calls are identical in every way except for the endpoint that is being used. What steps can I take to troubleshoot this issue? Any advice is much appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Commanderdrag"&gt; /u/Commanderdrag &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpzatm/gpu_utilized_only_on_apigenerate_endpoint_and_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpzatm/gpu_utilized_only_on_apigenerate_endpoint_and_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kpzatm/gpu_utilized_only_on_apigenerate_endpoint_and_not/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T00:59:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq66tk</id>
    <title>Can I use OpenWebUI for Mattermost integration?</title>
    <updated>2025-05-19T07:59:46+00:00</updated>
    <author>
      <name>/u/fensizor</name>
      <uri>https://old.reddit.com/user/fensizor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Noob question, but I need a self-hosted solution/platform with RAG support to be able to integrate LLM into Mattermost so it would answer users' questions inside threads as kind of first line support. Is OpenWebUI or any other solution would be able to help me with that? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fensizor"&gt; /u/fensizor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kq66tk/can_i_use_openwebui_for_mattermost_integration/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kq66tk/can_i_use_openwebui_for_mattermost_integration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kq66tk/can_i_use_openwebui_for_mattermost_integration/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T07:59:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqcr8b</id>
    <title>Is BotUI a good tool to make a customizable interface ?</title>
    <updated>2025-05-19T14:14:25+00:00</updated>
    <author>
      <name>/u/DoubleRealistic883</name>
      <uri>https://old.reddit.com/user/DoubleRealistic883</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys ! I have worked with AnythingLLM for a week now but this tool seems too limited for me, I want a Web UI that I can change as much as I want. I was looking for tools to make Web UI and I came accross BotUI that looks like the most permissive one. Is it a good idea to use it and connect it to my Ollama API ? Are there better tools ? I need to be able to customize everything : logos, background, add buttons, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DoubleRealistic883"&gt; /u/DoubleRealistic883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqcr8b/is_botui_a_good_tool_to_make_a_customizable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqcr8b/is_botui_a_good_tool_to_make_a_customizable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kqcr8b/is_botui_a_good_tool_to_make_a_customizable/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T14:14:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpwptc</id>
    <title>Why changing num_gpu has a much bigger impact on Gemma3 than Qwen3?</title>
    <updated>2025-05-18T22:47:47+00:00</updated>
    <author>
      <name>/u/S4lVin</name>
      <uri>https://old.reddit.com/user/S4lVin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, basically, was testing out some settings to have the best performance with each model.&lt;/p&gt; &lt;p&gt;I found out that by running the default num_gpu value (which i don't know what is it on Open WebUI) Gemma3 12B QAT runs at about 13-14T/s (Using ~40% GPU and ~95% CPU), while Qwen3 runs at about 60T/s (Using ~95% GPU and ~25% CPU).&lt;/p&gt; &lt;p&gt;If i increase the num_gpu value to 256, Gemma3 runs at about 60T/s (Using ~95% GPU and ~25% CPU), while Qwen3 runs the same as before.&lt;/p&gt; &lt;p&gt;Why does this happen? It's as if Qwen3 is already set with num_gpu maxed out, while Gemma3 does not. But i suppose num_gpu is set by default to all models, and it doesn't change from model to model, or am i wrong?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/S4lVin"&gt; /u/S4lVin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpwptc/why_changing_num_gpu_has_a_much_bigger_impact_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpwptc/why_changing_num_gpu_has_a_much_bigger_impact_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kpwptc/why_changing_num_gpu_has_a_much_bigger_impact_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-18T22:47:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq5ake</id>
    <title>Clara — A fully offline, Modular AI workspace (LLMs + Agents + Automation + Image Gen)</title>
    <updated>2025-05-19T06:56:03+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kq5ake/clara_a_fully_offline_modular_ai_workspace_llms/"&gt; &lt;img alt="Clara — A fully offline, Modular AI workspace (LLMs + Agents + Automation + Image Gen)" src="https://preview.redd.it/u6niruxjqo1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92c4cac8e33b1fe68fdc0af3f66d45dcdcf1c55a" title="Clara — A fully offline, Modular AI workspace (LLMs + Agents + Automation + Image Gen)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u6niruxjqo1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kq5ake/clara_a_fully_offline_modular_ai_workspace_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kq5ake/clara_a_fully_offline_modular_ai_workspace_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T06:56:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpzahp</id>
    <title>AI Model for Handwriting OCR Recognition?</title>
    <updated>2025-05-19T00:59:00+00:00</updated>
    <author>
      <name>/u/Wonderful-Truth-4849</name>
      <uri>https://old.reddit.com/user/Wonderful-Truth-4849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m pretty new to using offline AI models and could really use some advice. I’m in the process of digitizing some old diaries, and I’m considering subscribing to Transkribus, but before committing, I want to test out some offline OCR models to see what works best.&lt;/p&gt; &lt;p&gt;I did give ChatGPT a try for handwriting recognition, and it actually did a solid job, but unfortunately, due to copyright and permissions, I can’t use it for this project. So now I’m on the hunt for other good offline options.&lt;/p&gt; &lt;p&gt;Any recommendations or experiences with OCR models that work well for handwritten text would be super helpful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wonderful-Truth-4849"&gt; /u/Wonderful-Truth-4849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpzahp/ai_model_for_handwriting_ocr_recognition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpzahp/ai_model_for_handwriting_ocr_recognition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kpzahp/ai_model_for_handwriting_ocr_recognition/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T00:59:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqgfk8</id>
    <title>High CPU and Low GPU?</title>
    <updated>2025-05-19T16:42:24+00:00</updated>
    <author>
      <name>/u/sandman_br</name>
      <uri>https://old.reddit.com/user/sandman_br</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using VSCODO, CLINE, OLLAMA + deepcoder, and the code generation is very slow. But my CPU is at 80% and my GPU is at 5%.&lt;/p&gt; &lt;p&gt;Any clues why it is so slow and why the CPU is way heavily used than the GPU (RTX4070)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandman_br"&gt; /u/sandman_br &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqgfk8/high_cpu_and_low_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqgfk8/high_cpu_and_low_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kqgfk8/high_cpu_and_low_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T16:42:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqc20q</id>
    <title>Log auto analysis</title>
    <updated>2025-05-19T13:44:16+00:00</updated>
    <author>
      <name>/u/wizz772</name>
      <uri>https://old.reddit.com/user/wizz772</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SO I am working on a project and my aim is to figure out failures bases on error logs using AI,&lt;/p&gt; &lt;p&gt;I'm currently storing the logs with the manual analysis in a vector db&lt;/p&gt; &lt;p&gt;I plan on using ollama -&amp;gt; llama as a RAG for auto analysis how do I introduce RL and rate whether the output by RAG was good or not and better the output&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wizz772"&gt; /u/wizz772 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqc20q/log_auto_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqc20q/log_auto_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kqc20q/log_auto_analysis/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T13:44:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpr3nu</id>
    <title>My Godot game is using Ollama+LLama 3.1 to act as the Game Master</title>
    <updated>2025-05-18T18:40:49+00:00</updated>
    <author>
      <name>/u/According-Moose2931</name>
      <uri>https://old.reddit.com/user/According-Moose2931</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kpr3nu/my_godot_game_is_using_ollamallama_31_to_act_as/"&gt; &lt;img alt="My Godot game is using Ollama+LLama 3.1 to act as the Game Master" src="https://b.thumbs.redditmedia.com/fSFiY10TeECMbnJQvqKSTbwzh3rhXykErr_0HZIiO4Y.jpg" title="My Godot game is using Ollama+LLama 3.1 to act as the Game Master" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/According-Moose2931"&gt; /u/According-Moose2931 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kpr3nu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpr3nu/my_godot_game_is_using_ollamallama_31_to_act_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kpr3nu/my_godot_game_is_using_ollamallama_31_to_act_as/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-18T18:40:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq8bcj</id>
    <title>Any lightweight AI model for ollama that can be trained to do queries and read software manuals?</title>
    <updated>2025-05-19T10:29:02+00:00</updated>
    <author>
      <name>/u/Palova98</name>
      <uri>https://old.reddit.com/user/Palova98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;I will explain myself better here. &lt;/p&gt; &lt;p&gt;I work for an IT company that integrates an accountability software with basically no public knowledge. &lt;/p&gt; &lt;p&gt;We would like to train an AI that we can feed all the internal PDF manuals and the database structure so we can ask him to make queries for us and troubleshoot problems with the software (ChatGPT found a way to give the model access to a Microsoft SQL server, though I just read this information, still have to actually try) . &lt;/p&gt; &lt;p&gt;Sadly we have a few servers in our datacenter but they are all classic old-ish Xeon CPUs with, of course, tens of other VMs running, so when i tried an ollama docker container with llama3 it takes several minutes for the engine to answer anything. (16 vCPUs and 24G RAM). &lt;/p&gt; &lt;p&gt;So, now that you know the contest, I'm here to ask:&lt;/p&gt; &lt;p&gt;1) Does Ollama have better, lighter models than llama3 to do read and learn pdf manuals and read data from a database via query? &lt;/p&gt; &lt;p&gt;2) What kind of hardware do i need to make it usable? any embedded board like Nvidia's Orin Nano Super Dev kit can work? a mini-pc with an i9? A freakin' 5090 or some other serious GPU? &lt;/p&gt; &lt;p&gt;Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Palova98"&gt; /u/Palova98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kq8bcj/any_lightweight_ai_model_for_ollama_that_can_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kq8bcj/any_lightweight_ai_model_for_ollama_that_can_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kq8bcj/any_lightweight_ai_model_for_ollama_that_can_be/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T10:29:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqdqy7</id>
    <title>Summarizing information in a database</title>
    <updated>2025-05-19T14:55:21+00:00</updated>
    <author>
      <name>/u/newz2000</name>
      <uri>https://old.reddit.com/user/newz2000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm not quite sure the right words to search for. I have a sqlite database with a record of important customer communication. I would like to attempt to search it with a local llm and have been using Ollama on other projects successfully.&lt;/p&gt; &lt;p&gt;I can run SQL queries on the data and I have created a python tool that can create a report. But I'd like to take it to the next level. For example:&lt;/p&gt; &lt;p&gt;* When was it that I talked to Jack about his pricing questions?&lt;/p&gt; &lt;p&gt;* Who was it that said they had a child graduating this spring?&lt;/p&gt; &lt;p&gt;* Have I missed any important follow-ups from the last week?&lt;/p&gt; &lt;p&gt;I have Gemini as part of Google Workspace and my first thought was that I can create a Google Doc per person and then use Gemini to query it. This is possible, but since the data is constantly changing, this is actually harder than it sounds.&lt;/p&gt; &lt;p&gt;Any tips on how to find relevant info?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newz2000"&gt; /u/newz2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqdqy7/summarizing_information_in_a_database/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqdqy7/summarizing_information_in_a_database/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kqdqy7/summarizing_information_in_a_database/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T14:55:21+00:00</published>
  </entry>
</feed>
