<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-06-04T15:31:49+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1l1or5y</id>
    <title>DeepSeek-R1-0528</title>
    <updated>2025-06-02T17:54:58+00:00</updated>
    <author>
      <name>/u/cipherninjabyte</name>
      <uri>https://old.reddit.com/user/cipherninjabyte</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Reading at the hype about this particular model, downloaded it to my ollama server and tried it. I did use it, and unload it in openwebui. After more than 15 mins, it released cpu and memory. until then it was occupying more than 50% cpu. Is this expected? I also have other models locally but they release cpu immediately after I unload it manually.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cipherninjabyte"&gt; /u/cipherninjabyte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1or5y/deepseekr10528/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1or5y/deepseekr10528/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l1or5y/deepseekr10528/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-02T17:54:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1dq4o</id>
    <title>Uncensored Image Recognition Ai</title>
    <updated>2025-06-02T09:36:54+00:00</updated>
    <author>
      <name>/u/Zailor_s</name>
      <uri>https://old.reddit.com/user/Zailor_s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there, &lt;/p&gt; &lt;p&gt;I want to be able to give a pdf etc. file to the Ai and have it analyze the content and be able to describe it correctly. &lt;/p&gt; &lt;p&gt;I tried a lot of models, but they either describe something that doesnt exist or they cant describe images with censored content. &lt;/p&gt; &lt;p&gt;I want to run it the easiest way possible i.e. right now its via cmdâ€¦ and there is only 16gb of ram available. &lt;/p&gt; &lt;p&gt;There has to be something for this, but I could not find it yet. Pls help&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zailor_s"&gt; /u/Zailor_s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1dq4o/uncensored_image_recognition_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1dq4o/uncensored_image_recognition_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l1dq4o/uncensored_image_recognition_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-02T09:36:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1px5v</id>
    <title>Ollama models context</title>
    <updated>2025-06-02T18:38:56+00:00</updated>
    <author>
      <name>/u/airfryier0303456</name>
      <uri>https://old.reddit.com/user/airfryier0303456</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there, I'm struggling to get info about how context work based on hardware. I got 16 gb ram and etc 3060, running some small models quite smooth, i.e., llama 3.2, but the problem is context. Is I go further than 4k tokens, it just miss what was before those 4k tokens, and only &amp;quot;remembers&amp;quot; that last part. I'm implementing it via python with the API. Am I missing something? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/airfryier0303456"&gt; /u/airfryier0303456 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1px5v/ollama_models_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1px5v/ollama_models_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l1px5v/ollama_models_context/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-02T18:38:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1z8eq</id>
    <title>More multimodals please</title>
    <updated>2025-06-03T01:14:09+00:00</updated>
    <author>
      <name>/u/rorowhat</name>
      <uri>https://old.reddit.com/user/rorowhat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can we get more model support?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rorowhat"&gt; /u/rorowhat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1z8eq/more_multimodals_please/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1z8eq/more_multimodals_please/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l1z8eq/more_multimodals_please/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-03T01:14:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1l283bz</id>
    <title>Internet Access?</title>
    <updated>2025-06-03T10:08:45+00:00</updated>
    <author>
      <name>/u/6969_42</name>
      <uri>https://old.reddit.com/user/6969_42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have stopped using services such as ChatGPT and Grok due to privacy concerns. I dont want my prompts to be used to train data nor do I like all the censorship. Searching online I found Ollama and read that its all ran locally. I then downloaded an abliterated version of dolphin 3 and then asked it if it had access to the internet. It said that it did and that its running securely in the cloud. So does that mean that it is collecting my prompts to use for training? Is it not actually local and running without internet like I thought?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/6969_42"&gt; /u/6969_42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l283bz/internet_access/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l283bz/internet_access/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l283bz/internet_access/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-03T10:08:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2g2kz</id>
    <title>is ollama malware?</title>
    <updated>2025-06-03T16:15:55+00:00</updated>
    <author>
      <name>/u/3d_printing_kid</name>
      <uri>https://old.reddit.com/user/3d_printing_kid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently downloaded onto my new computer which was working fine until i downloaded it. first chrome stopped working i had to (for some reason) rename it? i dont really have any incriminating evidence and i really like the project and would sopport it, but i just want to know if other have had these issues before.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3d_printing_kid"&gt; /u/3d_printing_kid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2g2kz/is_ollama_malware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2g2kz/is_ollama_malware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l2g2kz/is_ollama_malware/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-03T16:15:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1i0b7</id>
    <title>What is the best LLM to run locally?</title>
    <updated>2025-06-02T13:26:55+00:00</updated>
    <author>
      <name>/u/Intelligent_Pop_4973</name>
      <uri>https://old.reddit.com/user/Intelligent_Pop_4973</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;PC specs:&lt;br /&gt; i7 12700&lt;br /&gt; 32 GB RAM&lt;br /&gt; RTX 3060 12G&lt;br /&gt; 1TB NVME&lt;/p&gt; &lt;p&gt;i need a universal llm like chatgpt but run locally&lt;/p&gt; &lt;p&gt;P.S im an absolute noob in LLMs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Intelligent_Pop_4973"&gt; /u/Intelligent_Pop_4973 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1i0b7/what_is_the_best_llm_to_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1i0b7/what_is_the_best_llm_to_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l1i0b7/what_is_the_best_llm_to_run_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-02T13:26:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1ximz</id>
    <title>Chrome extension</title>
    <updated>2025-06-02T23:49:38+00:00</updated>
    <author>
      <name>/u/CombatRaccoons</name>
      <uri>https://old.reddit.com/user/CombatRaccoons</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have ollama running on a server within my network. Im looking for a good chrome extension kinda like orion-ui. The problem im having is most chrome extension dont have an option to select a custom ollama host and point directly to http:/localhost:11434. Mine isnt local so this doesnt work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombatRaccoons"&gt; /u/CombatRaccoons &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1ximz/chrome_extension/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1ximz/chrome_extension/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l1ximz/chrome_extension/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-02T23:49:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1rc4i</id>
    <title>Use offline voice controlled agents to search and browse the internet with a contextually aware LLM in the next version of AI Runner</title>
    <updated>2025-06-02T19:33:11+00:00</updated>
    <author>
      <name>/u/w00fl35</name>
      <uri>https://old.reddit.com/user/w00fl35</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l1rc4i/use_offline_voice_controlled_agents_to_search_and/"&gt; &lt;img alt="Use offline voice controlled agents to search and browse the internet with a contextually aware LLM in the next version of AI Runner" src="https://external-preview.redd.it/aGpxZzlhZTZnazRmMcjVXgRLpZ5qhPQ96q4r0xpE25NahzVeLWn0o9J3ntg5.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff2d7e11c5abfec502109e9a5067a085cfc62605" title="Use offline voice controlled agents to search and browse the internet with a contextually aware LLM in the next version of AI Runner" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w00fl35"&gt; /u/w00fl35 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c8ocnz9pek4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1rc4i/use_offline_voice_controlled_agents_to_search_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l1rc4i/use_offline_voice_controlled_agents_to_search_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-02T19:33:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1l28cuf</id>
    <title>Ollama for Playlist name</title>
    <updated>2025-06-03T10:25:17+00:00</updated>
    <author>
      <name>/u/Old_Rock_9457</name>
      <uri>https://old.reddit.com/user/Old_Rock_9457</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone,&lt;br /&gt; I'm writing a python script for analyzing all the song in my library (with Essentia-Tensorflow) and cluster them to create multiple playlist (with scikit-learn).&lt;br /&gt; Now I would like to use Ollama LLM models to analyze the playlist created and assign some name that have sense.&lt;/p&gt; &lt;p&gt;Because this kind of stuff should run on homelab I would like to find a model that can run on low spec PC without external CPU, like my HP Mini with i5-6500, 16GB RAM, SSD and the integrated intel CPU.&lt;/p&gt; &lt;p&gt;What model do you suggest to use? Is there any way to take advantages to the integrated CPU?&lt;/p&gt; &lt;p&gt;It's not important if the model is high responsive, because will be something that run in batch. So even if it take a couple of minutes to reply it's totally fine (of course if it take 1 hours, become to long).&lt;/p&gt; &lt;p&gt;Also I'm using a promt like this, any suggestion to improve it?&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;These songs are selected to have similar genre, mood, bmp or other characteristics. &amp;quot; &amp;quot;Given the primary categories '{feature1} {feature2}', suggest only 1 concise, creative, and memorable playlist name. &amp;quot; &amp;quot;The generated name ABSOLUTELY MUST include both '{feature1}' and '{feature2}', but integrate them creatively, not just by directly re-using the tags. &amp;quot; &amp;quot;Keep the playlist name concise and not excessively long. &amp;quot; &amp;quot;The full category is '{category_name}' where the last feature is BPM&amp;quot; &amp;quot;GOOD EXAMPLE: For '80S Rock', a good name is 'Festive 80S Rock &amp;amp; Pop Mix'. &amp;quot; &amp;quot;GOOD EXAMPLE: For 'Ambient Electronic', a good name is 'Ambitive Electronic Experimental Fast'. &amp;quot; &amp;quot;BAD EXAMPLE: If categories are '80S Rock', do NOT suggest 'Midnight Pop Fever'. &amp;quot; &amp;quot;BAD EXAMPLE: If categories are 'Ambient Electronic', do NOT suggest 'Ambient Electronic - Electric Soundscapes - Ambient Artists, Tracks &amp;amp; Emotional Waves' (it's too long and verbose). &amp;quot; &amp;quot;BAD EXAMPLE: If categories are 'Blues Rock', do NOT suggest 'Blues Rock - Fast' (it's too direct and not creative enough). &amp;quot; &amp;quot;Your response MUST be ONLY the playlist name. Do NOT include any introductory or concluding remarks, explanations, bullet points, bolding, or any other formatting. Just the name.&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;feature and category_name are tags that essentia-tenworflow assign to the playlist and are what I'm actually using for the playlist name, so I have something like:&lt;br /&gt; - Electronic_Dance_Pop_Medium&lt;br /&gt; Instrumental_Jazz_Rock_Medium&lt;/p&gt; &lt;p&gt;I would like that the LLM starting from this title/feature and the list of songs name&amp;amp;arstist (generally 40 for each playlist) it assign some more evocative name.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old_Rock_9457"&gt; /u/Old_Rock_9457 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l28cuf/ollama_for_playlist_name/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l28cuf/ollama_for_playlist_name/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l28cuf/ollama_for_playlist_name/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-03T10:25:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1tjbb</id>
    <title>ðŸ’» I optimized Qwen3:30B MoE to run on my RTX 3070 laptop at ~24 tok/s - full breakdown inside</title>
    <updated>2025-06-02T20:59:35+00:00</updated>
    <author>
      <name>/u/kekePower</name>
      <uri>https://old.reddit.com/user/kekePower</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;br /&gt; I spent an evening tuning the &lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B"&gt;Qwen3:30B (Unsloth) MoE model&lt;/a&gt; on my RTX 3070 (8 GB) laptop using Ollama, and ended up squeezing out 24 tokens per second with a clean 8192 context â€” without hitting unified memory or frying my fans.&lt;/p&gt; &lt;p&gt;What started as a quick test turned into a deep dive on VRAM limits, layer offloading, and how Ollamaâ€™s Modelfile + CUDA backend work under the hood. I also benchmarked a bunch of smaller models like Qwen3 4B, Cogito 8B, Phi-4 Mini, and Gemma3 4Bâ€”itâ€™s all in there.&lt;/p&gt; &lt;p&gt;The post includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Exact Modelfiles for Qwen3 (Unsloth)&lt;/li&gt; &lt;li&gt;Comparison table: tok/s, layers, VRAM, context&lt;/li&gt; &lt;li&gt;Thermal and latency analysis&lt;/li&gt; &lt;li&gt;How to fix Unslothâ€™s Qwen3 to support &lt;code&gt;think&lt;/code&gt; / &lt;code&gt;no_think&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;ðŸ”— Full write-up here: &lt;a href="https://blog.kekepower.com/blog/2025/jun/02/optimizing_qwen3_large_language_models_on_a_consumer_rtx_3070_laptop.html"&gt;https://blog.kekepower.com/blog/2025/jun/02/optimizing_qwen3_large_language_models_on_a_consumer_rtx_3070_laptop.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If youâ€™ve tried similar optimizations or found other models that play nicely with 8 GB cards, Iâ€™d love to hear about it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kekePower"&gt; /u/kekePower &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1tjbb/i_optimized_qwen330b_moe_to_run_on_my_rtx_3070/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1tjbb/i_optimized_qwen330b_moe_to_run_on_my_rtx_3070/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l1tjbb/i_optimized_qwen330b_moe_to_run_on_my_rtx_3070/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-02T20:59:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2duya</id>
    <title>Strange memory usage</title>
    <updated>2025-06-03T14:48:58+00:00</updated>
    <author>
      <name>/u/vir_db</name>
      <uri>https://old.reddit.com/user/vir_db</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;I'm trying to use &lt;a href="https://ollama.com/jobautomation"&gt;jobautomation&lt;/a&gt;/&lt;a href="https://ollama.com/jobautomation/OpenEuroLLM-Italian"&gt;OpenEuroLLM-Italian&lt;/a&gt; model from JobAutomation suite. It's based on Gemma3 and is just 12.2B parameters (8.1GB).&lt;/p&gt; &lt;p&gt;I usually run Gemma3:27b (17GB) or Qwen3:32b (20 GB) without issues on my 3090 24GB card. They run 100% from GPU flawlessly.&lt;/p&gt; &lt;p&gt;But running &lt;strong&gt;OpenEuroLLM-Italian&lt;/strong&gt;, it runs only 18% from GPU and I cannot understand why.&lt;br /&gt; Somebody have any clue?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vir_db"&gt; /u/vir_db &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2duya/strange_memory_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2duya/strange_memory_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l2duya/strange_memory_usage/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-03T14:48:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2hspp</id>
    <title>Memory Leak on Linux</title>
    <updated>2025-06-03T17:22:41+00:00</updated>
    <author>
      <name>/u/GhostInThePudding</name>
      <uri>https://old.reddit.com/user/GhostInThePudding</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed what seems to be a memory leak for a while now (at least since 0.7.6, but maybe before as well and I just wasn't paying attention). I'm running Ollama on Linux Mint with an Nvidia GPU. I noticed sometimes when using Ollama, a large chunk of RAM shows as in use in System Monitor/Free/HTOP, but it isn't associated with any process or shared memory or anything I can find. Then when Ollama stops running (and there are no models running, or I restart the service), the memory still isn't freed.&lt;/p&gt; &lt;p&gt;I tried logging out, killing all the relevant processes, trying to hunt how what the memory is being used for, but it just won't free up or show what is using it.&lt;/p&gt; &lt;p&gt;If I then start using Ollama again, it won't reuse that memory and models will start using more memory instead, eventually getting to the point where I can have 20 or more GB of &amp;quot;used&amp;quot; RAM that isn't in use by any actual process and then running a model that uses the rest of my RAM will cause the OOM system to shutdown the current Ollama model, but still leave all that other memory in use.&lt;/p&gt; &lt;p&gt;Only a reboot ever frees the memory.&lt;/p&gt; &lt;p&gt;I'm currently running 0.9.0 and still have the same problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GhostInThePudding"&gt; /u/GhostInThePudding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2hspp/memory_leak_on_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2hspp/memory_leak_on_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l2hspp/memory_leak_on_linux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-03T17:22:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1l31hj8</id>
    <title>Please tell me a under 4B uncensored language model</title>
    <updated>2025-06-04T09:49:50+00:00</updated>
    <author>
      <name>/u/Successful-Dark-3297</name>
      <uri>https://old.reddit.com/user/Successful-Dark-3297</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Successful-Dark-3297"&gt; /u/Successful-Dark-3297 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l31hj8/please_tell_me_a_under_4b_uncensored_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l31hj8/please_tell_me_a_under_4b_uncensored_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l31hj8/please_tell_me_a_under_4b_uncensored_language/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-04T09:49:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2907k</id>
    <title>Best Ollama Models for Tools</title>
    <updated>2025-06-03T11:04:00+00:00</updated>
    <author>
      <name>/u/Material_Ad_2783</name>
      <uri>https://old.reddit.com/user/Material_Ad_2783</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm looking for advices to choose the best model for Ollama when using tools.&lt;/p&gt; &lt;p&gt;With ChatGPT4o it work's perfectly but working on edge it's really complicated.&lt;/p&gt; &lt;p&gt;I tested the latest Phi4-Mini for instance&lt;/p&gt; &lt;ul&gt; &lt;li&gt;JSON output explained in the prompt is not correctly fill. Missing required fields, ..&lt;/li&gt; &lt;li&gt;Never use it or too much. Hard to dÃ©cidÃ© which tool to use.&lt;/li&gt; &lt;li&gt;Fields content are not relevant and sometimes it hallucinate on fonction names.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We are far from Home Automation to control various IoT devices :-(&lt;/p&gt; &lt;p&gt;I read people &amp;quot;hard code&amp;quot; input/output to improve the results but ... It's not scalable. We need something that behave close to GPT4o.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Material_Ad_2783"&gt; /u/Material_Ad_2783 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2907k/best_ollama_models_for_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2907k/best_ollama_models_for_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l2907k/best_ollama_models_for_tools/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-03T11:04:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2m8t7</id>
    <title>starting off using Ollama</title>
    <updated>2025-06-03T20:20:30+00:00</updated>
    <author>
      <name>/u/Available-Ad1878</name>
      <uri>https://old.reddit.com/user/Available-Ad1878</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey I'm a masters student working in clinical research as a side project while im in school. &lt;/p&gt; &lt;p&gt;one of the post docs in my lab told me to use Ollama to process our data and output graphs + written papers as well. the way they do this is basically by uploading huge files of data that we have extracted from surgery records (looking at times vs outcomes vs costs of materials etc.) alongside papers on similar topics and previous papers from the lab to their Ollama and then prompting it heavily until they get what they need. some of the data is HIPAA protected as well, so im rly too sure about how this works but they told me that its fine to use it as long as its locally hosted and not in the cloud. &lt;/p&gt; &lt;p&gt;im working on an M2 MacBook Air right now, so let me know if that is going to restrict my usage heavily. but im here just to learn more about what model I should be using and how to go about that. thanks!&lt;/p&gt; &lt;p&gt;I also have to do a ton of reading (journal articles) so if theres models that could help with that in terms of giving me summaries or being able to recall anything I need, that would be great too. I know this is a lot but thanks again!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available-Ad1878"&gt; /u/Available-Ad1878 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2m8t7/starting_off_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2m8t7/starting_off_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l2m8t7/starting_off_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-03T20:20:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2pyrd</id>
    <title>bug in qwen 3 chat template?</title>
    <updated>2025-06-03T22:52:43+00:00</updated>
    <author>
      <name>/u/Expensive-Apricot-25</name>
      <uri>https://old.reddit.com/user/Expensive-Apricot-25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I noticed that when ever qwen 3 calls tools, it thinks that the user called the tool, or is talking to the model. I looked into the chat template and it turns out that for a tool response, it is labeled as a user message:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{{- else if eq .Role &amp;quot;tool&amp;quot; }}&amp;lt;|im_start|&amp;gt;user &amp;lt;tool_response&amp;gt; {{ .Content }} &amp;lt;/tool_response&amp;gt;&amp;lt;|im_end|&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I looked at the chat template for the official qwen page on hugging face, and the `user` marker is not there for a tool response.&lt;/p&gt; &lt;p&gt;Is this a bug? or is this intended behavior?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expensive-Apricot-25"&gt; /u/Expensive-Apricot-25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2pyrd/bug_in_qwen_3_chat_template/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2pyrd/bug_in_qwen_3_chat_template/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l2pyrd/bug_in_qwen_3_chat_template/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-03T22:52:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2i71o</id>
    <title>Is anyone productively using Aider and Ollama together?</title>
    <updated>2025-06-03T17:37:58+00:00</updated>
    <author>
      <name>/u/chanfle12</name>
      <uri>https://old.reddit.com/user/chanfle12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was experimenting with Aider yesterday and discovered a potential bug with its Ollama support. It appears the available models are hardcoded, and Aider isn't fetching the list of models directly from Ollama. This makes it seem broken.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Aider-AI/aider/issues/3081"&gt;https://github.com/Aider-AI/aider/issues/3081&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Is anyone else successfully using Aider with Ollama? If not, what alternatives are people using for local LLM integration?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chanfle12"&gt; /u/chanfle12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2i71o/is_anyone_productively_using_aider_and_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2i71o/is_anyone_productively_using_aider_and_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l2i71o/is_anyone_productively_using_aider_and_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-03T17:37:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2xq0p</id>
    <title>Locally downloading Qwen pretrained weights for finetuning</title>
    <updated>2025-06-04T05:35:49+00:00</updated>
    <author>
      <name>/u/hendy0</name>
      <uri>https://old.reddit.com/user/hendy0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm trying to load the pretrained weights of LLMs (Qwen2.5-0.5B for now) into a custom model architecture I created manually. I'm trying to mimic &lt;a href="https://github.com/rasbt/LLMs-from-scratch/blob/c278745aff419ae6c1d6409ca4279aa57ea749e4/ch06/01_main-chapter-code/previous_chapters.py#L251"&gt;this code&lt;/a&gt;. However, I wasn't able to find the checkpoints of the pretrained model online. Could someone help me with that or refer me to a place where I can load the pretrained weights? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hendy0"&gt; /u/hendy0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2xq0p/locally_downloading_qwen_pretrained_weights_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2xq0p/locally_downloading_qwen_pretrained_weights_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l2xq0p/locally_downloading_qwen_pretrained_weights_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-04T05:35:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1l36kvb</id>
    <title>smollm is crazy</title>
    <updated>2025-06-04T14:11:39+00:00</updated>
    <author>
      <name>/u/3d_printing_kid</name>
      <uri>https://old.reddit.com/user/3d_printing_kid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l36kvb/smollm_is_crazy/"&gt; &lt;img alt="smollm is crazy" src="https://external-preview.redd.it/NG9mN3N2ZGw0eDRmMYC1oXT879drMGhz7A_iST_bdDJ62X2-qbCshqC67I28.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=faaff70ac5a371991a4c0aa1609505f081603776" title="smollm is crazy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i was bored one day so i dicided to run smollm 135 m parameters. here is a video of the result:&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3d_printing_kid"&gt; /u/3d_printing_kid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fesotwdl4x4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l36kvb/smollm_is_crazy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l36kvb/smollm_is_crazy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-04T14:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2r6kb</id>
    <title>Building an extension that lets you try ANY clothing on with AI. Open sourcing it...</title>
    <updated>2025-06-03T23:48:11+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l2r6kb/building_an_extension_that_lets_you_try_any/"&gt; &lt;img alt="Building an extension that lets you try ANY clothing on with AI. Open sourcing it..." src="https://external-preview.redd.it/Y3IyeXJ5Zmt1czRmMZEFLO9nCJikC7mtBpPcIQAr59c4sK2P034UkenC8j1x.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=577a0bce4c774caddeb043ad3e991d8b058d5489" title="Building an extension that lets you try ANY clothing on with AI. Open sourcing it..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/iy5wdxekus4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2r6kb/building_an_extension_that_lets_you_try_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l2r6kb/building_an_extension_that_lets_you_try_any/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-03T23:48:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1l38ign</id>
    <title>geekom a6 mini PC 32gb ram *internal gpu* r7 6800h</title>
    <updated>2025-06-04T15:29:03+00:00</updated>
    <author>
      <name>/u/3d_printing_kid</name>
      <uri>https://old.reddit.com/user/3d_printing_kid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ok so what is the best llm i could run at maybe 5 tokens/second? also how do i make it use my integrated graphics?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3d_printing_kid"&gt; /u/3d_printing_kid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l38ign/geekom_a6_mini_pc_32gb_ram_internal_gpu_r7_6800h/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l38ign/geekom_a6_mini_pc_32gb_ram_internal_gpu_r7_6800h/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l38ign/geekom_a6_mini_pc_32gb_ram_internal_gpu_r7_6800h/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-04T15:29:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2vm2g</id>
    <title>AI Runner v4.11.0: web browsing with contextually aware agent + search via duckduckgo</title>
    <updated>2025-06-04T03:32:55+00:00</updated>
    <author>
      <name>/u/w00fl35</name>
      <uri>https://old.reddit.com/user/w00fl35</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yesterday I showed you a preview of the web browser tool I was working on for my AI Runner application. Today I have released it with &lt;a href="https://github.com/Capsize-Games/airunner/releases/tag/v4.11.0"&gt;v4.11.0 - you can see the full release notes here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Some key changes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The LLM can search via duckduckgo without an API key. The search can be extended to include other search engines (and will be in upcoming releases).&lt;/li&gt; &lt;li&gt;Integrated web browser with private browsing, bookmarks, history, keyboard controls and most importantly a contextually aware LLM&lt;/li&gt; &lt;li&gt;Completely reworked the chat area which was very sluggish in previous versions. Now its fast.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There are some known bugs&lt;/p&gt; &lt;ul&gt; &lt;li&gt;chat doesn't always show up on first load&lt;/li&gt; &lt;li&gt;browser is in its alpha stage - i tried to make it robust, but it probably needs some polish&lt;/li&gt; &lt;li&gt;the LLM will screw up a lot right now&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll be working on everything heavily over the next couple of days and will update you as I release. If you want a more stable LLM experience use a version prior to v4.11.0, but polishing the agent and giving it more tools is my primary focus for the next few days.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;AI Runner is a desktop application I built with Python. It allows you to run AI models offline on your own hardware. You can generate images, have voice conversations, create custom bots, and much more.&lt;/p&gt; &lt;p&gt;Check it out and if you like what you see, consider supporting the project by giving me a star.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Capsize-Games/airunner"&gt;https://github.com/Capsize-Games/airunner&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w00fl35"&gt; /u/w00fl35 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2vm2g/ai_runner_v4110_web_browsing_with_contextually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2vm2g/ai_runner_v4110_web_browsing_with_contextually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l2vm2g/ai_runner_v4110_web_browsing_with_contextually/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-04T03:32:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1l36vlk</id>
    <title>What are some features missing from the Ollama API that you would like to see?</title>
    <updated>2025-06-04T14:24:13+00:00</updated>
    <author>
      <name>/u/TheBroseph69</name>
      <uri>https://old.reddit.com/user/TheBroseph69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I plan on building an improved API for Ollama that would have features not currently found in the Ollama API. What are some features youâ€™d like to see?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheBroseph69"&gt; /u/TheBroseph69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l36vlk/what_are_some_features_missing_from_the_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l36vlk/what_are_some_features_missing_from_the_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l36vlk/what_are_some_features_missing_from_the_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-04T14:24:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2qdt2</id>
    <title>Ollama Video Editor</title>
    <updated>2025-06-03T23:11:21+00:00</updated>
    <author>
      <name>/u/AdamHYE</name>
      <uri>https://old.reddit.com/user/AdamHYE</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l2qdt2/ollama_video_editor/"&gt; &lt;img alt="Ollama Video Editor" src="https://preview.redd.it/9bqo8jd0os4f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7b0ba4f6ffdd7bcf5ef30581ec7a4a27d8affd60" title="Ollama Video Editor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Created an Ollama MCP to give ffmpegâ€™s advanced video/audio editing to an agent.&lt;/p&gt; &lt;p&gt;Runs 100% locally. React Vite frontend, Node Express mcp, Python Flask backend, simple Ollama agent. Scaffolded by Dyad.&lt;/p&gt; &lt;p&gt;When Iâ€™m ready to do sophisticated editing, Iâ€™ll wire this up to CrewAI. But if you just want to do single command requests, itâ€™s solid. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/hyepartners-gmail/vibevideo-mcp"&gt;https://github.com/hyepartners-gmail/vibevideo-mcp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdamHYE"&gt; /u/AdamHYE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9bqo8jd0os4f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l2qdt2/ollama_video_editor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l2qdt2/ollama_video_editor/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-03T23:11:21+00:00</published>
  </entry>
</feed>
