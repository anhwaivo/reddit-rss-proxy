<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-19T18:38:23+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1isp73j</id>
    <title>multiple models</title>
    <updated>2025-02-18T22:32:33+00:00</updated>
    <author>
      <name>/u/w38122077</name>
      <uri>https://old.reddit.com/user/w38122077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it possible with ollama to have two models running and each be available on a different port? I can run two and interact with them via the command line, but I can't seem to figure out how to have them available concurrently to Visual Code for use with chat and tab autocomplete &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w38122077"&gt; /u/w38122077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isp73j/multiple_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isp73j/multiple_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isp73j/multiple_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T22:32:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1is8g74</id>
    <title>DeepSeek or MS Bing AI?</title>
    <updated>2025-02-18T08:57:50+00:00</updated>
    <author>
      <name>/u/Wizard_of_Awes</name>
      <uri>https://old.reddit.com/user/Wizard_of_Awes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1is8g74/deepseek_or_ms_bing_ai/"&gt; &lt;img alt="DeepSeek or MS Bing AI?" src="https://a.thumbs.redditmedia.com/XX5Qjey4O_bAKYgdR-2EUjRrM2XntcwNHsjW11xn4V8.jpg" title="DeepSeek or MS Bing AI?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wizard_of_Awes"&gt; /u/Wizard_of_Awes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1is8g74"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is8g74/deepseek_or_ms_bing_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is8g74/deepseek_or_ms_bing_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T08:57:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1issdkb</id>
    <title>Cheapest way to host local model</title>
    <updated>2025-02-19T00:45:44+00:00</updated>
    <author>
      <name>/u/pur_ling_leaf</name>
      <uri>https://old.reddit.com/user/pur_ling_leaf</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m running a small startup and now trying to find an option how to smoothly shift from OpenAI API models to self-hosted. I checked Google cloud offerings, but they only have an enterprise plans for gpu renting. My questions: 1. What is the cheapest way to host local model? 2. If I host 32b model that requires 20gb of VRAM on a server with 22gb of VRAM, will this server be able to process requests to the model? 3. For small web-service, is it cheaper to go with OpenAI API or self-hosted model? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pur_ling_leaf"&gt; /u/pur_ling_leaf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1issdkb/cheapest_way_to_host_local_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1issdkb/cheapest_way_to_host_local_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1issdkb/cheapest_way_to_host_local_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T00:45:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1issp7p</id>
    <title>Modelfile PARAMETER Min/Max Values?</title>
    <updated>2025-02-19T01:00:07+00:00</updated>
    <author>
      <name>/u/AwJa44</name>
      <uri>https://old.reddit.com/user/AwJa44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The docs/modelfile.md page is nice, but it does not list the possible ranges of values for each parameter (e.g., mirostat_eta). Does anyone know where to find those? They just say float or int and give some defaults for some of them. &lt;/p&gt; &lt;p&gt;I've pored through the code a bit and found that the floats are 32-bit, and that 0 disables many of these. But, even the sparse code comments give no clue about minimum/maximum values possible.&lt;/p&gt; &lt;p&gt;When I see a 0.8, is that just assumed I should know that means the value should be between 0 and 1.0? (Should it?)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AwJa44"&gt; /u/AwJa44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1issp7p/modelfile_parameter_minmax_values/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1issp7p/modelfile_parameter_minmax_values/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1issp7p/modelfile_parameter_minmax_values/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T01:00:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1isispb</id>
    <title>Can I create an LLM model that will behave strictly the way I want?</title>
    <updated>2025-02-18T17:52:46+00:00</updated>
    <author>
      <name>/u/Nuvola_Rossa</name>
      <uri>https://old.reddit.com/user/Nuvola_Rossa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to create LLM models that can be uploaded locally. Those models should speak in a certain way or should only consider certain knowledge. For example, I want to create a model that would answer like a medieval man.&lt;/p&gt; &lt;p&gt;The idea is to have a chatbot for which the user shouldn't have to prompt anything specific for it to behave that way (for example if the user asks &amp;quot;what happen during world war II&amp;quot;, the model should answer &amp;quot;I have no idea&amp;quot; or something like that). &lt;/p&gt; &lt;p&gt;I would like to have several AI model that could be loaded in order to compare the answers. And i will need a GGUF (this is not optional) of each model.&lt;/p&gt; &lt;p&gt;I've been looking around for a way to do this but I can't find any way out of it. Any ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nuvola_Rossa"&gt; /u/Nuvola_Rossa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isispb/can_i_create_an_llm_model_that_will_behave/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isispb/can_i_create_an_llm_model_that_will_behave/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isispb/can_i_create_an_llm_model_that_will_behave/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T17:52:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1is22ug</id>
    <title>Most cost effective way of hosting 70B/32B param model</title>
    <updated>2025-02-18T02:29:31+00:00</updated>
    <author>
      <name>/u/topsy_here</name>
      <uri>https://old.reddit.com/user/topsy_here</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious &lt;/p&gt; &lt;p&gt;It comes down to efficiency. I see it like crypto mining. It’s about getting the best token count for least cost.&lt;/p&gt; &lt;p&gt;There’s Mac minis I’ve seen hosting the 72B param one. You gonna need about 8x of them which is about 3.5K usd each? &lt;/p&gt; &lt;p&gt;What about hosting on a VPS in Linus? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/topsy_here"&gt; /u/topsy_here &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is22ug/most_cost_effective_way_of_hosting_70b32b_param/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is22ug/most_cost_effective_way_of_hosting_70b32b_param/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is22ug/most_cost_effective_way_of_hosting_70b32b_param/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T02:29:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1isvaad</id>
    <title>wllama on crappy Chromebooks</title>
    <updated>2025-02-19T03:01:43+00:00</updated>
    <author>
      <name>/u/Over-Leader-9494</name>
      <uri>https://old.reddit.com/user/Over-Leader-9494</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;Apologies in advance as this relates to Wllama - &lt;a href="https://huggingface.co/spaces/ngxson/wllama"&gt;https://huggingface.co/spaces/ngxson/wllama&lt;/a&gt; A nifty way to use llama.cpp via WebAssembly to run in browser memory. &lt;/p&gt; &lt;p&gt;&amp;quot;Wllama is a project based on &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt;. It enables running LLM inference directly on browser by leveraging the power of &lt;strong&gt;WebAssembly&lt;/strong&gt;. It accepts GGUF as model format.&amp;quot;&lt;/p&gt; &lt;p&gt;In any case, I am trying to get this running on the crapiest hardware I could find and luckily a teacher is my friend and I have a 3.74 GB Celeron 3965Y @ 1.5 GHz... about $70 used these days &lt;a href="https://www.ebay.com/itm/176475517458"&gt;https://www.ebay.com/itm/176475517458&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It downloads, but freezes there.&lt;/p&gt; &lt;p&gt;Trying to set up a demo for kids at my daughter's school and prefer to use hardware they already have to help them understand SLM's.&lt;/p&gt; &lt;p&gt;Much appreciate if folks had any insights or experience here. This one may be a bridge to far, but I can hope.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Over-Leader-9494"&gt; /u/Over-Leader-9494 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isvaad/wllama_on_crappy_chromebooks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isvaad/wllama_on_crappy_chromebooks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isvaad/wllama_on_crappy_chromebooks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T03:01:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1it6ojw</id>
    <title>Great job, Deepseek! You really understood your instruction!</title>
    <updated>2025-02-19T14:25:33+00:00</updated>
    <author>
      <name>/u/identicalBadger</name>
      <uri>https://old.reddit.com/user/identicalBadger</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1it6ojw/great_job_deepseek_you_really_understood_your/"&gt; &lt;img alt="Great job, Deepseek! You really understood your instruction!" src="https://preview.redd.it/on0a0crbv3ke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd477b2cf5d92e20088ccaab37fea87beae67f77" title="Great job, Deepseek! You really understood your instruction!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/identicalBadger"&gt; /u/identicalBadger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/on0a0crbv3ke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it6ojw/great_job_deepseek_you_really_understood_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1it6ojw/great_job_deepseek_you_really_understood_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T14:25:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1isr8zl</id>
    <title>After great pains (learning curve), got llama.cpp running on my older AMD GPU (since Ollama isn’t compatible)…but the two things I want to use Ollama with don’t “communicate” with it in the way they do Ollama. HomeAssistant and Frigate use Ollama at port 11434, llama.cpp doesn’t have that…help?</title>
    <updated>2025-02-18T23:55:58+00:00</updated>
    <author>
      <name>/u/FantasyMaster85</name>
      <uri>https://old.reddit.com/user/FantasyMaster85</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've got an older AMD GPU that is running llama.cpp (built with Vulcan and fully utilizing my GPU...an RX 570) along with the given sub 4gb models at a perfectly acceptable TPS for my two use cases (HomeAssistant and Frigate), as tested manually running llama-server and passing queries to it manually. &lt;/p&gt; &lt;p&gt;The issue is that while both HomeAssistant and Frigate have a means to work with Ollama at port 11434, I can't for the life of me figure out how to expose the same functionality using llama.cpp...is it even possible?&lt;/p&gt; &lt;p&gt;I've tried llama-server using llama.cpp and it doesn't work with HomeAssistant or Frigate, despite the web UI created by it working fine (seems that's an &amp;quot;openAI&amp;quot; API versus the &amp;quot;Ollama&amp;quot; style API exposed by Ollama. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FantasyMaster85"&gt; /u/FantasyMaster85 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isr8zl/after_great_pains_learning_curve_got_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isr8zl/after_great_pains_learning_curve_got_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isr8zl/after_great_pains_learning_curve_got_llamacpp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T23:55:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1it28bf</id>
    <title>Running DeepSeek 70B</title>
    <updated>2025-02-19T10:16:02+00:00</updated>
    <author>
      <name>/u/Zockerdude15</name>
      <uri>https://old.reddit.com/user/Zockerdude15</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My current Setup is a System with an RTX 4090, a 7800X3D and 64 GB RAM. I can run anything up to 32B just fine with my 4090, however none of the 70B Models seem to utilize my GPU (Q4/Q2, even with gpu offloading Parameters set). Would it be possible to add something like a 4060Ti 16GB for it to work fully on gpu‘s ? Or would a dedicated system with something like 4 3060 12GB work better ? Current t/s on just CPU is about 1.2-1.5, which is too Slow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zockerdude15"&gt; /u/Zockerdude15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it28bf/running_deepseek_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it28bf/running_deepseek_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1it28bf/running_deepseek_70b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T10:16:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1it2ahn</id>
    <title>Bluetooth air quality analysis using Gemma AI ( source code available)</title>
    <updated>2025-02-19T10:20:13+00:00</updated>
    <author>
      <name>/u/bleuio</name>
      <uri>https://old.reddit.com/user/bleuio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1it2ahn/bluetooth_air_quality_analysis_using_gemma_ai/"&gt; &lt;img alt="Bluetooth air quality analysis using Gemma AI ( source code available)" src="https://external-preview.redd.it/UHpmaNAE0LOr_rVvObZX6INnvcJ0rTYBSLeN_WR3cBc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f7d44b7e7f75cb06354268111d61b224ef3b42b" title="Bluetooth air quality analysis using Gemma AI ( source code available)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bleuio"&gt; /u/bleuio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bleuio.com/blog/chat-with-hibouair-using-bleuio-smart-air-quality-analysis-with-google-technologies/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it2ahn/bluetooth_air_quality_analysis_using_gemma_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1it2ahn/bluetooth_air_quality_analysis_using_gemma_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T10:20:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1it2dag</id>
    <title>How to run llama3.1 on CPU only?</title>
    <updated>2025-02-19T10:25:31+00:00</updated>
    <author>
      <name>/u/Fantastic-Method2046</name>
      <uri>https://old.reddit.com/user/Fantastic-Method2046</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have latest ollama installed on a laptop with RTX3050 GPU. Now I'd like to run LLM inference (for example with previously downloaded llama3.1) on CPU only. Please help. I tried many things found on the internet. Some of them not works some of them runs with errors (for example Error: unknown flag: --num-gpu) etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fantastic-Method2046"&gt; /u/Fantastic-Method2046 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it2dag/how_to_run_llama31_on_cpu_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it2dag/how_to_run_llama31_on_cpu_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1it2dag/how_to_run_llama31_on_cpu_only/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T10:25:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1isuyd4</id>
    <title>8x AMD Instinct Mi50 AI Server #1 is in Progress..</title>
    <updated>2025-02-19T02:45:22+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1isuyd4/8x_amd_instinct_mi50_ai_server_1_is_in_progress/"&gt; &lt;img alt="8x AMD Instinct Mi50 AI Server #1 is in Progress.." src="https://preview.redd.it/q7q2q1n970ke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=051ed274f0a297d2b44d83c02fd2cc80ce6d0b7f" title="8x AMD Instinct Mi50 AI Server #1 is in Progress.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q7q2q1n970ke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isuyd4/8x_amd_instinct_mi50_ai_server_1_is_in_progress/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isuyd4/8x_amd_instinct_mi50_ai_server_1_is_in_progress/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T02:45:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1isqhm1</id>
    <title>Ollama Shell -- improved Terminal app for using local models</title>
    <updated>2025-02-18T23:22:47+00:00</updated>
    <author>
      <name>/u/sunkencity999</name>
      <uri>https://old.reddit.com/user/sunkencity999</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1isqhm1/ollama_shell_improved_terminal_app_for_using/"&gt; &lt;img alt="Ollama Shell -- improved Terminal app for using local models" src="https://b.thumbs.redditmedia.com/i1OWSPjS_iJXLlQo0MDCNlUzL5p_Rm9K-O4rQsmb8NM.jpg" title="Ollama Shell -- improved Terminal app for using local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y'all,&lt;/p&gt; &lt;p&gt;I am personally a huge fan of working directly in the terminal; the existing terminal shell for Ollama, in my opinion, leaves much to be desired, functionality and aesthetics-wise. SO, I figured I would create a Shell application that allows you to work with Ollama and models in the terminal in a way that is practical and reasonably efficient. You can analyze documents by dragging-and-dropping them in the chat, manage models (pull and delete), have continuous chat history and save system prompts for use as necessary. If working in the terminal / shell is something you enjoy as well, please give it a shot. Free, and of course I welcome contributors.&lt;br /&gt; &lt;a href="https://github.com/sunkencity999/ollama_shell"&gt;Ollama Shell on Github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ym0w7cmtizje1.png?width=1071&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=427fbffce0d8ba45e5906e3342f5aa1028bcd959"&gt;Main Interface&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tpfh7k4vizje1.png?width=1059&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0fa8276a0e793475538033c655452323b68354bc"&gt;Prompt selection after Model selection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n2mhbjjxizje1.png?width=920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d99ceda3ceb10582f95be390b8bf8b12a3fc641e"&gt;Query answered by LLM and provided (deepseek-r1:14b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunkencity999"&gt; /u/sunkencity999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isqhm1/ollama_shell_improved_terminal_app_for_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isqhm1/ollama_shell_improved_terminal_app_for_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isqhm1/ollama_shell_improved_terminal_app_for_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T23:22:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1it5szj</id>
    <title>How to build and run Ollama on PPC64LE systems</title>
    <updated>2025-02-19T13:45:08+00:00</updated>
    <author>
      <name>/u/icbts</name>
      <uri>https://old.reddit.com/user/icbts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1it5szj/how_to_build_and_run_ollama_on_ppc64le_systems/"&gt; &lt;img alt="How to build and run Ollama on PPC64LE systems" src="https://external-preview.redd.it/3XRNKRY7mw1IR8KHZ4Jh9lT_fAkGQsxOhoQnqIF98fQ.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a44da49e0908c1fdf43aef80ad4a34700ca7834c" title="How to build and run Ollama on PPC64LE systems" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/icbts"&gt; /u/icbts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=P4iEZiwfLm8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it5szj/how_to_build_and_run_ollama_on_ppc64le_systems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1it5szj/how_to_build_and_run_ollama_on_ppc64le_systems/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T13:45:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1it4gxn</id>
    <title>Run LLM on 5090 vs 3090 - how the 5090 performs running deepseek-r1 using Ollama?</title>
    <updated>2025-02-19T12:36:14+00:00</updated>
    <author>
      <name>/u/chain-77</name>
      <uri>https://old.reddit.com/user/chain-77</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1it4gxn/run_llm_on_5090_vs_3090_how_the_5090_performs/"&gt; &lt;img alt="Run LLM on 5090 vs 3090 - how the 5090 performs running deepseek-r1 using Ollama?" src="https://external-preview.redd.it/Ou-vKSYlV1cbaUGSZy0hMZHyER-_tVW8jE9HgZJ1UnA.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4669ccb1227cf76d727ffbf02d810dd6ad02d58c" title="Run LLM on 5090 vs 3090 - how the 5090 performs running deepseek-r1 using Ollama?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chain-77"&gt; /u/chain-77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/YxJYhhrhrDk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it4gxn/run_llm_on_5090_vs_3090_how_the_5090_performs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1it4gxn/run_llm_on_5090_vs_3090_how_the_5090_performs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T12:36:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ita4ph</id>
    <title>deepseek and ollama to create knowledge graphs</title>
    <updated>2025-02-19T16:50:05+00:00</updated>
    <author>
      <name>/u/Short-Honeydew-7000</name>
      <uri>https://old.reddit.com/user/Short-Honeydew-7000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ita4ph/deepseek_and_ollama_to_create_knowledge_graphs/"&gt; &lt;img alt="deepseek and ollama to create knowledge graphs" src="https://external-preview.redd.it/4ijyY7Qu-05QDvrnzoIwg803mKYJkGjXXn0YyImxyhE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6da44814a5c90a2b4de967ae82c0dd3e786dc7d2" title="deepseek and ollama to create knowledge graphs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Short-Honeydew-7000"&gt; /u/Short-Honeydew-7000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cognee.ai/blog/deep-dives/deepseek-ollama-and-graphs"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ita4ph/deepseek_and_ollama_to_create_knowledge_graphs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ita4ph/deepseek_and_ollama_to_create_knowledge_graphs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T16:50:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1itaci0</id>
    <title>Creating a model that will instrinsically behave the way I want no matter the prompt</title>
    <updated>2025-02-19T16:58:44+00:00</updated>
    <author>
      <name>/u/Nuvola_Rossa</name>
      <uri>https://old.reddit.com/user/Nuvola_Rossa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need to create a model that will behave the way I want (= talking in a certain way) without any prompt engineering, no matter what the user request is. I can achieve this using the modelfile and mofying the system prompt on ollama, but then I still don't have a GGUF file that I can export (this is mandatory for my study case)... &lt;/p&gt; &lt;p&gt;So I don't really need any training as any generic model (I'm using llama3.2) has all the knowledge I want already and I don't know what to do. Any advices?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nuvola_Rossa"&gt; /u/Nuvola_Rossa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itaci0/creating_a_model_that_will_instrinsically_behave/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itaci0/creating_a_model_that_will_instrinsically_behave/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1itaci0/creating_a_model_that_will_instrinsically_behave/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T16:58:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1it5ob9</id>
    <title>Model for object detection with bounding box</title>
    <updated>2025-02-19T13:38:44+00:00</updated>
    <author>
      <name>/u/PertinentOverthinker</name>
      <uri>https://old.reddit.com/user/PertinentOverthinker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there, i am newbie when it comes to computer vision and AI. I am wondering if there is AI model that can detect object of interest and draw the bounding box around it or give the coordinate of the bounding box to be plotted separately&lt;/p&gt; &lt;p&gt;thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PertinentOverthinker"&gt; /u/PertinentOverthinker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it5ob9/model_for_object_detection_with_bounding_box/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it5ob9/model_for_object_detection_with_bounding_box/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1it5ob9/model_for_object_detection_with_bounding_box/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T13:38:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1itbr79</id>
    <title>Optimal Hardware for Running Ollama Models with Marker for PDF to Markdown Conversion</title>
    <updated>2025-02-19T17:52:59+00:00</updated>
    <author>
      <name>/u/BigdadEdge</name>
      <uri>https://old.reddit.com/user/BigdadEdge</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I'm planning to convert large PDFs, like textbooks, into Markdown using the Marker tool in conjunction with Ollama's local LLMs. Given this setup, what hardware specifications would you recommend? Specifically, I'm interested in:&lt;/p&gt; &lt;p&gt;- The most suitable Ollama model for this task for the minimal hardware requirements, I still want the Ollama model to be fast, but I do not want to spend too much money on online computation when renting a server. &lt;/p&gt; &lt;p&gt;- Minimum and recommended CPU and RAM requirements&lt;/p&gt; &lt;p&gt;- The necessity and impact of a GPU on performance&lt;/p&gt; &lt;p&gt;Any insights or experiences would be greatly appreciated!&lt;/p&gt; &lt;p&gt;You can check out the [&lt;a href="https://github.com/VikParuchuri/marker"&gt;Marker GitHub repository&lt;/a&gt;] for more details on the project.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BigdadEdge"&gt; /u/BigdadEdge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itbr79/optimal_hardware_for_running_ollama_models_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itbr79/optimal_hardware_for_running_ollama_models_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1itbr79/optimal_hardware_for_running_ollama_models_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T17:52:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1itcpq4</id>
    <title>Specialized model without notice in Model desc on site.</title>
    <updated>2025-02-19T18:30:46+00:00</updated>
    <author>
      <name>/u/HeadGr</name>
      <uri>https://old.reddit.com/user/HeadGr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've downloaded &lt;strong&gt;3logic/llama-3.1-8b-instruct-phactual_fp16_he20&lt;/strong&gt; and found it was trained for assist patiens with single certain decease. When I asked &amp;quot;What d'you know about Alpha Centauri&amp;quot; model answered kinda &amp;quot;I know that Alpha Centauri is star system but I have no idea how it related to Diabetes type 2&amp;quot;. &lt;/p&gt; &lt;p&gt;Don't waste time and 16 Gb of disk space if you don't need exactly this assistant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HeadGr"&gt; /u/HeadGr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itcpq4/specialized_model_without_notice_in_model_desc_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itcpq4/specialized_model_without_notice_in_model_desc_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1itcpq4/specialized_model_without_notice_in_model_desc_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T18:30:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1itcvxk</id>
    <title>Seeking Recommendations on Open-Source RAG Frameworks</title>
    <updated>2025-02-19T18:37:39+00:00</updated>
    <author>
      <name>/u/BigdadEdge</name>
      <uri>https://old.reddit.com/user/BigdadEdge</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I’ve been exploring the &lt;a href="https://github.com/Mintplex-Labs/anything-llm"&gt;Anything LLM GitHub repository&lt;/a&gt; for LLM-based retrieval methods. However, it does not support advanced RAG techniques like Hybrid, Graph, or Agentic RAG. I'm looking for open-source frameworks or GitHub projects that implement these advanced methods. Any guidance on choosing the right tools for handling more complex data and tasks would be greatly appreciated.&lt;/p&gt; &lt;p&gt;Best regards,&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BigdadEdge"&gt; /u/BigdadEdge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itcvxk/seeking_recommendations_on_opensource_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itcvxk/seeking_recommendations_on_opensource_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1itcvxk/seeking_recommendations_on_opensource_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T18:37:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1istkgr</id>
    <title>Does Ollama cache prompts?</title>
    <updated>2025-02-19T01:40:06+00:00</updated>
    <author>
      <name>/u/palaceofcesi</name>
      <uri>https://old.reddit.com/user/palaceofcesi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay I’m a little confused and freaked out right now but my first thought is that I didn’t read the documentation properly.&lt;/p&gt; &lt;p&gt;Does Ollama cache prompts? &lt;/p&gt; &lt;p&gt;I previously used the deepseek-r1:32B with Ollama to create a presentation about a business product, call it Product A.&lt;/p&gt; &lt;p&gt;Then I used deepseek to create a presentation about Product B. In my prompt “ollama run deepseek-r1:32b $prompt” I made no reference whatsoever to Product A. And yet, in its response, I received multiple references to Product A in my creating a presentation for Product B. &lt;/p&gt; &lt;p&gt;The model was praising how well these two products work together.&lt;/p&gt; &lt;p&gt;That’s great, but I was not aware of any prompt caching in Ollama. This has a huge security implication because I’m running Ollama on sensitive documents on internal networks of non-air-gapped systems so if Ollama is caching the prompts and/or outputs and potentially uploading them over the network that would be a huge security risk.&lt;/p&gt; &lt;p&gt;Can someone tell me what’s going on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/palaceofcesi"&gt; /u/palaceofcesi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1istkgr/does_ollama_cache_prompts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1istkgr/does_ollama_cache_prompts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1istkgr/does_ollama_cache_prompts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T01:40:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1it71ge</id>
    <title>Ollama Portable Zip for Intel GPU has now come to Linux</title>
    <updated>2025-02-19T14:41:50+00:00</updated>
    <author>
      <name>/u/bigbigmind</name>
      <uri>https://old.reddit.com/user/bigbigmind</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;a href="https://github.com/intel/ipex-llm/releases/tag/v2.2.0-nightly"&gt;Download &lt;/a&gt;and unzip&lt;/li&gt; &lt;li&gt;./start-ollama.sh&lt;/li&gt; &lt;li&gt;./ollama run deepseek-r1:7b&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;See the guide &lt;a href="https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portablze_zip_quickstart.md"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigbigmind"&gt; /u/bigbigmind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it71ge/ollama_portable_zip_for_intel_gpu_has_now_come_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it71ge/ollama_portable_zip_for_intel_gpu_has_now_come_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1it71ge/ollama_portable_zip_for_intel_gpu_has_now_come_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T14:41:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1isvd1h</id>
    <title>Ollama Deepseek-R1 AI writes my Obsidian notes by watching my screen (open source)</title>
    <updated>2025-02-19T03:05:26+00:00</updated>
    <author>
      <name>/u/louis3195</name>
      <uri>https://old.reddit.com/user/louis3195</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1isvd1h/ollama_deepseekr1_ai_writes_my_obsidian_notes_by/"&gt; &lt;img alt="Ollama Deepseek-R1 AI writes my Obsidian notes by watching my screen (open source)" src="https://external-preview.redd.it/NGI4NHl2aTBpMGtlMaEzGEUaUQonH8wQBp1lwNMA6DwRZxj8nxN2XpHfUUNy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6aa25cc908b1aaf94262c3fa735cebef775b5e9c" title="Ollama Deepseek-R1 AI writes my Obsidian notes by watching my screen (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/louis3195"&gt; /u/louis3195 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1eo4wui0i0ke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isvd1h/ollama_deepseekr1_ai_writes_my_obsidian_notes_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isvd1h/ollama_deepseekr1_ai_writes_my_obsidian_notes_by/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T03:05:26+00:00</published>
  </entry>
</feed>
