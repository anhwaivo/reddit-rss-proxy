<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-26T20:48:36+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1i9qnvn</id>
    <title>How to use deepseek r1 alongside a game engine?</title>
    <updated>2025-01-25T16:28:10+00:00</updated>
    <author>
      <name>/u/Game-Lover44</name>
      <uri>https://old.reddit.com/user/Game-Lover44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How might one use r1 too makes games with a game engine. i tried godot with the 7b model but it dosent work and makes up its own programming lauguage. so what game engine can i use with deepseek r1? im looking for a free solution that has a interface.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Game-Lover44"&gt; /u/Game-Lover44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9qnvn/how_to_use_deepseek_r1_alongside_a_game_engine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9qnvn/how_to_use_deepseek_r1_alongside_a_game_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9qnvn/how_to_use_deepseek_r1_alongside_a_game_engine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T16:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9zz8u</id>
    <title>8x AMD Instinct Mi60 Server + vLLM + DeepSeek-R1-Qwen-14B-FP16</title>
    <updated>2025-01-25T23:19:35+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jxwbmm4l38fe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9zz8u/8x_amd_instinct_mi60_server_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9zz8u/8x_amd_instinct_mi60_server_vllm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T23:19:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia5sl7</id>
    <title>How to put image as input to deepseep r1?</title>
    <updated>2025-01-26T04:24:21+00:00</updated>
    <author>
      <name>/u/Spark0411</name>
      <uri>https://old.reddit.com/user/Spark0411</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have PC which does not have internet but I need to extract texts from the images. Is it possible to give image as input any llm model (deepseep r1 or etc) ? If you know fully offline OCR application, then that may also work but I will prefer llm model as it can be used for multi purpose &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spark0411"&gt; /u/Spark0411 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ia5sl7/how_to_put_image_as_input_to_deepseep_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ia5sl7/how_to_put_image_as_input_to_deepseep_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ia5sl7/how_to_put_image_as_input_to_deepseep_r1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T04:24:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia6tkw</id>
    <title>I connected llama3.2 to an IRC server. It was hilarious. Tell me what conversation I should make it have with itself. I have gotten 3 clients plus myself working at once.</title>
    <updated>2025-01-26T05:26:23+00:00</updated>
    <author>
      <name>/u/malformed-packet</name>
      <uri>https://old.reddit.com/user/malformed-packet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ia6tkw/i_connected_llama32_to_an_irc_server_it_was/"&gt; &lt;img alt="I connected llama3.2 to an IRC server. It was hilarious. Tell me what conversation I should make it have with itself. I have gotten 3 clients plus myself working at once." src="https://external-preview.redd.it/H4Bh2eY4U0J1DjNkHHXDisrlI59Jd9-BDZ41Xgi_oSU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6f88627b5a91ac681f815ef765b2409f9671098" title="I connected llama3.2 to an IRC server. It was hilarious. Tell me what conversation I should make it have with itself. I have gotten 3 clients plus myself working at once." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/malformed-packet"&gt; /u/malformed-packet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/longjoel/llama-bot-framework"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ia6tkw/i_connected_llama32_to_an_irc_server_it_was/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ia6tkw/i_connected_llama32_to_an_irc_server_it_was/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T05:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iabnph</id>
    <title>How to Generate Random RPG Character Names with an LLM</title>
    <updated>2025-01-26T10:56:48+00:00</updated>
    <author>
      <name>/u/Inevitable-Judge2642</name>
      <uri>https://old.reddit.com/user/Inevitable-Judge2642</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iabnph/how_to_generate_random_rpg_character_names_with/"&gt; &lt;img alt="How to Generate Random RPG Character Names with an LLM" src="https://external-preview.redd.it/iKnEKLG7VTAX2ecjFNkIHoIan9TbV0PXQvNHWAl_Kwk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=17c0af3fb105bc100fc7c148671e7ebed2fca4c2" title="How to Generate Random RPG Character Names with an LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Judge2642"&gt; /u/Inevitable-Judge2642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://k33g.hashnode.dev/how-to-generate-random-rpg-character-names-with-an-llm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iabnph/how_to_generate_random_rpg_character_names_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iabnph/how_to_generate_random_rpg_character_names_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T10:56:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9vzey</id>
    <title>RTX 4060 8GB vs RX 7600 XT 16GB for running 14B parameter models (Phi4, DeepSeek R1 Q4 quantized)</title>
    <updated>2025-01-25T20:19:08+00:00</updated>
    <author>
      <name>/u/fugxto</name>
      <uri>https://old.reddit.com/user/fugxto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm currently looking into upgrading my GPU and I'm wondering which one would be better suited for running 14B parameter models like Phi4 or DeepSeek R1, specifically the Q4 quantized versions. I'm deciding between the NVIDIA RTX 4060 (8GB VRAM) and the AMD Radeon RX 7600 XT (16GB VRAM).&lt;/p&gt; &lt;p&gt;Given that these models can be pretty demanding in terms of both VRAM and processing power, which one do you think would offer better performance for these tasks?&lt;/p&gt; &lt;p&gt;One concern I have is the state of ROCm for AMD GPUs, as I’m not familiar with its current support and compatibility with models like these.&lt;/p&gt; &lt;p&gt;Thanks in advance for your insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fugxto"&gt; /u/fugxto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9vzey/rtx_4060_8gb_vs_rx_7600_xt_16gb_for_running_14b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9vzey/rtx_4060_8gb_vs_rx_7600_xt_16gb_for_running_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9vzey/rtx_4060_8gb_vs_rx_7600_xt_16gb_for_running_14b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T20:19:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1iac061</id>
    <title>ollama-grid-search for Vision Model ?! Any alternative ?</title>
    <updated>2025-01-26T11:16:15+00:00</updated>
    <author>
      <name>/u/elgeekphoenix</name>
      <uri>https://old.reddit.com/user/elgeekphoenix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I really love the ollama-grid-search (&lt;a href="https://github.com/dezoito/ollama-grid-search"&gt;https://github.com/dezoito/ollama-grid-search&lt;/a&gt;) to compare side by side Text queries.&lt;/p&gt; &lt;p&gt;I wanted to know if there is any fork or alternative to compare vision (image) models ?&lt;/p&gt; &lt;p&gt;I know Msty doing it for text, and Open Webui doing it as an arena , but didn't see anything close for image input .&lt;/p&gt; &lt;p&gt;Thanks a lot&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elgeekphoenix"&gt; /u/elgeekphoenix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iac061/ollamagridsearch_for_vision_model_any_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iac061/ollamagridsearch_for_vision_model_any_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iac061/ollamagridsearch_for_vision_model_any_alternative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T11:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9smk3</id>
    <title>Running Deepseek-r1 7b distilled model locally in a PC with no GPU with Ollama.</title>
    <updated>2025-01-25T17:53:06+00:00</updated>
    <author>
      <name>/u/Secraciesmeet</name>
      <uri>https://old.reddit.com/user/Secraciesmeet</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secraciesmeet"&gt; /u/Secraciesmeet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9smk3/running_deepseekr1_7b_distilled_model_locally_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9smk3/running_deepseekr1_7b_distilled_model_locally_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9smk3/running_deepseekr1_7b_distilled_model_locally_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T17:53:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1iadcye</id>
    <title>Trying to get ui-tars-desktop running with local vlm and lm studios</title>
    <updated>2025-01-26T12:31:13+00:00</updated>
    <author>
      <name>/u/Designer_Award_2551</name>
      <uri>https://old.reddit.com/user/Designer_Award_2551</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried to get ui-tars desktop running with lm studios. But I can't get the program working.&lt;/p&gt; &lt;p&gt;Lm studio is up and running, loaded deepseek vlm. Got the local server running, but the settings of ui-tars do not allow me to connect to it? It only has a setting for hugging face (which is a paid service?) Do I mis anything? Or isn't it possible right now. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Designer_Award_2551"&gt; /u/Designer_Award_2551 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iadcye/trying_to_get_uitarsdesktop_running_with_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iadcye/trying_to_get_uitarsdesktop_running_with_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iadcye/trying_to_get_uitarsdesktop_running_with_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T12:31:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaermz</id>
    <title>LOCAL MODEL RECOMMENDATION</title>
    <updated>2025-01-26T13:39:22+00:00</updated>
    <author>
      <name>/u/potatopcuser4ever</name>
      <uri>https://old.reddit.com/user/potatopcuser4ever</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which model should i run locally with a laptop having i5 12th gen,8gb ram and only integrated intel gpu&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/potatopcuser4ever"&gt; /u/potatopcuser4ever &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaermz/local_model_recommendation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaermz/local_model_recommendation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaermz/local_model_recommendation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T13:39:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia71y4</id>
    <title>Deepseek_r1 considers “think” instead of “thiink”. Will the quantization and training parameters affect the performance? I am using 1.5b ; q4_k_m</title>
    <updated>2025-01-26T05:41:06+00:00</updated>
    <author>
      <name>/u/themoneylust</name>
      <uri>https://old.reddit.com/user/themoneylust</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ia71y4/deepseek_r1_considers_think_instead_of_thiink/"&gt; &lt;img alt="Deepseek_r1 considers “think” instead of “thiink”. Will the quantization and training parameters affect the performance? I am using 1.5b ; q4_k_m" src="https://preview.redd.it/heirhacyz9fe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef07b08bdb610da3b5499d2d699942ba0782c3eb" title="Deepseek_r1 considers “think” instead of “thiink”. Will the quantization and training parameters affect the performance? I am using 1.5b ; q4_k_m" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/themoneylust"&gt; /u/themoneylust &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/heirhacyz9fe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ia71y4/deepseek_r1_considers_think_instead_of_thiink/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ia71y4/deepseek_r1_considers_think_instead_of_thiink/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T05:41:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9zv0u</id>
    <title>Deepseek R1 + WebUI energy costs?</title>
    <updated>2025-01-25T23:13:58+00:00</updated>
    <author>
      <name>/u/DelPrive235</name>
      <uri>https://old.reddit.com/user/DelPrive235</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm pretty new to this. I am in the process of installing Browser Use Web UI. I'm considering using it with Deepseek R1 14b. Running locally on a MBP M1 Max 64GB ram. Will this be significantly more expensive in energy costs than using WebUI with GPT 4o? Am I likely to notice a spike in my energy costs using R1 14b? Would it be any less expensive to just use the Deepseek API running onto their servers? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DelPrive235"&gt; /u/DelPrive235 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9zv0u/deepseek_r1_webui_energy_costs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9zv0u/deepseek_r1_webui_energy_costs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9zv0u/deepseek_r1_webui_energy_costs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T23:13:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1iahxcs</id>
    <title>Difficulty Downloading DeepSeek Model from Ollama Due to DNS Resolution Error</title>
    <updated>2025-01-26T15:51:56+00:00</updated>
    <author>
      <name>/u/greyrabbit-21021420</name>
      <uri>https://old.reddit.com/user/greyrabbit-21021420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I'm encountering persistent issues while trying to download the latest DeepSeek model from Ollama. The download progresses to about 20-30% before resetting back to 0%, and eventually fails with the following error:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; ollama pull deepseek-r1:32b pulling manifest pulling 6150cb382311... 0% ▕ ▏ 0 B/ 19 GB Error: max retries exceeded: Get &amp;quot;https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/61/6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=66040c77ac1b787c3af820529859349a%2F20250126%2Fauto%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20250126T143543Z&amp;amp;X-Amz-Expires=86400&amp;amp;X-Amz-SignedHeaders=host&amp;amp;X-Amz-Signature=28801b0c9d00c2c0cc383eac2333e77446286deb3005c483aafd2328513a7690&amp;quot;: dial tcp: lookup dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com on 127.0.0.53:53: read udp 127.0.0.1:34287-&amp;gt;127.0.0.53:53: i/o timeout &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Steps I've already taken include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Verifying that my internet connection is stable and other downloads work fine.&lt;/li&gt; &lt;li&gt;Ensuring that I'm using the latest version of Ollama.&lt;/li&gt; &lt;li&gt;Checking firewall and proxy settings to ensure they aren't interfering with the connection.&lt;/li&gt; &lt;li&gt;Restarted my computer (several times) and reset my Network settings .&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Despite these efforts, the issue persists. Has anyone else experienced similar problems or have additional suggestions on how to resolve this? I am using Ubuntu 20.04 .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/greyrabbit-21021420"&gt; /u/greyrabbit-21021420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iahxcs/difficulty_downloading_deepseek_model_from_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iahxcs/difficulty_downloading_deepseek_model_from_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iahxcs/difficulty_downloading_deepseek_model_from_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T15:51:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iadttf</id>
    <title>Total noob question (qwen-coder2.5 7b): Generated a clock with image, but image not loading</title>
    <updated>2025-01-26T12:55:10+00:00</updated>
    <author>
      <name>/u/Dull-Dragonfly-6851</name>
      <uri>https://old.reddit.com/user/Dull-Dragonfly-6851</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;i am a total beginner when it comes to AI and coding. I have tried to get &lt;a href="http://bolt.diy"&gt;bolt.diy&lt;/a&gt; and qwen-cloder2.5 7B to work locally and it does. Somehow &lt;a href="http://bolt.diy"&gt;bolt.diy&lt;/a&gt; with qwen-coder2.5 is running very slow, but qwen-coder2.5 in open-webui is running very fast - but thats only a side-note.&lt;/p&gt; &lt;p&gt;Please remember, a total noob is asking:&lt;/p&gt; &lt;p&gt;My main question is, if its possible to somehow see the image directly in the preview in qwen-coder2.5/bolt diy, because right now when i ask qwen-coder &amp;quot;Create a 1 page website with a clock that is updating every second&amp;quot; it does create the clock and shows a preview of it. When i then ask &amp;quot;add a ai generated donkey that blinks every time the clock gets updated&amp;quot; it does add a image, but this image is not loading, like its missing.&lt;/p&gt; &lt;p&gt;I understand that its missing because there is no path to that image, but i saw this video: &lt;a href="https://www.youtube.com/watch?v=197FcjcZ22A&amp;amp;t=1531s"&gt;https://www.youtube.com/watch?v=197FcjcZ22A&amp;amp;t=1531s&lt;/a&gt; and there the guy does almost the same and it show a ai generated donkey image. Due to the fact that he works pretty fast and only the code is overlayed displayed i cannot see what he exactly is doing.&lt;/p&gt; &lt;p&gt;So, is there a way to get it done? I hope you understand what i mean.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dull-Dragonfly-6851"&gt; /u/Dull-Dragonfly-6851 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iadttf/total_noob_question_qwencoder25_7b_generated_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iadttf/total_noob_question_qwencoder25_7b_generated_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iadttf/total_noob_question_qwencoder25_7b_generated_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T12:55:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaez8g</id>
    <title>Outdate document about python-llama-cpp</title>
    <updated>2025-01-26T13:49:12+00:00</updated>
    <author>
      <name>/u/wo-tatatatatata</name>
      <uri>https://old.reddit.com/user/wo-tatatatatata</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp/"&gt;https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the document in the link above is outdated and would not work, anyone knows how i can use local model from ollama instead in this example?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wo-tatatatatata"&gt; /u/wo-tatatatatata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaez8g/outdate_document_about_pythonllamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaez8g/outdate_document_about_pythonllamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaez8g/outdate_document_about_pythonllamacpp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T13:49:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaksh1</id>
    <title>need help choosing entry level gpu</title>
    <updated>2025-01-26T17:40:01+00:00</updated>
    <author>
      <name>/u/SnooBananas5215</name>
      <uri>https://old.reddit.com/user/SnooBananas5215</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am new to learning local llm. I have i5 -10400 &amp;amp; 550 PSU with 32 gigs of ram running on msi h410 vh pro - supporting pcie 3.0. I am not sure weather to go for rtx 4070 12 gb (undervolt) or rtx 4060 ti 16gb, which one would be better&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooBananas5215"&gt; /u/SnooBananas5215 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaksh1/need_help_choosing_entry_level_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaksh1/need_help_choosing_entry_level_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaksh1/need_help_choosing_entry_level_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T17:40:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1iao84u</id>
    <title>[Deepseek r1] REPLACETheseStringsWithCorrespondingValues()</title>
    <updated>2025-01-26T19:45:26+00:00</updated>
    <author>
      <name>/u/vsarunhah</name>
      <uri>https://old.reddit.com/user/vsarunhah</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I just tried running Deepseek r1 1.5b and was just messing around with it and got some weird output that I haven't seen before. Anyone what this is about/where this could've come from? Not sure if it's an ollama thing or an r1 thing.&lt;/p&gt; &lt;p&gt;``` ollama run deepseek-r1:1.5b&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;hello &amp;lt;think&amp;gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;Hello! How can I assist you today? 😊&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;What are you good at &amp;lt;think&amp;gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;Hello! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. For the best results, always do your own research before using AI tools.&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;What are you optimized for? `&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;Hi! I'm DeepSeek-R1, an AI assistant created to help with various tasks and provide information. I optimize my performance based on extensive training data and advanced algorithms to deliver accurate and helpful responses. For more details about our products or services, please visit our official website.&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;What is the most recent data you have been trained on `&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;REPLACETheseStringsWithCorrespondingValues()&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;What is the most recent data you have been trained on? &amp;lt;think&amp;gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;Hi! I'm DeepSeek-R1, an AI assistant created to help with various tasks and provide information. For the best results, always do your own research before using AI tools. If you have any questions or need assistance, feel free to ask!&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;What is the most recent data you have been trained on `&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;REPLACETheseStringsWithCorrespondingValues() ```&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Seems reproducible too, which is pretty weird.&lt;/p&gt; &lt;p&gt;ollama version is 0.5.7&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vsarunhah"&gt; /u/vsarunhah &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iao84u/deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iao84u/deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iao84u/deepseek_r1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T19:45:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iap6ot</id>
    <title>running Ollama on Intet Arc iGPU</title>
    <updated>2025-01-26T20:21:33+00:00</updated>
    <author>
      <name>/u/JV_info</name>
      <uri>https://old.reddit.com/user/JV_info</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a Windows 11 on a mini PC (Geekom GT series G1 Mega) and it has the Intel Arc iGPU, + NPU&lt;br /&gt; I also have a local AI chat and my setup is this (Ollama + Docker + Openwebui)&lt;br /&gt; now I want to use my Intel ARC iGPU but no matter what I do, Ollama ends up running on CPU.&lt;/p&gt; &lt;p&gt;I tried the instruction from Intel, the PDF you shared as well but the problem is that the Intel oneAPI Base Toolkit they use in the instructions is version 2024.1.0 but the version available to download now is 2025.1 and it is not working, I mean when I run the &amp;quot;Ollama serve&amp;quot; it is missing some DLLs that I have but in a higher version... and it's a dead end.&lt;/p&gt; &lt;p&gt;So, I wanted to see if there is a way:&lt;/p&gt; &lt;p&gt;1- downloading the 2024.1.0 version.&lt;/p&gt; &lt;p&gt;2- or any other way to use for my case, Ollama + Docker + Openwebui, so that the intel Arc iGPU or NPU will be used when I use Ollama in OpenwebUi.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JV_info"&gt; /u/JV_info &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iap6ot/running_ollama_on_intet_arc_igpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iap6ot/running_ollama_on_intet_arc_igpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iap6ot/running_ollama_on_intet_arc_igpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T20:21:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9yv89</id>
    <title>Introducing Awesome Open Source AI: A list for tracking great open source models</title>
    <updated>2025-01-25T22:28:29+00:00</updated>
    <author>
      <name>/u/SuccessIsHardWork</name>
      <uri>https://old.reddit.com/user/SuccessIsHardWork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i9yv89/introducing_awesome_open_source_ai_a_list_for/"&gt; &lt;img alt="Introducing Awesome Open Source AI: A list for tracking great open source models" src="https://external-preview.redd.it/GXmJ82vxdZ1DvaPMBQ6TD4TQLzEhsWVdtuXgso5_c_Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce8f5a1c97427023bef612109fcd4409305118f3" title="Introducing Awesome Open Source AI: A list for tracking great open source models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuccessIsHardWork"&gt; /u/SuccessIsHardWork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/suncloudsmoon/awesome-open-source-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i9yv89/introducing_awesome_open_source_ai_a_list_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i9yv89/introducing_awesome_open_source_ai_a_list_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-25T22:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaodk3</id>
    <title>Issues running deepseek</title>
    <updated>2025-01-26T19:51:08+00:00</updated>
    <author>
      <name>/u/crazyT8nn1s</name>
      <uri>https://old.reddit.com/user/crazyT8nn1s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded deepseek-r1 (1.5b) version via ollama and tried running it on the terminal in MacOS.&lt;/p&gt; &lt;p&gt;I typed &amp;quot;Hi&amp;quot; at the prompt and I get nothing back. I just get another prompt. &amp;quot;&amp;gt;&amp;gt;&amp;gt;&amp;quot;&lt;/p&gt; &lt;p&gt;I don't see any &amp;quot;think&amp;quot; or anything of that sort. This is only isolated to deepseek. Other models I don't have this issue.&lt;/p&gt; &lt;p&gt;Any suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crazyT8nn1s"&gt; /u/crazyT8nn1s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaodk3/issues_running_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaodk3/issues_running_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaodk3/issues_running_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T19:51:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1iapk5s</id>
    <title>Can I restrict a model to only run on CPU?</title>
    <updated>2025-01-26T20:35:25+00:00</updated>
    <author>
      <name>/u/mindsetFPS</name>
      <uri>https://old.reddit.com/user/mindsetFPS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mindsetFPS"&gt; /u/mindsetFPS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iapk5s/can_i_restrict_a_model_to_only_run_on_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iapk5s/can_i_restrict_a_model_to_only_run_on_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iapk5s/can_i_restrict_a_model_to_only_run_on_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T20:35:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1iajtdg</id>
    <title>SQL DB to RAG</title>
    <updated>2025-01-26T17:03:23+00:00</updated>
    <author>
      <name>/u/PaymentDramatic6154</name>
      <uri>https://old.reddit.com/user/PaymentDramatic6154</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any good tutorials on how to do this? Looking for some help, everything ive been seeing about RAG is with pdfs or documents. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaymentDramatic6154"&gt; /u/PaymentDramatic6154 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iajtdg/sql_db_to_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iajtdg/sql_db_to_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iajtdg/sql_db_to_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T17:03:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaoigf</id>
    <title>Does deepseek r1 locally support reading images?</title>
    <updated>2025-01-26T19:56:28+00:00</updated>
    <author>
      <name>/u/PawanAgarwal</name>
      <uri>https://old.reddit.com/user/PawanAgarwal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iaoigf/does_deepseek_r1_locally_support_reading_images/"&gt; &lt;img alt="Does deepseek r1 locally support reading images?" src="https://preview.redd.it/ywwq5z6j8efe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc1f5bc8e418faa002c27dfce87f3f7daa503ccb" title="Does deepseek r1 locally support reading images?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PawanAgarwal"&gt; /u/PawanAgarwal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ywwq5z6j8efe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaoigf/does_deepseek_r1_locally_support_reading_images/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaoigf/does_deepseek_r1_locally_support_reading_images/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T19:56:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaht3c</id>
    <title>DeepSeek-R1's Bias</title>
    <updated>2025-01-26T15:47:14+00:00</updated>
    <author>
      <name>/u/cov_id19</name>
      <uri>https://old.reddit.com/user/cov_id19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iaht3c/deepseekr1s_bias/"&gt; &lt;img alt="DeepSeek-R1's Bias" src="https://preview.redd.it/41tjd3220dfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d3331ed008c8f650aba9895a60c4c649bd8ab47d" title="DeepSeek-R1's Bias" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cov_id19"&gt; /u/cov_id19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/41tjd3220dfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaht3c/deepseekr1s_bias/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaht3c/deepseekr1s_bias/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T15:47:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iacoys</id>
    <title>Is there a pure, quantised version of DeekSeekr1 for Ollama?</title>
    <updated>2025-01-26T11:55:10+00:00</updated>
    <author>
      <name>/u/john_alan</name>
      <uri>https://old.reddit.com/user/john_alan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see this: &lt;a href="https://ollama.com/library/deepseek-r1"&gt;https://ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;but it appears to be Llama3.2/Qwen trained with DeepSeek r1.&lt;/p&gt; &lt;p&gt;I essentially want an 8bit 70bn version of pure r1?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/john_alan"&gt; /u/john_alan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iacoys/is_there_a_pure_quantised_version_of_deekseekr1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iacoys/is_there_a_pure_quantised_version_of_deekseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iacoys/is_there_a_pure_quantised_version_of_deekseekr1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T11:55:10+00:00</published>
  </entry>
</feed>
