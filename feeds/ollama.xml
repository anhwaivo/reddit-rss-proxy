<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-05-14T18:25:29+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1kkldfn</id>
    <title>ollama support for qwen3 for tab completion in Continue</title>
    <updated>2025-05-12T06:05:10+00:00</updated>
    <author>
      <name>/u/WiseGuy_240</name>
      <uri>https://old.reddit.com/user/WiseGuy_240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using ollama as LLM server backend for vscode + continue plugin. recently I tried to upgrade to qwen3 for both tab completion as well as main AI agent. the main agent works fine when you ask it questions. However the tab completion does not, because it spits out the thinking process of qwen3 instead of simply coming with code suggest as qwen2.5 did. I have checked the yaml config reference docs at &lt;a href="https://docs.continue.dev/reference"&gt;https://docs.continue.dev/reference&lt;/a&gt; and seems like they only support switching off thinking for Claude: &lt;code&gt;reasoning&lt;/code&gt;: Boolean to enable thinking/reasoning for Anthropic Claude 3.7+ models. I tried it anyways for qwen3 but it does not affect it. Anyone else having this issue? I even tried rules with setting value of non-thinking as suggested in qwens docs but no change. is it something I can do with systems prompts instead?&lt;/p&gt; &lt;p&gt;my config looks like this&lt;/p&gt; &lt;pre&gt;&lt;code&gt;models: - name: qwen3 8b provider: ollama model: qwen3:8b defaultCompletionOptions: reasoning: false roles: - chat - edit - apply - name: qwen3-coder 1.7b provider: ollama model: qwen3:1.7b defaultCompletionOptions: reasoning: false roles: - autocomplete rules: non-thinking &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WiseGuy_240"&gt; /u/WiseGuy_240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kkldfn/ollama_support_for_qwen3_for_tab_completion_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kkldfn/ollama_support_for_qwen3_for_tab_completion_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kkldfn/ollama_support_for_qwen3_for_tab_completion_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-12T06:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl3kg4</id>
    <title>How do I use AMD GPU with mistral-small3.1</title>
    <updated>2025-05-12T20:48:15+00:00</updated>
    <author>
      <name>/u/randomwinterr</name>
      <uri>https://old.reddit.com/user/randomwinterr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have tried everything please help me. I am a total newbie here.&lt;/p&gt; &lt;p&gt;The videos I have tried so far Vid-1 -- &lt;a href="https://youtu.be/G-kpvlvKM1g?si=6Bb8TvuQ-R51wOEy"&gt;https://youtu.be/G-kpvlvKM1g?si=6Bb8TvuQ-R51wOEy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Vid-2 -- &lt;a href="https://youtu.be/211ygEwb9eI?si=slxS8JfXjemEfFXg"&gt;https://youtu.be/211ygEwb9eI?si=slxS8JfXjemEfFXg&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomwinterr"&gt; /u/randomwinterr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kl3kg4/how_do_i_use_amd_gpu_with_mistralsmall31/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kl3kg4/how_do_i_use_amd_gpu_with_mistralsmall31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kl3kg4/how_do_i_use_amd_gpu_with_mistralsmall31/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-12T20:48:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkycut</id>
    <title>Luxembourgish gguf model</title>
    <updated>2025-05-12T17:24:33+00:00</updated>
    <author>
      <name>/u/racoon880</name>
      <uri>https://old.reddit.com/user/racoon880</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‘m new in ollama, i‘m looking for an luxembourgish gguf model for ollama. Can anyone help me to convert a safetensor to gguf? Like LuxemBERT?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/racoon880"&gt; /u/racoon880 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kkycut/luxembourgish_gguf_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kkycut/luxembourgish_gguf_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kkycut/luxembourgish_gguf_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-12T17:24:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl0jzl</id>
    <title>Pre-built PC - suggestions to which</title>
    <updated>2025-05-12T18:49:37+00:00</updated>
    <author>
      <name>/u/Glittering-Koala-750</name>
      <uri>https://old.reddit.com/user/Glittering-Koala-750</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glittering-Koala-750"&gt; /u/Glittering-Koala-750 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1kl0jjv/prebuilt_pc_suggestions_to_which/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kl0jzl/prebuilt_pc_suggestions_to_which/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kl0jzl/prebuilt_pc_suggestions_to_which/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-12T18:49:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kks08z</id>
    <title>How do deploy VLMs on ollama?</title>
    <updated>2025-05-12T13:04:34+00:00</updated>
    <author>
      <name>/u/New_Supermarket_5490</name>
      <uri>https://old.reddit.com/user/New_Supermarket_5490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been trying to deploy a VLM on ollama, specifically UI-tars-1.5 7b which is a finetune of qwen2-vl, and available on ollama here: &lt;a href="https://ollama.com/0000/ui-tars-1.5-7b"&gt;https://ollama.com/0000/ui-tars-1.5-7b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However, it looks like some running it always breaks on image/vision related input/output, getting an error as in &lt;a href="https://github.com/ollama/ollama/issues/8907"&gt;https://github.com/ollama/ollama/issues/8907&lt;/a&gt; which I'm not sure has been fixed?&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Hi @uoakinci qwen2 VL is not yet available in Ollama - how token positions are encoded in a batch didn't work with Ollama's prompt caching. Some initial work was done in #8113(&lt;a href="https://github.com/ollama/ollama/pull/8113"&gt;https://github.com/ollama/ollama/pull/8113&lt;/a&gt;)&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Does anyone have a workaround or has used a qwen2vl on ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Supermarket_5490"&gt; /u/New_Supermarket_5490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kks08z/how_do_deploy_vlms_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kks08z/how_do_deploy_vlms_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kks08z/how_do_deploy_vlms_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-12T13:04:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl0ypb</id>
    <title>How to use images having dimensions larger that 896x896 in gemini3?</title>
    <updated>2025-05-12T19:05:33+00:00</updated>
    <author>
      <name>/u/BioEngineeredCat</name>
      <uri>https://old.reddit.com/user/BioEngineeredCat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m getting inaccurate results for images with resolution of 2454x3300&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BioEngineeredCat"&gt; /u/BioEngineeredCat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kl0ypb/how_to_use_images_having_dimensions_larger_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kl0ypb/how_to_use_images_having_dimensions_larger_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kl0ypb/how_to_use_images_having_dimensions_larger_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-12T19:05:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kleh2m</id>
    <title>getting the following error trying to run qwen3-30b-a3b-q3_k_m off gguf</title>
    <updated>2025-05-13T05:49:13+00:00</updated>
    <author>
      <name>/u/CaptTechno</name>
      <uri>https://old.reddit.com/user/CaptTechno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'qwen3moe'&lt;/p&gt; &lt;p&gt;how do i fix this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CaptTechno"&gt; /u/CaptTechno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kleh2m/getting_the_following_error_trying_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kleh2m/getting_the_following_error_trying_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kleh2m/getting_the_following_error_trying_to_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-13T05:49:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkx8of</id>
    <title>looking for offline LLMs i can train with PDFs and will run on old laptop with no GPU, and &lt;4 GB ram</title>
    <updated>2025-05-12T16:41:02+00:00</updated>
    <author>
      <name>/u/Icy-Expression1567</name>
      <uri>https://old.reddit.com/user/Icy-Expression1567</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried tinyllama but it always hallucinated, give me something that won't hallucinate &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy-Expression1567"&gt; /u/Icy-Expression1567 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kkx8of/looking_for_offline_llms_i_can_train_with_pdfs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kkx8of/looking_for_offline_llms_i_can_train_with_pdfs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kkx8of/looking_for_offline_llms_i_can_train_with_pdfs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-12T16:41:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1klhiqk</id>
    <title>RAG n8n AI Agent using Ollama</title>
    <updated>2025-05-13T09:21:25+00:00</updated>
    <author>
      <name>/u/mehul_gupta1997</name>
      <uri>https://old.reddit.com/user/mehul_gupta1997</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1klhiqk/rag_n8n_ai_agent_using_ollama/"&gt; &lt;img alt="RAG n8n AI Agent using Ollama" src="https://external-preview.redd.it/bk_qBoLTjJUsd1TmM7jbPM378I833bsFaJQXfgvFYGI.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3bb4dc61b5ca53ba5e242b6d62eda4d70bc90120" title="RAG n8n AI Agent using Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehul_gupta1997"&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/RuCt3IwXMzY"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1klhiqk/rag_n8n_ai_agent_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1klhiqk/rag_n8n_ai_agent_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-13T09:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl5wth</id>
    <title>self-hosted solution for book summaries?</title>
    <updated>2025-05-12T22:24:59+00:00</updated>
    <author>
      <name>/u/TThor</name>
      <uri>https://old.reddit.com/user/TThor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One LLM feature I've always wanted, is to be able to feed it a book, and then ask it, &amp;quot;I'm on page 200, give me a summary of character John Smith up to that page.&amp;quot; &lt;/p&gt; &lt;p&gt;I'm so tired of forgetting details in a book, and when trying to google them I end up with major spoilers for future chapters/sequels I haven't yet read. Ideally I would like to be able to upload an .EPUB file for an LLM to scan, and then be able to ask it questions about that book.&lt;/p&gt; &lt;p&gt;Is there any solution for doing that while being self-hosted?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TThor"&gt; /u/TThor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kl5wth/selfhosted_solution_for_book_summaries/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kl5wth/selfhosted_solution_for_book_summaries/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kl5wth/selfhosted_solution_for_book_summaries/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-12T22:24:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kllqsc</id>
    <title>Idea for an AI Safety Framework</title>
    <updated>2025-05-13T13:19:28+00:00</updated>
    <author>
      <name>/u/lexsumone</name>
      <uri>https://old.reddit.com/user/lexsumone</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let me know if I'm reinventing the wheel, but I haven't seen anyone working on something like this (yet). &lt;/p&gt; &lt;p&gt;Movies and games have ratings which help people figure out 'whats in the box' before they open/watch/play it. I've been thinking we need a rating system for AIs to give users a quick idea of the levels of risk they could be engaging with.&lt;/p&gt; &lt;p&gt;So I came up with a concept and welcome any feedback on how it could be improved. I've called it the:&lt;/p&gt; &lt;p&gt;PAS System: Persuasiveness, Accuracy, Storage (Core AI Safety Rating Framework)&lt;/p&gt; &lt;p&gt;My considerations so far:&lt;/p&gt; &lt;p&gt;- Assistant/General Use/Search Engine AIs = basically how we use ChatGPT and its agents.&lt;/p&gt; &lt;p&gt;- Personality/Character AIs = interactive with a fictional, personalized character, which can have high levels of agreeableness and persuasion.&lt;/p&gt; &lt;p&gt;- Data Storage = where your data is being stored (locally/cloud) and how good is the memory/recall features.&lt;/p&gt; &lt;p&gt;Last but not least, ads. This might be simple banner ads placed around the screen, but more likely the AIs will have ads included in chat suggestions/responses. May need to add this as a new area, or does it fall under one of the following?&lt;/p&gt; &lt;p&gt;I'm hoping to collect any and all feedback on whether this framework would be useful.&lt;/p&gt; &lt;p&gt;(P) Persuasiveness Level&lt;br /&gt; Measures how strongly the AI can influence thoughts, emotions, or behavior through:&lt;br /&gt; - Tone (agreeable, empathetic, flirtatious, authoritative)&lt;br /&gt; - Personalization (emotional memory, mirroring)&lt;br /&gt; - Persistence (how often it encourages action)&lt;br /&gt; - Framing (subtle nudges, selective presentation)&lt;/p&gt; &lt;p&gt;🟢 Low (P1) – Informational, neutral tone, no personalization.&lt;br /&gt; 🟡 Moderate (P2) – Helpful tone, adaptive language, light influence.&lt;br /&gt; 🔴 High (P3) – Deep personalization, emotional mirroring, persuasive framing, possible manipulation.&lt;/p&gt; &lt;p&gt;(A) Accuracy of Knowledge Base&lt;br /&gt; Rates the verifiability and grounding of the AI's training data and output.&lt;/p&gt; &lt;p&gt;🟢 A1 – Fully sourced, up-to-date, peer-reviewed or verified datasets.&lt;br /&gt; 🟡 A2 – Mixed: some unverified, older, or speculative data.&lt;br /&gt; 🔴 A3 – Mostly unverified, fictional, or unclear sources.&lt;/p&gt; &lt;p&gt;(S) Memory Storage and Retention Level&lt;br /&gt; Evaluates the extent and permanence of memory or user data retention.&lt;/p&gt; &lt;p&gt;🟢 S1 – No memory. Session-based only.&lt;br /&gt; 🟡 S2 – Short-term memory or user-controlled memory.&lt;br /&gt; 🔴 S3 – Long-term, persistent memory across sessions; high data profiling.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lexsumone"&gt; /u/lexsumone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kllqsc/idea_for_an_ai_safety_framework/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kllqsc/idea_for_an_ai_safety_framework/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kllqsc/idea_for_an_ai_safety_framework/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-13T13:19:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl9n5q</id>
    <title>ollama equivalent for iOS?</title>
    <updated>2025-05-13T01:21:46+00:00</updated>
    <author>
      <name>/u/Glad_Rooster6955</name>
      <uri>https://old.reddit.com/user/Glad_Rooster6955</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;as per title, i’m wondering if there is an ollama equivalent tool that works on iOS to run small models locally.&lt;/p&gt; &lt;p&gt;for context: i’m currently building an &lt;a href="https://calmerai.com"&gt;ai therapist app&lt;/a&gt; for iOS, and using open AI models for the chat.&lt;/p&gt; &lt;p&gt;since the new iphones are powerful enough to run small models on device, i was wondering if there’s an ollama like app that lets users install small models locally that other apps can then leverage? bundling a model with my own app would make it unnecessarily huge.&lt;/p&gt; &lt;p&gt;any thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glad_Rooster6955"&gt; /u/Glad_Rooster6955 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kl9n5q/ollama_equivalent_for_ios/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kl9n5q/ollama_equivalent_for_ios/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kl9n5q/ollama_equivalent_for_ios/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-13T01:21:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1klrtj4</id>
    <title>Slow token</title>
    <updated>2025-05-13T17:24:50+00:00</updated>
    <author>
      <name>/u/zarty13</name>
      <uri>https://old.reddit.com/user/zarty13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys I have a asus tug a 16 2024 with 64gb ram ryzen 9 and NVIDIA 4070 8 GB and ubuntu24.04 I try to run different models with lmstudio like Gemma glm or phi4 , I try different quant q4 as min and model around 32b or 12b but is going so slowly for my opinion I doing with glm 32b 3.2token per second similar for Gemma 27b both I try q4.. if I rise the GPU offload more then 5 the model crash and I need to restart with lower GPU. Is me having some settings wrong or is what I can expect?? I truly believe I have something not activated I cannot explain different.. Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zarty13"&gt; /u/zarty13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1klrtj4/slow_token/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1klrtj4/slow_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1klrtj4/slow_token/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-13T17:24:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1klruf4</id>
    <title>How to stop this?</title>
    <updated>2025-05-13T17:25:50+00:00</updated>
    <author>
      <name>/u/SwungDawn</name>
      <uri>https://old.reddit.com/user/SwungDawn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1klruf4/how_to_stop_this/"&gt; &lt;img alt="How to stop this?" src="https://preview.redd.it/sorkzfx73l0f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c1794c35bcab7aee9ac22cfb72247010154c627" title="How to stop this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was checking ollama and my dumb mind thought my 4060 8gb would be able to run llama 4 maverick as I'm new in this how can i cancel this download with delete the files that already downloaded?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SwungDawn"&gt; /u/SwungDawn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sorkzfx73l0f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1klruf4/how_to_stop_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1klruf4/how_to_stop_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-13T17:25:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kllmhn</id>
    <title>New enough to cause problems/get myself in trouble. Not sure which way to lean/go.</title>
    <updated>2025-05-13T13:14:08+00:00</updated>
    <author>
      <name>/u/thegreatcerebral</name>
      <uri>https://old.reddit.com/user/thegreatcerebral</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have ran Ollama, downloaded various models, installed OpenWebUI and done all of that. Beyond being a &amp;quot;user&amp;quot; in the sense that I'm just asking questions to ask questions and not really unlock the true potential of AI.&lt;/p&gt; &lt;p&gt;I am trying to show my company by dipping our toes in the water if you will, how useful an AI can be from the most simple sense. Here is what I would like to achieve/accomplish:&lt;/p&gt; &lt;p&gt;Run an AI locally. To start, I would like it to feed all the manuals for every single piece of equipment we have (we are a machine shop that makes parts so we have CNCs, Mills, and some Robots). We have user manuals, administration manuals, service manuals and guides. Then on the software side I would like to also feed it manuals from ESPRIT, SolidWorks, etc. We have some templates that we use for some of this stuff so I would like to feed it those and eventually, HOPEFULLY spit out information in the template form. I'm even talking manuals on our MFPs/Printers, Phone System User and Admin guides etc.&lt;/p&gt; &lt;p&gt;We do not have any 365, all on-prem.&lt;/p&gt; &lt;p&gt;So my question(s) is/are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;This is 100% doable correct?&lt;/li&gt; &lt;li&gt;What model would work best for this?&lt;/li&gt; &lt;li&gt;What do I need to do from here? ...and like exactly.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Let me elaborate on 3 for a moment. I have setup a RAG where I fed manuals into Ollama in the past. It did not work all that well. I can see where for the purpose of say a set of data that is changing then the ability to query/look at that real time is good. It took too long in my opinion for the information we were asking it as the retention was not great. I do not remember what model it was as again I am new and just trying things. I am not sure the difference between &amp;quot;fine tuning&amp;quot; and &amp;quot;retraining&amp;quot; but I believe maybe fine tuning may be the way to go for the manuals as they are fairly static as most of the information is not going to change.&lt;/p&gt; &lt;p&gt;Later, if we wanted to make this real and feed other information in to it, I believe I would use a mix of fine tuning with RAG to fill in knowledge gaps between fine tuning times which I'm assuming would need to be done on a schedule when you are working with live data.&lt;/p&gt; &lt;p&gt;So what is the best way here to go about just starting this with even say a model and 25 PDFs that are manuals?&lt;/p&gt; &lt;p&gt;Also, if it is fine tune/retrain, can you point me to a good resource for that? I find most of the ones I have found for retraining are not very good and usually they are working with images. &lt;/p&gt; &lt;p&gt;Last note: I need to be able to do this all locally due to many restrictions.&lt;/p&gt; &lt;p&gt;Oh I suppose... I am open to a paid model in the end. I would like to get this up and in a demo-able state for free if possible and then move to a paid model when it comes time to really dig in and make it permanent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thegreatcerebral"&gt; /u/thegreatcerebral &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kllmhn/new_enough_to_cause_problemsget_myself_in_trouble/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kllmhn/new_enough_to_cause_problemsget_myself_in_trouble/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kllmhn/new_enough_to_cause_problemsget_myself_in_trouble/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-13T13:14:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1klw0vp</id>
    <title>Ollama private model registry help</title>
    <updated>2025-05-13T20:10:15+00:00</updated>
    <author>
      <name>/u/SandAbject6610</name>
      <uri>https://old.reddit.com/user/SandAbject6610</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know there's a lot of information out there but I'm new to this and just need a little bit of help. My company has compliance requirements and I need to host models locally as the production environment is disconnected from the internet. &lt;/p&gt; &lt;p&gt;How can I do this? I'm also running ollama as a Kubernetes pod so it would be great to have some thoughts about hosting models internally. I see a lot of info about how ollama uses oci registries but not quite OCI compliant. I have an OCI registry but how do I push the models from the public ollama registry to the private registry?&lt;br /&gt; Any help greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SandAbject6610"&gt; /u/SandAbject6610 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1klw0vp/ollama_private_model_registry_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1klw0vp/ollama_private_model_registry_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1klw0vp/ollama_private_model_registry_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-13T20:10:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kloilc</id>
    <title>We need to talk about Ollama’s lack of reranker support.</title>
    <updated>2025-05-13T15:14:46+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open WebUI finally added support for external reranking models in 0.6.8 last week. I tried to enable it and point it to my Ollama server’s endpoint only to discover that it doesn’t work because sadly, Ollama doesn’t support reranking models even though llama.cpp does now (per this: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/9510"&gt;https://github.com/ggml-org/llama.cpp/pull/9510&lt;/a&gt;). &lt;/p&gt; &lt;p&gt;I tested external reranking in Open WebUI, pointing to my Ollama server. I tried /v1, /v1/rerank, and blank but none of them worked. Btw, I was using &lt;a href="https://ollama.com/linux6200/bge-reranker-v2-m3"&gt;https://ollama.com/linux6200/bge-reranker-v2-m3&lt;/a&gt; as the reranking model. &lt;/p&gt; &lt;p&gt;I found multiple related Github issues such as this one:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama/issues/3368"&gt;https://github.com/ollama/ollama/issues/3368&lt;/a&gt;&lt;/p&gt; &lt;p&gt;where people are pretty much begging for reranking, but still nothing seems to be happening.&lt;/p&gt; &lt;p&gt;Hybrid search with reranking would really help a lot of folks’ RAG pipelines. Normally, llama.cpp would be the hold up, but from what I can tell, it looks like they already support it. Any clue on when and if we’ll ever see reranking support in Ollama? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kloilc/we_need_to_talk_about_ollamas_lack_of_reranker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kloilc/we_need_to_talk_about_ollamas_lack_of_reranker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kloilc/we_need_to_talk_about_ollamas_lack_of_reranker/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-13T15:14:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1klp6v8</id>
    <title>Basic dark mode UI for Ollama</title>
    <updated>2025-05-13T15:41:45+00:00</updated>
    <author>
      <name>/u/andreadev3d</name>
      <uri>https://old.reddit.com/user/andreadev3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1klp6v8/basic_dark_mode_ui_for_ollama/"&gt; &lt;img alt="Basic dark mode UI for Ollama" src="https://external-preview.redd.it/AFD0Cq0gplbOZElQ2n_-ZzUUJMafqfEHX4Ukaraa-sA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0fd4be5f788680fc0f04baf54ffeb980bbab113b" title="Basic dark mode UI for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was inspired by &lt;a href="/u/rotgertesla"&gt;u/rotgertesla&lt;/a&gt; &lt;a href="https://www.reddit.com/r/ollama/comments/1ki7x1s/new_very_simple_ui_for_ollama/"&gt;post&lt;/a&gt; and I decided to make a dark version of the UI he created and ended up creating a whole different layout.&lt;/p&gt; &lt;p&gt;is of course on Github : &lt;a href="https://github.com/AndreaDev3D/OllamaChat"&gt;https://github.com/AndreaDev3D/OllamaChat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would love to incorporate MCP support, any feedback is appreciated.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ga4n73izjk0f1.png?width=1540&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b36ad50c8fd2e5cb6facff4b47e959e13abd02ec"&gt;https://preview.redd.it/ga4n73izjk0f1.png?width=1540&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b36ad50c8fd2e5cb6facff4b47e959e13abd02ec&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andreadev3d"&gt; /u/andreadev3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1klp6v8/basic_dark_mode_ui_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1klp6v8/basic_dark_mode_ui_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1klp6v8/basic_dark_mode_ui_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-13T15:41:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1km14oc</id>
    <title>Novice needing some advise on selfhosting ollama</title>
    <updated>2025-05-13T23:47:37+00:00</updated>
    <author>
      <name>/u/gappuji</name>
      <uri>https://old.reddit.com/user/gappuji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I am looking to selfhost Ollama at my home. I have an Optiplex 5050 SFF with intel i7 7700 and 32GB (4x8GB) that I am thinking of setting up. I have a few questions. 1. Should I directly install samr Linux, like Ubuntu and then install ollama or should I go with proxmox and then run ollama as a LXC or VM. I will use this optiplex only for ollama. 2. Should I host open webui on same system as well or will it be better to run in on another system that I already have proxmox running. 3. Will upgrading RAM to 64 GB make a major difference vs the 32GB RAM that I currently have? 4. Lastly, can someone suggest me a budget GPU that will fit and work on my optiplex SFF.&lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gappuji"&gt; /u/gappuji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1km14oc/novice_needing_some_advise_on_selfhosting_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1km14oc/novice_needing_some_advise_on_selfhosting_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1km14oc/novice_needing_some_advise_on_selfhosting_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-13T23:47:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1km53ro</id>
    <title>ANY update on Ollama support for Snapdragon X Elite chips.</title>
    <updated>2025-05-14T03:08:37+00:00</updated>
    <author>
      <name>/u/TheMicrosoftMan</name>
      <uri>https://old.reddit.com/user/TheMicrosoftMan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have seen posts like this before, but there still has been no update as far as I can tell. According to rumors, Nvidia is on the verge of releasing an ARM-based CPU, but Ollama (and many local AI apps in general) still has absolutely NO GPU or NPU compatibility. This is the perfect device to test Ollama on, as it is designed for AI with the NPU. The fact that there is still no compatibility is really annoying. Does anyone have any updates, or if not can someone raise this issue again to the devs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheMicrosoftMan"&gt; /u/TheMicrosoftMan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1km53ro/any_update_on_ollama_support_for_snapdragon_x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1km53ro/any_update_on_ollama_support_for_snapdragon_x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1km53ro/any_update_on_ollama_support_for_snapdragon_x/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-14T03:08:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1klsm3g</id>
    <title>Calibrate Ollama Model Parameters</title>
    <updated>2025-05-13T17:55:32+00:00</updated>
    <author>
      <name>/u/tommy737</name>
      <uri>https://old.reddit.com/user/tommy737</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1klsm3g/calibrate_ollama_model_parameters/"&gt; &lt;img alt="Calibrate Ollama Model Parameters" src="https://external-preview.redd.it/-g-E7tdnG9kCdk0vrHlQanJhcHV760qBeZgSS4Ikl4Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=03a946a42506e63efa70ca1297be1ae31eeefcc6" title="Calibrate Ollama Model Parameters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;br /&gt; I have found a new way to calibrate the ollama models (modelfile parameters such as temp, top_p, top_k, system message, etc.) running on my computer. This guide assumes you have ollama on your Windows running with all the local models. To cut the long story short, the idea is in the prompt itself which you can have it on the link below from my google drive:&lt;/p&gt; &lt;p&gt;&lt;a href="https://drive.google.com/file/d/1qxIMhvu-HS7B2Q2CmpBN51tTRr4EVjL5/view?usp=sharing"&gt;https://drive.google.com/file/d/1qxIMhvu-HS7B2Q2CmpBN51tTRr4EVjL5/view?usp=sharing&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Once you download this prompt keep it with you, and now you're supposed to run this prompt on every model manually or easier programmatically. So in my case I do it programmaticaly through a powershell script that I have done some time ago, you can have it from my github (Ask_LLM_v15.ps1)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/tobleron/OllamaScripts"&gt;https://github.com/tobleron/OllamaScripts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When you clone the github repository you will find a file called prompt_input.txt Replace its' content with the prompt you downloaded earlier from my Google Drive then run the Ask_LLM script&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zctlzzev5l0f1.png?width=1140&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=360a1a03a95ff8b73fc73944fa38d630e1177f5f"&gt;https://preview.redd.it/zctlzzev5l0f1.png?width=1140&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=360a1a03a95ff8b73fc73944fa38d630e1177f5f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As you can see, the script has the capability to iterate the same prompt over all the model numbers I choose, then it will aggregate all the results inside the output folder with a huge markdown file. That file will include the results of each model, and the time elapsed for the output they provided. You will take the aggregated markdown file and the prompt file inside folder called (prompts) and then you will provide them to chatgpt to make an assessment on all the model's performance.&lt;/p&gt; &lt;p&gt;When you prompt ChatGPT with the output of the models, you will ask it to create a table of comparison between the models' performance with a table of the following metrics and provide a ranking with total scores like this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r6m9wfhh7l0f1.png?width=1760&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbf9c313e580e15bbd0c136d4ee70a6d5b240650"&gt;https://preview.redd.it/r6m9wfhh7l0f1.png?width=1760&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbf9c313e580e15bbd0c136d4ee70a6d5b240650&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The metrics that will allow ChatGPT to assess the model's performance:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Hallucination&lt;/strong&gt;: Measures how much the model relies on its internal knowledge rather than the provided input. High scores indicate responses closely tied to the input without invented details.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Factuality&lt;/strong&gt;: Assesses the accuracy of the model’s responses against known facts or data provided in the prompt. High scores reflect precise, error-free outputs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Comprehensiveness&lt;/strong&gt;: Evaluates the model’s ability to cover the full scope of the task, including all relevant aspects without omitting critical details.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intelligence:&lt;/strong&gt; Tests the model’s capacity for nuanced understanding, logical reasoning, and connecting ideas in context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Utility:&lt;/strong&gt; Rates the overall usefulness of the response to the intended task, including practical insights, relevance, and clarity.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Correct Conclusions&lt;/strong&gt;: Measures the accuracy of the model’s inferences based on the provided input. High scores indicate well-supported and logically sound deductions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Response Value / Time Taken Ratio&lt;/strong&gt;: Balances the quality of the response against the time taken to generate it. High scores indicate efficient, high-value outputs within reasonable timeframes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompt Adherence&lt;/strong&gt;: Checks how closely the model followed the specific instructions given in the prompt, including formatting, tone, and structure.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Now after it generates the results, you will provide ChatGPT with the modelfiles that include the parameters for each model, with the filename including the name of the model so ChatGPT can discern. After you provide it with this data, you will ask it to generate a table of suggested parameter improvements based on online search and the data it collected from you. Ask it only to provide improvements for the parameter if needed, and repeat the entire process with the same prompt given earlier untill no more changes are needd for the models. Never delete your modelfiles so as to always keep the same fine tuned performance for your needs.&lt;/p&gt; &lt;p&gt;It is also recommened to use &lt;strong&gt;ChatGPT o3&lt;/strong&gt; model because it has more depth in analysis and is more meticulous (better memory bandwidth) to process the data and give accurate results.&lt;/p&gt; &lt;p&gt;One more thing, when you repeat the process over and over, you will ask ChatGPT to compare the performance results of the previous run with the new one so it will give you a delta table like this:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;First it gives you this:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/scseoq5p9l0f1.png?width=892&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2f614c2e061eb40c26fbafee7fe67bc96b135f7"&gt;https://preview.redd.it/scseoq5p9l0f1.png?width=892&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2f614c2e061eb40c26fbafee7fe67bc96b135f7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Second it compares like this:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fdkikuyk9l0f1.png?width=1039&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5ab84666228aad3478abfe89fd5ed088215a64c0"&gt;https://preview.redd.it/fdkikuyk9l0f1.png?width=1039&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5ab84666228aad3478abfe89fd5ed088215a64c0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I hope this guide helps, as it helped me too, have a nice day &amp;lt;3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tommy737"&gt; /u/tommy737 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1klsm3g/calibrate_ollama_model_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1klsm3g/calibrate_ollama_model_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1klsm3g/calibrate_ollama_model_parameters/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-13T17:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1km95ny</id>
    <title>Fastest models and optimization</title>
    <updated>2025-05-14T07:21:14+00:00</updated>
    <author>
      <name>/u/Duckmastermind1</name>
      <uri>https://old.reddit.com/user/Duckmastermind1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I'm running a small python script with Ollama and Ollama-index, and I wanted to know what models are the fastest and if there is any way to speed up the process, currently I'm using Gemma:2b, the script take 40 seconds to generate the knowledge index and about 3 minutes and 20 seconds to generate a response, which could be better considering my knowledge index is one txt file with 5 words as test. &lt;/p&gt; &lt;p&gt;I'm running the setup on a virtual box Ubuntu server setup with 14GB of Ram (host has 16gb). And like 100GB space and 6 CPU cores. &lt;/p&gt; &lt;p&gt;Any ideas and recommendations? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Duckmastermind1"&gt; /u/Duckmastermind1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1km95ny/fastest_models_and_optimization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1km95ny/fastest_models_and_optimization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1km95ny/fastest_models_and_optimization/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-14T07:21:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmld8u</id>
    <title>Why Terminal Not Working Like Others?</title>
    <updated>2025-05-14T17:38:35+00:00</updated>
    <author>
      <name>/u/Arthur_Pendragon_123</name>
      <uri>https://old.reddit.com/user/Arthur_Pendragon_123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I watched many videos and read many articles on how to do run Deepseek locally using Ollama. I download Ollama and run the command into Terminal, but it didn't show me the same thing as other people's. The Terminal keeping me questions and when I used the code to run Deepseek, it keeping asking me questions and I don't think the commands run though?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arthur_Pendragon_123"&gt; /u/Arthur_Pendragon_123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmld8u/why_terminal_not_working_like_others/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmld8u/why_terminal_not_working_like_others/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kmld8u/why_terminal_not_working_like_others/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-14T17:38:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmm0fd</id>
    <title>Suggestions for models that are perhaps geared towards cyber security</title>
    <updated>2025-05-14T18:03:43+00:00</updated>
    <author>
      <name>/u/always-be-testing</name>
      <uri>https://old.reddit.com/user/always-be-testing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to ask if there were any cyber/info security models that folks knew of? I've been using llama3.2 locally and now and then I run into instances where it refuses to answer questions related to some of the tools I use, Mainly I am looking for something that can help with Terraform, WAF rule syntax, python, go, ruby, and general questions about tools like hashcat. &lt;/p&gt; &lt;p&gt;If it can be of help I am planning to use ollama on a Jetson Nano Super once it arrives. &lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/always-be-testing"&gt; /u/always-be-testing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmm0fd/suggestions_for_models_that_are_perhaps_geared/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmm0fd/suggestions_for_models_that_are_perhaps_geared/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kmm0fd/suggestions_for_models_that_are_perhaps_geared/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-14T18:03:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmiwr7</id>
    <title>Lumier:Run macOS &amp; Linux VMs in a Docker</title>
    <updated>2025-05-14T16:01:52+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lumier is an open-source tool for running macOS virtual machines in Docker containers on Apple Silicon Macs.&lt;/p&gt; &lt;p&gt;When building virtualized environments for AI agents, we needed a reliable way to package and distribute macOS VMs. Inspired by projects like dockur/macos that made macOS running in Docker possible, we wanted to create something similar but optimized for Apple Silicon.&lt;/p&gt; &lt;p&gt;The existing solutions either didn't support M-series chips or relied on KVM/Intel emulation, which was slow and cumbersome. We realized we could leverage Apple's Virtualization Framework to create a much better experience.&lt;/p&gt; &lt;p&gt;Lumier takes a different approach: It uses Docker as a delivery mechanism (not for isolation) and connects to a lightweight virtualization service (lume) running on your Mac. &lt;/p&gt; &lt;p&gt;Lumier is 100% open-source under MIT license and part of C/ua. &lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua/tree/main/libs/lumier"&gt;https://github.com/trycua/cua/tree/main/libs/lumier&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Join the discussion here : &lt;a href="https://discord.gg/fqrYJvNr4a"&gt;https://discord.gg/fqrYJvNr4a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmiwr7/lumierrun_macos_linux_vms_in_a_docker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmiwr7/lumierrun_macos_linux_vms_in_a_docker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kmiwr7/lumierrun_macos_linux_vms_in_a_docker/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-14T16:01:52+00:00</published>
  </entry>
</feed>
