<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-20T16:07:14+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jecyck</id>
    <title>Swapping from Chatgpt to ollama</title>
    <updated>2025-03-18T19:05:37+00:00</updated>
    <author>
      <name>/u/Pirate_dolphin</name>
      <uri>https://old.reddit.com/user/Pirate_dolphin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working with &lt;a href="https://github.com/BRlkl/AGI-Samantha"&gt;AGI Samantha &lt;/a&gt;and it's working fine. I had to make some tweaks but its visual, self prompting and can now take my terminal or speech input. It has a locally recorded short term memory, long term memory and a subconcious. &lt;/p&gt; &lt;p&gt;When I convert this to ollama the model is repeating these inputs back to me, rather than taking them internally and acting with them.&lt;/p&gt; &lt;p&gt;Any suggestions on how this could be done? I'm thinking about changing the model file instead of leaving them in the script&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pirate_dolphin"&gt; /u/Pirate_dolphin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jecyck/swapping_from_chatgpt_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jecyck/swapping_from_chatgpt_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jecyck/swapping_from_chatgpt_to_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T19:05:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jegywj</id>
    <title>Ollama and Gemma3</title>
    <updated>2025-03-18T21:50:03+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Installed latest Ollama, 0.6.1&lt;/p&gt; &lt;p&gt;Trying to run any Gemma3, and gettings this:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama run gemma3:27b&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Error: Post &amp;quot;http://127.0.0.1:11434/api/generate&amp;quot;: EOF&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Any other model, llama3.3, aya,mistral,deepseek works! &lt;/p&gt; &lt;p&gt;What is the problem here, why Gemma3 does not work but all others do?&lt;/p&gt; &lt;p&gt;I have 2x 7900 XTX. Loads of RAM and CPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jegywj/ollama_and_gemma3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jegywj/ollama_and_gemma3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jegywj/ollama_and_gemma3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T21:50:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1je1k10</id>
    <title>Mistral Small 3.1</title>
    <updated>2025-03-18T10:10:50+00:00</updated>
    <author>
      <name>/u/laurentbourrelly</name>
      <uri>https://old.reddit.com/user/laurentbourrelly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you are looking for a small model, Mistral is an interesting option. Unfortunately, like all small models, it hallucinates a lot.&lt;/p&gt; &lt;p&gt;The new Mistral just came out and looks promising &lt;a href="https://mistral.ai/news/mistral-small-3-1"&gt;https://mistral.ai/news/mistral-small-3-1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/laurentbourrelly"&gt; /u/laurentbourrelly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1je1k10/mistral_small_31/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1je1k10/mistral_small_31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1je1k10/mistral_small_31/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T10:10:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jekene</id>
    <title>I built a tool that uses AI to help with your shell.</title>
    <updated>2025-03-19T00:23:40+00:00</updated>
    <author>
      <name>/u/SpectreBoyo</name>
      <uri>https://old.reddit.com/user/SpectreBoyo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While I have decent experience with the shell, I’ve seen many developers struggle doing basic tasks within their terminal, which is incredibly crippling as most projects usually start with a shell command. &lt;/p&gt; &lt;p&gt;I built CLAII for this exact reason, helping people do the annoying part of starting a project, or finding a lesser known tool for their specific use case, without leaving their terminal emulator. &lt;/p&gt; &lt;p&gt;While it supports APIs, It was originally built with Ollama in mind, partially because I’ve been generally surprised with the qwen coder models, and because current API pricing is out of reach for people with no access to direct payment options such as myself. But I want your help.&lt;/p&gt; &lt;p&gt;CLAII was built entirely from my viewpoint, and I want to expand it, to include more cases for windows and macOS, which I do not have access to, or have much experience with for development and working with the shell. I have tried to adapt for these OSes but I still need help testing it. &lt;/p&gt; &lt;p&gt;I also need help testing it with more advanced models, while qwen is great! It may not be perfect, and more advanced models can show some gaps I may have overlooked! &lt;/p&gt; &lt;p&gt;Try it out if you want! Give me your honest opinions and if you encounter any bugs or errors, please let me know! &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/YoussefAlkent/CLAII"&gt;https://github.com/YoussefAlkent/CLAII&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can check it out here! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpectreBoyo"&gt; /u/SpectreBoyo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jekene/i_built_a_tool_that_uses_ai_to_help_with_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jekene/i_built_a_tool_that_uses_ai_to_help_with_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jekene/i_built_a_tool_that_uses_ai_to_help_with_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-19T00:23:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jendzv</id>
    <title>Is this a normal amount of ram with Ollama+Text Web UI?</title>
    <updated>2025-03-19T02:55:41+00:00</updated>
    <author>
      <name>/u/Account1893242379482</name>
      <uri>https://old.reddit.com/user/Account1893242379482</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jendzv/is_this_a_normal_amount_of_ram_with_ollamatext/"&gt; &lt;img alt="Is this a normal amount of ram with Ollama+Text Web UI?" src="https://preview.redd.it/siacq4gt9kpe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45a4ea325dec8e22e5cff0cb56209eb1defcf2bf" title="Is this a normal amount of ram with Ollama+Text Web UI?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Account1893242379482"&gt; /u/Account1893242379482 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/siacq4gt9kpe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jendzv/is_this_a_normal_amount_of_ram_with_ollamatext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jendzv/is_this_a_normal_amount_of_ram_with_ollamatext/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-19T02:55:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jebv52</id>
    <title>PrivateLLMLens updated (zero web server single page HTML file)</title>
    <updated>2025-03-18T18:21:26+00:00</updated>
    <author>
      <name>/u/Specialist_Laugh_231</name>
      <uri>https://old.reddit.com/user/Specialist_Laugh_231</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jebv52/privatellmlens_updated_zero_web_server_single/"&gt; &lt;img alt="PrivateLLMLens updated (zero web server single page HTML file)" src="https://preview.redd.it/l183ie98phpe1.gif?width=640&amp;amp;crop=smart&amp;amp;s=ca6e8f3b06dc7665ea9c1444981d972afbdcc6b7" title="PrivateLLMLens updated (zero web server single page HTML file)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist_Laugh_231"&gt; /u/Specialist_Laugh_231 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l183ie98phpe1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jebv52/privatellmlens_updated_zero_web_server_single/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jebv52/privatellmlens_updated_zero_web_server_single/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T18:21:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1je5u77</id>
    <title>Example running Gemma 3 locally for OCR using Ollama</title>
    <updated>2025-03-18T14:09:48+00:00</updated>
    <author>
      <name>/u/Elegant-Army-8888</name>
      <uri>https://old.reddit.com/user/Elegant-Army-8888</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google DeepMind has been cooking lately, while everyone has been focusing on the Gemini 2.0 Flash native image generation release, Gemma 3 is really a nifty little tool for developers&lt;/p&gt; &lt;p&gt;I build this demo python app in a couple of hours with Claude 3.7 in &lt;a href="/u/cursor_ai"&gt;u/cursor_ai&lt;/a&gt; showcasing that.&lt;br /&gt; The app uses Streamlit for the UI, Ollama as the backend running Gemma 3 vision locally, PIL for image processing, and pdf2image for PDF support.&lt;/p&gt; &lt;p&gt;And I can run it all locally on my 3 year old Macbook Pro. Takes about 30 seconds per image, but that's ok by me. If you have more than 32 gb of memory, and an RTX or M4 i'm sure it's even faster.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/adspiceprospice/localOCR"&gt;https://github.com/adspiceprospice/localOCR&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Elegant-Army-8888"&gt; /u/Elegant-Army-8888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1je5u77/example_running_gemma_3_locally_for_ocr_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1je5u77/example_running_gemma_3_locally_for_ocr_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1je5u77/example_running_gemma_3_locally_for_ocr_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T14:09:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf1e1l</id>
    <title>Anyone else not loving today’s Nvidia driver update</title>
    <updated>2025-03-19T16:41:59+00:00</updated>
    <author>
      <name>/u/Turtle2k</name>
      <uri>https://old.reddit.com/user/Turtle2k</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Woke up to no AI this morning. After all the updates still no AI. lol. I’m think it is probably a me problem but just curious if anyone else is out there not recovering from there automatic updates very well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turtle2k"&gt; /u/Turtle2k &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jf1e1l/anyone_else_not_loving_todays_nvidia_driver_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jf1e1l/anyone_else_not_loving_todays_nvidia_driver_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jf1e1l/anyone_else_not_loving_todays_nvidia_driver_update/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-19T16:41:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jesz6w</id>
    <title>*Temp fix* http://127.0.0.1:36365/completion": EOF with image attachments</title>
    <updated>2025-03-19T09:23:25+00:00</updated>
    <author>
      <name>/u/DaleCooperHS</name>
      <uri>https://old.reddit.com/user/DaleCooperHS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following the lead from &lt;a href="https://www.reddit.com/r/ollama/comments/1jdkevi/creating_gemma_3_from_gguf_with_mmproj_not_working/"&gt;OP&lt;/a&gt; I have reproduced the process to fix the issue with getting the model to interact with images when using custom GGUF downloaded form from Huggingface in order to have higher quants. &lt;/p&gt; &lt;p&gt;Here are the instructions on how to do it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Download the full weight from hugginface.&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You will need:&lt;br /&gt; - Huggingface account name and access token (access token needs to be created in your hugginface profile under the tab &amp;quot;Access Tokens)&lt;br /&gt; - granted access to the models (by requesting &amp;quot;grant access&amp;quot; on the huggingface pages below)&lt;br /&gt; - Git (or manual download)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3-4b-it"&gt;Gemma 3 4b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3-12b-it"&gt;Gemma 3 12b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3-27b-pt"&gt;Gemma 3 27b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Use git command &amp;quot;git clone&amp;quot; to clone the huggingface repo. You can find the full command under the 3 dots on the model page next to &amp;quot;Train&amp;quot;)&lt;/p&gt; &lt;p&gt;Insert your credentials when prompted and download the weights.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Create a ModelFile&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In the same folder where you downloaded the model create a file with any text editor and paste this:&lt;/p&gt; &lt;p&gt;FROM .&lt;/p&gt; &lt;p&gt;# Inference parameters&lt;/p&gt; &lt;p&gt;PARAMETER num_ctx 8192&lt;/p&gt; &lt;p&gt;PARAMETER stop &amp;quot;&amp;lt;end\_of\_turn&amp;gt;&amp;quot;&lt;/p&gt; &lt;p&gt;PARAMETER temperature 1&lt;/p&gt; &lt;p&gt;# Template for conversation formatting&lt;/p&gt; &lt;p&gt;TEMPLATE &amp;quot;&amp;quot;&amp;quot;{{- range $i, $_ := .Messages }}&lt;/p&gt; &lt;p&gt;{{- $last := eq (len (slice $.Messages $i)) 1 }}&lt;/p&gt; &lt;p&gt;{{- if or (eq .Role &amp;quot;user&amp;quot;) (eq .Role &amp;quot;system&amp;quot;) }}&amp;lt;start\_of\_turn&amp;gt;user&lt;/p&gt; &lt;p&gt;{{ .Content }}&amp;lt;end\_of\_turn&amp;gt;&lt;/p&gt; &lt;p&gt;{{ if $last }}&amp;lt;start\_of\_turn&amp;gt;model&lt;/p&gt; &lt;p&gt;{{ end }}&lt;/p&gt; &lt;p&gt;{{- else if eq .Role &amp;quot;assistant&amp;quot; }}&amp;lt;start\_of\_turn&amp;gt;model&lt;/p&gt; &lt;p&gt;{{ .Content }}{{ if not $last }}&amp;lt;end\_of\_turn&amp;gt;&lt;/p&gt; &lt;p&gt;{{ end }}&lt;/p&gt; &lt;p&gt;{{- end }}&lt;/p&gt; &lt;p&gt;{{- end }}&amp;quot;&amp;quot;&amp;quot; &lt;/p&gt; &lt;p&gt;Save the file as a ModelFIle (no file extensions like .txt)&lt;/p&gt; &lt;p&gt;&lt;em&gt;(NOTE: THe temperature can either be 0.1 or 1. I tested both and I can not find a difference yet.)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Create the GGUF&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Open a terminal in the location of your files and run: &lt;/p&gt; &lt;p&gt;ollama create --quantize q8_0 Gemma3 -f ModelFile&lt;/p&gt; &lt;h1&gt;Where:&lt;/h1&gt; &lt;p&gt;- q8_0 is the quant size you want&lt;/p&gt; &lt;p&gt;- Gemma3 is the name you want to give to the model&lt;/p&gt; &lt;p&gt;- ModelFile is the exact name (cap sensitive) of the ModelFile you create&lt;/p&gt; &lt;p&gt;THis should create the model for you and should now support images.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DaleCooperHS"&gt; /u/DaleCooperHS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jesz6w/temp_fix_http12700136365completion_eof_with_image/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jesz6w/temp_fix_http12700136365completion_eof_with_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jesz6w/temp_fix_http12700136365completion_eof_with_image/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-19T09:23:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jein5y</id>
    <title>Local Agents with Full Machine Access!!!</title>
    <updated>2025-03-18T23:02:52+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Ollama community!&lt;/p&gt; &lt;p&gt;I've just released a major new feature for Observer AI that I think many of you will find interesting: &lt;strong&gt;full Jupyter Server integration&lt;/strong&gt; with Python code execution capabilities!&lt;/p&gt; &lt;h1&gt;What this means:&lt;/h1&gt; &lt;p&gt;Observer AI can now connect to your existing Jupyter server, allowing you to execute Python code directly on your machine with agent's responses! This creates a complete perception-action loop where agents can:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Observe your screen (via OCR or screenshots with vision models)&lt;/li&gt; &lt;li&gt;Process what they see with LLMs running locally through Ollama&lt;/li&gt; &lt;li&gt;Execute Python code to perform actions on your system!!&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Potential use cases:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Data processing: Agents that monitor spreadsheets and write files!&lt;/li&gt; &lt;li&gt;Automation tools: Create personalized workflow automations triggered by screen content&lt;/li&gt; &lt;li&gt;Screen regulation: Watches for violent content and shuts down the computer! hahaha&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Looking for feedback:&lt;/h1&gt; &lt;p&gt;As fellow Ollama users who are comfortable with technical setups, I'd love your thoughts on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What kinds of agents would you build with Python execution capabilities? (I can help you through discord or dm's)&lt;/li&gt; &lt;li&gt;Any security considerations you'd want to see addressed? (everything is local so no RCE yet i think)&lt;/li&gt; &lt;li&gt;Feature requests or improvements to the Jupyter integration?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Observer AI remains 100% open source and local-first - try it at &lt;a href="https://app.observer-ai.com"&gt;https://app.observer-ai.com&lt;/a&gt; or check out the code at &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for all the support and feedback so far!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jein5y/local_agents_with_full_machine_access/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jein5y/local_agents_with_full_machine_access/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jein5y/local_agents_with_full_machine_access/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T23:02:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf01v1</id>
    <title>Is there a locally hosted Vercel v0 alternative (text-to-frontend)?</title>
    <updated>2025-03-19T15:45:36+00:00</updated>
    <author>
      <name>/u/depressedclassical</name>
      <uri>https://old.reddit.com/user/depressedclassical</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I've been wondering lately what local alternatives are there (if any) to Vercel's V0 I could use? Any text-to-frontend client could probably do, I think. It just has to show me the code it came up with as well as the result. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/depressedclassical"&gt; /u/depressedclassical &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jf01v1/is_there_a_locally_hosted_vercel_v0_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jf01v1/is_there_a_locally_hosted_vercel_v0_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jf01v1/is_there_a_locally_hosted_vercel_v0_alternative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-19T15:45:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf3qst</id>
    <title>Converting image to Markdown</title>
    <updated>2025-03-19T18:18:08+00:00</updated>
    <author>
      <name>/u/gevorgter</name>
      <uri>https://old.reddit.com/user/gevorgter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to convert image to markdown. Problem is that text extracted is cut off in a middle of the image.&lt;/p&gt; &lt;p&gt;Why is that? Is it because of small context size?&lt;/p&gt; &lt;p&gt;I set env variable OLLAMA_CONTEXT_LENGTH to &amp;quot;8192 ollama serve&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gevorgter"&gt; /u/gevorgter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jf3qst/converting_image_to_markdown/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jf3qst/converting_image_to_markdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jf3qst/converting_image_to_markdown/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-19T18:18:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf0uaz</id>
    <title>OllamaCode: I built a local version of a tool that is inspired (but far from) Claude Code / GitHub Copilot Command Line. (Still work in progress)</title>
    <updated>2025-03-19T16:18:53+00:00</updated>
    <author>
      <name>/u/Loud-Consideration-2</name>
      <uri>https://old.reddit.com/user/Loud-Consideration-2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/tooyipjee/OllamaCode"&gt;https://github.com/tooyipjee/OllamaCode&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loud-Consideration-2"&gt; /u/Loud-Consideration-2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jf0uaz/ollamacode_i_built_a_local_version_of_a_tool_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jf0uaz/ollamacode_i_built_a_local_version_of_a_tool_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jf0uaz/ollamacode_i_built_a_local_version_of_a_tool_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-19T16:18:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfamey</id>
    <title>Can you run LLM on Dedicated GPU and Integrated GPU simultaneously?</title>
    <updated>2025-03-19T23:07:37+00:00</updated>
    <author>
      <name>/u/QuestionQuest117</name>
      <uri>https://old.reddit.com/user/QuestionQuest117</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a Framework 16 laptop with both a dedicated GPU (dGPU) and an integrated GPU (iGPU). The models I want to run are unfortunately just a little bigger than the VRAM allows on my dGPU and so the CPU takes over to run the rest of the model. This, as expected, results in a performance hit, but I'm wondering if the iGPU can be used to handle the overflow instead. Both the CPU and iGPU use system RAM to get the job done, but an iGPU should theoretically perform better than the CPU. Is my hypothesis correct and if so, is it possible to run the leftovers of the model via the iGPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuestionQuest117"&gt; /u/QuestionQuest117 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfamey/can_you_run_llm_on_dedicated_gpu_and_integrated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfamey/can_you_run_llm_on_dedicated_gpu_and_integrated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jfamey/can_you_run_llm_on_dedicated_gpu_and_integrated/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-19T23:07:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf1jk9</id>
    <title>A Pull-first Ollama Docker Image</title>
    <updated>2025-03-19T16:48:25+00:00</updated>
    <author>
      <name>/u/liquidcoffeee</name>
      <uri>https://old.reddit.com/user/liquidcoffeee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jf1jk9/a_pullfirst_ollama_docker_image/"&gt; &lt;img alt="A Pull-first Ollama Docker Image" src="https://external-preview.redd.it/O6jkPQ3KokuUlGzEZT1NOOkp7VEuoZ5FK3Tb9FUZ1yg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ed8daa1a145940da84bc8f68afc51ff9fabcba8" title="A Pull-first Ollama Docker Image" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liquidcoffeee"&gt; /u/liquidcoffeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.dolthub.com/blog/2025-03-19-a-pull-first-ollama-docker-image/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jf1jk9/a_pullfirst_ollama_docker_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jf1jk9/a_pullfirst_ollama_docker_image/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-19T16:48:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfgx1t</id>
    <title>Local/Cloud Orchestration Demo</title>
    <updated>2025-03-20T04:29:38+00:00</updated>
    <author>
      <name>/u/Emotional-Evening-62</name>
      <uri>https://old.reddit.com/user/Emotional-Evening-62</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you are switching between local model and cloud model for LLMs, check this orchestration demo. It seamlessly switches between cloud and local model, while still maintaining the context.&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/j0dOVWWzBrE?si=SjUJQFNdfsp1aR9T"&gt;https://youtu.be/j0dOVWWzBrE?si=SjUJQFNdfsp1aR9T&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For more info check &lt;a href="https://oblix.ai"&gt;https://oblix.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emotional-Evening-62"&gt; /u/Emotional-Evening-62 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfgx1t/localcloud_orchestration_demo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfgx1t/localcloud_orchestration_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jfgx1t/localcloud_orchestration_demo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-20T04:29:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfczef</id>
    <title>Is there a point to keeping earlier models of a certain series?</title>
    <updated>2025-03-20T00:59:24+00:00</updated>
    <author>
      <name>/u/digitalextremist</name>
      <uri>https://old.reddit.com/user/digitalextremist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following up on the great answers here: &lt;a href="https://www.reddit.com/r/ollama/comments/1jfb2s1/what_are_the_uses_for_small_models_below_7b/"&gt;https://www.reddit.com/r/ollama/comments/1jfb2s1/what_are_the_uses_for_small_models_below_7b/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If we have &lt;code&gt;gemma&lt;/code&gt; and &lt;code&gt;gemma2&lt;/code&gt; and now &lt;code&gt;gemma3&lt;/code&gt; is there ever a point to keeping the earlier models?&lt;/p&gt; &lt;p&gt;Same with &lt;code&gt;phi3&lt;/code&gt; and &lt;code&gt;phi4&lt;/code&gt; and &lt;code&gt;wizardlm&lt;/code&gt; and then &lt;code&gt;wizardlm2&lt;/code&gt; ... etc.&lt;/p&gt; &lt;p&gt;Is there something the earlier models still have over the later ones?&lt;/p&gt; &lt;p&gt;I am catching up to the pack on basic LLM hygeine as my drive fills with models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/digitalextremist"&gt; /u/digitalextremist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfczef/is_there_a_point_to_keeping_earlier_models_of_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfczef/is_there_a_point_to_keeping_earlier_models_of_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jfczef/is_there_a_point_to_keeping_earlier_models_of_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-20T00:59:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfnspn</id>
    <title>Connect to your self-hosted LLMs. From anywhere.</title>
    <updated>2025-03-20T12:30:53+00:00</updated>
    <author>
      <name>/u/smilulilu</name>
      <uri>https://old.reddit.com/user/smilulilu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jfnspn/connect_to_your_selfhosted_llms_from_anywhere/"&gt; &lt;img alt="Connect to your self-hosted LLMs. From anywhere." src="https://external-preview.redd.it/w7-9BozDXx3jvu1jDs1gQZsTB8GxOXeHgT1T6uUjNfk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e744cb352536eba2767bc6bd474ad0d58dccfceb" title="Connect to your self-hosted LLMs. From anywhere." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to share a small hobby project of mine which I have been building for a couple of months now. I'm looking for some early development test users for some feedback. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Project name&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Reititin&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Reititin connects to your self-hosted LLMs seamlessly. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You create a new agent from Reititin UI and run a simple script on your LLM host machine that connects your Reititin account to your self-hosted LLM.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it's built&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;To allow ease of access to self-hosted LLMs and agents from anywhere. No need for custom VPCs, Tunnels, Proxys, and SSH stuff.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Who it's for&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Reititin is built for people who want to self-host their LLMs and are looking for a simple way to connect to their LLMs from anywhere.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smilulilu"&gt; /u/smilulilu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://test.reititin.com"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfnspn/connect_to_your_selfhosted_llms_from_anywhere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jfnspn/connect_to_your_selfhosted_llms_from_anywhere/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-20T12:30:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jflkgi</id>
    <title>AI powered search engine</title>
    <updated>2025-03-20T10:12:59+00:00</updated>
    <author>
      <name>/u/ItzCrazyKns</name>
      <uri>https://old.reddit.com/user/ItzCrazyKns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jflkgi/ai_powered_search_engine/"&gt; &lt;img alt="AI powered search engine" src="https://external-preview.redd.it/RgriRfUPiH8s8ITdZ836LlSdgg6OmxhWyD5fMfYJIbQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be368c909adb16715ee85011371deeecc490ac0d" title="AI powered search engine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ItzCrazyKns"&gt; /u/ItzCrazyKns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ItzCrazyKns/Perplexica"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jflkgi/ai_powered_search_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jflkgi/ai_powered_search_engine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-20T10:12:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf9lww</id>
    <title>Gemma3:12b performance on my machine for reference.</title>
    <updated>2025-03-19T22:22:55+00:00</updated>
    <author>
      <name>/u/Inner-End7733</name>
      <uri>https://old.reddit.com/user/Inner-End7733</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I finally ran &amp;quot;--verbose&amp;quot; in ollama to see what performance I'm getting on my system:&lt;/p&gt; &lt;p&gt;System: rtx 3060 (12gb vram), xeon w2135 and 64gb (4x16) DDR4 2666 ECC. Running Ollama in a docker container. &lt;/p&gt; &lt;p&gt;I asked Gemma3:12b &amp;quot;what is quantum physics&amp;quot;&lt;/p&gt; &lt;p&gt;total duration: 43.488294213s&lt;/p&gt; &lt;p&gt;load duration: 60.655667ms&lt;/p&gt; &lt;p&gt;prompt eval count: 14 token(s)&lt;/p&gt; &lt;p&gt;prompt eval duration: 60.532467ms&lt;/p&gt; &lt;p&gt;prompt eval rate: 231.28 tokens/s&lt;/p&gt; &lt;p&gt;eval count: 1402 token(s)&lt;/p&gt; &lt;p&gt;eval duration: 43.365955326s&lt;/p&gt; &lt;p&gt;eval rate: 32.33 tokens/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inner-End7733"&gt; /u/Inner-End7733 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jf9lww/gemma312b_performance_on_my_machine_for_reference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jf9lww/gemma312b_performance_on_my_machine_for_reference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jf9lww/gemma312b_performance_on_my_machine_for_reference/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-19T22:22:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jffkne</id>
    <title>MCPs with Ollama</title>
    <updated>2025-03-20T03:12:07+00:00</updated>
    <author>
      <name>/u/cartman-unplugged</name>
      <uri>https://old.reddit.com/user/cartman-unplugged</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone know of a good way to write custom MCP servers and use it with Ollama?&lt;/p&gt; &lt;p&gt;I found one (mcphost), written in Go lang by someone, but looking for other options. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cartman-unplugged"&gt; /u/cartman-unplugged &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jffkne/mcps_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jffkne/mcps_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jffkne/mcps_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-20T03:12:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfm3mg</id>
    <title>app for mac to manage running models, start and stop models, etc</title>
    <updated>2025-03-20T10:50:04+00:00</updated>
    <author>
      <name>/u/Normal-Programmer-51</name>
      <uri>https://old.reddit.com/user/Normal-Programmer-51</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;does this exist? any recommendations? I have a preference for open source but closed source would be fine as much as I can run without CLI (which I can do but I always forgot and is a hurdle leave an open terminal to serve ollama&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Normal-Programmer-51"&gt; /u/Normal-Programmer-51 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfm3mg/app_for_mac_to_manage_running_models_start_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfm3mg/app_for_mac_to_manage_running_models_start_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jfm3mg/app_for_mac_to_manage_running_models_start_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-20T10:50:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfb2s1</id>
    <title>What are the uses for small models ( below 7b )?</title>
    <updated>2025-03-19T23:28:10+00:00</updated>
    <author>
      <name>/u/digitalextremist</name>
      <uri>https://old.reddit.com/user/digitalextremist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Right now I have ~500gb of models and I am trying to find a way to prioritize them and shed some. Seems like it would be wise to have a ~1TB M.2 drive just for LLMs, going forward. But while I feel the squeeze...&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Understanding that anything under &lt;code&gt;32b&lt;/code&gt; is the actual small point ( perhaps &lt;code&gt;70b&lt;/code&gt; ) ... what are the real uses for the teeny tiny models?&lt;/p&gt; &lt;p&gt;My go-to use-cases are code ( non-vibe-coding ) tending toward local-only multi-model agentic soon, document review and analysis in various ways, devops tending toward VPC management by MCP soon, and general information analysis and knowledge-base development. Most of the time I see myself headed into tool-dependence also, but am not there yet.&lt;/p&gt; &lt;p&gt;How do small models fit there? And where do the mid-range models fit? It seems like if something can be thrown at a &lt;code&gt;&amp;gt;8b&lt;/code&gt; model for the same time-cost, why not?&lt;/p&gt; &lt;p&gt;The only real use I can see right now is for conversation for its own sake, for example in the case of psychological interventions where an LLM can supplement companionship, once properly prepared not to death-spiral if someone is in a compromised mental state and will be for years at a time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/digitalextremist"&gt; /u/digitalextremist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfb2s1/what_are_the_uses_for_small_models_below_7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfb2s1/what_are_the_uses_for_small_models_below_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jfb2s1/what_are_the_uses_for_small_models_below_7b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-19T23:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jflnxl</id>
    <title>Structured Outputs in Ollama - What's Your Recipe for Success?</title>
    <updated>2025-03-20T10:19:41+00:00</updated>
    <author>
      <name>/u/RMCPhoto</name>
      <uri>https://old.reddit.com/user/RMCPhoto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with Ollama's structured output feature (using JSON schemas via Pydantic models) and wanted to hear how others are implementing this in their projects. My results have been a bit mixed with Gemma3 and Phi4.&lt;/p&gt; &lt;p&gt;My goal has been information extraction from text.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Questions:&lt;/strong&gt; 1. &lt;strong&gt;Model Performance&lt;/strong&gt;: Which local models (e.g. llama3.1, mixtral, Gemma, phi) have you found most reliable for structured output generation? And for what use case? 2. &lt;strong&gt;Schema Design&lt;/strong&gt;: How are you leveraging Pydantic's field labels/descriptions in your JSON schemas? Are you including semantic descriptions to guide the model? 3. &lt;strong&gt;Prompt Engineering&lt;/strong&gt;: Do you explicitly restate the desired output structure in your prompts &lt;em&gt;in addition&lt;/em&gt; to passing the schema, or rely solely on the schema definition? 4. &lt;strong&gt;Validation Patterns&lt;/strong&gt;: What error handling strategies work best when parsing model responses?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discussion Points:&lt;/strong&gt; - Have you found certain schema structures (nested objects vs flat) work better? - Any clever uses of enums or constrained types? - How does structured output performance compare between models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RMCPhoto"&gt; /u/RMCPhoto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jflnxl/structured_outputs_in_ollama_whats_your_recipe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jflnxl/structured_outputs_in_ollama_whats_your_recipe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jflnxl/structured_outputs_in_ollama_whats_your_recipe/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-20T10:19:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfjose</id>
    <title>Where’s Mistral Small 3.1?</title>
    <updated>2025-03-20T07:48:33+00:00</updated>
    <author>
      <name>/u/tjevns</name>
      <uri>https://old.reddit.com/user/tjevns</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m surprised to see that there’s still no sign of Mistral Small 3.1 available from Ollama. New open models usually have usually appeared by now from official model release. It’s been a couple of days now. Any ideas why?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tjevns"&gt; /u/tjevns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfjose/wheres_mistral_small_31/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jfjose/wheres_mistral_small_31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jfjose/wheres_mistral_small_31/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-20T07:48:33+00:00</published>
  </entry>
</feed>
