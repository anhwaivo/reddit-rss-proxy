<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-27T16:06:56+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ib0hak</id>
    <title>creative ideas</title>
    <updated>2025-01-27T05:07:42+00:00</updated>
    <author>
      <name>/u/Choice_Complaint9171</name>
      <uri>https://old.reddit.com/user/Choice_Complaint9171</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;has anyone attempted to process medical x-rays through ollama vision &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Choice_Complaint9171"&gt; /u/Choice_Complaint9171 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib0hak/creative_ideas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib0hak/creative_ideas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib0hak/creative_ideas/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T05:07:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib0osq</id>
    <title>jacksonville Florida Ollama AI Entrepeneurs</title>
    <updated>2025-01-27T05:20:11+00:00</updated>
    <author>
      <name>/u/Choice_Complaint9171</name>
      <uri>https://old.reddit.com/user/Choice_Complaint9171</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;who in jax is looking to collaborate on creative AI opportunities. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Choice_Complaint9171"&gt; /u/Choice_Complaint9171 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib0osq/jacksonville_florida_ollama_ai_entrepeneurs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib0osq/jacksonville_florida_ollama_ai_entrepeneurs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib0osq/jacksonville_florida_ollama_ai_entrepeneurs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T05:20:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1iapk5s</id>
    <title>Can I restrict a model to only run on CPU?</title>
    <updated>2025-01-26T20:35:25+00:00</updated>
    <author>
      <name>/u/mindsetFPS</name>
      <uri>https://old.reddit.com/user/mindsetFPS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mindsetFPS"&gt; /u/mindsetFPS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iapk5s/can_i_restrict_a_model_to_only_run_on_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iapk5s/can_i_restrict_a_model_to_only_run_on_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iapk5s/can_i_restrict_a_model_to_only_run_on_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T20:35:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib1ryi</id>
    <title>Difference in Llama 3.2 1B instruct and Llama 3.1 8B</title>
    <updated>2025-01-27T06:31:39+00:00</updated>
    <author>
      <name>/u/lonesomhelme</name>
      <uri>https://old.reddit.com/user/lonesomhelme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y'all, noob LLM tweaker here. I wanted to know the difference when working with these models. &lt;/p&gt; &lt;p&gt;For context, I deployed both these models on a cloud provider using vLLM inference. From what I noticed it was not easy to get proper responses from 3.2 1b model and the response were short. When I switched to 3.1 8b I could get better responses.&lt;/p&gt; &lt;p&gt;I want to understand what could've been the issue - is it my setup or the model?&lt;/p&gt; &lt;p&gt;More Context: I tried Llama 3.2 1B from Ollama and everything worked great. But when I switched to pulling the model from HuggingFace it messed up everything. Essentially, it's the same model, so it should work the same. What could be wrong here??&lt;/p&gt; &lt;p&gt;Appreciate any help in understanding this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lonesomhelme"&gt; /u/lonesomhelme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib1ryi/difference_in_llama_32_1b_instruct_and_llama_31_8b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib1ryi/difference_in_llama_32_1b_instruct_and_llama_31_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib1ryi/difference_in_llama_32_1b_instruct_and_llama_31_8b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T06:31:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaq51g</id>
    <title>UI-Tars vs Brower-Use Web-UI</title>
    <updated>2025-01-26T20:57:18+00:00</updated>
    <author>
      <name>/u/DelPrive235</name>
      <uri>https://old.reddit.com/user/DelPrive235</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone know which agent is performing better and more stable at the moment? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DelPrive235"&gt; /u/DelPrive235 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaq51g/uitars_vs_broweruse_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaq51g/uitars_vs_broweruse_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaq51g/uitars_vs_broweruse_webui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T20:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaoigf</id>
    <title>Does deepseek r1 locally support reading images?</title>
    <updated>2025-01-26T19:56:28+00:00</updated>
    <author>
      <name>/u/PawanAgarwal</name>
      <uri>https://old.reddit.com/user/PawanAgarwal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iaoigf/does_deepseek_r1_locally_support_reading_images/"&gt; &lt;img alt="Does deepseek r1 locally support reading images?" src="https://preview.redd.it/ywwq5z6j8efe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc1f5bc8e418faa002c27dfce87f3f7daa503ccb" title="Does deepseek r1 locally support reading images?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PawanAgarwal"&gt; /u/PawanAgarwal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ywwq5z6j8efe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaoigf/does_deepseek_r1_locally_support_reading_images/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaoigf/does_deepseek_r1_locally_support_reading_images/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T19:56:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1iacoys</id>
    <title>Is there a pure, quantised version of DeekSeekr1 for Ollama?</title>
    <updated>2025-01-26T11:55:10+00:00</updated>
    <author>
      <name>/u/john_alan</name>
      <uri>https://old.reddit.com/user/john_alan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see this: &lt;a href="https://ollama.com/library/deepseek-r1"&gt;https://ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;but it appears to be Llama3.2/Qwen trained with DeepSeek r1.&lt;/p&gt; &lt;p&gt;I essentially want an 8bit 70bn version of pure r1?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/john_alan"&gt; /u/john_alan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iacoys/is_there_a_pure_quantised_version_of_deekseekr1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iacoys/is_there_a_pure_quantised_version_of_deekseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iacoys/is_there_a_pure_quantised_version_of_deekseekr1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T11:55:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib4emq</id>
    <title>Size issue with FuseO1 models on ollama</title>
    <updated>2025-01-27T09:29:28+00:00</updated>
    <author>
      <name>/u/alok_saurabh</name>
      <uri>https://old.reddit.com/user/alok_saurabh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ib4emq/size_issue_with_fuseo1_models_on_ollama/"&gt; &lt;img alt="Size issue with FuseO1 models on ollama" src="https://preview.redd.it/4w6rge0m9ife1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=89c0d64e0e157ad8de64255b90257c5a3a8b5c48" title="Size issue with FuseO1 models on ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 3090 + 64 gb ram box. I am able to run R1 32b as expected with good token generation speeds. But same model size of fuseO1 due some reason when loaded to memory expands to 69 gb. If I add another 3090 it starts to occupy 84 gb of ram+ vram. Is something wrong with this model or my setup ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alok_saurabh"&gt; /u/alok_saurabh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4w6rge0m9ife1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib4emq/size_issue_with_fuseo1_models_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib4emq/size_issue_with_fuseo1_models_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T09:29:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib17tv</id>
    <title>Gibberish output a fair amount of the time?</title>
    <updated>2025-01-27T05:54:15+00:00</updated>
    <author>
      <name>/u/ysaric</name>
      <uri>https://old.reddit.com/user/ysaric</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run the ipex version of Ollama (0.5.1-ipexllm-20250123 using instructions from &lt;a href="https://ipex-llm-latest.readthedocs.io/en/latest/doc/LLM/Quickstart/ollama%5C_quickstart.html"&gt;https://ipex-llm-latest.readthedocs.io/en/latest/doc/LLM/Quickstart/ollama\_quickstart.html&lt;/a&gt;) for my Intel a770 16GB card, I have an older Ryzen 5 3600x CPU and 64 GB RAM, although I'm moving this week hopefully to a Ryzen 9 5950X. My OS is Windows 11, I'm running Ollama on a current version of Miniforge3. On the front end I'm using open Webui running via Docker Desktop. For the most part I run models in the 8B range (7B-8B, LLama 3 or 3.1, Deepseek-R1) or in the 13-14B range (Phi4, Qwen 2.5).&lt;/p&gt; &lt;p&gt;The issue I'm having is that a fair amount of time the output from even a pretty simple prompt is basically gibberish, ex:&lt;/p&gt; &lt;p&gt;&amp;quot;List ten awesome bird watching locations in North America&lt;/p&gt; &lt;p&gt;ollama.com/library/deepseek-r1:7bToday at 00:48&lt;/p&gt; &lt;p&gt;search as the0 to jQuery as &amp;gt;&amp;gt; in as as0: in&amp;gt; as the implode you&amp;gt; The on them you your1***&amp;gt; their. we0. &amp;lt; They**:&lt;br /&gt; as .&lt;/p&gt; &lt;p&gt;0&amp;gt;&lt;br /&gt; }}`&amp;lt;/&amp;lt;&amp;gt;&amp;gt; they} they and1.&lt;br /&gt; 13 as0:&lt;br /&gt; your, and&amp;quot;&lt;/p&gt; &lt;p&gt;What's weird is that sometimes these models will work fine, and then sometimes they output gibberish. Some models seem more likely to spout gibberish, but honestly that's all anecdote. None seem immune. My miniforge window is showing no obvious errors or crashes, nothing like I am exceeding memory capacity.&lt;/p&gt; &lt;p&gt;Just thought I would see if anyone else has run into or runs into issues like this, and certainly if there was a way to stop it from happening or make it happen less often I would be all ears.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ysaric"&gt; /u/ysaric &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib17tv/gibberish_output_a_fair_amount_of_the_time/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib17tv/gibberish_output_a_fair_amount_of_the_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib17tv/gibberish_output_a_fair_amount_of_the_time/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T05:54:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1iavfc6</id>
    <title>8x AMD Instinct Mi60 Server + vLLM + unsloth/DeepSeek-R1-Distill-Qwen-32B FP16</title>
    <updated>2025-01-27T00:46:36+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vpaab64pkffe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iavfc6/8x_amd_instinct_mi60_server_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iavfc6/8x_amd_instinct_mi60_server_vllm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T00:46:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib6toj</id>
    <title>Same model on Ollama performing worse than Cloud providers (Groq, HF, ...)</title>
    <updated>2025-01-27T11:45:33+00:00</updated>
    <author>
      <name>/u/ParaplegicGuru</name>
      <uri>https://old.reddit.com/user/ParaplegicGuru</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. &lt;/p&gt; &lt;p&gt;I have a prompt that returns a wrong answer to a question when asking LLama hosted on Ollama. However, when asking the same model hosted on Groq or Hugging Face or any other cloud provider I get a correct answer to the question.&lt;/p&gt; &lt;p&gt;The prompt is a RAG prompt, it contains instructions, context and a question. The context and the question are in portuguese. However, I am sending exactly the same prompt in all situations. Why do they all answer correctly but the Ollama one does not? &lt;/p&gt; &lt;p&gt;I am going insane over this, I would thank any help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaplegicGuru"&gt; /u/ParaplegicGuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib6toj/same_model_on_ollama_performing_worse_than_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib6toj/same_model_on_ollama_performing_worse_than_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib6toj/same_model_on_ollama_performing_worse_than_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T11:45:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaht3c</id>
    <title>DeepSeek-R1's Bias</title>
    <updated>2025-01-26T15:47:14+00:00</updated>
    <author>
      <name>/u/cov_id19</name>
      <uri>https://old.reddit.com/user/cov_id19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iaht3c/deepseekr1s_bias/"&gt; &lt;img alt="DeepSeek-R1's Bias" src="https://preview.redd.it/41tjd3220dfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d3331ed008c8f650aba9895a60c4c649bd8ab47d" title="DeepSeek-R1's Bias" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cov_id19"&gt; /u/cov_id19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/41tjd3220dfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iaht3c/deepseekr1s_bias/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iaht3c/deepseekr1s_bias/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-26T15:47:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib7fk6</id>
    <title>data extraction</title>
    <updated>2025-01-27T12:16:19+00:00</updated>
    <author>
      <name>/u/Big_Award9653</name>
      <uri>https://old.reddit.com/user/Big_Award9653</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I am new to data extraction. Please help&lt;br /&gt; there's a comment/review column in my google sheets, which contains long text like paragraphs of 10 lines. Now, i have to extract the product id from that column. Regex doesn't seem a good approach here.&lt;/p&gt; &lt;p&gt;For example i have to extract all the product ids from below comment. :&lt;br /&gt; I ordered prodcut123 but received a different product which has id as 456. I want refund of $10.&lt;/p&gt; &lt;p&gt;output : ['Product123', 'Product456']&lt;/p&gt; &lt;p&gt;how do i do this ? Help me out with free resources. I am using python (Jupyter Notebook).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Big_Award9653"&gt; /u/Big_Award9653 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib7fk6/data_extraction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib7fk6/data_extraction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib7fk6/data_extraction/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T12:16:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib7fqc</id>
    <title>Updated to ollama 0.5.7 -- everything slow?</title>
    <updated>2025-01-27T12:16:32+00:00</updated>
    <author>
      <name>/u/soyokaze42</name>
      <uri>https://old.reddit.com/user/soyokaze42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I updated to ollama 0.5.7 and I feel like everything is unreasonably slow now. Just checking if it's only me (on macOS and on a Docker image) or if you are also experiencing this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/soyokaze42"&gt; /u/soyokaze42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib7fqc/updated_to_ollama_057_everything_slow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib7fqc/updated_to_ollama_057_everything_slow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib7fqc/updated_to_ollama_057_everything_slow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T12:16:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib86hn</id>
    <title>deepseek-r1 671B - windows 11</title>
    <updated>2025-01-27T12:52:00+00:00</updated>
    <author>
      <name>/u/Polymorphin</name>
      <uri>https://old.reddit.com/user/Polymorphin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ib86hn/deepseekr1_671b_windows_11/"&gt; &lt;img alt="deepseek-r1 671B - windows 11" src="https://b.thumbs.redditmedia.com/Y0jBBoAZwDT2O3DpZt-H4FBG2JIrF0--Sf_qDgvjlyA.jpg" title="deepseek-r1 671B - windows 11" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys,&lt;br /&gt; I got 2 questions:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. How can i change the folder where models are downloaded / inserted to for ollama? Currently its the standard path on C i guess i want to change it to D.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;ollama run deepseek-r1:671b -&amp;gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8yujz4ra9jfe1.png?width=972&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=03963fc4c67407c0614138f60a30bb7dcb9a9a2e"&gt;https://preview.redd.it/8yujz4ra9jfe1.png?width=972&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=03963fc4c67407c0614138f60a30bb7dcb9a9a2e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Will my PC be able to run it ?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Prozessor Intel(R) Core(TM) i7-14700KF 3.40 GHz&lt;/p&gt; &lt;p&gt;Installierter RAM 48,0 GB (47,8 GB verwendbar)&lt;/p&gt; &lt;p&gt;GTX 4080&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/90ft2ef19jfe1.png?width=1914&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7a0a29c73e5ffc9924773c15dc25d7be13cfb069"&gt;https://preview.redd.it/90ft2ef19jfe1.png?width=1914&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7a0a29c73e5ffc9924773c15dc25d7be13cfb069&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Polymorphin"&gt; /u/Polymorphin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib86hn/deepseekr1_671b_windows_11/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib86hn/deepseekr1_671b_windows_11/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib86hn/deepseekr1_671b_windows_11/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T12:52:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib8opq</id>
    <title>Building for LLMs</title>
    <updated>2025-01-27T13:14:08+00:00</updated>
    <author>
      <name>/u/vir_db</name>
      <uri>https://old.reddit.com/user/vir_db</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;i'm planning to build a new (but cheap) installation for Ollama and other LLM related stuff (like Comfyui and OpenDai Speech).&lt;/p&gt; &lt;p&gt;Currently I'm running on already owned commodity hardware that works fine, but it cannot support dual GPU configuration.&lt;/p&gt; &lt;p&gt;I've the opportunity to get a &lt;a href="https://www.asrock.com/MB/Intel/B660M%20Pro%20RS/index.it.asp"&gt;Asrock B660M Pro RS &lt;/a&gt;used mobo with i5 CPU for cheap&lt;/p&gt; &lt;p&gt;My questions is: this mobo will supports dual GPU (rtx 3060 and gtx 1060, that I already own but maybe in future something better)?&lt;/p&gt; &lt;p&gt;As far as I can see, there is enough space, but I want to avoid surprises.&lt;/p&gt; &lt;p&gt;All that stuff, will be supported by i5 processor, 64GB of RAM and &lt;a href="https://www.fsplifestyle.com/en/product/HYDROGPRO1000W.html"&gt;1000w modular ATX power supply&lt;/a&gt; (I already own this one).&lt;/p&gt; &lt;p&gt;Thanks a lot&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vir_db"&gt; /u/vir_db &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib8opq/building_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib8opq/building_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib8opq/building_for_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T13:14:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib939q</id>
    <title>Idea on how to deal with dates on RAG</title>
    <updated>2025-01-27T13:31:22+00:00</updated>
    <author>
      <name>/u/AmrElsayedEGY</name>
      <uri>https://old.reddit.com/user/AmrElsayedEGY</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a RAG pipeline that fetch the data from vector DB (Chroma) and then pass it to LLM model (Ollama), My vector db has info for sales and customers, &lt;/p&gt; &lt;p&gt;So if user asked something like &amp;quot;What is the latest order?&amp;quot;, The search inside Vector DB probably will get wrong answers cause it will not consider date, it only will check for similarity between query and the DB, So it will get random documents, (k is something around 10)&lt;/p&gt; &lt;p&gt;So my question is, What approaches should i use to accomplish this? I need the context being passed to LLM to contain the correct data, I have both customer and sales info in the same vector DB&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AmrElsayedEGY"&gt; /u/AmrElsayedEGY &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib939q/idea_on_how_to_deal_with_dates_on_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib939q/idea_on_how_to_deal_with_dates_on_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib939q/idea_on_how_to_deal_with_dates_on_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T13:31:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib98w0</id>
    <title>DeepSeek with VSCode very chatty</title>
    <updated>2025-01-27T13:37:56+00:00</updated>
    <author>
      <name>/u/liquidnitrogen</name>
      <uri>https://old.reddit.com/user/liquidnitrogen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying DeepSeek 8b with VSCode connected via `Connect` plugin - if i ask DeepSeek in VSCode to review the code it goes into a long rant talking to itself before spitting out the answer, however if i copy paste the code to &lt;a href="http://chat.deepseek.com"&gt;chat.deepseek.com&lt;/a&gt; or ask locally on web-openui and ask to do the same, it right away give the concise answer. Can someone please explain why there is a difference and how I can make it work like the web interface?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liquidnitrogen"&gt; /u/liquidnitrogen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib98w0/deepseek_with_vscode_very_chatty/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib98w0/deepseek_with_vscode_very_chatty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib98w0/deepseek_with_vscode_very_chatty/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T13:37:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib9m4s</id>
    <title>Ollama not installing</title>
    <updated>2025-01-27T13:55:25+00:00</updated>
    <author>
      <name>/u/Kind_Hedgehog5496</name>
      <uri>https://old.reddit.com/user/Kind_Hedgehog5496</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can someone please help. I am very new to much of this and trying to understand what went wrong. I am trying to install Ollama on my mac and went through all the steps but I keep getting this response. when I enter into terminal &lt;/p&gt; &lt;p&gt;&amp;quot;ollama run llama3.2&amp;quot;&lt;/p&gt; &lt;p&gt;it responses with &lt;/p&gt; &lt;p&gt;&amp;quot;zsh: command not found: ollama&amp;quot;&lt;/p&gt; &lt;p&gt;can someone help me fix this issue. &lt;/p&gt; &lt;p&gt;i downloaded the ollama from the website and followed the steps for the install but still get this. &lt;/p&gt; &lt;p&gt;thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kind_Hedgehog5496"&gt; /u/Kind_Hedgehog5496 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib9m4s/ollama_not_installing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib9m4s/ollama_not_installing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib9m4s/ollama_not_installing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T13:55:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibaogt</id>
    <title>homebrew ollama client problems with local network access</title>
    <updated>2025-01-27T14:44:49+00:00</updated>
    <author>
      <name>/u/sandipb</name>
      <uri>https://old.reddit.com/user/sandipb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running ollama on a remote host in my local network. On my laptop, I installed ollama via homebrew, and set OLLAMA_HOST correctly.&lt;/p&gt; &lt;p&gt;I can reach the host via curl, and I can use the ollama commandline via sudo to access the remote host. However, it doesn't work with my user privileges.&lt;/p&gt; &lt;p&gt;I am suspecting that this is due to the Local Network access permissions that Sequoia 15.2 has. Since the commandline is not prompting a dialog to ask for the permission, I cannot whitelist it.&lt;/p&gt; &lt;p&gt;Doesn anyone have a workaround I can use? I am considering reverting to the ollama desktop app to fix this, but I prefer homebrew to keep such apps updated.&lt;/p&gt; &lt;p&gt;```shell-session $ ollama list Error: Head &amp;quot;&lt;a href="http://192.168.1.20:11434/%22:"&gt;http://192.168.1.20:11434/&amp;quot;:&lt;/a&gt; dial tcp 192.168.1.20:11434: connect: no route to host&lt;/p&gt; &lt;p&gt;$ curl -s $OLLAMA_HOST;echo Ollama is running&lt;/p&gt; &lt;p&gt;$ sudo -E ollama list Password: NAME ID SIZE MODIFIED&lt;/p&gt; &lt;p&gt;hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF:Q8_0 69f0e15b99a1 8.5 GB 6 days ago&lt;/p&gt; &lt;p&gt;llama3.2:latest a80c4f17acd5 2.0 GB 5 weeks ago ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandipb"&gt; /u/sandipb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibaogt/homebrew_ollama_client_problems_with_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibaogt/homebrew_ollama_client_problems_with_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibaogt/homebrew_ollama_client_problems_with_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T14:44:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibbpzo</id>
    <title>Difference in Llama 3.2 1B instruct and Llama 3.1 8B</title>
    <updated>2025-01-27T15:28:11+00:00</updated>
    <author>
      <name>/u/lonesomhelme</name>
      <uri>https://old.reddit.com/user/lonesomhelme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y'all, noob LLM tweaker here. I wanted to know the difference when working with these models. &lt;/p&gt; &lt;p&gt;For context, I deployed both these models on a cloud provider using vLLM inference. From what I noticed it was not easy to get proper responses from 3.2 1b model and the response were short. When I switched to 3.1 8b I could get better responses.&lt;/p&gt; &lt;p&gt;I want to understand what could've been the issue - is it my setup or the model?&lt;/p&gt; &lt;p&gt;More Context: I tried Llama 3.2 1B from Ollama and everything worked great. But when I switched to pulling the model from HuggingFace it messed up everything. Essentially, it's the same model, so it should work the same. What could be wrong here??&lt;/p&gt; &lt;p&gt;Appreciate any help in understanding this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lonesomhelme"&gt; /u/lonesomhelme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibbpzo/difference_in_llama_32_1b_instruct_and_llama_31_8b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibbpzo/difference_in_llama_32_1b_instruct_and_llama_31_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibbpzo/difference_in_llama_32_1b_instruct_and_llama_31_8b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T15:28:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib0ddn</id>
    <title>ollama server</title>
    <updated>2025-01-27T05:01:35+00:00</updated>
    <author>
      <name>/u/Choice_Complaint9171</name>
      <uri>https://old.reddit.com/user/Choice_Complaint9171</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i want to setup ollama to be able to receive request from me in other words i want to be able to communicate with my local ollama on my desktop pc remotely through my iphone can someone confirm if this is possible i would assume you can make this possible with webui something like assigning authentication through ip address for remote use &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Choice_Complaint9171"&gt; /u/Choice_Complaint9171 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib0ddn/ollama_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib0ddn/ollama_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib0ddn/ollama_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T05:01:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib8ysc</id>
    <title>Guidance for Creating RAG with Ollama and LangChain for Java Code Analysis</title>
    <updated>2025-01-27T13:26:04+00:00</updated>
    <author>
      <name>/u/Terrible-Parfait-868</name>
      <uri>https://old.reddit.com/user/Terrible-Parfait-868</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Team,&lt;/p&gt; &lt;p&gt;I need assistance in creating a Retrieval-Augmented Generation (RAG) setup using Ollama and LangChain to analyze Java code.&lt;/p&gt; &lt;p&gt;Expectations:&lt;/p&gt; &lt;p&gt;The system should identify code connections using import statements.&lt;/p&gt; &lt;p&gt;It should track values or functions from imports.&lt;/p&gt; &lt;p&gt;It should answer questions like:&lt;/p&gt; &lt;p&gt;What does a specific function do?&lt;/p&gt; &lt;p&gt;What input is expected for a given function?&lt;/p&gt; &lt;p&gt;Challenge: The project comprises over 200 Java files and exceeds 200k total tokens, making efficient handling critical.&lt;/p&gt; &lt;p&gt;Looking forward to your suggestions or guidance on how to approach this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terrible-Parfait-868"&gt; /u/Terrible-Parfait-868 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib8ysc/guidance_for_creating_rag_with_ollama_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ib8ysc/guidance_for_creating_rag_with_ollama_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ib8ysc/guidance_for_creating_rag_with_ollama_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T13:26:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibadzz</id>
    <title>LLM powered scraping with ollama</title>
    <updated>2025-01-27T14:31:29+00:00</updated>
    <author>
      <name>/u/Financial-Article-12</name>
      <uri>https://old.reddit.com/user/Financial-Article-12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been developing Parsera, a simple yet powerful Python library that leverages LLMs for web scraping. Many users requested Ollama support, and now that I’ve added it, I wanted to share it with the Ollama community.&lt;/p&gt; &lt;p&gt;If you are looking for a way to extract data from websites (especially when dealing with multiple websites), Parsera lets you do it with just a few lines of code, without the need to develop custom scrapers.&lt;/p&gt; &lt;p&gt;Give it a try and let me know what you think. Feedback is always welcome!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/raznem/parsera"&gt;github.com/raznem/parsera&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial-Article-12"&gt; /u/Financial-Article-12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibadzz/llm_powered_scraping_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibadzz/llm_powered_scraping_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibadzz/llm_powered_scraping_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T14:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iawxvm</id>
    <title>Deepseek-R1:8b</title>
    <updated>2025-01-27T02:02:52+00:00</updated>
    <author>
      <name>/u/Choice_Complaint9171</name>
      <uri>https://old.reddit.com/user/Choice_Complaint9171</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone please help me from my ignorance is it me or does deepseek R1 :8b seem to resemble a young adult with high IQ and a lot of self doubt &lt;/p&gt; &lt;p&gt;To confirm I’m not ignorant myself try asking “is buffalo a part of New York?” In your setup. &lt;/p&gt; &lt;h1&gt;just your Average AI Contributor&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Choice_Complaint9171"&gt; /u/Choice_Complaint9171 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iawxvm/deepseekr18b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iawxvm/deepseekr18b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iawxvm/deepseekr18b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T02:02:52+00:00</published>
  </entry>
</feed>
