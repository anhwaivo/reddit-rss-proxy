<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-06-02T17:07:02+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1kymosf</id>
    <title>I built a local email summary dashboard</title>
    <updated>2025-05-29T21:25:25+00:00</updated>
    <author>
      <name>/u/vishruth555</name>
      <uri>https://old.reddit.com/user/vishruth555</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kymosf/i_built_a_local_email_summary_dashboard/"&gt; &lt;img alt="I built a local email summary dashboard" src="https://preview.redd.it/6p78z7ujgs3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=367bc299f4e2cf14d71a166dbe4e68f63942f8bf" title="I built a local email summary dashboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I often forget to check my emails, so I developed a tool that summarizes my inbox into a concise dashboard.&lt;/p&gt; &lt;p&gt;Features: ‚Ä¢ Runs locally using Ollama, Gemini api key can also be used for faster summaries at the cost of your privacy &lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ Summarizes Gmail inboxes into a clean, readable format ‚Ä¢ can be run in a container &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Check it out here: &lt;a href="https://github.com/vishruth555/mailBrief"&gt;https://github.com/vishruth555/mailBrief&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôd love to hear your feedback or suggestions for improvement!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vishruth555"&gt; /u/vishruth555 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6p78z7ujgs3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kymosf/i_built_a_local_email_summary_dashboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kymosf/i_built_a_local_email_summary_dashboard/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-29T21:25:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzc4i9</id>
    <title>Sorry for the NOOB question. :) - How to connect local OLLAMA instance with my MCP-Servers completely offline?</title>
    <updated>2025-05-30T18:47:44+00:00</updated>
    <author>
      <name>/u/Dorfmueller</name>
      <uri>https://old.reddit.com/user/Dorfmueller</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dorfmueller"&gt; /u/Dorfmueller &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/mcp/comments/1kxi0ik/sorry_for_the_noob_question_how_to_connect_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kzc4i9/sorry_for_the_noob_question_how_to_connect_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kzc4i9/sorry_for_the_noob_question_how_to_connect_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-30T18:47:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz7jdc</id>
    <title>Hosting Qwen 3 4B</title>
    <updated>2025-05-30T15:44:51+00:00</updated>
    <author>
      <name>/u/prahasanam-boi</name>
      <uri>https://old.reddit.com/user/prahasanam-boi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I vibe coded a telegram bot that uses Qwen 3 4B model (currently served via ollama). The bot works fine with my 16 gb laptop (No GPU) and can be currently accessed at a time by 3 people (didn't test further). Now I have two questions :&lt;/p&gt; &lt;p&gt;1) What are the ways to host this bot somewhere cheap and reliable. Is there any preference from experienced people here ? (At the most there will be 3/4 people user at a time)&lt;/p&gt; &lt;p&gt;2) Currently the maximum number of users gonna be 4/5, so ollama is fine. However, I am curious to know what is the reliable tool to scale this bot for many users, say in the order of 1000s of users. Any direction in this regard will be helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prahasanam-boi"&gt; /u/prahasanam-boi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kz7jdc/hosting_qwen_3_4b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kz7jdc/hosting_qwen_3_4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kz7jdc/hosting_qwen_3_4b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-30T15:44:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz4g01</id>
    <title>LLM for text to speech similar to Elevenlabs?</title>
    <updated>2025-05-30T13:38:02+00:00</updated>
    <author>
      <name>/u/sethshoultes</name>
      <uri>https://old.reddit.com/user/sethshoultes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for recommendations for a TTS LLM to create an audio book of my writings. I have over 1.1 million words written and don't want to burn up credits on Elevenlabs.&lt;/p&gt; &lt;p&gt;I'm currently using Ollama with Open WebUI as well as LM Studio on a Mac Studio M3 64gb.&lt;/p&gt; &lt;p&gt;Any recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sethshoultes"&gt; /u/sethshoultes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kz4g01/llm_for_text_to_speech_similar_to_elevenlabs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kz4g01/llm_for_text_to_speech_similar_to_elevenlabs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kz4g01/llm_for_text_to_speech_similar_to_elevenlabs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-30T13:38:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz1pk7</id>
    <title>[Release] Cognito AI Search v1.2.0 ‚Äì Fully Re-imagined, Lightning Fast, Now Prettier Than Ever</title>
    <updated>2025-05-30T11:21:31+00:00</updated>
    <author>
      <name>/u/kekePower</name>
      <uri>https://old.reddit.com/user/kekePower</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; üëã&lt;/p&gt; &lt;p&gt;Just dropped &lt;strong&gt;v1.2.0&lt;/strong&gt; of &lt;a href="https://github.com/kekePower/cognito-ai-search"&gt;Cognito AI Search&lt;/a&gt; ‚Äî and it‚Äôs the biggest update yet.&lt;/p&gt; &lt;p&gt;Over the last few days I‚Äôve completely reimagined the experience with a new UI, performance boosts, PDF export, and deep architectural cleanup. The goal remains the same: private AI + anonymous web search, in one fast and beautiful interface you can fully control.&lt;/p&gt; &lt;p&gt;Here‚Äôs what‚Äôs new:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Major UI/UX Overhaul&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Brand-new ‚ÄúHolographic Shard‚Äù design system (crystalline UI, glow effects, glass morphism)&lt;/li&gt; &lt;li&gt;Dark and light mode support with responsive layouts for all screen sizes&lt;/li&gt; &lt;li&gt;Updated typography, icons, gradients, and no-scroll landing experience&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Performance Improvements&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Build time cut from 5 seconds to 2 seconds (60% faster)&lt;/li&gt; &lt;li&gt;Removed 30,000+ lines of unused UI code and 28 unused dependencies&lt;/li&gt; &lt;li&gt;Reduced bundle size, faster initial page load, improved interactivity&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Enhanced Search &amp;amp; AI&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;200+ categorized search suggestions across 16 AI/tech domains&lt;/li&gt; &lt;li&gt;Export your searches and AI answers as beautifully formatted PDFs (supports LaTeX, Markdown, code blocks)&lt;/li&gt; &lt;li&gt;Modern Next.js 15 form system with client-side transitions and real-time loading feedback&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Improved Architecture&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Modular separation of the &lt;a href="https://ollama.com/"&gt;Ollama&lt;/a&gt; and &lt;a href="https://github.com/searxng/searxng"&gt;SearXNG&lt;/a&gt; integration layers&lt;/li&gt; &lt;li&gt;Reusable React components and hooks&lt;/li&gt; &lt;li&gt;Type-safe API and caching layer with automatic expiration and deduplication&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes &amp;amp; Compatibility&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hydration issues fixed (no more React warnings)&lt;/li&gt; &lt;li&gt;Fixed Firefox layout bugs and Zen browser quirks&lt;/li&gt; &lt;li&gt;Compatible with Ollama 0.9.0+ and self-hosted SearXNG setups&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Still fully local. No tracking. No telemetry. Just you, your machine, and clean search.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Try it now ‚Üí &lt;a href="https://github.com/kekePower/cognito-ai-search"&gt;https://github.com/kekePower/cognito-ai-search&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full release notes ‚Üí &lt;a href="https://github.com/kekePower/cognito-ai-search/blob/main/docs/RELEASE_NOTES_v1.2.0.md"&gt;https://github.com/kekePower/cognito-ai-search/blob/main/docs/RELEASE_NOTES_v1.2.0.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback, issues, or even a PR if you find something worth tweaking. Thanks for all the support so far ‚Äî this has been a blast to build.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kekePower"&gt; /u/kekePower &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kz1pk7/release_cognito_ai_search_v120_fully_reimagined/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kz1pk7/release_cognito_ai_search_v120_fully_reimagined/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kz1pk7/release_cognito_ai_search_v120_fully_reimagined/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-30T11:21:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzf75z</id>
    <title>Is there any ollama frontend that can work like novelAI.</title>
    <updated>2025-05-30T20:54:04+00:00</updated>
    <author>
      <name>/u/blueandazure</name>
      <uri>https://old.reddit.com/user/blueandazure</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Where you can set cards for characters locations and themes ect for the ai to remember and you can work to write a story together, but using ollama as the backend.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blueandazure"&gt; /u/blueandazure &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kzf75z/is_there_any_ollama_frontend_that_can_work_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kzf75z/is_there_any_ollama_frontend_that_can_work_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kzf75z/is_there_any_ollama_frontend_that_can_work_like/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-30T20:54:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kza8y8</id>
    <title>The "simplified" model version names are actually increasing confusion</title>
    <updated>2025-05-30T17:31:35+00:00</updated>
    <author>
      <name>/u/florinandrei</name>
      <uri>https://old.reddit.com/user/florinandrei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I understand what Ollama is trying to do - make it dead simple to run LLMs locally. That includes the way the models in the Ollama collection are named.&lt;/p&gt; &lt;p&gt;But I think the &amp;quot;simplification&amp;quot; has been taken too far. The updated DeepSeek-R1 has been released recently. Ollama already had a deepseek-r1 model name in its collection.&lt;/p&gt; &lt;p&gt;Instead of starting a new name, e.g. deepseek-r1-0528 or something, the updates are now overwriting the old name. But wait, not all the old name tags are updated! Only some. Wow.&lt;/p&gt; &lt;p&gt;It's even hard to tell now which tags are the old DeepSeek, and which are the new. It seems like deepseek-r1:8b is the new version. It seems like none of the others are the updated model, but that's a little unclear w.r.t. the biggest model.&lt;/p&gt; &lt;p&gt;Folks, I'm all for simplifying things. But please don't dumb it down to the point where you're increasing confusion. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/florinandrei"&gt; /u/florinandrei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kza8y8/the_simplified_model_version_names_are_actually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kza8y8/the_simplified_model_version_names_are_actually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kza8y8/the_simplified_model_version_names_are_actually/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-30T17:31:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzni23</id>
    <title>Best uncensored model for writing stories</title>
    <updated>2025-05-31T03:33:52+00:00</updated>
    <author>
      <name>/u/MilaAmane</name>
      <uri>https://old.reddit.com/user/MilaAmane</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been playing around with ollama and I was wondering what the best uncensored, a I model for storytelling, is not for role play, but just for storytelling. Cause one thing i've noticed about a lot of the other models is that they all have the same. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MilaAmane"&gt; /u/MilaAmane &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kzni23/best_uncensored_model_for_writing_stories/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kzni23/best_uncensored_model_for_writing_stories/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kzni23/best_uncensored_model_for_writing_stories/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-31T03:33:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1l04ept</id>
    <title>Crawl4AI + Ollama + Remote headless browsers tutorial</title>
    <updated>2025-05-31T18:45:16+00:00</updated>
    <author>
      <name>/u/BlitzBrowser_</name>
      <uri>https://old.reddit.com/user/BlitzBrowser_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l04ept/crawl4ai_ollama_remote_headless_browsers_tutorial/"&gt; &lt;img alt="Crawl4AI + Ollama + Remote headless browsers tutorial" src="https://preview.redd.it/4y834cnww54f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67b137c74397b13a76dd9f03692a63f99376ef13" title="Crawl4AI + Ollama + Remote headless browsers tutorial" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlitzBrowser_"&gt; /u/BlitzBrowser_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4y834cnww54f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l04ept/crawl4ai_ollama_remote_headless_browsers_tutorial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l04ept/crawl4ai_ollama_remote_headless_browsers_tutorial/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-31T18:45:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzp7cz</id>
    <title>Thinking models</title>
    <updated>2025-05-31T05:11:46+00:00</updated>
    <author>
      <name>/u/HashMismatch</name>
      <uri>https://old.reddit.com/user/HashMismatch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama has just released 0.9 supporting showing the ‚Äúthought process‚Äù of thinking models (like DeepSeek-R1 and Qwen3) separate to the output. If a LLM is essentially text prediction based on a vector database and conceptual analytics, how is it ‚Äúthinking‚Äù at all? Is the ‚Äúthinking‚Äù output just text prediction as well?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HashMismatch"&gt; /u/HashMismatch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kzp7cz/thinking_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kzp7cz/thinking_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kzp7cz/thinking_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-31T05:11:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzy7yk</id>
    <title>Is Llama-Guard-4 coming to Ollama?</title>
    <updated>2025-05-31T14:20:37+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Llama-guard3 is in Ollama, but what about the Llama-guard-4? Is it coming? &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-Guard-4-12B"&gt;https://huggingface.co/meta-llama/Llama-Guard-4-12B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kzy7yk/is_llamaguard4_coming_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kzy7yk/is_llamaguard4_coming_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kzy7yk/is_llamaguard4_coming_to_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-31T14:20:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1l04s8s</id>
    <title>How to access ollama with an apache reverse proxy?</title>
    <updated>2025-05-31T19:01:39+00:00</updated>
    <author>
      <name>/u/jgpip</name>
      <uri>https://old.reddit.com/user/jgpip</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have ollama and open webui setup and working fine locally. I can access &lt;a href="http://10.1.50.200:8080"&gt;http://10.1.50.200:8080&lt;/a&gt; and log in and access everything normally.&lt;/p&gt; &lt;p&gt;I have an apache server setup to do reverse proxy of my other services. I try to setup a domain &lt;a href="https://ollama.mydomain.com"&gt;https://ollama.mydomain.com&lt;/a&gt; and I can access it. I can log in but all I get is spinning circles and the new chat menu on the left.&lt;/p&gt; &lt;p&gt;I have this in my config file for ollama.mydomain.com&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ProxyPass / http://10.1.50.200:8080/ ProxyPassReverse / http://10.1.50.200:8080/ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What am I missing to get this working?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jgpip"&gt; /u/jgpip &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l04s8s/how_to_access_ollama_with_an_apache_reverse_proxy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l04s8s/how_to_access_ollama_with_an_apache_reverse_proxy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l04s8s/how_to_access_ollama_with_an_apache_reverse_proxy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-31T19:01:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1l08k4w</id>
    <title>Ollama refuses to use GPU even on 1.5b parameter models</title>
    <updated>2025-05-31T21:49:44+00:00</updated>
    <author>
      <name>/u/FaithfulWise</name>
      <uri>https://old.reddit.com/user/FaithfulWise</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, for some context here, I am using a 8gb RTX 3070, rx 5500, 32gb of ram and 512gb of storage dedicated to ollama. I've been trying to run Qwen3 on my gpu with no avail, even the 0.6 billion parameter model fails to run on gpu and cpu is being used. In ollama's logs, the gpu is being detected but it isn't using it. Any help is appreciated! (I want to run qwen3:8b or qwen3:4b)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FaithfulWise"&gt; /u/FaithfulWise &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l08k4w/ollama_refuses_to_use_gpu_even_on_15b_parameter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l08k4w/ollama_refuses_to_use_gpu_even_on_15b_parameter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l08k4w/ollama_refuses_to_use_gpu_even_on_15b_parameter/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-31T21:49:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1l08z0a</id>
    <title>Dual 5090 vs single PRO 6000 for inference, etc</title>
    <updated>2025-05-31T22:08:13+00:00</updated>
    <author>
      <name>/u/jsconiers</name>
      <uri>https://old.reddit.com/user/jsconiers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm putting together a high end workstation and purchased a 5090 thinking I would go to two 5090s later on. My use case at this time is running multiple different models (largest available) based on use and mostly inference and image generation but I would also want to dive into minor model training for specific tasks later. A single 5090 at the moment fits my needs. There is a possibility I could get a Pro 6000 at a reduced price. My question is would a dual 5090 or a single pro 6000 be better. I'm under the impression the dual 5090s would beat the single pro 6000 in almost every aspect except available memory (64gb vs 96gb) though I am aware two 5090s doesn't double a single 5090's performance. Power consumnption is not a problem as the workstaiton has dual 1600 PSUs. This is a dual xeon workstation with full bandwidth PCIE5 slots and 256GB of memory. What would be your advice? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jsconiers"&gt; /u/jsconiers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l08z0a/dual_5090_vs_single_pro_6000_for_inference_etc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l08z0a/dual_5090_vs_single_pro_6000_for_inference_etc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l08z0a/dual_5090_vs_single_pro_6000_for_inference_etc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-31T22:08:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzz0g1</id>
    <title>Use MCP to run computer use in a VM.</title>
    <updated>2025-05-31T14:55:49+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kzz0g1/use_mcp_to_run_computer_use_in_a_vm/"&gt; &lt;img alt="Use MCP to run computer use in a VM." src="https://external-preview.redd.it/dnZ6YXlybnVzNDRmMSBTlOtFiw3CN60nCKAl7ym9Md7o0mszJARyFHwBNilc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c7521462a35e1eaa1e4e1ae38f70242267ef8ef" title="Use MCP to run computer use in a VM." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MCP Server with Computer Use Agent runs through Claude Desktop, Cursor, and other MCP clients.&lt;/p&gt; &lt;p&gt;An example use case lets try using Claude as a tutor to learn how to use Tableau.&lt;/p&gt; &lt;p&gt;The MCP Server implementation exposes CUA's full functionality through standardized tool calls. It supports single-task commands and multi-task sequences, giving Claude Desktop direct access to all of Cua's computer control capabilities.&lt;/p&gt; &lt;p&gt;This is the first MCP-compatible computer control solution that works directly with Claude Desktop's and Cursor's built-in MCP implementation. Simple configuration in your claude_desktop_config.json or cursor_config.json connects Claude or Cursor directly to your desktop environment.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Discord : &lt;a href="https://discord.gg/4fuebBsAUj"&gt;https://discord.gg/4fuebBsAUj&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/aod9h4yus44f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kzz0g1/use_mcp_to_run_computer_use_in_a_vm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kzz0g1/use_mcp_to_run_computer_use_in_a_vm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-31T14:55:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0megt</id>
    <title>Minisforum UM890 Pro Mini-PC Barebone AMD Ryzen 9 8945HS, Radeon 780M, Oculink f√ºr eGPU, USB4, Wi-Fi 6E, 2√ó 2.5G LAN. Good for Olama?</title>
    <updated>2025-06-01T11:10:13+00:00</updated>
    <author>
      <name>/u/mswedv777</name>
      <uri>https://old.reddit.com/user/mswedv777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you think? Will IT BE W√∂rth with 128 GB RAM trying to use as Add on to a proxmox Server with some ai Assistent Features as wake on LAN in demand ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mswedv777"&gt; /u/mswedv777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l0megt/minisforum_um890_pro_minipc_barebone_amd_ryzen_9/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l0megt/minisforum_um890_pro_minipc_barebone_amd_ryzen_9/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l0megt/minisforum_um890_pro_minipc_barebone_amd_ryzen_9/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-01T11:10:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1l04tac</id>
    <title>Crawl4AI + Ollama + Remote headless browsers</title>
    <updated>2025-05-31T19:02:50+00:00</updated>
    <author>
      <name>/u/BlitzBrowser_</name>
      <uri>https://old.reddit.com/user/BlitzBrowser_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l04tac/crawl4ai_ollama_remote_headless_browsers/"&gt; &lt;img alt="Crawl4AI + Ollama + Remote headless browsers" src="https://preview.redd.it/atz2h7hk064f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad5782230ae93558ada6cc6a3e7626d97db706a6" title="Crawl4AI + Ollama + Remote headless browsers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlitzBrowser_"&gt; /u/BlitzBrowser_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/atz2h7hk064f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l04tac/crawl4ai_ollama_remote_headless_browsers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l04tac/crawl4ai_ollama_remote_headless_browsers/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-31T19:02:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0egt1</id>
    <title>Improving your prompts helps small models perform their best</title>
    <updated>2025-06-01T02:47:57+00:00</updated>
    <author>
      <name>/u/newz2000</name>
      <uri>https://old.reddit.com/user/newz2000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working on some of my automations for my business. The production version uses 8b or 14b models but for testing I use deepseek-r1:1.5b. It's faster and seems to give me realistic output, including triggering the same types of problems.&lt;/p&gt; &lt;p&gt;Generally, the results of r1:1.5b are not nearly good enough. But I was reading my prompt and realized I was not being as explicit as I could be. I left out some instructions that a human would intuitively know. The larger models pick up on it, so I've never thought much about it.&lt;/p&gt; &lt;p&gt;I did some testing and worked on refining my prompts to be more precise and clear and in a few iterations I have almost as good results from the 1.5b model as I do on the 8b model. I'm running a more lengthy test now to confirm.&lt;/p&gt; &lt;p&gt;It's hard to describe my use case without putting you to sleep, but essentially, it takes a human question and creates a series of steps (like a checklist) that would be done in order to complete a process that would answer that question.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newz2000"&gt; /u/newz2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l0egt1/improving_your_prompts_helps_small_models_perform/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l0egt1/improving_your_prompts_helps_small_models_perform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l0egt1/improving_your_prompts_helps_small_models_perform/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-01T02:47:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1f6b7</id>
    <title>Perplexity Pro 1 Year Subscriptio.n $7 only</title>
    <updated>2025-06-02T11:05:32+00:00</updated>
    <author>
      <name>/u/shanks2020</name>
      <uri>https://old.reddit.com/user/shanks2020</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have some codes for sell for just 7$. each one gives you 1 full year of subscription in Perplexity Pro.&lt;/p&gt; &lt;h1&gt;DM me to get yours.&lt;/h1&gt; &lt;p&gt;With Pro you get :&lt;/p&gt; &lt;p&gt;Unlimited Pro Searches&lt;/p&gt; &lt;p&gt;Top AI Models (Gemini 2.5 Pro, GPT-4.1, Claude 4 Sonnet, Grok 3, R1 etc )&lt;/p&gt; &lt;p&gt;Unlimited File Uploads (PDFs, images, etc.)&lt;/p&gt; &lt;p&gt;AI Image Generation&lt;/p&gt; &lt;p&gt;Ad-Free Experience&lt;/p&gt; &lt;p&gt;Priority Support.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shanks2020"&gt; /u/shanks2020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1f6b7/perplexity_pro_1_year_subscription_7_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1f6b7/perplexity_pro_1_year_subscription_7_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l1f6b7/perplexity_pro_1_year_subscription_7_only/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-02T11:05:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0qrdq</id>
    <title>App-Use : Create virtual desktops for AI agents to focus on specific apps.</title>
    <updated>2025-06-01T14:49:35+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l0qrdq/appuse_create_virtual_desktops_for_ai_agents_to/"&gt; &lt;img alt="App-Use : Create virtual desktops for AI agents to focus on specific apps." src="https://external-preview.redd.it/cmUzYXI4cG53YjRmMYsTHh_R0WswrUJBBa-0t3y7YsS9UlwJcbvZWkm9vo2Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ab2cd258beb0758e7cc4deb1174decda701b776a" title="App-Use : Create virtual desktops for AI agents to focus on specific apps." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;App-Use lets you scope agents to just the apps they need. Instead of full desktop access, say &amp;quot;only work with Safari and Notes&amp;quot; or &amp;quot;just control iPhone Mirroring&amp;quot; - visual isolation without new processes for perfectly focused automation.&lt;/p&gt; &lt;p&gt;Running computer-use on the entire desktop often causes agent hallucinations and loss of focus when they see irrelevant windows and UI elements. App-Use solves this by creating composited views where agents only see what matters, dramatically improving task completion accuracy&lt;/p&gt; &lt;p&gt;Currently macOS-only (Quartz compositing engine). &lt;/p&gt; &lt;p&gt;Read the full guide: &lt;a href="https://trycua.com/blog/app-use"&gt;https://trycua.com/blog/app-use&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cz0moxznwb4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l0qrdq/appuse_create_virtual_desktops_for_ai_agents_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l0qrdq/appuse_create_virtual_desktops_for_ai_agents_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-01T14:49:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0phdu</id>
    <title>Gemma3 runs poorly on Ollama 0.7.0 or newer</title>
    <updated>2025-06-01T13:52:31+00:00</updated>
    <author>
      <name>/u/mlaihk</name>
      <uri>https://old.reddit.com/user/mlaihk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am noticing that gemma3 models becomes more sluggish and hallucinate more since ollama 0.7.0. anyone noticing the same?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlaihk"&gt; /u/mlaihk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l0phdu/gemma3_runs_poorly_on_ollama_070_or_newer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l0phdu/gemma3_runs_poorly_on_ollama_070_or_newer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l0phdu/gemma3_runs_poorly_on_ollama_070_or_newer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-01T13:52:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1g64p</id>
    <title>Why is my GPU not working at its max performance?</title>
    <updated>2025-06-02T12:00:07+00:00</updated>
    <author>
      <name>/u/Intelligent_Pop_4973</name>
      <uri>https://old.reddit.com/user/Intelligent_Pop_4973</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l1g64p/why_is_my_gpu_not_working_at_its_max_performance/"&gt; &lt;img alt="Why is my GPU not working at its max performance?" src="https://b.thumbs.redditmedia.com/BiXQB8zArYYr8Y17QhjweZWQK6XybG29mJS8z0_STeU.jpg" title="Why is my GPU not working at its max performance?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im using qwen2.5-coder32B with open-webui, and when i try to create some code my GPU just idles at around 25%, but when i use some other models like qwen3:8B GPU is maxxed out.&lt;br /&gt; PC specs:&lt;br /&gt; i7 12700&lt;br /&gt; 32 GB RAM&lt;br /&gt; RTX 3060 12G&lt;br /&gt; 1TB NVME&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ult68occ6i4f1.png?width=212&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=be2f9f22664951110548a3a827578682d4c371dd"&gt;qwen2.5-coder:32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mfdh0oqd6i4f1.png?width=230&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=60d3552215ba49f55101c898bc5a4481ea8173b2"&gt;qwen3:8B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Intelligent_Pop_4973"&gt; /u/Intelligent_Pop_4973 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1g64p/why_is_my_gpu_not_working_at_its_max_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1g64p/why_is_my_gpu_not_working_at_its_max_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l1g64p/why_is_my_gpu_not_working_at_its_max_performance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-02T12:00:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1l192vf</id>
    <title>Ryzen 6800H miniPC</title>
    <updated>2025-06-02T04:33:03+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1l192vf/ryzen_6800h_minipc/"&gt; &lt;img alt="Ryzen 6800H miniPC" src="https://b.thumbs.redditmedia.com/HSKQjsSWCdqdBB44V0HLdEQL5Xd5F_mTztb3TOQuEPg.jpg" title="Ryzen 6800H miniPC" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently purchase the Acemagic S3A miniPC with the Ryzen 6800H CPU using iGPU Radeon 680M. Paired it with 64GB of Crucial DDR5 4800Mhz memory and a 2TB NVMe Gen4 drive.&lt;/p&gt; &lt;p&gt;System switch be in Performance Mode. In the BIOS you have to use CTLR+F1 to view advanced settings.&lt;/p&gt; &lt;p&gt;Advanced tab - AMD CBS &amp;gt; NBIO Common Option &amp;gt; GFX Config &amp;gt; UMA Frame buffer Size (up to 16GB)&lt;/p&gt; &lt;p&gt;DDR5-4800 dual-channel memory provides a theoretical bandwidth of 38.4 GB/s per channel, resulting in a total bandwidth of 78.6 GB/s for the dual-channel configuration.&lt;/p&gt; &lt;p&gt;Verify the numbers for Eval Rate:&lt;/p&gt; &lt;p&gt;(DDR5 Bandwidth divided by Model size) times 75% efficiency&lt;/p&gt; &lt;p&gt;(78.6 Gb/s/17 GB) * .75 = approx 3.4 tokens per second&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1l192vf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l192vf/ryzen_6800h_minipc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l192vf/ryzen_6800h_minipc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-02T04:33:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1dq4o</id>
    <title>Uncensored Image Recognition Ai</title>
    <updated>2025-06-02T09:36:54+00:00</updated>
    <author>
      <name>/u/Zailor_s</name>
      <uri>https://old.reddit.com/user/Zailor_s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there, &lt;/p&gt; &lt;p&gt;I want to be able to give a pdf etc. file to the Ai and have it analyze the content and be able to describe it correctly. &lt;/p&gt; &lt;p&gt;I tried a lot of models, but they either describe something that doesnt exist or they cant describe images with censored content. &lt;/p&gt; &lt;p&gt;I want to run it the easiest way possible i.e. right now its via cmd‚Ä¶ and there is only 16gb of ram available. &lt;/p&gt; &lt;p&gt;There has to be something for this, but I could not find it yet. Pls help&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zailor_s"&gt; /u/Zailor_s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1dq4o/uncensored_image_recognition_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1dq4o/uncensored_image_recognition_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l1dq4o/uncensored_image_recognition_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-02T09:36:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1i0b7</id>
    <title>What is the best LLM to run locally?</title>
    <updated>2025-06-02T13:26:55+00:00</updated>
    <author>
      <name>/u/Intelligent_Pop_4973</name>
      <uri>https://old.reddit.com/user/Intelligent_Pop_4973</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;PC specs:&lt;br /&gt; i7 12700&lt;br /&gt; 32 GB RAM&lt;br /&gt; RTX 3060 12G&lt;br /&gt; 1TB NVME&lt;/p&gt; &lt;p&gt;i need a universal llm like chatgpt but run locally&lt;/p&gt; &lt;p&gt;P.S im an absolute noob in LLMs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Intelligent_Pop_4973"&gt; /u/Intelligent_Pop_4973 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1i0b7/what_is_the_best_llm_to_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1l1i0b7/what_is_the_best_llm_to_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1l1i0b7/what_is_the_best_llm_to_run_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-02T13:26:55+00:00</published>
  </entry>
</feed>
