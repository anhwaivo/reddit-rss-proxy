<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-07-06T09:06:19+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1lqnwac</id>
    <title>Help!! Ollama on AMD</title>
    <updated>2025-07-03T12:07:40+00:00</updated>
    <author>
      <name>/u/nqdat1995</name>
      <uri>https://old.reddit.com/user/nqdat1995</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Could someone help me run Ollama on my AMD Radeon 6800 GPU. I run Ollama but it always runs on CPU instead :((&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nqdat1995"&gt; /u/nqdat1995 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqnwac/help_ollama_on_amd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqnwac/help_ollama_on_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqnwac/help_ollama_on_amd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T12:07:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqphsw</id>
    <title>What's the difference between ollama.embeddings() and ollama.embed() ? Why do the methods return different embeddings for the same model (code in description)?</title>
    <updated>2025-07-03T13:23:21+00:00</updated>
    <author>
      <name>/u/LordTerminator</name>
      <uri>https://old.reddit.com/user/LordTerminator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am calling both methods to compare the embeddings they return.&lt;/p&gt; &lt;p&gt;&lt;code&gt;ll = ollama.embeddings(model='llama3.2',&lt;/code&gt;&lt;br /&gt; &lt;code&gt;prompt = 'The sky is blue because of rayleigh scattering'&lt;/code&gt;&lt;br /&gt; &lt;code&gt;)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;llm = dict(ll)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;llm['embedding']&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ll = ollama.embed(model='llama3.2',&lt;/code&gt;&lt;br /&gt; &lt;code&gt;input = 'The sky is blue because of rayleigh scattering'&lt;/code&gt;&lt;br /&gt; &lt;code&gt;)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;llm = dict(ll)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;llm['embeddings'][0]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;They return different embeddings for the same model. Why is that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LordTerminator"&gt; /u/LordTerminator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqphsw/whats_the_difference_between_ollamaembeddings_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqphsw/whats_the_difference_between_ollamaembeddings_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqphsw/whats_the_difference_between_ollamaembeddings_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T13:23:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq8yx7</id>
    <title>Best lightweight model for running on CPU with low RAM?</title>
    <updated>2025-07-02T22:13:36+00:00</updated>
    <author>
      <name>/u/-ThatGingerKid-</name>
      <uri>https://old.reddit.com/user/-ThatGingerKid-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've got an unRAID server and I've set up Open WebUI and Ollama on it. Problem is, I've only got 16gb of RAM and no GPU... I plan to upgrade eventually, but can't afford that right now. As a beginner, the sheer mass of options in Ollama is a bit overwhelming. What options would you recommend for lightweight hardware?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-ThatGingerKid-"&gt; /u/-ThatGingerKid- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lq8yx7/best_lightweight_model_for_running_on_cpu_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lq8yx7/best_lightweight_model_for_running_on_cpu_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lq8yx7/best_lightweight_model_for_running_on_cpu_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T22:13:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqfy1w</id>
    <title>A little project to analyze stock trends and explain major movements</title>
    <updated>2025-07-03T03:55:32+00:00</updated>
    <author>
      <name>/u/m19990328</name>
      <uri>https://old.reddit.com/user/m19990328</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lqfy1w/a_little_project_to_analyze_stock_trends_and/"&gt; &lt;img alt="A little project to analyze stock trends and explain major movements" src="https://b.thumbs.redditmedia.com/-tJiAQz3zixSDJrySlOjJVjaLSvDEuXQ8cBOZ83hpYo.jpg" title="A little project to analyze stock trends and explain major movements" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a tool that tries to explain the market movements to better understand the risk of investing in any stocks. I'd love to hear your opinion.&lt;/p&gt; &lt;p&gt;👉&lt;a href="https://github.com/CyrusCKF/stock-gone-wrong"&gt;https://github.com/CyrusCKF/stock-gone-wrong&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m19990328"&gt; /u/m19990328 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lqfy1w"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqfy1w/a_little_project_to_analyze_stock_trends_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqfy1w/a_little_project_to_analyze_stock_trends_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T03:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqor90</id>
    <title>Build a Multi-Agent AI Investment Advisor using Ollama, LangGraph, and Streamlit</title>
    <updated>2025-07-03T12:49:41+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lqor90/build_a_multiagent_ai_investment_advisor_using/"&gt; &lt;img alt="Build a Multi-Agent AI Investment Advisor using Ollama, LangGraph, and Streamlit" src="https://external-preview.redd.it/jrYQtvyJEJq6ekkw4MqwVzItNTy5yAMF5kFGArliMc8.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10054d9e3a79cf2b91c7dfa3d5941441e8535236" title="Build a Multi-Agent AI Investment Advisor using Ollama, LangGraph, and Streamlit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/FXPYOq63eWY?si=W7L7eCU1Ad3mOUd3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqor90/build_a_multiagent_ai_investment_advisor_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqor90/build_a_multiagent_ai_investment_advisor_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T12:49:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrai9k</id>
    <title>Ollama and side Hussle</title>
    <updated>2025-07-04T05:10:58+00:00</updated>
    <author>
      <name>/u/penguinlinux</name>
      <uri>https://old.reddit.com/user/penguinlinux</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to drop in and say how much I genuinely love Ollama. I’m constantly amazed at the quality and range of models available, and the fact that I don’t even need a GPU to use it blows my mind. I’m running everything on a small PC with a Ryzen CPU and 32GB of RAM, and it’s been smooth sailing.&lt;/p&gt; &lt;p&gt;Over the last few months, I’ve been using Ollama not just for fun, but as the foundation of a real side hustle. I’ve been writing and publishing books on KDP, and before anyone rolls their eyes no, it’s not AI slop.&lt;/p&gt; &lt;p&gt;What makes the difference for me is how I approach it. I’ve crafted a set of advanced prompts that I feed to models like gemma3n, phi4, and llama3.2. I’ve also built some clever Python scripts to orchestrate the whole thing, and I don’t just stop at generating content. I run everything through layers of agents that review, expand, and refine the material. I’m often surprised by the quality myself it feels like these books come to life in a way I never imagined possible.&lt;/p&gt; &lt;p&gt;This hasn’t been an overnight success. It took weeks of trial and error, adjusting prompts, restructuring my workflows, and staying persistent when nothing seemed to work. But now I’ve got over 70 books published, and after a slow start back in March, I'm consistently selling at least 5 books a day. No ads, no gimmicks. Just quietly working in the background, creating value.&lt;/p&gt; &lt;p&gt;I know there’s a lot of skepticism around AI generated books, and honestly I get it. But I’m really intentional with my process. I don’t treat this as a quick cash grab I treat it like real publishing. I want every book I release to actually help and provide value for the buyer like before I post a book i read it and think would i but this if it sucks i scrape it and refine it until I get something that i feel someone would get value from my book.&lt;/p&gt; &lt;p&gt;Huge thanks to the Ollama team and the whole open model ecosystem. This tool gave me the chance to do something creative, meaningful, and profitable all without needing a high-end machine. I’m excited to keep pushing the boundaries of what’s possible here. There are many other ideas I have and I am reinvesting money into buying more PC's to create more advanced workflows.&lt;/p&gt; &lt;p&gt;Curious if there are other people doing the same ! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/penguinlinux"&gt; /u/penguinlinux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrai9k/ollama_and_side_hussle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrai9k/ollama_and_side_hussle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lrai9k/ollama_and_side_hussle/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T05:10:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq6a33</id>
    <title>It’s finally here. Thanks to the Ollama community, I'm launching Observer AI v1.0 this Friday 🚀 – the open-source agent builder you helped shape.</title>
    <updated>2025-07-02T20:22:22+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Ollama community!,&lt;/p&gt; &lt;p&gt;Some of you might remember my earlier posts about a project I was building—an open-source way to create local AI agents. I've been tinkering, coding, and taking in all your amazing feedback for months. Today, I'm incredibly excited (and a little nervous!) to announce that &lt;strong&gt;Observer AI v1.0 is officially launching this Friday!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For anyone who missed it, &lt;strong&gt;Observer AI 👁️&lt;/strong&gt; is a privacy-first platform for building your own micro-agents that run locally on your machine.&lt;/p&gt; &lt;p&gt;The whole idea started because, like many of you, I was blown away by the power of local models but wanted a simple, powerful way to connect them to my own computer—to let them see my screen, react to events, and automate tasks without sending my screen data to cloud providers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This Project is a Love Letter to Ollama and This Community&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Observer AI would not exist without Ollama.&lt;/strong&gt; The sheer accessibility and power of what the Ollama team has built was what gave me the vision of this project.&lt;/p&gt; &lt;p&gt;And more importantly, it wouldn't be what it is today without &lt;strong&gt;YOU&lt;/strong&gt;. Every comment, suggestion, and bit of encouragement I've received from this community has directly shaped the features and direction of Observer. You told me what you wanted to see in a local agent platform, and I did my best to build it. So, from the bottom of my heart, &lt;strong&gt;thank you.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Launch This Friday&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The core Observer AI platform is, and will always be, &lt;strong&gt;free and open-source&lt;/strong&gt;. That's non-negotiable.&lt;/p&gt; &lt;p&gt;To help support the project's future development (I'm a solo dev, so server costs and coffee are my main fuel!), I'm also introducing an optional &lt;strong&gt;Observer Pro&lt;/strong&gt; subscription. This will give users unlimited access to the hosted Ob-Server models for those who might not be running a local instance 24/7. It's my way of trying to make the project sustainable long-term.&lt;/p&gt; &lt;p&gt;I'd be incredibly grateful if you'd take a look. Star the repo if you think it's cool, try building an agent, and let me know what you think. I'm building this for you, and your feedback is what will guide v1.1 and beyond.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;App Link:&lt;/strong&gt; &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GitHub (all the code is here!):&lt;/strong&gt; &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Twitter/X:&lt;/strong&gt; &lt;a href="https://x.com/AppObserverAI"&gt;https://x.com/AppObserverAI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord:&lt;/strong&gt; &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll be hanging out here all day to answer any questions. Let's build some cool stuff together!&lt;/p&gt; &lt;p&gt;Cheers,&lt;br /&gt; Roy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lq6a33/its_finally_here_thanks_to_the_ollama_community/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lq6a33/its_finally_here_thanks_to_the_ollama_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lq6a33/its_finally_here_thanks_to_the_ollama_community/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T20:22:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqlwli</id>
    <title>Best light llm for ocr summarize chat</title>
    <updated>2025-07-03T10:13:04+00:00</updated>
    <author>
      <name>/u/SuperMindHero</name>
      <uri>https://old.reddit.com/user/SuperMindHero</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I would like to run a local model 32 ram i7 12g. The goal is OCR for small pdf files max 2pages, summarize of text, chat with limited context and rag logic for specialized knowedge&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuperMindHero"&gt; /u/SuperMindHero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqlwli/best_light_llm_for_ocr_summarize_chat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqlwli/best_light_llm_for_ocr_summarize_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqlwli/best_light_llm_for_ocr_summarize_chat/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T10:13:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lraoso</id>
    <title>Question: Choosing Mac Studio for a "small" MVP project</title>
    <updated>2025-07-04T05:21:28+00:00</updated>
    <author>
      <name>/u/linnk87</name>
      <uri>https://old.reddit.com/user/linnk87</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm developing a small project involving image analysis using gemma3:27b. It looks like it could work, but for my MVP version I kinda need to run this model 24/7 for around 2 weeks.&lt;/p&gt; &lt;p&gt;If the MVP works, I'll need to run it way more (2 months to 1 year) for more experimentation and potentially first customers. &lt;/p&gt; &lt;p&gt;Remember: 24/7 doing inferences. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Do you think a Mac Studio M3 Ultra can sustain it?&lt;/li&gt; &lt;li&gt;Or do you think it will burn? lmao&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I have a gaming PC with a 4090 where I've been testing my development. It gets pretty hot after a few hours of inference and windows crashed at least once. The MacStudio is way more power efficient (which is also why I think it could be a good option), but for sustained work I'm not sure how stable would it be.&lt;/p&gt; &lt;p&gt;For an MVP the Mac Studio seems perfect: easy to manage, relatively cheap, power efficient, and powerful enough for production. Still, it's $10K I don't want to burn.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/linnk87"&gt; /u/linnk87 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lraoso/question_choosing_mac_studio_for_a_small_mvp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lraoso/question_choosing_mac_studio_for_a_small_mvp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lraoso/question_choosing_mac_studio_for_a_small_mvp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T05:21:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lr265k</id>
    <title>Serene Pub v0.3.0 Alpha Released — Offline AI Roleplay Client w/ Lorebooks+</title>
    <updated>2025-07-03T22:01:03+00:00</updated>
    <author>
      <name>/u/doolijb</name>
      <uri>https://old.reddit.com/user/doolijb</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doolijb"&gt; /u/doolijb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lr18jg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lr265k/serene_pub_v030_alpha_released_offline_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lr265k/serene_pub_v030_alpha_released_offline_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T22:01:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqzzxp</id>
    <title>use ollama with browser</title>
    <updated>2025-07-03T20:28:37+00:00</updated>
    <author>
      <name>/u/RealFullMetal</name>
      <uri>https://old.reddit.com/user/RealFullMetal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lqzzxp/use_ollama_with_browser/"&gt; &lt;img alt="use ollama with browser" src="https://external-preview.redd.it/ICK3WGgddRobWxwgfzh4IdlTfyzsA9tf-6ErBoiqdM0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1cab1b8518887baecef9354d7a843dcc3e755426" title="use ollama with browser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to be able ask questions on website using local models, so added ollama support in browserOS - &lt;a href="https://github.com/browseros-ai/BrowserOS"&gt;https://github.com/browseros-ai/BrowserOS&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Quick demo :) wdyt?&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1lqzzxp/video/6d6fop82ypaf1/player"&gt;https://reddit.com/link/1lqzzxp/video/6d6fop82ypaf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RealFullMetal"&gt; /u/RealFullMetal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqzzxp/use_ollama_with_browser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqzzxp/use_ollama_with_browser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqzzxp/use_ollama_with_browser/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T20:28:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrfgy8</id>
    <title>Ollama hangs without timeout</title>
    <updated>2025-07-04T10:30:43+00:00</updated>
    <author>
      <name>/u/NaiveWonder4836</name>
      <uri>https://old.reddit.com/user/NaiveWonder4836</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lrfgy8/ollama_hangs_without_timeout/"&gt; &lt;img alt="Ollama hangs without timeout" src="https://b.thumbs.redditmedia.com/NXALozAqIZ1aW9VvUEyPKaSL8NzpqPwsWKy5QrMzNRM.jpg" title="Ollama hangs without timeout" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/2d2si40j4uaf1.png?width=860&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=81876cc07e3da5cc2ab0008b0946db17c1b36aaf"&gt;&amp;lt;SOLVED&amp;gt; The port 127.0.0.1:11434 was running a process. After killing it and running this command again, it was solved&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NaiveWonder4836"&gt; /u/NaiveWonder4836 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrfgy8/ollama_hangs_without_timeout/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrfgy8/ollama_hangs_without_timeout/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lrfgy8/ollama_hangs_without_timeout/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T10:30:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqqocr</id>
    <title>Ollama Local AI Journaling App.</title>
    <updated>2025-07-03T14:14:51+00:00</updated>
    <author>
      <name>/u/Frosty-Cap-4282</name>
      <uri>https://old.reddit.com/user/Frosty-Cap-4282</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This was born out of a personal need — I journal daily , and I didn’t want to upload my thoughts to some cloud server and also wanted to use AI. So I built Vinaya to be:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Private&lt;/strong&gt;: Everything stays on your device. No servers, no cloud, no trackers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simple&lt;/strong&gt;: Clean UI built with Electron + React. No bloat, just journaling.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Insightful&lt;/strong&gt;: Semantic search, mood tracking, and AI-assisted reflections (all offline).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Link to the app: &lt;a href="https://vinaya-journal.vercel.app/"&gt;https://vinaya-journal.vercel.app/&lt;/a&gt;&lt;br /&gt; Github: &lt;a href="https://github.com/BarsatKhadka/Vinaya-Journal"&gt;https://github.com/BarsatKhadka/Vinaya-Journal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m not trying to build a SaaS or chase growth metrics. I just wanted something I could trust and use daily. If this resonates with anyone else, I’d love feedback or thoughts.&lt;/p&gt; &lt;p&gt;If you like the idea or find it useful and want to encourage me to consistently refine it but don’t know me personally and feel shy to say it — just drop a ⭐ on GitHub. That’ll mean a lot :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Frosty-Cap-4282"&gt; /u/Frosty-Cap-4282 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqqocr/ollama_local_ai_journaling_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqqocr/ollama_local_ai_journaling_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqqocr/ollama_local_ai_journaling_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T14:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrd7lq</id>
    <title>Two local LLM 4 newbie</title>
    <updated>2025-07-04T08:01:57+00:00</updated>
    <author>
      <name>/u/Powerful-Shine8690</name>
      <uri>https://old.reddit.com/user/Powerful-Shine8690</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wish to initialize my notebook to support two local LLMs (&lt;strong&gt;running NOT at the same time&lt;/strong&gt;).&lt;/p&gt; &lt;p&gt;First'll do:&lt;/p&gt; &lt;p&gt;- Work &lt;strong&gt;only in local&lt;/strong&gt;, w/out Internet access, throught my .md files (write for &lt;a href="http://Obsidian.MD"&gt;Obsidian.MD&lt;/a&gt; platform), about 1K files, in Italian language, then suggest me internal link and indexing datas;&lt;/p&gt; &lt;p&gt;- Trasform scanned text (Jpg, Pic, Jpeg, Png, Pdf and ePub) into text MD files. Scanned texts are writen in Italian, Latin and Ancient Greek;&lt;/p&gt; &lt;p&gt;Second'll do:&lt;/p&gt; &lt;p&gt;- Work locally (but also &lt;strong&gt;online&lt;/strong&gt; if necessary) to help me in JavaScript, CSS, Powershell and Python programming with Microsoft Visual Studio Code.&lt;/p&gt; &lt;p&gt;Here is my configuration:&lt;/p&gt; &lt;p&gt;PC: - Acer Predator PH317-56&lt;/p&gt; &lt;p&gt;CPU: - 12th Gen Intel i7-12700H&lt;/p&gt; &lt;p&gt;RAM: - 2x16Gb Samsung DDR5 x4800 (@2400MHz) + 2 slot free&lt;/p&gt; &lt;p&gt;Graph: - NVIDIA GeForce RTX 3070 Ti Laptop GPU 8Gb GDDR6&lt;/p&gt; &lt;p&gt;2x SSD: - Crucial P3 4TB M.2 2280 PCIe 4.0 NVMe (Os + Progr)&lt;/p&gt; &lt;pre&gt;&lt;code&gt; \- WD Black WDS800T2XHE 8 TB M.2 2280 PCIe 4.0 NVMe (Doc) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Os: - Win 11 Pro updated&lt;/p&gt; &lt;p&gt;What you expert can suggest me? Tnx in advance&lt;/p&gt; &lt;p&gt;Emanuele&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Shine8690"&gt; /u/Powerful-Shine8690 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrd7lq/two_local_llm_4_newbie/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrd7lq/two_local_llm_4_newbie/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lrd7lq/two_local_llm_4_newbie/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T08:01:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lri3xg</id>
    <title>nous-hermes2-mixtral asking for ssh access</title>
    <updated>2025-07-04T12:57:26+00:00</updated>
    <author>
      <name>/u/YetToBeTold</name>
      <uri>https://old.reddit.com/user/YetToBeTold</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I am new to this local AI self hosting, and i installed nous-hermes2-mixtral because chatgpt said its good with engineering, anyways i wanted to try a few models till i find the one that suits me, but what happened was I asked the model if it can access a pdf file in a certain directory, and it replied that it needs authority to do so, and asked me to generate an ssh key with ssh-keygen and shared its public key with me so i add it in authorized_keys under ~/.ssh.&lt;/p&gt; &lt;p&gt;Is this normal or dangerous? &lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YetToBeTold"&gt; /u/YetToBeTold &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lri3xg/noushermes2mixtral_asking_for_ssh_access/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lri3xg/noushermes2mixtral_asking_for_ssh_access/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lri3xg/noushermes2mixtral_asking_for_ssh_access/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T12:57:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqpsjh</id>
    <title>Ollama based AI presentation generator and API - Gamma Alternative</title>
    <updated>2025-07-03T13:36:35+00:00</updated>
    <author>
      <name>/u/goodboydhrn</name>
      <uri>https://old.reddit.com/user/goodboydhrn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lqpsjh/ollama_based_ai_presentation_generator_and_api/"&gt; &lt;img alt="Ollama based AI presentation generator and API - Gamma Alternative" src="https://preview.redd.it/awcrxuqjwnaf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=aa54d3b167b836a81007137b327da2d5800fd272" title="Ollama based AI presentation generator and API - Gamma Alternative" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Me and my roommates are building Presenton, which is an AI presentation generator that can run entirely on your own device. It has Ollama built in so, all you need is add Pexels (free image provider) API Key and start generating high quality presentations which can be exported to PPTX and PDF. It even works on CPU(can generate professional presentation with as small as 3b models)!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Presentation Generation UI&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It has beautiful user-interface which can be used to create presentations.&lt;/li&gt; &lt;li&gt;7+ beautiful themes to choose from.&lt;/li&gt; &lt;li&gt;Can choose number of slides, languages and themes.&lt;/li&gt; &lt;li&gt;Can create presentation from PDF, PPTX, DOCX, etc files directly.&lt;/li&gt; &lt;li&gt;Export to PPTX, PDF.&lt;/li&gt; &lt;li&gt;Share presentation link.(if you host on public IP)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Presentation Generation over API&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can even host the instance to generation presentation over API. (1 endpoint for all above features)&lt;/li&gt; &lt;li&gt;All above features supported over API&lt;/li&gt; &lt;li&gt;You'll get two links; first the static presentation file (pptx/pdf) which you requested and editable link through which you can edit the presentation and export the file.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love for you to try it out! Very easy docker based setup and deployment.&lt;/p&gt; &lt;p&gt;Here's the github link: &lt;a href="https://github.com/presenton/presenton"&gt;https://github.com/presenton/presenton&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Also check out the docs here: &lt;a href="https://docs.presenton.ai/"&gt;https://docs.presenton.ai&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Feedbacks are very appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goodboydhrn"&gt; /u/goodboydhrn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/awcrxuqjwnaf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqpsjh/ollama_based_ai_presentation_generator_and_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqpsjh/ollama_based_ai_presentation_generator_and_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T13:36:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrquyz</id>
    <title>Built an offline AI chat app for macOS that works with local LLMs via Ollama</title>
    <updated>2025-07-04T19:11:35+00:00</updated>
    <author>
      <name>/u/Disastrous-Parsnip93</name>
      <uri>https://old.reddit.com/user/Disastrous-Parsnip93</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a lightweight macOS desktop chat application that runs entirely offline and communicates with local LLMs through Ollama. No internet required once set up!&lt;/p&gt; &lt;p&gt;Key features:&lt;/p&gt; &lt;p&gt;- 🧠 Local LLM integration via Ollama&lt;/p&gt; &lt;p&gt;- 💬 Clean, modern chat interface with real-time streaming&lt;/p&gt; &lt;p&gt;- 📝 Full markdown support with syntax highlighting&lt;/p&gt; &lt;p&gt;- 🕘 Persistent chat history&lt;/p&gt; &lt;p&gt;- 🔄 Easy model switching&lt;/p&gt; &lt;p&gt;- 🎨 Auto dark/light theme&lt;/p&gt; &lt;p&gt;- 📦 Under 20MB final app size&lt;/p&gt; &lt;p&gt;Built with Tauri, React, and Rust for optimal performance. The app automatically detects available Ollama models and provides a native macOS experience.&lt;/p&gt; &lt;p&gt;Perfect for anyone who wants to chat with AI models privately without sending data to external servers. Works great with llama3, codellama, and other Ollama models.&lt;/p&gt; &lt;p&gt;Available on GitHub with releases for macOS. Would love feedback from the community!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/abhijeetlokhande1996/local-chat-releases/releases/download/v0.1.0/Local.Chat_0.1.0_aarch64.dmg"&gt;https://github.com/abhijeetlokhande1996/local-chat-releases/releases/download/v0.1.0/Local.Chat_0.1.0_aarch64.dmg&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous-Parsnip93"&gt; /u/Disastrous-Parsnip93 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrquyz/built_an_offline_ai_chat_app_for_macos_that_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrquyz/built_an_offline_ai_chat_app_for_macos_that_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lrquyz/built_an_offline_ai_chat_app_for_macos_that_works/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T19:11:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrpjci</id>
    <title>Please... how can I set the reasoning effort😭😭</title>
    <updated>2025-07-04T18:14:56+00:00</updated>
    <author>
      <name>/u/Open-Flounder-7194</name>
      <uri>https://old.reddit.com/user/Open-Flounder-7194</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lrpjci/please_how_can_i_set_the_reasoning_effort/"&gt; &lt;img alt="Please... how can I set the reasoning effort😭😭" src="https://preview.redd.it/ckugsto8dwaf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9407005119d0bbe95bb2c6de4e1a2b491749d3a1" title="Please... how can I set the reasoning effort😭😭" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried setting it to &amp;quot;none&amp;quot; but it did not seem to work, does Deepseek R1 not support the reasoning effort API or is &amp;quot;none&amp;quot; not an accepted value and it defaulted to medium or something like high? If possible how could I include something like Thinkless to still get reasoning if I need it or at least a button at the prompt window to enable or disable rasoning?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Open-Flounder-7194"&gt; /u/Open-Flounder-7194 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ckugsto8dwaf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrpjci/please_how_can_i_set_the_reasoning_effort/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lrpjci/please_how_can_i_set_the_reasoning_effort/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T18:14:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrtj0m</id>
    <title>Use all your favorite MCP servers in your meetings</title>
    <updated>2025-07-04T21:12:19+00:00</updated>
    <author>
      <name>/u/Square-Test-515</name>
      <uri>https://old.reddit.com/user/Square-Test-515</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lrtj0m/use_all_your_favorite_mcp_servers_in_your_meetings/"&gt; &lt;img alt="Use all your favorite MCP servers in your meetings" src="https://external-preview.redd.it/ZHpxdml1OXJheGFmMf7so8CSE-8PmjQuJPM-OgOW72CEju6_3gCE3GVMC0Pl.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2489d6fca27e489a49dd4b43863047cc47c4ac67" title="Use all your favorite MCP servers in your meetings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;We've been working on an open-source project called joinly for the last two months. The idea is that you can connect your favourite MCP servers (e.g. Asana, Notion and Linear) to an AI agent and send that agent to any browser-based video conference. This essentially allows you to create your own custom meeting assistant that can perform tasks in real time during the meeting.&lt;/p&gt; &lt;p&gt;So, how does it work? Ultimately, joinly is also just a MCP server that you can host yourself, providing your agent with essential meeting tools (such as speak_text and send_chat_message) alongside automatic real-time transcription. By the way, we've designed it so that you can select your own LLM (e.g., Ollama), TTS and STT providers. &lt;/p&gt; &lt;p&gt;We made a quick video to show how it works connecting it to the Tavily and GitHub MCP servers and let joinly explain how joinly works. Because we think joinly best speaks for itself.&lt;/p&gt; &lt;p&gt;We'd love to hear your feedback or ideas on which other MCP servers you'd like to use in your meetings. Or just try it out yourself 👉 &lt;a href="https://github.com/joinly-ai/joinly"&gt;https://github.com/joinly-ai/joinly&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Square-Test-515"&gt; /u/Square-Test-515 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/p9inht9raxaf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lrtj0m/use_all_your_favorite_mcp_servers_in_your_meetings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lrtj0m/use_all_your_favorite_mcp_servers_in_your_meetings/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-04T21:12:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls1bc8</id>
    <title>New feature "Expose Ollama to the network"</title>
    <updated>2025-07-05T04:18:56+00:00</updated>
    <author>
      <name>/u/yAmIDoingThisAtHome</name>
      <uri>https://old.reddit.com/user/yAmIDoingThisAtHome</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How to utilize this? How is it different from http://&amp;lt;ollama\_host&amp;gt;:11434 ?&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.9.5"&gt;https://github.com/ollama/ollama/releases/tag/v0.9.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yAmIDoingThisAtHome"&gt; /u/yAmIDoingThisAtHome &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ls1bc8/new_feature_expose_ollama_to_the_network/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ls1bc8/new_feature_expose_ollama_to_the_network/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ls1bc8/new_feature_expose_ollama_to_the_network/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-05T04:18:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lso1ys</id>
    <title>Web search doesn’t return current results, using OpenWebUI with Ollama</title>
    <updated>2025-07-06T00:15:13+00:00</updated>
    <author>
      <name>/u/AliasJackBauer</name>
      <uri>https://old.reddit.com/user/AliasJackBauer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve just setup a Z440 workstation with a 3090 for LLM learning. I’ve got OpenWebUI with Ollama configured. I’ve been experimenting with gemma3 27b. I’m trying to get web search configured. I have it enabled in the configuration. I’ve tried both google pse and Searxng and it never returns current results when I do a query like “ what’s the weather for ‘some city’” even though it says it’s checking the web. Looking for what I can do to debug this a bit and figure out whey it’s not working.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AliasJackBauer"&gt; /u/AliasJackBauer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lso1ys/web_search_doesnt_return_current_results_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lso1ys/web_search_doesnt_return_current_results_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lso1ys/web_search_doesnt_return_current_results_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-06T00:15:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsp9py</id>
    <title>Is it possible to play real tabletop, board, and card games using local free ai's?</title>
    <updated>2025-07-06T01:19:32+00:00</updated>
    <author>
      <name>/u/Game-Lover44</name>
      <uri>https://old.reddit.com/user/Game-Lover44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have no real friends to play with. Is it possible to use ai to act as a teammate or opponent. I want to play games on a real table instead of digital would something like this be possible to do locally or is it too complex? how would i set something like this up? &lt;/p&gt; &lt;p&gt;&lt;em&gt;are there better things to do?&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Game-Lover44"&gt; /u/Game-Lover44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsp9py/is_it_possible_to_play_real_tabletop_board_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsp9py/is_it_possible_to_play_real_tabletop_board_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lsp9py/is_it_possible_to_play_real_tabletop_board_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-06T01:19:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsjf75</id>
    <title>Preferred frameworks when working with Ollama models?</title>
    <updated>2025-07-05T20:34:51+00:00</updated>
    <author>
      <name>/u/SeaworthinessLeft160</name>
      <uri>https://old.reddit.com/user/SeaworthinessLeft160</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'd like to know what you're using for your projects (personally or professionally) when working with models via Ollama (and if possible, how you handle prompt management or logging).&lt;/p&gt; &lt;p&gt;Personally, I’ve mostly just been using Ollama with Pydantic. I started exploring Instructor, but from what I can tell, I’m already doing pretty much the same thing just with Ollama and Pydantic, so I’m not sure I actually need Instructor. I’ve been thinking about trying out Langchain next, but honestly, I get a bit confused. I keep seeing OpenAI wrappers everywhere, and the standard setup I keep coming across is an OpenAI wrapper using the Ollama API underneath, usually combined with Langchain.&lt;/p&gt; &lt;p&gt;Thanks for any help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SeaworthinessLeft160"&gt; /u/SeaworthinessLeft160 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsjf75/preferred_frameworks_when_working_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsjf75/preferred_frameworks_when_working_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lsjf75/preferred_frameworks_when_working_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-05T20:34:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsbeyy</id>
    <title>How safe is to download models that are not official release</title>
    <updated>2025-07-05T14:40:56+00:00</updated>
    <author>
      <name>/u/ResponsibleTruck4717</name>
      <uri>https://old.reddit.com/user/ResponsibleTruck4717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know anyone can upload models how safe is to download it? are we expose to any risks like pickles file have?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleTruck4717"&gt; /u/ResponsibleTruck4717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsbeyy/how_safe_is_to_download_models_that_are_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsbeyy/how_safe_is_to_download_models_that_are_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lsbeyy/how_safe_is_to_download_models_that_are_not/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-05T14:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsql6h</id>
    <title>Ollama use A LOT of memory even after offloading model to GPU</title>
    <updated>2025-07-06T02:32:36+00:00</updated>
    <author>
      <name>/u/Only_Comfortable_224</name>
      <uri>https://old.reddit.com/user/Only_Comfortable_224</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My PC has Windows11 + 16GB RAM +16GB VRAM (AMD rx9070). When I run smaller models (e.g. qwen3 14B q4 quantization) on Ollama, even though I offload all the layers to GPU, it still uses almost all the memory (~15 out of 16GB) as shown in task manager. I can confirm the GPU is being used because the VRAM usage is almost all used. I don't have such issue when using LM studio, which only uses VRAM and leaves the system RAM free so I can comfortably run other applications. Any idea how to solve the problem for Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Only_Comfortable_224"&gt; /u/Only_Comfortable_224 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsql6h/ollama_use_a_lot_of_memory_even_after_offloading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsql6h/ollama_use_a_lot_of_memory_even_after_offloading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lsql6h/ollama_use_a_lot_of_memory_even_after_offloading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-06T02:32:36+00:00</published>
  </entry>
</feed>
