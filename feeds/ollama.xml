<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-07-08T15:24:53+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1lsjf75</id>
    <title>Preferred frameworks when working with Ollama models?</title>
    <updated>2025-07-05T20:34:51+00:00</updated>
    <author>
      <name>/u/SeaworthinessLeft160</name>
      <uri>https://old.reddit.com/user/SeaworthinessLeft160</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'd like to know what you're using for your projects (personally or professionally) when working with models via Ollama (and if possible, how you handle prompt management or logging).&lt;/p&gt; &lt;p&gt;Personally, I’ve mostly just been using Ollama with Pydantic. I started exploring Instructor, but from what I can tell, I’m already doing pretty much the same thing just with Ollama and Pydantic, so I’m not sure I actually need Instructor. I’ve been thinking about trying out Langchain next, but honestly, I get a bit confused. I keep seeing OpenAI wrappers everywhere, and the standard setup I keep coming across is an OpenAI wrapper using the Ollama API underneath, usually combined with Langchain.&lt;/p&gt; &lt;p&gt;Thanks for any help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SeaworthinessLeft160"&gt; /u/SeaworthinessLeft160 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsjf75/preferred_frameworks_when_working_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsjf75/preferred_frameworks_when_working_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lsjf75/preferred_frameworks_when_working_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-05T20:34:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsp9py</id>
    <title>Is it possible to play real tabletop, board, and card games using local free ai's?</title>
    <updated>2025-07-06T01:19:32+00:00</updated>
    <author>
      <name>/u/Game-Lover44</name>
      <uri>https://old.reddit.com/user/Game-Lover44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have no real friends to play with. Is it possible to use ai to act as a teammate or opponent. I want to play games on a real table instead of digital would something like this be possible to do locally or is it too complex? how would i set something like this up? &lt;/p&gt; &lt;p&gt;&lt;em&gt;are there better things to do?&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Game-Lover44"&gt; /u/Game-Lover44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsp9py/is_it_possible_to_play_real_tabletop_board_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsp9py/is_it_possible_to_play_real_tabletop_board_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lsp9py/is_it_possible_to_play_real_tabletop_board_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-06T01:19:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsbeyy</id>
    <title>How safe is to download models that are not official release</title>
    <updated>2025-07-05T14:40:56+00:00</updated>
    <author>
      <name>/u/ResponsibleTruck4717</name>
      <uri>https://old.reddit.com/user/ResponsibleTruck4717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know anyone can upload models how safe is to download it? are we expose to any risks like pickles file have?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleTruck4717"&gt; /u/ResponsibleTruck4717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsbeyy/how_safe_is_to_download_models_that_are_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsbeyy/how_safe_is_to_download_models_that_are_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lsbeyy/how_safe_is_to_download_models_that_are_not/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-05T14:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lso1ys</id>
    <title>Web search doesn’t return current results, using OpenWebUI with Ollama</title>
    <updated>2025-07-06T00:15:13+00:00</updated>
    <author>
      <name>/u/AliasJackBauer</name>
      <uri>https://old.reddit.com/user/AliasJackBauer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve just setup a Z440 workstation with a 3090 for LLM learning. I’ve got OpenWebUI with Ollama configured. I’ve been experimenting with gemma3 27b. I’m trying to get web search configured. I have it enabled in the configuration. I’ve tried both google pse and Searxng and it never returns current results when I do a query like “ what’s the weather for ‘some city’” even though it says it’s checking the web. Looking for what I can do to debug this a bit and figure out whey it’s not working.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AliasJackBauer"&gt; /u/AliasJackBauer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lso1ys/web_search_doesnt_return_current_results_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lso1ys/web_search_doesnt_return_current_results_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lso1ys/web_search_doesnt_return_current_results_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-06T00:15:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsql6h</id>
    <title>Ollama use A LOT of memory even after offloading model to GPU</title>
    <updated>2025-07-06T02:32:36+00:00</updated>
    <author>
      <name>/u/Only_Comfortable_224</name>
      <uri>https://old.reddit.com/user/Only_Comfortable_224</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My PC has Windows11 + 16GB RAM +16GB VRAM (AMD rx9070). When I run smaller models (e.g. qwen3 14B q4 quantization) on Ollama, even though I offload all the layers to GPU, it still uses almost all the memory (~15 out of 16GB) as shown in task manager. I can confirm the GPU is being used because the VRAM usage is almost all used. I don't have such issue when using LM studio, which only uses VRAM and leaves the system RAM free so I can comfortably run other applications. Any idea how to solve the problem for Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Only_Comfortable_224"&gt; /u/Only_Comfortable_224 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsql6h/ollama_use_a_lot_of_memory_even_after_offloading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsql6h/ollama_use_a_lot_of_memory_even_after_offloading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lsql6h/ollama_use_a_lot_of_memory_even_after_offloading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-06T02:32:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltfuqi</id>
    <title>Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler</title>
    <updated>2025-07-06T23:56:31+00:00</updated>
    <author>
      <name>/u/Fluid-Engineering769</name>
      <uri>https://old.reddit.com/user/Fluid-Engineering769</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ltfuqi/websitecrawler_extract_data_from_websites_in_llm/"&gt; &lt;img alt="Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler" src="https://external-preview.redd.it/8pG6rnRIoCry-dvcBOF-au9YmfpNVda4S3Exgl6tAS8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d809c1ecb009b988b05c33802e7cbb69bf85e8a8" title="Website-Crawler: Extract data from websites in LLM ready JSON or CSV format. Crawl or Scrape entire website with Website Crawler" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluid-Engineering769"&gt; /u/Fluid-Engineering769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/pc8544/Website-Crawler"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltfuqi/websitecrawler_extract_data_from_websites_in_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ltfuqi/websitecrawler_extract_data_from_websites_in_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-06T23:56:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsyodw</id>
    <title>JSON response formatting</title>
    <updated>2025-07-06T11:08:51+00:00</updated>
    <author>
      <name>/u/barrulus</name>
      <uri>https://old.reddit.com/user/barrulus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all How do you get Ollama models to respond with structured JSON reliably?&lt;/p&gt; &lt;p&gt;It seems to me that I write my app to read the json response and then the. est response comes with malformat or a change in array location or whatever. &lt;/p&gt; &lt;p&gt;edit: I already provide the schema with every prompt. That was the first thing I tried. Very limited success. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/barrulus"&gt; /u/barrulus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsyodw/json_response_formatting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lsyodw/json_response_formatting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lsyodw/json_response_formatting/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-06T11:08:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltq3cy</id>
    <title>HELP ME : Ollama is utilizing my CPU more than my GPU.</title>
    <updated>2025-07-07T09:52:36+00:00</updated>
    <author>
      <name>/u/HighlightPrudent554</name>
      <uri>https://old.reddit.com/user/HighlightPrudent554</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ltq3cy/help_me_ollama_is_utilizing_my_cpu_more_than_my/"&gt; &lt;img alt="HELP ME : Ollama is utilizing my CPU more than my GPU." src="https://b.thumbs.redditmedia.com/MZQc-XntQLCWzMAi8Idw1hsWHKajJBiXh8mqNjhmPeQ.jpg" title="HELP ME : Ollama is utilizing my CPU more than my GPU." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/g5096yniafbf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f6e5c74fdd262227623dede0b89b62a1e82eb7cc"&gt;https://preview.redd.it/g5096yniafbf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f6e5c74fdd262227623dede0b89b62a1e82eb7cc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My GPU is not being utilized as much as my CPU on the KDE Neon distribution I'm currently using. On my previous Ubuntu distribution, my GPU usage was around 90%, compared to my CPU. I'm not sure what went wrong. I added the following options to /etc/modprobe.d/nvidia-power-management.conf to address wake-up issues with the GPU not functioning after sleep:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Code options nvidia NVreg_PreserveVideoMemoryAllocations=1 options nvidia NVreg_TemporaryFilePath=/tmp &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Since then, Ollama has been using my GPU less than my CPU. I've been searching for answers for a week.&lt;/p&gt; &lt;p&gt;i am running llama3.1 8b model. i used same models on both distros.&lt;/p&gt; &lt;p&gt;help me guys.............&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HighlightPrudent554"&gt; /u/HighlightPrudent554 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltq3cy/help_me_ollama_is_utilizing_my_cpu_more_than_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltq3cy/help_me_ollama_is_utilizing_my_cpu_more_than_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ltq3cy/help_me_ollama_is_utilizing_my_cpu_more_than_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T09:52:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltoggx</id>
    <title>Open Web UI APIEndpoint with One Time Use FIle</title>
    <updated>2025-07-07T08:04:30+00:00</updated>
    <author>
      <name>/u/aeldexis</name>
      <uri>https://old.reddit.com/user/aeldexis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ltoggx/open_web_ui_apiendpoint_with_one_time_use_file/"&gt; &lt;img alt="Open Web UI APIEndpoint with One Time Use FIle" src="https://b.thumbs.redditmedia.com/P2OONlK2hcNqv00iarVpdg97XaKwYWTMQVGA4oqGtco.jpg" title="Open Web UI APIEndpoint with One Time Use FIle" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was reading the docs for open web ui's api endpoint to implement into my personal app and i dont quite understand it.&lt;/p&gt; &lt;p&gt;My goal is to upload a file (docx or pdf) and get a response in a json format.&lt;/p&gt; &lt;p&gt;But I have no idea how to handle the file.&lt;/p&gt; &lt;p&gt;Im able to get the completions api to work on postman but im not sure how to get the file upload to work.&lt;/p&gt; &lt;p&gt;Any examples I could follow?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qsbjbv6qlebf1.png?width=1252&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43e0296f92b0b4705b2d6b016d455c02483bea23"&gt;https://preview.redd.it/qsbjbv6qlebf1.png?width=1252&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43e0296f92b0b4705b2d6b016d455c02483bea23&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aeldexis"&gt; /u/aeldexis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltoggx/open_web_ui_apiendpoint_with_one_time_use_file/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltoggx/open_web_ui_apiendpoint_with_one_time_use_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ltoggx/open_web_ui_apiendpoint_with_one_time_use_file/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T08:04:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltp2ef</id>
    <title>Ollama force IGPu use</title>
    <updated>2025-07-07T08:45:47+00:00</updated>
    <author>
      <name>/u/MashiatILias</name>
      <uri>https://old.reddit.com/user/MashiatILias</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I'm new here in the Ollama and AI world. I can run AIs on my laptop well enough like the small ones from 2-less billion. But they all run on the CPU. I want it to run my on IGPU which is the Irisi XE-G4. But, how to do that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MashiatILias"&gt; /u/MashiatILias &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltp2ef/ollama_force_igpu_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltp2ef/ollama_force_igpu_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ltp2ef/ollama_force_igpu_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T08:45:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltxlsf</id>
    <title>LangChain/Crew/AutoGen made it easy to build agents, but operating them is a joke</title>
    <updated>2025-07-07T15:46:12+00:00</updated>
    <author>
      <name>/u/ImmuneCoder</name>
      <uri>https://old.reddit.com/user/ImmuneCoder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built an internal support agent using LangChain + OpenAI + some simple tool calls.&lt;/p&gt; &lt;p&gt;Getting to a working prototype took 3 days with Cursor and just messing around. Great.&lt;/p&gt; &lt;p&gt;But actually trying to operate that agent across multiple teams was absolute chaos.&lt;/p&gt; &lt;p&gt;– No structured logs of intermediate reasoning&lt;/p&gt; &lt;p&gt;– No persistent memory or traceability&lt;/p&gt; &lt;p&gt;– No access control (anyone could run/modify it)&lt;/p&gt; &lt;p&gt;– No ability to validate outputs at scale&lt;/p&gt; &lt;p&gt;It’s like deploying a microservice with no logs, no auth, and no monitoring. The frameworks are designed for demos, not real workflows. And everyone I know is duct-taping together JSON dumps + Slack logs to stay afloat.&lt;/p&gt; &lt;p&gt;So, what does agent infra actually look like after the first prototype for you guys?&lt;/p&gt; &lt;p&gt;Would love to hear real setups. Especially if you’ve gone past the LangChain happy path.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ImmuneCoder"&gt; /u/ImmuneCoder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltxlsf/langchaincrewautogen_made_it_easy_to_build_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltxlsf/langchaincrewautogen_made_it_easy_to_build_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ltxlsf/langchaincrewautogen_made_it_easy_to_build_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T15:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltlgag</id>
    <title>OrangePi Zero 3 runs Ollama</title>
    <updated>2025-07-07T04:53:36+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those that are curious about running LLM on &lt;a href="https://en.wikipedia.org/wiki/Single-board_computer"&gt;SBC&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Here is &lt;a href="http://www.orangepi.org/html/hardWare/computerAndMicrocontrollers/details/Orange-Pi-Zero-3.html"&gt;Orange Pi Zero 3&lt;/a&gt; (aarch64) packed with 4gb DDR4 running Debian 12 'Bookworm'/ &lt;a href="https://dietpi.com/#home"&gt;DietPi&lt;/a&gt; using &lt;code&gt;ollama -v&lt;/code&gt; 0.9.5&lt;/p&gt; &lt;p&gt;I even used &lt;a href="https://ollama.com/library/llama3.2:1b"&gt;llama3.2:1b&lt;/a&gt; to create this markdown table:&lt;/p&gt; &lt;p&gt;*Eval Rate Tokens per Second is average of 3 runs.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;MODEL&lt;/th&gt; &lt;th align="left"&gt;SIZE GB&lt;/th&gt; &lt;th align="left"&gt;EVAL RATE TS/S&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3:1b&lt;/td&gt; &lt;td align="left"&gt;1.4&lt;/td&gt; &lt;td align="left"&gt;3.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama3.2:1b&lt;/td&gt; &lt;td align="left"&gt;2.2&lt;/td&gt; &lt;td align="left"&gt;3.16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2.5:1.5b-instruct-q5_K_M&lt;/td&gt; &lt;td align="left"&gt;1.7&lt;/td&gt; &lt;td align="left"&gt;2.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tinydolphin:1.1b-v2.8-q6_K&lt;/td&gt; &lt;td align="left"&gt;1.6&lt;/td&gt; &lt;td align="left"&gt;2.61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tinyllama:1.1b-chat-v1-q6_K&lt;/td&gt; &lt;td align="left"&gt;1.3&lt;/td&gt; &lt;td align="left"&gt;2.52&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Here is the &lt;code&gt;ollama run --verbose llama3.2:1b&lt;/code&gt; numbers from creating markdown table&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Value&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Total Duration&lt;/td&gt; &lt;td align="left"&gt;2m54.721763625s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Load Duration&lt;/td&gt; &lt;td align="left"&gt;41.594289562s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Prompt Eval Count&lt;/td&gt; &lt;td align="left"&gt;389 token(s)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Prompt Eval Duration&lt;/td&gt; &lt;td align="left"&gt;1m17.397468287s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Prompt Eval Rate&lt;/td&gt; &lt;td align="left"&gt;5.03 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Eval Count&lt;/td&gt; &lt;td align="left"&gt;163 token(s)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Eval Duration&lt;/td&gt; &lt;td align="left"&gt;55.571782235s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Eval Rate&lt;/td&gt; &lt;td align="left"&gt;2.93 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I was able to run &lt;a href="https://ollama.com/library/llama3.2:3b-instruct-q5_K_M"&gt;llama3.2:3b-instruct-q5_K_M&lt;/a&gt; and &lt;code&gt;ollama ps&lt;/code&gt; reported 4.0GB usage. Eval Rate dropped to 1.21 Tokens/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltlgag/orangepi_zero_3_runs_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltlgag/orangepi_zero_3_runs_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ltlgag/orangepi_zero_3_runs_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T04:53:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltvl0w</id>
    <title>(Kramer UI for Ollama) I was tired of dealing with Docker, so I built a simple, portable Windows UI for Ollama.</title>
    <updated>2025-07-07T14:27:15+00:00</updated>
    <author>
      <name>/u/DanielKramer_</name>
      <uri>https://old.reddit.com/user/DanielKramer_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ltvl0w/kramer_ui_for_ollama_i_was_tired_of_dealing_with/"&gt; &lt;img alt="(Kramer UI for Ollama) I was tired of dealing with Docker, so I built a simple, portable Windows UI for Ollama." src="https://external-preview.redd.it/9sqw7guDNkr_lh4DzQzAQ3_oGbJPe0qHLVbjofkhPuc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d5b553bbacb3aa769ebe7746d6025ee8190093ba" title="(Kramer UI for Ollama) I was tired of dealing with Docker, so I built a simple, portable Windows UI for Ollama." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share a small project I built for my own purposes: Kramer UI for Ollama.&lt;/p&gt; &lt;p&gt;I love Ollama for its simplicity and its model management, but setting up a UI for it has always been a pain point. I used to use OpenWebUI and it was great, but I'd rather not have to set up docker. And using Ollama through the CLI makes me feel like a simpleton because I can't even edit my messages.&lt;/p&gt; &lt;p&gt;I wanted a UI as simple as Ollama to accompany it. So I built it. Kramer UI is a single, portable executable file for Windows. There's no installer. You just run the .exe and you're ready to start chatting.&lt;/p&gt; &lt;p&gt;My goal was to make interacting with your local models as frictionless as possible.&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Uses 45mb of ram&lt;/li&gt; &lt;li&gt;Edit your messages&lt;/li&gt; &lt;li&gt;Models' thoughts are hidden behind dropdown&lt;/li&gt; &lt;li&gt;Model selector&lt;/li&gt; &lt;li&gt;Currently no support for conversation history&lt;/li&gt; &lt;li&gt;You can probably compile it for Linux and Mac too&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can download the executable directly from the GitHub releases page [here.] (&lt;a href="https://github.com/dvkramer/kramer-ui/releases/"&gt;https://github.com/dvkramer/kramer-ui/releases/&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ltf7soogrgbf1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dfccfc5d106dd35e413bc7f65b35b758d6d05703"&gt;https://preview.redd.it/ltf7soogrgbf1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dfccfc5d106dd35e413bc7f65b35b758d6d05703&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All feedback, suggestions, and ideas are welcome! Let me know what you think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanielKramer_"&gt; /u/DanielKramer_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltvl0w/kramer_ui_for_ollama_i_was_tired_of_dealing_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltvl0w/kramer_ui_for_ollama_i_was_tired_of_dealing_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ltvl0w/kramer_ui_for_ollama_i_was_tired_of_dealing_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T14:27:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltzcks</id>
    <title>Looking for advice.</title>
    <updated>2025-07-07T16:51:54+00:00</updated>
    <author>
      <name>/u/Devve2kcccc</name>
      <uri>https://old.reddit.com/user/Devve2kcccc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm building a SaaS ERP for textile manufacturing and want to add an AI agent to &lt;strong&gt;analyze and compare transport/invoice documents&lt;/strong&gt;. In our process, clients send raw materials (e.g., T-shirts), we manufacture, and then send the finished goods back. Right now, someone manually compares multiple documents (transport guides, invoices, etc.) to verify if &lt;strong&gt;quantities, sizes, and products&lt;/strong&gt; match — and flag any inconsistencies.&lt;/p&gt; &lt;p&gt;I want to automate this with a service that can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ingest 1 or more related documents (PDFs, scans, etc.)&lt;/li&gt; &lt;li&gt;Parse and normalize the data (structured or unstructured)&lt;/li&gt; &lt;li&gt;Detect mismatches (quantities, prices, product references)&lt;/li&gt; &lt;li&gt;Generate a validation report or alert the company&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key challenge:&lt;/h1&gt; &lt;p&gt;The &lt;strong&gt;biggest problem&lt;/strong&gt; is that &lt;strong&gt;every company uses different software&lt;/strong&gt; and formats — so transport documents and invoices come in &lt;strong&gt;very different layouts and structures&lt;/strong&gt;. We need a dynamic and flexible system that can understand and extract key information &lt;strong&gt;regardless of the template&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;What I’m looking for:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Best practices&lt;/strong&gt; for parsing (OCR vs. structured PDF/XML, etc.)&lt;/li&gt; &lt;li&gt;Whether to use &lt;strong&gt;AI (LLMs?) or rule-based logic&lt;/strong&gt;, or both&lt;/li&gt; &lt;li&gt;Tools/libraries for &lt;strong&gt;document comparison &amp;amp; anomaly detection&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open-source / budget-friendly&lt;/strong&gt; options (we're a startup)&lt;/li&gt; &lt;li&gt;LLM models or services that work well for &lt;strong&gt;document understanding&lt;/strong&gt;, ideally something we can run locally or affordably scale&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you’ve built something similar — especially in &lt;strong&gt;logistics, finance, or manufacturing&lt;/strong&gt; — I’d love to hear what tools and strategies worked for you (and what to avoid).&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Devve2kcccc"&gt; /u/Devve2kcccc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltzcks/looking_for_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltzcks/looking_for_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ltzcks/looking_for_advice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T16:51:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltqs3f</id>
    <title>should i replace gemma 3?</title>
    <updated>2025-07-07T10:34:52+00:00</updated>
    <author>
      <name>/u/Otherwise-Brick4923</name>
      <uri>https://old.reddit.com/user/Otherwise-Brick4923</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hi everyone,&lt;/strong&gt;&lt;br /&gt; I'm trying to create a workflow that can check a client's order against the supplier's order confirmation for any discrepancies. Everything is working quite well so far, but when I started testing the system by intentionally introducing errors, Gemma simply ignored them.&lt;/p&gt; &lt;p&gt;For example:&lt;br /&gt; The client's name is &lt;strong&gt;Lius&lt;/strong&gt;, but I entered &lt;strong&gt;Dius&lt;/strong&gt;, and Gemma marked it as correct.&lt;/p&gt; &lt;p&gt;Now I'm considering switching to the new &lt;strong&gt;Gemma 3n&lt;/strong&gt;, hoping it might perform better.&lt;/p&gt; &lt;p&gt;Has anyone experienced something similar or have an idea why Gemma isn't recognizing these errors?&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Otherwise-Brick4923"&gt; /u/Otherwise-Brick4923 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltqs3f/should_i_replace_gemma_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ltqs3f/should_i_replace_gemma_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ltqs3f/should_i_replace_gemma_3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T10:34:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lud29c</id>
    <title>please critique my python ollama api that interfaces with a bash terminal</title>
    <updated>2025-07-08T02:20:15+00:00</updated>
    <author>
      <name>/u/printingbooks</name>
      <uri>https://old.reddit.com/user/printingbooks</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://pastebin.com/HnTg2M6X"&gt;https://pastebin.com/HnTg2M6X&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ask me questions if you want. it isnt totally complete. devstral outputs JSON coded stuff like indicating if somthing is a command to a chat message or even a keystroke(but this isnt fully implemented yet)&lt;/p&gt; &lt;p&gt;thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/printingbooks"&gt; /u/printingbooks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lud29c/please_critique_my_python_ollama_api_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lud29c/please_critique_my_python_ollama_api_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lud29c/please_critique_my_python_ollama_api_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T02:20:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1luhl9d</id>
    <title>Ollama still using cuda even after replacing gpu</title>
    <updated>2025-07-08T06:33:52+00:00</updated>
    <author>
      <name>/u/KoftaBozo2235</name>
      <uri>https://old.reddit.com/user/KoftaBozo2235</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used to have llama3.1 running in Ubuntu WSL on an rtx 4070, but now ive replaced it with a 9070xt and it wont work on the gpu no matter what i do. I've installed rocm, set environment variables, tried uninstalling nvidia libraries, but it still shows supported_gpu=0 whenever i run serve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KoftaBozo2235"&gt; /u/KoftaBozo2235 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luhl9d/ollama_still_using_cuda_even_after_replacing_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luhl9d/ollama_still_using_cuda_even_after_replacing_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1luhl9d/ollama_still_using_cuda_even_after_replacing_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T06:33:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1luhllb</id>
    <title>Ollama using GPU when run standalone but CPU when run through Llamaindex?</title>
    <updated>2025-07-08T06:34:28+00:00</updated>
    <author>
      <name>/u/Neogohan1</name>
      <uri>https://old.reddit.com/user/Neogohan1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I'm just trying to go through initial setup of llamaindex using ollama running the following code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from llama_index.llms.ollama import Ollama llm=Ollama(model=&amp;quot;deepseek-r1&amp;quot;,request_timeout=360.0) resp = llm.complete(&amp;quot;Who is Paul Graham?&amp;quot;) print(resp) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When I run this i can see my RAM and CPU going up but GPU stays 0%.&lt;/p&gt; &lt;p&gt;However if I open a cmd prompt and just use &amp;quot;ollama run deepseek-r1&amp;quot; and prompt the model there, i can see it runs on GPU at like 30%, and is much faster. Is there a way to ensure it runs on GPU when I use it as part of a python script/using llamaindex?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neogohan1"&gt; /u/Neogohan1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luhllb/ollama_using_gpu_when_run_standalone_but_cpu_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1luhllb/ollama_using_gpu_when_run_standalone_but_cpu_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1luhllb/ollama_using_gpu_when_run_standalone_but_cpu_when/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T06:34:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lttm5g</id>
    <title>Want to create a private LLM for ingesting engineering handbooks &amp; IP.</title>
    <updated>2025-07-07T13:03:07+00:00</updated>
    <author>
      <name>/u/The_ZMD</name>
      <uri>https://old.reddit.com/user/The_ZMD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to create a ollama-private gpt on my pc. This will be primarily used to ingest couple of engineering handbook so that it can understand some technical stuff. Some of my research papers, subjects/books I read for education, so it knows what I know and what I don't know.&lt;/p&gt; &lt;p&gt;Additonally I need it to compare multiple vendor data, give me best option do some basic analysis, generate report, etc. Do I need to start from scratch or something similar exists? Like a pre trained neural netrowk (like Physics inspired neural network)?&lt;/p&gt; &lt;p&gt;PC specs: 10850k, 32 gb ram, 6900xt, multiple gen 4 ssd and hdd.&lt;/p&gt; &lt;p&gt;Any help is appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The_ZMD"&gt; /u/The_ZMD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lttm5g/want_to_create_a_private_llm_for_ingesting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lttm5g/want_to_create_a_private_llm_for_ingesting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lttm5g/want_to_create_a_private_llm_for_ingesting/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-07T13:03:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1lujfpb</id>
    <title>Ollama Auto Start Despite removed from "Open at Login"</title>
    <updated>2025-07-08T08:39:22+00:00</updated>
    <author>
      <name>/u/frozencoconut03</name>
      <uri>https://old.reddit.com/user/frozencoconut03</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lujfpb/ollama_auto_start_despite_removed_from_open_at/"&gt; &lt;img alt="Ollama Auto Start Despite removed from &amp;quot;Open at Login&amp;quot;" src="https://b.thumbs.redditmedia.com/H5gbzUNdSAKkhBj3cvclCDiU4XP-WvIW5b5y17kVmJs.jpg" title="Ollama Auto Start Despite removed from &amp;quot;Open at Login&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am on a Mac, and for whatever reason, Ollama starts auto starting when I log in to my Mac, despite it not being in the &amp;quot;Open at Login&amp;quot; section. Anyway to fix it?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jv4v2cyb4mbf1.png?width=944&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9febfaccabf15413a49a4183951c308b9e6c9743"&gt;https://preview.redd.it/jv4v2cyb4mbf1.png?width=944&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9febfaccabf15413a49a4183951c308b9e6c9743&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frozencoconut03"&gt; /u/frozencoconut03 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lujfpb/ollama_auto_start_despite_removed_from_open_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lujfpb/ollama_auto_start_despite_removed_from_open_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lujfpb/ollama_auto_start_despite_removed_from_open_at/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T08:39:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lujrg3</id>
    <title>Tool calls issue since v0.8.0</title>
    <updated>2025-07-08T09:01:45+00:00</updated>
    <author>
      <name>/u/awolCZ</name>
      <uri>https://old.reddit.com/user/awolCZ</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, &lt;/p&gt; &lt;p&gt;we are having some issues with gemma3 tools model (PetrosStav) since Ollama v0.8.0. Any help would be appreciated because we are struggling with this for some time.&lt;/p&gt; &lt;p&gt;In v0.7.1, which is the last version which works as expected for us with PetrosStav/gemma3-tools model, tool calls are correctly returned in json parameter - tool_calls. But in 0.8.0, tools calls are returned in content of the message, like this:&lt;br /&gt; {&amp;quot;role&amp;quot;:&amp;quot;assistant&amp;quot;,&amp;quot;content&amp;quot;:&amp;quot;```tool_call\n{\&amp;quot;name\&amp;quot;: \&amp;quot;filterData\&amp;quot;, \&amp;quot;parameters\&amp;quot;: {\&amp;quot;start_datetime\&amp;quot;: \&amp;quot;2025-07-08T00:00:00+02:00\&amp;quot;, \&amp;quot;end_datetime\&amp;quot;: \&amp;quot;2025-07-08T23:59:59+02:00\&amp;quot;}}\n```&amp;quot;}&lt;/p&gt; &lt;p&gt;I'm not sure what exactly changed as changelog was mentioning tool calls streaming only, but it seems like Modelfile of gemma3-tools model somehow became incompatible with Ollama 0.8.0+&lt;/p&gt; &lt;p&gt;Any advice on how to fix this? &lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/awolCZ"&gt; /u/awolCZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lujrg3/tool_calls_issue_since_v080/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lujrg3/tool_calls_issue_since_v080/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lujrg3/tool_calls_issue_since_v080/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T09:01:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lujtdv</id>
    <title>A Good LLM for Python.</title>
    <updated>2025-07-08T09:05:31+00:00</updated>
    <author>
      <name>/u/SultanGreat</name>
      <uri>https://old.reddit.com/user/SultanGreat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a mac m1 mini 8gb and I want the best possible programming (python) llm. So far I tried gemma, llama, deepseek-coder, codellama-pyrhon and a lot more. Some didn't run smoothly others were worse&lt;/p&gt; &lt;p&gt;Currently am using qwen2.5-code 7b, which is good but I want a python focussed llm&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SultanGreat"&gt; /u/SultanGreat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lujtdv/a_good_llm_for_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lujtdv/a_good_llm_for_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lujtdv/a_good_llm_for_python/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T09:05:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lujwy0</id>
    <title>Being a psychologist to your (over)thinking LLM</title>
    <updated>2025-07-08T09:12:20+00:00</updated>
    <author>
      <name>/u/specy_dev</name>
      <uri>https://old.reddit.com/user/specy_dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How reasoning models tend to overthink and why they are not always the best choice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/specy_dev"&gt; /u/specy_dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://specy.app/blog/posts/being-a-psychologist-to-your-overthinking-llm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lujwy0/being_a_psychologist_to_your_overthinking_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lujwy0/being_a_psychologist_to_your_overthinking_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T09:12:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lucq0b</id>
    <title>codex-&gt;ollama (airgapped)</title>
    <updated>2025-07-08T02:03:39+00:00</updated>
    <author>
      <name>/u/neurostream</name>
      <uri>https://old.reddit.com/user/neurostream</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lucq0b/codexollama_airgapped/"&gt; &lt;img alt="codex-&amp;gt;ollama (airgapped)" src="https://external-preview.redd.it/R2JhsIcLOAV6dx1FDhG0En51rNtJ_9CXmjw-Xp6cleg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52a869c1d852237aad81466ecf37b39fd9c9cb4e" title="codex-&amp;gt;ollama (airgapped)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;it's been out there that openai's codex cli agent now has support for other providers, and it also works with local ollama.&lt;/p&gt; &lt;p&gt;trying it out was less involved than i thought. there's no OpenAI account settings, bindings, tokens, or registration cookie calls... it just works like any other shell command.&lt;/p&gt; &lt;p&gt;you set the model name (from your &amp;quot;ollama ls&amp;quot; output) and local ollama port with &amp;quot;codex --config&amp;quot; options (see example below).&lt;/p&gt; &lt;p&gt;&lt;em&gt;installing&lt;/em&gt; download the cli for your os/arch (you can brew install codex on macos). i extracted codex-exec-x86_64-unknown-linux-gnu.tar.gz for my ubuntu thinkpad and renamed it &amp;quot;codex&amp;quot;. &lt;/p&gt; &lt;p&gt;same with codex-exec and code-linux-sandbox (not sure if all 3 are required or just the main codex util, but i just put them all in the PATH.&lt;/p&gt; &lt;p&gt;&lt;em&gt;internet access/airgapping&lt;/em&gt;&lt;/p&gt; &lt;p&gt;internet route from the machine running it isn't required. but you might end up using it in an internet workflow where codex might, for example, use curl to trigger a remote webhook or git to push a branch to your remote repo.&lt;/p&gt; &lt;p&gt;&lt;em&gt;example&lt;/em&gt; shell&amp;gt; cd myrepo shell&amp;gt; codex exec --config model_provider=ollama --config model_providers.ollama.base_url=&lt;a href="http://127.0.01:11423/v1"&gt;http://127.0.01:11423/v1&lt;/a&gt; --config model=qwen3:235b-a22b-q8_0 &amp;quot;summarize what this whole code repo is about&amp;quot;&lt;/p&gt; &lt;p&gt;codex will run shell commands from the current folder to figure it out.. like ls, find , cat, and grep. it outputs the response (describing the repo, in this case) to stdout and returns to the shell prompt.&lt;/p&gt; &lt;p&gt;leave off the &amp;quot;exec&amp;quot; to start in terminal UI mode, which can you supervise tasks in continuous context and without scripting. but i think many will find the power for complex projects is in chaining codex runs together with scripts (like piping a codex exec output back into codex, etc).&lt;/p&gt; &lt;p&gt;you can create a -/.codex/config.toml file and move the --config switches there to keep your command line clean. There are more configuration options (like setting the context size) documented in the github repo for codex.&lt;/p&gt; &lt;p&gt;&lt;em&gt;read/write and allowed shell commands&lt;/em&gt; that example above is &amp;quot;read only&amp;quot;, but for read-write look at &amp;quot;codex help&amp;quot; to see the &amp;quot;--dangerously&amp;quot; switch, which overrides all the &lt;em&gt;sandboxing&lt;/em&gt; and &lt;em&gt;approval&lt;/em&gt; policies (the actual configuration topics that switch should bring your attention to for safe use). then, your prompts can make/update/delete files (code, scripts, documentation, etc) and folders and even run other commands.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Tool calling models and MCP&lt;/em&gt; the model you set has to support tool calling, and i also prefer reasoning models - which significantly narrows down the available options for tools+thinking models i'd &amp;quot;ollama pull&amp;quot; for this. but i've only been able to get qwen3 to be consistent. (anyone know how make other tool models get along with codex better? deepseek-r1 sometimes works) &lt;/p&gt; &lt;p&gt;the latest codex releases also supports using codex as an both an mcp server and mcp client - which i don't know how to do yet (help?); but that might stabilize the consistency across different tool-enabled models.&lt;/p&gt; &lt;p&gt;&lt;em&gt;one-off codex runs vs codexes of codexes of codexes&lt;/em&gt; I think working with smaller models locally will mean less &amp;quot;build huge app in one prompt while i sleep&amp;quot; -type of magical experiences rn. So I'm expecting to decompose my projects and workflows with a bunch of smaller codex script modules. i've also never used langchain or langraph, but maybe harnessing codex with those frameworks is where i should look next? &lt;/p&gt; &lt;p&gt;i'm a more of network cable infra monkey irl , so i hope this clicks with those who are coming from where i'm at.&lt;/p&gt; &lt;p&gt;&lt;em&gt;TL;DR&lt;/em&gt; you can run: &lt;/p&gt; &lt;p&gt;&lt;em&gt;codex &amp;quot;summarize the git history of this branch&amp;quot;&lt;/em&gt; &lt;/p&gt; &lt;p&gt;and it works with local ollama tool models without talking to openai by putting &lt;a href="http://127.0.01:11423/v1"&gt;http://127.0.01:11423/v1&lt;/a&gt; and the model name (like qwen3) in the config.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neurostream"&gt; /u/neurostream &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/openai/codex/releases"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lucq0b/codexollama_airgapped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lucq0b/codexollama_airgapped/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T02:03:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lug5su</id>
    <title>Open Source Alternative to NotebookLM</title>
    <updated>2025-07-08T05:04:15+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a &lt;strong&gt;Highly Customizable AI Research Agent&lt;/strong&gt; that connects to your personal external sources and search engines (Tavily, LinkUp), Slack, Linear, Notion, YouTube, GitHub, Discord, and more coming soon.&lt;/p&gt; &lt;p&gt;I'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here’s a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;📊 &lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt; &lt;li&gt;Hierarchical Indices (2-tiered RAG setup)&lt;/li&gt; &lt;li&gt;Combines Semantic + Full-Text Search with Reciprocal Rank Fusion (Hybrid Search)&lt;/li&gt; &lt;li&gt;Offers a RAG-as-a-Service API Backend&lt;/li&gt; &lt;li&gt;50+ File extensions supported&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🎙️ &lt;strong&gt;Podcasts&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blazingly fast podcast generation agent (3-minute podcast in under 20 seconds)&lt;/li&gt; &lt;li&gt;Convert chat conversations into engaging audio&lt;/li&gt; &lt;li&gt;Multiple TTS providers supported&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;ℹ️ &lt;strong&gt;External Sources Integration&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Search engines (Tavily, LinkUp)&lt;/li&gt; &lt;li&gt;Slack&lt;/li&gt; &lt;li&gt;Linear&lt;/li&gt; &lt;li&gt;Notion&lt;/li&gt; &lt;li&gt;YouTube videos&lt;/li&gt; &lt;li&gt;GitHub&lt;/li&gt; &lt;li&gt;Discord&lt;/li&gt; &lt;li&gt;...and more on the way&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🔖 &lt;strong&gt;Cross-Browser Extension&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The SurfSense extension lets you save any dynamic webpage you want, including authenticated content.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lug5su/open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lug5su/open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lug5su/open_source_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-08T05:04:15+00:00</published>
  </entry>
</feed>
