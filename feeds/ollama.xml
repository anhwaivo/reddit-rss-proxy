<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-30T10:49:03+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1icivtq</id>
    <title>Why do local models still censor stuff?</title>
    <updated>2025-01-29T01:59:16+00:00</updated>
    <author>
      <name>/u/Sure-Year2141</name>
      <uri>https://old.reddit.com/user/Sure-Year2141</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I asked it a very grotesque question and it straight up refused to answer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sure-Year2141"&gt; /u/Sure-Year2141 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icivtq/why_do_local_models_still_censor_stuff/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icivtq/why_do_local_models_still_censor_stuff/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icivtq/why_do_local_models_still_censor_stuff/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T01:59:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ictf5d</id>
    <title>best model using 4080 super for general tasks?</title>
    <updated>2025-01-29T13:11:44+00:00</updated>
    <author>
      <name>/u/ButterscotchOk1476</name>
      <uri>https://old.reddit.com/user/ButterscotchOk1476</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im going to get the 5080 when it releases which is just a few percent faster than the 4080 super, what models can I run locally? I mainly use ai for creative writing and programming. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ButterscotchOk1476"&gt; /u/ButterscotchOk1476 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ictf5d/best_model_using_4080_super_for_general_tasks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ictf5d/best_model_using_4080_super_for_general_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ictf5d/best_model_using_4080_super_for_general_tasks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T13:11:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1iczj8c</id>
    <title>What's the "minimum" parameters needed to have a useful model for everyday q&amp;a?</title>
    <updated>2025-01-29T17:38:52+00:00</updated>
    <author>
      <name>/u/abrandis</name>
      <uri>https://old.reddit.com/user/abrandis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So what is your feeling about what's the smalles model size in terms of parameters that yields USEFUL and mostly ACCURATE results most of the time. &lt;/p&gt; &lt;p&gt;I understand this is subjective, but for I find any models under 32b parameters just aren't very accurate...what are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abrandis"&gt; /u/abrandis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iczj8c/whats_the_minimum_parameters_needed_to_have_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iczj8c/whats_the_minimum_parameters_needed_to_have_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iczj8c/whats_the_minimum_parameters_needed_to_have_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T17:38:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1id17lf</id>
    <title>Deepseek-r1:8b broken english and refers to itself in the second person?</title>
    <updated>2025-01-29T18:46:23+00:00</updated>
    <author>
      <name>/u/AJaxx92</name>
      <uri>https://old.reddit.com/user/AJaxx92</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1id17lf/deepseekr18b_broken_english_and_refers_to_itself/"&gt; &lt;img alt="Deepseek-r1:8b broken english and refers to itself in the second person?" src="https://b.thumbs.redditmedia.com/S5hijmY0eFS2f7fSehoLDBZX7qVtW8eH3eltx5fEX4A.jpg" title="Deepseek-r1:8b broken english and refers to itself in the second person?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/atnwc2ks9zfe1.png?width=1064&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=216007c0f818ae007cbdc609e33c32ca20b9c85d"&gt;https://preview.redd.it/atnwc2ks9zfe1.png?width=1064&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=216007c0f818ae007cbdc609e33c32ca20b9c85d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Not the first odd response I've had. It's used Chinese for 'woof woof' when I told it to think like a dog before, but I wasn't expecting broken english. Is it to do with my hardware (12600k, 3080 ti, 32gb 3600mhz), or is it the model?&lt;/p&gt; &lt;p&gt;Edit: just tried asking the same question with the 14b model, and it definitely gave me a better response but still included Chinese with no explanation.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ec947p37ezfe1.png?width=1059&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=faccf6ccdf65e228bb362f9c7c4d97db35eab549"&gt;https://preview.redd.it/ec947p37ezfe1.png?width=1059&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=faccf6ccdf65e228bb362f9c7c4d97db35eab549&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit 2: is it translating on the fly or seeing something different to me? There wasn't &lt;em&gt;that&lt;/em&gt; much Chinese in the last response.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s9wcjenwezfe1.png?width=1054&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=03b91ad575246eb45a13bf51b2c1927993a77464"&gt;https://preview.redd.it/s9wcjenwezfe1.png?width=1054&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=03b91ad575246eb45a13bf51b2c1927993a77464&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AJaxx92"&gt; /u/AJaxx92 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id17lf/deepseekr18b_broken_english_and_refers_to_itself/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id17lf/deepseekr18b_broken_english_and_refers_to_itself/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1id17lf/deepseekr18b_broken_english_and_refers_to_itself/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T18:46:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1icxuzx</id>
    <title>KIOXIA Releases AiSAQ as Open-Source Software to Reduce DRAM Needs in AI Systems</title>
    <updated>2025-01-29T16:31:35+00:00</updated>
    <author>
      <name>/u/Kqyxzoj</name>
      <uri>https://old.reddit.com/user/Kqyxzoj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.techpowerup.com/331706/kioxia-releases-aisaq-as-open-source-software-to-reduce-dram-needs-in-ai-systems"&gt;TechPowerUp:&lt;/a&gt; &lt;em&gt;&amp;quot;Kioxia Corporation, a world leader in memory solutions, today announced the open-source release of its new All-in-Storage ANNS with Product Quantization (AiSAQ) technology. A novel &amp;quot;approximate nearest neighbor&amp;quot; search (ANNS) algorithm optimized for SSDs, KIOXIA AiSAQ software delivers scalable performance for retrieval-augmented generation (RAG) without placing index data in DRAM - and instead searching directly on SSDs.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Generative AI systems demand significant compute, memory and storage resources. While they have the potential to drive transformative breakthroughs across various industries, their deployment often comes with high costs. RAG is a critical phase of AI that refines large language models (LLMs) with data specific to the company or application.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Now in general using SSDs instead of VRAM or DRAM is going to be pretty damn slow. But for some use cases such as RAG, SSDs can be part of the solution:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.techpowerup.com/331706/kioxia-releases-aisaq-as-open-source-software-to-reduce-dram-needs-in-ai-systems"&gt;TechPowerUp: KIOXIA Releases AiSAQ as Open-Source Software to Reduce DRAM Needs in AI Systems&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/kioxiaamerica/aisaq-diskann"&gt;KIOXIA AiSAQ-DiskANN repo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2404.06004"&gt;arXiv:2404.06004: Writes Hurt: Lessons in Cache Design for Optane NVRAM&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Incidentally, that Kioxia repo is a fork of &lt;a href="https://github.com/microsoft/DiskANN"&gt;Microsoft's DiskANN repo&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It would be cool if something like this could be integrated in an Ollama RAG pipeline to lower the DRAM requirements. Running a &lt;a href="https://www.reddit.com/r/selfhosted/comments/1ic8zil/yes_you_can_run_deepseekr1_locally_on_your_device/"&gt;heavily quantized version of DeepSeek-R1 locally&lt;/a&gt; combined with RAG, where most of the RAG storage requirements are pushed to SSD sounds like an interesting idea.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kqyxzoj"&gt; /u/Kqyxzoj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icxuzx/kioxia_releases_aisaq_as_opensource_software_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icxuzx/kioxia_releases_aisaq_as_opensource_software_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icxuzx/kioxia_releases_aisaq_as_opensource_software_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T16:31:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1icxpnt</id>
    <title>modelfile for DeepSeek-R1</title>
    <updated>2025-01-29T16:25:37+00:00</updated>
    <author>
      <name>/u/Fine-Degree431</name>
      <uri>https://old.reddit.com/user/Fine-Degree431</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone created a modelfile for deepseek, to set parameters such as temperature, context in ollama, if so can you share how it was done, and is used when using deepseek.&lt;/p&gt; &lt;p&gt;Thank you for your help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fine-Degree431"&gt; /u/Fine-Degree431 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icxpnt/modelfile_for_deepseekr1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icxpnt/modelfile_for_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icxpnt/modelfile_for_deepseekr1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T16:25:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1id36qa</id>
    <title>`ollama pull` keeps failing and restarting?</title>
    <updated>2025-01-29T20:05:51+00:00</updated>
    <author>
      <name>/u/XiPingTing</name>
      <uri>https://old.reddit.com/user/XiPingTing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried running &lt;code&gt;ollama pull deepseek-r1:14b&lt;/code&gt; overnight. The logs show it gets to about 30% done then gives up and restarts. I think I've downloaded the model from Hugging Face:&lt;/p&gt; &lt;p&gt;huggingface-cli download bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF --include &amp;quot;DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf&amp;quot; --local-dir ./&lt;/p&gt; &lt;p&gt;Where do I put this file and what do I name it to so I can run &lt;code&gt;ollama pull deepseek-r1:14b&lt;/code&gt;? I've got the &lt;code&gt;ollama pull deepseek-r1:7b&lt;/code&gt; model working just fine&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XiPingTing"&gt; /u/XiPingTing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id36qa/ollama_pull_keeps_failing_and_restarting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id36qa/ollama_pull_keeps_failing_and_restarting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1id36qa/ollama_pull_keeps_failing_and_restarting/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T20:05:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1idbzcx</id>
    <title>DeepSeek R1 moe carte extension vram !</title>
    <updated>2025-01-30T02:32:54+00:00</updated>
    <author>
      <name>/u/MoreIndependent5967</name>
      <uri>https://old.reddit.com/user/MoreIndependent5967</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In fact you will need pcle 5 cards, 400 GB of vram... With a 5090 graphics card to manage the 30b active parameters, in pcle 5 bottleneck (64 GB second speed) Then you will need software libraries that support vram use external on pcle! I think it’s the future to have cards dedicated to vram pcle 5&lt;/p&gt; &lt;p&gt;I find it ridiculous to have to systematically repurchase graphics cards with GPU processors for large moe models without a single GPU sufficient with vram extension cards&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoreIndependent5967"&gt; /u/MoreIndependent5967 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idbzcx/deepseek_r1_moe_carte_extension_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idbzcx/deepseek_r1_moe_carte_extension_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idbzcx/deepseek_r1_moe_carte_extension_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T02:32:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1icrcdx</id>
    <title>Why don't we use NVMe instead of VRAM</title>
    <updated>2025-01-29T11:04:25+00:00</updated>
    <author>
      <name>/u/infinity6570</name>
      <uri>https://old.reddit.com/user/infinity6570</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why don't we use NMVe storage drives on PCIe lanes to directly serve the GPU instead of loading huge models to VRAM?? Yes, it will be slower and will have more latency, but being able to run something vs nothing is better right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/infinity6570"&gt; /u/infinity6570 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icrcdx/why_dont_we_use_nvme_instead_of_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icrcdx/why_dont_we_use_nvme_instead_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icrcdx/why_dont_we_use_nvme_instead_of_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T11:04:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1id5z7c</id>
    <title>Multimodal ollama</title>
    <updated>2025-01-29T22:02:35+00:00</updated>
    <author>
      <name>/u/DifficultTomatillo29</name>
      <uri>https://old.reddit.com/user/DifficultTomatillo29</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using ollama constantly on my mac - and love it - have python programs that talk to ollama via rest api using various models - but now I have some tasks I want to move into multimodal - particularly in terms of getting an image back - is that possible using ollama? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DifficultTomatillo29"&gt; /u/DifficultTomatillo29 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id5z7c/multimodal_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id5z7c/multimodal_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1id5z7c/multimodal_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T22:02:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1idengd</id>
    <title>Production-Ready Ollama Alternative for Handling Next.js Requests?</title>
    <updated>2025-01-30T04:40:20+00:00</updated>
    <author>
      <name>/u/Apprehensive-Sale-52</name>
      <uri>https://old.reddit.com/user/Apprehensive-Sale-52</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, a few weeks ago I saw a post saying Ollama isn't for production, but now I can't find that post and I don't remember the answer, so could you say me what is the best Ollama alternative for production? I want to expose a port to recive requests from a nextjs website, or just link me to that post, thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apprehensive-Sale-52"&gt; /u/Apprehensive-Sale-52 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idengd/productionready_ollama_alternative_for_handling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idengd/productionready_ollama_alternative_for_handling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idengd/productionready_ollama_alternative_for_handling/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T04:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1idgkww</id>
    <title>Download bandwidth restrictions</title>
    <updated>2025-01-30T06:24:39+00:00</updated>
    <author>
      <name>/u/Mrfudog</name>
      <uri>https://old.reddit.com/user/Mrfudog</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have set up ollama on a ubuntu VM on my proxmox host. The GPU is mounted directly on the VM. When pulling a model I get no more than 10MBps bandwidth even though I can get more on the VM (tested through ookla). Anybody got an idea if this is restricted on ubuntu or ollama itself and if I can increase this or is it limited by the server providing the model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mrfudog"&gt; /u/Mrfudog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idgkww/download_bandwidth_restrictions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idgkww/download_bandwidth_restrictions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idgkww/download_bandwidth_restrictions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T06:24:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1idcbpw</id>
    <title>How to run Ollama remotely.</title>
    <updated>2025-01-30T02:48:56+00:00</updated>
    <author>
      <name>/u/Lelentos</name>
      <uri>https://old.reddit.com/user/Lelentos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not as tech savvy as most of you, so sorry if this is a dumb question.&lt;/p&gt; &lt;p&gt;I've managed to get Ollama working on my local network, so I have it running on my desktop but can use chatbox.app on my laptop or phone. Simple enough.&lt;/p&gt; &lt;p&gt;Now I want to find out how to use it while out of the house. How do I route back to it? Would I be better off with remote desktop?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lelentos"&gt; /u/Lelentos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idcbpw/how_to_run_ollama_remotely/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idcbpw/how_to_run_ollama_remotely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idcbpw/how_to_run_ollama_remotely/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T02:48:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1idiu7q</id>
    <title>Any advice on how to get rocm working on Ubuntu?</title>
    <updated>2025-01-30T08:52:16+00:00</updated>
    <author>
      <name>/u/AxlIsAShoto</name>
      <uri>https://old.reddit.com/user/AxlIsAShoto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Rocm is installed, I added my user to some groups, tried installing ollama directly in ubuntu, and also using docker.&lt;/p&gt; &lt;p&gt;When I install ollama directly it says my amd gpu is ready for use or something along those lines.&lt;/p&gt; &lt;p&gt;No error messages at all just the gpu is mostly idle and my cpu goes crazy when any model I've tried is replying to my questions.&lt;/p&gt; &lt;p&gt;I'm using an RX 6700 XT on Ubuntu 24.04. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AxlIsAShoto"&gt; /u/AxlIsAShoto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idiu7q/any_advice_on_how_to_get_rocm_working_on_ubuntu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idiu7q/any_advice_on_how_to_get_rocm_working_on_ubuntu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idiu7q/any_advice_on_how_to_get_rocm_working_on_ubuntu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T08:52:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1icv7wv</id>
    <title>Hardware requirements for running the full size deepseek R1 with ollama?</title>
    <updated>2025-01-29T14:39:12+00:00</updated>
    <author>
      <name>/u/BC547</name>
      <uri>https://old.reddit.com/user/BC547</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My machine runs the Deepseek R1-14B model fine, but the 34B and 70B are too slow for practical use. I am looking at building a machine capable of running the full 671B model fast enough that it's not too annoying as a coding assistant. What kind of hardware do i need?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BC547"&gt; /u/BC547 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icv7wv/hardware_requirements_for_running_the_full_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icv7wv/hardware_requirements_for_running_the_full_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icv7wv/hardware_requirements_for_running_the_full_size/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T14:39:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1idjux3</id>
    <title>How can I use Ollama models in Linux as an alternative to Mac Siri?</title>
    <updated>2025-01-30T10:12:16+00:00</updated>
    <author>
      <name>/u/ReceptionLow2817</name>
      <uri>https://old.reddit.com/user/ReceptionLow2817</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently I've installed the Ollama and available models from library but wanna use it like Siri in Mac or iPad or iPhone to automate my tasks. Anyone have thought how can I accomplish this using these open-source models??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ReceptionLow2817"&gt; /u/ReceptionLow2817 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idjux3/how_can_i_use_ollama_models_in_linux_as_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idjux3/how_can_i_use_ollama_models_in_linux_as_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idjux3/how_can_i_use_ollama_models_in_linux_as_an/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T10:12:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1idjvga</id>
    <title>Troubleshooting Ollama docker/Hardware</title>
    <updated>2025-01-30T10:13:26+00:00</updated>
    <author>
      <name>/u/notdademurphy</name>
      <uri>https://old.reddit.com/user/notdademurphy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Need suggestions on where to start trouble shooting slow OpenWebUI in docker. I'm new to running LLM's and i'm unsure of where to start. &lt;/p&gt; &lt;p&gt;Hardware CPU: AMD Ryzen 7950x&lt;br /&gt; RAM: 64GB&lt;br /&gt; GPU: Nvidia RTX 4090 24GB&lt;br /&gt; Storage: Samsung m2 SSD 980 PRO 2TB &lt;/p&gt; &lt;p&gt;When trying to use OpenWebUI inside docker connected to my Ollama container the performance is really slow. Before getting a response from a prompt the UI waits two minutes but then render the response quickly. &lt;/p&gt; &lt;p&gt;When i earlier used OpenwebUI in docker connected to a Ollama instance on the windows host there was no waiting time. &lt;/p&gt; &lt;p&gt;Running Ollama in docker with nvidia support.&lt;br /&gt; Did a basic test running 'ollama run llama3.1 --verbose' inside the docker container. &lt;/p&gt; &lt;p&gt;The result is 114 tokens/s.&lt;br /&gt; total duration: 6.734057125s&lt;br /&gt; load duration: 69.839869ms&lt;br /&gt; prompt eval count: 14 token(s)&lt;br /&gt; prompt eval duration: 3ms&lt;br /&gt; prompt eval rate: 4666.67 tokens/s&lt;br /&gt; eval count: 762 token(s)&lt;br /&gt; eval duration: 6.659s&lt;br /&gt; eval rate: 114.43 tokens/s &lt;/p&gt; &lt;p&gt;Separate question, what type/size of models are reasonable to run on my hardware?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notdademurphy"&gt; /u/notdademurphy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idjvga/troubleshooting_ollama_dockerhardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idjvga/troubleshooting_ollama_dockerhardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idjvga/troubleshooting_ollama_dockerhardware/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T10:13:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1icyl2l</id>
    <title>Will there ever be uncensored self hosted AI?</title>
    <updated>2025-01-29T17:00:55+00:00</updated>
    <author>
      <name>/u/mshriver2</name>
      <uri>https://old.reddit.com/user/mshriver2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tried out Ollama today for the first time as I was excited at the possibility of having a fully uncensored and unrestricted AI that could answer any question. Unfortunately even self hosted it is just as censored as Chat-GPT or any other large AI model. Do you think we will ever have a fully open source completely unrestricted AI? I don't understand how a company gets to decide what code runs or doesn't run on my own hardware.&lt;/p&gt; &lt;p&gt;Apologies for the rant in advance.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I should be the one deciding what is &amp;quot;legal&amp;quot; or &amp;quot;ethical&amp;quot; not my computer.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model used for testing: DeepSeek-R1-Distill-Qwen-32B&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mshriver2"&gt; /u/mshriver2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icyl2l/will_there_ever_be_uncensored_self_hosted_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icyl2l/will_there_ever_be_uncensored_self_hosted_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icyl2l/will_there_ever_be_uncensored_self_hosted_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T17:00:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1idk3gm</id>
    <title>Running a single LLM across multiple GPUs</title>
    <updated>2025-01-30T10:30:13+00:00</updated>
    <author>
      <name>/u/Agreeable-Worker7659</name>
      <uri>https://old.reddit.com/user/Agreeable-Worker7659</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was recently thinking of running a LLM like Deepseek r1 32b on a GPU, but the problem is that it won't fit into the memory of any single GPU I could afford. Funnily enough, it runs at around human speech speed on my Ryzen 9 9950x and 64GB DDR5, but being able to run it a bit faster on GPUs would be really good.&lt;/p&gt; &lt;p&gt;Therefore the idea was to see if it could be somehow distributed across several GPUs, but if I understand correctly, that's only possible with nVlink that's available only since Volta architecture pro-grade GPUs alike Quadro or Tesla? Would it be correct to assume that with something like 2x Tesla P40 it just won't work, since they can't appear as a single unit with shared memory? Are there any AMD alternatives capable of running such setup at a budget?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agreeable-Worker7659"&gt; /u/Agreeable-Worker7659 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idk3gm/running_a_single_llm_across_multiple_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idk3gm/running_a_single_llm_across_multiple_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idk3gm/running_a_single_llm_across_multiple_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T10:30:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1id5kkt</id>
    <title>Multi GPU</title>
    <updated>2025-01-29T21:45:21+00:00</updated>
    <author>
      <name>/u/666devi</name>
      <uri>https://old.reddit.com/user/666devi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Can I mix a 4080super and a 1080ti to handle bigger models? Is it worth it or should i just sell the 1080?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/666devi"&gt; /u/666devi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id5kkt/multi_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id5kkt/multi_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1id5kkt/multi_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T21:45:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1id92b2</id>
    <title>Cursor + Ollama -- Help a Blind Guy?</title>
    <updated>2025-01-30T00:18:10+00:00</updated>
    <author>
      <name>/u/mdizak</name>
      <uri>https://old.reddit.com/user/mdizak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Life decided to play a funny prank on me years ago and make me suddenly and totally blind. Can anyone quickly help here?&lt;/p&gt; &lt;p&gt;Looking to get Cursor IDE working with local install of Ollama. Within Cursor, I can go into Preferences -&amp;gt; Models screen and see the various textboxes for API keys, and can also see just under the OpenAI API Key textbox there's a link to override the API URL. I'm assuming this is what I want to flip the API endpoint to my local Ollama install.&lt;/p&gt; &lt;p&gt;For the life of me though, I'm unable to click on that link via screen reader, and posting on &lt;a href="/r/blind"&gt;/r/blind&lt;/a&gt; was of no help. So let's go with manual settings change route. &lt;/p&gt; &lt;p&gt;Found the settings file located at ~APP_CONFIG/Cursor/User/settings.json. On Linux Mint at least, this means at: ~/.config/Cursor/User/settings.json&lt;/p&gt; &lt;p&gt;Unfortunately, this file only lists modified settings instead of all settings. This is where I need help. &lt;/p&gt; &lt;p&gt;Could one of you kind souls that has Cursor running with local Ollama look in your settings.json file, and look for the settings that seem appropriate? Just need to send the API endpoint to Ollama's of &lt;a href="http://127.0.0.1:11434/api/generate"&gt;http://127.0.0.1:11434/api/generate&lt;/a&gt;, and put &amp;quot;deepseek-r1&amp;quot; as the model name. Looking through the setting names, it should be pretty obvious, plus any settings that looks like it's needed to enable / activate it.&lt;/p&gt; &lt;p&gt;If anyone is kind enough to reply with those setting names, would be greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mdizak"&gt; /u/mdizak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id92b2/cursor_ollama_help_a_blind_guy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id92b2/cursor_ollama_help_a_blind_guy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1id92b2/cursor_ollama_help_a_blind_guy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T00:18:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1idjwkh</id>
    <title>how to use Ollama with C++</title>
    <updated>2025-01-30T10:15:50+00:00</updated>
    <author>
      <name>/u/Reasonable-Falcon470</name>
      <uri>https://old.reddit.com/user/Reasonable-Falcon470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi i am bad with python i just cant i tried and i cant find any C++ instructions for Ollama&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable-Falcon470"&gt; /u/Reasonable-Falcon470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idjwkh/how_to_use_ollama_with_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idjwkh/how_to_use_ollama_with_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idjwkh/how_to_use_ollama_with_c/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T10:15:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1idf5ez</id>
    <title>How can I force Ollama to use the cpu instead of gpu?</title>
    <updated>2025-01-30T05:06:19+00:00</updated>
    <author>
      <name>/u/ResponsibleTruck4717</name>
      <uri>https://old.reddit.com/user/ResponsibleTruck4717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know I can force it use cpu instead of gpu when I'm using docker but I'm looking for solution without docker.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleTruck4717"&gt; /u/ResponsibleTruck4717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idf5ez/how_can_i_force_ollama_to_use_the_cpu_instead_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idf5ez/how_can_i_force_ollama_to_use_the_cpu_instead_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idf5ez/how_can_i_force_ollama_to_use_the_cpu_instead_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T05:06:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1id6y97</id>
    <title>I feel bad for the AI lol after seeing its chain of thought. 😭</title>
    <updated>2025-01-29T22:44:12+00:00</updated>
    <author>
      <name>/u/Tricky_Reflection_75</name>
      <uri>https://old.reddit.com/user/Tricky_Reflection_75</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1id6y97/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt; &lt;img alt="I feel bad for the AI lol after seeing its chain of thought. 😭" src="https://b.thumbs.redditmedia.com/enUEH3eKOb0e3umQhJmhNKG1mhLeM6dFReIv_8oKkzc.jpg" title="I feel bad for the AI lol after seeing its chain of thought. 😭" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/910r117yg0ge1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c699f0b744adc486372072f5a73072f6d893f97e"&gt;https://preview.redd.it/910r117yg0ge1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c699f0b744adc486372072f5a73072f6d893f97e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tricky_Reflection_75"&gt; /u/Tricky_Reflection_75 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id6y97/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1id6y97/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1id6y97/i_feel_bad_for_the_ai_lol_after_seeing_its_chain/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-29T22:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1idh8ft</id>
    <title>Deepseek r1 671b on my local PC</title>
    <updated>2025-01-30T07:03:27+00:00</updated>
    <author>
      <name>/u/Geschirrtuch</name>
      <uri>https://old.reddit.com/user/Geschirrtuch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;Two days ago, I turned night into day, and in the end, I managed to get R1 running on my local PC. Yesterday, I uploaded a video on YouTube showing how I did it: &lt;a href="https://www.youtube.com/watch?v=O3Lk3xSkAdk"&gt;https://www.youtube.com/watch?v=O3Lk3xSkAdk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I don't post here often, so I'm not sure if sharing the link is okay—I hope it is.&lt;/p&gt; &lt;p&gt;The video is in German, but with subtitles, everyone should be able to understand it.&lt;br /&gt; Be careful if you want to try this yourself! ;)&lt;/p&gt; &lt;p&gt;Update:&lt;/p&gt; &lt;p&gt;For those who don't feel like watching the video: The &amp;quot;trick&amp;quot; was using Windows' pagefile. I set up three of them on three different SSDs, which gave me around 750GB of virtual memory in total.&lt;/p&gt; &lt;p&gt;Loading the model and answering a question took my PC about 90 minutes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Geschirrtuch"&gt; /u/Geschirrtuch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1idh8ft/deepseek_r1_671b_on_my_local_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-30T07:03:27+00:00</published>
  </entry>
</feed>
