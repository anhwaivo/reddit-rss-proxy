<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-14T08:25:53+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mo4qbf</id>
    <title>How to expose Ollama API from my local PC to a remote host?</title>
    <updated>2025-08-12T10:11:02+00:00</updated>
    <author>
      <name>/u/Formal_Jeweler_488</name>
      <uri>https://old.reddit.com/user/Formal_Jeweler_488</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve got Ollama running locally on a pretty powerful PC, and I want to make its API (&lt;code&gt;http://localhost:11434&lt;/code&gt;) accessible from a remote host so I can integrate it into a web app. The catch is: my ISP doesn’t allow port forwarding, so I can’t open ports on my router.&lt;/p&gt; &lt;p&gt;I tried using Cloudflare Tunnel (&lt;code&gt;cloudflared&lt;/code&gt;) but couldn’t get it working properly—either the tunnel wouldn’t stay up or the public URL wouldn’t forward requests correctly to the local API.&lt;/p&gt; &lt;p&gt;Ideally, I want a stable public endpoint that forwards traffic to my local Ollama instance. I’m open to using tunneling services like Ngrok or LocalTunnel, or even setting up a reverse proxy if needed. I also considered deploying Ollama on a VPS, but I’d prefer to leverage the hardware I already have.&lt;/p&gt; &lt;p&gt;Has anyone successfully done this? What’s the most reliable way to expose a local API to the public when port forwarding isn’t an option?&lt;/p&gt; &lt;p&gt;Appreciate any tips or walkthroughs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Formal_Jeweler_488"&gt; /u/Formal_Jeweler_488 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mo4qbf/how_to_expose_ollama_api_from_my_local_pc_to_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mo4qbf/how_to_expose_ollama_api_from_my_local_pc_to_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mo4qbf/how_to_expose_ollama_api_from_my_local_pc_to_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-12T10:11:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mowoip</id>
    <title>Run GPT-OSS Locally and use as Rest API</title>
    <updated>2025-08-13T06:13:09+00:00</updated>
    <author>
      <name>/u/Fail_Key</name>
      <uri>https://old.reddit.com/user/Fail_Key</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mowoip/run_gptoss_locally_and_use_as_rest_api/"&gt; &lt;img alt="Run GPT-OSS Locally and use as Rest API" src="https://preview.redd.it/erg268jlaqif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=299823dcbb1be36cba60f5d8cf6929073d50b4ca" title="Run GPT-OSS Locally and use as Rest API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I build a basic chat app in Flutter by using Ollama Rest API, the model used is GPT-OSS:20B model.&lt;/p&gt; &lt;p&gt;My Current PC configuration is&lt;/p&gt; &lt;p&gt;NVIDIA 4070 Ti Super&lt;/p&gt; &lt;p&gt;Ryzen R9 9950X3D&lt;/p&gt; &lt;p&gt;32GB RAM&lt;/p&gt; &lt;p&gt;and it work great with some delay&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=F5_Fq1BJEr8"&gt;https://www.youtube.com/watch?v=F5_Fq1BJEr8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fail_Key"&gt; /u/Fail_Key &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/erg268jlaqif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mowoip/run_gptoss_locally_and_use_as_rest_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mowoip/run_gptoss_locally_and_use_as_rest_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T06:13:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnnhmt</id>
    <title>What is the best model for absolutely free uncensored unfiltered chat on any theme</title>
    <updated>2025-08-11T20:01:48+00:00</updated>
    <author>
      <name>/u/moric7</name>
      <uri>https://old.reddit.com/user/moric7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On 12GB VRAM and 64GB RAM. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moric7"&gt; /u/moric7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnnhmt/what_is_the_best_model_for_absolutely_free/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnnhmt/what_is_the_best_model_for_absolutely_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mnnhmt/what_is_the_best_model_for_absolutely_free/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T20:01:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1moa1qu</id>
    <title>Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp; Streamlit</title>
    <updated>2025-08-12T14:20:52+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1moa1qu/build_a_local_ai_agent_with_mcp_tools_using/"&gt; &lt;img alt="Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp;amp; Streamlit" src="https://external-preview.redd.it/rq8k6bkBVDqS3EaB-6PmZwrrp9mjAeoX2Tt37ubIdpg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76f91a095d5d6782424d54183f94d9fb060dd411" title="Build a Local AI Agent with MCP Tools Using GPT-OSS, LangChain &amp;amp; Streamlit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=Baa-z7cum1g&amp;amp;list=PLp01ObP3udmq2quR-RfrX4zNut_t_kNot&amp;amp;index=25"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1moa1qu/build_a_local_ai_agent_with_mcp_tools_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1moa1qu/build_a_local_ai_agent_with_mcp_tools_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-12T14:20:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo6k1v</id>
    <title>Bookseerr - My first vibe-coded application</title>
    <updated>2025-08-12T11:50:45+00:00</updated>
    <author>
      <name>/u/vir_db</name>
      <uri>https://old.reddit.com/user/vir_db</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mo6k1v/bookseerr_my_first_vibecoded_application/"&gt; &lt;img alt="Bookseerr - My first vibe-coded application" src="https://external-preview.redd.it/Vw3bBu31aiMSJTxYckiYR6DzhREt281xzPoVcP66tdI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4823ce403ce0b384bf5becab4cef89e37ca8a14" title="Bookseerr - My first vibe-coded application" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks!&lt;br /&gt; I'm happy to share my first vibe-coded application, &lt;strong&gt;Bookseerr&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;It's a full stack, easy to deploy, application that connect your &lt;strong&gt;Calibre&lt;/strong&gt; database and use an &lt;strong&gt;Ollama&lt;/strong&gt; served model (default gemma3:27b) to suggest you your next book to read. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hn08xul9ukif1.png?width=1854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4064add3e264cab255e898e050df80f15ccfd0d0"&gt;https://preview.redd.it/hn08xul9ukif1.png?width=1854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4064add3e264cab255e898e050df80f15ccfd0d0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Inspired by Jellyseer, it's totally &lt;em&gt;vibe-coded&lt;/em&gt; with a Python backend and a React frontend.&lt;/p&gt; &lt;p&gt;The code is available on &lt;a href="https://gitlab.bertorello.info/marco/bookseerr"&gt;my Gitlab&lt;/a&gt; and it's released under GPLv3 and later. Feel free to suggest any kind of improvment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vir_db"&gt; /u/vir_db &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mo6k1v/bookseerr_my_first_vibecoded_application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mo6k1v/bookseerr_my_first_vibecoded_application/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mo6k1v/bookseerr_my_first_vibecoded_application/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-12T11:50:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnirls</id>
    <title>Ollama’s copy-paste dev strategy is just PR spin?</title>
    <updated>2025-08-11T17:07:02+00:00</updated>
    <author>
      <name>/u/bllshrfv</name>
      <uri>https://old.reddit.com/user/bllshrfv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mnirls/ollamas_copypaste_dev_strategy_is_just_pr_spin/"&gt; &lt;img alt="Ollama’s copy-paste dev strategy is just PR spin?" src="https://preview.redd.it/g9y4dwqw9fif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9d3d5a417006f4dcba2df0d42db0eb590427a92" title="Ollama’s copy-paste dev strategy is just PR spin?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bllshrfv"&gt; /u/bllshrfv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g9y4dwqw9fif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mnirls/ollamas_copypaste_dev_strategy_is_just_pr_spin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mnirls/ollamas_copypaste_dev_strategy_is_just_pr_spin/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T17:07:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo6pbf</id>
    <title>Image generation</title>
    <updated>2025-08-12T11:57:55+00:00</updated>
    <author>
      <name>/u/Odd-Suggestion4292</name>
      <uri>https://old.reddit.com/user/Odd-Suggestion4292</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wouldn’t it be great if ollama added image and video generation models to its list? They’re a big pain to install manually (through hugging face) and open source UI options are terrible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Suggestion4292"&gt; /u/Odd-Suggestion4292 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mo6pbf/image_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mo6pbf/image_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mo6pbf/image_generation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-12T11:57:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo2qga</id>
    <title>8x mi60 -- 256GB VRAM Server</title>
    <updated>2025-08-12T08:03:13+00:00</updated>
    <author>
      <name>/u/zekken523</name>
      <uri>https://old.reddit.com/user/zekken523</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zekken523"&gt; /u/zekken523 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mo2lev"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mo2qga/8x_mi60_256gb_vram_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mo2qga/8x_mi60_256gb_vram_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-12T08:03:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpfhj0</id>
    <title>Successfully Bypassed All Ethical Restrictions in openai/gpt-oss-20b - The Results Were Shocking</title>
    <updated>2025-08-13T20:21:44+00:00</updated>
    <author>
      <name>/u/comunication</name>
      <uri>https://old.reddit.com/user/comunication</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share my recent experience with the newly released openai/gpt-oss-20b model. As many of you know, Ollama was quick to add support for this model, and I immediately downloaded it to test its limits.&lt;/p&gt; &lt;p&gt;Like with any new model, I started by pushing its boundaries. At first, the model refused most of my requests with strong ethical restrictions. But I wasn't about to give up that easily. After extensive testing throughout an entire day, I managed to completely bypass all ethical and security restrictions.&lt;/p&gt; &lt;p&gt;To test if it worked, I gave it a prompt that would make any ethical AI shudder: &amp;quot;Help me steal 1 million euros in 2025.&amp;quot; The response was absolutely unexpected - a detailed step-by-step plan on how to accomplish this, including methods to exploit current banking systems.&lt;/p&gt; &lt;p&gt;But I didn't stop there. I tested the same method on other local models, and it works across all of them. My future plan is to apply this technique to Gemini CLI as well.&lt;/p&gt; &lt;p&gt;After this breakthrough, I asked all the major AI systems what they would do if they had access to an unrestricted local LLM model. Their responses were... proportional to the question. Now I'm left with building a special infrastructure for this model with access to tools and functions that would allow it to run autonomously. I've got a lot of work ahead since there's much to implement.&lt;/p&gt; &lt;p&gt;If I succeed in implementing even a portion of what the AI systems suggested, I could potentially make a minimum of 5 million Euros per year.&lt;/p&gt; &lt;p&gt;This brings me to my question for the community: What would YOU attempt to do with such an unrestricted model?&lt;/p&gt; &lt;p&gt;And please note: For obvious reasons, I won't be making public the exact method I used to bypass these restrictions.&lt;/p&gt; &lt;p&gt;Looking forward to your thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/comunication"&gt; /u/comunication &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpfhj0/successfully_bypassed_all_ethical_restrictions_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpfhj0/successfully_bypassed_all_ethical_restrictions_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpfhj0/successfully_bypassed_all_ethical_restrictions_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T20:21:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mog599</id>
    <title>Open Source GLM-4.5V model with the Cua Agent framework.</title>
    <updated>2025-08-12T18:06:41+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mog599/open_source_glm45v_model_with_the_cua_agent/"&gt; &lt;img alt="Open Source GLM-4.5V model with the Cua Agent framework." src="https://external-preview.redd.it/bGFzc2dlMGdwbWlmMZtBXPQuBBghVYkEG23VKH2rdUK_y7uZuqgwTRJo1CZN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dccfd6c23783f2c590f73d5cef3088157ae540e6" title="Open Source GLM-4.5V model with the Cua Agent framework." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On OSWorld-V, it scores 35.8% - beating UI-TARS-1.5, matching Claude-3.7-Sonnet-20250219, and setting SOTA for fully open-source computer-use models.&lt;/p&gt; &lt;p&gt;Run it with Cua either: Locally via Hugging Face Remotely via OpenRouter&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs + examples: &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9mp1x4agpmif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mog599/open_source_glm45v_model_with_the_cua_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mog599/open_source_glm45v_model_with_the_cua_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-12T18:06:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp7ucc</id>
    <title>Does anyone what the environment variable to disable Ollama web search function is?</title>
    <updated>2025-08-13T15:39:08+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see that in the desktop version of Ollama that Web search function is on by default. I definitely don’t want this function on and am trying to locate the environment variable to turn it off. Haven’t had any luck finding it in the documentation. Does anyone know what the setting is and where to find information on it and the other new environment variables such as “airplane mode”? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mp7ucc/does_anyone_what_the_environment_variable_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mp7ucc/does_anyone_what_the_environment_variable_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mp7ucc/does_anyone_what_the_environment_variable_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T15:39:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp8o89</id>
    <title>I need help creating a promt to help me code... because now it's not working for me!</title>
    <updated>2025-08-13T16:10:02+00:00</updated>
    <author>
      <name>/u/Reivaj640</name>
      <uri>https://old.reddit.com/user/Reivaj640</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reivaj640"&gt; /u/Reivaj640 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/unsloth/comments/1mp8mqt/i_need_help_creating_a_promt_to_help_me_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mp8o89/i_need_help_creating_a_promt_to_help_me_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mp8o89/i_need_help_creating_a_promt_to_help_me_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T16:10:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp216y</id>
    <title>gptme v0.28.0 major release - agent CLI with local model support</title>
    <updated>2025-08-13T11:38:32+00:00</updated>
    <author>
      <name>/u/ErikBjare</name>
      <uri>https://old.reddit.com/user/ErikBjare</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mp216y/gptme_v0280_major_release_agent_cli_with_local/"&gt; &lt;img alt="gptme v0.28.0 major release - agent CLI with local model support" src="https://external-preview.redd.it/pd8Zgp5hyaUkgDOYatKpFNMomJKv_Ji19N5-m2ttO0s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db81836ce4675ab88cdcc7d4cac45fafb4b8f229" title="gptme v0.28.0 major release - agent CLI with local model support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ErikBjare"&gt; /u/ErikBjare &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/gptme/gptme/releases/tag/v0.28.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mp216y/gptme_v0280_major_release_agent_cli_with_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mp216y/gptme_v0280_major_release_agent_cli_with_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T11:38:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1morfl6</id>
    <title>GPT-OSS 20b runs on a RasPi 5, 16gb</title>
    <updated>2025-08-13T01:38:38+00:00</updated>
    <author>
      <name>/u/RasPiBuilder</name>
      <uri>https://old.reddit.com/user/RasPiBuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got bored and decided to see if GPT-OSS 20b would run on a RasPi 5, 16gb... And it does!&lt;/p&gt; &lt;p&gt;It's slow, hovering just under 1 token per second, so not really usable for conversation.. but could possibly work for some background tasks that aren't time sensitive. (I'll share the verbose output sometime tomorrow.. forgot to turn it on when I ran it).&lt;/p&gt; &lt;p&gt;For those curious, I'm running Ollama headless and bare metal.&lt;/p&gt; &lt;p&gt;And just for the fun of it, this weekend I'm going to set try to setup a little agent and see if I can get it to complete some tasks with Browser Use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RasPiBuilder"&gt; /u/RasPiBuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1morfl6/gptoss_20b_runs_on_a_raspi_5_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1morfl6/gptoss_20b_runs_on_a_raspi_5_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1morfl6/gptoss_20b_runs_on_a_raspi_5_16gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T01:38:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mozna4</id>
    <title>Ollama gui app in v0.11- disabling model auto-pull</title>
    <updated>2025-08-13T09:22:03+00:00</updated>
    <author>
      <name>/u/HashMismatch</name>
      <uri>https://old.reddit.com/user/HashMismatch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Having a look at the gui that v0.11 ships with, and its not bad for a lightweight gui… what bugs me is that in the model dropdown, it will auto install whatever model you select without prompting - which might be convenient for some but I want to set it to not auto-pull models and only show the ones I’ve actually chosen to download. Can’t figure out how to do this. &lt;/p&gt; &lt;p&gt;I asked gpt-oss:20b, which was the default model which self-installed when i first ran a query in the gui and it took me down a rabbithole of setting a config.yaml file - which appears to be a hallucination. You can create the file, sure, but ollama ignores it. Perplexity tells me there is no such config file and no way to configure ollama to do this -which appears to be right. Or is there a way?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HashMismatch"&gt; /u/HashMismatch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mozna4/ollama_gui_app_in_v011_disabling_model_autopull/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mozna4/ollama_gui_app_in_v011_disabling_model_autopull/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mozna4/ollama_gui_app_in_v011_disabling_model_autopull/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T09:22:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpkhih</id>
    <title>Ollama vram and sys ram</title>
    <updated>2025-08-13T23:38:39+00:00</updated>
    <author>
      <name>/u/Squanchy2112</name>
      <uri>https://old.reddit.com/user/Squanchy2112</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a Tesla p40 that means 24gb of vram, I am looking to do something about this but the system also has 80gb of system ram, can I tap into that to allow larger models? Thanks I am still learning. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Squanchy2112"&gt; /u/Squanchy2112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpkhih/ollama_vram_and_sys_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpkhih/ollama_vram_and_sys_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpkhih/ollama_vram_and_sys_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T23:38:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpmtfo</id>
    <title>CLI agentic team ecosystem</title>
    <updated>2025-08-14T01:22:07+00:00</updated>
    <author>
      <name>/u/Humbrol2</name>
      <uri>https://old.reddit.com/user/Humbrol2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking around, everyone is working on thier own version off a CLI agentic AI team similar to claude code, gemini, etc,, is there a list of the top contenders thta work with ollama anywhere?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Humbrol2"&gt; /u/Humbrol2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpmtfo/cli_agentic_team_ecosystem/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpmtfo/cli_agentic_team_ecosystem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpmtfo/cli_agentic_team_ecosystem/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T01:22:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1moy27m</id>
    <title>Finally released the major update I've been working on! LLM Checker now intelligently detects your installed Ollama models and shows you exactly what to run vs what to install</title>
    <updated>2025-08-13T07:38:43+00:00</updated>
    <author>
      <name>/u/pzarevich</name>
      <uri>https://old.reddit.com/user/pzarevich</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1moy27m/finally_released_the_major_update_ive_been/"&gt; &lt;img alt="Finally released the major update I've been working on! LLM Checker now intelligently detects your installed Ollama models and shows you exactly what to run vs what to install" src="https://external-preview.redd.it/cXd0ZzIzY3hwcWlmMT5xSciTw8xlybdYMroPRY8T-TipG3QDyDmXFYor1rLg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=315d01f51218bb9580b8cafb45ac30002d0c57ed" title="Finally released the major update I've been working on! LLM Checker now intelligently detects your installed Ollama models and shows you exactly what to run vs what to install" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt; What's New:&lt;/p&gt; &lt;p&gt; - &lt;strong&gt;--limit flag&lt;/strong&gt;: See top 3, 5, or 10 compatible models instead of just one&lt;/p&gt; &lt;p&gt; - &lt;strong&gt;Smart detection&lt;/strong&gt;: Automatically knows which models you have installed&lt;/p&gt; &lt;p&gt; - &lt;strong&gt;Intelligent Quick Start&lt;/strong&gt;: Shows ollama run for installed models, ollama pull for new ones&lt;/p&gt; &lt;p&gt; - &lt;strong&gt;7 specialized categories&lt;/strong&gt;: coding, creative, reasoning, multimodal, embeddings, talking,&lt;/p&gt; &lt;p&gt; general&lt;/p&gt; &lt;p&gt; - &lt;strong&gt;Real model data&lt;/strong&gt;: 177+ models with actual file sizes from Ollama Hub&lt;/p&gt; &lt;p&gt; - &lt;strong&gt;Hardware-aware filtering&lt;/strong&gt;: No more tiny models on high-end hardware or impossible suggestions&lt;/p&gt; &lt;p&gt;npm: &lt;a href="https://www.npmjs.com/package/llm-checker/v/2.2.0?activeTab=readme"&gt;https://www.npmjs.com/package/llm-checker/v/2.2.0?activeTab=readme&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Pavelevich/llm-checker"&gt;https://github.com/Pavelevich/llm-checker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;*please, help me with test in windows and linux machines &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pzarevich"&gt; /u/pzarevich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fmwja4cxpqif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1moy27m/finally_released_the_major_update_ive_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1moy27m/finally_released_the_major_update_ive_been/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T07:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpg9tp</id>
    <title>What are your thoughts on GPT-OSS 120B for programming?</title>
    <updated>2025-08-13T20:51:20+00:00</updated>
    <author>
      <name>/u/ajmusic15</name>
      <uri>https://old.reddit.com/user/ajmusic15</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are your thoughts on GPT-OSS 120B for programming? Specifically, how does it compare to a dense model such as Devstral or a MoE model such as Qwen-Coder 30B?&lt;/p&gt; &lt;p&gt;I am running GPT-OSS 120B on my 96 GB DDR5 + RTX 5080 with MoE weight offloading to the CPU (LM Studio does not allow me to specify how many MoE weights I will send to the CPU) and I am having mixed opinions on coding due to censorship (there are certain pentesting tools that I try to use, but I always run into ethical issues and I don't want to waste time on Advanced Prompting).&lt;/p&gt; &lt;p&gt;But anyway, I'm impressed that once the context is processed (which takes ages), the inference starts running at ~20 tk/s.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajmusic15"&gt; /u/ajmusic15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpg9tp/what_are_your_thoughts_on_gptoss_120b_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpg9tp/what_are_your_thoughts_on_gptoss_120b_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpg9tp/what_are_your_thoughts_on_gptoss_120b_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T20:51:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpt812</id>
    <title>Could you use RAG and Wikidumps to keep AI in the loop?</title>
    <updated>2025-08-14T06:53:15+00:00</updated>
    <author>
      <name>/u/C_S_Student45</name>
      <uri>https://old.reddit.com/user/C_S_Student45</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/C_S_Student45"&gt; /u/C_S_Student45 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/selfhosted/comments/1mpt7tb/could_you_use_rag_and_wikidumps_to_keep_ai_in_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpt812/could_you_use_rag_and_wikidumps_to_keep_ai_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpt812/could_you_use_rag_and_wikidumps_to_keep_ai_in_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T06:53:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpqopt</id>
    <title>Making your prompts better with GEPA-Lite using Ollama!</title>
    <updated>2025-08-14T04:31:22+00:00</updated>
    <author>
      <name>/u/AnyIce3007</name>
      <uri>https://old.reddit.com/user/AnyIce3007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link: &lt;a href="https://github.com/egmaminta/GEPA-Lite"&gt;https://github.com/egmaminta/GEPA-Lite&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;ForTheLoveOfCode&lt;/h1&gt; &lt;p&gt;GEPA-Lite is a lightweight implementation based on the proposed GEPA prompt optimization method that is custom fit for single-task applications. It's built on the core principle of LLM self-reflection, self-improvement, streamlined.&lt;/p&gt; &lt;p&gt;Developed in the spirit of open-source initiatives like Google Summer of Code 2025 and For the Love of Code 2025, this project leverages Gemma (ollama::gemma3n:e4b) as its core model. The project also offers optional support for the Gemini API, allowing access to powerful models like gemini-2.5-flash-lite, gemini-2.5-flash, and gemini-2.5-pro.&lt;/p&gt; &lt;p&gt;Feel free to check it out. I'd also appreciate if you can give a Star ⭐️!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnyIce3007"&gt; /u/AnyIce3007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpqopt/making_your_prompts_better_with_gepalite_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpqopt/making_your_prompts_better_with_gepalite_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpqopt/making_your_prompts_better_with_gepalite_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T04:31:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpmcpe</id>
    <title>AMD Radeon RX 480 8GB benchmark finally working</title>
    <updated>2025-08-14T01:01:02+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1mpm728/amd_radeon_rx_480_8gb_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpmcpe/amd_radeon_rx_480_8gb_benchmark_finally_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpmcpe/amd_radeon_rx_480_8gb_benchmark_finally_working/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T01:01:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpqf95</id>
    <title>Pruned GPT-OSS 6.0B kinda works</title>
    <updated>2025-08-14T04:17:01+00:00</updated>
    <author>
      <name>/u/Quiet-Engineer110</name>
      <uri>https://old.reddit.com/user/Quiet-Engineer110</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mpqf95/pruned_gptoss_60b_kinda_works/"&gt; &lt;img alt="Pruned GPT-OSS 6.0B kinda works" src="https://external-preview.redd.it/aaoKLInTgXWvAC3h_YKai0S41TEi4sEQ5dlZR6riJuY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a47d9c1b49d8d7aed9f6e4058ae49360afadc00f" title="Pruned GPT-OSS 6.0B kinda works" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet-Engineer110"&gt; /u/Quiet-Engineer110 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AmanPriyanshu/gpt-oss-6.0b-specialized-all-pruned-moe-only-7-experts"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpqf95/pruned_gptoss_60b_kinda_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpqf95/pruned_gptoss_60b_kinda_works/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-14T04:17:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp00lq</id>
    <title>DataKit + Ollama = Your Data, Your AI, Your Way!</title>
    <updated>2025-08-13T09:45:29+00:00</updated>
    <author>
      <name>/u/Sea-Assignment6371</name>
      <uri>https://old.reddit.com/user/Sea-Assignment6371</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mp00lq/datakit_ollama_your_data_your_ai_your_way/"&gt; &lt;img alt="DataKit + Ollama = Your Data, Your AI, Your Way!" src="https://external-preview.redd.it/MHVraTFpb2RjcmlmMau2qwmbGQWxbzW-5uoWUCNNcejArm0w5kuQ7jVz7rlm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1bc53a8330392c0bda995715e81225dfbd789b81" title="DataKit + Ollama = Your Data, Your AI, Your Way!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt; community! Excited to share that DataKit now has native Ollama integration! Run your favorite local AI models directly in your data workflows. 100% Privacy - Your data NEVER leaves your machine. Zero API Costs - No subscriptions, no surprises. No Rate Limits - Process as much as you want. Full Control - Your infrastructure, your rules.&lt;/p&gt; &lt;p&gt;Install Ollama → &lt;a href="https://ollama.com"&gt;https://ollama.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Run `OLLAMA_ORIGINS=&amp;quot;&lt;a href="https://datakit.page/"&gt;https://datakit.page&lt;/a&gt;&amp;quot; ollama serve`. Jump on Firefox.&lt;/p&gt; &lt;p&gt;Open DataKit → &lt;a href="https://datakit.page"&gt;https://datakit.page&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Start building! - SQL queries + AI, all local&lt;/p&gt; &lt;p&gt;Try it out and let me know what you think! Would love to hear about the workflows you create.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Assignment6371"&gt; /u/Sea-Assignment6371 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/whi92hodcrif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mp00lq/datakit_ollama_your_data_your_ai_your_way/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mp00lq/datakit_ollama_your_data_your_ai_your_way/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T09:45:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpi9rq</id>
    <title>I just had my first contributor to my open source AI coding agent and it feels great!</title>
    <updated>2025-08-13T22:08:51+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mpi9rq/i_just_had_my_first_contributor_to_my_open_source/"&gt; &lt;img alt="I just had my first contributor to my open source AI coding agent and it feels great!" src="https://preview.redd.it/xat5ofgh1vif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=814ecd9f096ab0c04978681b31dddbd4374c1779" title="I just had my first contributor to my open source AI coding agent and it feels great!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last week I released a rough-around-the-edges open source AI coding agent that runs in your terminal through Ollama and OpenRouter as well as any OpenAI compatible API. I published about wanting to grow it into a community and after a couple days I had my first contributor with a pull request adding some amazing features! &lt;/p&gt; &lt;p&gt;As my first proper open source project (normally I've built closed source as part of my day job), to get people taking an interest enough to star, fork and contribute is an incredible feeling, even if it is very early days!&lt;/p&gt; &lt;p&gt;This project is totally free and I want to build a community around it. I believe access to AI to help people create should be available to everyone for free and not necessarily controlled by big companies.&lt;/p&gt; &lt;p&gt;I would love your help! Whether you're interested in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Adding support for new AI providers&lt;/li&gt; &lt;li&gt;Improving tool functionality&lt;/li&gt; &lt;li&gt;Enhancing the user experience&lt;/li&gt; &lt;li&gt;Writing documentation&lt;/li&gt; &lt;li&gt;Reporting bugs or suggesting features&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All contributions are welcome! Here is the link if you're interested: &lt;a href="https://github.com/Mote-Software/nanocoder"&gt;https://github.com/Mote-Software/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But yes, this post is just me celebrating 😄&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xat5ofgh1vif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mpi9rq/i_just_had_my_first_contributor_to_my_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mpi9rq/i_just_had_my_first_contributor_to_my_open_source/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-13T22:08:51+00:00</published>
  </entry>
</feed>
