<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-25T16:40:08+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mx0lix</id>
    <title>Local AI for students</title>
    <updated>2025-08-22T08:09:40+00:00</updated>
    <author>
      <name>/u/just-rundeer</name>
      <uri>https://old.reddit.com/user/just-rundeer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I’d like to give ~20 students access to a local AI system in class.&lt;/p&gt; &lt;p&gt;The main idea: build a simple RAG (retrieval-augmented generation) so they can look up rules/answers on their own when they don’t want to ask me.&lt;/p&gt; &lt;p&gt;Would a Beelink mini PC with 32GB RAM be enough to host a small LLM (7B–13B, quantized) plus a RAG index for ~20 simultaneous users?&lt;/p&gt; &lt;p&gt;Any experiences with performance under classroom conditions? Would you recommend Beelink or a small tower PC with GPU for more scalability?&lt;/p&gt; &lt;p&gt;Perfect would be if I could create something like Study and Learn mode but that will probably need GPU power then I am willing to spend. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/just-rundeer"&gt; /u/just-rundeer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mx0lix/local_ai_for_students/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mx0lix/local_ai_for_students/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mx0lix/local_ai_for_students/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-22T08:09:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxada7</id>
    <title>How much video ram do I need to run 70b at full context?</title>
    <updated>2025-08-22T15:57:27+00:00</updated>
    <author>
      <name>/u/Suspicious-Half2593</name>
      <uri>https://old.reddit.com/user/Suspicious-Half2593</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been considering buying three 7600 xt’s so that I can use larger models, would this been enough for full context and does anyone have an estimate on on tokens per second? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suspicious-Half2593"&gt; /u/Suspicious-Half2593 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxada7/how_much_video_ram_do_i_need_to_run_70b_at_full/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxada7/how_much_video_ram_do_i_need_to_run_70b_at_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mxada7/how_much_video_ram_do_i_need_to_run_70b_at_full/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-22T15:57:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxwsr5</id>
    <title>Architecture for a Small-Scale Al Interface for MSSQL</title>
    <updated>2025-08-23T09:20:44+00:00</updated>
    <author>
      <name>/u/lokiiiiie</name>
      <uri>https://old.reddit.com/user/lokiiiiie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for some advice on the best way to add a simple AI feature to our internal application. Prompt like &amp;quot;What were our total sales last quarter?&amp;quot; All data can be get it from database, and get answers directly from our live Microsoft SQL Server database which holds financial data. &lt;/p&gt; &lt;p&gt;My plan:- ollama- openwebui -( postgres converted db)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lokiiiiie"&gt; /u/lokiiiiie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxwsr5/architecture_for_a_smallscale_al_interface_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxwsr5/architecture_for_a_smallscale_al_interface_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mxwsr5/architecture_for_a_smallscale_al_interface_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-23T09:20:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxqthv</id>
    <title>Best model for my use case (updated)</title>
    <updated>2025-08-23T03:26:32+00:00</updated>
    <author>
      <name>/u/guacgang</name>
      <uri>https://old.reddit.com/user/guacgang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a post a few days ago but I should probably give more context (no pun intended).&lt;/p&gt; &lt;p&gt;I am building an application where the model needs to make recommendations on rock climbing routes, including details about weather, difficulty, suggested gear, etc.&lt;/p&gt; &lt;p&gt;It also needs to be able to review videos that users/climbers upload and make suggestions on technique.&lt;/p&gt; &lt;p&gt;I am a broke ass college student with a MacBook (M2 chip). Originally I was using 4o-mini but I want to switch to ollama because I don't want to keep paying for API credits and also because I think in the future most companies will be using local models for cost/security reasons and I want experience using them.&lt;/p&gt; &lt;p&gt;The plan is to scrape a variety of popular climbing websites for data and then build a RAG system for the LLM to use. Keeping the size of this model as low as possible is crucial for the testing phase because running ollama 3.2 8b makes my laptop shit its pants. How much does quality degrade as model size decreases?&lt;/p&gt; &lt;p&gt;Any help is super appreciated, especially resources on building RAG pipelines&lt;/p&gt; &lt;p&gt;So far the scraper is the most annoying part, for a couple reasons:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I often find that the scraper will work perfectly for one page on a site but is total garbage for others&lt;br /&gt;&lt;/li&gt; &lt;li&gt;I need to scrape through the html but the most important website I'm scraping also has JS and other lazy loading procedures which causes me to miss data (especially hard to get ALL of the photos for a climb, not just a couple if I get any at all). Same is true for the comments under climbs, which is arguably some of the most important data since that is where climbers actively discuss conditions and access for the route.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Having a single scraper seems unreasonable, what chunking strategies do you guys suggest? Has anyone dealt with this issue before?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/guacgang"&gt; /u/guacgang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxqthv/best_model_for_my_use_case_updated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxqthv/best_model_for_my_use_case_updated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mxqthv/best_model_for_my_use_case_updated/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-23T03:26:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxzzs6</id>
    <title>Ollama Dashboard - Noob Question</title>
    <updated>2025-08-23T12:21:14+00:00</updated>
    <author>
      <name>/u/WalterKEKWh1te</name>
      <uri>https://old.reddit.com/user/WalterKEKWh1te</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So im kinda late to the party and been spending the past 2 weeks reading technical documentation and understand basics.&lt;/p&gt; &lt;p&gt;I managed to install ollama with an embed model, install postgres and pg vektor, obsidian, vs code with continue and connect all that shit. i also managed to setup open llm vtuber and whisper and make my llm more ayaya but thats besides the point. I decided to go with python as a framework and vs code and continue for coding.&lt;/p&gt; &lt;p&gt;Now thanks to Gaben the allmighty MCP got born. So i am looking for a gui frontend for my llm to implement mcp services. as far as i understand langchain and llamaindex used to be solid base. now there is crewai and many more.&lt;/p&gt; &lt;p&gt;I feel kinda lost and overwhelmed here because i dont know who supports just basic local ollama with some rag/sql and local preconfigured mcp servers. Its just for personal use.&lt;/p&gt; &lt;p&gt;And is there a thing that combines Open LLM Vtube with lets say Langchain to make an Ollama Dashboard? Control Input: Voice, Whisper, Llava, Prompt Tempering ... Control Agent: LLM, Tools via MCP or API Call ... Output Control: TTS, Avatar Control Is that a thing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WalterKEKWh1te"&gt; /u/WalterKEKWh1te &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxzzs6/ollama_dashboard_noob_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxzzs6/ollama_dashboard_noob_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mxzzs6/ollama_dashboard_noob_question/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-23T12:21:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1my1yfl</id>
    <title>One app to chat with multiple LLMs (Google, Ollama, Docker)</title>
    <updated>2025-08-23T13:48:51+00:00</updated>
    <author>
      <name>/u/Working-Magician-823</name>
      <uri>https://old.reddit.com/user/Working-Magician-823</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1my1yfl/one_app_to_chat_with_multiple_llms_google_ollama/"&gt; &lt;img alt="One app to chat with multiple LLMs (Google, Ollama, Docker)" src="https://b.thumbs.redditmedia.com/rcwy9JFCqxc-0whdvDCCdOBHn6swdWsMDKSgBZH0HrQ.jpg" title="One app to chat with multiple LLMs (Google, Ollama, Docker)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Working-Magician-823"&gt; /u/Working-Magician-823 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1my1oue/one_app_to_chat_with_multiple_llms_google_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1my1yfl/one_app_to_chat_with_multiple_llms_google_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1my1yfl/one_app_to_chat_with_multiple_llms_google_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-23T13:48:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxtpuz</id>
    <title>Mac Mini M4 32GB vs limited PC upgrade for local AI - tight budget</title>
    <updated>2025-08-23T06:08:18+00:00</updated>
    <author>
      <name>/u/Street_Trek_7754</name>
      <uri>https://old.reddit.com/user/Street_Trek_7754</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! I need your advice on a budget decision.&lt;/p&gt; &lt;p&gt;I currently have a desktop PC with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Intel i9 10th generstion&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;48 GB of RAM&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Radeon RX 7600 XT (16GB VRAM)&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm considering whether to buy a &lt;strong&gt;Mac Mini M4 with 32GB of RAM&lt;/strong&gt; or make small upgrades to my current setup. The primary use would be for &lt;strong&gt;local AI models&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The problem is that I have a &lt;strong&gt;limited budget&lt;/strong&gt; and my case is pretty much &lt;strong&gt;maxed out&lt;/strong&gt;: I can't do major hardware upgrades, at most increase the RAM.&lt;/p&gt; &lt;p&gt;My questions: 1. Can the 32GB Mac Mini M4 compete with my current setup for local AI? 2. Is it worth making the leap considering I would have less total RAM (32GB vs. 48GB)? 3. Does the Mac's unified architecture make up for the difference in RAM? 4. Has anyone made a similar switch and can share their experience?&lt;/p&gt; &lt;p&gt;Given budget and space constraints, should I stick with the PC and perhaps simply increase the RAM, or does the Mac Mini M4 offer a significant performance boost for the AI?&lt;/p&gt; &lt;p&gt;Thanks for any advice!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Street_Trek_7754"&gt; /u/Street_Trek_7754 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxtpuz/mac_mini_m4_32gb_vs_limited_pc_upgrade_for_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mxtpuz/mac_mini_m4_32gb_vs_limited_pc_upgrade_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mxtpuz/mac_mini_m4_32gb_vs_limited_pc_upgrade_for_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-23T06:08:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1myeliv</id>
    <title>Offline Dev LLM</title>
    <updated>2025-08-23T22:10:45+00:00</updated>
    <author>
      <name>/u/uvuguy</name>
      <uri>https://old.reddit.com/user/uvuguy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Long story short I want to build a local offline LLM that would specialize in docs and interpretation. Preferably one that cites. If I need to remember an obscure bash command it would do it if I need to remember certain Python or JavaScript syntax it will do it. i keep hearing Ollama and vLLM but are those the best for this use case. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uvuguy"&gt; /u/uvuguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1myeliv/offline_dev_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1myeliv/offline_dev_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1myeliv/offline_dev_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-23T22:10:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1my7010</id>
    <title>ThinkPad for Local LLM Inference - Linux Compatibility Questions</title>
    <updated>2025-08-23T17:09:02+00:00</updated>
    <author>
      <name>/u/1guyonearth</name>
      <uri>https://old.reddit.com/user/1guyonearth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking to purchase a ThinkPad (or Legion if necessary) for running local LLMs and would love some real-world experiences from the community.&lt;/p&gt; &lt;h1&gt;My Requirements:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Running Linux (prefer Fedora/Arch/openSUSE - NOT Ubuntu)&lt;/li&gt; &lt;li&gt;Local LLM inference (7B-70B parameter models)&lt;/li&gt; &lt;li&gt;Professional build quality preferred&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;My Dilemma:&lt;/h1&gt; &lt;p&gt;I'm torn between NVIDIA and AMD graphics. Historically, I've had frustrating experiences with NVIDIA proprietary drivers on Linux (driver conflicts, kernel updates breaking things, etc.), but I also know CUDA ecosystem is still dominant for LLM frameworks like llama.cpp, Ollama, and others.&lt;/p&gt; &lt;h1&gt;Specific Questions:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;For NVIDIA users (RTX 4070/4080/4090 mobile):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How has your recent experience been with NVIDIA drivers on non-Ubuntu distros?&lt;/li&gt; &lt;li&gt;Any issues with driver stability during kernel updates?&lt;/li&gt; &lt;li&gt;Which distro handles NVIDIA best in your experience?&lt;/li&gt; &lt;li&gt;Performance with popular LLM tools (Ollama, llama.cpp, etc.)?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;For AMD users (RX 7900M or similar):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How mature is ROCm support now for LLM inference?&lt;/li&gt; &lt;li&gt;Any compatibility issues with popular LLM frameworks?&lt;/li&gt; &lt;li&gt;Performance comparison vs NVIDIA if you've used both?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ThinkPad-specific:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;P1 Gen 6/7 vs Legion Pro 7i for sustained workloads?&lt;/li&gt; &lt;li&gt;Thermal performance during extended inference sessions?&lt;/li&gt; &lt;li&gt;Linux compatibility issues with either line?&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Current Considerations:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;ThinkPad P1 Gen 7 (RTX 4090 mobile) - premium price but professional build&lt;/li&gt; &lt;li&gt;Legion Pro 7i (RTX 4090 mobile) - better price/performance, gaming design&lt;/li&gt; &lt;li&gt;Any AMD alternatives worth considering?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would really appreciate hearing from anyone running LLMs locally on modern ThinkPads or Legions with Linux. What's been your actual day-to-day experience?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1guyonearth"&gt; /u/1guyonearth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1my7010/thinkpad_for_local_llm_inference_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1my7010/thinkpad_for_local_llm_inference_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1my7010/thinkpad_for_local_llm_inference_linux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-23T17:09:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1my3k3t</id>
    <title>AMD 395 128GB ram VS Apple Mac Air 10-core 32GB ram</title>
    <updated>2025-08-23T14:54:54+00:00</updated>
    <author>
      <name>/u/quantrpeter</name>
      <uri>https://old.reddit.com/user/quantrpeter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi&lt;br /&gt; If running local model such as codellama, AMD 395 128GB ram VS Apple Mac Air 10-core 32GB ram, AMD sure win, right? &lt;/p&gt; &lt;p&gt;My long duration of use is in library. Can AMD maintains 4-5 hours usage of vscode/netbeans after 2 years use?&lt;/p&gt; &lt;p&gt;thanks&lt;br /&gt; Peter&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantrpeter"&gt; /u/quantrpeter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1my3k3t/amd_395_128gb_ram_vs_apple_mac_air_10core_32gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1my3k3t/amd_395_128gb_ram_vs_apple_mac_air_10core_32gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1my3k3t/amd_395_128gb_ram_vs_apple_mac_air_10core_32gb_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-23T14:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1myd6al</id>
    <title>I wrote a guide on Layered Reward Architecture (LRA) to fix the "single-reward fallacy" in production RLHF/RLVR.</title>
    <updated>2025-08-23T21:11:17+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1myd6al/i_wrote_a_guide_on_layered_reward_architecture/"&gt; &lt;img alt="I wrote a guide on Layered Reward Architecture (LRA) to fix the &amp;quot;single-reward fallacy&amp;quot; in production RLHF/RLVR." src="https://preview.redd.it/5rao3fyc4ukf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a36db47e73b28fa5323e305e5054373dab3759a4" title="I wrote a guide on Layered Reward Architecture (LRA) to fix the &amp;quot;single-reward fallacy&amp;quot; in production RLHF/RLVR." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share a framework for making RLHF more robust, especially for complex systems that chain LLMs, RAG, and tools.&lt;/p&gt; &lt;p&gt;We all know a single scalar reward is brittle. It gets gamed, starves components (like the retriever), and is a nightmare to debug. I call this the &amp;quot;single-reward fallacy.&amp;quot;&lt;/p&gt; &lt;p&gt;My post details the &lt;strong&gt;Layered Reward Architecture (LRA)&lt;/strong&gt;, which decomposes the reward into a vector of verifiable signals from specialized models and rules. The core idea is to fail fast and reward granularly.&lt;/p&gt; &lt;p&gt;The layers I propose are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Structural:&lt;/strong&gt; Is the output format (JSON, code syntax) correct?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Task-Specific:&lt;/strong&gt; Does it pass unit tests or match a ground truth?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semantic:&lt;/strong&gt; Is it factually grounded in the provided context?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Behavioral/Safety:&lt;/strong&gt; Does it pass safety filters?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qualitative:&lt;/strong&gt; Is it helpful and well-written? (The final, expensive check)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the guide, I cover the architecture, different methods for weighting the layers (including regressing against human labels), and provide code examples for Best-of-N reranking and PPO integration.&lt;/p&gt; &lt;p&gt;Would love to hear how you all are approaching this problem. Are you using multi-objective rewards? How are you handling credit assignment in chained systems?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full guide here:&lt;/strong&gt;&lt;a href="https://pavankunchalapk.medium.com/the-layered-reward-architecture-lra-a-complete-guide-to-multi-layer-multi-model-reward-631405e1c1af"&gt;The Layered Reward Architecture (LRA): A Complete Guide to Multi-Layer, Multi-Model Reward Mechanisms | by Pavan Kunchala | Aug, 2025 | Medium&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Single rewards in RLHF are broken for complex systems. I wrote a guide on using a multi-layered reward system (LRA) with different verifiers for syntax, facts, safety, etc., to make training more stable and debuggable.&lt;/p&gt; &lt;p&gt;&lt;em&gt;P.S. I'm currently looking for my next role in the LLM / Computer Vision space and would love to connect about any opportunities&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Portfolio:&lt;/em&gt; &lt;a href="https://pavan-portfolio-tawny.vercel.app/"&gt;Pavan Kunchala - AI Engineer &amp;amp; Full-Stack Developer&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5rao3fyc4ukf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1myd6al/i_wrote_a_guide_on_layered_reward_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1myd6al/i_wrote_a_guide_on_layered_reward_architecture/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-23T21:11:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1my91m1</id>
    <title>Is there a Ollama GUI app for Linux like there is for macOS and Windows?</title>
    <updated>2025-08-23T18:26:45+00:00</updated>
    <author>
      <name>/u/gianndev_</name>
      <uri>https://old.reddit.com/user/gianndev_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean a single executable that works on Linux (i've read there is already something similar for macOS and windows), not something like OpenwebUI. I'd like to have a bettere UX that the terminal one.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gianndev_"&gt; /u/gianndev_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1my91m1/is_there_a_ollama_gui_app_for_linux_like_there_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1my91m1/is_there_a_ollama_gui_app_for_linux_like_there_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1my91m1/is_there_a_ollama_gui_app_for_linux_like_there_is/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-23T18:26:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mycbtn</id>
    <title>Anyone know if Ollama will implement support for --cpu-moe ?</title>
    <updated>2025-08-23T20:36:50+00:00</updated>
    <author>
      <name>/u/The_Councillor</name>
      <uri>https://old.reddit.com/user/The_Councillor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As per:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The_Councillor"&gt; /u/The_Councillor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mycbtn/anyone_know_if_ollama_will_implement_support_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mycbtn/anyone_know_if_ollama_will_implement_support_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mycbtn/anyone_know_if_ollama_will_implement_support_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-23T20:36:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mygicl</id>
    <title>Model recommendation for homelab use</title>
    <updated>2025-08-23T23:33:59+00:00</updated>
    <author>
      <name>/u/Zageyiff</name>
      <uri>https://old.reddit.com/user/Zageyiff</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What local LLM model would you recommend me. My uses case would be:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Karakeep: tagging and summarization of bookmarks,&lt;/li&gt; &lt;li&gt;Frigate: generate descriptive text based on the thumbnails of your tracked objects.&lt;/li&gt; &lt;li&gt;Home Assistant: ollama integration&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In that order of priority&lt;/p&gt; &lt;p&gt;My current setup runs on Proxmox, running VMs and a few LXCs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ASRock X570 Phantom Gaming 4&lt;/li&gt; &lt;li&gt;Ryzen 5700G (3% cpu usage, ~0.6 load)&lt;/li&gt; &lt;li&gt;64GB RAM (using ~40GB), I could upgrade up to 128GB if needed&lt;/li&gt; &lt;li&gt;1TB NVME (30% used) for OS, LXCs, and VMs&lt;/li&gt; &lt;li&gt;HDD RAID 28TB (4TB + 12TB + 12TB), used 13TB, free 14TB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I see &lt;a href="https://github.com/ROCm/ROCm/issues/2774"&gt;ROCm could support the dGPU in the Ryzen 5700G&lt;/a&gt;, which could help with local LLMs I'm passing through the discrete GPU to a VM, where it's used for other tasks like jellyfin transcoding (very occasionally)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zageyiff"&gt; /u/Zageyiff &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mygicl/model_recommendation_for_homelab_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mygicl/model_recommendation_for_homelab_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mygicl/model_recommendation_for_homelab_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-23T23:33:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1myj3bs</id>
    <title>ollama + webui + iis reverse proxy</title>
    <updated>2025-08-24T01:40:43+00:00</updated>
    <author>
      <name>/u/dangit541</name>
      <uri>https://old.reddit.com/user/dangit541</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;br /&gt; I have it running locally no problem, but it seems WebUI is ignoring my ollama connection and uses localhost&lt;br /&gt; &lt;a href="http://localhost:11434/api/version"&gt;http://localhost:11434/api/version&lt;/a&gt;&lt;/p&gt; &lt;p&gt;my settings:&lt;br /&gt; Docker with &lt;a href="http://ghcr.io/open-webui/open-webui:main"&gt;ghcr.io/open-webui/open-webui:main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;tried multiple settings in iis. redirections are working and if i just put https://mine_web_adress/ollama/ i have response that is running. WebUI is loading but chats not produce output and &amp;quot;connection&amp;quot; settings in admin panel not loading.&lt;/p&gt; &lt;p&gt;chat error: Unexpected token 'd', &amp;quot;data: {&amp;quot;id&amp;quot;... is not valid JSON&lt;/p&gt; &lt;p&gt;i even used nginx with same results. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dangit541"&gt; /u/dangit541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1myj3bs/ollama_webui_iis_reverse_proxy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1myj3bs/ollama_webui_iis_reverse_proxy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1myj3bs/ollama_webui_iis_reverse_proxy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-24T01:40:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mymurl</id>
    <title>How can I run models in a good frontend interface</title>
    <updated>2025-08-24T05:03:49+00:00</updated>
    <author>
      <name>/u/Straight-Mark4321</name>
      <uri>https://old.reddit.com/user/Straight-Mark4321</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Straight-Mark4321"&gt; /u/Straight-Mark4321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mymurl/how_can_i_run_models_in_a_good_frontend_interface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mymurl/how_can_i_run_models_in_a_good_frontend_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mymurl/how_can_i_run_models_in_a_good_frontend_interface/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-24T05:03:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mze1uv</id>
    <title>Oumnix: A New AI Architecture (non-Transformer architecture)</title>
    <updated>2025-08-25T01:54:03+00:00</updated>
    <author>
      <name>/u/oumnix</name>
      <uri>https://old.reddit.com/user/oumnix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m not here to sell, beg, or hype.&lt;br /&gt; This is &lt;strong&gt;not a Transformer architecture&lt;/strong&gt; it’s a different path.&lt;br /&gt; Minimal version, trained from zero (no fine-tuning) on a laptop GPU (RTX 4060).&lt;/p&gt; &lt;p&gt;Result: 50M parameters trained from scratch, loss → &lt;strong&gt;8.5 → 0.9 in 13 minutes&lt;/strong&gt;.&lt;br /&gt; Video: &lt;a href="https://www.youtube.com/watch?v=pOzOnSE1IAY"&gt;YouTube&lt;/a&gt;&lt;br /&gt; Repo: &lt;a href="https://github.com/qrv0/oumnix-minimal"&gt;oumnix-minimal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;No papers. No replicas. Just an alternative architecture that exists outside the Transformer highway.&lt;/p&gt; &lt;p&gt;I expect downvotes, noise, and accusations that’s fine.&lt;br /&gt; But facts don’t vanish: &lt;strong&gt;other architectures are possible.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oumnix"&gt; /u/oumnix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mze1uv/oumnix_a_new_ai_architecture_nontransformer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mze1uv/oumnix_a_new_ai_architecture_nontransformer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mze1uv/oumnix_a_new_ai_architecture_nontransformer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-25T01:54:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1myvwo7</id>
    <title>Mini M4 chaining</title>
    <updated>2025-08-24T13:41:42+00:00</updated>
    <author>
      <name>/u/le-greffier</name>
      <uri>https://old.reddit.com/user/le-greffier</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/le-greffier"&gt; /u/le-greffier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/macmini/comments/1myvwau/mini_m4_chaining/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1myvwo7/mini_m4_chaining/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1myvwo7/mini_m4_chaining/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-24T13:41:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1myjt8y</id>
    <title>Built an easy way to chat with Ollama + MCP servers via Telegram (open source + free)</title>
    <updated>2025-08-24T02:17:21+00:00</updated>
    <author>
      <name>/u/WalrusVegetable4506</name>
      <uri>https://old.reddit.com/user/WalrusVegetable4506</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1myjt8y/built_an_easy_way_to_chat_with_ollama_mcp_servers/"&gt; &lt;img alt="Built an easy way to chat with Ollama + MCP servers via Telegram (open source + free)" src="https://external-preview.redd.it/bGoycHIzNTNpdmtmMRJKmnbj-HqhmebMror90RICm6HPnLt2_JmUpeF6YyzA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=23904c8af0a3e2673e4096913ccadbf3bda4a073" title="Built an easy way to chat with Ollama + MCP servers via Telegram (open source + free)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi y'all! I've been working on Tome with &lt;a href="/u/TomeHanks"&gt;u/TomeHanks&lt;/a&gt; and &lt;a href="/u/_march"&gt;u/_march&lt;/a&gt; (an open source LLM+MCP desktop client for MacOS and Windows) and we just shipped a new feature that lets you chat with models on the go using Telegram.&lt;/p&gt; &lt;p&gt;Basically you can set up a Telegram bot, connect it to the Tome desktop app, and then you can send and receive messages from anywhere via Telegram. The video above shows off MCPs for iTerm (controlling the terminal), scryfall (a Magic the Gathering API) and Playwright (controlling a web browser), you can use any LLM via Ollama or API, and any MCP server, and do lots of weird and fun things.&lt;/p&gt; &lt;p&gt;For more details on how to get started I wrote a blog post here: &lt;a href="https://blog.runebook.ai/tome-relays-chat-with-llms-mcp-via-telegram"&gt;https://blog.runebook.ai/tome-relays-chat-with-llms-mcp-via-telegram&lt;/a&gt; It's pretty simple, you can probably get it going in 10 minutes.&lt;/p&gt; &lt;p&gt;Here's our GitHub repo: &lt;a href="https://github.com/runebookai/tome"&gt;https://github.com/runebookai/tome&lt;/a&gt; so you can see the source code and download the latest release. Let me know if you have any questions, thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WalrusVegetable4506"&gt; /u/WalrusVegetable4506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4l95on43ivkf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1myjt8y/built_an_easy_way_to_chat_with_ollama_mcp_servers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1myjt8y/built_an_easy_way_to_chat_with_ollama_mcp_servers/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-24T02:17:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzlmkb</id>
    <title>Dude about VRAM, RAM and PCIe Bandwidth</title>
    <updated>2025-08-25T09:15:53+00:00</updated>
    <author>
      <name>/u/ajmusic15</name>
      <uri>https://old.reddit.com/user/ajmusic15</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why do I get the impression that running a model at 100% on the CPU depending on which model and its size is faster than running them on GPU with Offload? And it is especially strange since it is a PCIe 5.0 x16 very close to the processor (about 5cm from the processor.).&lt;/p&gt; &lt;p&gt;This is a system with Ryzen 9 7945HX (MoDT) + 96 GB DDR5 in Dual Channel + RTX 5080 (Not enough for me to sell it and give difference for a 5090).&lt;/p&gt; &lt;p&gt;Does anyone have any idea of the possible reason?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajmusic15"&gt; /u/ajmusic15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mzlmkb/dude_about_vram_ram_and_pcie_bandwidth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mzlmkb/dude_about_vram_ram_and_pcie_bandwidth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mzlmkb/dude_about_vram_ram_and_pcie_bandwidth/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-25T09:15:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzn0dc</id>
    <title>Is there a way to test how will a fully upgraded Mac mini will do and what it can run? (M4 pro, 14 core CPU, 20 core GPU, 64ram, with 5tb external storage)</title>
    <updated>2025-08-25T10:39:58+00:00</updated>
    <author>
      <name>/u/not-bilbo-baggings</name>
      <uri>https://old.reddit.com/user/not-bilbo-baggings</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/not-bilbo-baggings"&gt; /u/not-bilbo-baggings &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mzn0dc/is_there_a_way_to_test_how_will_a_fully_upgraded/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mzn0dc/is_there_a_way_to_test_how_will_a_fully_upgraded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mzn0dc/is_there_a_way_to_test_how_will_a_fully_upgraded/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-25T10:39:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mza1np</id>
    <title>Is it worth upgrading RAM from 64Gb to 128Gb?</title>
    <updated>2025-08-24T22:50:12+00:00</updated>
    <author>
      <name>/u/NervousMood8071</name>
      <uri>https://old.reddit.com/user/NervousMood8071</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ask this because I want to run Ollama on my Linux box at home. I only have an RTX-4060 Ti with 16Gb of VRAM snd the upgrade to the RAM is much cheaper than upgrading to a GPU with 24Gb.&lt;/p&gt; &lt;p&gt;What Ollama models/sizes are best suited for these options:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;16gb Vram + 64Gb Ram&lt;/li&gt; &lt;li&gt;16gb Vram + 128Gb Ram&lt;/li&gt; &lt;li&gt;24Gb Vram + 64Gb Ram&lt;/li&gt; &lt;li&gt; 24Gb Vram + 128Gb Ram&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I'm asking as I want to understand the Ram/Vram usage with Ollama and the optimal upgrades to my rig. Oh it is a I9 12900K with DDR5 if that helps.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NervousMood8071"&gt; /u/NervousMood8071 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mza1np/is_it_worth_upgrading_ram_from_64gb_to_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mza1np/is_it_worth_upgrading_ram_from_64gb_to_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mza1np/is_it_worth_upgrading_ram_from_64gb_to_128gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-24T22:50:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzsdh9</id>
    <title>Making an RAG embedding Redis with Ollama</title>
    <updated>2025-08-25T14:43:13+00:00</updated>
    <author>
      <name>/u/jasonhon2013</name>
      <uri>https://old.reddit.com/user/jasonhon2013</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found that many people actually need to use RAG. Many applications, like web search apps or SWE agents, require RAG. There are lots of vector databases like DuckDB and many other options such as faiss file. However, none of them really offer caching solutions (something like Redis).&lt;/p&gt; &lt;p&gt;So, I decided to build one using Ollama embeddings yeah cuz I really love the Ollama community, For now, it only supports Ollama embeddings, loll(the reason must not because I am so lazy lolll).&lt;/p&gt; &lt;p&gt;But, like with my previous projects, I’m looking for ideas and guidance from you all (of course, I appreciate your support!). Would you mind taking a little time to share your thoughts and ideas? The project is still very far from finished, but I want to see if this idea is valid.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/JasonHonKL/PardusDB"&gt;https://github.com/JasonHonKL/PardusDB&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jasonhon2013"&gt; /u/jasonhon2013 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mzsdh9/making_an_rag_embedding_redis_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mzsdh9/making_an_rag_embedding_redis_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mzsdh9/making_an_rag_embedding_redis_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-25T14:43:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzuy5y</id>
    <title>Computer-Use Agents SOTA Challenge @ Hack the North (YC interview for top team) + Global Online ($2000 prize)</title>
    <updated>2025-08-25T16:17:41+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mzuy5y/computeruse_agents_sota_challenge_hack_the_north/"&gt; &lt;img alt="Computer-Use Agents SOTA Challenge @ Hack the North (YC interview for top team) + Global Online ($2000 prize)" src="https://preview.redd.it/52u8vi3vx6lf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cd13c45bd9eb470a70bbdff166f34c15707854da" title="Computer-Use Agents SOTA Challenge @ Hack the North (YC interview for top team) + Global Online ($2000 prize)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re bringing something new to Hack the North, Canada’s largest hackathon, this year: a head-to-head competition for Computer-Use Agents - on-site at Waterloo and a Global online challenge. From September 12–14, 2025, teams build on the Cua Agent Framework and are scored in HUD’s OSWorld-Verified environment to push past today’s SOTA on OS-World.&lt;/p&gt; &lt;p&gt;On-site (Track A) Build during the weekend and submit a repo with a one-line start command. HUD executes your command in a clean environment and runs OSWorld-Verified. Scores come from official benchmark results; ties break by median, then wall-clock time, then earliest submission. Any model setup is allowed (cloud or local). Provide temporary credentials if needed.&lt;/p&gt; &lt;p&gt;HUD runs official evaluations immediately after submission. Winners are announced at the closing ceremony.&lt;/p&gt; &lt;p&gt;Deadline: Sept 15, 8:00 AM EDT&lt;/p&gt; &lt;p&gt;Global Online (Track B) Open to anyone, anywhere. Build on your own timeline and submit a repo using Cua + Ollama/Ollama Cloud with a short write-up (what's local or hybrid about your design). Judged by Cua and Ollama teams on: Creativity (30%), Technical depth (30%), Use of Ollama/Cloud (30%), Polish (10%). A ≤2-min demo video helps but isn't required.&lt;/p&gt; &lt;p&gt;Winners announced after judging is complete.&lt;/p&gt; &lt;p&gt;Deadline: Sept 22, 8:00 AM EDT (1 week after Hack the North)&lt;/p&gt; &lt;p&gt;Submission &amp;amp; rules (both tracks) Deadlines: Sept 15, 8:00 AM EDT (Track A) / Sept 22, 8:00 AM EDT (Track B) Deliverables: repo + README start command; optional short demo video; brief model/tool notes Where to submit: links shared in the Hack the North portal and Discord Commit freeze: we evaluate the submitted SHA Rules: no human-in-the-loop after the start command; internet/model access allowed if declared; use temporary/test credentials; you keep your IP; by submitting, you allow benchmarking and publication of scores/short summaries.&lt;/p&gt; &lt;p&gt;Join us, bring a team, pick a model stack, and push what agents can do on real computers. We can’t wait to see what you build at Hack the North 2025.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Join the Discord here: &lt;a href="https://discord.gg/YuUavJ5F3J"&gt;https://discord.gg/YuUavJ5F3J&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog : &lt;a href="https://www.trycua.com/blog/cua-hackathon"&gt;https://www.trycua.com/blog/cua-hackathon&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/52u8vi3vx6lf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mzuy5y/computeruse_agents_sota_challenge_hack_the_north/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mzuy5y/computeruse_agents_sota_challenge_hack_the_north/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-25T16:17:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzl90r</id>
    <title>I built Husk, a native, private, and open-source iOS client for your local models</title>
    <updated>2025-08-25T08:51:47+00:00</updated>
    <author>
      <name>/u/nathan12581</name>
      <uri>https://old.reddit.com/user/nathan12581</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Ollama a lot and wanted a really clean, polished, and native way to interact with my privately hosted models on my iPhone. While there are some great options out there, I wanted something that felt like a first-party Apple app—fast, private, and simple.&lt;/p&gt; &lt;p&gt;Husk is an open-source, Ollama-compatible app for iOS. The whole idea is to provide a beautiful and seamless experience for chatting with your models without your data ever leaving your control.&lt;/p&gt; &lt;h1&gt;Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fully Offline &amp;amp; Private:&lt;/strong&gt; It's a native Ollama client. Your conversations stay on your devices.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optional iCloud Sync:&lt;/strong&gt; If you want, you can sync your chat history across your devices using Apple's end-to-end encryption (macOS support coming soon!).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Attachments:&lt;/strong&gt; You can attach text-based files to your chats (image support for multimodal models is on the roadmap!).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Highly Customisable:&lt;/strong&gt; You can set custom names, system prompts, and other parameters for your models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open Source:&lt;/strong&gt; The entire project is open-source under the MIT license.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To help support me, I've put Husk on the App Store with a small fee. If you buy it, thank you so much! It directly funds continued development.&lt;/p&gt; &lt;p&gt;However, since it's fully open-source, you are more than welcome to build and install yourself from the GitHub repo. The instructions are all in the README.&lt;/p&gt; &lt;p&gt;I'm also planning to add macOS support and integrations for other model providers soon.&lt;/p&gt; &lt;p&gt;I'd love to hear what you all think! Any feedback, feature requests, or bug reports are super welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; I made a native, private, open-source iOS app for Ollama. It's a paid app on the App Store to support development, but you can also build it yourself for free from the Github Repo&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nathan12581"&gt; /u/nathan12581 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mzl90r/i_built_husk_a_native_private_and_opensource_ios/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mzl90r/i_built_husk_a_native_private_and_opensource_ios/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mzl90r/i_built_husk_a_native_private_and_opensource_ios/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-25T08:51:47+00:00</published>
  </entry>
</feed>
