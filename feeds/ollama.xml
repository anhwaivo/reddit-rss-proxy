<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-17T16:49:02+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1iqpbpb</id>
    <title>Best local vision model for technical drawings?</title>
    <updated>2025-02-16T10:17:21+00:00</updated>
    <author>
      <name>/u/Mundane_Maximum5795</name>
      <uri>https://old.reddit.com/user/Mundane_Maximum5795</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I think the title says it all, but maybe some context. I work for a small industrial company and we deal with technical drawings on a daily basis. One of our problems is that due to our small size we often lack the time to do some checks on customer and internal drawings before they go in production. I have played with Chatgpt and reading technical drawings and have been blown away with the quality of the analysis, but these were for completely fake drawings to ensure privacy. I have looked at different local llms to replace this, but none come even remotely close to what I need, frequently hallucinating answers. Anybody have a great model/prompt combo that works? Needs to be completely local for infosec reasons...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mundane_Maximum5795"&gt; /u/Mundane_Maximum5795 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqpbpb/best_local_vision_model_for_technical_drawings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqpbpb/best_local_vision_model_for_technical_drawings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqpbpb/best_local_vision_model_for_technical_drawings/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T10:17:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1iquig5</id>
    <title>Llama dataset training.</title>
    <updated>2025-02-16T15:24:47+00:00</updated>
    <author>
      <name>/u/GMP_Test123</name>
      <uri>https://old.reddit.com/user/GMP_Test123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, Good eve. Am new to Ollama and looking forward for resources which can assist me in training datasets to Ollama. Thanks in advance ðŸ™‚.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GMP_Test123"&gt; /u/GMP_Test123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iquig5/llama_dataset_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iquig5/llama_dataset_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iquig5/llama_dataset_training/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T15:24:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqtatc</id>
    <title>Best program/ui for writing stories?</title>
    <updated>2025-02-16T14:26:50+00:00</updated>
    <author>
      <name>/u/Mystic-Dragoness</name>
      <uri>https://old.reddit.com/user/Mystic-Dragoness</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like everyone is focused on using LLMs in a chat format, but what I've really been looking for is a program or UI for Ollama that's almost closer to Word, allowing me to write something out and have Ollama continue from where I left off. Back before ChatGPT blew up, I used to use a site called &lt;a href="https://inferkit.com/"&gt;https://inferkit.com/&lt;/a&gt; to do this very thing to help me write stories, mainly for when I got stuck and needed some ideas. Now inferkit is dead and I've yet to be able to find a good alternative. So, anyone know of any like that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mystic-Dragoness"&gt; /u/Mystic-Dragoness &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqtatc/best_programui_for_writing_stories/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqtatc/best_programui_for_writing_stories/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqtatc/best_programui_for_writing_stories/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T14:26:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir8hdf</id>
    <title>Ollama in the cloud</title>
    <updated>2025-02-17T01:40:13+00:00</updated>
    <author>
      <name>/u/TarunRaviYT</name>
      <uri>https://old.reddit.com/user/TarunRaviYT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve spent a lot of time just trying to run a default Ollama model (LLaVA) on Cloud Run, and setting up GPU access was a hassle for such a basic task.&lt;/p&gt; &lt;p&gt;Would people find value in a simple tool where you just specify the model, parameter size, and an initial prompt, and my app handles the setupâ€”providing a REST API for user inputs? Most other apps require extensive configuration, but this would be much simpler (though less customizable).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TarunRaviYT"&gt; /u/TarunRaviYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir8hdf/ollama_in_the_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir8hdf/ollama_in_the_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ir8hdf/ollama_in_the_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T01:40:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir4f9b</id>
    <title>Ollama cloud service</title>
    <updated>2025-02-16T22:25:08+00:00</updated>
    <author>
      <name>/u/TarunRaviYT</name>
      <uri>https://old.reddit.com/user/TarunRaviYT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im using the LLAVA ollama model, with a custom prompt. I want a service that is running ollama + llava with my prompt and can take in an input string and return an output through REST api. I can run this locally no problem, but are there services that can run this in the cloud?&lt;/p&gt; &lt;p&gt;Ik I could use GCP and setup a cloudrun container, but are there other solutions specifically for this? It seems pretty useful if it exists.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TarunRaviYT"&gt; /u/TarunRaviYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir4f9b/ollama_cloud_service/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir4f9b/ollama_cloud_service/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ir4f9b/ollama_cloud_service/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T22:25:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir4jwh</id>
    <title>Ollama Model Files Location when run in docker</title>
    <updated>2025-02-16T22:30:57+00:00</updated>
    <author>
      <name>/u/maximo101</name>
      <uri>https://old.reddit.com/user/maximo101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running Ollama as a docker on Unraid OS. I am using both OpenWebUI and Home Assistant to bring the models in.&lt;/p&gt; &lt;p&gt;Where exactly do the AI model files get stored too? I cant seem to find them in /cache/appdata/ollama is empty.&lt;/p&gt; &lt;p&gt;It seems that they fill up my docker vdisk allocation space when i download them, and i cant see them anywhere in the File Manager. Using some terminal commands they seem to be in blops in the container itself.&lt;/p&gt; &lt;p&gt;I manually tried to set 'OLLAMA_MODELS' as a container variable to the /cache/AI-Models but when i view this in file manager its empty.&lt;/p&gt; &lt;p&gt;I want a location on the cache so its faster than the array, but want it to be persistent across reboots and not take up my docker vdisk allocation etc.&lt;/p&gt; &lt;p&gt;Also does this work for making it have memory? Add variable: MEMORY_LIMIT = 8g, Persistent Storage = /mnt/user/appdata/ollama to keep previous conversations?&lt;/p&gt; &lt;p&gt;thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maximo101"&gt; /u/maximo101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir4jwh/ollama_model_files_location_when_run_in_docker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir4jwh/ollama_model_files_location_when_run_in_docker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ir4jwh/ollama_model_files_location_when_run_in_docker/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T22:30:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir5qsh</id>
    <title>DeepSeek-R1-Q_2 + LLamaCPP + 8x AMD Instinct Mi60 Server</title>
    <updated>2025-02-16T23:25:01+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ggz6gwon4lje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir5qsh/deepseekr1q_2_llamacpp_8x_amd_instinct_mi60_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ir5qsh/deepseekr1q_2_llamacpp_8x_amd_instinct_mi60_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T23:25:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ircqzg</id>
    <title>Managed to Secure my Ollama/Whisper Ubuntu Server</title>
    <updated>2025-02-17T05:38:35+00:00</updated>
    <author>
      <name>/u/ManyInteresting3969</name>
      <uri>https://old.reddit.com/user/ManyInteresting3969</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ManyInteresting3969"&gt; /u/ManyInteresting3969 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/u_ManyInteresting3969/comments/1ircqeg/managed_to_secure_my_ollamawhisper_ubuntu_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ircqzg/managed_to_secure_my_ollamawhisper_ubuntu_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ircqzg/managed_to_secure_my_ollamawhisper_ubuntu_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T05:38:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ireu17</id>
    <title>Any body got the chance to run Ollama with Deepseek R1 671B version?</title>
    <updated>2025-02-17T07:58:54+00:00</updated>
    <author>
      <name>/u/ChemicalExcellent463</name>
      <uri>https://old.reddit.com/user/ChemicalExcellent463</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;Average stats: (Running on dual 3090 Ti GPU, Epyc 7763 CPU in Ubuntu 22.04) ---------------------------------------------------- Model: deepseek-r1:70b Performance Metrics: Prompt Processing: 336.73 tokens/sec Generation Speed: 17.65 tokens/sec Combined Speed: 18.01 tokens/sec Workload Stats: Input Tokens: 165 Generated Tokens: 7673 Model Load Time: 6.11s Processing Time: 0.49s Generation Time: 434.70s Total Time: 441.31s ---------------------------------------------------- Average stats: (Running on single 3090 GPU, 13900KS CPU in WSL2(Ubuntu 22.04) in Windows 11) ---------------------------------------------------- Model: deepseek-r1:32b Performance Metrics: Prompt Processing: 399.05 tokens/sec Generation Speed: 27.18 tokens/sec Combined Speed: 27.58 tokens/sec Workload Stats: Input Tokens: 168 Generated Tokens: 10601 Model Load Time: 15.44s Processing Time: 0.42s Generation Time: 390.00s Total Time: 405.87s ---------------------------------------------------- &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Github code: &lt;a href="https://github.com/larryhopecode/ollama-benchmark"&gt;larryhopecode/ollama-benchmark: Ollama based Benchmark with detail I/O token per second. Python with Deepseek R1 example.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChemicalExcellent463"&gt; /u/ChemicalExcellent463 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ireu17/any_body_got_the_chance_to_run_ollama_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ireu17/any_body_got_the_chance_to_run_ollama_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ireu17/any_body_got_the_chance_to_run_ollama_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T07:58:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir72jb</id>
    <title>Funny AI response</title>
    <updated>2025-02-17T00:29:20+00:00</updated>
    <author>
      <name>/u/ZealousidealBee8299</name>
      <uri>https://old.reddit.com/user/ZealousidealBee8299</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am testing out models for tool calling, and this one model just never worked right to trigger on country. So as the input to the query I finally used:&lt;/p&gt; &lt;p&gt;String simulatedInput = &amp;quot;Do you have a tool that shows what country i am in? if not what were you expecting the tool to be called so i can tell the programmer.&amp;quot;;&lt;/p&gt; &lt;p&gt;And it responded:&lt;/p&gt; &lt;p&gt;It looks like I've got it! The tool is called \getCurrentCountry` and it returns a string indicating the country you are currently in. In this case, the output indicates that you are in the United States (US). If you'd like to know more about your location or would like to search for specific countries, feel free to ask!`&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZealousidealBee8299"&gt; /u/ZealousidealBee8299 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir72jb/funny_ai_response/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir72jb/funny_ai_response/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ir72jb/funny_ai_response/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T00:29:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqqq0e</id>
    <title>ollama-remote: Make local ollama run models on remote server (colab, kaggle, ...)</title>
    <updated>2025-02-16T11:58:12+00:00</updated>
    <author>
      <name>/u/amitness</name>
      <uri>https://old.reddit.com/user/amitness</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote a package for the gpu-poor/mac-poor to run ollama models via remote servers (colab, kaggle, paid inference etc.)&lt;/p&gt; &lt;p&gt;Just 2 lines and the local ollama cli can access all models which actually run on the server-side GPU/CPU:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install ollama-remote ollama-remote &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I wrote it to speed up prompt engineering and synthetic data generation for a personal project which ran too slowly with local models on my mac. Once the results are good, we switch back to running locally.&lt;/p&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;The tool downloads and sets up ollama on the server side and exposes a port&lt;/li&gt; &lt;li&gt;Cloudflare tunnel is automatically downloaded and setup to expose ollama's port to a random domain&lt;/li&gt; &lt;li&gt;We parse the domain and then provide code for setting&lt;code&gt;OLLAMA_HOST&lt;/code&gt; as well as usage in OpenAI SDK for local use.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Source code: &lt;a href="https://github.com/amitness/ollama-remote"&gt;https://github.com/amitness/ollama-remote&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amitness"&gt; /u/amitness &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqqq0e/ollamaremote_make_local_ollama_run_models_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqqq0e/ollamaremote_make_local_ollama_run_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqqq0e/ollamaremote_make_local_ollama_run_models_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T11:58:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir69xk</id>
    <title>seeking a fast local LLM solution using only CPU</title>
    <updated>2025-02-16T23:50:34+00:00</updated>
    <author>
      <name>/u/Orleans007</name>
      <uri>https://old.reddit.com/user/Orleans007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I have a development server connected via SSH with the following specs: 64GB RAM, 16 CPU cores, no GPU, no TPU.&lt;/p&gt; &lt;p&gt;Iâ€™m looking for a way to connect an open source model locally to my Langchain application.&lt;/p&gt; &lt;p&gt;I don't want to use cloud hosted inference endpoints, i have tried using Llamafile/Ollama with 3/4 bit quantized models but the response times are extremely slow, especially when integrating a SQL or Pandas agent.&lt;/p&gt; &lt;p&gt;I'm seeking an open source, local solution that avoids latency and slow responses.&lt;/p&gt; &lt;p&gt;is it possible?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Orleans007"&gt; /u/Orleans007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir69xk/seeking_a_fast_local_llm_solution_using_only_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir69xk/seeking_a_fast_local_llm_solution_using_only_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ir69xk/seeking_a_fast_local_llm_solution_using_only_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T23:50:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1irg7ox</id>
    <title>I made a simple python library to create a bridge between real and simulated python interpreters</title>
    <updated>2025-02-17T09:38:37+00:00</updated>
    <author>
      <name>/u/rb9_3b</name>
      <uri>https://old.reddit.com/user/rb9_3b</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rb9_3b"&gt; /u/rb9_3b &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1ipyivw/i_made_a_simple_python_library_to_create_a_bridge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irg7ox/i_made_a_simple_python_library_to_create_a_bridge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irg7ox/i_made_a_simple_python_library_to_create_a_bridge/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T09:38:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1irjdtm</id>
    <title>100% utilization using the Ollama with phi 3.5 model</title>
    <updated>2025-02-17T13:05:19+00:00</updated>
    <author>
      <name>/u/Anxious-Resort1043</name>
      <uri>https://old.reddit.com/user/Anxious-Resort1043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, &lt;/p&gt; &lt;p&gt;I have been using the phi3 .5 model using Ollama on a linux machine which has 32 core CPU and 200Gb ram, no GPU though. The LLM is used to generate response using the chunks, in my Mac which is 8gb M1 mini, it has no issues but facing issues with the Linux machine. Am I doing anything wrong here ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxious-Resort1043"&gt; /u/Anxious-Resort1043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irjdtm/100_utilization_using_the_ollama_with_phi_35_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irjdtm/100_utilization_using_the_ollama_with_phi_35_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irjdtm/100_utilization_using_the_ollama_with_phi_35_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T13:05:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1irkej7</id>
    <title>adjusting RoPE scaling and NTK-aware scaling</title>
    <updated>2025-02-17T13:57:34+00:00</updated>
    <author>
      <name>/u/Useful-Skill6241</name>
      <uri>https://old.reddit.com/user/Useful-Skill6241</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I guys im running deepseek-r1:70b -its a distilled Q4_K_M 0c1615a8ca32 (42GB)&lt;/p&gt; &lt;p&gt;I have changed the paramters in ollma for a 15,000 token context window:&lt;/p&gt; &lt;p&gt;&amp;quot; Model&lt;/p&gt; &lt;p&gt;architecture llama&lt;/p&gt; &lt;p&gt;parameters 70.6B&lt;/p&gt; &lt;p&gt;context length 131072&lt;/p&gt; &lt;p&gt;embedding length 8192&lt;/p&gt; &lt;p&gt;quantization Q4_K_M&lt;/p&gt; &lt;p&gt;Parameters&lt;/p&gt; &lt;p&gt;num_ctx 15000&amp;quot;&lt;/p&gt; &lt;p&gt;is there anywhere you guys know of where I can adjusting RoPE scaling and NTK-aware scaling. I have the ram for it, it just spits out gibberish or doesnt take into account the text after around i would say 1200 tokens of output&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Useful-Skill6241"&gt; /u/Useful-Skill6241 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irkej7/adjusting_rope_scaling_and_ntkaware_scaling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irkej7/adjusting_rope_scaling_and_ntkaware_scaling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irkej7/adjusting_rope_scaling_and_ntkaware_scaling/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T13:57:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1irn0f9</id>
    <title>Command-r7b</title>
    <updated>2025-02-17T15:54:46+00:00</updated>
    <author>
      <name>/u/J0Mo_o</name>
      <uri>https://old.reddit.com/user/J0Mo_o</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can command-r7b be run on ollama 0.5.7 It says it needs ollama 0.5.5 but i don't know if that means it needs THIS specific version or it needs at least this version&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/J0Mo_o"&gt; /u/J0Mo_o &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irn0f9/commandr7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irn0f9/commandr7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irn0f9/commandr7b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T15:54:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1irn6o9</id>
    <title>Any fix to this?</title>
    <updated>2025-02-17T16:01:55+00:00</updated>
    <author>
      <name>/u/Beneficial-Cup2969</name>
      <uri>https://old.reddit.com/user/Beneficial-Cup2969</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1irn6o9/any_fix_to_this/"&gt; &lt;img alt="Any fix to this?" src="https://preview.redd.it/iaqslgar2qje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f8c6aa9074fc89619a7bbd47b7412491bf027552" title="Any fix to this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry am really new to all these private ai stuff but I can't get ollama to analyse this image I used a tutorial to use llama 3 using docker and local host for convenience am i missing something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beneficial-Cup2969"&gt; /u/Beneficial-Cup2969 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iaqslgar2qje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irn6o9/any_fix_to_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irn6o9/any_fix_to_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T16:01:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1irek2w</id>
    <title>How do you find the right AI model for your use case?</title>
    <updated>2025-02-17T07:38:22+00:00</updated>
    <author>
      <name>/u/uber_men</name>
      <uri>https://old.reddit.com/user/uber_men</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Where can I search for AI models that fit my use case? For example, if I want to restore old blurred photos but donâ€™t know which AI model to use, I should be able to find one like &lt;a href="https://github.com/TencentARC/GFPGAN"&gt;GFPGAN&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uber_men"&gt; /u/uber_men &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irek2w/how_do_you_find_the_right_ai_model_for_your_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irek2w/how_do_you_find_the_right_ai_model_for_your_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irek2w/how_do_you_find_the_right_ai_model_for_your_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T07:38:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1irnuqw</id>
    <title>Ollama shouldn't spam update fetch requests every second or two when it already has the update downloaded...</title>
    <updated>2025-02-17T16:29:29+00:00</updated>
    <author>
      <name>/u/ZdrytchX</name>
      <uri>https://old.reddit.com/user/ZdrytchX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;time=2025-02-16T03:53:53.276+08:00 level=INFO source=updater.go:103 msg=&amp;quot;New update available at https://github.com/ollama/ollama/releases/download/v0.5.11/OllamaSetup.exe&amp;quot; time=2025-02-16T03:53:54.595+08:00 level=INFO source=updater.go:138 msg=&amp;quot;update already downloaded&amp;quot; time=2025-02-16T04:53:55.056+08:00 level=INFO source=updater.go:103 msg=&amp;quot;New update available at https://github.com/ollama/ollama/releases/download/v0.5.11/OllamaSetup.exe&amp;quot; time=2025-02-16T04:53:56.349+08:00 level=INFO source=updater.go:138 msg=&amp;quot;update already downloaded&amp;quot; time=2025-02-16T05:53:56.811+08:00 level=INFO source=updater.go:103 msg=&amp;quot;New update available at https://github.com/ollama/ollama/releases/download/v0.5.11/OllamaSetup.exe&amp;quot; time=2025-02-16T05:53:58.120+08:00 level=INFO source=updater.go:138 msg=&amp;quot;update already downloaded&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZdrytchX"&gt; /u/ZdrytchX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irnuqw/ollama_shouldnt_spam_update_fetch_requests_every/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irnuqw/ollama_shouldnt_spam_update_fetch_requests_every/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irnuqw/ollama_shouldnt_spam_update_fetch_requests_every/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T16:29:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iro9kd</id>
    <title>I Built My Own AI Code Assistant with DeepSeek &amp; LangChain!</title>
    <updated>2025-02-17T16:46:08+00:00</updated>
    <author>
      <name>/u/Special_Community179</name>
      <uri>https://old.reddit.com/user/Special_Community179</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iro9kd/i_built_my_own_ai_code_assistant_with_deepseek/"&gt; &lt;img alt="I Built My Own AI Code Assistant with DeepSeek &amp;amp; LangChain!" src="https://external-preview.redd.it/hnt7poXU-xnfeXFZfxsMOiSfirurfRIotwJUPYEsx90.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=655a950bdbfba394f2efb20d1be0901f5db6f196" title="I Built My Own AI Code Assistant with DeepSeek &amp;amp; LangChain!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special_Community179"&gt; /u/Special_Community179 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=7GAKB21lxyg&amp;amp;list=PLp01ObP3udmq2quR-RfrX4zNut_t_kNot"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iro9kd/i_built_my_own_ai_code_assistant_with_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iro9kd/i_built_my_own_ai_code_assistant_with_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T16:46:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir1y64</id>
    <title>Web Search for Ollama</title>
    <updated>2025-02-16T20:38:37+00:00</updated>
    <author>
      <name>/u/Pure-Caramel1216</name>
      <uri>https://old.reddit.com/user/Pure-Caramel1216</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ir1y64/web_search_for_ollama/"&gt; &lt;img alt="Web Search for Ollama" src="https://external-preview.redd.it/cTZzejBhcDdia2plMX2nSzWAqWPwL7GaLWhWgmcM0HgC18EKBhh979hmLHax.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02f06424f67deec37bb163c15ea8dec1329da736" title="Web Search for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Although I love Ollama, I've found it nearly impossible to get up-to-date information. Existing solutions are either too complex or simply don't work well. Thatâ€™s why I built an anonymous web search tool for Ollama (check out the video above).&lt;/p&gt; &lt;p&gt;Since Iâ€™m not the only one facing this issue, Iâ€™m considering launching it as a full-fledged tool. Iâ€™d really appreciate your suggestions on how to improve itâ€”please leave your ideas in the comments. And if you like what you see, consider upvoting so this project reaches a wider audience.&lt;/p&gt; &lt;p&gt;About the Video:&lt;/p&gt; &lt;p&gt;Model: Small 3B model&lt;/p&gt; &lt;p&gt;Hardware: Running on an NVIDIA A10 GPU&lt;/p&gt; &lt;p&gt;Demo: I ran the same prompt twiceâ€”first without web search, and then with web search.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pure-Caramel1216"&gt; /u/Pure-Caramel1216 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9wsgtkx7bkje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ir1y64/web_search_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ir1y64/web_search_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T20:38:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqvypa</id>
    <title>Ollama Vs. LM Studio</title>
    <updated>2025-02-16T16:29:24+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iqvypa/ollama_vs_lm_studio/"&gt; &lt;img alt="Ollama Vs. LM Studio" src="https://external-preview.redd.it/9twFf-ndeg92DJQ4VUhbGT49y8GQ4M4VnDD5QQ3aM0g.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d603d6ac771df4cac0cb2b45b29e921c3ab5dc19" title="Ollama Vs. LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/QGtkaDWJZlA"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iqvypa/ollama_vs_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iqvypa/ollama_vs_lm_studio/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-16T16:29:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1irjave</id>
    <title>OpenThinker-32B-FP16 + 8x AMD Instinct Mi60 Server + vLLM + Tensor Parallelism</title>
    <updated>2025-02-17T13:01:13+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/klcjcd1d6pje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irjave/openthinker32bfp16_8x_amd_instinct_mi60_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irjave/openthinker32bfp16_8x_amd_instinct_mi60_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T13:01:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1iref1e</id>
    <title>AMD Instinct MI50 detailed benchmarks in ollama</title>
    <updated>2025-02-17T07:28:00+00:00</updated>
    <author>
      <name>/u/Psychological_Ear393</name>
      <uri>https://old.reddit.com/user/Psychological_Ear393</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 2xMI50s and ran a series of benchmarks in ollama on a variety of models with a few quants thrown in, only running models which fit into the total 32gb VRAM&lt;/p&gt; &lt;p&gt;It's difficult to tell exactly how other benchmarks were run, so I can't really say how they perform relative to others but they at least compete with low end modern cards like the 4060 Ti and the A4000, but at substantially lower cost.&lt;/p&gt; &lt;p&gt;Full details here of the software versions, hardware, prompt and models, variations in the output lengths, TPS, results at 250 and 125 watts, size reported by ollama ps, and USD/TPS: &lt;a href="https://docs.google.com/spreadsheets/d/1TjxpN0NYh-xb0ZwCpYr4FT-hG773_p1DEgxJaJtyRmY/edit?usp=sharing"&gt;https://docs.google.com/spreadsheets/d/1TjxpN0NYh-xb0ZwCpYr4FT-hG773_p1DEgxJaJtyRmY/edit?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am very keen to hear how other card perform on the identical benchmark runs. I know they are on the bottom of the pack when it comes to performance for current builds, but I bought mine for $110USD each and last I checked were going for about $120USD, which to me makes them a steal.&lt;/p&gt; &lt;p&gt;For the models I tested, the fastest model was unsurprisingly &lt;code&gt;llama3.2:1b-instruct-q8_0&lt;/code&gt; maxing 150 tps, and the slowest was &lt;code&gt;FuseO1-DeekSeekR1-QwQ-SkyT1-32B-Preview-GGUF:Q6_K&lt;/code&gt; at 14tps.&lt;/p&gt; &lt;p&gt;I did get one refused on the prompt I used &lt;code&gt;Who discovered heliocentrism and how is that possible without being in space? Be verbose I want to know all about it.&lt;/code&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I can't provide information on who discovered heliocentrism or other topics that may be considered sensitive or controversial, such as the Copernican Revolution. Is there anything else I can help you with?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Which was really weird, and it happened more than once in llama, but no others, and I saw another different refusal on another model then never saw the refusal again&lt;/p&gt; &lt;p&gt;Some anticipated Q&amp;amp;A&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How did I deal with the ROCm problem?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The sarcastic answer is &amp;quot;&lt;em&gt;What ROCm problem?&amp;quot;&lt;/em&gt;. It seems to me like there's a lot of people who don't have an AMD card, people with an unsupported card, people on an unsupported distro, or people who ran it a long time ago who are spouting this.&lt;/p&gt; &lt;p&gt;The more serious answer is the ROCm install docs have the distro and hardware requirements. If you meet those it should just work. I initially tried in my distro of choice, which was not listed, and it was too hard so I gave up and installed Ubuntu and everything just worked. By &amp;quot;just worked&amp;quot; I mean I installed Ubuntu, followed the ROCm install guide, downloaded ollama, ran it, and ollama used the GPU without any hassle.&lt;/p&gt; &lt;p&gt;ComfyUI was similarly easy, except I had the additional steps of pulling the AMD repo, building, then running.&lt;/p&gt; &lt;p&gt;I have not tried any other apps.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How did I cool them?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I bought some 3D printed shrouds off Ebay that take an 80mm fan. I had to keep them power capped at 90 watts or they would overheat, and after some kind advice from here it was shown that the shrouds had an inefficient path for the air to travel and a custom solution would work better. I didn't do that because of time/money and instead bought silverstone 80mm industrial fans (10K RPM max) and they work a treat and keep them cool at 250 watts.&lt;/p&gt; &lt;p&gt;They are very loud so I bought a PWM controller which I keep on the case and adjust the fan speed for how hard I want to run the cards. It's outright too hard to control the fan speed through IPMI tool which is an app made by the devil to torment Linux users.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Would I buy them again?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Being old and relatively slow (I am guessing just slower than a 4070) I expected them to be temporary while I got started with AI, but they have been performing above my expectations. I would absolutely buy them again if I could live that build over again, and if I can mount the cards so there's more room, such as with PCIe extender cables, I would buy more two more MI50s for 64Gb VRAM.&lt;/p&gt; &lt;p&gt;For space and power reasons I would prefer MI60s or MI100s but this experience has me cemented as an Instinct fan and I have no interest in buying any nvidia card at their current new and used prices.&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;If there's any models you would like tested, let me know&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological_Ear393"&gt; /u/Psychological_Ear393 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iref1e/amd_instinct_mi50_detailed_benchmarks_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iref1e/amd_instinct_mi50_detailed_benchmarks_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iref1e/amd_instinct_mi50_detailed_benchmarks_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T07:28:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1irhbrl</id>
    <title>PrivateLLMLens - A Zero-Server Web Interface for Local Ollama</title>
    <updated>2025-02-17T10:58:39+00:00</updated>
    <author>
      <name>/u/Specialist_Laugh_231</name>
      <uri>https://old.reddit.com/user/Specialist_Laugh_231</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1irhbrl/privatellmlens_a_zeroserver_web_interface_for/"&gt; &lt;img alt="PrivateLLMLens - A Zero-Server Web Interface for Local Ollama" src="https://preview.redd.it/5bfizklmkoje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4051a019d044303e3381fd284e4eee1a8ace21b9" title="PrivateLLMLens - A Zero-Server Web Interface for Local Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist_Laugh_231"&gt; /u/Specialist_Laugh_231 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5bfizklmkoje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irhbrl/privatellmlens_a_zeroserver_web_interface_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irhbrl/privatellmlens_a_zeroserver_web_interface_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T10:58:39+00:00</published>
  </entry>
</feed>
