<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-07-24T18:44:54+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1m6dow3</id>
    <title>Disable ssl check</title>
    <updated>2025-07-22T13:10:46+00:00</updated>
    <author>
      <name>/u/qtm_music</name>
      <uri>https://old.reddit.com/user/qtm_music</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there a way to disable ssl check for ollama in docker? I work on windows, my corporate proxy replaces certificates, is there a way to disable the check?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/qtm_music"&gt; /u/qtm_music &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6dow3/disable_ssl_check/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6dow3/disable_ssl_check/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m6dow3/disable_ssl_check/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T13:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5z1x7</id>
    <title>Is there a simple way to "enhance" a model with the content of a book?</title>
    <updated>2025-07-21T23:52:30+00:00</updated>
    <author>
      <name>/u/RaticateLV99</name>
      <uri>https://old.reddit.com/user/RaticateLV99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run some DnD adventures and I want to teach local models with the content of a book.&lt;/p&gt; &lt;p&gt;But, I also want to add more details about &lt;strong&gt;my adventure from time to time&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Is there a simple way to enhance the model with the content of my adventures and the content of the books?&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RaticateLV99"&gt; /u/RaticateLV99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m5z1x7/is_there_a_simple_way_to_enhance_a_model_with_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m5z1x7/is_there_a_simple_way_to_enhance_a_model_with_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m5z1x7/is_there_a_simple_way_to_enhance_a_model_with_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-21T23:52:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1m638nd</id>
    <title>Realtime codebase indexing for coding agents with ~ 50 lines of Python (open source)</title>
    <updated>2025-07-22T03:11:53+00:00</updated>
    <author>
      <name>/u/Whole-Assignment6240</name>
      <uri>https://old.reddit.com/user/Whole-Assignment6240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Would love to share my open source project that buildings realtime indexing &amp;amp; context for coding agents ~ 50 lines of Python on the &lt;a href="https://github.com/cocoindex-io/cocoindex/blob/main/examples/code_embedding/main.py#L11-L84"&gt;indexing path&lt;/a&gt;. Full blog and explanation &lt;a href="https://cocoindex.io/blogs/index-code-base-for-rag"&gt;here&lt;/a&gt;. Would love your feedback and appreciate a star on the repo if it is helpful, thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Assignment6240"&gt; /u/Whole-Assignment6240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m638nd/realtime_codebase_indexing_for_coding_agents_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m638nd/realtime_codebase_indexing_for_coding_agents_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m638nd/realtime_codebase_indexing_for_coding_agents_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T03:11:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6j0n9</id>
    <title>"You are a teacher. Teach me about a random topic"</title>
    <updated>2025-07-22T16:38:52+00:00</updated>
    <author>
      <name>/u/TutorialDoctor</name>
      <uri>https://old.reddit.com/user/TutorialDoctor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This prompt doesn't generate random topics for llama 3.2 or Gemma 3 4B. In fact, it often generates the same topic on Bioluminescence or the Science of Color and one other topic.&lt;/p&gt; &lt;p&gt;What does it generate for you? I'm using ollama locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TutorialDoctor"&gt; /u/TutorialDoctor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6j0n9/you_are_a_teacher_teach_me_about_a_random_topic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6j0n9/you_are_a_teacher_teach_me_about_a_random_topic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m6j0n9/you_are_a_teacher_teach_me_about_a_random_topic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T16:38:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5jqbo</id>
    <title>Use llm to gather insights of market fluctuations</title>
    <updated>2025-07-21T14:02:07+00:00</updated>
    <author>
      <name>/u/m19990328</name>
      <uri>https://old.reddit.com/user/m19990328</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m5jqbo/use_llm_to_gather_insights_of_market_fluctuations/"&gt; &lt;img alt="Use llm to gather insights of market fluctuations" src="https://preview.redd.it/diudkgucf8ef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6f52d897baf99be2923a0d6ecfbc861119fda53" title="Use llm to gather insights of market fluctuations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I've recently built a project that explores stock price trends and gathers market insights. Last time I shared it here, some of you showed interest. Now, I've packaged it as a Windows app with a GUI. Feel free to check it out!&lt;/p&gt; &lt;p&gt;Project: &lt;a href="https://github.com/CyrusCKF/stock-gone-wrong"&gt;https://github.com/CyrusCKF/stock-gone-wrong&lt;/a&gt;&lt;br /&gt; Download: &lt;a href="https://github.com/CyrusCKF/stock-gone-wrong/releases/tag/v0.1.0-alpha"&gt;https://github.com/CyrusCKF/stock-gone-wrong/releases/tag/v0.1.0-alpha&lt;/a&gt; (Windows may display a warning)&lt;/p&gt; &lt;p&gt;To use this function, first navigate to the &lt;strong&gt;&amp;quot;Events&amp;quot;&lt;/strong&gt; tab. Enter your ticker, select a date range, and click the button. The stock trends will be split into several &amp;quot;&lt;em&gt;major events&amp;quot;&lt;/em&gt;. Use the slider to select an event you're interested in, then click &lt;strong&gt;&amp;quot;Find News&amp;quot;&lt;/strong&gt;. This will initialize an Ollama agent to scrape and summarize stock news around the timeframe. Note that this process may take several minutes, depending on your machine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DISCLAIMER&lt;/strong&gt; This tool is not intended to provide stock-picking recommendations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m19990328"&gt; /u/m19990328 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/diudkgucf8ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m5jqbo/use_llm_to_gather_insights_of_market_fluctuations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m5jqbo/use_llm_to_gather_insights_of_market_fluctuations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-21T14:02:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6svz1</id>
    <title>Moving 1 big Ollama model to another PC</title>
    <updated>2025-07-22T22:56:56+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently I started using GPUStack and got it installed and working on 3 systems with 7 GPUs. Problem is that I exceeded my 1.2 TB internet usage. I wanted to test larger 70B models but needed to wait several days for my ISP to reset the meter. I took the time to figure out how to transfer individual ollama models to other network systems.&lt;/p&gt; &lt;p&gt;First issue is that models are store as:&lt;/p&gt; &lt;p&gt;&lt;em&gt;sha256-f1b16b5d5d524a6de624e11ac48cc7d2a9b5cab399aeab6346bd0600c94cfd12&lt;/em&gt;&lt;/p&gt; &lt;p&gt;We get can needed info like path to model and model sha256 name:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama show --modelfile llava:13b-v1.5-q8_0&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Modelfile generated by &amp;quot;ollama show&amp;quot; # To build a new Modelfile based on this, replace FROM with: # FROM llava:13b-v1.5-q8_0 FROM /usr/share/ollama/.ollama/models/blobs/sha256-f1b16b5d5d524a6de624e11ac48cc7d2a9b5cab399aeab6346bd0600c94cfd12 FROM /usr/share/ollama/.ollama/models/blobs/sha256-0af93a69825fd741ffdc7c002dcd47d045c795dd55f73a3e08afa484aff1bcd3 TEMPLATE &amp;quot;{{ .System }} USER: {{ .Prompt }} ASSSISTANT: &amp;quot; PARAMETER stop USER: PARAMETER stop ASSSISTANT: LICENSE &amp;quot;&amp;quot;&amp;quot;LLAMA 2 COMMUNITY LICENSE AGREEMENT Llama 2 Version Release Date: July 18, 2023 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I used the first listed sha256- file based on the size (13G)&lt;/p&gt; &lt;p&gt;&lt;code&gt;ls -lhS /usr/share/ollama/.ollama/models/blobs/sha256-f1b*&lt;/code&gt;&lt;/p&gt; &lt;p&gt;-rw-r--r-- 1 ollama ollama 13G May 17&lt;/p&gt; &lt;p&gt;From SOURCE PC:&lt;/p&gt; &lt;p&gt;Will be using scp and ssh to remote into destination pc so if necessary just install:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo apt install openssh-server&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This is where we will have model info saved&lt;/p&gt; &lt;p&gt;&lt;code&gt;mkdir ~/models.txt&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Lets find a big model to transfer&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama list | sort -k3&lt;/code&gt;&lt;/p&gt; &lt;p&gt;On my system I'll use llava:13b-v1.5-q8_0&lt;/p&gt; &lt;p&gt;ollama show --modelfile llava:13b-v1.5-q8_0&lt;/p&gt; &lt;p&gt;simpler view&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama show --modelfile llava:13b-v1.5-q8_0 | grep FROM \ | tee -a ~/models.txt; echo &amp;quot;&amp;quot; &amp;gt;&amp;gt; ~/models.txt &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;By appending &amp;gt;&amp;gt; the output to 'models.txt' we have a record \&lt;/p&gt; &lt;p&gt;of data on both PC.&lt;/p&gt; &lt;p&gt;Now add the sha256- model number then scp transfer to local \&lt;/p&gt; &lt;p&gt;remote PC's home directory.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;scp ~/models.txt user3@10.0.0.34:~ &amp;amp;&amp;amp; scp \ /usr/share/ollama/.ollama/models/blobs/sha256-xxx user3@10.0.0.34:~ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is what full command looks like.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;scp ~/models.txt user3@10.0.0.34:~ &amp;amp;&amp;amp; scp \ /usr/share/ollama/.ollama/models/blobs/\ sha256-f1b16b5d5d524a6de624e11ac48cc7d2a9b5cab399aeab6346bd0600c94cfd12 \ user3@10.0.0.34:~ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;About 2 minutes to transfer 12GB over 1 Gigabit Ethernet network (1000Base-T or Gb3 or 1 GigE)&lt;/p&gt; &lt;p&gt;Lets get into remote PC (ssh), change permission (chown) \&lt;/p&gt; &lt;p&gt;of the file and move (mv) file to correct path for ollama.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ssh user3@10.0.0.34 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;view the transferred file.&lt;/p&gt; &lt;p&gt;&lt;code&gt;cat ~/models.txt&lt;/code&gt;&lt;/p&gt; &lt;p&gt;copy sha256- (or just tab auto complete) number and change permission&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo chown ollama:ollama sha256-*&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Move to ollama blobs folder, view in size order and then ready to \&lt;/p&gt; &lt;p&gt;ollama pull&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo mv ~/sha256-* /usr/share/ollama/.ollama/models/blobs/ &amp;amp;&amp;amp; ls -lhS /usr/share/ollama/.ollama/models/blobs/ ; echo &amp;quot;ls -lhS then pull model&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;formatting issues:&lt;/p&gt; &lt;p&gt;sudo mv ~/sha256-* /usr/share/ollama/.ollama/models/blobs/ &amp;amp;&amp;amp; \&lt;/p&gt; &lt;p&gt;ls -lhS /usr/share/ollama/.ollama/models/blobs/ ; \&lt;/p&gt; &lt;p&gt;echo &amp;quot;ls -lhS then pull model&amp;quot;&lt;/p&gt; &lt;p&gt;ollama pull llava:13b-v1.5-q8_0&lt;/p&gt; &lt;p&gt;Ollama will recognize the largest part of the file and only download \&lt;/p&gt; &lt;p&gt;the smaller needed parts. Should be done in a few seconds.&lt;/p&gt; &lt;p&gt;Now I just need to figure out how to get GPUStack to use my already \&lt;/p&gt; &lt;p&gt;download ollama file instead of downloading it all over again.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6svz1/moving_1_big_ollama_model_to_another_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6svz1/moving_1_big_ollama_model_to_another_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m6svz1/moving_1_big_ollama_model_to_another_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T22:56:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6mfml</id>
    <title>Models which perform better as Q8 (int8) over Q4_(X_Y)?</title>
    <updated>2025-07-22T18:44:56+00:00</updated>
    <author>
      <name>/u/RecoJohnson</name>
      <uri>https://old.reddit.com/user/RecoJohnson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone tested models that perform more accurately or with more efficiency with Q8 quantization instead of the more common Q4_K_M etc?&lt;/p&gt; &lt;p&gt;AMD's newer consumer video cards improved the performance of int8 and fp16 computation and I want to learn more about it and am curious if Q8 models are going to take over in the long run with attention techniques.&lt;/p&gt; &lt;p&gt;I would love to see some benchmarks if anyone has done their own testing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RecoJohnson"&gt; /u/RecoJohnson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6mfml/models_which_perform_better_as_q8_int8_over_q4_x_y/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6mfml/models_which_perform_better_as_q8_int8_over_q4_x_y/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m6mfml/models_which_perform_better_as_q8_int8_over_q4_x_y/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T18:44:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6jypu</id>
    <title>Why isn't this already a standard in robotics?</title>
    <updated>2025-07-22T17:13:50+00:00</updated>
    <author>
      <name>/u/PranavVermaa</name>
      <uri>https://old.reddit.com/user/PranavVermaa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I was playing around with Ollama and got this working in under &lt;strong&gt;2 minutes&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;You give it a natural language command like:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Run 10 meters&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;It instantly returns:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;action&amp;quot;: &amp;quot;run&amp;quot;, &amp;quot;distance_meters&amp;quot;: 10, &amp;quot;unit&amp;quot;: &amp;quot;meters&amp;quot; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I didn’t tweak anything. I just used llama3.2:3b and created a straightforward system prompt in a Modelfile. That’s all. No additional tools. No ROS integration yet. But the main idea is — the whole &amp;quot;understand action and structure it&amp;quot; issue is pretty much resolved with a good LLM and some JSON formatting.&lt;/p&gt; &lt;p&gt;Think about what we could achieve if we had:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time voice-to-action systems,&lt;/li&gt; &lt;li&gt;A lightweight LLM operating on-device (or at the edge),&lt;/li&gt; &lt;li&gt;A basic robotic API to process these tokens and carry them out.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I feel like we’ve made robotics interfaces way too complicated for years.&lt;br /&gt; This is so simple now. What are we waiting for?&lt;/p&gt; &lt;p&gt;For Reference, here is my Modelfile that I used: &lt;a href="https://pastebin.com/TaXBQGZK"&gt;https://pastebin.com/TaXBQGZK&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PranavVermaa"&gt; /u/PranavVermaa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6jypu/why_isnt_this_already_a_standard_in_robotics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6jypu/why_isnt_this_already_a_standard_in_robotics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m6jypu/why_isnt_this_already_a_standard_in_robotics/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T17:13:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1m73rj1</id>
    <title>Need Help - Local LLM &amp; Lots of Files! (Privacy Concerns)</title>
    <updated>2025-07-23T08:29:07+00:00</updated>
    <author>
      <name>/u/AreBee73</name>
      <uri>https://old.reddit.com/user/AreBee73</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AreBee73"&gt; /u/AreBee73 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1m73q8n/need_help_local_llm_lots_of_files_privacy_concerns/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m73rj1/need_help_local_llm_lots_of_files_privacy_concerns/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m73rj1/need_help_local_llm_lots_of_files_privacy_concerns/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-23T08:29:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6mice</id>
    <title>Local Long Term Memory with Ollama?</title>
    <updated>2025-07-22T18:47:39+00:00</updated>
    <author>
      <name>/u/Debug_Mode_On</name>
      <uri>https://old.reddit.com/user/Debug_Mode_On</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For whatever reason I prefer to run everything local. When I search long term memory for my little conversational bot, I see a lot of solutions. Many of them are cloud based. Is there a standard solution to offer my little chat bot long term memory that runs locally with Ollama that I should be looking at? Or a tutorial you would recommend? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Debug_Mode_On"&gt; /u/Debug_Mode_On &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6mice/local_long_term_memory_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6mice/local_long_term_memory_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m6mice/local_long_term_memory_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T18:47:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m70f7q</id>
    <title>**🔓 I built Hearth-UI — A fully-featured desktop app for chatting with local LLMs (Ollama-ready, attachments, themes, markdown, and more)**</title>
    <updated>2025-07-23T04:59:48+00:00</updated>
    <author>
      <name>/u/Vast-Helicopter-3719</name>
      <uri>https://old.reddit.com/user/Vast-Helicopter-3719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m70f7q/i_built_hearthui_a_fullyfeatured_desktop_app_for/"&gt; &lt;img alt="**🔓 I built Hearth-UI — A fully-featured desktop app for chatting with local LLMs (Ollama-ready, attachments, themes, markdown, and more)**" src="https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417" title="**🔓 I built Hearth-UI — A fully-featured desktop app for chatting with local LLMs (Ollama-ready, attachments, themes, markdown, and more)**" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! 👋&lt;/p&gt; &lt;p&gt;I recently put together a desktop AI chat interface called &lt;strong&gt;Hearth-UI&lt;/strong&gt;, made for anyone using &lt;a href="https://ollama.com/"&gt;Ollama&lt;/a&gt; for local LLMs like LLaMA3, Mistral, Gemma, etc.&lt;/p&gt; &lt;p&gt;It includes everything I wish existed in a typical Ollama UI — and it’s fully offline, customizable, and open-source.&lt;/p&gt; &lt;p&gt;🧠 Features:&lt;/p&gt; &lt;p&gt;✅ &lt;strong&gt;Multi-session chat history&lt;/strong&gt; (rename, delete, auto-save)&lt;br /&gt; ✅ &lt;strong&gt;Markdown + syntax highlighting&lt;/strong&gt; (like ChatGPT)&lt;br /&gt; ✅ &lt;strong&gt;Streaming responses&lt;/strong&gt; + prompt &lt;strong&gt;queueing while streaming&lt;/strong&gt;&lt;br /&gt; ✅ &lt;strong&gt;File uploads&lt;/strong&gt; &amp;amp; &lt;strong&gt;drag-and-drop attachments&lt;/strong&gt;&lt;br /&gt; ✅ Beautiful &lt;strong&gt;theme picker&lt;/strong&gt; (Dark/Light/Blue/Green/etc)&lt;br /&gt; ✅ &lt;strong&gt;Cancel response mid-generation&lt;/strong&gt; (Stop button)&lt;br /&gt; ✅ Export chat to &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.json&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;&lt;br /&gt; ✅ &lt;strong&gt;Electron-powered desktop app for Windows&lt;/strong&gt; (macOS/Linux coming)&lt;br /&gt; ✅ Works with your existing &lt;code&gt;ollama serve&lt;/code&gt; — no cloud, no signup&lt;/p&gt; &lt;h1&gt;🔧 Tech stack:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Ollama (as LLM backend)&lt;/li&gt; &lt;li&gt;HTML/CSS/JS (Vanilla frontend)&lt;/li&gt; &lt;li&gt;Electron for standalone app&lt;/li&gt; &lt;li&gt;Node.js backend (for model list &amp;amp; /chat proxy)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6cjcdgu90kef1.png?width=3790&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51c3f0e53500960e0fe3313f94699dd725d33187"&gt;https://preview.redd.it/6cjcdgu90kef1.png?width=3790&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51c3f0e53500960e0fe3313f94699dd725d33187&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;GitHub link:&lt;/h1&gt; &lt;p&gt;👉 &lt;a href="https://github.com/Saurabh682/Hearth-UI"&gt;https://github.com/Saurabh682/Hearth-UI&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;🙏 I'd love your feedback on:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Other must-have features?&lt;/li&gt; &lt;li&gt;Would a Windows/exe help?&lt;/li&gt; &lt;li&gt;Any bugs or improvement ideas?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for checking it out. Hope it helps the self-hosted LLM community!&lt;br /&gt; ❤️&lt;/p&gt; &lt;h1&gt;🏷️ Tags:&lt;/h1&gt; &lt;p&gt;&lt;code&gt;[Electron] [Ollama] [Local LLM] [Desktop AI UI] [Markdown] [Self Hosted]&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast-Helicopter-3719"&gt; /u/Vast-Helicopter-3719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m70f7q/i_built_hearthui_a_fullyfeatured_desktop_app_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m70f7q/i_built_hearthui_a_fullyfeatured_desktop_app_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m70f7q/i_built_hearthui_a_fullyfeatured_desktop_app_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-23T04:59:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7biv2</id>
    <title>integrate an LLM that filters emails</title>
    <updated>2025-07-23T14:58:35+00:00</updated>
    <author>
      <name>/u/Background-Basil-871</name>
      <uri>https://old.reddit.com/user/Background-Basil-871</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I work on a side project to read and filter my emails. The project work with Node and ollama package.&lt;br /&gt; The goals is to retrieve my emails and sort them with a LLM.&lt;/p&gt; &lt;p&gt;I have a small chat box where I can say for exemple : &amp;quot;Give me only mail talking about cars&amp;quot;. Then, the LLM must give me back a array of mail ID matching my requierment.&lt;br /&gt; Look pretty simple but i'm struggling a bit, in fact, it give me back also some email out of the purpose.&lt;br /&gt; First it maybe a bad prompt&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;Your a agent that analyze emails and that can ONLY return the mail IDs that match the user's requirements. Your response must contain ONLY the mail IDs in a array [], if no mail match the user's requirements, return an empty array. Example: '[id1,id2,id3]'. You must check the subjects and mails body.&amp;quot;; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Full method&lt;/p&gt; &lt;pre&gt;&lt;code&gt; const formattedMails = mails .map(( mail ) =&amp;gt; { const cleanBody = removeHtmlTags( mail .body) || &amp;quot;No body content&amp;quot;; return `ID: ${ mail .id} | Subject: ${ mail .subject} | From: ${ mail .from } | Body: ${cleanBody.substring(0, 500)}...`; }) .join(&amp;quot;\n\n&amp;quot;); console.log(&amp;quot;Sending to AI:&amp;quot;, { systemPrompt, userPrompt, mailCount: mails .length, formattedMails, }); const response = await ollama.chat({ model: &amp;quot;mistral&amp;quot;, messages: [ { role: &amp;quot;system&amp;quot;, content: systemPrompt, }, { role: &amp;quot;user&amp;quot;, content: `User request: ${ userPrompt }\n\nAvailable emails:\n${formattedMails}\n\nReturn only the matching mail IDs separated by commas:`, }, ], }); return response.message.content; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I use Mistral.&lt;/p&gt; &lt;p&gt;I&amp;quot;m very new to this kind of thing. Idk if the problem come from the prompt, agent or may be a too big prompt ?&lt;/p&gt; &lt;p&gt;Any help or idea is welcome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Background-Basil-871"&gt; /u/Background-Basil-871 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7biv2/integrate_an_llm_that_filters_emails/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7biv2/integrate_an_llm_that_filters_emails/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7biv2/integrate_an_llm_that_filters_emails/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-23T14:58:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6pxln</id>
    <title>Digital twins that attend meetings for you. Dystopia or soon reality?</title>
    <updated>2025-07-22T20:56:54+00:00</updated>
    <author>
      <name>/u/DerErzfeind61</name>
      <uri>https://old.reddit.com/user/DerErzfeind61</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m6pxln/digital_twins_that_attend_meetings_for_you/"&gt; &lt;img alt="Digital twins that attend meetings for you. Dystopia or soon reality?" src="https://external-preview.redd.it/Mnh0bG1lNW1vaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80201ecaa90deb38fdf6db31774e15c9a1e0c78e" title="Digital twins that attend meetings for you. Dystopia or soon reality?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In more and more meetings these days there are AI notetakers that someone has sent instead of showing up themselves. You can think what you want about these notetakers, but they seem to have become part of our everyday working lives. This raises the question of how long it will be before the next stage of development occurs and we are sitting in meetings with “digital twins” who are standing in for an absent employee.&lt;/p&gt; &lt;p&gt;To find out, I tried to build such a digital twin and it actually turned out to be very easy to create a meeting agent that can actively interact with other participants, share insights about my work and answer follow-up questions for me. Of course, many of the leading providers of voice clones and personalized LLMs are closed-source, which increases the privacy issue that already exists with AI Notetakers. However, my approach using joinly could also be implemented with Chatterbox and a self-hosted LLM with few-shot prompting, for example.&lt;/p&gt; &lt;p&gt;But there are of course many other critical questions: how exactly can we control what these digital twins disclose or are allowed to decide, ethical concerns about whether my company is allowed to create such a twin for me, how this is compatible with meeting etiquette and of course whether we shouldn't simply plan better meetings instead.&lt;/p&gt; &lt;p&gt;What do you think? Will such digital twins catch on? Would you use one to skip a boring meeting?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DerErzfeind61"&gt; /u/DerErzfeind61 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ynyz3f5mohef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6pxln/digital_twins_that_attend_meetings_for_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m6pxln/digital_twins_that_attend_meetings_for_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T20:56:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7juxs</id>
    <title>Trying to make an v1/chat/completions</title>
    <updated>2025-07-23T20:13:03+00:00</updated>
    <author>
      <name>/u/Shiro212</name>
      <uri>https://old.reddit.com/user/Shiro212</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im trying to make myself a API running on my local deepseek wth cURL. Maybe someone can help me out? Because im a new with it..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shiro212"&gt; /u/Shiro212 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7juxs/trying_to_make_an_v1chatcompletions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7juxs/trying_to_make_an_v1chatcompletions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7juxs/trying_to_make_an_v1chatcompletions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-23T20:13:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7qqqa</id>
    <title>Ollama and load balancer</title>
    <updated>2025-07-24T01:02:31+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When there is multiple servers all running Ollama and In front haproxy balancing the load. If the app is calling a different model, can haproxy see that and direct it to specific server?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7qqqa/ollama_and_load_balancer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7qqqa/ollama_and_load_balancer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7qqqa/ollama_and_load_balancer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T01:02:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7emel</id>
    <title>Mac vs PC for hosting llm locally</title>
    <updated>2025-07-23T16:56:01+00:00</updated>
    <author>
      <name>/u/trtinker</name>
      <uri>https://old.reddit.com/user/trtinker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking to buy a laptop/pc recently but can't decide whether to get a PC with gpu or just get a macbook. What do you guys think of macbook for hosting llm locally? I know that mac can host 8b models but how is the experience, is it good enough? Is macbook air sufficient or I should consider for macbook pro m4? If Im going to build a PC, then the GPU will likely be rtx3060 12gb vram as that fits my budget. Honestly I dont have a clear idea of how big the llm I'm going to host but Im planning to play around with llm for personal projects, maybe post training?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/trtinker"&gt; /u/trtinker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7emel/mac_vs_pc_for_hosting_llm_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7emel/mac_vs_pc_for_hosting_llm_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7emel/mac_vs_pc_for_hosting_llm_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-23T16:56:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1m78gkr</id>
    <title>Ollama + Open WebUI -- is there a way for the same query to run through the same model multiple times (could be 3 times, could be 100 times), then gather all the answers together to summarise/count?</title>
    <updated>2025-07-23T12:53:19+00:00</updated>
    <author>
      <name>/u/jinnyjuice</name>
      <uri>https://old.reddit.com/user/jinnyjuice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't know if it matters, but I followed this to install (because Nvidia drivers on Linux is a pain!): &lt;a href="https://github.com/NeuralFalconYT/Ollama-Open-WebUI-Windows-Installation/blob/main/README.md"&gt;https://github.com/NeuralFalconYT/Ollama-Open-WebUI-Windows-Installation/blob/main/README.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I would like to type in a query into a model with some preset system prompt. I would like that model to run over this query multiple times. Then after all of them are done, I would like for the responses to be gathered for a summary. Would such task be possible?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinnyjuice"&gt; /u/jinnyjuice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m78gkr/ollama_open_webui_is_there_a_way_for_the_same/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m78gkr/ollama_open_webui_is_there_a_way_for_the_same/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m78gkr/ollama_open_webui_is_there_a_way_for_the_same/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-23T12:53:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7w5k7</id>
    <title>RAG on large Excel files</title>
    <updated>2025-07-24T05:41:23+00:00</updated>
    <author>
      <name>/u/One-Will5139</name>
      <uri>https://old.reddit.com/user/One-Will5139</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my RAG project, large Excel files are being extracted, but when I query the data, the system responds that it doesn't exist. It seems the project fails to process or retrieve information correctly when the dataset is too large.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One-Will5139"&gt; /u/One-Will5139 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7w5k7/rag_on_large_excel_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7w5k7/rag_on_large_excel_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7w5k7/rag_on_large_excel_files/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T05:41:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7wryh</id>
    <title>RAG project fails to retrieve info from large Excel files – data ingested but not found at query time. Need help debugging.</title>
    <updated>2025-07-24T06:17:48+00:00</updated>
    <author>
      <name>/u/One-Will5139</name>
      <uri>https://old.reddit.com/user/One-Will5139</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a beginner building a RAG system and running into a strange issue with large Excel files.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt;&lt;br /&gt; When I ingest large Excel files, the system appears to extract and process the data correctly during ingestion. However, when I later query the system for specific information from those files, it responds as if the data doesn’t exist.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Details of my tech stack and setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Django&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAG/LLM Orchestration:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;LangChain for managing LLM calls, embeddings, and retrieval&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vector Store:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Qdrant (accessed via langchain-qdrant + qdrant-client)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;File Parsing:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Excel/CSV: &lt;code&gt;pandas&lt;/code&gt;, &lt;code&gt;openpyxl&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM Details:&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat Model:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;gpt-4o&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Embedding Model:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;text-embedding-ada-002&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One-Will5139"&gt; /u/One-Will5139 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7wryh/rag_project_fails_to_retrieve_info_from_large/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7wryh/rag_project_fails_to_retrieve_info_from_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7wryh/rag_project_fails_to_retrieve_info_from_large/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T06:17:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7u0yw</id>
    <title>which model to do text extraction and layout from images, that can fit on a 64 GB system using a RTX 4070 super?</title>
    <updated>2025-07-24T03:43:48+00:00</updated>
    <author>
      <name>/u/fttklr</name>
      <uri>https://old.reddit.com/user/fttklr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been trying few models with Ollama but they are way bigger than my puny 12GB VRAM card, so they run entirely on the CPU and it takes ages to do anything. As I was not able to find a way to use both GPU and CPU to improve performances I thought that maybe it is better to use a smaller model at this point.&lt;/p&gt; &lt;p&gt;Is there a suggested model that works in Ollama, that can do extraction of text from images ? Bonus points if it can replicate the layout but just text would be already enough. I was told that anything below 8B won't be doing much that is useful (and I tried with standard OCR software and they are not that useful so want to try with AI systems at this point).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fttklr"&gt; /u/fttklr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7u0yw/which_model_to_do_text_extraction_and_layout_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7u0yw/which_model_to_do_text_extraction_and_layout_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7u0yw/which_model_to_do_text_extraction_and_layout_from/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T03:43:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7zp4f</id>
    <title>Can Ollama cache processed context instead of re-parsing each time?</title>
    <updated>2025-07-24T09:22:18+00:00</updated>
    <author>
      <name>/u/Pyrore</name>
      <uri>https://old.reddit.com/user/Pyrore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm fairly new to running LLMs locally. I'm using Ollama with Open WebUI. I'm mostly running Gemma 3 27B at 4 bit quantitation and 32k context, which fits into the VRAM of my RTX 5090 laptop GPU (23/24GB). It's only 9GB if I stick to the default 2k context, so it's definitely fitting the context into VRAM.&lt;/p&gt; &lt;p&gt;The problem I have is that it seems to be processing the tokens from the conversation each prompt in the CPU (Ryzen AI 9 HX370/890M). I see the CPU load go up to around 70-80% with no GPU load. Then it switches to GPU at 100% load (I hear the fans whirring up at this point) and starts producing its response at around 15 tokens a second.&lt;/p&gt; &lt;p&gt;As the conversation progresses, the first CPU stage gets slower and slower (assumed due to the longer and longer context). The delay grows geometrically, the first 6-8k of context all run within a minute. When hit about 16k context tokens (around 12k words) it's taking the best part of an hour to process the context, but once it offloads to the GPU, it's still as fast as ever.&lt;/p&gt; &lt;p&gt;Is there any way to speed this up? E.g. by caching the processed context and simply appending to it, or shift the context processing to the GPU? One thread suggested setting the environment variable OLLAMA_NUM_PARALELL to 1 instead of the current default of 4, this was supposed to make Ollama cache the context as long as you stick to a single chat, but it didn't work.&lt;/p&gt; &lt;p&gt;Thanks in advance for any advice you can give!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pyrore"&gt; /u/Pyrore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7zp4f/can_ollama_cache_processed_context_instead_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7zp4f/can_ollama_cache_processed_context_instead_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7zp4f/can_ollama_cache_processed_context_instead_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T09:22:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7ufom</id>
    <title>My new Chrome extension lets you easily query Ollama and copy any text with a click.</title>
    <updated>2025-07-24T04:04:57+00:00</updated>
    <author>
      <name>/u/Sea-Reception-2697</name>
      <uri>https://old.reddit.com/user/Sea-Reception-2697</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Reception-2697"&gt; /u/Sea-Reception-2697 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m7u9fz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7ufom/my_new_chrome_extension_lets_you_easily_query/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7ufom/my_new_chrome_extension_lets_you_easily_query/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T04:04:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8cjcj</id>
    <title>How I got Ollama to use my GPU in Docker &amp; WSL2 (RTX 3090TI)</title>
    <updated>2025-07-24T18:39:16+00:00</updated>
    <author>
      <name>/u/lid_z</name>
      <uri>https://old.reddit.com/user/lid_z</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;Background: &lt;ol&gt; &lt;li&gt;I use &lt;a href="https://github.com/louislam/dockge"&gt;Dockge&lt;/a&gt; for managing my containers&lt;/li&gt; &lt;li&gt;I'm using my gaming PC so it needs to stay windows (until SteamOS is publicly available)&lt;/li&gt; &lt;li&gt;When I say WSL I mean WSL2. dont feel like typing the 2 every time.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Install Nvidia tools onto WSL (See instructions here: &lt;a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation"&gt;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation&lt;/a&gt; or here: &lt;a href="https://hub.docker.com/r/ollama/ollama#nvidia-gpu"&gt;https://hub.docker.com/r/ollama/ollama#nvidia-gpu&lt;/a&gt; ) &lt;ol&gt; &lt;li&gt;Open WSL terminal on the host machine&lt;/li&gt; &lt;li&gt;Follow the instructions in either of the guides linked above&lt;/li&gt; &lt;li&gt;go into docker desktop and restart the docker engine (See more here about how to do that: &lt;a href="https://docs.docker.com/reference/cli/docker/desktop/restart/"&gt;https://docs.docker.com/reference/cli/docker/desktop/restart/&lt;/a&gt; )&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Use this compose file with special attention (you shouldn't need to change anything just highlighting what makes the Nvidia GPU available in the compose) to the &amp;quot;deploy&amp;quot; &amp;amp; &amp;quot;environment&amp;quot; keys: &lt;/li&gt; &lt;/ol&gt; &lt;blockquote&gt; &lt;p&gt;&lt;code&gt;services:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;webui:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;image:&lt;/code&gt; &lt;a href="http://ghcr.io/open-webui/open-webui:main"&gt;&lt;code&gt;ghcr.io/open-webui/open-webui:main&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;container_name: webui&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ports:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- 7000:8080/tcp&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;volumes:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- open-webui:/app/backend/data&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;extra_hosts:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- host.docker.internal:host-gateway&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;depends_on:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- ollama&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;restart: unless-stopped&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;image: ollama/ollama&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;container_name: ollama&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;deploy:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;resources:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;reservations:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;devices:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- driver: nvidia&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;count: 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;capabilities:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- gpu&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;environment:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- TZ=America/New_York&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- gpus=all&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;expose:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- 11434/tcp&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ports:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- 11434:11434/tcp&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;healthcheck:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;test: ollama --version || exit 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;volumes:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- ollama:/root/.ollama&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;restart: unless-stopped&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;volumes:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama: null&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;open-webui: null&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;networks: {}&lt;/code&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lid_z"&gt; /u/lid_z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8cjcj/how_i_got_ollama_to_use_my_gpu_in_docker_wsl2_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8cjcj/how_i_got_ollama_to_use_my_gpu_in_docker_wsl2_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m8cjcj/how_i_got_ollama_to_use_my_gpu_in_docker_wsl2_rtx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T18:39:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7pahj</id>
    <title>How do HF models get to "ollama pull"?</title>
    <updated>2025-07-23T23:54:24+00:00</updated>
    <author>
      <name>/u/neurostream</name>
      <uri>https://old.reddit.com/user/neurostream</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like Hugging Face is sort of the main release hub for new models.&lt;/p&gt; &lt;p&gt;Can I point the ollama cli with an env var or other config method to pull directly from HF? &lt;/p&gt; &lt;p&gt;How do models make their way from HF to the ollama.com registry where one can access them with an &amp;quot;ollama pull&amp;quot;?&lt;/p&gt; &lt;p&gt;Are the gemma, deepseek, mistral, and qwen models on ollama.com posted there by the same official owners that first release them through HF? Like, are the popular/top listings still the &amp;quot;official&amp;quot; model, or are they re-releases by other specialty users and teams?&lt;/p&gt; &lt;p&gt;Does the GGUF format they end up in - also split in to parts/layers with the ORAS registry storage scheme used by ollama.com - entail any loss of quality or features for the same quant/architecture the HF version is?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neurostream"&gt; /u/neurostream &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7pahj/how_do_hf_models_get_to_ollama_pull/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7pahj/how_do_hf_models_get_to_ollama_pull/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7pahj/how_do_hf_models_get_to_ollama_pull/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-23T23:54:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8660w</id>
    <title>Usecase for 16GB MacBook Air M4</title>
    <updated>2025-07-24T14:39:06+00:00</updated>
    <author>
      <name>/u/Fluffy-Platform5153</name>
      <uri>https://old.reddit.com/user/Fluffy-Platform5153</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all,&lt;/p&gt; &lt;p&gt;I am looking for a model that works best for the following-&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Letter writing&lt;/li&gt; &lt;li&gt;English correction&lt;/li&gt; &lt;li&gt;Analysing images/ pdfs and extracting text&lt;/li&gt; &lt;li&gt;Answering Questions from text in PDF/ images and drafting written content based on extractions from the doc&lt;/li&gt; &lt;li&gt;NO Excel related stuff. Pure text based work&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Typical office stuff but i need a local one since data is company confidential&lt;/p&gt; &lt;p&gt;Kindly advise?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluffy-Platform5153"&gt; /u/Fluffy-Platform5153 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8660w/usecase_for_16gb_macbook_air_m4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8660w/usecase_for_16gb_macbook_air_m4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m8660w/usecase_for_16gb_macbook_air_m4/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T14:39:06+00:00</published>
  </entry>
</feed>
