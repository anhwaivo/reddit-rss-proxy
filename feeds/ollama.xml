<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-13T07:48:34+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1in88nw</id>
    <title>How many Ollama models can I have on my list.. but just running one at a time. That are 7b and I have 16 GB of RAM.. I run the Ollama via WSL. I have two models but wondering if I can fit several but just use one at a time..</title>
    <updated>2025-02-11T20:17:36+00:00</updated>
    <author>
      <name>/u/Emergency-Radish-696</name>
      <uri>https://old.reddit.com/user/Emergency-Radish-696</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emergency-Radish-696"&gt; /u/Emergency-Radish-696 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1in88nw/how_many_ollama_models_can_i_have_on_my_list_but/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1in88nw/how_many_ollama_models_can_i_have_on_my_list_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1in88nw/how_many_ollama_models_can_i_have_on_my_list_but/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T20:17:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ina9uj</id>
    <title>Quickly deploy Ollama on the most affordable GPUs on the market</title>
    <updated>2025-02-11T21:41:08+00:00</updated>
    <author>
      <name>/u/Dylan-from-Shadeform</name>
      <uri>https://old.reddit.com/user/Dylan-from-Shadeform</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We made a template on our platform, Shadeform, to quickly deploy Ollama on the most affordable cloud GPUs on the market.&lt;/p&gt; &lt;p&gt;For context, Shadeform is a GPU marketplace for cloud providers like Lambda, Paperspace, Nebius, Datacrunch and more that lets you compare their on-demand pricing and spin up with one account.&lt;/p&gt; &lt;p&gt;This Ollama template lets you pre-load Ollama onto any of these instances, so it's ready to go as soon as the instance is active.&lt;/p&gt; &lt;p&gt;Takes &amp;lt; 5 min and works like butter.&lt;/p&gt; &lt;p&gt;Here's how it works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Follow &lt;a href="https://platform.shadeform.ai/templates/a1aaa5e1-d1ec-42ed-9261-ed69778cfa5a"&gt;this link&lt;/a&gt; to the Ollama template.&lt;/li&gt; &lt;li&gt;Click &amp;quot;Deploy Template&amp;quot;&lt;/li&gt; &lt;li&gt;Pick a GPU type&lt;/li&gt; &lt;li&gt;Pick the lowest priced listing&lt;/li&gt; &lt;li&gt;Click &amp;quot;Deploy&amp;quot;&lt;/li&gt; &lt;li&gt;Wait for the instance to become active&lt;/li&gt; &lt;li&gt;Download your private key and SSH&lt;/li&gt; &lt;li&gt;Run this command, and swap out the {model_name} with whatever you want&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker exec -it ollama ollama pull {model_name} &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Paste &lt;a href="http://localhost:8080"&gt;&lt;code&gt;http://localhost:8080&lt;/code&gt;&lt;/a&gt; into your browser&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dylan-from-Shadeform"&gt; /u/Dylan-from-Shadeform &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ina9uj/quickly_deploy_ollama_on_the_most_affordable_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ina9uj/quickly_deploy_ollama_on_the_most_affordable_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ina9uj/quickly_deploy_ollama_on_the_most_affordable_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T21:41:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1inb3pe</id>
    <title>Ollama spitting out gibberish on Windows 10 with RTX 3060. Only returning @ 'at' symbols to any and all prompts. How do I fix it?</title>
    <updated>2025-02-11T22:15:34+00:00</updated>
    <author>
      <name>/u/shittywhopper</name>
      <uri>https://old.reddit.com/user/shittywhopper</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shittywhopper"&gt; /u/shittywhopper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imgur.com/a/CErnNdv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inb3pe/ollama_spitting_out_gibberish_on_windows_10_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inb3pe/ollama_spitting_out_gibberish_on_windows_10_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T22:15:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1inmmba</id>
    <title>Trying to setup Scourhead (an ai that can search the web) with Ollama but does not seem to work</title>
    <updated>2025-02-12T08:44:00+00:00</updated>
    <author>
      <name>/u/Medo1024</name>
      <uri>https://old.reddit.com/user/Medo1024</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to setup the app scourhead on my laptop (windows) and after download it says it needs Ollama and wants to download it, when i click on download it gives me a message that says 'scourhead was unable to download the model from Ollama, please insure Ollama is running, that the host and port are correct, and the model name is valid, then try again.' I checked the settings for the download and this is it 'Ollama Host: localhost OllamaPort: 11434 Model: llama3.2:3b. Pls help (ps: tried to download ollama and then restart the scourhead app but it still did not work)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Medo1024"&gt; /u/Medo1024 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inmmba/trying_to_setup_scourhead_an_ai_that_can_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inmmba/trying_to_setup_scourhead_an_ai_that_can_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inmmba/trying_to_setup_scourhead_an_ai_that_can_search/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T08:44:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1inofh0</id>
    <title>How to deploy deepseek-r1∶671b locally using Ollama?</title>
    <updated>2025-02-12T11:04:46+00:00</updated>
    <author>
      <name>/u/U2509</name>
      <uri>https://old.reddit.com/user/U2509</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 8 A100, each with 40GB video memory, and 1TB of RAM. How to deploy deepseek-r1∶671b locally? I cannot load the model using the video memory alone. Is there any parameter that Ollama can configure to load the model using my 1TB of RAM? thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/U2509"&gt; /u/U2509 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inofh0/how_to_deploy_deepseekr1671b_locally_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inofh0/how_to_deploy_deepseekr1671b_locally_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inofh0/how_to_deploy_deepseekr1671b_locally_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T11:04:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1inw25m</id>
    <title>Ide.py</title>
    <updated>2025-02-12T17:15:01+00:00</updated>
    <author>
      <name>/u/GentReviews</name>
      <uri>https://old.reddit.com/user/GentReviews</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Made a cool community project with the goal of making an interesting ollama based agentic tool using cli -based on aider, and other similar tools &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/unaveragetech/IDE.OLLAMA"&gt;https://github.com/unaveragetech/IDE.OLLAMA&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GentReviews"&gt; /u/GentReviews &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inw25m/idepy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inw25m/idepy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inw25m/idepy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T17:15:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1inw5t1</id>
    <title>Actions to query an llm</title>
    <updated>2025-02-12T17:19:01+00:00</updated>
    <author>
      <name>/u/GentReviews</name>
      <uri>https://old.reddit.com/user/GentReviews</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/unaveragetech/Gitbot"&gt;https://github.com/unaveragetech/Gitbot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Made a fun lil tool to allow anyone to ask a one shot question to an llm using GitHub codespaces, actions and a lil creativity &lt;/p&gt; &lt;p&gt;The readme explains how to use it This was made in 24hr for a small project so I’m open to changes that can be made if you have ideas &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GentReviews"&gt; /u/GentReviews &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inw5t1/actions_to_query_an_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inw5t1/actions_to_query_an_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inw5t1/actions_to_query_an_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T17:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1inyagc</id>
    <title>Environment variables on Windows 11</title>
    <updated>2025-02-12T18:44:15+00:00</updated>
    <author>
      <name>/u/buddy1616</name>
      <uri>https://old.reddit.com/user/buddy1616</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm running into an issue getting Ollama to respect environment variables on Windows 11. Running it on my Windows 10 machine, everything works fine, but the same setup gets ignored on 11. Trying to set the OLLAMA_MODELS folder, the OLLAMA_KEEP_ALIVE and the OLLAMA_HOST value. On my laptop running windows 10 they get honored after you run Ollama from the start bar (but oddly enough running it from CMD just defaults everything). On the windows 11 machine, I can't get it to pull them in at all. Any ideas or known issues with Ollama on Windows 11?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/buddy1616"&gt; /u/buddy1616 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inyagc/environment_variables_on_windows_11/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inyagc/environment_variables_on_windows_11/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inyagc/environment_variables_on_windows_11/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T18:44:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1invo23</id>
    <title>Best Model for Text Understanding</title>
    <updated>2025-02-12T16:59:43+00:00</updated>
    <author>
      <name>/u/KhoteSikke</name>
      <uri>https://old.reddit.com/user/KhoteSikke</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! What are some good models in Ollama for Text understanding. Basically understading a text and generating a JSON. Preferably similar to llama3 instruct. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KhoteSikke"&gt; /u/KhoteSikke &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1invo23/best_model_for_text_understanding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1invo23/best_model_for_text_understanding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1invo23/best_model_for_text_understanding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T16:59:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1incg27</id>
    <title>One-liner RAG with Ollama</title>
    <updated>2025-02-11T23:13:25+00:00</updated>
    <author>
      <name>/u/yusufcanbayrak</name>
      <uri>https://old.reddit.com/user/yusufcanbayrak</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1incg27/oneliner_rag_with_ollama/"&gt; &lt;img alt="One-liner RAG with Ollama" src="https://external-preview.redd.it/aZg7arYZxu_ozI5IITwuT0FrG_0ip5ZXROM-WLJsfoU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe965c43d4c500c5565b2da8db64a4f480271700" title="One-liner RAG with Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've created tlm almost a year ago as an experimental project for CLI assistance. Now, introduce another feature that can be beneficial and more natural to use for RAG with open-source models using Ollama.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/yusufcanb/tlm/releases/tag/1.2"&gt;Release 1.2 · yusufcanb/tlm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/s3nufytdelie1.gif"&gt;tlm ask&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yusufcanbayrak"&gt; /u/yusufcanbayrak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1incg27/oneliner_rag_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1incg27/oneliner_rag_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1incg27/oneliner_rag_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-11T23:13:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1io39zy</id>
    <title>Ollama slow with open webui</title>
    <updated>2025-02-12T22:10:03+00:00</updated>
    <author>
      <name>/u/alde27</name>
      <uri>https://old.reddit.com/user/alde27</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run DeepSeek with Ollama on my Mac, and then subscribe to a VPS hosted by HotSinger to run deekseep and ollama in a private server. Fir both open webUI is extremely slow when using DeepSeek, taking a very long time to respond. However, the same model performs quickly when accessed through the terminal. Do you know why?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alde27"&gt; /u/alde27 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1io39zy/ollama_slow_with_open_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1io39zy/ollama_slow_with_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1io39zy/ollama_slow_with_open_webui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T22:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1inqxnl</id>
    <title>I want to introduce Telemarketers to Ollama - Anyone else do this yet?</title>
    <updated>2025-02-12T13:36:22+00:00</updated>
    <author>
      <name>/u/Comfortable_Ad_8117</name>
      <uri>https://old.reddit.com/user/Comfortable_Ad_8117</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I get upwards of 15 Telemarketing calls a day from people who want me to buy home solar, small business loans, sell my house, donate to the fraternal order of the police and more. &lt;/p&gt; &lt;p&gt;I would love to have the local LLM answer the phone - convert speech to text, generate a response then text back to speech in close to real time. I'm not sure if this is even possible, let alone is my hardware capable. &lt;/p&gt; &lt;p&gt;I have a decent Ryzen 7 64GB with a pair of RTX 3060's 12GB &lt;/p&gt; &lt;p&gt;Has anyone done this before? &lt;/p&gt; &lt;p&gt;How do you get the PC to answer the phone? I'm assuming you have to forward the calls to some kind of Google Voice number or some VOIP service with an API that can pickup the call?&lt;/p&gt; &lt;p&gt;If you can get the PC to answer the phone what would be used to handle the STT and TTS aspect and be fast enough? &lt;/p&gt; &lt;p&gt;I would love to hear from someone who has attempted this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable_Ad_8117"&gt; /u/Comfortable_Ad_8117 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inqxnl/i_want_to_introduce_telemarketers_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inqxnl/i_want_to_introduce_telemarketers_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inqxnl/i_want_to_introduce_telemarketers_to_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T13:36:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1inpm0l</id>
    <title>1-Click AI Tools in your browser - completely free to use with Ollama</title>
    <updated>2025-02-12T12:26:23+00:00</updated>
    <author>
      <name>/u/rajatrocks</name>
      <uri>https://old.reddit.com/user/rajatrocks</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1inpm0l/1click_ai_tools_in_your_browser_completely_free/"&gt; &lt;img alt="1-Click AI Tools in your browser - completely free to use with Ollama" src="https://external-preview.redd.it/Kc2hxFEu0Tcm7R53gZtdKq4h35EjhCxOfec1FTNUVPw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6b5d82b208e81c0db81ed49a653153e474089379" title="1-Click AI Tools in your browser - completely free to use with Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there - I built a Chrome/Edge extension called Ask Steve: &lt;a href="https://asksteve.to/"&gt;https://asksteve.to&lt;/a&gt; that gives you 1-Click AI Tools in your browser (along with Chat and several other integration points).&lt;/p&gt; &lt;p&gt;I recently added the ability to connect to local models for free and it works great with Ollama! Detailed instructions are here: &lt;a href="https://www.asksteve.to/docs/local-models"&gt;https://www.asksteve.to/docs/local-models&lt;/a&gt; - it does require a bit of additional config at startup to enable an extension to connect to Ollama's local server.&lt;/p&gt; &lt;p&gt;You can also assign specific models to Tools - so you can use a fast model like Phi for everyday Tools, and something like DeepSeek R1 for something that would benefit from a reasoning model.&lt;/p&gt; &lt;p&gt;If you get a chance to try it out, I'd welcome any feedback!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1inpm0l/video/li2i506cbpie1/player"&gt;Connect Ask Steve to Ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;0:00 - 1:18 Intro &amp;amp; Initial setup&lt;br /&gt; 2:26 - 3:10 Connect Ollama&lt;br /&gt; 4:00 - 5:56 Testing &amp;amp; assigning a specific model to a specific Tool&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rajatrocks"&gt; /u/rajatrocks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inpm0l/1click_ai_tools_in_your_browser_completely_free/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inpm0l/1click_ai_tools_in_your_browser_completely_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inpm0l/1click_ai_tools_in_your_browser_completely_free/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T12:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1io8qdu</id>
    <title>Best Model for Assisting with Novel Writing</title>
    <updated>2025-02-13T02:24:12+00:00</updated>
    <author>
      <name>/u/robinhoodrefugee</name>
      <uri>https://old.reddit.com/user/robinhoodrefugee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, my use case is getting help with writing a full length novel (75,000 - 100,000+ characters). The idea is not to have the LLM write text for me. I want to be feeding my own writing in, along with plot devices, character traits, setting information, conflicts, arcs, themes, etc., so that I can then query it later down the line and ensure I'm consistent in my writing. For example, &amp;quot;when John reveals where the money is, does the location make sense?&amp;quot;&lt;/p&gt; &lt;p&gt;ChatGPT has memory issues remembering this much text so I am turning to offline LLMs. I just installed Ollama. I tried installing deepseek-r1:7b but the install progress kept going up and down and it never completed. It got up to like 2% install (peaked at 130mb out of 4.7gb) and then actually went back down to 0%. It did this multiple times before I finally gave up.&lt;/p&gt; &lt;p&gt;Here are my specs: GPU: Intel UHD Graphics 620 and 1.17TB free of hard disc space out of 1.81TB. I have 32GB of RAM.&lt;/p&gt; &lt;p&gt;Can someone recommend a model that will meet my needs and specs? Again, I want it to be able to remember everything I tell it about my story, so I'm not sure what's going to be appropriate for this use case. I am brand new to LLMs besides ChatGPT which I've been using for less than six months.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robinhoodrefugee"&gt; /u/robinhoodrefugee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1io8qdu/best_model_for_assisting_with_novel_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1io8qdu/best_model_for_assisting_with_novel_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1io8qdu/best_model_for_assisting_with_novel_writing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T02:24:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1io9k5e</id>
    <title>AI Art</title>
    <updated>2025-02-13T03:07:25+00:00</updated>
    <author>
      <name>/u/Worldly_Cockroach_49</name>
      <uri>https://old.reddit.com/user/Worldly_Cockroach_49</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just watched the video below on RAG vs. Fine Tuning. Here were some of my thoughts that came from it. I really haven’t learned anything yet about LLM, but I wanted to share this idea. &lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/00Q0G84kq3M?si=prvNCZsHywxiqTo1"&gt;https://youtu.be/00Q0G84kq3M?si=prvNCZsHywxiqTo1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I can do AI art. I’ve been taking a lot of pictures, saving videos, listening, speaking, writing and all of the above. I’ve started to take data on my self and I think I get into a habit of recording more. I want to be able to hear my inner voice. I could use all these forms of data to train an ai model. It would all be local and the model could only pull from there. A RAG model seems to be the most fitting compared to fine tuning. If I were doing an exhibition, I could have the RAG model, representing myself without societal constraints or personal identities. Then, I could have a fine-tuned model + RAG for each type of hat I have to wear, older brother, pre-med student, black male, partner, etc. When exploring the exhibit, the audience can see how based on the hat I’m wearing at that time I answer differently. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worldly_Cockroach_49"&gt; /u/Worldly_Cockroach_49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1io9k5e/ai_art/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1io9k5e/ai_art/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1io9k5e/ai_art/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T03:07:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ingiis</id>
    <title>GitHub Actions + Ollama = Free Compute</title>
    <updated>2025-02-12T02:27:09+00:00</updated>
    <author>
      <name>/u/Silent-Treat-6512</name>
      <uri>https://old.reddit.com/user/Silent-Treat-6512</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you guys do when you are bored? I created a simple AI bot which runs a full Ollama stack in Github Actions (free compute), pulls mistral model and ask for &amp;quot;some deep insight&amp;quot; this website now gets updated EVERY HOUR (Changed it to Daily) - Cost to run $0 &lt;/p&gt; &lt;p&gt;&lt;a href="https://ai.aww.sm/"&gt;https://ai.aww.sm/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Full code on GitHub, link on website. Let me know your thoughts.&lt;/p&gt; &lt;p&gt;It’s currently tasked to generate thoughts around Humans vs AI dominance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silent-Treat-6512"&gt; /u/Silent-Treat-6512 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ingiis/github_actions_ollama_free_compute/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ingiis/github_actions_ollama_free_compute/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ingiis/github_actions_ollama_free_compute/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T02:27:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1iob5gc</id>
    <title>Persisting trained model</title>
    <updated>2025-02-13T04:34:32+00:00</updated>
    <author>
      <name>/u/Confident-Mistake400</name>
      <uri>https://old.reddit.com/user/Confident-Mistake400</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apology in advance for asking a basic question. I’m new to LLMA and finished setting up ollama and open-ui in two separate docker containers. I downloaded two models (deepseek r1 and mistral 7b) and they both are stored on mounted volume. Both are up and running just fine. The issue i’m running into is, the data I feed to the models only lasted for that chat session. How do i train the models so that trained data persists across different chat sessions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Confident-Mistake400"&gt; /u/Confident-Mistake400 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iob5gc/persisting_trained_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iob5gc/persisting_trained_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iob5gc/persisting_trained_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T04:34:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1io4lsp</id>
    <title>I use a 7900xt on Windows...how stupid am I?</title>
    <updated>2025-02-12T23:06:57+00:00</updated>
    <author>
      <name>/u/halfam</name>
      <uri>https://old.reddit.com/user/halfam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How much pain am I in with this combo? What are some models I can run with it? I know AMD wasn't supported before but has it gotten better? I know me using Windows makes it even worse.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/halfam"&gt; /u/halfam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1io4lsp/i_use_a_7900xt_on_windowshow_stupid_am_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1io4lsp/i_use_a_7900xt_on_windowshow_stupid_am_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1io4lsp/i_use_a_7900xt_on_windowshow_stupid_am_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T23:06:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioc1gz</id>
    <title>Help creating Modelfile without .txt extension !</title>
    <updated>2025-02-13T05:28:05+00:00</updated>
    <author>
      <name>/u/rokudevice</name>
      <uri>https://old.reddit.com/user/rokudevice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im following a tutorial to run a Hugging Face Model with Ollama. I get to the point where I type &amp;quot;ollama create uncensored_wizard_7b -f .\Modelfile&amp;quot; but i get an error saying &amp;quot;no Modelfile or safetensors files found&amp;quot;.&lt;/p&gt; &lt;p&gt;He says in the video to make sure NOT to save the Modelfile as a txt file, so I made sure to delete that from the name of the file, however it still saves it as a .txt. So when I do run the model and i ask it stuff, it just responds with gibberish or blank statements.&lt;/p&gt; &lt;p&gt;How do I fix this? What file extension/format do I use??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rokudevice"&gt; /u/rokudevice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioc1gz/help_creating_modelfile_without_txt_extension/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ioc1gz/help_creating_modelfile_without_txt_extension/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ioc1gz/help_creating_modelfile_without_txt_extension/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T05:28:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1io3c9b</id>
    <title>ollama and ollama_host variable</title>
    <updated>2025-02-12T22:12:38+00:00</updated>
    <author>
      <name>/u/DifficultTomatillo29</name>
      <uri>https://old.reddit.com/user/DifficultTomatillo29</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have the most annoying little problem with ollama. I'm on a Mac - and I use it to host ollama for a pc, the pc not having any gpu to speak of. If I kill ollama, export the environment variable OLLAMA_HOST &amp;quot;0.0.0.0:11434&amp;quot; - and run ollama serve - everything works.. but if I run ollama by just clicking on the app - there's no way to inject the environment variable - and I can't find any way to just globally set ollama to always run like that - is there some sort of .config file or something that ollama supports - &lt;/p&gt; &lt;p&gt;ie I don't want it to be possible on my machine to run ollama and _not_ have it listen to other machines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DifficultTomatillo29"&gt; /u/DifficultTomatillo29 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1io3c9b/ollama_and_ollama_host_variable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1io3c9b/ollama_and_ollama_host_variable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1io3c9b/ollama_and_ollama_host_variable/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T22:12:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1io1hjo</id>
    <title>URL to screenshots server for your self-hosted AI projects (MIT license)</title>
    <updated>2025-02-12T20:54:56+00:00</updated>
    <author>
      <name>/u/gkamer8</name>
      <uri>https://old.reddit.com/user/gkamer8</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1io1hjo/url_to_screenshots_server_for_your_selfhosted_ai/"&gt; &lt;img alt="URL to screenshots server for your self-hosted AI projects (MIT license)" src="https://external-preview.redd.it/k5m2RHvy9bfmZbcB7x2-uONoP8GvfJMHaFfCG-IoXcw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3ba3c756d03047423ca7edf67a7a613af44994a" title="URL to screenshots server for your self-hosted AI projects (MIT license)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gkamer8"&gt; /u/gkamer8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/goodreasonai/ScrapeServ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1io1hjo/url_to_screenshots_server_for_your_selfhosted_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1io1hjo/url_to_screenshots_server_for_your_selfhosted_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T20:54:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iocfx5</id>
    <title>Kimi.ai</title>
    <updated>2025-02-13T05:54:04+00:00</updated>
    <author>
      <name>/u/Training-Regular9096</name>
      <uri>https://old.reddit.com/user/Training-Regular9096</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iocfx5/kimiai/"&gt; &lt;img alt="Kimi.ai" src="https://preview.redd.it/c9murwgoiuie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=99592dd2bf8a4816be93f1e6a22bebdfbf777757" title="Kimi.ai" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just tried few problems for coding and it seems like a pretty decent model. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Training-Regular9096"&gt; /u/Training-Regular9096 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c9murwgoiuie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iocfx5/kimiai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iocfx5/kimiai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-13T05:54:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1inp06h</id>
    <title>AMD 395 can run llama 70B without GPU</title>
    <updated>2025-02-12T11:51:44+00:00</updated>
    <author>
      <name>/u/grigio</name>
      <uri>https://old.reddit.com/user/grigio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Non enough but a good start&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/AMDGPU_/status/1889588690214637747"&gt;https://x.com/AMDGPU_/status/1889588690214637747&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grigio"&gt; /u/grigio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inp06h/amd_395_can_run_llama_70b_without_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inp06h/amd_395_can_run_llama_70b_without_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inp06h/amd_395_can_run_llama_70b_without_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T11:51:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1io31bf</id>
    <title>Promptable object tracking robot, built with Moondream &amp; OpenCV Optical Flow (open source)</title>
    <updated>2025-02-12T22:00:02+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1io31bf/promptable_object_tracking_robot_built_with/"&gt; &lt;img alt="Promptable object tracking robot, built with Moondream &amp;amp; OpenCV Optical Flow (open source)" src="https://external-preview.redd.it/dHlrejFmeXk1c2llMTEDy-zmwY-2zxEHn6L-Fnq1X838PMp4mnmxIFCi0bu_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7dd1c2a72530eab6a7d43b6046676f4d888b4d96" title="Promptable object tracking robot, built with Moondream &amp;amp; OpenCV Optical Flow (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vy021eyy5sie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1io31bf/promptable_object_tracking_robot_built_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1io31bf/promptable_object_tracking_robot_built_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T22:00:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1inx1k8</id>
    <title>Ollama on mini PC Intel Ultra 5</title>
    <updated>2025-02-12T17:54:47+00:00</updated>
    <author>
      <name>/u/Parenormale</name>
      <uri>https://old.reddit.com/user/Parenormale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1inx1k8/ollama_on_mini_pc_intel_ultra_5/"&gt; &lt;img alt="Ollama on mini PC Intel Ultra 5" src="https://preview.redd.it/corfx7mcyqie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e5d130a0a99c468aaeed36474580d1582b3e45e" title="Ollama on mini PC Intel Ultra 5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;with arc and ipex-llm I feel like an alien in the AI ​​llm context I spent €600 it's mini it consumes 50w it flies and it's precise, here I published all my tests with the various language models &lt;/p&gt; &lt;p&gt;I think the performance is great for this little GPU accelerated PC.&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtube.com/@onlyrottami?si=MSYffeaGo0axCwh9"&gt;https://youtube.com/@onlyrottami?si=MSYffeaGo0axCwh9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Parenormale"&gt; /u/Parenormale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/corfx7mcyqie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1inx1k8/ollama_on_mini_pc_intel_ultra_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1inx1k8/ollama_on_mini_pc_intel_ultra_5/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-12T17:54:47+00:00</published>
  </entry>
</feed>
