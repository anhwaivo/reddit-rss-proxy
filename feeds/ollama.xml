<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-06T23:23:34+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1iitfiv</id>
    <title>Function Calling in Terminal + DeepSeek-R1-Distill-Llama-70B-Q_8 + vLLM -&gt; Sometimes...</title>
    <updated>2025-02-06T03:43:17+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7h5utciiwfhe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iitfiv/function_calling_in_terminal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iitfiv/function_calling_in_terminal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T03:43:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij4cr1</id>
    <title>[Guide] Mac Pro 2019 (MacPro7,1) w/ Linux &amp; Local LLM/AI [Link]</title>
    <updated>2025-02-06T14:54:53+00:00</updated>
    <author>
      <name>/u/Faisal_Biyari</name>
      <uri>https://old.reddit.com/user/Faisal_Biyari</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/macpro/comments/1ij3k4s/guide_mac_pro_2019_macpro71_w_linux_local_llmai/"&gt;https://www.reddit.com/r/macpro/comments/1ij3k4s/guide_mac_pro_2019_macpro71_w_linux_local_llmai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Worth Checking out. I did not know the best subreddit to post.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Faisal_Biyari"&gt; /u/Faisal_Biyari &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij4cr1/guide_mac_pro_2019_macpro71_w_linux_local_llmai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij4cr1/guide_mac_pro_2019_macpro71_w_linux_local_llmai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij4cr1/guide_mac_pro_2019_macpro71_w_linux_local_llmai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T14:54:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij5biv</id>
    <title>How can I run ollama webUI using an RX 7900 XTX?</title>
    <updated>2025-02-06T15:36:28+00:00</updated>
    <author>
      <name>/u/Altruistic-Pickle-57</name>
      <uri>https://old.reddit.com/user/Altruistic-Pickle-57</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is only using the cpu and it is really slow, is there any way to make it run with the gpu?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altruistic-Pickle-57"&gt; /u/Altruistic-Pickle-57 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij5biv/how_can_i_run_ollama_webui_using_an_rx_7900_xtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij5biv/how_can_i_run_ollama_webui_using_an_rx_7900_xtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij5biv/how_can_i_run_ollama_webui_using_an_rx_7900_xtx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T15:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1iilug6</id>
    <title>Ollama + DataBridge: Creating an interactive learning platform under 2 minutes!</title>
    <updated>2025-02-05T21:46:26+00:00</updated>
    <author>
      <name>/u/yes-no-maybe_idk</name>
      <uri>https://old.reddit.com/user/yes-no-maybe_idk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=tfqIa_6lqQU"&gt;https://www.youtube.com/watch?v=tfqIa_6lqQU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Learn how to turn any video into an interactive learning tool with Databridge! In this demo, we'll show you how to ingest a lecture video and generate engaging questions with DataBridge, all locally Using Ollama and DataBridge.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/databridge-org/databridge-core"&gt;https://github.com/databridge-org/databridge-core&lt;/a&gt;&lt;br /&gt; Docs: &lt;a href="https://databridge.gitbook.io/databridge-docs"&gt;https://databridge.gitbook.io/databridge-docs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear comments, see you build cool stuff (or maybe even contribute to our OSS library).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yes-no-maybe_idk"&gt; /u/yes-no-maybe_idk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iilug6/ollama_databridge_creating_an_interactive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iilug6/ollama_databridge_creating_an_interactive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iilug6/ollama_databridge_creating_an_interactive/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T21:46:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij5pvy</id>
    <title>Getting "@@@@@@" output while making inference on deepseek-r1:32b locally with Ollama. Does anyone knows how to solve it?</title>
    <updated>2025-02-06T15:53:33+00:00</updated>
    <author>
      <name>/u/D4v3-X</name>
      <uri>https://old.reddit.com/user/D4v3-X</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ij5pvy/getting_output_while_making_inference_on/"&gt; &lt;img alt="Getting &amp;quot;@@@@@@&amp;quot; output while making inference on deepseek-r1:32b locally with Ollama. Does anyone knows how to solve it?" src="https://preview.redd.it/a280k0d2jjhe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f38230ca5b703d2749fa39ebf56f9097b76e3141" title="Getting &amp;quot;@@@@@@&amp;quot; output while making inference on deepseek-r1:32b locally with Ollama. Does anyone knows how to solve it?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/D4v3-X"&gt; /u/D4v3-X &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a280k0d2jjhe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij5pvy/getting_output_while_making_inference_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij5pvy/getting_output_while_making_inference_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T15:53:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij1aaz</id>
    <title>Best coding models for Consumer Hardware</title>
    <updated>2025-02-06T12:22:41+00:00</updated>
    <author>
      <name>/u/Willbo_Bagg1ns</name>
      <uri>https://old.reddit.com/user/Willbo_Bagg1ns</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve recently been experimenting with Deepseek-r1:32b and Deepseek-coderv2:16b on my RTX 4090. I can run both these models with really good performance locally, but want to explore other models that can handle coding tasks.&lt;/p&gt; &lt;p&gt;What are the best coding models available through ollama? I plan to integrate one of these models with my IDE (vscode).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Willbo_Bagg1ns"&gt; /u/Willbo_Bagg1ns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij1aaz/best_coding_models_for_consumer_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij1aaz/best_coding_models_for_consumer_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij1aaz/best_coding_models_for_consumer_hardware/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T12:22:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij1d1l</id>
    <title>Do I need local AI for Electrical Engineering studying ?</title>
    <updated>2025-02-06T12:27:04+00:00</updated>
    <author>
      <name>/u/wolfson10</name>
      <uri>https://old.reddit.com/user/wolfson10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey !&lt;/p&gt; &lt;p&gt;I'm currently using ChaptGPT plus and Deepseek for Electrical Engineering studying , I'm paying for ChatGPT and still have limitations for example the O1 model , meanwhile Deepseek has a lot of &amp;quot;Server busy&amp;quot; and it's annoying but it's solving math and electrical problems great and I think even better than Chatgpt O4 model.&lt;/p&gt; &lt;p&gt;I thought maybe to try Ollama + Open WebUI + Deppseek R1 as a local chat .&lt;/p&gt; &lt;p&gt;Will I able to upload math/electrical problems images and it will analyze them and solve ?&lt;/p&gt; &lt;p&gt;What will be the best for me ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wolfson10"&gt; /u/wolfson10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij1d1l/do_i_need_local_ai_for_electrical_engineering/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij1d1l/do_i_need_local_ai_for_electrical_engineering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij1d1l/do_i_need_local_ai_for_electrical_engineering/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T12:27:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij6xse</id>
    <title>Crashing after playing with num_ctx</title>
    <updated>2025-02-06T16:43:06+00:00</updated>
    <author>
      <name>/u/ActionzheZ</name>
      <uri>https://old.reddit.com/user/ActionzheZ</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running the distilled deepseek r1:32b on a 12GB VRAM 3080Ti and 64GB of RAM. The model when running uses about 10.9GB VRAM+11GB RAM. &lt;/p&gt; &lt;p&gt;I was a bit annoyed by the model losing context clues, so I created a new modelfile with the parameter set to 10240, increasing the default 2K to 10K. This new modelfile ran fine with test scenario in terminal, though slower compared to 2K as expected.&lt;/p&gt; &lt;p&gt;I'm using Ollama with Chatbox AI, so I opened an existing conversation with roughly 15k existing tokens, switched to the new modelfile, and gave prompt. It worked fine the for the first prompt requiring some previous context larger than 2K. I saw it working, so then I gave it a follow up, the model crashed. Repeating would crash again. I figured ok maybe the context is too much, so I created another one with context 4k. To my surprise, this crashed as well. I then simply used the original model, but it is now also crashing! Wtf? The original model was running fine on my machine before. &lt;/p&gt; &lt;p&gt;I have plenty of RAM left to spare, so while I may be out of dedicated VRAM I'm certainly nowhere near exceeding shared VRAM. Even if somehow the larger context does not behave when dedicated VRAM is full, running the default settings should be fine as it was working before...? Why I'm I suddenly crashing on everything? The crashing seems to be happening right as the &amp;lt;think&amp;gt; is about to start populate texts...&lt;/p&gt; &lt;p&gt;Did I accidentally mess up a setting somewhere in the original model when I did all that modelfiles ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ActionzheZ"&gt; /u/ActionzheZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij6xse/crashing_after_playing_with_num_ctx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij6xse/crashing_after_playing_with_num_ctx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij6xse/crashing_after_playing_with_num_ctx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T16:43:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij8vhn</id>
    <title>Ollama still downloads models into ~/.ollama</title>
    <updated>2025-02-06T18:01:17+00:00</updated>
    <author>
      <name>/u/TheTobruk</name>
      <uri>https://old.reddit.com/user/TheTobruk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, following &lt;a href="https://github.com/ollama/ollama/issues/680"&gt;this thread&lt;/a&gt; I made changes to my systemctl ollama service:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[Service] WorkingDirectory=/home/deutschegabanna/external.dump.d/ollama Environment=&amp;quot;HOME=/home/deutschegabanna/external.dump.d/ollama Environment=&amp;quot;OLLAMA_MODELS=/home/deutschegabanna/external.dump.d/ollama&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And given necessary permissions for the new folder:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; ls -al ~/external.dump.d/ollama drwxrwxr-x 2 ollama ollama 4096 Feb 3 14:50 . &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Yet ollama still downloads the models into the wrong directory - namely ~/.ollama. Why? I've restarted my computer since making those changes so the systemctl runner-daemon and all services should load up with new settings.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; ollama serve 2025/02/06 18:59:08 routes.go:1187: INFO server config env=&amp;quot;map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/deutschegabanna/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]&amp;quot; time=2025-02-06T18:59:08.851+01:00 level=INFO source=images.go:432 msg=&amp;quot;total blobs: 0&amp;quot; time=2025-02-06T18:59:08.851+01:00 level=INFO source=images.go:439 msg=&amp;quot;total unused blobs removed: 0&amp;quot; time=2025-02-06T18:59:08.852+01:00 level=INFO source=routes.go:1238 msg=&amp;quot;Listening on 127.0.0.1:11434 (version 0.5.7)&amp;quot; &amp;gt; echo $OLLAMA_MODELS &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If I forcefully export the environment variable for the duration of the boot, I get this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Error: mkdir /home/deutschegabanna/external.dump.d/ollama/blobs: permission denied &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheTobruk"&gt; /u/TheTobruk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij8vhn/ollama_still_downloads_models_into_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij8vhn/ollama_still_downloads_models_into_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij8vhn/ollama_still_downloads_models_into_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T18:01:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiycpb</id>
    <title>How to Ollama on-demand?</title>
    <updated>2025-02-06T09:02:35+00:00</updated>
    <author>
      <name>/u/RamenKomplex</name>
      <uri>https://old.reddit.com/user/RamenKomplex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;We would like to run a ollama server for our team and let team members use it via the openweb ui. We can create a new VM on Aws and run it there but since it won't be used all the time, this would be a waste of the rather expensive resources. &lt;/p&gt; &lt;p&gt;Are there any smarter ways of deploying ollama based openwebui for team use in a way that we only pay for the GPU/CPU only when chat is being used?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RamenKomplex"&gt; /u/RamenKomplex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iiycpb/how_to_ollama_ondemand/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iiycpb/how_to_ollama_ondemand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iiycpb/how_to_ollama_ondemand/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T09:02:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijbsxq</id>
    <title>Is it possible to make a sort of "welcome message" for the deepseek model?</title>
    <updated>2025-02-06T20:00:22+00:00</updated>
    <author>
      <name>/u/wholelottaturbulenze</name>
      <uri>https://old.reddit.com/user/wholelottaturbulenze</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm wondering if I can make the deepseek model say something like &amp;quot;Hello World!&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wholelottaturbulenze"&gt; /u/wholelottaturbulenze &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijbsxq/is_it_possible_to_make_a_sort_of_welcome_message/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijbsxq/is_it_possible_to_make_a_sort_of_welcome_message/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijbsxq/is_it_possible_to_make_a_sort_of_welcome_message/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T20:00:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijc911</id>
    <title>Open WebUI: Server Connection Error</title>
    <updated>2025-02-06T20:17:49+00:00</updated>
    <author>
      <name>/u/TeTeOtaku</name>
      <uri>https://old.reddit.com/user/TeTeOtaku</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, im trying to setup Deepseek with WebUI and i have this error message everytime i try to connect. It just doesnt work.&lt;/p&gt; &lt;p&gt;I tried installing WebUI using both normal powershell and wsl but to no luck, it still doesn't make the connection. I searched the sub up and down and used different commands and either it won't find the connect or it doesn't load up WebUI at all (had one error where they told me i need backend access)&lt;/p&gt; &lt;p&gt;This is the last command i used: sudo docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=&lt;a href="http://127.0.0.1:11434"&gt;http://127.0.0.1:11434&lt;/a&gt; --name open-webui --restart always &lt;a href="http://ghcr.io/open-webui/open-webui:main"&gt;ghcr.io/open-webui/open-webui:main&lt;/a&gt; and it still didn't work&lt;/p&gt; &lt;p&gt;Any other ideas how to make it work?&lt;/p&gt; &lt;p&gt;Thank you very much&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TeTeOtaku"&gt; /u/TeTeOtaku &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijc911/open_webui_server_connection_error/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijc911/open_webui_server_connection_error/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijc911/open_webui_server_connection_error/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T20:17:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij7dmw</id>
    <title>Please help me out with this issue in downloading ollama deepseek model</title>
    <updated>2025-02-06T17:00:47+00:00</updated>
    <author>
      <name>/u/oye_ap</name>
      <uri>https://old.reddit.com/user/oye_ap</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ij7dmw/please_help_me_out_with_this_issue_in_downloading/"&gt; &lt;img alt="Please help me out with this issue in downloading ollama deepseek model" src="https://b.thumbs.redditmedia.com/QQEzKHUA0gBIZdUfvktJsyHlCUrLE0zeG37WUPx9imo.jpg" title="Please help me out with this issue in downloading ollama deepseek model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's my first time trying to download ollama and deepseek, I'm not much into tech &amp;amp; while I try to run deepseek, this is the issue that I'm facing&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r8c1h6r66khe1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d9f7fdce5488ae0a9a3554d584c0855a516ee8a5"&gt;https://preview.redd.it/r8c1h6r66khe1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d9f7fdce5488ae0a9a3554d584c0855a516ee8a5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oye_ap"&gt; /u/oye_ap &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij7dmw/please_help_me_out_with_this_issue_in_downloading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij7dmw/please_help_me_out_with_this_issue_in_downloading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij7dmw/please_help_me_out_with_this_issue_in_downloading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T17:00:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijdh12</id>
    <title>Local app to store and retrieve content</title>
    <updated>2025-02-06T21:07:43+00:00</updated>
    <author>
      <name>/u/nomadicArc</name>
      <uri>https://old.reddit.com/user/nomadicArc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I am planning to build a local web app that uses Ollama to store and retrieve memories of any types(text, media, gpx, etc). What I want to achieve is storing memories that can later be used by the llm to respond to questions related to events/persons/dates/etc.&lt;/p&gt; &lt;p&gt;After some research here's what I thought to use:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Backend:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;express js server&lt;/li&gt; &lt;li&gt;ollama&lt;/li&gt; &lt;li&gt;sqlite to store the plain text an paths for files&lt;/li&gt; &lt;li&gt;chroma db for storing embeddings&lt;/li&gt; &lt;li&gt;langchain&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Frontend:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;next js&lt;/li&gt; &lt;li&gt;tailwind&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The concerns I have are:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;how is the data going to grow and scale over the years?&lt;/li&gt; &lt;li&gt;is it better to use a current model that I always provide with context and data or to train a new model?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Since I am complete beginner with working/implementing with llm's I would really appreciate any advice I can get. Thank you for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nomadicArc"&gt; /u/nomadicArc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijdh12/local_app_to_store_and_retrieve_content/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijdh12/local_app_to_store_and_retrieve_content/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijdh12/local_app_to_store_and_retrieve_content/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T21:07:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijdp1e</id>
    <title>Reins</title>
    <updated>2025-02-06T21:16:38+00:00</updated>
    <author>
      <name>/u/Genshard23</name>
      <uri>https://old.reddit.com/user/Genshard23</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone else figured out how to connect to your local host? I installed the Mac app and it instantly found the address. Though when I go to load that address or even search the local network from my iPhone on the same wifi I get nada😝 any help would be greatly appreciated. I’m wicked new to all this &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Genshard23"&gt; /u/Genshard23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijdp1e/reins/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijdp1e/reins/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijdp1e/reins/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T21:16:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijdyka</id>
    <title>Exposing Ollama Container Port Without Authentication - Is This Secure?</title>
    <updated>2025-02-06T21:26:59+00:00</updated>
    <author>
      <name>/u/Otherwise_Debt2227</name>
      <uri>https://old.reddit.com/user/Otherwise_Debt2227</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. I have Ollama installed on a dedicated machine for AI and use a laptop to program with VSCode that has an extension using the Ollama from the dedicated machine.&lt;/p&gt; &lt;p&gt;Unfortunately, even though both machines are connected to the same local network, the Ollama container port (11434) is exposed on the network without any authentication. This means anyone with access to my local network can use my AI models without needing to authenticate.&lt;/p&gt; &lt;p&gt;Do you know of any extension that could be used in this case to enable authentication? Or would it be secure to leave things like this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Otherwise_Debt2227"&gt; /u/Otherwise_Debt2227 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijdyka/exposing_ollama_container_port_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijdyka/exposing_ollama_container_port_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijdyka/exposing_ollama_container_port_without/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T21:26:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ije4bw</id>
    <title>Rtx 3050 + i5-11th isn't enough?!</title>
    <updated>2025-02-06T21:33:23+00:00</updated>
    <author>
      <name>/u/Brief_Ticket4771</name>
      <uri>https://old.reddit.com/user/Brief_Ticket4771</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have dell g15 5511 with RTX 3050 4gb vram and i5 11th 16 RAM which local ai model from ollama is good for coding and what about performance? and will it need additional RAM ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brief_Ticket4771"&gt; /u/Brief_Ticket4771 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ije4bw/rtx_3050_i511th_isnt_enough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ije4bw/rtx_3050_i511th_isnt_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ije4bw/rtx_3050_i511th_isnt_enough/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T21:33:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij23pm</id>
    <title>Can a malicious model execute code ?</title>
    <updated>2025-02-06T13:07:27+00:00</updated>
    <author>
      <name>/u/niilzon</name>
      <uri>https://old.reddit.com/user/niilzon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Could a hypothetical malicious model (such as Deepseek or any other) execute code on a machine running Ollama (without exploiting a vulnerability in Ollama) ?&lt;/p&gt; &lt;p&gt;It's not clear to me if models used by Ollama are &amp;quot;just weights&amp;quot; or if they also embed custom dependencies, executables etc&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/niilzon"&gt; /u/niilzon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij23pm/can_a_malicious_model_execute_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij23pm/can_a_malicious_model_execute_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij23pm/can_a_malicious_model_execute_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T13:07:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijfdun</id>
    <title>NPU Support?</title>
    <updated>2025-02-06T22:26:15+00:00</updated>
    <author>
      <name>/u/TheMicrosoftMan</name>
      <uri>https://old.reddit.com/user/TheMicrosoftMan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a Surface Pro 11 with a Snapdragon X Elite chip, which has a NPU with 40 TOPS which would be a lot more power efficient and could be faster than the built in CPU and GPU. A couple months ago, Ollama didn't support NPUs, has that changed? Is there any way to add support in any method?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheMicrosoftMan"&gt; /u/TheMicrosoftMan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijfdun/npu_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijfdun/npu_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijfdun/npu_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T22:26:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij71ow</id>
    <title>[Help] Why I can't pull this model from offical ollama model page?</title>
    <updated>2025-02-06T16:47:22+00:00</updated>
    <author>
      <name>/u/josephwang123</name>
      <uri>https://old.reddit.com/user/josephwang123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to pull this model: &lt;a href="https://ollama.com/daiseur/darkidol-llama-3.1-8b_instruct-1.2"&gt;https://ollama.com/daiseur/darkidol-llama-3.1-8b_instruct-1.2&lt;/a&gt;&lt;br /&gt; Both &lt;code&gt;ollama pull&lt;/code&gt; and &lt;code&gt;ollama serve&lt;/code&gt; is failed.&lt;br /&gt; Can anybody help?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/josephwang123"&gt; /u/josephwang123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij71ow/help_why_i_cant_pull_this_model_from_offical/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij71ow/help_why_i_cant_pull_this_model_from_offical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij71ow/help_why_i_cant_pull_this_model_from_offical/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T16:47:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijbn6m</id>
    <title>Best EU alternative for lambda labs to run ollama</title>
    <updated>2025-02-06T19:54:00+00:00</updated>
    <author>
      <name>/u/SelectSpread</name>
      <uri>https://old.reddit.com/user/SelectSpread</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;we deployed ollama on lambda labs servers. Our CEO is not amused, as lambda labs is a U.S. company (cloud act, etc. not perfect when it comes to GDPR)&lt;/p&gt; &lt;p&gt;What's the best EU alternative to run on demand workloads on at least Nvidia (G)H200 GPU&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SelectSpread"&gt; /u/SelectSpread &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijbn6m/best_eu_alternative_for_lambda_labs_to_run_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijbn6m/best_eu_alternative_for_lambda_labs_to_run_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijbn6m/best_eu_alternative_for_lambda_labs_to_run_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T19:54:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij942k</id>
    <title>Could i run deepseek 16B Rtx 3070 with GTX 1080Ti?</title>
    <updated>2025-02-06T18:10:49+00:00</updated>
    <author>
      <name>/u/Efficient_Band181</name>
      <uri>https://old.reddit.com/user/Efficient_Band181</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 2 of these cards laying around as I havent done much gaming lately, the GTX is older but i thought might as well because of the 11GB of Vram.&lt;/p&gt; &lt;p&gt;Could it run at least the 16B model in parallel? Or should I just use the lesser 8GB 3070 I have alone, just want to make sure before buying the motherboard.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Efficient_Band181"&gt; /u/Efficient_Band181 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij942k/could_i_run_deepseek_16b_rtx_3070_with_gtx_1080ti/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij942k/could_i_run_deepseek_16b_rtx_3070_with_gtx_1080ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij942k/could_i_run_deepseek_16b_rtx_3070_with_gtx_1080ti/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T18:10:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij345i</id>
    <title>Can anyone help me? i really don't understand what i'm doing wrong.</title>
    <updated>2025-02-06T13:57:13+00:00</updated>
    <author>
      <name>/u/RevolutionaryBus4545</name>
      <uri>https://old.reddit.com/user/RevolutionaryBus4545</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ij345i/can_anyone_help_me_i_really_dont_understand_what/"&gt; &lt;img alt="Can anyone help me? i really don't understand what i'm doing wrong." src="https://preview.redd.it/gurn3vbgyihe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81c1c17805ce018b0fc08afcf7535c8018e411f7" title="Can anyone help me? i really don't understand what i'm doing wrong." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RevolutionaryBus4545"&gt; /u/RevolutionaryBus4545 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gurn3vbgyihe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij345i/can_anyone_help_me_i_really_dont_understand_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij345i/can_anyone_help_me_i_really_dont_understand_what/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T13:57:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij7nuo</id>
    <title>UNCENSORED AI MODELS</title>
    <updated>2025-02-06T17:12:07+00:00</updated>
    <author>
      <name>/u/yng_kydd</name>
      <uri>https://old.reddit.com/user/yng_kydd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some months ago i tried for the first time wizard vicuna and i was ok with it being a lil slow and not that optimized, i wasn't even complaining cause at least i had an AI uncensored, something i could ask for everything.&lt;/p&gt; &lt;p&gt;This week i've seen a post talking about other new models that are pretty much better like tiger gemma, dolphin and others&lt;/p&gt; &lt;p&gt;i've been searching about this for quite a lot and i'd want to ask y'all what is the best uncensored AI model right now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yng_kydd"&gt; /u/yng_kydd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij7nuo/uncensored_ai_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij7nuo/uncensored_ai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij7nuo/uncensored_ai_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T17:12:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij2pw7</id>
    <title>🎉 Being Thankful for Everyone Who Made This Project a Super Hit! 🚀</title>
    <updated>2025-02-06T13:37:59+00:00</updated>
    <author>
      <name>/u/akhilpanja</name>
      <uri>https://old.reddit.com/user/akhilpanja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are thrilled to announce that our project, DeepSeek-RAG-Chatbot, has officially hit 100 stars on GitHub repo: &lt;a href="https://github.com/SaiAkhil066/DeepSeek-RAG-Chatbot.git"&gt;https://github.com/SaiAkhil066/DeepSeek-RAG-Chatbot.git&lt;/a&gt; 🌟✨ &lt;/p&gt; &lt;p&gt;This journey has been incredible, and we couldn’t have achieved this milestone without the support of our amazing community. Your contributions, feedback, and enthusiasm have helped shape this project into what it is today!&lt;/p&gt; &lt;p&gt;🔍 Performance Boost The graph above showcases the significant improvements in Graph Context Relevancy and Graph Context Recall after integrating GraphRAG and further advancements. Our system is now more accurate, contextually aware, and efficient in retrieving relevant information.&lt;/p&gt; &lt;p&gt;We are committed to making this project even better and look forward to the next milestones! 🚀&lt;/p&gt; &lt;p&gt;Thank you all once again for being part of this journey. Let’s keep building together! 💡🔥 ￼&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akhilpanja"&gt; /u/akhilpanja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij2pw7/being_thankful_for_everyone_who_made_this_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij2pw7/being_thankful_for_everyone_who_made_this_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij2pw7/being_thankful_for_everyone_who_made_this_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T13:37:59+00:00</published>
  </entry>
</feed>
