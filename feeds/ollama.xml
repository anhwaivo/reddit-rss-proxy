<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-06-19T18:08:47+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ldgux6</id>
    <title>Expose ollama internally with https</title>
    <updated>2025-06-17T07:21:19+00:00</updated>
    <author>
      <name>/u/oturais</name>
      <uri>https://old.reddit.com/user/oturais</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. &lt;/p&gt; &lt;p&gt;I have an application that consumes openai api but only allows https endpoints. &lt;/p&gt; &lt;p&gt;Is there any easy way to configure ollama to expose the api on https?&lt;/p&gt; &lt;p&gt;I've seen some posts about creating a reverse pricy with nginx, but I'm struggling with that. Any other approach? &lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oturais"&gt; /u/oturais &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldgux6/expose_ollama_internally_with_https/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldgux6/expose_ollama_internally_with_https/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ldgux6/expose_ollama_internally_with_https/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T07:21:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldon8y</id>
    <title>UI and tools for multiuser RAG with central knowledge base</title>
    <updated>2025-06-17T14:30:44+00:00</updated>
    <author>
      <name>/u/TommyWolfheart</name>
      <uri>https://old.reddit.com/user/TommyWolfheart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi.&lt;/p&gt; &lt;p&gt;I am developing an LLM system for an organisation's documentation with Ollama and would like, when everyone in the organisation chats with the system, for it to do RAG with a central/global knowledge base.&lt;/p&gt; &lt;p&gt;Open WebUl’s documentation on RAG seems to suggest that an individual has to upload their own documents to do RAG with them.&lt;/p&gt; &lt;p&gt;I would appreciate guidance on what UI to use to achieve what I want to do. I’m very happy to use LangChain but not sure how I would go about integrating the resulting system with Open WebUI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TommyWolfheart"&gt; /u/TommyWolfheart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldon8y/ui_and_tools_for_multiuser_rag_with_central/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldon8y/ui_and_tools_for_multiuser_rag_with_central/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ldon8y/ui_and_tools_for_multiuser_rag_with_central/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T14:30:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1le1wur</id>
    <title>Iphone app</title>
    <updated>2025-06-17T23:14:22+00:00</updated>
    <author>
      <name>/u/Zealousideal_Neck317</name>
      <uri>https://old.reddit.com/user/Zealousideal_Neck317</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, i just downloaded the app and i need help First i will tell you why i want to use this ai. From my understanding these types of bots, feel free to correct me (just please do it nicely) are better for uncensored, unfiltered chat. What i want to use it for is RP. I like to chat with ai bots to creat a story, and naturally stories get to a NSFW point, sexual or violent. The bots i am currently usually (idk if i can say the name) has bee insane with the guidlimes as it calls it. Like it won’t do a simple scene of teasing! So please help me and tell me if this is a better option &lt;/p&gt; &lt;p&gt;And to my important question I opened the app and it showed me that i needed to choose a server. From your knowledge which would be best for my case, knowing what i use it for and that it is on the app not a pc&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal_Neck317"&gt; /u/Zealousideal_Neck317 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1le1wur/iphone_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1le1wur/iphone_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1le1wur/iphone_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T23:14:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld1bj1</id>
    <title>I made a macos MCP client</title>
    <updated>2025-06-16T18:53:55+00:00</updated>
    <author>
      <name>/u/SandwichConscious336</name>
      <uri>https://old.reddit.com/user/SandwichConscious336</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ld1bj1/i_made_a_macos_mcp_client/"&gt; &lt;img alt="I made a macos MCP client" src="https://preview.redd.it/q0m3hohr5c7f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=94ec86313f925cd3386804c2445f1b9f706a53c3" title="I made a macos MCP client" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am working on adding MCP support for my native macos Ollama client app. I am looking for people currently using Ollama locally (with a client or not) who are curious about MCP and would like a way to easy use MCP servers (local and remote).&lt;/p&gt; &lt;p&gt;Reply and DM me if you're interested in testing my MCP integration.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SandwichConscious336"&gt; /u/SandwichConscious336 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q0m3hohr5c7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ld1bj1/i_made_a_macos_mcp_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ld1bj1/i_made_a_macos_mcp_client/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-16T18:53:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldts6f</id>
    <title>My AI Interview Prep Side Project Now Has an "AI Coach" to Pinpoint Your Weak Skills!</title>
    <updated>2025-06-17T17:47:35+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ldts6f/my_ai_interview_prep_side_project_now_has_an_ai/"&gt; &lt;img alt="My AI Interview Prep Side Project Now Has an &amp;quot;AI Coach&amp;quot; to Pinpoint Your Weak Skills!" src="https://external-preview.redd.it/cXJkNHN3cnl5aTdmMUx0vRJ7fJWQFnAwyVuCziX8lbU-1zIfwzcA4VGtmYcd.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9ea99d70802d659868b41360d51236101f26349" title="My AI Interview Prep Side Project Now Has an &amp;quot;AI Coach&amp;quot; to Pinpoint Your Weak Skills!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Been working hard on my personal project, an AI-powered interview preparer, and just rolled out a new core feature I'm pretty excited about: the &lt;strong&gt;AI Coach&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;The main idea is to go beyond just giving you mock interview questions. After you do a practice interview in the app, this new AI Coach (which uses &lt;strong&gt;Agno agents&lt;/strong&gt; to orchestrate a local LLM like Llama/Mistral via Ollama) actually analyzes your answers to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tell you which skills you demonstrated well.&lt;/li&gt; &lt;li&gt;More importantly, &lt;strong&gt;pinpoint specific skills where you might need more work.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;It even gives you an overall score and a breakdown by criteria like accuracy, clarity, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Plus, you're not just limited to feedback after an interview. You can also &lt;strong&gt;tell the AI Coach which specific skills you want to learn or improve on&lt;/strong&gt;, and it can offer guidance or track your focus there.&lt;/p&gt; &lt;p&gt;The frontend for displaying all this feedback is built with &lt;strong&gt;React and TypeScript&lt;/strong&gt; (loving TypeScript for managing the data structures here!).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech Stack for this feature &amp;amp; the broader app:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI Coach Logic:&lt;/strong&gt; Agno agents, local LLMs (Ollama)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; Python, FastAPI, SQLAlchemy&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; React, TypeScript, Zustand, Framer Motion&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This has been a super fun challenge, especially the prompt engineering to get nuanced skill-based feedback from the LLMs and making sure the Agno agents handle the analysis flow correctly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I built this because I always wished I had more targeted feedback after practice interviews – not just &amp;quot;good job&amp;quot; but &amp;quot;you need to work on X skill specifically.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What do you guys think?&lt;/li&gt; &lt;li&gt;What kind of skill-based feedback would be most useful to you from an AI coach?&lt;/li&gt; &lt;li&gt;Anyone else playing around with Agno agents or local LLMs for complex analysis tasks?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear your thoughts, suggestions, or if you're working on something similar!&lt;/p&gt; &lt;p&gt;You can check out my previous post about the main app here: &lt;a href="https://www.reddit.com/r/ollama/comments/1ku0b3j/im_building_an_ai_interview_prep_tool_to_get_real/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;https://www.reddit.com/r/ollama/comments/1ku0b3j/im_building_an_ai_interview_prep_tool_to_get_real/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🚀 P.S. I am looking for new roles , If you like my work and have any Opportunites in Computer Vision or LLM Domain do contact me&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;My Email:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=mailto%3Apavankunchalaofficial%40gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My GitHub Profile (for more projects):&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My Resume:&lt;/strong&gt; &lt;a href="https://drive.google.com/file/d/1LVMVgAPKGUJbnrfE09OLJ0MrEZlBccOT/view"&gt;https://drive.google.com/file/d/1LVMVgAPKGUJbnrfE09OLJ0MrEZlBccOT/view&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dv6mwgtyyi7f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldts6f/my_ai_interview_prep_side_project_now_has_an_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ldts6f/my_ai_interview_prep_side_project_now_has_an_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T17:47:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldu0p0</id>
    <title>Trying to connect Ollama with WhatsApp using Node.js but no response — Where is the clear documentation?</title>
    <updated>2025-06-17T17:56:37+00:00</updated>
    <author>
      <name>/u/Oz_Ar4L</name>
      <uri>https://old.reddit.com/user/Oz_Ar4L</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am completely new to this and have no formal programming experience, but I am trying a simple personal project:&lt;br /&gt; I want a bot to read messages coming through WhatsApp (using whatsapp-web.js) and respond using a local Ollama model that I have customized (called &amp;quot;Nergal&amp;quot;).&lt;/p&gt; &lt;p&gt;The WhatsApp part already works. The bot responds to simple commands like &amp;quot;Hi Nergal&amp;quot; and &amp;quot;Bye Nergal.&amp;quot;&lt;br /&gt; What I can’t get to work is connecting to Ollama so it responds based on the user’s message.&lt;/p&gt; &lt;p&gt;I have been searching for days but can’t find clear and straightforward documentation on how to integrate Ollama into a Node.js bot.&lt;/p&gt; &lt;p&gt;Does anyone have a working example or know where I can read documentation that explains how to do it?&lt;/p&gt; &lt;p&gt;I really appreciate any guidance. 🙏&lt;/p&gt; &lt;pre&gt;&lt;code&gt;const qrcode = require('qrcode-terminal'); const { Client, LocalAuth } = require('whatsapp-web.js'); const ollama = require('ollama') const client = new Client({ authStrategy: new LocalAuth() }); client.on('qr', qr =&amp;gt; { qrcode.generate(qr, {small: true}); }); client.on('ready', () =&amp;gt; { console.log('Nergal is Awake!'); }); client.on('message_create', message =&amp;gt; { if (message.body === 'Hi N') { // send back &amp;quot;pong&amp;quot; to the chat the message was sent in client.sendMessage(message.from, 'Hello User'); } if (message.body === 'Bye N') { // send back &amp;quot;pong&amp;quot; to the chat the message was sent in client.sendMessage(message.from, 'Bye User'); } if (message.body.toLowerCase().includes('Nergal')) { async function generarTexto() { const response = await ollama.chat({ model: 'Nergal', messages: [{ role: 'user', content: 'What is Nergal?' }] }) console.log(response.message.content) } generarTexto() } }); client.initialize(); &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Oz_Ar4L"&gt; /u/Oz_Ar4L &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldu0p0/trying_to_connect_ollama_with_whatsapp_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldu0p0/trying_to_connect_ollama_with_whatsapp_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ldu0p0/trying_to_connect_ollama_with_whatsapp_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T17:56:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldva49</id>
    <title>Blog: You Can’t Have an AI Strategy Without a Data Strategy</title>
    <updated>2025-06-17T18:44:47+00:00</updated>
    <author>
      <name>/u/UnderstandingTop1424</name>
      <uri>https://old.reddit.com/user/UnderstandingTop1424</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking for feedback for the blog -- &lt;a href="https://quarklabs.substack.com/p/you-cant-have-an-ai-strategy-without"&gt;https://quarklabs.substack.com/p/you-cant-have-an-ai-strategy-without&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UnderstandingTop1424"&gt; /u/UnderstandingTop1424 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldva49/blog_you_cant_have_an_ai_strategy_without_a_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldva49/blog_you_cant_have_an_ai_strategy_without_a_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ldva49/blog_you_cant_have_an_ai_strategy_without_a_data/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T18:44:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldse04</id>
    <title>Help with Llama (fairly new to this sorry)</title>
    <updated>2025-06-17T16:55:03+00:00</updated>
    <author>
      <name>/u/AdventurousReturn316</name>
      <uri>https://old.reddit.com/user/AdventurousReturn316</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can I run LLaMA 3 8B Q4 locally using Ollama or a similar tool. My laptop is a 2019 Lenovo with Windows 11 (64-bit), an Intel i5-9300H (4 cores, 8 threads), 16 GB DDR4 RAM, and an NVIDIA GTX 1650 (4GB VRAM). I’ve got a 256 GB SSD and a 1 TB HDD. Virtualization is enabled, GPU idles at ~45°C, and CPU usage sits around 8–10% when idle.&lt;/p&gt; &lt;p&gt;Can I run LLaMA 3 8B Q4 on this setup reliably? Is 16GB Ram good enough? Thank you in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdventurousReturn316"&gt; /u/AdventurousReturn316 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldse04/help_with_llama_fairly_new_to_this_sorry/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldse04/help_with_llama_fairly_new_to_this_sorry/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ldse04/help_with_llama_fairly_new_to_this_sorry/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T16:55:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldkx9x</id>
    <title>40 GPU Cluster Concurrency Test</title>
    <updated>2025-06-17T11:42:58+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ldkx9x/40_gpu_cluster_concurrency_test/"&gt; &lt;img alt="40 GPU Cluster Concurrency Test" src="https://external-preview.redd.it/5oL5EdtwMwlXnOo15ZpuIiB4WAkPdKcGlE7lZy0SHHk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e578354f37d691c35448462a42256a439ce14a9" title="40 GPU Cluster Concurrency Test" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/aq4y2p9e5h7f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldkx9x/40_gpu_cluster_concurrency_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ldkx9x/40_gpu_cluster_concurrency_test/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T11:42:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldupri</id>
    <title>🚀 I built a lightweight web UI for Ollama – great for local LLMs!</title>
    <updated>2025-06-17T18:22:37+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ldupri/i_built_a_lightweight_web_ui_for_ollama_great_for/"&gt; &lt;img alt="🚀 I built a lightweight web UI for Ollama – great for local LLMs!" src="https://external-preview.redd.it/1DSkBtE8nncEw-aEby0cNCk5U9TnOLRhDdejd1tL2ak.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16e59db3ddf1a21ab36d3f484534eedc264f0b8a" title="🚀 I built a lightweight web UI for Ollama – great for local LLMs!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1ldupay/i_built_a_lightweight_web_ui_for_ollama_great_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldupri/i_built_a_lightweight_web_ui_for_ollama_great_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ldupri/i_built_a_lightweight_web_ui_for_ollama_great_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T18:22:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lefhgu</id>
    <title>how to stop reasoning thinking output in any reasoning / thinking model using ChatOllama - langchain ollama package?</title>
    <updated>2025-06-18T12:12:09+00:00</updated>
    <author>
      <name>/u/Informal-Victory8655</name>
      <uri>https://old.reddit.com/user/Informal-Victory8655</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;how to stop reasoning thinking output in any reasoning / thinking model using ChatOllama - langchain ollama package?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Informal-Victory8655"&gt; /u/Informal-Victory8655 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lefhgu/how_to_stop_reasoning_thinking_output_in_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lefhgu/how_to_stop_reasoning_thinking_output_in_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lefhgu/how_to_stop_reasoning_thinking_output_in_any/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-18T12:12:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lenjwv</id>
    <title>Claude Code vs Cursor: In-depth Comparison and Review</title>
    <updated>2025-06-18T17:45:01+00:00</updated>
    <author>
      <name>/u/thomheinrich</name>
      <uri>https://old.reddit.com/user/thomheinrich</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there,&lt;/p&gt; &lt;p&gt;perhaps you are interested in my &lt;strong&gt;in-depth comparison of Cursor and Claude Code&lt;/strong&gt; - I use both of them a lot and I guess my video could be helpful for some of you; if this is the case, I would appreciate your feedback, like, comment or share, as I just started doing some videos.&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/ICWKqnaEQ5I?si=jaCyXIqvlRZLUWVA"&gt;https://youtu.be/ICWKqnaEQ5I?si=jaCyXIqvlRZLUWVA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Best&lt;/p&gt; &lt;p&gt;Thom&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thomheinrich"&gt; /u/thomheinrich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lenjwv/claude_code_vs_cursor_indepth_comparison_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lenjwv/claude_code_vs_cursor_indepth_comparison_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lenjwv/claude_code_vs_cursor_indepth_comparison_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-18T17:45:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldsl00</id>
    <title>Sadly the truth</title>
    <updated>2025-06-17T17:02:05+00:00</updated>
    <author>
      <name>/u/Virtual4P</name>
      <uri>https://old.reddit.com/user/Virtual4P</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ldsl00/sadly_the_truth/"&gt; &lt;img alt="Sadly the truth" src="https://preview.redd.it/551hwa6yfi7f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=360466aeae57613ec984fa66508c12bfca7f9e4e" title="Sadly the truth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Virtual4P"&gt; /u/Virtual4P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/551hwa6yfi7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ldsl00/sadly_the_truth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ldsl00/sadly_the_truth/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-17T17:02:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lerekw</id>
    <title>Strix Halo 64GB worth it?</title>
    <updated>2025-06-18T20:16:58+00:00</updated>
    <author>
      <name>/u/riklaunim</name>
      <uri>https://old.reddit.com/user/riklaunim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;128GB variants of Flow Z13 aren't available in the region, only 64GB showed up at ~2500 EUR and I'm considering it or just something more vanilla at half the price :)&lt;/p&gt; &lt;p&gt;Outside of general dev work I want something that can run most models for experimenting/testing. The other option is to just pick iGPU Intel/AMD with SODIMMs and pump it with 128GB of DDR5 - it's slower, iGPU much weaker but still can somewhat run most of the things - at like half the price and without questionable Asus :P&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/riklaunim"&gt; /u/riklaunim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lerekw/strix_halo_64gb_worth_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lerekw/strix_halo_64gb_worth_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lerekw/strix_halo_64gb_worth_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-18T20:16:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1levoew</id>
    <title>Chat History w/ Python API vs. How the Terminal works</title>
    <updated>2025-06-18T23:17:29+00:00</updated>
    <author>
      <name>/u/SweetpeaTheNerd</name>
      <uri>https://old.reddit.com/user/SweetpeaTheNerd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running some experiments, and I need to make sure that each individual chat session I automate with python is running as it would if someone pulled up Llama3.2 in their terminal and started chatting with it.&lt;/p&gt; &lt;p&gt;I know that when using the python API, I need to pass along the chat history in the messages. I am new to LLMs and Transformers, but it sounds like every time I make a chat request with the python API, it acts like it is a completely new model and reads the context, rather than remembering &amp;quot;How&amp;quot; it came about those answers (internal weights and stuff that led to it).&lt;/p&gt; &lt;p&gt;Is this what it is doing when I run it in the terminal? Not &amp;quot;remembering&amp;quot; how it got there, just looking at what it got and chatting based on that? Or for the individual chat session within the terminal is it maintaining some sort of state?&lt;/p&gt; &lt;p&gt;Basically, when I send a chat message and append all the previous messages in the chat, is this EXACTLY what is happening behind the scenes when I chat with Llama3.2 in my terminal? tyia&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SweetpeaTheNerd"&gt; /u/SweetpeaTheNerd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1levoew/chat_history_w_python_api_vs_how_the_terminal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1levoew/chat_history_w_python_api_vs_how_the_terminal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1levoew/chat_history_w_python_api_vs_how_the_terminal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-18T23:17:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lew0kj</id>
    <title>Ollama to excel list or to do</title>
    <updated>2025-06-18T23:33:27+00:00</updated>
    <author>
      <name>/u/ogreleprechaun1001</name>
      <uri>https://old.reddit.com/user/ogreleprechaun1001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ok. Forgive the newb question. But work whitelisted ollama for us to use. I want to integrate with either excel or todo to track my tasks and complete tasks I’ve done. Etc. just trying to slowly branch out in this world&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ogreleprechaun1001"&gt; /u/ogreleprechaun1001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lew0kj/ollama_to_excel_list_or_to_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lew0kj/ollama_to_excel_list_or_to_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lew0kj/ollama_to_excel_list_or_to_do/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-18T23:33:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lepsuz</id>
    <title>Stop Ollama spillover to CPU</title>
    <updated>2025-06-18T19:12:45+00:00</updated>
    <author>
      <name>/u/TheMicrosoftMan</name>
      <uri>https://old.reddit.com/user/TheMicrosoftMan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama runs well on my Nvidia GPU when the model fits within its VRAM, but once it goes over, it just goes crazy. Instead of using the GPU for inference and just using the system RAM as spillover, it switches the entire inference over to CPU. I have seen people add commands like --(command) when starting Ollama, but I don't want to have to do that every time. I just want to open the Ollama app on Windows and have it work. LM Studio has a feature that continues to use GPU and just spills over the model in system RAM. Can Ollama do the same?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheMicrosoftMan"&gt; /u/TheMicrosoftMan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lepsuz/stop_ollama_spillover_to_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lepsuz/stop_ollama_spillover_to_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lepsuz/stop_ollama_spillover_to_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-18T19:12:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lew2lh</id>
    <title>DeepSeek-R1 Tool calling</title>
    <updated>2025-06-18T23:36:10+00:00</updated>
    <author>
      <name>/u/YoungPsedo</name>
      <uri>https://old.reddit.com/user/YoungPsedo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see that Deepseek-r1 has been updated recently and it now has the tool icon when viewing in Ollama. I tried to implement an agent using LangGraph and use the latest Deepseek-r1 model as my LLM. I'm still running into the &lt;/p&gt; &lt;p&gt;&lt;code&gt;registry.ollama.ai/library/deepseek-r1:latest does not support toolsregistry.ollama.ai/library/deepseek-r1:latest does not support tools&lt;/code&gt; &lt;/p&gt; &lt;p&gt;error. Any ideas on why this is still happening even though is it supposed to have tool support now? For additional context I'm using &lt;a href="https://langchain-ai.github.io/langgraph/tutorials/get-started/2-add-tools/#9-use-prebuilts"&gt;https://langchain-ai.github.io/langgraph/tutorials/get-started/2-add-tools/#9-use-prebuilts&lt;/a&gt; and importing ChatOllama.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YoungPsedo"&gt; /u/YoungPsedo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lew2lh/deepseekr1_tool_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lew2lh/deepseekr1_tool_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lew2lh/deepseekr1_tool_calling/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-18T23:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lerg05</id>
    <title>Why does ollama not use my gpu</title>
    <updated>2025-06-18T20:18:33+00:00</updated>
    <author>
      <name>/u/Odd_Art_8778</name>
      <uri>https://old.reddit.com/user/Odd_Art_8778</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lerg05/why_does_ollama_not_use_my_gpu/"&gt; &lt;img alt="Why does ollama not use my gpu" src="https://preview.redd.it/c75fr7ouuq7f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=89c51d485aae690139a7a1cd0d995df509344b63" title="Why does ollama not use my gpu" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using a fine tuned llama3.2, which is 2gb, I have 8.8gb shared gpu memory, from what I read if my model is larger than my vram then it doesn’t use gpu but I don’t think that’s the case here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Art_8778"&gt; /u/Odd_Art_8778 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c75fr7ouuq7f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lerg05/why_does_ollama_not_use_my_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lerg05/why_does_ollama_not_use_my_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-18T20:18:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lf2l48</id>
    <title>I am Getting this error constantly, Please help.</title>
    <updated>2025-06-19T05:13:43+00:00</updated>
    <author>
      <name>/u/National-Cut302</name>
      <uri>https://old.reddit.com/user/National-Cut302</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lf2l48/i_am_getting_this_error_constantly_please_help/"&gt; &lt;img alt="I am Getting this error constantly, Please help." src="https://preview.redd.it/pzzh3w4oht7f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9601c1d677e9d445717bfc7b2a497afff0197b6" title="I am Getting this error constantly, Please help." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am doing my project to implement a locally hosted LLM for a local web page. My server security here is high and in most cases outright bans most websites and web pages(including YouTube, completely).&lt;/p&gt; &lt;p&gt;But the IT department told that there is no such blocking for ollama as you are able to view the web page and also download the ollama software. The software is downloaded and even is running in the background but I am not able to pull as model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/National-Cut302"&gt; /u/National-Cut302 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pzzh3w4oht7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lf2l48/i_am_getting_this_error_constantly_please_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lf2l48/i_am_getting_this_error_constantly_please_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-19T05:13:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lf36c2</id>
    <title>How to serve a LLM with REST API using Ollama</title>
    <updated>2025-06-19T05:50:00+00:00</updated>
    <author>
      <name>/u/keepmybodymoving</name>
      <uri>https://old.reddit.com/user/keepmybodymoving</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I followed an instruction to set up a REST API to serve nomic-embed-text (&lt;a href="https://ollama.com/library/nomic-embed-text"&gt;https://ollama.com/library/nomic-embed-text&lt;/a&gt;) using Docker and Ollama on HF space. Here's the example curl command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl http://user-space.hf.space/api/embeddings -d '{ &amp;quot;model&amp;quot;: &amp;quot;nomic-embed-text&amp;quot;, &amp;quot;prompt&amp;quot;: &amp;quot;The sky is blue because of Rayleigh scattering&amp;quot; }' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I pulled the model and Ollama is running on HF space. I got the embedding of the prompt. Everything works perfectly. I have a few questions:&lt;br /&gt; 1. Why is the URL ending &amp;quot;api/embeddings&amp;quot;? Where is it defined?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I would like to serve a language model. Let's say llama3.2:1b (&lt;a href="https://ollama.com/library/llama3.2"&gt;https://ollama.com/library/llama3.2&lt;/a&gt;). In that case, what would be the URL to curl? There is no REST API example on Ollama llama page.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/keepmybodymoving"&gt; /u/keepmybodymoving &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lf36c2/how_to_serve_a_llm_with_rest_api_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lf36c2/how_to_serve_a_llm_with_rest_api_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lf36c2/how_to_serve_a_llm_with_rest_api_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-19T05:50:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lf1dy7</id>
    <title>Which is the best open source model to be used for a Chatbot with tools</title>
    <updated>2025-06-19T04:03:43+00:00</updated>
    <author>
      <name>/u/Dragov_75</name>
      <uri>https://old.reddit.com/user/Dragov_75</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I am trying to build a chatbot using tools and MCP servers and I want to know which is the best open source model less than 8b parameters ( as my laptop cannot run beyond ) that I can use for my project.&lt;/p&gt; &lt;p&gt;The chatbot would need to use tools communicating through an MCP server. &lt;/p&gt; &lt;p&gt;Any suggestions would help alot thanks :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dragov_75"&gt; /u/Dragov_75 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lf1dy7/which_is_the_best_open_source_model_to_be_used/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lf1dy7/which_is_the_best_open_source_model_to_be_used/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lf1dy7/which_is_the_best_open_source_model_to_be_used/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-19T04:03:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1leqii6</id>
    <title>Ummmm.......WOW.</title>
    <updated>2025-06-18T19:40:59+00:00</updated>
    <author>
      <name>/u/huskylawyer</name>
      <uri>https://old.reddit.com/user/huskylawyer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are moments in life that are monumental and game-changing. This is one of those moments for me.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt; I’m a 53-year-old attorney with virtually zero formal coding or software development training. I can roll up my sleeves and do some basic HTML or use the Windows command prompt, for simple &amp;quot;ipconfig&amp;quot; queries, but that's about it. Many moons ago, I built a dual-boot Linux/Windows system, but that’s about the greatest technical feat I’ve ever accomplished on a personal PC. I’m a noob, lol.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;AI.&lt;/strong&gt; As AI seemingly took over the world’s consciousness, I approached it with skepticism and even resistance (&amp;quot;Great, we're creating Skynet&amp;quot;). Not more than 30 days ago, I had &lt;em&gt;never&lt;/em&gt; even deliberately used a publicly available paid or free AI service. I hadn’t tried ChatGPT or enabled AI features in the software I use. Probably the most AI usage I experienced was seeing AI-generated responses from normal Google searches.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Awakening.&lt;/strong&gt; A few weeks ago, a young attorney at my firm asked about using AI. He wrote a persuasive memo, and because of it, I thought, &amp;quot;You know what, I’m going to learn it.&amp;quot;&lt;/p&gt; &lt;p&gt;So I went down the AI rabbit hole. I did some research (Google and YouTube videos), read some blogs, and then I looked at my personal gaming machine and thought it could run a local LLM (I didn’t even know what the acronym stood for less than a month ago!). It’s an i9-14900k rig with an RTX 5090 GPU, 64 GBs of RAM, and 6 TB of storage. When I built it, I didn't even think about AI – I was focused on my flight sim hobby and Monster Hunter Wilds. But after researching, I learned that this thing can run a local and private LLM!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Today.&lt;/strong&gt; I devoured how-to videos on creating a local LLM environment. I started basic: I deployed Ubuntu for a Linux environment using WSL2, then installed the Nvidia toolkits for 50-series cards. Eventually, I got Docker working, and after a lot of trial and error (5+ hours at least), I managed to get Ollama and Open WebUI installed and working great. I settled on Gemma3 12B as my first locally-run model.&lt;/p&gt; &lt;p&gt;I am just blown away. The use cases are absolutely endless. And because it’s local and private, I have unlimited usage?! Mind blown. I can’t even believe that I waited this long to embrace AI. And Ollama seems really easy to use (granted, I’m doing basic stuff and just using command line inputs).&lt;/p&gt; &lt;p&gt;So for anyone on the fence about AI, or feeling intimidated by getting into the OS weeds (Linux) and deploying a local LLM, know this: If a 53-year-old AARP member with zero technical training on Linux or AI can do it, so can you.&lt;/p&gt; &lt;p&gt;Today, during the firm partner meeting, I’m going to show everyone my setup and argue for a locally hosted AI solution – I have no doubt it will help the firm.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT: I appreciate everyone's support and suggestions! I have looked up many of the plugins and suggested apps that folks have suggested and will undoubtedly try out a few (e.g,, MCP, Open Notebook Tika Apache, etc.). Some of the recommended apps seem pretty technical because I'm not very experienced with Linux environments (though I do love the OS as it seems &amp;quot;light&amp;quot; and intuitive), but I am learning! Thank you and looking forward to being more active on this sub-reddit.&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/huskylawyer"&gt; /u/huskylawyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1leqii6/ummmmwow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1leqii6/ummmmwow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1leqii6/ummmmwow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-18T19:40:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfewhm</id>
    <title>Computer-Use on Windows Sandbox</title>
    <updated>2025-06-19T16:18:06+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lfewhm/computeruse_on_windows_sandbox/"&gt; &lt;img alt="Computer-Use on Windows Sandbox" src="https://external-preview.redd.it/czZrdjhkZnVzdzdmMUUIhfD3WmHuxYkgbFXnt7PvLDhATd-8_6cYVR-PGp7c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c92117946169490ae095a6d6855f81630c035e7" title="Computer-Use on Windows Sandbox" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing Windows Sandbox support - run computer-use agents on Windows business apps without VMs or cloud costs.&lt;/p&gt; &lt;p&gt;Your enterprise software runs on Windows, but testing agents required expensive cloud instances. Windows Sandbox changes this - it's Microsoft's built-in lightweight virtualization sitting on every Windows 10/11 machine, ready for instant agent development.&lt;/p&gt; &lt;p&gt;Enterprise customers kept asking for AutoCAD automation, SAP integration, and legacy Windows software support. Traditional VM testing was slow and resource-heavy. Windows Sandbox solves this with disposable, seconds-to-boot Windows environments for safe agent testing.&lt;/p&gt; &lt;p&gt;What you can build: AutoCAD drawing automation, SAP workflow processing, Bloomberg terminal trading bots, manufacturing execution system integration, or any Windows-only enterprise software automation - all tested safely in disposable sandbox environments.&lt;/p&gt; &lt;p&gt;Free with Windows 10/11, boots in seconds, completely disposable. Perfect for development and testing before deploying to Windows cloud instances (coming later this month).&lt;/p&gt; &lt;p&gt;Check out the github here : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog : &lt;a href="https://www.trycua.com/blog/windows-sandbox"&gt;https://www.trycua.com/blog/windows-sandbox&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ip375inusw7f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lfewhm/computeruse_on_windows_sandbox/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lfewhm/computeruse_on_windows_sandbox/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-19T16:18:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfg3p6</id>
    <title>What is that thing</title>
    <updated>2025-06-19T17:05:53+00:00</updated>
    <author>
      <name>/u/Virtual4P</name>
      <uri>https://old.reddit.com/user/Virtual4P</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lfg3p6/what_is_that_thing/"&gt; &lt;img alt="What is that thing" src="https://preview.redd.it/bdza8l4e1x7f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a136b781cb8b57c0325bb5d9cd331b9455969150" title="What is that thing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Virtual4P"&gt; /u/Virtual4P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bdza8l4e1x7f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lfg3p6/what_is_that_thing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lfg3p6/what_is_that_thing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-19T17:05:53+00:00</published>
  </entry>
</feed>
