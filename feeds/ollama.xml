<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-10T06:36:27+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1j66pg3</id>
    <title>RLAMA -- A document AI question-answering tool that connects to your local Ollama models.</title>
    <updated>2025-03-08T02:09:46+00:00</updated>
    <author>
      <name>/u/DonTizi</name>
      <uri>https://old.reddit.com/user/DonTizi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey!&lt;/p&gt; &lt;p&gt;I developed RLAMA to solve a straightforward but frustrating problem: how to easily query my own documents with a local LLM without using cloud services.&lt;/p&gt; &lt;h1&gt;What it actually is&lt;/h1&gt; &lt;p&gt;RLAMA is a command-line tool that bridges your local documents and Ollama models. It implements RAG (Retrieval-Augmented Generation) in a minimalist way:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Index a folder of documents rlama rag llama3 project-docs ./documentation # Start an interactive session rlama run project-docs &amp;gt; How does the authentication module work? &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;You point the tool to a folder containing your files (.txt, .md, .pdf, source code, etc.)&lt;/li&gt; &lt;li&gt;RLAMA extracts text from the documents and generates embeddings via Ollama&lt;/li&gt; &lt;li&gt;When you ask a question, it retrieves relevant passages and sends them to the model&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The tool handles many formats automatically. For PDFs, it first tries pdftotext, then tesseract if necessary. For binary files, it has several fallback methods to extract what it can.&lt;/p&gt; &lt;h1&gt;Problems it solves&lt;/h1&gt; &lt;p&gt;I use it daily for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Finding information in old technical documents without having to reread everything&lt;/li&gt; &lt;li&gt;Exploring code I'm not familiar with (e.g., &amp;quot;explain how part X works&amp;quot;)&lt;/li&gt; &lt;li&gt;Creating summaries of long documents&lt;/li&gt; &lt;li&gt;Querying my research or meeting notes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The real time-saver comes from being able to ask questions instead of searching for keywords. For example, I can ask &amp;quot;What are the possible errors in the authentication API?&amp;quot; and get consolidated answers from multiple files.&lt;/p&gt; &lt;h1&gt;Why use it?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;It's simple&lt;/strong&gt;: four commands are enough (rag, run, list, delete)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It's local&lt;/strong&gt;: no data is sent over the internet&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It's lightweight&lt;/strong&gt;: no need for Docker or a complete stack&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It's flexible&lt;/strong&gt;: compatible with all Ollama models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I created it because other solutions were either too complex to configure or required sending my documents to external services.&lt;/p&gt; &lt;p&gt;If you already have Ollama installed and are looking for a simple way to query your documents, this might be useful for you.&lt;/p&gt; &lt;h1&gt;In conclusion&lt;/h1&gt; &lt;p&gt;I've found that in discussions on &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; point to several pressing needs for local RAG without cloud dependencies: we need to simplify the ingestion of data (PDFs, web pages, videos...) via tools that can automatically transform them into usable text, reduce hardware requirements or better leverage common hardware (model quantization, multi-GPU support) to improve performance, and integrate advanced retrieval methods (hybrid search, rerankers, etc.) to increase answer reliability.&lt;/p&gt; &lt;p&gt;The emergence of integrated solutions (OpenWebUI, LangChain/Langroid, RAGStack, etc.) moves in this direction: the ultimate goal is a tool where users only need to provide their local files to benefit from an AI assistant trained on their own knowledge, while remaining 100% private and local so I wanted to develop something easy to use!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/dontizi/rlama"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DonTizi"&gt; /u/DonTizi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j66pg3/rlama_a_document_ai_questionanswering_tool_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j66pg3/rlama_a_document_ai_questionanswering_tool_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j66pg3/rlama_a_document_ai_questionanswering_tool_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-08T02:09:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6l37b</id>
    <title>Ollama somehow utilizes CPU although GPU VRAM is not fully utilized</title>
    <updated>2025-03-08T16:41:50+00:00</updated>
    <author>
      <name>/u/PFGSnoopy</name>
      <uri>https://old.reddit.com/user/PFGSnoopy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently experimenting with Ollama as the AI Backend for the HomeAssistant Voicee Assistant.&lt;/p&gt; &lt;p&gt;My Setup is as this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Minisforum 795S7 &lt;ul&gt; &lt;li&gt;AMD Ryzen 9 7945HX&lt;/li&gt; &lt;li&gt;64GB DDR5 RAM&lt;/li&gt; &lt;li&gt;2x 2TB NVMe SSD in a RAID1 configuration&lt;/li&gt; &lt;li&gt;NVIDIA RTX 2000 Ada, 16 VRAM&lt;/li&gt; &lt;li&gt;Proxmox 8.3&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Ollama is running in a VM on Proxmox &lt;ul&gt; &lt;li&gt;Ubuntu Server&lt;/li&gt; &lt;li&gt;8 CPU cores desdicated to the VM&lt;/li&gt; &lt;li&gt;20GB RAM desicated to the VM&lt;/li&gt; &lt;li&gt;GPU passed trough to the VM&lt;/li&gt; &lt;li&gt;LLM: Qwen2.5:7B&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Raspberry Pi 5B &lt;ul&gt; &lt;li&gt;8GB RAM&lt;/li&gt; &lt;li&gt;HAOS on a 256GB NVMe SSD&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Currently I'm just testing text queries from the HA web frontend to the Ollama backend.&lt;/p&gt; &lt;p&gt;One thing is that Ollama takes forever to come up with a reply, although it is very responsive when queried directly in a command shell on the server (SSH).&lt;/p&gt; &lt;p&gt;The other strange thing is that Ollama is utilizing 100% of the GPUs compute power and 50% of its VRAM and additionally almost 100% of 2 CPU cores (as you can see in the image above).&lt;/p&gt; &lt;p&gt;I was under the impression that Ollama would only utilize the CPU if there wasn't enough VRAM on the GPU. Am I wrong?&lt;/p&gt; &lt;p&gt;The other thing that puzzles me, is that I have seen videos of people that got near instant replies while using a Tesla P4, which is about half as fast as my RTX 2000 Ada (and it has only half the VRAM, too).&lt;/p&gt; &lt;p&gt;Without the Speech-to-Text part queries already take 10+ seconds. If I add Speech-to-Text, I estimate response times on every query via the HomeAssistant Voice Assistant will take 30 sekonds or more. That way I won't be able to retire Alexa any time soon.&lt;/p&gt; &lt;p&gt;I'm pretty sure I'm doing something wrong (probably both on the Ollama and the HomeAssistent end of things. But at the moment I feel way over my head and don't know where to start looking for the cause(s) for the bad performance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PFGSnoopy"&gt; /u/PFGSnoopy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j6l37b/ollama_somehow_utilizes_cpu_although_gpu_vram_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j6l37b/ollama_somehow_utilizes_cpu_although_gpu_vram_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j6l37b/ollama_somehow_utilizes_cpu_although_gpu_vram_is/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-08T16:41:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6llpq</id>
    <title>How to force ollama to give random answers</title>
    <updated>2025-03-08T17:04:56+00:00</updated>
    <author>
      <name>/u/mathgoy</name>
      <uri>https://old.reddit.com/user/mathgoy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am using ollama to generate weekly menus and send them to my home assistant.&lt;/p&gt; &lt;p&gt;However, after few tests, I am figuring out that it always comes with the same recepies. &lt;/p&gt; &lt;p&gt;How can I &amp;quot;force&amp;quot; it to come with new ideas every weeks. I am using mistral and llama3.2&lt;/p&gt; &lt;p&gt;FYI, I am using nodered to send prompts to my ollama. what ollama outputs is a JSON file with my weekly menu so I can parse it easily and display it into home assistant.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mathgoy"&gt; /u/mathgoy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j6llpq/how_to_force_ollama_to_give_random_answers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j6llpq/how_to_force_ollama_to_give_random_answers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j6llpq/how_to_force_ollama_to_give_random_answers/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-08T17:04:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6kie5</id>
    <title>Practicality of running small models on my gpu-less dedicated server?</title>
    <updated>2025-03-08T16:15:42+00:00</updated>
    <author>
      <name>/u/wreck_of_u</name>
      <uri>https://old.reddit.com/user/wreck_of_u</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a dedicated server (in a datacenter), 2x10 core xeon, 1TB raid SSD, 64GB (DDR4) ram. I use it to host a bunch of docker containers running some Node APIs, postrgre, mariadb, and mongo, and web servers. It's very underutilized for now, maybe under load it uses 2 cores and 4GB ram max lol. I'm holding on to this server until it dies because I got it for really cheap a few years ago. &lt;/p&gt; &lt;p&gt;I have 1 app that makes calls to OpenAI Whisper-1 for speech to text, and 4o-mini for simple text transcription (paragraphs to bullet form). To be honest with the small number of tokens I send/receive it basically costs nothing (for now).&lt;/p&gt; &lt;p&gt;I was wondering what is the practicality of running ollama on my server, and using one of the smaller models, maybe a Deepseek R1 1.5 or something (I'm able to run 1.5b on my gpu-less laptop with 40GB ddr5 4800 ram)? Will it be painfully slow on a DDR4 (I think it's an ecc 2100mhz maybe slower)? I'm not going to train, just basic inference.&lt;/p&gt; &lt;p&gt;Should I just forget it, and get it off my mind, and just continue using the simpler method of api calls to OpenAI? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wreck_of_u"&gt; /u/wreck_of_u &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j6kie5/practicality_of_running_small_models_on_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j6kie5/practicality_of_running_small_models_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j6kie5/practicality_of_running_small_models_on_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-08T16:15:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j70zzs</id>
    <title>DeepSeek's thinking phase is breaking the front end of my application, I think it's a JSON key issue but I cannot find any docs.</title>
    <updated>2025-03-09T05:52:00+00:00</updated>
    <author>
      <name>/u/motuwed</name>
      <uri>https://old.reddit.com/user/motuwed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using Ollama to host DeepSeek R1 locally, and have written some basic python code to communicate with the model as well as using the front end library &amp;quot;Gradio&amp;quot; to make it all interactive. This works when I ask it simple questions that don't require reasoning or &amp;quot;thinking&amp;quot;. However as soon as I ask it a question where it needs to think, the front end and more specifically the model's response bubble goes blank, even though a response is being displayed in terminal. I believe I need to collect the &amp;quot;thinking&amp;quot; content as well to stream it and prevent Gradio from timing out, but I can't find any docs on the JSON structure. Could anybody help me?&lt;/p&gt; &lt;p&gt;Here is a snippet of my code for reference:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def generate_response(user_input, history): data = { &amp;quot;model&amp;quot;: &amp;quot;deepseek-r1:7b&amp;quot;, &amp;quot;prompt&amp;quot;: user_input, &amp;quot;system&amp;quot;: &amp;quot;Answer prompts with concise answers&amp;quot;, } response = requests.post(url, json=data, stream=True, timeout=None) if response.status_code == 200: generated_text = &amp;quot;&amp;quot; print(&amp;quot;Generated Text: \n&amp;quot;, end=&amp;quot; &amp;quot;, flush=True) # Iterate over the response stream line by line for line in response.iter_lines(): if line: try: decoded_line = line.decode('utf-8') result = json.loads(decoded_line) # Append new content to generated_text chunk = result.get(&amp;quot;response&amp;quot;, &amp;quot;&amp;quot;) print(chunk, end=&amp;quot;&amp;quot;, flush=True) yield generated_text + chunk generated_text += chunk &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/motuwed"&gt; /u/motuwed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j70zzs/deepseeks_thinking_phase_is_breaking_the_front/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j70zzs/deepseeks_thinking_phase_is_breaking_the_front/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j70zzs/deepseeks_thinking_phase_is_breaking_the_front/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T05:52:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6mm6r</id>
    <title>How to use ollama models in vscode?</title>
    <updated>2025-03-08T17:50:30+00:00</updated>
    <author>
      <name>/u/blnkslt</name>
      <uri>https://old.reddit.com/user/blnkslt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm wondering what are available options to make use of ollama models on vscode? Which one do you use? There are a couple of ollama-* extensions but none of them seem to gain much popularity. What I'm looking for is an extension like Augment Code which you can plug your locally ruining ollama models or plug them to available API providers. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blnkslt"&gt; /u/blnkslt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j6mm6r/how_to_use_ollama_models_in_vscode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j6mm6r/how_to_use_ollama_models_in_vscode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j6mm6r/how_to_use_ollama_models_in_vscode/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-08T17:50:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1j724ne</id>
    <title>Docker GPU Offloading issue resolved!?</title>
    <updated>2025-03-09T07:06:44+00:00</updated>
    <author>
      <name>/u/Inner-End7733</name>
      <uri>https://old.reddit.com/user/Inner-End7733</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was having issues getting Ollama/docker to cooperate with my rtx 3060, after seemingly following all the steps.&lt;/p&gt; &lt;p&gt;I initially didnt install docker desktop, and I tried this time on reinstall, and as such I installed all the KVM stuff on my machine and turned virtualization on in my bios. I couldn't get the .deb file to install after that and frustratedly went back and installed the docker engine through command line with the instructions.&lt;/p&gt; &lt;p&gt;when I remade the container ollama showed up on Nvidia-smi and There was a noticable performance increase. So if you're having trouble with GPU offloading using docker, maybe try installing KVM and turning on virtualization in your bios.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inner-End7733"&gt; /u/Inner-End7733 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j724ne/docker_gpu_offloading_issue_resolved/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j724ne/docker_gpu_offloading_issue_resolved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j724ne/docker_gpu_offloading_issue_resolved/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T07:06:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j72frr</id>
    <title>I'll just leave that here, in case anyone needs it. Appreciate feedback</title>
    <updated>2025-03-09T07:28:37+00:00</updated>
    <author>
      <name>/u/Competitive_Cat_2098</name>
      <uri>https://old.reddit.com/user/Competitive_Cat_2098</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j72frr/ill_just_leave_that_here_in_case_anyone_needs_it/"&gt; &lt;img alt="I'll just leave that here, in case anyone needs it. Appreciate feedback" src="https://b.thumbs.redditmedia.com/wyWBvIO8eNpoxhSTrD4CqyIbqlTZL1aMdnN_ROxkWXw.jpg" title="I'll just leave that here, in case anyone needs it. Appreciate feedback" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive_Cat_2098"&gt; /u/Competitive_Cat_2098 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LangChain/comments/1j7295m/i_created_a_mcp_manager_installer_that_also_lets/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j72frr/ill_just_leave_that_here_in_case_anyone_needs_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j72frr/ill_just_leave_that_here_in_case_anyone_needs_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T07:28:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j72ihw</id>
    <title>Increase max model output lerngth for use in ComfyUI</title>
    <updated>2025-03-09T07:33:55+00:00</updated>
    <author>
      <name>/u/Famous_Assistant5390</name>
      <uri>https://old.reddit.com/user/Famous_Assistant5390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a complete novice to Ollama. I want to use it as an elaborate prompt generator for Flux pictures using ComfyUI. I am adapting the workflow by &amp;quot;Murphylanga&amp;quot; that I saw in a youtube video and that is also posted on Civitai.&lt;/p&gt; &lt;p&gt;I want to generate a very detailed description of an input image with a vision model and then pass it on to several virtual specialists to refine it using Gemma 2 until the final prompt is generated. The problem is that the default output length is not sufficient for the detailed image description that I am prompting the Ollama Vision node for. The description is interrupted about halfway through.&lt;/p&gt; &lt;p&gt;I've read that the maximum output length can be set by CLI. Is there also a possibility to specify this in a config file or even via a Comfy node? It's made complicated by the fact that I want to switch models during the process. The description is obviously created by a vision model, but for the refinement I want to use a stronger model like Gemma 2.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Famous_Assistant5390"&gt; /u/Famous_Assistant5390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j72ihw/increase_max_model_output_lerngth_for_use_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j72ihw/increase_max_model_output_lerngth_for_use_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j72ihw/increase_max_model_output_lerngth_for_use_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T07:33:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6dp3j</id>
    <title>Ollama with granite3.2-vision is excellent for OCR and for processing text afterwards</title>
    <updated>2025-03-08T09:35:15+00:00</updated>
    <author>
      <name>/u/ML-Future</name>
      <uri>https://old.reddit.com/user/ML-Future</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;granite3.2-vision: I just want to say that after a day of testing it is exactly what I was looking for.&lt;/p&gt; &lt;p&gt;It can work perfectly locally with less than 12gb of ram.&lt;/p&gt; &lt;p&gt;I have tested it to interpret some documents in Spanish and then process their data. Considering its size the performance and precision are surprising.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ML-Future"&gt; /u/ML-Future &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j6dp3j/ollama_with_granite32vision_is_excellent_for_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j6dp3j/ollama_with_granite32vision_is_excellent_for_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j6dp3j/ollama_with_granite32vision_is_excellent_for_ocr/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-08T09:35:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6yjru</id>
    <title>Mac Studio 512GB</title>
    <updated>2025-03-09T03:25:07+00:00</updated>
    <author>
      <name>/u/FewMixture574</name>
      <uri>https://old.reddit.com/user/FewMixture574</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First post here. &lt;/p&gt; &lt;p&gt;Genuinely curious what everyone thinks about the new Mac Studio that can be configured to have 512GB unified memory!&lt;/p&gt; &lt;p&gt;I have been on the fence for a bit on what I’m going to do for my own local server - I’ve got quad 3090s and was (wishfully) hoping that 5090s could replace them, but I should have known supply and prices were going to be trash. &lt;/p&gt; &lt;p&gt;But now the idea of spending ~$2k on a 5090 seems a bit ridiculous.&lt;/p&gt; &lt;p&gt;When comparing the two (and yes, this is an awful metric):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;the 5090 comes out to be ~$62.50 per GB of usable memory&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;the Mac Studio comes out to be ~$17.50 per GB of usable memory if purchasing the top tier with 512GB. &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And this isn’t even taking into account power draw, heat, space, etc. &lt;/p&gt; &lt;p&gt;Is anyone else thinking this way? Am I crazy?&lt;/p&gt; &lt;p&gt;I see people slamming together multiple kW of servers with 6-8 AMD cards here and just wonder “what am I missing?”&lt;/p&gt; &lt;p&gt;Is it simply the cost? &lt;/p&gt; &lt;p&gt;I know that the apple silicon has been behind nvidia, but surely the usable memory of the apple studio should make up for that by a lot. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FewMixture574"&gt; /u/FewMixture574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j6yjru/mac_studio_512gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j6yjru/mac_studio_512gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j6yjru/mac_studio_512gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T03:25:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7awgx</id>
    <title>Using an MCP SSE Server with LangchainJS and Ollama</title>
    <updated>2025-03-09T16:10:28+00:00</updated>
    <author>
      <name>/u/Inevitable-Judge2642</name>
      <uri>https://old.reddit.com/user/Inevitable-Judge2642</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j7awgx/using_an_mcp_sse_server_with_langchainjs_and/"&gt; &lt;img alt="Using an MCP SSE Server with LangchainJS and Ollama" src="https://external-preview.redd.it/YTk-yCk_75-45gUIzFNaFbK1SU_2Y_qd053s-1zVoAY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64771ea2e71eb0c7a22890f858247203f2579eb5" title="Using an MCP SSE Server with LangchainJS and Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Judge2642"&gt; /u/Inevitable-Judge2642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://k33g.hashnode.dev/using-an-mcp-sse-server-with-langchainjs-and-ollama"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7awgx/using_an_mcp_sse_server_with_langchainjs_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7awgx/using_an_mcp_sse_server_with_langchainjs_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T16:10:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j775a2</id>
    <title>Ollama is not compatible with GPU anymore</title>
    <updated>2025-03-09T13:01:24+00:00</updated>
    <author>
      <name>/u/Inevitable_Cut_1309</name>
      <uri>https://old.reddit.com/user/Inevitable_Cut_1309</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j775a2/ollama_is_not_compatible_with_gpu_anymore/"&gt; &lt;img alt="Ollama is not compatible with GPU anymore" src="https://b.thumbs.redditmedia.com/6l83o2IL-Lp4AkXI1iPGr_0O0PU_zT-R0iXTjYtnbgI.jpg" title="Ollama is not compatible with GPU anymore" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have recently reinstalled cuda toolkit(12.5) and torch (11.8)&lt;br /&gt; I have NVIDIA GeForce RTX 4070, and my driver version is 572.60&lt;br /&gt; I am using Cuda 12.5 for Ollama compatibility, but every time I run my Ollama instead of the GPU, it starts running on the CPU.&lt;/p&gt; &lt;p&gt;The GPU used to be utilized 100% before the reinstallation, but now it doesn't consume more than 10% of the GPU.&lt;br /&gt; I have set the GPU for Olama to RTX 4070.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tl1naxrmsnne1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b0b3ef7f3db480b2833d82273a55bfc13c41829"&gt;https://preview.redd.it/tl1naxrmsnne1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b0b3ef7f3db480b2833d82273a55bfc13c41829&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mpan0v4aunne1.png?width=839&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4691fc4961bc11d9fe4bd90d1007e6df23a16235"&gt;https://preview.redd.it/mpan0v4aunne1.png?width=839&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4691fc4961bc11d9fe4bd90d1007e6df23a16235&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When I use the command ollama ps, it shows that it consumes 100% GPU.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4q73hr8runne1.png?width=725&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e696b64fde7a4cd744de5a54c977dd18e2706329"&gt;The GPU while running the ollama instance &lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have tried changing my Cuda version to 11.8, 12.3 and 12.8, but it doesn't make a difference. I am using cudnn 8.9.7.&lt;/p&gt; &lt;p&gt;I am doing this on a Windows 11. The models used to run at a 100% efficiency and now don't cross the 5-10% mark.&lt;br /&gt; I have tried reinstalling ollama as well.&lt;/p&gt; &lt;p&gt;These are the issues I see in ollama log file :&lt;/p&gt; &lt;p&gt;Key not found: llama.attention.key_length&lt;/p&gt; &lt;p&gt;key not found: llama.attention.value_length&lt;/p&gt; &lt;p&gt;ggml_backend_load_best: failed to load ... ggml-cpu-alderlake.dll&lt;/p&gt; &lt;p&gt;Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address is normally permitted.&lt;/p&gt; &lt;p&gt;Can someone tell me what to do here?&lt;/p&gt; &lt;p&gt;Edit:&lt;/p&gt; &lt;p&gt;I ran a code using my torch, and it is able to use 100% of the GPU:&lt;br /&gt; The code is :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import torch import time device = torch.device(&amp;quot;cuda&amp;quot; if torch.cuda.is_available() else &amp;quot;cpu&amp;quot;) print(f&amp;quot;Using device: {device}&amp;quot;) # Large matrix size for heavy computation size = 30000 # Increase this for more load iterations = 10 # Number of multiplications a = torch.randn(size, size, device=device) b = torch.randn(size, size, device=device) print(&amp;quot;Starting matrix multiplications...&amp;quot;) start_time = time.time() for i in range(iterations): c = torch.mm(a, b) # Matrix multiplication torch.cuda.synchronize() # Ensure GPU finishes before timing end_time = time.time() print(f&amp;quot;Completed {iterations} multiplications in {end_time - start_time:.2f} seconds&amp;quot;) print(&amp;quot;Final value from matrix:&amp;quot;, c[0, 0].item()) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable_Cut_1309"&gt; /u/Inevitable_Cut_1309 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j775a2/ollama_is_not_compatible_with_gpu_anymore/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j775a2/ollama_is_not_compatible_with_gpu_anymore/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j775a2/ollama_is_not_compatible_with_gpu_anymore/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T13:01:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7i9uc</id>
    <title>Ollama Search part 3</title>
    <updated>2025-03-09T21:32:15+00:00</updated>
    <author>
      <name>/u/Pure-Caramel1216</name>
      <uri>https://old.reddit.com/user/Pure-Caramel1216</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the part 3 of Ollama search, I’ve built a new version of my project that keeps the same level of accuracy as before but runs 3 to 4 times faster. Plus, it now includes a handy RAG feature that lets it remember our conversations, along with a full web search for the latest info.&lt;/p&gt; &lt;p&gt;If you want to try it out! Just &lt;a href="https://www.ollamasearch.cloud/"&gt;sign up&lt;/a&gt;, and it will be available soon for everyone who registers. Your feedback means a lot to me, so please drop any suggestions or ideas you have in the comments, and if you like what you see, an upvote would be amazing to help get this into more hands.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pure-Caramel1216"&gt; /u/Pure-Caramel1216 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7i9uc/ollama_search_part_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7i9uc/ollama_search_part_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7i9uc/ollama_search_part_3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T21:32:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7iso1</id>
    <title>I can't make a rag system with fastapi</title>
    <updated>2025-03-09T21:54:57+00:00</updated>
    <author>
      <name>/u/Ok_Impact4403</name>
      <uri>https://old.reddit.com/user/Ok_Impact4403</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to make a small project but i can't make the rag system, I had one made with python for the console, but for a website I can't seem to be able to do it, I asked chatgpt, gemini, claude 3.7, none of them could help me out, the code made sense but the response that i was hoping to get never came. I eliminate the code that was really not doing anything, and if anyone knows anything I would be really appreciated, I send here the code that was for the website and also the modified version that I had for the terminal.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;the html&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&amp;lt;!DOCTYPE html&amp;gt;&lt;br /&gt; &amp;lt;html lang=&amp;quot;pt-pt&amp;quot;&amp;gt;&lt;br /&gt; &amp;lt;head&amp;gt;&lt;br /&gt; &amp;lt;meta charset=&amp;quot;UTF-8&amp;quot;&amp;gt;&lt;br /&gt; &amp;lt;meta name=&amp;quot;viewport&amp;quot; content=&amp;quot;width=device-width, initial-scale=1.0&amp;quot;&amp;gt;&lt;br /&gt; &amp;lt;title&amp;gt;OficinaStudy&amp;lt;/title&amp;gt;&lt;br /&gt; &amp;lt;style&amp;gt;&lt;br /&gt; body {&lt;br /&gt; font-family: Arial, sans-serif;&lt;br /&gt; margin: 20px;&lt;br /&gt; } &lt;/p&gt; &lt;p&gt;/* STYLE CHAT BOX */&lt;br /&gt; #chat-box {&lt;br /&gt; margin: 20px 0;&lt;br /&gt; padding: 10px;&lt;br /&gt; border: 1px solid #ccc;&lt;br /&gt; max-width: 100%;&lt;br /&gt; min-height: 300px;&lt;br /&gt; overflow-y: auto;&lt;br /&gt; } &lt;/p&gt; &lt;p&gt;/* STYLE INPUT BOX */&lt;br /&gt; #input-box {&lt;br /&gt; width: calc(100% - 20px);&lt;br /&gt; padding: 10px;&lt;br /&gt; margin-bottom: 20px;&lt;br /&gt; }&lt;br /&gt; #box {&lt;br /&gt; width: calc(100% - 20px);&lt;br /&gt; padding: 10px;&lt;br /&gt; margin-bottom: 20px;&lt;br /&gt; }&lt;br /&gt; &amp;lt;/style&amp;gt;&lt;br /&gt; &amp;lt;/head&amp;gt;&lt;br /&gt; &amp;lt;body&amp;gt;&lt;br /&gt; &amp;lt;h1&amp;gt;OficinaStudy AI&amp;lt;/h1&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;lt;!-- CHATBOX --&amp;gt;&lt;br /&gt; &amp;lt;div id=&amp;quot;chat-box&amp;quot;&amp;gt;&amp;lt;/div&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;lt;!-- INPUT BOX --&amp;gt;&lt;br /&gt; &amp;lt;input type=&amp;quot;text&amp;quot; id=&amp;quot;input-box&amp;quot; placeholder=&amp;quot;Type your message here...&amp;quot; /&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;lt;!-- SEND BUTTON --&amp;gt;&lt;br /&gt; &amp;lt;button id=&amp;quot;send-button&amp;quot;&amp;gt;Send&amp;lt;/button&amp;gt;&lt;br /&gt; &amp;lt;button id=&amp;quot;rag&amp;quot;&amp;gt;RAG&amp;lt;/button&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;lt;script&amp;gt;&lt;br /&gt; // ATRIBUIR UM VALOR AOS IDS&lt;br /&gt; const chatBox = document.getElementById(&amp;quot;chat-box&amp;quot;);&lt;br /&gt; const inputBox = document.getElementById(&amp;quot;input-box&amp;quot;);&lt;br /&gt; const sendButton = document.getElementById(&amp;quot;send-button&amp;quot;);&lt;br /&gt; const rag = document.getElementById(&amp;quot;RAG&amp;quot;); &lt;/p&gt; &lt;p&gt;// ADICIONAR ACAO AO BOTAO&lt;br /&gt; sendButton.addEventListener(&amp;quot;click&amp;quot;, async () =&amp;gt; {&lt;br /&gt; const userInput = inputBox.value; &lt;/p&gt; &lt;p&gt;// DEFINIR AS PALAVRAS CHAVE&lt;br /&gt; const keywordList = [&amp;quot;exercicio&amp;quot;, &amp;quot;escolhas&amp;quot;, &amp;quot;multiplas&amp;quot;, &amp;quot;exercício&amp;quot;, &amp;quot;múltiplas&amp;quot;, &amp;quot;escolha&amp;quot;]; &lt;/p&gt; &lt;p&gt;function checkKeywords() {&lt;br /&gt; userInputLower = userInput.toLowerCase();&lt;br /&gt; const hasKeyword = keywordList.some(keyword =&amp;gt; userInputLower.includes(keyword)); &lt;/p&gt; &lt;p&gt;if (hasKeyword) {&lt;br /&gt; alert(&amp;quot;sim!!! c:&amp;quot;);&lt;br /&gt; const newInput = document.createElement(&amp;quot;input&amp;quot;);&lt;br /&gt; newInput.type = &amp;quot;text&amp;quot;;&lt;br /&gt; &lt;a href="http://newInput.id"&gt;newInput.id&lt;/a&gt; = &amp;quot;box&amp;quot;;&lt;br /&gt; newInput.placeholder = &amp;quot;Type your message here...&amp;quot;; &lt;/p&gt; &lt;p&gt;document.body.appendChild(newInput);&lt;br /&gt; } else {&lt;br /&gt; alert(&amp;quot;nao :c&amp;quot;);&lt;br /&gt; }&lt;br /&gt; }&lt;br /&gt; checkKeywords(); &lt;/p&gt; &lt;p&gt;// RETIRAR OS ESPACOS EM BRANCO&lt;br /&gt; if (!userInput.trim()) return; &lt;/p&gt; &lt;p&gt;// ADICIONAR O USERINPUT À CHATBOX&lt;br /&gt; chatBox.innerHTML += `&amp;lt;div&amp;gt;&amp;lt;strong&amp;gt;You:&amp;lt;/strong&amp;gt; ${userInput}&amp;lt;/div&amp;gt;`;&lt;br /&gt; inputBox.value = &amp;quot;&amp;quot;; &lt;/p&gt; &lt;p&gt;// ESTABELECER LIGACAO COM O &lt;a href="http://SERVER.PY"&gt;SERVER.PY&lt;/a&gt; E TRANSFORMAR EM JSON&lt;br /&gt; try { &lt;/p&gt; &lt;p&gt;const response = await fetch(&amp;quot;http://localhost:5000/generate&amp;quot;, {&lt;br /&gt; method: &amp;quot;POST&amp;quot;,&lt;br /&gt; headers: {&lt;br /&gt; &amp;quot;Content-Type&amp;quot;: &amp;quot;application/json&amp;quot;&lt;br /&gt; },&lt;br /&gt; body: JSON.stringify({ input: userInput })&lt;br /&gt; });&lt;br /&gt; const data = await response.json(); &lt;/p&gt; &lt;p&gt;// ADICIONAR A RESPOSTA DA IA À CHATBOX&lt;br /&gt; if (data.response) {&lt;br /&gt; chatBox.innerHTML += `&amp;lt;div&amp;gt;&amp;lt;strong&amp;gt;Buddy:&amp;lt;/strong&amp;gt; ${data.response}&amp;lt;/div&amp;gt;`;&lt;br /&gt; } else {&lt;br /&gt; // DIZER QUE HÁ UM ERRO SE FOR O CASO&lt;br /&gt; chatBox.innerHTML += `&amp;lt;div&amp;gt;&amp;lt;strong&amp;gt;Buddy:&amp;lt;/strong&amp;gt; Error: ${data.error || &amp;quot;Erro desconhecido :(&amp;quot;}&amp;lt;/div&amp;gt;`;&lt;br /&gt; }&lt;br /&gt; } catch (error) {&lt;br /&gt; // DIZER SE HOUVE UM ERRO AO CONECTAR COM O SERVIDOR&lt;br /&gt; chatBox.innerHTML += `&amp;lt;div&amp;gt;&amp;lt;strong&amp;gt;Buddy:&amp;lt;/strong&amp;gt; Ops! Houve um erro ao conectar com o servidor! :( &amp;lt;/div&amp;gt;`;&lt;br /&gt; } &lt;/p&gt; &lt;p&gt;chatBox.scrollTop = chatBox.scrollHeight;&lt;br /&gt; }); &lt;/p&gt; &lt;p&gt;rag.addEventListener(&amp;quot;click&amp;quot;, async () =&amp;gt; { &lt;/p&gt; &lt;p&gt;})&lt;br /&gt; &amp;lt;/script&amp;gt;&lt;br /&gt; &amp;lt;/body&amp;gt;&lt;br /&gt; &amp;lt;/html&amp;gt;&lt;br /&gt; &amp;lt;!DOCTYPE html&amp;gt;&lt;br /&gt; &amp;lt;html lang=&amp;quot;pt-pt&amp;quot;&amp;gt;&lt;br /&gt; &amp;lt;head&amp;gt;&lt;br /&gt; &amp;lt;meta charset=&amp;quot;UTF-8&amp;quot;&amp;gt;&lt;br /&gt; &amp;lt;meta name=&amp;quot;viewport&amp;quot; content=&amp;quot;width=device-width, initial-scale=1.0&amp;quot;&amp;gt;&lt;br /&gt; &amp;lt;title&amp;gt;OficinaStudy&amp;lt;/title&amp;gt;&lt;br /&gt; &amp;lt;style&amp;gt;&lt;br /&gt; body {&lt;br /&gt; font-family: Arial, sans-serif;&lt;br /&gt; margin: 20px;&lt;br /&gt; } &lt;/p&gt; &lt;p&gt;/* STYLE CHAT BOX */&lt;br /&gt; #chat-box {&lt;br /&gt; margin: 20px 0;&lt;br /&gt; padding: 10px;&lt;br /&gt; border: 1px solid #ccc;&lt;br /&gt; max-width: 100%;&lt;br /&gt; min-height: 300px;&lt;br /&gt; overflow-y: auto;&lt;br /&gt; } &lt;/p&gt; &lt;p&gt;/* STYLE INPUT BOX */&lt;br /&gt; #input-box {&lt;br /&gt; width: calc(100% - 20px);&lt;br /&gt; padding: 10px;&lt;br /&gt; margin-bottom: 20px;&lt;br /&gt; }&lt;br /&gt; #box {&lt;br /&gt; width: calc(100% - 20px);&lt;br /&gt; padding: 10px;&lt;br /&gt; margin-bottom: 20px;&lt;br /&gt; }&lt;br /&gt; &amp;lt;/style&amp;gt;&lt;br /&gt; &amp;lt;/head&amp;gt;&lt;br /&gt; &amp;lt;body&amp;gt;&lt;br /&gt; &amp;lt;h1&amp;gt;OficinaStudy AI&amp;lt;/h1&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;lt;!-- CHATBOX --&amp;gt;&lt;br /&gt; &amp;lt;div id=&amp;quot;chat-box&amp;quot;&amp;gt;&amp;lt;/div&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;lt;!-- INPUT BOX --&amp;gt;&lt;br /&gt; &amp;lt;input type=&amp;quot;text&amp;quot; id=&amp;quot;input-box&amp;quot; placeholder=&amp;quot;Type your message here...&amp;quot; /&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;lt;!-- SEND BUTTON --&amp;gt;&lt;br /&gt; &amp;lt;button id=&amp;quot;send-button&amp;quot;&amp;gt;Send&amp;lt;/button&amp;gt;&lt;br /&gt; &amp;lt;button id=&amp;quot;rag&amp;quot;&amp;gt;RAG&amp;lt;/button&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;lt;script&amp;gt;&lt;br /&gt; // ATRIBUIR UM VALOR AOS IDS&lt;br /&gt; const chatBox = document.getElementById(&amp;quot;chat-box&amp;quot;);&lt;br /&gt; const inputBox = document.getElementById(&amp;quot;input-box&amp;quot;);&lt;br /&gt; const sendButton = document.getElementById(&amp;quot;send-button&amp;quot;);&lt;br /&gt; const rag = document.getElementById(&amp;quot;RAG&amp;quot;); &lt;/p&gt; &lt;p&gt;// ADICIONAR ACAO AO BOTAO&lt;br /&gt; sendButton.addEventListener(&amp;quot;click&amp;quot;, async () =&amp;gt; {&lt;br /&gt; const userInput = inputBox.value; &lt;/p&gt; &lt;p&gt;// DEFINIR AS PALAVRAS CHAVE&lt;br /&gt; const keywordList = [&amp;quot;exercicio&amp;quot;, &amp;quot;escolhas&amp;quot;, &amp;quot;multiplas&amp;quot;, &amp;quot;exercício&amp;quot;, &amp;quot;múltiplas&amp;quot;, &amp;quot;escolha&amp;quot;]; &lt;/p&gt; &lt;p&gt;function checkKeywords() {&lt;br /&gt; userInputLower = userInput.toLowerCase();&lt;br /&gt; const hasKeyword = keywordList.some(keyword =&amp;gt; userInputLower.includes(keyword)); &lt;/p&gt; &lt;p&gt;if (hasKeyword) {&lt;br /&gt; alert(&amp;quot;sim!!! c:&amp;quot;);&lt;br /&gt; const newInput = document.createElement(&amp;quot;input&amp;quot;);&lt;br /&gt; newInput.type = &amp;quot;text&amp;quot;;&lt;br /&gt; &lt;a href="http://newInput.id"&gt;newInput.id&lt;/a&gt; = &amp;quot;box&amp;quot;;&lt;br /&gt; newInput.placeholder = &amp;quot;Type your message here...&amp;quot;; &lt;/p&gt; &lt;p&gt;document.body.appendChild(newInput);&lt;br /&gt; } else {&lt;br /&gt; alert(&amp;quot;nao :c&amp;quot;);&lt;br /&gt; }&lt;br /&gt; }&lt;br /&gt; checkKeywords(); &lt;/p&gt; &lt;p&gt;// RETIRAR OS ESPACOS EM BRANCO&lt;br /&gt; if (!userInput.trim()) return; &lt;/p&gt; &lt;p&gt;// ADICIONAR O USERINPUT À CHATBOX&lt;br /&gt; chatBox.innerHTML += `&amp;lt;div&amp;gt;&amp;lt;strong&amp;gt;You:&amp;lt;/strong&amp;gt; ${userInput}&amp;lt;/div&amp;gt;`;&lt;br /&gt; inputBox.value = &amp;quot;&amp;quot;; &lt;/p&gt; &lt;p&gt;// ESTABELECER LIGACAO COM O &lt;a href="http://SERVER.PY"&gt;SERVER.PY&lt;/a&gt; E TRANSFORMAR EM JSON&lt;br /&gt; try { &lt;/p&gt; &lt;p&gt;const response = await fetch(&amp;quot;http://localhost:5000/generate&amp;quot;, {&lt;br /&gt; method: &amp;quot;POST&amp;quot;,&lt;br /&gt; headers: {&lt;br /&gt; &amp;quot;Content-Type&amp;quot;: &amp;quot;application/json&amp;quot;&lt;br /&gt; },&lt;br /&gt; body: JSON.stringify({ input: userInput })&lt;br /&gt; });&lt;br /&gt; const data = await response.json(); &lt;/p&gt; &lt;p&gt;// ADICIONAR A RESPOSTA DA IA À CHATBOX&lt;br /&gt; if (data.response) {&lt;br /&gt; chatBox.innerHTML += `&amp;lt;div&amp;gt;&amp;lt;strong&amp;gt;Buddy:&amp;lt;/strong&amp;gt; ${data.response}&amp;lt;/div&amp;gt;`;&lt;br /&gt; } else {&lt;br /&gt; // DIZER QUE HÁ UM ERRO SE FOR O CASO&lt;br /&gt; chatBox.innerHTML += `&amp;lt;div&amp;gt;&amp;lt;strong&amp;gt;Buddy:&amp;lt;/strong&amp;gt; Error: ${data.error || &amp;quot;Erro desconhecido :(&amp;quot;}&amp;lt;/div&amp;gt;`;&lt;br /&gt; }&lt;br /&gt; } catch (error) {&lt;br /&gt; // DIZER SE HOUVE UM ERRO AO CONECTAR COM O SERVIDOR&lt;br /&gt; chatBox.innerHTML += `&amp;lt;div&amp;gt;&amp;lt;strong&amp;gt;Buddy:&amp;lt;/strong&amp;gt; Ops! Houve um erro ao conectar com o servidor! :( &amp;lt;/div&amp;gt;`;&lt;br /&gt; } &lt;/p&gt; &lt;p&gt;chatBox.scrollTop = chatBox.scrollHeight;&lt;br /&gt; }); &lt;/p&gt; &lt;p&gt;rag.addEventListener(&amp;quot;click&amp;quot;, async () =&amp;gt; { &lt;/p&gt; &lt;p&gt;})&lt;br /&gt; &amp;lt;/script&amp;gt;&lt;br /&gt; &amp;lt;/body&amp;gt;&lt;br /&gt; &amp;lt;/html&amp;gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;the&lt;/strong&gt; &lt;a href="http://server.py"&gt;&lt;strong&gt;server.py&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;from typing import Dict&lt;br /&gt; from fastapi import FastAPI, HTTPException&lt;br /&gt; from fastapi.middleware.cors import CORSMiddleware&lt;br /&gt; from pydantic import BaseModel&lt;br /&gt; import ollama &lt;/p&gt; &lt;p&gt;app = FastAPI() &lt;/p&gt; &lt;p&gt;# Enable CORS&lt;br /&gt; app.add_middleware(&lt;br /&gt; CORSMiddleware,&lt;br /&gt; allow_origins=[&amp;quot;*&amp;quot;], # Adjust this for security in production&lt;br /&gt; allow_credentials=True,&lt;br /&gt; allow_methods=[&amp;quot;*&amp;quot;],&lt;br /&gt; allow_headers=[&amp;quot;*&amp;quot;],&lt;br /&gt; ) &lt;/p&gt; &lt;p&gt;model = &amp;quot;gemma2mod3&amp;quot; # Model name&lt;br /&gt; conversation_history = [] # Store conversation history &lt;/p&gt; &lt;p&gt;# Define request body model&lt;br /&gt; class UserInput(BaseModel):&lt;br /&gt; input: str &lt;/p&gt; &lt;p&gt;&lt;a href="/u/app"&gt;u/app&lt;/a&gt;.post(&amp;quot;/generate&amp;quot;)&lt;br /&gt; async def generate_response(user_input: UserInput) -&amp;gt; Dict[str, str]:&lt;br /&gt; try:&lt;br /&gt; global conversation_history &lt;/p&gt; &lt;p&gt;if not user_input.input:&lt;br /&gt; raise HTTPException(status_code=400, detail=&amp;quot;No input provided&amp;quot;) &lt;/p&gt; &lt;p&gt;# Add user message to history&lt;br /&gt; conversation_history.append({&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: user_input.input}) &lt;/p&gt; &lt;p&gt;# Format conversation history&lt;br /&gt; formatted_history = &amp;quot;\n&amp;quot;.join(&lt;br /&gt; [f&amp;quot;{msg['role'].capitalize()}: {msg['content']}&amp;quot; for msg in conversation_history]&lt;br /&gt; ) &lt;/p&gt; &lt;p&gt;# Generate response&lt;br /&gt; response = ollama.generate(model=model, prompt=formatted_history)&lt;br /&gt; assistant_response = response.get('response', &amp;quot;&amp;quot;) &lt;/p&gt; &lt;p&gt;# Add assistant response to history&lt;br /&gt; conversation_history.append({&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: assistant_response}) &lt;/p&gt; &lt;p&gt;return {&amp;quot;response&amp;quot;: assistant_response} &lt;/p&gt; &lt;p&gt;except Exception as e:&lt;br /&gt; raise HTTPException(status_code=500, detail=str(e)) &lt;/p&gt; &lt;p&gt;if __name__ == &amp;quot;__main__&amp;quot;:&lt;br /&gt; import uvicorn&lt;br /&gt; uvicorn.run(app, host=&amp;quot;0.0.0.0&amp;quot;, port=5000)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The rag for terminal&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;import torch&lt;br /&gt; from sentence_transformers import SentenceTransformer, util&lt;br /&gt; import os&lt;br /&gt; from openai import OpenAI&lt;br /&gt; import argparse &lt;/p&gt; &lt;p&gt;# Function to open a file and return its contents as a string&lt;br /&gt; def open_file(filepath):&lt;br /&gt; with open(filepath, 'r', encoding='utf-8') as infile:&lt;br /&gt; return infile.read() &lt;/p&gt; &lt;p&gt;# Function to get relevant context from the vault based on user input&lt;br /&gt; def get_relevant_context(user_input, vault_embeddings, vault_content, model, top_k=2):&lt;br /&gt; if vault_embeddings.nelement() == 0: # Check if the tensor has any elements&lt;br /&gt; return []&lt;br /&gt; # Encode the user input&lt;br /&gt; input_embedding = model.encode([user_input])&lt;br /&gt; # Compute cosine similarity between the input and vault embeddings&lt;br /&gt; cos_scores = util.cos_sim(input_embedding, vault_embeddings)[0]&lt;br /&gt; # Adjust top_k if it's greater than the number of available scores&lt;br /&gt; top_k = min(top_k, len(cos_scores))&lt;br /&gt; # Sort the scores and get the top-k indices&lt;br /&gt; top_indices = torch.topk(cos_scores, k=top_k)[1].tolist()&lt;br /&gt; # Get the corresponding context from the vault&lt;br /&gt; relevant_context = [vault_content[idx].strip() for idx in top_indices]&lt;br /&gt; return relevant_context &lt;/p&gt; &lt;p&gt;# Function to interact with the Ollama model&lt;br /&gt; def ollama_chat(user_input, system_message, vault_embeddings, vault_content, model, ollama_model, conversation_history):&lt;br /&gt; relevant_context = []&lt;br /&gt; user_input = user_input.replace(&amp;quot;search_vault&amp;quot;, &amp;quot;&amp;quot;).strip()&lt;br /&gt; relevant_context = get_relevant_context(user_input, vault_embeddings, vault_content, model) &lt;/p&gt; &lt;p&gt;if relevant_context:&lt;br /&gt; context_str = &amp;quot;\n&amp;quot;.join(relevant_context)&lt;br /&gt; print(&amp;quot;Context Pulled from Documents: \n\n&amp;quot; + context_str)&lt;br /&gt; user_input_with_context = context_str + &amp;quot;\n\n&amp;quot; + user_input&lt;br /&gt; else:&lt;br /&gt; user_input_with_context = user_input &lt;/p&gt; &lt;p&gt;conversation_history.append({&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: user_input_with_context})&lt;br /&gt; messages = [&lt;br /&gt; {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: system_message},&lt;br /&gt; *conversation_history&lt;br /&gt; ] &lt;/p&gt; &lt;p&gt;response = client.chat.completions.create(&lt;br /&gt; model=ollama_model,&lt;br /&gt; messages=messages&lt;br /&gt; ) &lt;/p&gt; &lt;p&gt;response_text = response.choices[0].message.content&lt;br /&gt; conversation_history.append({&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: response_text})&lt;br /&gt; return response_text &lt;/p&gt; &lt;p&gt;# Configuration for the Ollama API client&lt;br /&gt; client = OpenAI(&lt;br /&gt; base_url='http://localhost:11434/v1',&lt;br /&gt; api_key='llama3'&lt;br /&gt; ) &lt;/p&gt; &lt;p&gt;# Parse command-line arguments&lt;br /&gt; parser = argparse.ArgumentParser(description=&amp;quot;Ollama Chat&amp;quot;)&lt;br /&gt; parser.add_argument(&amp;quot;--model&amp;quot;, default=&amp;quot;Oficina-AI&amp;quot;, help=&amp;quot;Ollama model to use (default: Oficina-AI)&amp;quot;)&lt;br /&gt; args = parser.parse_args() &lt;/p&gt; &lt;p&gt;# Load the model and vault content&lt;br /&gt; model = SentenceTransformer(&amp;quot;all-MiniLM-L6-v2&amp;quot;)&lt;br /&gt; vault_content = []&lt;br /&gt; if os.path.exists(&amp;quot;vault.txt&amp;quot;):&lt;br /&gt; with open(&amp;quot;vault.txt&amp;quot;, &amp;quot;r&amp;quot;, encoding='utf-8') as vault_file:&lt;br /&gt; vault_content = vault_file.readlines()&lt;br /&gt; vault_embeddings = model.encode(vault_content) if vault_content else [] &lt;/p&gt; &lt;p&gt;vault_embeddings_tensor = torch.tensor(vault_embeddings)&lt;br /&gt; conversation_history = []&lt;br /&gt; system_message = &amp;quot;You are a helpful assistant that helps students by providing exercises and explanations using available resources. If information is found in the vault, it must be considered absolute truth. You should base your reasoning and opinions strictly on what is written in the &lt;a href="http://vault.You"&gt;vault.You&lt;/a&gt; are also an artificial inteligence helping students from all around the world study and have better grades, you should try to get used to any user that talks to you by imitating their behaviour, humor, and the way they talk to you, your principal job is to give students exercises when those are asked, those exercises could be for an example, true or false with or without justificating the falses, multiple choice, writting an answer or any other type of exercise that they ask. You should try to make them feel confortable, and when they ask you to explain something, you will explaint it.&amp;quot; &lt;/p&gt; &lt;p&gt;while True:&lt;br /&gt; user_input = input(&amp;quot;&amp;gt;&amp;gt;&amp;gt; &amp;quot;)&lt;br /&gt; if user_input.lower() == 'quit':&lt;br /&gt; break &lt;/p&gt; &lt;p&gt;response = ollama_chat(user_input, system_message, vault_embeddings_tensor, vault_content, model, conversation_history)&lt;br /&gt; #response = traduzir_para_pt_pt(response)&lt;br /&gt; print(&amp;quot;Response: \n\n&amp;quot; + response)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Impact4403"&gt; /u/Ok_Impact4403 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7iso1/i_cant_make_a_rag_system_with_fastapi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7iso1/i_cant_make_a_rag_system_with_fastapi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7iso1/i_cant_make_a_rag_system_with_fastapi/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T21:54:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j780k8</id>
    <title>Instructions in python SDK to use models as translators.</title>
    <updated>2025-03-09T13:49:14+00:00</updated>
    <author>
      <name>/u/nosumable</name>
      <uri>https://old.reddit.com/user/nosumable</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, new in this beautiful community!&lt;/p&gt; &lt;p&gt;Some days ago restarted a project to translate Chinese text from table tennis videos with my 16 GB vram gpu. In the past I used gCloud API to do the OCR and translation, the OCR was good but the translation was horrible.&lt;/p&gt; &lt;p&gt;I decided to go OpenSource. For the OCR I chose to use paddleocr (it works great) and for the translation I have found models as chatgpt Claude or deepseek works extremely good. So I decided to try a local approach with deepseek. The problem here arises, I cannot control what the model output gives, even if I order it to give the translation in a specific format to parse it after. Several question arises:&lt;/p&gt; &lt;p&gt;1) How do you handle this, I have read some other SDK have more methods that might me suitable for this&lt;/p&gt; &lt;p&gt;2) are there specific models that work better with translations? I was using 32b deepseek R1, but it might be overkill as speed translating is slow (performance is not a must, but if I can get some lighter model it would be nice)&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nosumable"&gt; /u/nosumable &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j780k8/instructions_in_python_sdk_to_use_models_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j780k8/instructions_in_python_sdk_to_use_models_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j780k8/instructions_in_python_sdk_to_use_models_as/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T13:49:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7et4q</id>
    <title>Finetuning Llama 3.2 to Generate ASCII Cats (Full Tutorial)</title>
    <updated>2025-03-09T19:01:00+00:00</updated>
    <author>
      <name>/u/YungMixtape2004</name>
      <uri>https://old.reddit.com/user/YungMixtape2004</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j7et4q/finetuning_llama_32_to_generate_ascii_cats_full/"&gt; &lt;img alt="Finetuning Llama 3.2 to Generate ASCII Cats (Full Tutorial)" src="https://external-preview.redd.it/wZRAp0Z9Mp9JFxo0ZyDa-g6jwhUluZPSeazeek3nw58.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9aa8adb1463dff4004332fd167e93588aafe3ea2" title="Finetuning Llama 3.2 to Generate ASCII Cats (Full Tutorial)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YungMixtape2004"&gt; /u/YungMixtape2004 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/-H1-lr_sIZk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7et4q/finetuning_llama_32_to_generate_ascii_cats_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7et4q/finetuning_llama_32_to_generate_ascii_cats_full/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T19:01:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6ydpa</id>
    <title>Latest qwq thinking model with unsloth parameters</title>
    <updated>2025-03-09T03:15:12+00:00</updated>
    <author>
      <name>/u/DanielUpsideDown</name>
      <uri>https://old.reddit.com/user/DanielUpsideDown</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unsloth published an article on how to run qwq with optimized parameters &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"&gt;here&lt;/a&gt;. I made a modelfile and uploaded it to ollama - &lt;a href="https://ollama.com/driftfurther/qwq-unsloth"&gt;https://ollama.com/driftfurther/qwq-unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It fits perfectly into 24 GB VRAM and it is amazing at its performance. Coding in particular has been incredible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanielUpsideDown"&gt; /u/DanielUpsideDown &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j6ydpa/latest_qwq_thinking_model_with_unsloth_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j6ydpa/latest_qwq_thinking_model_with_unsloth_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j6ydpa/latest_qwq_thinking_model_with_unsloth_parameters/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T03:15:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7fqdu</id>
    <title>Best model for questions about PC hardware</title>
    <updated>2025-03-09T19:40:44+00:00</updated>
    <author>
      <name>/u/haemakatus</name>
      <uri>https://old.reddit.com/user/haemakatus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was wondering if there is a Ollama model trained on PC components such as motherboard chipsets, memory, GPUs etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/haemakatus"&gt; /u/haemakatus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7fqdu/best_model_for_questions_about_pc_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7fqdu/best_model_for_questions_about_pc_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7fqdu/best_model_for_questions_about_pc_hardware/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T19:40:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7ci9z</id>
    <title>Apple specs in the future</title>
    <updated>2025-03-09T17:21:06+00:00</updated>
    <author>
      <name>/u/jamboman_</name>
      <uri>https://old.reddit.com/user/jamboman_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Started to use ollama about a week ago. I use a Mac mini M2 and have 256gb, with 24gb ram.&lt;/p&gt; &lt;p&gt;It works great, and I have no complaints.&lt;/p&gt; &lt;p&gt;But it made me think...we know that AI is going to rapidly improve, and things are going to change wildly. So...with that in mind, and with apple making machines with everything on one chip, it's going to mean that we could be wanting to upgrade machines more and more frequently in the future.&lt;/p&gt; &lt;p&gt;I want to upgrade today, but I also want to know that should better LLMs come out, with more demands, that I can upgrade to maintain performance.&lt;/p&gt; &lt;p&gt;Sorry of this has been asked before &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jamboman_"&gt; /u/jamboman_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7ci9z/apple_specs_in_the_future/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7ci9z/apple_specs_in_the_future/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7ci9z/apple_specs_in_the_future/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T17:21:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7kp2l</id>
    <title>Best model for text summarization (2025)</title>
    <updated>2025-03-09T23:20:53+00:00</updated>
    <author>
      <name>/u/Unhappy_Bunch</name>
      <uri>https://old.reddit.com/user/Unhappy_Bunch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run Ollama on my desktop with 64GB ram and an RTX4080. I currently use llama3.1 8B for summarization text of all types. &lt;/p&gt; &lt;p&gt;What other models do you guys suggest that might be more accurate? &lt;/p&gt; &lt;p&gt;What other tips do you have for accuracy? &lt;/p&gt; &lt;p&gt;TIA&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unhappy_Bunch"&gt; /u/Unhappy_Bunch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7kp2l/best_model_for_text_summarization_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7kp2l/best_model_for_text_summarization_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7kp2l/best_model_for_text_summarization_2025/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T23:20:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7rit5</id>
    <title>What PSU for dual 3090</title>
    <updated>2025-03-10T05:27:44+00:00</updated>
    <author>
      <name>/u/Timziito</name>
      <uri>https://old.reddit.com/user/Timziito</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey fellow humans 🙂 I have been able to get two 3090 msi cards with three 8 pins per gpu. &lt;/p&gt; &lt;p&gt;What would be an reasonable power supply? And atx3.0 or atx3.1&lt;/p&gt; &lt;p&gt;Best regards Tim&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Timziito"&gt; /u/Timziito &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7rit5/what_psu_for_dual_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7rit5/what_psu_for_dual_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7rit5/what_psu_for_dual_3090/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T05:27:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7p4sg</id>
    <title>Learning question</title>
    <updated>2025-03-10T03:05:02+00:00</updated>
    <author>
      <name>/u/tshawkins</name>
      <uri>https://old.reddit.com/user/tshawkins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What would be the problems associated having a RAG based AI, self update. &lt;/p&gt; &lt;p&gt;Often when conversing with an AI, it will say something outright false, would it be feasable to determine a command with a corrective intent, and then insert the correction into the RAG database as a high weight fact. &lt;/p&gt; &lt;p&gt;Something like. &lt;/p&gt; &lt;p&gt;AI: the eiffel tower is is in london. &lt;/p&gt; &lt;p&gt;Me: that is not correct, the eiffel tower is in paris. &lt;/p&gt; &lt;p&gt;AI: sorry, do you want me to remember that the eiffel tower is in paris?&lt;/p&gt; &lt;p&gt;Me: yes&lt;/p&gt; &lt;p&gt;AI: the location of the eiffel tower has been updated.&lt;/p&gt; &lt;p&gt;Me: where is the eiffle tower.&lt;/p&gt; &lt;p&gt;AI: the eiffle tower is in paris. &lt;/p&gt; &lt;p&gt;Note: that an AI will appear to do this right now, but as soon as the session ends, all facts learned are forgotten. with a self updating RAG system it will becom part of its permenant memory. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tshawkins"&gt; /u/tshawkins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7p4sg/learning_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7p4sg/learning_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7p4sg/learning_question/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T03:05:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j72bsk</id>
    <title>MY JARVIS PROJECT</title>
    <updated>2025-03-09T07:20:44+00:00</updated>
    <author>
      <name>/u/cython_boy</name>
      <uri>https://old.reddit.com/user/cython_boy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! So I’ve been messing around with AI and ended up building Jarvis , my own personal assistant. It listens for “Hey Jarvis” understands what I need, and does things like sending emails, making calls, checking the weather, and more. It’s all powered by Gemini AI and ollama . with some smart intent handling using LangChain. (using ibm granite-dense models with gemini.)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ganeshnikhil/J.A.R.V.I.S.2.0"&gt;Github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Listens to my voice 🎙️ &lt;/p&gt; &lt;p&gt;- Figures out if it needs AI, a function call , agentic modes , or a quick response &lt;/p&gt; &lt;p&gt;- Executes tasks like emailing, news updates, rag knowledge base or even making calls (adb).&lt;/p&gt; &lt;p&gt;- Handles errors without breaking (because trust me, it broke a lot at first) &lt;/p&gt; &lt;p&gt;- **Wake word chaos** – It kept activating randomly, had to fine-tune that &lt;/p&gt; &lt;p&gt;- **Task confusion** – Balancing AI responses with simple predefined actions , mixed approach.&lt;/p&gt; &lt;p&gt;- **Complex queries** – Ended up using ML to route requests properly &lt;/p&gt; &lt;p&gt;Review my project , I want a feedback to improve it furthure , i am open for all kind of suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cython_boy"&gt; /u/cython_boy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j72bsk/my_jarvis_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j72bsk/my_jarvis_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j72bsk/my_jarvis_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-09T07:20:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7ntrn</id>
    <title>I Fine-Tuned a Tiny LLM to Write Git Commits Offline—Check It Out!</title>
    <updated>2025-03-10T01:56:46+00:00</updated>
    <author>
      <name>/u/VictorCTavernari</name>
      <uri>https://old.reddit.com/user/VictorCTavernari</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good evening, Ollama community!&lt;/p&gt; &lt;p&gt;I've been an enthusiast of local open-source LLMs for about a year now. Typically, I prefer keeping my git commits small with clear, meaningful messages, especially when working with others. When ChatGPT launched GPTs, I created a dedicated model for writing commit messages: &lt;a href="https://chatgpt.com/g/g-1RdmhTAHg-git-commit-message-pro"&gt;Git Commit Message Pro&lt;/a&gt;. However, I encountered some privacy limitations, which led me to explore fine-tuning my own local LLM that could produce an initial draft requiring minimal edits. Using Ollama, I built &lt;a href="https://ollama.com/tavernari/git-commit-message"&gt;tavernari/git-commit-message&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;tavernari/git-commit-message&lt;/h1&gt; &lt;p&gt;In my first version, I used the 7B Mistral model, which occupies about 4.4 GB. While functional, it was resource-intensive and often produced slow and unsatisfactory responses.&lt;/p&gt; &lt;p&gt;Recently, there has been considerable hype around DeepSeekR1, a smaller model trained to &amp;quot;think&amp;quot; more effectively. Inspired by this, I created a smaller, reasoning-focused version dedicated specifically to writing commit messages.&lt;/p&gt; &lt;p&gt;This was my first attempt at fine-tuning. Although the results aren't perfect yet, I believe that with further training and refinement, I can achieve better outcomes.&lt;/p&gt; &lt;p&gt;Hence, I introduced the &amp;quot;reasoning&amp;quot; version: &lt;a href="https://ollama.com/tavernari/git-commit-message:reasoning"&gt;tavernari/git-commit-message:reasoning&lt;/a&gt;. This version uses a small 3B model (1.9 GB) optimized for enhanced reasoning capabilities. Additionally, I developed another version leveraging Chain of Thought (&lt;a href="https://arxiv.org/pdf/2502.18600"&gt;Chain of Thought&lt;/a&gt;), which also showed promising results, though it hasn't been deeply explored yet.&lt;/p&gt; &lt;h1&gt;Agentic Git Commit Message&lt;/h1&gt; &lt;p&gt;Despite its decent performance, the model struggled with larger contexts. To address this, I created an agentic bash script that incrementally evaluates git diffs, helping the LLM generate commits without losing context.&lt;/p&gt; &lt;p&gt;Script functionalities include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Adding context to improve commit message quality.&lt;/li&gt; &lt;li&gt;Editing the generated message before committing.&lt;/li&gt; &lt;li&gt;Generating only the message with the --only-message option.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Installation is straightforward and explained on the model’s profile page: &lt;a href="https://ollama.com/tavernari/git-commit-message:reasoning"&gt;tavernari/git-commit-message:reasoning&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Project Goal&lt;/h1&gt; &lt;p&gt;My goal is to provide commit messages that are sufficiently good, needing only minor manual adjustments, and most importantly, functioning completely offline to ensure your intellectual work remains secure and private.&lt;/p&gt; &lt;p&gt;I've invested some financial resources into the fine-tuning process, aiming ultimately to create something beneficial for the community. In the future, I'll continue dedicating time to training and refining the model to enhance its quality.&lt;/p&gt; &lt;p&gt;The idea is to offer a practical, efficient tool that prioritizes the security and privacy of your work.&lt;/p&gt; &lt;p&gt;Feel free to use, suggest improvements, and collaborate!&lt;/p&gt; &lt;p&gt;My HuggingFace: &lt;a href="https://huggingface.co/Tavernari/git-commit-message"&gt;https://huggingface.co/Tavernari/git-commit-message&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VictorCTavernari"&gt; /u/VictorCTavernari &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7ntrn/i_finetuned_a_tiny_llm_to_write_git_commits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7ntrn/i_finetuned_a_tiny_llm_to_write_git_commits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7ntrn/i_finetuned_a_tiny_llm_to_write_git_commits/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T01:56:46+00:00</published>
  </entry>
</feed>
