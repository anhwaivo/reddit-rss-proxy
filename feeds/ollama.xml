<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-19T00:25:14+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1is52a1</id>
    <title>Is there a ISO/USB distro to install "everything" to dedicated PC</title>
    <updated>2025-02-18T05:09:32+00:00</updated>
    <author>
      <name>/u/DAE51D</name>
      <uri>https://old.reddit.com/user/DAE51D</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a decent HP 800 g9 which uses Intel Arc (not ideal i know) and is an i5 it's 32 gigs. I'm wondering if there's an ISO or USB that I can download and install on this PC as a dedicated AI ollama (or other). It seems most of the installers run on an existing system which IMHO seems like it has a lot of wasted resources for things that I don't need to run the models. I'm just getting into all of this so I'm not trying to run a 40 B model or anything, but the lxc I set up on proxmox is painfully slow and feels like an old 300 baud modem. I know I can buy a $300 or more Nvidia card but was hoping for now to use the hardware I have access to and upgrade later if this is a thing I want to keep using versus just buying a Claude subscription or something. This can also upgrade to the optional NVIDIA® GeForce® RTX 3050Ti graphics (if that helps)&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.hp.com/us-en/shop/mdp/desktops-and-workstations/elitedesk-800-mini#S800"&gt;https://www.hp.com/us-en/shop/mdp/desktops-and-workstations/elitedesk-800-mini#S800&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If such a distro does not exist what is the best way to install a dedicated LLM system. I presume I use a version of Linux? And what installer or do I go with Docker containers or some other method?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DAE51D"&gt; /u/DAE51D &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is52a1/is_there_a_isousb_distro_to_install_everything_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is52a1/is_there_a_isousb_distro_to_install_everything_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is52a1/is_there_a_isousb_distro_to_install_everything_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T05:09:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1is11ld</id>
    <title>Testing cards (AMD Instinct Mi50s) 14 out of 14 tested good! 12 more to go..</title>
    <updated>2025-02-18T01:37:49+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1is0poz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is11ld/testing_cards_amd_instinct_mi50s_14_out_of_14/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is11ld/testing_cards_amd_instinct_mi50s_14_out_of_14/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T01:37:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1irhbrl</id>
    <title>PrivateLLMLens - A Zero-Server Web Interface for Local Ollama</title>
    <updated>2025-02-17T10:58:39+00:00</updated>
    <author>
      <name>/u/Specialist_Laugh_231</name>
      <uri>https://old.reddit.com/user/Specialist_Laugh_231</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1irhbrl/privatellmlens_a_zeroserver_web_interface_for/"&gt; &lt;img alt="PrivateLLMLens - A Zero-Server Web Interface for Local Ollama" src="https://preview.redd.it/5bfizklmkoje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4051a019d044303e3381fd284e4eee1a8ace21b9" title="PrivateLLMLens - A Zero-Server Web Interface for Local Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist_Laugh_231"&gt; /u/Specialist_Laugh_231 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5bfizklmkoje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irhbrl/privatellmlens_a_zeroserver_web_interface_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irhbrl/privatellmlens_a_zeroserver_web_interface_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T10:58:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1irwerk</id>
    <title>Found 10 T4 GPU's</title>
    <updated>2025-02-17T22:09:08+00:00</updated>
    <author>
      <name>/u/ShortSpinach5484</name>
      <uri>https://old.reddit.com/user/ShortSpinach5484</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello community. I was decommissioning 10 old vmware host at work and found out that there was a 70w fanless T4 Gpu in each host. And I got ok to build a gpu farm to run local llms on them. But how should i build a gpu farm? Shure i can install debian/ubuntu on everything but is there a easy way to build a gpu farm?&lt;/p&gt; &lt;p&gt;Is there a easy way to do something like google colabs or kaggle?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShortSpinach5484"&gt; /u/ShortSpinach5484 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irwerk/found_10_t4_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irwerk/found_10_t4_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irwerk/found_10_t4_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T22:09:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1is89dr</id>
    <title>Trying to finetune a model based on a text file</title>
    <updated>2025-02-18T08:43:56+00:00</updated>
    <author>
      <name>/u/thecrazytughlaq</name>
      <uri>https://old.reddit.com/user/thecrazytughlaq</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am fairly new to ollama and GenAI.&lt;/p&gt; &lt;p&gt;I am trying to build a custom model from existing model (llama3.2 for now) and run the new model in my streamlit notebook. &lt;/p&gt; &lt;p&gt;I have a pdf containing the data which I converted to .txt after cleaning (around 10k characters).&lt;/p&gt; &lt;p&gt;I tried doing it by copy pasting the text into the SYSTEM part of the modelfile and creating a new model using the command&lt;/p&gt; &lt;p&gt;ollama create name -f test.modelfile&lt;/p&gt; &lt;p&gt;but it does not give me the results, although the model is created, it gives no relevant results.&lt;/p&gt; &lt;p&gt;How do I do this?&lt;/p&gt; &lt;p&gt;TLDR - I have a text file, I want to use it to train a ollama model and then use the model for creating streamlit notebook. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecrazytughlaq"&gt; /u/thecrazytughlaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is89dr/trying_to_finetune_a_model_based_on_a_text_file/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is89dr/trying_to_finetune_a_model_based_on_a_text_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is89dr/trying_to_finetune_a_model_based_on_a_text_file/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T08:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1isc3nh</id>
    <title>Need help with text extraction from images (and PDFs) for verification purposes – which OpenAI model or alternative should I use?</title>
    <updated>2025-02-18T12:59:11+00:00</updated>
    <author>
      <name>/u/waeljlassii</name>
      <uri>https://old.reddit.com/user/waeljlassii</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm working on a project where I need to extract text from images, and in some cases, PDFs, for verification purposes. I know that OpenAI has some models capable of handling text extraction, but I'm wondering which one would be the best fit for this kind of task via API.&lt;/p&gt; &lt;p&gt;Has anyone here worked on something similar or can recommend a suitable model or API? Any alternatives to OpenAI models that might be better for this task? I'd appreciate any advice or pointers from those who have experience with image and PDF text extraction.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waeljlassii"&gt; /u/waeljlassii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isc3nh/need_help_with_text_extraction_from_images_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isc3nh/need_help_with_text_extraction_from_images_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isc3nh/need_help_with_text_extraction_from_images_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T12:59:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1isczhb</id>
    <title>I Cannot Uninstall Ollama on Windows 11</title>
    <updated>2025-02-18T13:44:56+00:00</updated>
    <author>
      <name>/u/petercli</name>
      <uri>https://old.reddit.com/user/petercli</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1isczhb/i_cannot_uninstall_ollama_on_windows_11/"&gt; &lt;img alt="I Cannot Uninstall Ollama on Windows 11" src="https://b.thumbs.redditmedia.com/1xDxlViYPCkuD2zsHQomVLt12ZdjQq0UpHMAY9iVxtw.jpg" title="I Cannot Uninstall Ollama on Windows 11" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vm2ueqv0iwje1.png?width=1022&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0565f10d8157917a109a15dfc425ad8698f7c5e"&gt;https://preview.redd.it/vm2ueqv0iwje1.png?width=1022&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0565f10d8157917a109a15dfc425ad8698f7c5e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9d4dq716iwje1.png?width=563&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5a099c2cb0dd78f53cbf59cc2f89f6ccd4cb6e8"&gt;https://preview.redd.it/9d4dq716iwje1.png?width=563&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5a099c2cb0dd78f53cbf59cc2f89f6ccd4cb6e8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Folder C:\Users\xx\AppData\Local\Programs\Ollama exists , but does not have the file listed above. &lt;/p&gt; &lt;p&gt;Can I just delete C:\Users\xx\.ollama folder ? &lt;/p&gt; &lt;p&gt;Task manager shows Ollama running. &lt;/p&gt; &lt;p&gt;PS&amp;gt; I've written Professional Windows installers - LMK if you want free help .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/petercli"&gt; /u/petercli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isczhb/i_cannot_uninstall_ollama_on_windows_11/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isczhb/i_cannot_uninstall_ollama_on_windows_11/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isczhb/i_cannot_uninstall_ollama_on_windows_11/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T13:44:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1irrxqs</id>
    <title>DataBridge Feature Dump: We'll implement anything you want!!</title>
    <updated>2025-02-17T19:09:59+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! &lt;/p&gt; &lt;p&gt;Thanks to the power of the &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; community, &lt;a href="https://github.com/databridge-org/databridge-core"&gt;DataBridge&lt;/a&gt; just hit 350 stars! As a token of our gratitude, we're committing to implementing the &lt;strong&gt;top 3 feature requests&lt;/strong&gt; from &lt;em&gt;you&lt;/em&gt; :) &lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to participate:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Leave your dream feature or improvement - RAG or otherwise - as a reply to this post! Upvote existing ideas you’d love to see. We’ll tally the votes and build the &lt;strong&gt;top 3 most-requested features&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Let’s shape DataBridge’s future together—drop your requests below! 🚀&lt;/p&gt; &lt;p&gt;&lt;em&gt;(We'll start tallying at 2:00 pm ET on the 18th of Feb - happy to start working on stuff before that tho!)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Huge thanks again for being part of this journey! 🙌 ❤️&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irrxqs/databridge_feature_dump_well_implement_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irrxqs/databridge_feature_dump_well_implement_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irrxqs/databridge_feature_dump_well_implement_anything/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T19:09:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1irvwt0</id>
    <title>CheerBot. Ollama based Cheer or Nudge through OCR of your screen.</title>
    <updated>2025-02-17T21:48:43+00:00</updated>
    <author>
      <name>/u/maranone5</name>
      <uri>https://old.reddit.com/user/maranone5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Just a quick and dirty proof of concept!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here's a little productivity monitor I threw together:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Take two screenshots&lt;/strong&gt; of your screen at a predefined time difference.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OCR with Tesseract&lt;/strong&gt; to extract text (sorry graphic designers, no design elements here!).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Analyze the text&lt;/strong&gt; through Ollama (basically, your personal productivity assistant).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Nudge or Cheer&lt;/strong&gt; accordingly using a pet personality for a bit of fun motivation.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;No APIs, no cloud – all execution is &lt;strong&gt;local&lt;/strong&gt; and &lt;strong&gt;safe&lt;/strong&gt; on your machine. Think of it like a modern-day Clippy.&lt;/p&gt; &lt;p&gt;The current version is a bit of a &lt;strong&gt;CLI mess&lt;/strong&gt; – if anyone’s up for a challenge, feel free to create a &lt;strong&gt;GUI or popup version&lt;/strong&gt; to make it more user-friendly and visually appealing!&lt;/p&gt; &lt;p&gt;Feel free to check it out and let me know what you think! 🙌&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/maranone/CheerBot"&gt;https://github.com/maranone/CheerBot&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maranone5"&gt; /u/maranone5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irvwt0/cheerbot_ollama_based_cheer_or_nudge_through_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1irvwt0/cheerbot_ollama_based_cheer_or_nudge_through_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1irvwt0/cheerbot_ollama_based_cheer_or_nudge_through_ocr/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T21:48:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1isegnq</id>
    <title>I have a Ryzen 5 CPU with Radeon Graphics and 3050 Laptop. Is it possible to make these GPUs (iGPU and dGPU) to work together in Ollama? Or any other alternatives?</title>
    <updated>2025-02-18T14:54:33+00:00</updated>
    <author>
      <name>/u/Ambitious_Fee_5818</name>
      <uri>https://old.reddit.com/user/Ambitious_Fee_5818</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious_Fee_5818"&gt; /u/Ambitious_Fee_5818 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isegnq/i_have_a_ryzen_5_cpu_with_radeon_graphics_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isegnq/i_have_a_ryzen_5_cpu_with_radeon_graphics_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isegnq/i_have_a_ryzen_5_cpu_with_radeon_graphics_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T14:54:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1isbug5</id>
    <title>Model will not load back into GPU</title>
    <updated>2025-02-18T12:44:55+00:00</updated>
    <author>
      <name>/u/Board-Upbeat</name>
      <uri>https://old.reddit.com/user/Board-Upbeat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m having an issue where any model will not load back into GPU. My setup is ollama and a Jupyter notebook running in docker containers within a Linux system. I load up a model and it initially works as intended using the GPU, but after 5 mins of inactivity it unloads from the GPU. When trying to load the model back I get errors trying to find a CUDA enabled device and proceeds to use CPU. I have tried keep_alive=-1 which works, unless you want to switch models and then I get a similar error. Any ideas of what I could try to get this working as intended? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Board-Upbeat"&gt; /u/Board-Upbeat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isbug5/model_will_not_load_back_into_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isbug5/model_will_not_load_back_into_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isbug5/model_will_not_load_back_into_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T12:44:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1is9thv</id>
    <title>Is it possible to feed a model with a saas application and train the model with its codebase?</title>
    <updated>2025-02-18T10:37:35+00:00</updated>
    <author>
      <name>/u/devfullstack98</name>
      <uri>https://old.reddit.com/user/devfullstack98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The PHP application has approximately 20 million lines of code.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devfullstack98"&gt; /u/devfullstack98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is9thv/is_it_possible_to_feed_a_model_with_a_saas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is9thv/is_it_possible_to_feed_a_model_with_a_saas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is9thv/is_it_possible_to_feed_a_model_with_a_saas/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T10:37:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ishfpk</id>
    <title>BrowserUse with Deepseek Ollama dead slow</title>
    <updated>2025-02-18T16:59:14+00:00</updated>
    <author>
      <name>/u/sammopus</name>
      <uri>https://old.reddit.com/user/sammopus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried using Deepseek R1-14B on M1 PRO 16GB, it worked fine with Ollama, decent tokens/s but when I used it with BrowserUse, it is dead slow! Impractical to use, did anyone get it working on similar spec?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sammopus"&gt; /u/sammopus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ishfpk/browseruse_with_deepseek_ollama_dead_slow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ishfpk/browseruse_with_deepseek_ollama_dead_slow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ishfpk/browseruse_with_deepseek_ollama_dead_slow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T16:59:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iryvnt</id>
    <title>Deepscaler km q4 is insanly fast abd small even on CPU only</title>
    <updated>2025-02-17T23:56:07+00:00</updated>
    <author>
      <name>/u/tshawkins</name>
      <uri>https://old.reddit.com/user/tshawkins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running on NUC11PRO 11th gen i7 with 32GB - 4200 RAM, no GPU, consumes a little over 1.1GB of RAM, Ideal for running in the background on a dev machine. &lt;/p&gt; &lt;p&gt;The Model Is a 1.5B parameter distilation of deepseek-r1. &lt;/p&gt; &lt;p&gt;Im going to be exploring its code generation abilities, to see if it is usefull as a enclosed code generator, ie no requests to the internet. &lt;/p&gt; &lt;p&gt;It would interesting to start a leader board of size vs capabilities. To understand which models are best for edge AI use. &lt;/p&gt; &lt;p&gt;thawkins@TimServFed01:~$ ollama run deepscaler:1.5b-preview-q4_K_M --verbose&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;tell me a story about 10 ants who save thier anthill from drowning during a flood, how they earned the respect of thier queen and all the other ants in the anthill. &amp;lt;think&amp;gt; Okay, so I need to write a story about 10 ants who saved their anthill from drowning during a flood, and explain how they got respect from their queen and everyone else. Hmm, let's break this down.&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;First, setting the scene: a big flood hits an ancient, powerful anthill. The ants are probably scared because of the high water level. They need to escape or find shelter quickly. But maybe they don't know much about each other or how to help.&lt;/p&gt; &lt;p&gt;They decide to go together. Maybe they leave their usual positions and start searching for spots where the water is less. That could save some lives. But why are there only 10 ants? Maybe because it's a small group, so they can work together.&lt;/p&gt; &lt;p&gt;When the flood comes in, the water level rises, and the ants find an area with low water. They go in, maybe even get drenched. But then someone notices them, like maybe a bird or a duck that spots their tiny collection. The birds might offer help by giving information or clues on where to go.&lt;/p&gt; &lt;p&gt;The queen is really important because she's known for being the leader and enforcing rules. Maybe she gets called out if they don't behave well, but when they manage to escape or protect the anthill, her respect grows. Perhaps she tells everyone about their bravery.&lt;/p&gt; &lt;p&gt;Other ants might get curious and offer help too. They could even start a little community effort, maybe building shelters or finding other ways to help. Their teamwork is key here.&lt;/p&gt; &lt;p&gt;In the end, the group realizes that working together was crucial. Maybe they find a way to ensure their safety despite the flood. They probably learn from this experience how important it is to be part of something bigger than themselves.&lt;/p&gt; &lt;p&gt;I should make sure the story shows their bravery, how they rely on each other, and the respect they receive from their queen and others. Maybe include some dialogue or actions that show their cooperation and determination. &amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Title: The Evacuation Sequence&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Once upon a time, in a lush, ancient anthill with its powerful guardian queen, there lived ten tiny ants who were deeply rooted in their tiny world but had always been curious about the world around them. These ants knew they had to act quickly because the flood was coming, and the water level was rising.&lt;/p&gt; &lt;p&gt;The ants realized that the most effective way to escape was by working together. They decided to go to a hidden spot where the water would be lower. However, their knowledge was limited, so they started searching in different areas. Despite their small size, their collective effort ensured they found an unobstructed path through the flood's waters.&lt;/p&gt; &lt;p&gt;As the flood approached, many ants were caught by some birds that spotted them. These birds offered information, guiding the ants on their escape route. The queen noticed the group's bravery and was immediately informed about their efforts.&lt;/p&gt; &lt;p&gt;The queen, ever the leader of her colony, recognized their bravery and began to notice any discrepancies in their behavior. She taught everyone about their role as part of a larger community. As the group worked together, they realized that their collective strength was essential for survival.&lt;/p&gt; &lt;p&gt;In the end, the ants successfully escaped the flood, protecting their anthill from drowning. They learned that working together could yield greater results than individual efforts. Their bravery earned them respect not only among their queen but also by becoming a model community effort.&lt;/p&gt; &lt;p&gt;Thus, their journey became a symbol of unity and resilience, teaching all who heard about it to work together for shared survival.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;total duration: 34.749197247s. load duration: 10.291253ms. prompt eval count: 44 token(s). prompt eval duration: 40ms. prompt eval rate: 1100.00 tokens/s. eval count: 708 token(s). eval duration: 34.697s. eval rate: 20.41 tokens/s. &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;Send a message (/? for help).&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tshawkins"&gt; /u/tshawkins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iryvnt/deepscaler_km_q4_is_insanly_fast_abd_small_even/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iryvnt/deepscaler_km_q4_is_insanly_fast_abd_small_even/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iryvnt/deepscaler_km_q4_is_insanly_fast_abd_small_even/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-17T23:56:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1islc2c</id>
    <title>Local Llama3.2:3b - No conversational memory?</title>
    <updated>2025-02-18T19:31:50+00:00</updated>
    <author>
      <name>/u/DelosBoard2052</name>
      <uri>https://old.reddit.com/user/DelosBoard2052</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I read somewhere that these local models should have conversational memory. I'm running the Llama3.2:3b model, but it seems to have ZERO memory of ANY previous utterance - it seems to view every input as a new conversation. Here's an example of an interaction after approximately a dozen interchanges within the same session:&lt;/p&gt; &lt;p&gt;&lt;code&gt;****&amp;gt; ^Can you summarize our conversation so far today?&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;We haven't had a conversation yet, this is the first message. What would you like to talk about? I can summarize our conversation at the end if you'd like.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;****&amp;gt; ^Do you remember what you just stated to me in your immediate past utterance?&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;This is the beginning of our conversation, and I haven't made any previous statements. I'm a large language model, I don't have personal memories or recall previous conversations. Each time you interact with me, it's a new conversation. What would you like to talk about?&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I'm wondering if there's something I'm not doing that I should have when I installed Ollama, or if the Llama3.2:3b model doesn't support conversational memory. If it's simply the latter, what models DO support conversational memory?&lt;/p&gt; &lt;p&gt;EDIT: I should add that I'm running Ollama inside of a python script, so I capture Ollama's output to send it to my TTS system. Does it work differently through the CLI by any chance?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DelosBoard2052"&gt; /u/DelosBoard2052 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1islc2c/local_llama323b_no_conversational_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1islc2c/local_llama323b_no_conversational_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1islc2c/local_llama323b_no_conversational_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T19:31:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ismy41</id>
    <title>Where did Ollamahub go?!</title>
    <updated>2025-02-18T20:36:22+00:00</updated>
    <author>
      <name>/u/PaulLee420</name>
      <uri>https://old.reddit.com/user/PaulLee420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I get a 404 error when I try to go to ollamahub.com - did something happen to it? Does it hate my VPN? &lt;/p&gt; &lt;p&gt;Trying to learn using Modelfiles and...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaulLee420"&gt; /u/PaulLee420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ismy41/where_did_ollamahub_go/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ismy41/where_did_ollamahub_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ismy41/where_did_ollamahub_go/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T20:36:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1isn6zh</id>
    <title>LLM evaluations using Ollama</title>
    <updated>2025-02-18T20:46:19+00:00</updated>
    <author>
      <name>/u/FlimsyProperty8544</name>
      <uri>https://old.reddit.com/user/FlimsyProperty8544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I run an open-source project &lt;a href="https://github.com/confident-ai/deepeval"&gt;DeepEval&lt;/a&gt;, which contains 20+ LLM-as-a-judge metrics to help evaluate LLM applications (i.e. RAG metrics, conversational metrics, red-teaming metrics). We integrated with Ollama, so you can now run evaluations for free using Ollama models locally on your device. Here’s how you can get started: &lt;a href="https://docs.confident-ai.com/docs/metrics-introduction#using-ollama"&gt;https://docs.confident-ai.com/docs/metrics-introduction#using-ollama&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FlimsyProperty8544"&gt; /u/FlimsyProperty8544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isn6zh/llm_evaluations_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isn6zh/llm_evaluations_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isn6zh/llm_evaluations_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T20:46:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1is774t</id>
    <title>Would my 4060 be enough for 14b deepseek or should I go down to 8b?</title>
    <updated>2025-02-18T07:26:27+00:00</updated>
    <author>
      <name>/u/Squidlers17</name>
      <uri>https://old.reddit.com/user/Squidlers17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title suggests, I am not sure on how many parameters should I use for my 4060 laptop I am also using Ryzen 9 7945HX&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Squidlers17"&gt; /u/Squidlers17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is774t/would_my_4060_be_enough_for_14b_deepseek_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is774t/would_my_4060_be_enough_for_14b_deepseek_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is774t/would_my_4060_be_enough_for_14b_deepseek_or/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T07:26:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1isoyob</id>
    <title>Ollama issues with new 7900xt</title>
    <updated>2025-02-18T22:25:46+00:00</updated>
    <author>
      <name>/u/flamingreaper1</name>
      <uri>https://old.reddit.com/user/flamingreaper1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all!&lt;/p&gt; &lt;p&gt;I recently got a new 7900xt to primary use for ollama and other gpu stuff. The issue I've been running into is that it'll run really well for a period of time then throw up this error - &lt;/p&gt; &lt;p&gt;SyntaxError: Unexpected token '&amp;lt;', &amp;quot;&amp;lt;!DOCTYPE &amp;quot;... is not valid JSON&lt;/p&gt; &lt;p&gt;And it won't work again without a server reboot or running &amp;quot;modprobe -r amdgpu&amp;quot; followed by &amp;quot;modprobe amdgpu&amp;quot;. Which then it'll run again for a bit before getting the error again. As well with web search I get maybe 3 back and forth before getting the error.&lt;/p&gt; &lt;p&gt;Any ideas? &lt;/p&gt; &lt;p&gt;Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/flamingreaper1"&gt; /u/flamingreaper1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isoyob/ollama_issues_with_new_7900xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isoyob/ollama_issues_with_new_7900xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isoyob/ollama_issues_with_new_7900xt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T22:25:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1isp73j</id>
    <title>multiple models</title>
    <updated>2025-02-18T22:32:33+00:00</updated>
    <author>
      <name>/u/w38122077</name>
      <uri>https://old.reddit.com/user/w38122077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it possible with ollama to have two models running and each be available on a different port? I can run two and interact with them via the command line, but I can't seem to figure out how to have them available concurrently to Visual Code for use with chat and tab autocomplete &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w38122077"&gt; /u/w38122077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isp73j/multiple_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isp73j/multiple_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isp73j/multiple_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T22:32:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1is8g74</id>
    <title>DeepSeek or MS Bing AI?</title>
    <updated>2025-02-18T08:57:50+00:00</updated>
    <author>
      <name>/u/Wizard_of_Awes</name>
      <uri>https://old.reddit.com/user/Wizard_of_Awes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1is8g74/deepseek_or_ms_bing_ai/"&gt; &lt;img alt="DeepSeek or MS Bing AI?" src="https://a.thumbs.redditmedia.com/XX5Qjey4O_bAKYgdR-2EUjRrM2XntcwNHsjW11xn4V8.jpg" title="DeepSeek or MS Bing AI?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wizard_of_Awes"&gt; /u/Wizard_of_Awes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1is8g74"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is8g74/deepseek_or_ms_bing_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is8g74/deepseek_or_ms_bing_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T08:57:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1isispb</id>
    <title>Can I create an LLM model that will behave strictly the way I want?</title>
    <updated>2025-02-18T17:52:46+00:00</updated>
    <author>
      <name>/u/Nuvola_Rossa</name>
      <uri>https://old.reddit.com/user/Nuvola_Rossa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to create LLM models that can be uploaded locally. Those models should speak in a certain way or should only consider certain knowledge. For example, I want to create a model that would answer like a medieval man.&lt;/p&gt; &lt;p&gt;The idea is to have a chatbot for which the user shouldn't have to prompt anything specific for it to behave that way (for example if the user asks &amp;quot;what happen during world war II&amp;quot;, the model should answer &amp;quot;I have no idea&amp;quot; or something like that). &lt;/p&gt; &lt;p&gt;I would like to have several AI model that could be loaded in order to compare the answers. And i will need a GGUF (this is not optional) of each model.&lt;/p&gt; &lt;p&gt;I've been looking around for a way to do this but I can't find any way out of it. Any ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nuvola_Rossa"&gt; /u/Nuvola_Rossa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isispb/can_i_create_an_llm_model_that_will_behave/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isispb/can_i_create_an_llm_model_that_will_behave/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isispb/can_i_create_an_llm_model_that_will_behave/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T17:52:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1is22ug</id>
    <title>Most cost effective way of hosting 70B/32B param model</title>
    <updated>2025-02-18T02:29:31+00:00</updated>
    <author>
      <name>/u/topsy_here</name>
      <uri>https://old.reddit.com/user/topsy_here</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious &lt;/p&gt; &lt;p&gt;It comes down to efficiency. I see it like crypto mining. It’s about getting the best token count for least cost.&lt;/p&gt; &lt;p&gt;There’s Mac minis I’ve seen hosting the 72B param one. You gonna need about 8x of them which is about 3.5K usd each? &lt;/p&gt; &lt;p&gt;What about hosting on a VPS in Linus? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/topsy_here"&gt; /u/topsy_here &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is22ug/most_cost_effective_way_of_hosting_70b32b_param/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1is22ug/most_cost_effective_way_of_hosting_70b32b_param/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1is22ug/most_cost_effective_way_of_hosting_70b32b_param/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T02:29:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1isr8zl</id>
    <title>After great pains (learning curve), got llama.cpp running on my older AMD GPU (since Ollama isn’t compatible)…but the two things I want to use Ollama with don’t “communicate” with it in the way they do Ollama. HomeAssistant and Frigate use Ollama at port 11434, llama.cpp doesn’t have that…help?</title>
    <updated>2025-02-18T23:55:58+00:00</updated>
    <author>
      <name>/u/FantasyMaster85</name>
      <uri>https://old.reddit.com/user/FantasyMaster85</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've got an older AMD GPU that is running llama.cpp (built with Vulcan and fully utilizing my GPU...an RX 570) along with the given sub 4gb models at a perfectly acceptable TPS for my two use cases (HomeAssistant and Frigate), as tested manually running llama-server and passing queries to it manually. &lt;/p&gt; &lt;p&gt;The issue is that while both HomeAssistant and Frigate have a means to work with Ollama at port 11434, I can't for the life of me figure out how to expose the same functionality using llama.cpp...is it even possible?&lt;/p&gt; &lt;p&gt;I've tried llama-server using llama.cpp and it doesn't work with HomeAssistant or Frigate, despite the web UI created by it working fine (seems that's an &amp;quot;openAI&amp;quot; API versus the &amp;quot;Ollama&amp;quot; style API exposed by Ollama. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FantasyMaster85"&gt; /u/FantasyMaster85 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isr8zl/after_great_pains_learning_curve_got_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isr8zl/after_great_pains_learning_curve_got_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isr8zl/after_great_pains_learning_curve_got_llamacpp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T23:55:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1isqhm1</id>
    <title>Ollama Shell -- improved Terminal app for using local models</title>
    <updated>2025-02-18T23:22:47+00:00</updated>
    <author>
      <name>/u/sunkencity999</name>
      <uri>https://old.reddit.com/user/sunkencity999</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1isqhm1/ollama_shell_improved_terminal_app_for_using/"&gt; &lt;img alt="Ollama Shell -- improved Terminal app for using local models" src="https://b.thumbs.redditmedia.com/i1OWSPjS_iJXLlQo0MDCNlUzL5p_Rm9K-O4rQsmb8NM.jpg" title="Ollama Shell -- improved Terminal app for using local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y'all,&lt;/p&gt; &lt;p&gt;I am personally a huge fan of working directly in the terminal; the existing terminal shell for Ollama, in my opinion, leaves much to be desired, functionality and aesthetics-wise. SO, I figured I would create a Shell application that allows you to work with Ollama and models in the terminal in a way that is practical and reasonably efficient. You can analyze documents by dragging-and-dropping them in the chat, manage models (pull and delete), have continuous chat history and save system prompts for use as necessary. If working in the terminal / shell is something you enjoy as well, please give it a shot. Free, and of course I welcome contributors.&lt;br /&gt; &lt;a href="https://github.com/sunkencity999/ollama_shell"&gt;Ollama Shell on Github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ym0w7cmtizje1.png?width=1071&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=427fbffce0d8ba45e5906e3342f5aa1028bcd959"&gt;Main Interface&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tpfh7k4vizje1.png?width=1059&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0fa8276a0e793475538033c655452323b68354bc"&gt;Prompt selection after Model selection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n2mhbjjxizje1.png?width=920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d99ceda3ceb10582f95be390b8bf8b12a3fc641e"&gt;Query answered by LLM and provided (deepseek-r1:14b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunkencity999"&gt; /u/sunkencity999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isqhm1/ollama_shell_improved_terminal_app_for_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isqhm1/ollama_shell_improved_terminal_app_for_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isqhm1/ollama_shell_improved_terminal_app_for_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T23:22:47+00:00</published>
  </entry>
</feed>
