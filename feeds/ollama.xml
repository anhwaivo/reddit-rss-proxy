<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-12T17:05:49+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1j87nei</id>
    <title>Basic LLM performance testing of A100, RTX A6000, H100, H200 Spot GPU instances from DataCrunch</title>
    <updated>2025-03-10T19:52:13+00:00</updated>
    <author>
      <name>/u/olegsmith7</name>
      <uri>https://old.reddit.com/user/olegsmith7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j87nei/basic_llm_performance_testing_of_a100_rtx_a6000/"&gt; &lt;img alt="Basic LLM performance testing of A100, RTX A6000, H100, H200 Spot GPU instances from DataCrunch" src="https://b.thumbs.redditmedia.com/U6Cl4e55WuO1awTEvPao4sFK8IYedHOPJq7lu7eU8gA.jpg" title="Basic LLM performance testing of A100, RTX A6000, H100, H200 Spot GPU instances from DataCrunch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I benchmarked Rackspace Spot Kubernetes nodes with A30 and H100 GPUs for self-hosting LLMs last month. Yesterday, I conducted a similar assessment of A100, RTX A6000, H100, and H200 GPU-powered VMs from DataCrunch. Performance test results indicate the following findings:&lt;/p&gt; &lt;p&gt;- Based on cost per token per second (tps) per hour, the most cost-effective options are: Nvidia A100 40GB VRAM for 32b models (€0.1745/hour) and Nvidia H100 80GB VRAM for 70b models (€0.5180/hour)&lt;/p&gt; &lt;p&gt;- Token throughput (tokens per second) scales almost proportionally with model size: a 32b model (20GB size) yields twice the number of tokens per second compared to a 70b model (43GB size).&lt;/p&gt; &lt;p&gt;- H200 doesn't provide better single-conversation performance than H100, but it should show better overall throughput performance for multi-conversation load across multiple NVLinked H200 (e.g. 4x 8H200).&lt;/p&gt; &lt;p&gt;- New qwq:32b model a bit slower than qwen2.5-coder:32b in terms of token throughput.&lt;/p&gt; &lt;p&gt;- DataCrunch offers better prices than Rackspace Spot&lt;/p&gt; &lt;p&gt;read more &lt;a href="https://oleg.smetan.in/posts/2025-03-09-datacrunch-spot-llm-performance-test"&gt;https://oleg.smetan.in/posts/2025-03-09-datacrunch-spot-llm-performance-test&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ps8n078z2xne1.png?width=3644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=110130fb3c9d7706472c9d322337208279663e5a"&gt;https://preview.redd.it/ps8n078z2xne1.png?width=3644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=110130fb3c9d7706472c9d322337208279663e5a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/olegsmith7"&gt; /u/olegsmith7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j87nei/basic_llm_performance_testing_of_a100_rtx_a6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j87nei/basic_llm_performance_testing_of_a100_rtx_a6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j87nei/basic_llm_performance_testing_of_a100_rtx_a6000/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T19:52:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7ntrn</id>
    <title>I Fine-Tuned a Tiny LLM to Write Git Commits Offline—Check It Out!</title>
    <updated>2025-03-10T01:56:46+00:00</updated>
    <author>
      <name>/u/VictorCTavernari</name>
      <uri>https://old.reddit.com/user/VictorCTavernari</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good evening, Ollama community!&lt;/p&gt; &lt;p&gt;I've been an enthusiast of local open-source LLMs for about a year now. Typically, I prefer keeping my git commits small with clear, meaningful messages, especially when working with others. When ChatGPT launched GPTs, I created a dedicated model for writing commit messages: &lt;a href="https://chatgpt.com/g/g-1RdmhTAHg-git-commit-message-pro"&gt;Git Commit Message Pro&lt;/a&gt;. However, I encountered some privacy limitations, which led me to explore fine-tuning my own local LLM that could produce an initial draft requiring minimal edits. Using Ollama, I built &lt;a href="https://ollama.com/tavernari/git-commit-message"&gt;tavernari/git-commit-message&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;tavernari/git-commit-message&lt;/h1&gt; &lt;p&gt;In my first version, I used the 7B Mistral model, which occupies about 4.4 GB. While functional, it was resource-intensive and often produced slow and unsatisfactory responses.&lt;/p&gt; &lt;p&gt;Recently, there has been considerable hype around DeepSeekR1, a smaller model trained to &amp;quot;think&amp;quot; more effectively. Inspired by this, I created a smaller, reasoning-focused version dedicated specifically to writing commit messages.&lt;/p&gt; &lt;p&gt;This was my first attempt at fine-tuning. Although the results aren't perfect yet, I believe that with further training and refinement, I can achieve better outcomes.&lt;/p&gt; &lt;p&gt;Hence, I introduced the &amp;quot;reasoning&amp;quot; version: &lt;a href="https://ollama.com/tavernari/git-commit-message:reasoning"&gt;tavernari/git-commit-message:reasoning&lt;/a&gt;. This version uses a small 3B model (1.9 GB) optimized for enhanced reasoning capabilities. Additionally, I developed another version leveraging Chain of Thought (&lt;a href="https://arxiv.org/pdf/2502.18600"&gt;Chain of Thought&lt;/a&gt;), which also showed promising results, though it hasn't been deeply explored yet.&lt;/p&gt; &lt;h1&gt;Agentic Git Commit Message&lt;/h1&gt; &lt;p&gt;Despite its decent performance, the model struggled with larger contexts. To address this, I created an agentic bash script that incrementally evaluates git diffs, helping the LLM generate commits without losing context.&lt;/p&gt; &lt;p&gt;Script functionalities include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Adding context to improve commit message quality.&lt;/li&gt; &lt;li&gt;Editing the generated message before committing.&lt;/li&gt; &lt;li&gt;Generating only the message with the --only-message option.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Installation is straightforward and explained on the model’s profile page: &lt;a href="https://ollama.com/tavernari/git-commit-message:reasoning"&gt;tavernari/git-commit-message:reasoning&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Project Goal&lt;/h1&gt; &lt;p&gt;My goal is to provide commit messages that are sufficiently good, needing only minor manual adjustments, and most importantly, functioning completely offline to ensure your intellectual work remains secure and private.&lt;/p&gt; &lt;p&gt;I've invested some financial resources into the fine-tuning process, aiming ultimately to create something beneficial for the community. In the future, I'll continue dedicating time to training and refining the model to enhance its quality.&lt;/p&gt; &lt;p&gt;The idea is to offer a practical, efficient tool that prioritizes the security and privacy of your work.&lt;/p&gt; &lt;p&gt;Feel free to use, suggest improvements, and collaborate!&lt;/p&gt; &lt;p&gt;My HuggingFace: &lt;a href="https://huggingface.co/Tavernari/git-commit-message"&gt;https://huggingface.co/Tavernari/git-commit-message&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VictorCTavernari"&gt; /u/VictorCTavernari &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7ntrn/i_finetuned_a_tiny_llm_to_write_git_commits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j7ntrn/i_finetuned_a_tiny_llm_to_write_git_commits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j7ntrn/i_finetuned_a_tiny_llm_to_write_git_commits/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T01:56:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8pc0q</id>
    <title>Best LLMs with 8 GB RAM, 2.10 GHz for coding, content generation, chat?</title>
    <updated>2025-03-11T12:15:51+00:00</updated>
    <author>
      <name>/u/tsfongapucchiacc</name>
      <uri>https://old.reddit.com/user/tsfongapucchiacc</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tsfongapucchiacc"&gt; /u/tsfongapucchiacc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8pc0q/best_llms_with_8_gb_ram_210_ghz_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8pc0q/best_llms_with_8_gb_ram_210_ghz_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j8pc0q/best_llms_with_8_gb_ram_210_ghz_for_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-11T12:15:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j883k5</id>
    <title>Fine tuning ollama model</title>
    <updated>2025-03-10T20:11:16+00:00</updated>
    <author>
      <name>/u/Snoo_44191</name>
      <uri>https://old.reddit.com/user/Snoo_44191</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys I am using QWQ 32B with crew ai locally on my RTX A6000 48GB Vram GPU. The crew hallucinates a lot at most of the times , mainly while tool calling and also sometimes in normal tasks . I have edited the model file and set num ctx to 16000 , still i dont get a stable streamlined output , it changes after each iteration ! (My prompts are perfect as they work awesome with open ai or Gemini api&amp;quot;s) I was suggested by one redditor to fine tune the model for crew ai , but i am not able to understand how to craft the dataset , what should it exactly be ? So that the model learns to call tools better and interact with crewai better ? &lt;/p&gt; &lt;p&gt;Any help on this would be extremely relieving!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snoo_44191"&gt; /u/Snoo_44191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j883k5/fine_tuning_ollama_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j883k5/fine_tuning_ollama_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j883k5/fine_tuning_ollama_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T20:11:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8a6z5</id>
    <title>How to test an AMD Instinct Mi50/Mi60 GPU</title>
    <updated>2025-03-10T21:37:39+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x91gel5sfxne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8a6z5/how_to_test_an_amd_instinct_mi50mi60_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j8a6z5/how_to_test_an_amd_instinct_mi50mi60_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T21:37:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j82dfc</id>
    <title>Ollama + Apple Notes - I built ChatGPT for Apple Notes</title>
    <updated>2025-03-10T16:14:58+00:00</updated>
    <author>
      <name>/u/arne226</name>
      <uri>https://old.reddit.com/user/arne226</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j82dfc/ollama_apple_notes_i_built_chatgpt_for_apple_notes/"&gt; &lt;img alt="Ollama + Apple Notes - I built ChatGPT for Apple Notes" src="https://external-preview.redd.it/MnNtYmp0NDUwd25lMR9ljqoPJ2qXFOb1yIG9LCnvszZJql-YtrXAxdd8XVVd.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7be12c0a6c664fe32deb128ec7777f0e93665081" title="Ollama + Apple Notes - I built ChatGPT for Apple Notes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arne226"&gt; /u/arne226 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9yo1vx450wne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j82dfc/ollama_apple_notes_i_built_chatgpt_for_apple_notes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j82dfc/ollama_apple_notes_i_built_chatgpt_for_apple_notes/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-10T16:14:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8ef9z</id>
    <title>How do I train an untrained AI?</title>
    <updated>2025-03-11T00:45:30+00:00</updated>
    <author>
      <name>/u/AnaverageuserX</name>
      <uri>https://old.reddit.com/user/AnaverageuserX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With untrained AIs do I just feed them random Text-Based datasets with the desired language/intel I want? Or do I feed them other stuff like random numbers? I'm using the Msty App with the Model &amp;quot;untrained-suave-789.IQ3_S-1741651430874:latest&amp;quot; and am curious on how to train it to well.. Not speak gibberish.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnaverageuserX"&gt; /u/AnaverageuserX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8ef9z/how_do_i_train_an_untrained_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8ef9z/how_do_i_train_an_untrained_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j8ef9z/how_do_i_train_an_untrained_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-11T00:45:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8tmpu</id>
    <title>I've created a ollama clone with syntax highlighting and cloud support</title>
    <updated>2025-03-11T15:37:29+00:00</updated>
    <author>
      <name>/u/Suspicious_Raise_589</name>
      <uri>https://old.reddit.com/user/Suspicious_Raise_589</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That's it.&lt;/p&gt; &lt;p&gt;I've written an AI-Chat client in C# which supports multiple agents (models with custom system prompts) and does supports &lt;strong&gt;syntax highlighting&lt;/strong&gt; for both markdown responses and code blocks. And the better: everything runs in your terminal. No Electron crap for your computer.&lt;/p&gt; &lt;p&gt;Also, it also supports cloud-based AI agents, such as OpenAI, Groq, Google...&lt;/p&gt; &lt;p&gt;That's an example of it:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/7a990586-36a9-4f4c-9636-77b9e6036cf7"&gt;https://github.com/user-attachments/assets/7a990586-36a9-4f4c-9636-77b9e6036cf7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's fully customizeable, open-source and &lt;strong&gt;free&lt;/strong&gt;. Fork or download it &lt;a href="https://github.com/CypherPotato/copilot4"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suspicious_Raise_589"&gt; /u/Suspicious_Raise_589 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8tmpu/ive_created_a_ollama_clone_with_syntax/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8tmpu/ive_created_a_ollama_clone_with_syntax/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j8tmpu/ive_created_a_ollama_clone_with_syntax/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-11T15:37:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8w3hm</id>
    <title>New to using the chat function, what’s a good way to extract the answer?</title>
    <updated>2025-03-11T17:19:09+00:00</updated>
    <author>
      <name>/u/lumpboysupreme</name>
      <uri>https://old.reddit.com/user/lumpboysupreme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I call api/chat I get a stream of lines classed as bytes objects, with the response being split into individual words under ‘content’. I cannot jsonify or subset this object, and while I could add an elaborate text splitting operation to extract the needed values, that seems highly inefficient. Is there a better way of doing this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lumpboysupreme"&gt; /u/lumpboysupreme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8w3hm/new_to_using_the_chat_function_whats_a_good_way/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8w3hm/new_to_using_the_chat_function_whats_a_good_way/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j8w3hm/new_to_using_the_chat_function_whats_a_good_way/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-11T17:19:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8egbd</id>
    <title>Openmanus+ollama</title>
    <updated>2025-03-11T00:46:54+00:00</updated>
    <author>
      <name>/u/Choice_Complaint9171</name>
      <uri>https://old.reddit.com/user/Choice_Complaint9171</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone accomplished openmanus ollama and webui on windows &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Choice_Complaint9171"&gt; /u/Choice_Complaint9171 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8egbd/openmanusollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8egbd/openmanusollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j8egbd/openmanusollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-11T00:46:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1j96ky3</id>
    <title>Best LLM for local code generation? Rx 7800 xt 16GB VRAM ~15GB usable VRAM.</title>
    <updated>2025-03-12T00:42:48+00:00</updated>
    <author>
      <name>/u/SecretAd2701</name>
      <uri>https://old.reddit.com/user/SecretAd2701</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SecretAd2701"&gt; /u/SecretAd2701 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j96ky3/best_llm_for_local_code_generation_rx_7800_xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j96ky3/best_llm_for_local_code_generation_rx_7800_xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j96ky3/best_llm_for_local_code_generation_rx_7800_xt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-12T00:42:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8u5ck</id>
    <title>ScribePal v1.2.0 Released!</title>
    <updated>2025-03-11T15:59:34+00:00</updated>
    <author>
      <name>/u/Code-Forge-Temple</name>
      <uri>https://old.reddit.com/user/Code-Forge-Temple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm excited to announce the release of &lt;strong&gt;ScribePal v1.2.0&lt;/strong&gt;! This minor update brings several new enhancements and improvements designed to elevate your private AI-assisted browsing experience.&lt;/p&gt; &lt;h2&gt;What's New&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Show Chat Keyboard Shortcut:&lt;/strong&gt;&lt;br /&gt; Quickly open the chat interface using a convenient keyboard shortcut.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Image Capture and Interpretation:&lt;/strong&gt;&lt;br /&gt; Capture an image directly from the webpage and have it interpreted by vision LLMs. Use the &lt;code&gt;@captured-image&lt;/code&gt; tag to reference the captured image in your chat.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Suggestions Menu for Tag References:&lt;/strong&gt;&lt;br /&gt; A new suggestions menu assists with tag references during conversations, making it easier to insert &lt;code&gt;@captured-text&lt;/code&gt; or &lt;code&gt;@captured-image&lt;/code&gt; tags.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Scroll Chat During Prompt Update:&lt;/strong&gt;&lt;br /&gt; Scroll up and down the conversation even as the LLM prompt continues to update.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Copy Message Option:&lt;/strong&gt;&lt;br /&gt; Easily copy any message from your conversation with a single click.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;How to Upgrade&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Visit the &lt;a href="https://github.com/code-forge-temple/scribe-pal/releases"&gt;Releases&lt;/a&gt; page.&lt;/li&gt; &lt;li&gt;Download the updated package for your browser (Chromium-based or Gecko-based).&lt;/li&gt; &lt;li&gt;Follow the installation instructions provided in the README.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Demo &amp;amp; Feedback&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Tutorial Video:&lt;/strong&gt;&lt;br /&gt; Watch this &lt;a href="https://www.youtube.com/watch?v=m7pw6q5qgY0"&gt;short video tutorial&lt;/a&gt; to see the new features in action.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Share Your Thoughts:&lt;/strong&gt;&lt;br /&gt; Your feedback is valuable! Let me know what you think and suggest further improvements on the forum.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Repository &lt;a href="https://github.com/code-forge-temple/scribe-pal"&gt;GitHub&lt;/a&gt;&lt;/h2&gt; &lt;h2&gt;License&lt;/h2&gt; &lt;p&gt;ScribePal is licensed under the GNU General Public License v3.0. For details, see the &lt;a href="https://github.com/code-forge-temple/scribe-pal/blob/main/LICENSE"&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt; &lt;p&gt;Enjoy the new features of ScribePal v1.2.0 and happy browsing!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Code-Forge-Temple"&gt; /u/Code-Forge-Temple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8u5ck/scribepal_v120_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8u5ck/scribepal_v120_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j8u5ck/scribepal_v120_released/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-11T15:59:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8ouip</id>
    <title>Completely new to this - would love orientation</title>
    <updated>2025-03-11T11:47:44+00:00</updated>
    <author>
      <name>/u/twack3r</name>
      <uri>https://old.reddit.com/user/twack3r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;DeepseekR1 is what finally jostled me into looking into the viability of running a local LLM and I am fascinated to eg develop a chatbot that can access a large volume of private data and have a chat with it about it.&lt;/p&gt; &lt;p&gt;I am quite tech savvy hw wise, very much into highend VR (Varjo XR4, AVP) and a system builder and overclocker as my hobby for around 30 years by now.&lt;/p&gt; &lt;p&gt;I am looking for a vector to get started on this but I’m afraid I lack the basics:&lt;/p&gt; &lt;p&gt;I understand that ollama is a GUI that enables easy rollout of local LLMs.&lt;/p&gt; &lt;p&gt;On the other hand I am aware that Linux is the preferred OS for running AI tasks? Would it be correct to assume that ollama sacrifices ease of access for finetuning and performance?&lt;/p&gt; &lt;p&gt;I am absolutely prepared to learn Linux basics and set up an OS, so that is not the issue. I’m just trying to get to grips with what OS and software suites are the ideal entry point into the ecosystem.&lt;/p&gt; &lt;p&gt;Hardware wise I‘m definitely not set up ideally but it‘s a powerful gaming PC which I would hope to at least do first steps on. It‘s a 12900k at 5.0 GHz all core, 32 GiB DDR4 6400, 8 TiB m2 SSDs, a 5090 and a 3090, on an ASUS Z690 extreme glacial.&lt;/p&gt; &lt;p&gt;I’m thankful for any pointers, advice, links to beginner level tutorials etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/twack3r"&gt; /u/twack3r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8ouip/completely_new_to_this_would_love_orientation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8ouip/completely_new_to_this_would_love_orientation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j8ouip/completely_new_to_this_would_love_orientation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-11T11:47:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8qjhe</id>
    <title>How to know if the model is using NPU and GPU during runtime. What is the size of the occupation?</title>
    <updated>2025-03-11T13:19:00+00:00</updated>
    <author>
      <name>/u/No_Investment_946</name>
      <uri>https://old.reddit.com/user/No_Investment_946</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Investment_946"&gt; /u/No_Investment_946 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8qjhe/how_to_know_if_the_model_is_using_npu_and_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8qjhe/how_to_know_if_the_model_is_using_npu_and_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j8qjhe/how_to_know_if_the_model_is_using_npu_and_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-11T13:19:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j97t2l</id>
    <title>Can we combine function ?</title>
    <updated>2025-03-12T01:38:51+00:00</updated>
    <author>
      <name>/u/OldNefariousness1590</name>
      <uri>https://old.reddit.com/user/OldNefariousness1590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you think we can combine RAG and OCR into one by using Ollama, OCR, Vision, and DeepSeek in the same time?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OldNefariousness1590"&gt; /u/OldNefariousness1590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j97t2l/can_we_combine_function/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j97t2l/can_we_combine_function/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j97t2l/can_we_combine_function/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-12T01:38:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j917d6</id>
    <title>Mini M4 RAG iOS Swift coding</title>
    <updated>2025-03-11T20:47:55+00:00</updated>
    <author>
      <name>/u/utilitycoder</name>
      <uri>https://old.reddit.com/user/utilitycoder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone using RAG with Ollama on a high power Mac to run Ollama for Xcode iOS development? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/utilitycoder"&gt; /u/utilitycoder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j917d6/mini_m4_rag_ios_swift_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j917d6/mini_m4_rag_ios_swift_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j917d6/mini_m4_rag_ios_swift_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-11T20:47:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8slxj</id>
    <title>Build a RAG based on structured data.</title>
    <updated>2025-03-11T14:53:56+00:00</updated>
    <author>
      <name>/u/sportoholic</name>
      <uri>https://old.reddit.com/user/sportoholic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to build a system which can help me get answers or understand the data. The actual data is all just numbers, no text. &lt;/p&gt; &lt;p&gt;For example: I want to know which users deposited most amount of money in the last month or what is the probability of a user getting churned. &lt;/p&gt; &lt;p&gt;How to approach this scenario? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sportoholic"&gt; /u/sportoholic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8slxj/build_a_rag_based_on_structured_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8slxj/build_a_rag_based_on_structured_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j8slxj/build_a_rag_based_on_structured_data/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-11T14:53:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1j96mlv</id>
    <title>Running Ollama on my laptop with shared memory?</title>
    <updated>2025-03-12T00:44:59+00:00</updated>
    <author>
      <name>/u/ShreddinPB</name>
      <uri>https://old.reddit.com/user/ShreddinPB</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, so im pretty new to this and have been reading! I have an Eluktronics Mech-15 G3 laptop with a AMD Ryzen 5900HX with integrated graphics and a 3070. I went thru all the different control panels (Eluktronics, AMD Adrenalin, NVidia CP) and in the NVidia one I see this.&lt;br /&gt; Dedicated Video Memory: 8192 MB GDDR6&lt;br /&gt; System video memory: 0 MB&lt;br /&gt; Shared system Memory: 16079 MB&lt;br /&gt; Total available graphics memory: 24271 MB &lt;/p&gt; &lt;p&gt;Does this mean my system is sharing its memory with the NVidia card? I thought it would only share it with the integrated card.&lt;br /&gt; The system has 32GB DDR4 3200, I couldnt find a way to adjust how much memory is shared in any of those control panels, or in the BIOS. The BIOS was VERY sparse on any setting to adjust anything hardware based, no memory timings/voltages, anything.&lt;br /&gt; I found some RAM on Amazon that would take the laptop to 64gb, I should be able to share more then and run larger models?&lt;br /&gt; I do understand using shared memory will make it slow, but as im just getting started im not really worried about it being slow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShreddinPB"&gt; /u/ShreddinPB &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j96mlv/running_ollama_on_my_laptop_with_shared_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j96mlv/running_ollama_on_my_laptop_with_shared_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j96mlv/running_ollama_on_my_laptop_with_shared_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-12T00:44:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8qp3g</id>
    <title>STOP asking for "the best model for my pc"</title>
    <updated>2025-03-11T13:26:33+00:00</updated>
    <author>
      <name>/u/valdecircarvalho</name>
      <uri>https://old.reddit.com/user/valdecircarvalho</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Really! Don´t be lazy.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/ollama/search/?q=best"&gt;https://www.reddit.com/r/ollama/search/?q=best&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Dozens and dozens of posts asking for &amp;quot;the best model for my pc&amp;quot; that are totally useless. &lt;/p&gt; &lt;p&gt;It´s your PC, it´s your configuration, it´s your needs.&lt;/p&gt; &lt;p&gt;Do your home work and at least TRY by yourself. It will cost you nothing. Only a couple of minutes and you will get way better results. &lt;/p&gt; &lt;p&gt;Also you can check your GPU against some models using a GPU compatibility calculator like this one: &lt;a href="https://aleibovici.github.io/ollama-gpu-calculator/"&gt;React App&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thank you and enjoy the ride!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valdecircarvalho"&gt; /u/valdecircarvalho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8qp3g/stop_asking_for_the_best_model_for_my_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j8qp3g/stop_asking_for_the_best_model_for_my_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j8qp3g/stop_asking_for_the_best_model_for_my_pc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-11T13:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9b24o</id>
    <title>ParLlama v0.3.21 released. Now with better support for thinking models.</title>
    <updated>2025-03-12T04:31:58+00:00</updated>
    <author>
      <name>/u/probello</name>
      <uri>https://old.reddit.com/user/probello</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j9b24o/parllama_v0321_released_now_with_better_support/"&gt; &lt;img alt="ParLlama v0.3.21 released. Now with better support for thinking models." src="https://external-preview.redd.it/OT_MknFDG5u93mwSuu-sa0VXbm_9rh9AgkvflNF7_lU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0066b7b6c1b8042a9c5b33d1314fda1bd8aba70d" title="ParLlama v0.3.21 released. Now with better support for thinking models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d4naokems6oe1.png?width=1265&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55f833a6c81e91dc93e70f3edbeedd8805c26d22"&gt;https://preview.redd.it/d4naokems6oe1.png?width=1265&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55f833a6c81e91dc93e70f3edbeedd8805c26d22&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What My project Does:&lt;/h1&gt; &lt;p&gt;PAR LLAMA is a powerful TUI (Text User Interface) written in Python and designed for easy management and use of Ollama and Large Language Models as well as interfacing with online Providers such as Ollama, OpenAI, GoogleAI, Anthropic, Bedrock, Groq, xAI, OpenRouter&lt;/p&gt; &lt;h1&gt;Whats New:&lt;/h1&gt; &lt;h1&gt;v0.3.21&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Fix error caused by LLM response containing certain markup&lt;/li&gt; &lt;li&gt;Added llm config options for OpenAI Reasoning Effort, and Anthropic's Reasoning Token Budget&lt;/li&gt; &lt;li&gt;Better display in chat area for &amp;quot;thinking&amp;quot; portions of a LLM response&lt;/li&gt; &lt;li&gt;Fixed issues caused by deleting a message from chat while its still being generated by the LLM&lt;/li&gt; &lt;li&gt;Data and cache locations now use proper XDG locations&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;v0.3.20&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Fix unsupported format string error caused by missing temperature setting&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;v0.3.19&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Fix missing package error caused by previous update&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;v0.3.18&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Updated dependencies for some major performance improvements&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;v0.3.17&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Fixed crash on startup if Ollama is not available&lt;/li&gt; &lt;li&gt;Fixed markdown display issues around fences&lt;/li&gt; &lt;li&gt;Added &amp;quot;thinking&amp;quot; fence for deepseek thought output&lt;/li&gt; &lt;li&gt;Much better support for displaying max input context size&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;v0.3.16&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Added providers xAI, OpenRouter, Deepseek and LiteLLM&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Easy-to-use interface for interacting with Ollama and cloud hosted LLMs&lt;/li&gt; &lt;li&gt;Dark and Light mode support, plus custom themes&lt;/li&gt; &lt;li&gt;Flexible installation options (uv, pipx, pip or dev mode)&lt;/li&gt; &lt;li&gt;Chat session management&lt;/li&gt; &lt;li&gt;Custom prompt library support&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;GitHub and PyPI&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;PAR LLAMA is under active development and getting new features all the time.&lt;/li&gt; &lt;li&gt;Check out the project on GitHub or for full documentation, installation instructions, and to contribute: &lt;a href="https://github.com/paulrobello/parllama"&gt;https://github.com/paulrobello/parllama&lt;/a&gt;&lt;/li&gt; &lt;li&gt;PyPI &lt;a href="https://pypi.org/project/parllama/"&gt;https://pypi.org/project/parllama/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Comparison:&lt;/h1&gt; &lt;p&gt;I have seen many command line and web applications for interacting with LLM's but have not found any TUI related applications as feature reach as PAR LLAMA&lt;/p&gt; &lt;h1&gt;Target Audience&lt;/h1&gt; &lt;p&gt;Anybody that loves or wants to love terminal interactions and LLM's&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/probello"&gt; /u/probello &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9b24o/parllama_v0321_released_now_with_better_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9b24o/parllama_v0321_released_now_with_better_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j9b24o/parllama_v0321_released_now_with_better_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-12T04:31:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9knwg</id>
    <title>OlLama with an model not want work in serve try in terminal and with chesire... (10 hours of attempt)</title>
    <updated>2025-03-12T13:56:58+00:00</updated>
    <author>
      <name>/u/Taro_Happy</name>
      <uri>https://old.reddit.com/user/Taro_Happy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nothing, I’ve tried in 40 different ways, spending 10 hours to make it work. I followed every guide step by step.&lt;br /&gt; But nothing, it just won’t run. I have Windows, I even tried running it on Docker, but it doesn’t work (not to mention that it annoys me that it uses my local D drive).&lt;br /&gt; &lt;code&gt;ollama run deepseek-r1:1.5b&lt;/code&gt;&lt;br /&gt; &lt;code&gt;ollama serve&lt;/code&gt; not run so close other lamma (is problem with docker? bho) however after 10 mins resolve and work write wall of types &lt;/p&gt; &lt;p&gt;I also tried from the Docker terminal with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl http://localhost:11434/api/generate -d '{ I just want to build my own vertical AI... but apparently, even though I’m a programmer, I actually suck and don’t even understand English properly. have serve https://i.imgur.com/8nWCwKa.png &amp;gt;&amp;gt; &amp;quot;model&amp;quot;: &amp;quot;deepseek-r1&amp;quot;, &amp;gt;&amp;gt; &amp;quot;prompt&amp;quot;:&amp;quot;Why is the sky blue?&amp;quot; &amp;gt;&amp;gt; }' Invoke-WebRequest : Impossibile trovare un parametro posizionale che accetta l'argomento '{ &amp;quot;model&amp;quot;: &amp;quot;deepseek-r1&amp;quot;, &amp;quot;prompt&amp;quot;:&amp;quot;Why is the sky blue?&amp;quot; }'. In riga:1 car:1 + curl http://localhost:11434/api/generate -d '{ + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : InvalidArgument: (:) [Invoke-WebRequest], ParameterBindingException + FullyQualifiedErrorId : PositionalParameterNotFound,Microsoft.PowerShell.Commands.InvokeWebRequestCommand PS C:\Users\chrig&amp;gt; curl http://localhost:11434/api/generate -d '{ &amp;gt;&amp;gt; &amp;quot;model&amp;quot;: &amp;quot;llama3.2&amp;quot;, &amp;gt;&amp;gt; &amp;quot;prompt&amp;quot;:&amp;quot;Why is the sky blue?&amp;quot; &amp;gt;&amp;gt; }' Invoke-WebRequest : Impossibile trovare un parametro posizionale che accetta l'argomento '{ &amp;quot;model&amp;quot;: &amp;quot;llama3.2&amp;quot;, &amp;quot;prompt&amp;quot;:&amp;quot;Why is the sky blue?&amp;quot; }'. In riga:1 car:1 + curl http://localhost:11434/api/generate -d '{ + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : InvalidArgument: (:) [Invoke-WebRequest], ParameterBindingException + FullyQualifiedErrorId : PositionalParameterNotFound,Microsoft.PowerShell.Commands.InvokeWebRequestCommand &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;try so with keshire, with insucess.&lt;br /&gt; Wanted upload photos but for strange reason reddit block me.&lt;/p&gt; &lt;p&gt;I just want to build my own vertical AI... but apparently, even though I’m a programmer, I actually suck and don’t even understand English properly.&lt;br /&gt; So if want tell me another &amp;quot;guide&amp;quot; that work and expain ALL I can try follow (I am very near to unistall all and stop this experience).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Taro_Happy"&gt; /u/Taro_Happy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9knwg/ollama_with_an_model_not_want_work_in_serve_try/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9knwg/ollama_with_an_model_not_want_work_in_serve_try/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j9knwg/ollama_with_an_model_not_want_work_in_serve_try/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-12T13:56:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9kv29</id>
    <title>Ollama info about gemma3 context length isn't consistent</title>
    <updated>2025-03-12T14:05:53+00:00</updated>
    <author>
      <name>/u/Fade78</name>
      <uri>https://old.reddit.com/user/Fade78</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On the official page there is, if we take the example of the 27b model, a context length in the specs of 8k ( gemma3.context_length=8192) but in the text description it is written 128k.&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/gemma3"&gt;https://ollama.com/library/gemma3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What does it mean? Ollama can't run it with the full context? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fade78"&gt; /u/Fade78 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9kv29/ollama_info_about_gemma3_context_length_isnt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9kv29/ollama_info_about_gemma3_context_length_isnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j9kv29/ollama_info_about_gemma3_context_length_isnt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-12T14:05:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9ia22</id>
    <title>gemma3:12b vs phi4:14b vs..</title>
    <updated>2025-03-12T11:53:24+00:00</updated>
    <author>
      <name>/u/grigio</name>
      <uri>https://old.reddit.com/user/grigio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried some preliminary benchmarks with gemma3 but it seems phi4 is still superior. What is your under 14b preferred model? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grigio"&gt; /u/grigio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9ia22/gemma312b_vs_phi414b_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9ia22/gemma312b_vs_phi414b_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j9ia22/gemma312b_vs_phi414b_vs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-12T11:53:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9lqyz</id>
    <title>OpenArc 1.0.2: OpenAI endpoints, OpenWebUI support! Get faster inference from Intel CPUs, GPUs and NPUs now with community tooling</title>
    <updated>2025-03-12T14:50:07+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;Today I am launching &lt;a href="https://github.com/SearchSavior/OpenArc/tree/main"&gt;OpenArc&lt;/a&gt; 1.0.2 with fully supported OpenWebUI functionality! &lt;/p&gt; &lt;p&gt;Nailing OpenAI compatibility so early in OpenArc's development positions the project to mature with community tooling as Intel releases more hardware, expands support for NPU devices, smaller models become more performant and as we evolve past the Transformer to whatever comes next. &lt;/p&gt; &lt;p&gt;I plan to use OpenArc as a development tool for my work projects which require acceleration for other types of ML beyond LLMs- embeddings, classifiers, OCR with Paddle. Frontier models can't do everything with enough accuracy and are not silver bullets&lt;/p&gt; &lt;p&gt;The repo details how to get OpenWebUI setup; for now it is the only chat front-end I have time to maintain. If you have other tools you wanted to see integrated open an issue or submit a pull request. &lt;/p&gt; &lt;p&gt;What's up next :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Confirm openai support for other implementations like smolagents, Autogen&lt;/li&gt; &lt;li&gt;&lt;p&gt;Move from conda to uv. This week I was enlightened and will never go back to conda.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Vision support for Qwen2-VL, Qwen2.5-VL, Phi-4 multi-modal, olmOCR (which is qwen2vl 7b tune) InternVL2 and probably more&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;An official &lt;a href="https://discord.gg/PnuTBVcr"&gt;Discord!&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Best way to reach me.&lt;/li&gt; &lt;li&gt;If you are interested in contributing join the Discord!&lt;/li&gt; &lt;li&gt;If you need help converting models &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Discussions on GitHub for:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SearchSavior/OpenArc/discussions/11"&gt;Linux Drivers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SearchSavior/OpenArc/discussions/12"&gt;Windows Drivers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SearchSavior/OpenArc/discussions/13"&gt;Environment Setup&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Instructions and models for testing out text generation for &lt;a href="https://github.com/SearchSavior/OpenArc/issues/14"&gt;NPU devices&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;A sister repo, &lt;a href="https://github.com/SearchSavior/OpenArcProjects"&gt;OpenArcProjects&lt;/a&gt;!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Share the things you build with OpenArc, OpenVINO, oneapi toolkit, IPEX-LLM and future tooling from Intel&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for checking out OpenArc. I hope it ends up being a useful tool.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9lqyz/openarc_102_openai_endpoints_openwebui_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9lqyz/openarc_102_openai_endpoints_openwebui_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j9lqyz/openarc_102_openai_endpoints_openwebui_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-12T14:50:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9gic5</id>
    <title>Ollama 0.6 with support for Google Gemma 3</title>
    <updated>2025-03-12T09:57:13+00:00</updated>
    <author>
      <name>/u/jmorganca</name>
      <uri>https://old.reddit.com/user/jmorganca</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1j9gic5/ollama_06_with_support_for_google_gemma_3/"&gt; &lt;img alt="Ollama 0.6 with support for Google Gemma 3" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="Ollama 0.6 with support for Google Gemma 3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jmorganca"&gt; /u/jmorganca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/library/gemma3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1j9gic5/ollama_06_with_support_for_google_gemma_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1j9gic5/ollama_06_with_support_for_google_gemma_3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-12T09:57:13+00:00</published>
  </entry>
</feed>
