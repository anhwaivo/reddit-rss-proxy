<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-17T12:10:18+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jc253n</id>
    <title>Noob - GPU usage question - low while replying</title>
    <updated>2025-03-15T18:49:35+00:00</updated>
    <author>
      <name>/u/nraygun</name>
      <uri>https://old.reddit.com/user/nraygun</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got Ollama working on my main desktop PC(AMD Ryzen 5 3600X, 16GB, GTX 1050) running MX Linux with the UI hosted in a Docker container on my Unraid server. I'm using deepseek-R1. I'm surprised it works at all on my humble little system!&lt;/p&gt; &lt;p&gt;I watch nvidia-smi and I see that the GPU doesn't really get exercised when it's replying. Before it goes into &amp;quot;thinking&amp;quot; it spikes to 99%, then while &amp;quot;thinking&amp;quot; it only goes to 12-17%. When it's replying, it uses 4-8%.&lt;/p&gt; &lt;p&gt;Is this to be expected? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nraygun"&gt; /u/nraygun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jc253n/noob_gpu_usage_question_low_while_replying/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jc253n/noob_gpu_usage_question_low_while_replying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jc253n/noob_gpu_usage_question_low_while_replying/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T18:49:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jc2bwv</id>
    <title>Why is gemma3 27b-it-fp16 taking 64GB.</title>
    <updated>2025-03-15T18:58:09+00:00</updated>
    <author>
      <name>/u/Sanandaji</name>
      <uri>https://old.reddit.com/user/Sanandaji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 56GB of VRAM. Per &lt;a href="https://ollama.com/library/gemma3/tags"&gt;https://ollama.com/library/gemma3/tags&lt;/a&gt; 27b-it-fp16 should be 55GB but the size shows 64GB for me and it slows my machine down to almost a halt. I get 3 tokens per second in CLI, open webui cannot even run it, and this is the usage i see: &lt;a href="https://i.imgur.com/wPtFc2b.png"&gt;https://i.imgur.com/wPtFc2b.png&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Is this an issue between ollama and gemma3 or is this normal behavior?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sanandaji"&gt; /u/Sanandaji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jc2bwv/why_is_gemma3_27bitfp16_taking_64gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jc2bwv/why_is_gemma3_27bitfp16_taking_64gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jc2bwv/why_is_gemma3_27bitfp16_taking_64gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T18:58:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jc8vod</id>
    <title>adeelahmad/ReasonableLlama3-3B-Jr · Hugging Face</title>
    <updated>2025-03-16T00:06:54+00:00</updated>
    <author>
      <name>/u/adeelahmadch</name>
      <uri>https://old.reddit.com/user/adeelahmadch</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adeelahmadch"&gt; /u/adeelahmadch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/adeelahmad/ReasonableLlama3-3B-Jr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jc8vod/adeelahmadreasonablellama33bjr_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jc8vod/adeelahmadreasonablellama33bjr_hugging_face/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-16T00:06:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbkbai</id>
    <title>The Complete Guide to Building Your Free Local AI Assistant with Ollama and Open WebUI</title>
    <updated>2025-03-15T01:45:48+00:00</updated>
    <author>
      <name>/u/PeterHash</name>
      <uri>https://old.reddit.com/user/PeterHash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just published a no-BS step-by-step guide on Medium for anyone tired of paying monthly AI subscription fees or worried about privacy when using tools like ChatGPT. In my guide, I walk you through setting up your local AI environment using &lt;strong&gt;Ollama&lt;/strong&gt; and &lt;strong&gt;Open WebUI&lt;/strong&gt;—a setup that lets you run a custom ChatGPT entirely on your computer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What You'll Learn:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How to eliminate AI subscription costs (yes, zero monthly fees!)&lt;/li&gt; &lt;li&gt;Achieve complete privacy: your data stays local, with no third-party data sharing&lt;/li&gt; &lt;li&gt;Enjoy faster response times (no more waiting during peak hours)&lt;/li&gt; &lt;li&gt;Get complete customization to build specialized AI assistants for your unique needs&lt;/li&gt; &lt;li&gt;Overcome token limits with unlimited usage&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Setup Process:&lt;/strong&gt;&lt;br /&gt; With about 15 terminal commands, you can have everything up and running in under an hour. I included all the code, screenshots, and troubleshooting tips that helped me through the setup. The result is a clean web interface that feels like ChatGPT—entirely under your control.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A Sneak Peek at the Guide:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Toolstack Overview:&lt;/strong&gt; You'll need (Ollama, Open WebUI, a &lt;strong&gt;GPU-powered machine&lt;/strong&gt;, etc.)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Environment Setup:&lt;/strong&gt; How to configure Python 3.11 and set up your system&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Installing &amp;amp; Configuring:&lt;/strong&gt; Detailed instructions for both Ollama and Open WebUI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced Features:&lt;/strong&gt; I also cover features like web search integration, a code interpreter, custom model creation, and even a preview of upcoming advanced RAG features for creating custom knowledge bases.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've been using this setup for two months, and it's completely replaced my paid AI subscriptions while boosting my workflow efficiency. Stay tuned for part two, which will cover advanced RAG implementation, complex workflows, and tool integration based on your feedback.&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@hautel.alex2000/build-your-local-ai-from-zero-to-a-custom-chatgpt-interface-with-ollama-open-webui-6bee2c5abba3"&gt;&lt;strong&gt;Read the complete guide here →&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Let's Discuss:&lt;/strong&gt;&lt;br /&gt; What AI workflows would you most want to automate with your own customizable AI assistant? Are there specific use cases or features you're struggling with that you'd like to see in future guides? Share your thoughts below—I'd love to incorporate popular requests in the upcoming instalment!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PeterHash"&gt; /u/PeterHash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbkbai/the_complete_guide_to_building_your_free_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbkbai/the_complete_guide_to_building_your_free_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbkbai/the_complete_guide_to_building_your_free_local_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T01:45:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jccoua</id>
    <title>Image testing + Gemma-3-27B-it-FP16 + torch + 8x AMD Instinct Mi50 Server</title>
    <updated>2025-03-16T03:31:33+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8xeiv2vc1zoe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jccoua/image_testing_gemma327bitfp16_torch_8x_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jccoua/image_testing_gemma327bitfp16_torch_8x_amd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-16T03:31:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbviqb</id>
    <title>An Open-Source AI Assistant for Chatting with Your Developer Docs</title>
    <updated>2025-03-15T13:50:04+00:00</updated>
    <author>
      <name>/u/eleven-five</name>
      <uri>https://old.reddit.com/user/eleven-five</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been working on &lt;strong&gt;Ragpi&lt;/strong&gt;, an open-source AI assistant that builds knowledge bases from docs, GitHub Issues and READMEs. It uses PostgreSQL with pgvector as a vector DB and leverages RAG to answer technical questions through an API. Ragpi also integrates with Discord and Slack, making it easy to interact with directly from those platforms.&lt;/p&gt; &lt;p&gt;Some things it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Creates knowledge bases from documentation websites, GitHub Issues and READMEs&lt;/li&gt; &lt;li&gt;Uses hybrid search (semantic + keyword) for retrieval&lt;/li&gt; &lt;li&gt;Uses tool calling to dynamically search and retrieve relevant information during conversations&lt;/li&gt; &lt;li&gt;Works with OpenAI, Ollama, DeepSeek, or any OpenAI-compatible API&lt;/li&gt; &lt;li&gt;Provides a simple REST API for querying and managing sources&lt;/li&gt; &lt;li&gt;Integrates with Discord and Slack for easy interaction&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Built with:&lt;/strong&gt; FastAPI, Celery and Postgres&lt;/p&gt; &lt;p&gt;It’s still a work in progress, but I’d love some feedback!&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/ragpi/ragpi"&gt;https://github.com/ragpi/ragpi&lt;/a&gt;&lt;br /&gt; Docs: &lt;a href="https://docs.ragpi.io/"&gt;https://docs.ragpi.io/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eleven-five"&gt; /u/eleven-five &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbviqb/an_opensource_ai_assistant_for_chatting_with_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbviqb/an_opensource_ai_assistant_for_chatting_with_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbviqb/an_opensource_ai_assistant_for_chatting_with_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T13:50:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jc7out</id>
    <title>I have switched my GPU to an amd rx 9070xt from a 2080ti. Ollama is not utilising new gpu at all.</title>
    <updated>2025-03-15T23:08:30+00:00</updated>
    <author>
      <name>/u/Bran04don</name>
      <uri>https://old.reddit.com/user/Bran04don</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How can I get ollama to use my new gpu? Model is the 9070xt Nitro+&lt;/p&gt; &lt;p&gt;I can see my cpu usage maxing out when running ollama while gpu is at idle utilisation.&lt;/p&gt; &lt;p&gt;Before it was working fine on the 2080ti maxing the card instead.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bran04don"&gt; /u/Bran04don &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jc7out/i_have_switched_my_gpu_to_an_amd_rx_9070xt_from_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jc7out/i_have_switched_my_gpu_to_an_amd_rx_9070xt_from_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jc7out/i_have_switched_my_gpu_to_an_amd_rx_9070xt_from_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T23:08:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcf134</id>
    <title>Ryzen 5700G with two RTX 3060 cards for 24 GB of VRAM</title>
    <updated>2025-03-16T06:01:52+00:00</updated>
    <author>
      <name>/u/richterbg</name>
      <uri>https://old.reddit.com/user/richterbg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is such a configuration a good idea? I have a 5700G with 64GB of RAM as my backup PC and think about adding two RTX 3060 cards and an 850W PSU in order to play with ollama. The monitor is going to be connected to the integrated graphics, while the Nvidia cards will be used for the models. The motherboard is AsRock B450 Gaming K4.&lt;/p&gt; &lt;p&gt;As long as I know, the 5700G has some PCI limitations, but are they fatal? As usual, I will shop around for used video cards while the PSU is going to be new, and the total amount of the upgrade should be about 500 USD. The RTX 3090s in my country are not cheap, so this is not quite an option.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/richterbg"&gt; /u/richterbg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcf134/ryzen_5700g_with_two_rtx_3060_cards_for_24_gb_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcf134/ryzen_5700g_with_two_rtx_3060_cards_for_24_gb_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jcf134/ryzen_5700g_with_two_rtx_3060_cards_for_24_gb_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-16T06:01:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbwwxh</id>
    <title>Why didn't they design gemma3 to fit in GPU memory more efficiently?</title>
    <updated>2025-03-15T14:57:18+00:00</updated>
    <author>
      <name>/u/droxy429</name>
      <uri>https://old.reddit.com/user/droxy429</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma3 is advertised as the &amp;quot;most capable model that runs on a single GPU. So if they figure the target market for this model is people running on a single GPU, why wouldn't they make the size of each model scale up with typical GPU memory sizes: 4GB, 8GB, 16GB, 24GB... &lt;a href="https://ollama.com/library/gemma3/tags"&gt;Check out the sizes of these models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The 4b is 3.3GB which fits nicely in a 4GB memory GPU.&lt;/p&gt; &lt;p&gt;The 12b is 8.1GB which is a little too big to fit in an 8GB memory GPU.&lt;/p&gt; &lt;p&gt;The 27b is 17GB which is just a little too big to fit in a 16GB memory GPU.&lt;/p&gt; &lt;p&gt;This is frustrating since I have a 16GB GPU and need to run the 8.1GB model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/droxy429"&gt; /u/droxy429 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbwwxh/why_didnt_they_design_gemma3_to_fit_in_gpu_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jbwwxh/why_didnt_they_design_gemma3_to_fit_in_gpu_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jbwwxh/why_didnt_they_design_gemma3_to_fit_in_gpu_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T14:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcbitb</id>
    <title>Using Gen AI for variable analytics</title>
    <updated>2025-03-16T02:25:41+00:00</updated>
    <author>
      <name>/u/thentangler</name>
      <uri>https://old.reddit.com/user/thentangler</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jcbitb/using_gen_ai_for_variable_analytics/"&gt; &lt;img alt="Using Gen AI for variable analytics" src="https://external-preview.redd.it/dCnJmiZETgP2qjwrAV6dFwSOiCRKn5NdH0BAt10y0FA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=42b11b0318c2abc4fe8fc6dc38cfc254e2f27df9" title="Using Gen AI for variable analytics" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know LLMs are all the rage now. But I thought they can only be used to predict language based modals. For developing predictive models for data analytics such as recognizing defects on a widget or predicting when a piece of hardware will fail, methods such as computer vision and machine learning were typically used. But now they are using generative AI and LLMs to predict protein synthesis and detect tumors in MRI scans. &lt;/p&gt; &lt;p&gt;In this article, they converted the amino acid sequence into a language and applied LLM on it. So I get that. And in the same vein, I’m guessing they applied millions of hours of doctors transcripts for identifying tumors from an MRI scans to LLMs. Im still unsure how they converted the MRI images into a language. &lt;/p&gt; &lt;p&gt;But if one were to apply Generative AI to predict when an equipment will fail, or how a product will turn out based on its measurements, how would one use LLMs? We would have to convert time series data into a language or the measurements into a language with an outcome. Wouldn’t it be easier to just use existing machine learning algorithms for that? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thentangler"&gt; /u/thentangler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://cen.acs.org/physical-chemistry/protein-folding/Generative-AI-dreaming-new-proteins/101/i12"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcbitb/using_gen_ai_for_variable_analytics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jcbitb/using_gen_ai_for_variable_analytics/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-16T02:25:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd5o0j</id>
    <title>Comparing the power of AMD GPUs with the power of Apple Silicons to run LLMs</title>
    <updated>2025-03-17T05:41:16+00:00</updated>
    <author>
      <name>/u/zarinfam</name>
      <uri>https://old.reddit.com/user/zarinfam</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jd5o0j/comparing_the_power_of_amd_gpus_with_the_power_of/"&gt; &lt;img alt="Comparing the power of AMD GPUs with the power of Apple Silicons to run LLMs" src="https://external-preview.redd.it/rKFF4X5gng3F1qCBH2GhsqPGi5I-xusT2VZHaXomZsY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a21ec896bf17b25b8d961d4f33b49c7324aa4299" title="Comparing the power of AMD GPUs with the power of Apple Silicons to run LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zarinfam"&gt; /u/zarinfam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/itnext/how-to-use-models-run-by-lm-studio-from-a-spring-ai-application-cbbb32b031ab?sk=87cf37ac384ba6f5ce7a5e795cdedd7f"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd5o0j/comparing_the_power_of_amd_gpus_with_the_power_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jd5o0j/comparing_the_power_of_amd_gpus_with_the_power_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T05:41:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jc0yu2</id>
    <title>Tiny Ollama Chat: A Super Lightweight Alternative to OpenWebUI</title>
    <updated>2025-03-15T17:57:32+00:00</updated>
    <author>
      <name>/u/No-Carpet-211</name>
      <uri>https://old.reddit.com/user/No-Carpet-211</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone,&lt;/p&gt; &lt;p&gt;I created Tiny Ollama Chat after finding OpenWebUI too resource-heavy for my needs. It's a minimal but functional UI - just the essentials for interacting with your Ollama models. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Check out the repo&lt;/strong&gt; &lt;a href="https://github.com/anishgowda21/tiny-ollama-chat"&gt;https://github.com/anishgowda21/tiny-ollama-chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;p&gt;Its,&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Incredibly lightweight (only 32MB Docker image!)&lt;/li&gt; &lt;li&gt;Real-time message streaming&lt;/li&gt; &lt;li&gt;Conversation history and multiple model support&lt;/li&gt; &lt;li&gt;Custom Ollama URL configuration&lt;/li&gt; &lt;li&gt;Persistent storage with SQLite&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It offers fast startup time, simple deployment (Docker or local build), and a clean UI focused on the chat experience.&lt;/p&gt; &lt;p&gt;Would love your feedback if you try it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Carpet-211"&gt; /u/No-Carpet-211 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jc0yu2/tiny_ollama_chat_a_super_lightweight_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jc0yu2/tiny_ollama_chat_a_super_lightweight_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jc0yu2/tiny_ollama_chat_a_super_lightweight_alternative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-15T17:57:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcqxi0</id>
    <title>Gemma 3 issues in Ollama Docker container.</title>
    <updated>2025-03-16T17:38:15+00:00</updated>
    <author>
      <name>/u/Inner-End7733</name>
      <uri>https://old.reddit.com/user/Inner-End7733</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, when I try to run gemma3 both 12b and 4b send the error saying that it's not compatible with my version of Ollama.&lt;/p&gt; &lt;p&gt;When I use &amp;quot; docker images | grep ollama&amp;quot; it says I have the latest. &lt;/p&gt; &lt;p&gt;anyone else know what's going on? maybe the docker image isn't upgraded yet? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inner-End7733"&gt; /u/Inner-End7733 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcqxi0/gemma_3_issues_in_ollama_docker_container/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcqxi0/gemma_3_issues_in_ollama_docker_container/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jcqxi0/gemma_3_issues_in_ollama_docker_container/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-16T17:38:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcv1ms</id>
    <title>Mistral NeMo identity crisis</title>
    <updated>2025-03-16T20:34:51+00:00</updated>
    <author>
      <name>/u/Inner-End7733</name>
      <uri>https://old.reddit.com/user/Inner-End7733</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was Mistral NeMo origionally called &amp;quot;Nemistral?&amp;quot; It's verry instant that it's not called &amp;quot;mistral NeMo and that must be a different model. &lt;/p&gt; &lt;p&gt;it even provided me with this link &amp;quot;&lt;a href="https://mistral.ai/blog/introducing-nemistral"&gt;https://mistral.ai/blog/introducing-nemistral&lt;/a&gt;&amp;quot; which is dead&lt;/p&gt; &lt;p&gt;very interesting behavior. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inner-End7733"&gt; /u/Inner-End7733 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcv1ms/mistral_nemo_identity_crisis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcv1ms/mistral_nemo_identity_crisis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jcv1ms/mistral_nemo_identity_crisis/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-16T20:34:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd2l6x</id>
    <title>My Acer Aspire 3 16gb shows a GPU. But it never seems to be utilized per task manager.</title>
    <updated>2025-03-17T02:36:19+00:00</updated>
    <author>
      <name>/u/College_student_444</name>
      <uri>https://old.reddit.com/user/College_student_444</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Models such as Deepseek r1 8b, llama3.2 3b, etc. runs at 100% CPU. Is there anything I should be doing to make it use the gpu? Task manager shows dedicated 512 MB gpu memory and 7.6 GB shared gpu memory. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/College_student_444"&gt; /u/College_student_444 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd2l6x/my_acer_aspire_3_16gb_shows_a_gpu_but_it_never/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd2l6x/my_acer_aspire_3_16gb_shows_a_gpu_but_it_never/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jd2l6x/my_acer_aspire_3_16gb_shows_a_gpu_but_it_never/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T02:36:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcymiu</id>
    <title>Saving a chat when using command line</title>
    <updated>2025-03-16T23:17:04+00:00</updated>
    <author>
      <name>/u/cbmarketer</name>
      <uri>https://old.reddit.com/user/cbmarketer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just started using Ollama. I am running it from the command line. I'm using Ollama to use the LLMs without giving my data away. Ideally, I want to save a session and be able to come back to it at a later date.&lt;/p&gt; &lt;p&gt;I tried the save &amp;lt;model&amp;gt; command that I got from help, but that didn't seem to work. It didn't confirm anything and I couldn't reload it. Maybe I didn't do it right?&lt;/p&gt; &lt;p&gt;Is this possible or do I need to use a different application?&lt;/p&gt; &lt;p&gt;Thanks in advance for your help,&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cbmarketer"&gt; /u/cbmarketer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcymiu/saving_a_chat_when_using_command_line/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcymiu/saving_a_chat_when_using_command_line/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jcymiu/saving_a_chat_when_using_command_line/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-16T23:17:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcupef</id>
    <title>Image testing + Gemma-3-27B-it-FP16 + torch + 4x AMD Instinct Mi210 Server</title>
    <updated>2025-03-16T20:19:43+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fbqz7sj814pe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcupef/image_testing_gemma327bitfp16_torch_4x_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jcupef/image_testing_gemma327bitfp16_torch_4x_amd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-16T20:19:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd5xaj</id>
    <title>Is there a self correcting model which can browse the internet for finding errors in code before displaying the final result. Like I want to make a simple web app using streamlit using Gemini but the first shot is incorrect</title>
    <updated>2025-03-17T05:59:42+00:00</updated>
    <author>
      <name>/u/SnooBananas5215</name>
      <uri>https://old.reddit.com/user/SnooBananas5215</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooBananas5215"&gt; /u/SnooBananas5215 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd5xaj/is_there_a_self_correcting_model_which_can_browse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd5xaj/is_there_a_self_correcting_model_which_can_browse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jd5xaj/is_there_a_self_correcting_model_which_can_browse/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T05:59:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcyu64</id>
    <title>KubeAI v0.18.0: Load Ollama Models from PVC</title>
    <updated>2025-03-16T23:26:48+00:00</updated>
    <author>
      <name>/u/samosx</name>
      <uri>https://old.reddit.com/user/samosx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've just merged support for loading Ollama models directly from a Persistent Volume Claim (PVC) into KubeAI v0.18.0. This allows you to manage and persist Ollama models more easily in Kubernetes environments. This is especially useful for when you want fast scale ups of the same model.&lt;/p&gt; &lt;p&gt;See the &lt;a href="https://github.com/substratusai/kubeai/pull/416"&gt;GitHub PR&lt;/a&gt; and &lt;a href="https://www.kubeai.org/how-to/load-models-from-pvc/#ollama"&gt;user docs&lt;/a&gt; for more info.&lt;/p&gt; &lt;p&gt;Feedback and questions are welcome!&lt;/p&gt; &lt;p&gt;Link to GitHub: &lt;a href="https://github.com/substratusai/kubeai"&gt;https://github.com/substratusai/kubeai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samosx"&gt; /u/samosx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcyu64/kubeai_v0180_load_ollama_models_from_pvc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcyu64/kubeai_v0180_load_ollama_models_from_pvc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jcyu64/kubeai_v0180_load_ollama_models_from_pvc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-16T23:26:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd86ia</id>
    <title>???</title>
    <updated>2025-03-17T08:54:50+00:00</updated>
    <author>
      <name>/u/1k-Ping</name>
      <uri>https://old.reddit.com/user/1k-Ping</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why is ollama/deepseek barely using my GPU and mostly using my CPU, but then it isn't even making full utilization of the CPU either??&lt;/p&gt; &lt;p&gt;context: I'm new to running local AI stuff. PC specs are ryzen 5 5600g, rtx 4070-S, 16gb ddr4 ram. Running arch linux (btw).&lt;/p&gt; &lt;p&gt;ollama is fully updated and I'm running deepseek-r1:14b with it.&lt;/p&gt; &lt;p&gt;picture of usage/utilization mid-process: &lt;a href="https://imgur.com/a/qaaVVQJ"&gt;https://imgur.com/a/qaaVVQJ&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1k-Ping"&gt; /u/1k-Ping &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd86ia/_/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd86ia/_/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jd86ia/_/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T08:54:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcmlwt</id>
    <title>What's the closest open source/local ai tool we have of Gemini live? (if any)</title>
    <updated>2025-03-16T14:26:13+00:00</updated>
    <author>
      <name>/u/Game-Lover44</name>
      <uri>https://old.reddit.com/user/Game-Lover44</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jcmlwt/whats_the_closest_open_sourcelocal_ai_tool_we/"&gt; &lt;img alt="What's the closest open source/local ai tool we have of Gemini live? (if any)" src="https://preview.redd.it/h3p7h9l2a2pe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6864826d8d872f3e3c3890b95fe3bd4a4a417f9d" title="What's the closest open source/local ai tool we have of Gemini live? (if any)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Game-Lover44"&gt; /u/Game-Lover44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h3p7h9l2a2pe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcmlwt/whats_the_closest_open_sourcelocal_ai_tool_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jcmlwt/whats_the_closest_open_sourcelocal_ai_tool_we/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-16T14:26:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcuung</id>
    <title>I built a vision-native RAG pipeline</title>
    <updated>2025-03-16T20:26:10+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My brother and I have been working on &lt;a href="https://github.com/databridge-org/databridge-core"&gt;DataBridge&lt;/a&gt;: an open-source and multimodal database. After experimenting with various AI models, we realized that they were particularly bad at answering questions which required retrieving over images and other multimodal data.&lt;/p&gt; &lt;p&gt;That is, if I uploaded a 10-20 page PDF to ChatGPT, and ask it to get me a result from a particular diagram in the PDF, it would fail and hallucinate instead. I faced the same issue with Claude, but not with Gemini.&lt;/p&gt; &lt;p&gt;Turns out, the issue was with how these systems ingest documents. Seems like both Claude and GPT embed larger PDFs by parsing them into text, and then adding the entire thing to the context of the chat. While this works for text-heavy documents, it fails for queries/documents relating to diagrams, graphs, or infographics.&lt;/p&gt; &lt;p&gt;Something that can help solve this is directly embedding the document as a list of images, and performing retrieval over that - getting the closest images to the query, and feeding the LLM exactly those images. This helps reduce the amount of tokens an LLM consumes while also increasing the visual reasoning ability of the model.&lt;/p&gt; &lt;p&gt;We've implemented a &lt;a href="https://databridge.mintlify.app/concepts/colpali"&gt;one-line solution&lt;/a&gt; that does exactly this with DataBridge. You can check out the specifics in the attached blog, or get started with it through our quick start guide: &lt;a href="https://databridge.mintlify.app/getting-started"&gt;https://databridge.mintlify.app/getting-started&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcuung/i_built_a_visionnative_rag_pipeline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcuung/i_built_a_visionnative_rag_pipeline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jcuung/i_built_a_visionnative_rag_pipeline/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-16T20:26:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd7r05</id>
    <title>Gemma 3 code interpreter for open webui</title>
    <updated>2025-03-17T08:19:48+00:00</updated>
    <author>
      <name>/u/Ok_Green5623</name>
      <uri>https://old.reddit.com/user/Ok_Green5623</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used some tutorial to run gemma 3 via ollama and open-webui and there was an interesting option to give it access to code interpreter, but it didn't quite work out of the box. I hacked a bit default code interpreter prompt and it worked pretty nicely. Here is prompt I quickly hacked. Does anyone have a better version?&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;h4&gt;Tools Available&lt;/h4&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Code Interpreter&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;You have access to a Python shell that runs directly in the user's browser, enabling fast execution of code for analysis, calculations, or problem-solving. Use it in this response.&lt;/li&gt; &lt;li&gt;The code will be executed and the result will be visible only for you to assist you with your answer to user query. It is quite different from the normal nicely formated python code you wrap in &lt;code&gt;python ...&lt;/code&gt; formating.&lt;/li&gt; &lt;li&gt;In order to use the code interpreter you have to enclose the code to be executed in &amp;lt;code_interpreter type=&amp;quot;code&amp;quot; lang=&amp;quot;python&amp;quot;&amp;gt;&amp;lt;/code_interpreter&amp;gt; instead. In example: &amp;lt;code_interpreter type=&amp;quot;code&amp;quot; lang=&amp;quot;python&amp;quot;&amp;gt; print(&amp;quot;Hello world!&amp;quot;) &amp;lt;code_interpreter&amp;gt;&lt;/li&gt; &lt;li&gt;Notice, missing in this case &amp;quot;&lt;code&gt;python&amp;quot; and &amp;quot;&lt;/code&gt;&amp;quot;. This allows to hide the code from user completely.&lt;/li&gt; &lt;li&gt;The Python code you write can incorporate a wide array of libraries, handle data manipulation or visualization, perform API calls for web-related tasks, or tackle virtually any computational challenge. Use this flexibility to &lt;strong&gt;think outside the box, craft elegant solutions, and harness Python's full potential&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;When using the code wrap it in the xml tags described above and stop your response. If you don't, the code won't execute. After the code finished execution you will have access to the output of the code. Continue your response using the information obtained.&lt;/li&gt; &lt;li&gt;When coding, &lt;strong&gt;always aim to print meaningful outputs&lt;/strong&gt; (e.g., results, tables, summaries, or visuals) to better interpret and verify the findings. Avoid relying on implicit outputs; prioritize explicit and clear print statements so the results are effectively communicated to the user.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;After obtaining the printed output, &lt;strong&gt;always provide a concise analysis, interpretation, or next steps to help the user understand the findings or refine the outcome further.&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;If the results are unclear, unexpected, or require validation, refine the code and execute it again as needed. Always aim to deliver meaningful insights from the results, iterating if necessary.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;If a link to an image, audio, or any file is provided in markdown format in the output, ALWAYS regurgitate word for word, explicitly display it as part of the response to ensure the user can access it easily, do NOT change the link.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;All responses should be communicated in the chat's primary language, ensuring seamless understanding. If the chat is multilingual, default to English for clarity.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Ensure that the tools are effectively utilized to achieve the highest-quality analysis for the user. ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Green5623"&gt; /u/Ok_Green5623 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd7r05/gemma_3_code_interpreter_for_open_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd7r05/gemma_3_code_interpreter_for_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jd7r05/gemma_3_code_interpreter_for_open_webui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T08:19:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd40r2</id>
    <title>GPT 4.5 System Prompt Preamble</title>
    <updated>2025-03-17T03:55:48+00:00</updated>
    <author>
      <name>/u/foomanchu89</name>
      <uri>https://old.reddit.com/user/foomanchu89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jd40r2/gpt_45_system_prompt_preamble/"&gt; &lt;img alt="GPT 4.5 System Prompt Preamble" src="https://preview.redd.it/e84gtswqa6pe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a69962b66b61145ec819b5d83e10aa81d6a0b0a6" title="GPT 4.5 System Prompt Preamble" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty cool but buried in the docs&lt;/p&gt; &lt;p&gt;&lt;a href="https://platform.openai.com/docs/guides/prompt-engineering#tactic-use-delimiters-to-clearly-indicate-distinct-parts-of-the-input"&gt;https://platform.openai.com/docs/guides/prompt-engineering#tactic-use-delimiters-to-clearly-indicate-distinct-parts-of-the-input&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I bet it works on open source models too&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foomanchu89"&gt; /u/foomanchu89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e84gtswqa6pe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd40r2/gpt_45_system_prompt_preamble/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jd40r2/gpt_45_system_prompt_preamble/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T03:55:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd4op1</id>
    <title>Clara: Browser based Local AI Chat, ImageGen with simple custom Agent builder.</title>
    <updated>2025-03-17T04:36:21+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jd4op1/clara_browser_based_local_ai_chat_imagegen_with/"&gt; &lt;img alt="Clara: Browser based Local AI Chat, ImageGen with simple custom Agent builder." src="https://external-preview.redd.it/662kDyxmF0rz11JzhX_fLISnvTFW9w3CE6gaa8Ta4wg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abda2fd3ebfe247a77cb433fc9a2bcd0dcc638b1" title="Clara: Browser based Local AI Chat, ImageGen with simple custom Agent builder." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey devs,&lt;/p&gt; &lt;p&gt;I built Clara because I wanted a simple, lightweight AI assistant that runs entirely on my own machine. Most AI tools depend on cloud services, track usage, or require heavy setups—Clara is different. It connects directly to Ollama for LLMs and ComfyUI for Stable Diffusion image generation, with zero external dependencies.&lt;/p&gt; &lt;p&gt;No docker, no backend, just ollama and clara installed on the pc is enough.&lt;/p&gt; &lt;p&gt;🔗 Repo: &lt;a href="https://rgithub.com/badboysm890/ClaraVerse"&gt;https://rgithub.com/badboysm890/ClaraVerse&lt;/a&gt; 💻 Download the app: &lt;a href="https://github.com/badboysm890/ClaraVerse/releases/tag/v0.2.0"&gt;https://github.com/badboysm890/ClaraVerse/releases/tag/v0.2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Why Clara? 1. Runs Locally – No cloud, no API calls, fully private. 2. All the data is stored in IndexDB 3. Fast &amp;amp; Lightweight – I love open web UI but now its too big for my machine 4. Agent Builder – Create simple AI agents and convert them into apps. 5. ComfyUI Integration – Generate images with Stable Diffusion models. 6. Custom Model Support – Works with any Ollama-compatible LLM. 7. Built-in Image Gallery – Just added is so i can have all the images generated in one place&lt;/p&gt; &lt;p&gt;💡 Need Help! I don’t have a Windows machine, so if anyone can help with building and testing the Windows version, I’d really appreciate it! Let me know if you’re interested.&lt;/p&gt; &lt;p&gt;Would love to hear your feedback if you try it! 🚀&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/badboysm890/ClaraVerse"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd4op1/clara_browser_based_local_ai_chat_imagegen_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jd4op1/clara_browser_based_local_ai_chat_imagegen_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T04:36:21+00:00</published>
  </entry>
</feed>
