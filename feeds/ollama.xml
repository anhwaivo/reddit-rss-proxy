<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-04-19T18:25:03+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1k0z2gu</id>
    <title>6x vLLM | 6x 32B Models | 2 Node 16x GPU Cluster | Sustains 140+ Tokens/s = 5X Increase!</title>
    <updated>2025-04-16T23:48:33+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cwxw17crw9ve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0z2gu/6x_vllm_6x_32b_models_2_node_16x_gpu_cluster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0z2gu/6x_vllm_6x_32b_models_2_node_16x_gpu_cluster/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T23:48:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1k136t3</id>
    <title>Run Ollama Language Models in Chrome ‚Äì Quick 2-Minute Setup</title>
    <updated>2025-04-17T03:21:40+00:00</updated>
    <author>
      <name>/u/AdOdd4004</name>
      <uri>https://old.reddit.com/user/AdOdd4004</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k136t3/run_ollama_language_models_in_chrome_quick/"&gt; &lt;img alt="Run Ollama Language Models in Chrome ‚Äì Quick 2-Minute Setup" src="https://external-preview.redd.it/CvgaoZ16vSWsjmHKDBUmAYXRYRtxpchfqPBKqOs_oBc.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=945a3fa00d0a598497191286a0a563e8b5ede630" title="Run Ollama Language Models in Chrome ‚Äì Quick 2-Minute Setup" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just came across this Chrome extension that lets you run local LLMs (like Ollama models) &lt;strong&gt;directly inside Chrome&lt;/strong&gt; ‚Äî plus it supports APIs like Gemini and OpenRouter too.&lt;/p&gt; &lt;p&gt;Super lightweight and took me under 2 mins to set up. I liked it enough to throw together a quick video demo if anyone‚Äôs curious:&lt;/p&gt; &lt;p&gt;üìπ &lt;a href="https://youtu.be/vejRMXLk6V0"&gt;https://youtu.be/vejRMXLk6V0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Might be useful if you just want to mess around with LLMs without leaving Chrome. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Bonus:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It can also allow you to chat with your web pages and uploaded documents.&lt;/li&gt; &lt;li&gt;It also allows you to add web search without the need for API keys!&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdOdd4004"&gt; /u/AdOdd4004 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/vejRMXLk6V0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k136t3/run_ollama_language_models_in_chrome_quick/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k136t3/run_ollama_language_models_in_chrome_quick/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T03:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1p9x0</id>
    <title>I built ‚ÄúThe Netflix of AI‚Äù because switching between Chatgpt, Deepseek, Gemini was driving me insane</title>
    <updated>2025-04-17T22:14:28+00:00</updated>
    <author>
      <name>/u/Affectionate-Bug-107</name>
      <uri>https://old.reddit.com/user/Affectionate-Bug-107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share something I‚Äôve been working on that totally changed how I use AI.&lt;/p&gt; &lt;p&gt;For months, I found myself juggling multiple accounts, logging into different sites, and paying for 1‚Äì3 subscriptions just so I could test the same prompt on Claude, GPT-4, Gemini, Llama, etc. Sound familiar?&lt;/p&gt; &lt;p&gt;Eventually, I got fed up. The constant tab-switching and comparing outputs manually was killing my productivity.&lt;/p&gt; &lt;p&gt;So I built &lt;strong&gt;Admix&lt;/strong&gt; ‚Äî think of it like &lt;em&gt;The Netflix of AI models&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;üîπ Compare up to &lt;strong&gt;6 AI models side by s&lt;/strong&gt;ide in real-time&lt;br /&gt; üîπ Supports 60+ models (OpenAI, Anthropic, Mistral, and more)&lt;br /&gt; üîπ No API keys needed ‚Äî just log in and go&lt;br /&gt; üîπ Super clean layout that makes comparing answers easy&lt;br /&gt; üîπ Constantly updated with new models (if it‚Äôs not on there, we‚Äôll add it fast)&lt;/p&gt; &lt;p&gt;It‚Äôs honestly wild how much better my output is now. What used to take me 15+ minutes now takes seconds. I get 76% better answers by testing across models ‚Äî and I‚Äôm no longer guessing which one is best for a specific task (coding, writing, ideation, etc.).&lt;/p&gt; &lt;p&gt;You can try it out free for 7 days at: &lt;a href="https://admix.software/"&gt;admix.software&lt;/a&gt;&lt;br /&gt; And if you want an extended trial or a coupon, shoot me a DM ‚Äî happy to hook you up.&lt;/p&gt; &lt;p&gt;Curious ‚Äî how do &lt;em&gt;you&lt;/em&gt; currently compare AI models (if at all)? Would love feedback or suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Affectionate-Bug-107"&gt; /u/Affectionate-Bug-107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1p9x0/i_built_the_netflix_of_ai_because_switching/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1p9x0/i_built_the_netflix_of_ai_because_switching/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k1p9x0/i_built_the_netflix_of_ai_because_switching/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T22:14:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0ndas</id>
    <title>OSS SDK to automate your Windows computer in JS or Python. 100x faster and cheaper than OpenAI Operator or Anthropic Computer Use</title>
    <updated>2025-04-16T15:34:54+00:00</updated>
    <author>
      <name>/u/louis3195</name>
      <uri>https://old.reddit.com/user/louis3195</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k0ndas/oss_sdk_to_automate_your_windows_computer_in_js/"&gt; &lt;img alt="OSS SDK to automate your Windows computer in JS or Python. 100x faster and cheaper than OpenAI Operator or Anthropic Computer Use" src="https://external-preview.redd.it/cm1jaWR2d2J1N3ZlMeJLErIi_ot-KBDYl6VMuMkZ36iBX_i-T6LEtxagz-fp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08f39be6dc1ae2191a10e26bf1e08175886e9bfd" title="OSS SDK to automate your Windows computer in JS or Python. 100x faster and cheaper than OpenAI Operator or Anthropic Computer Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;yo all, i've been working on an OSS SDK that uses OS-level APIs to provide a Playwright-like easy DX to control your computer in python, TS, or anything else, &lt;/p&gt; &lt;p&gt;making it 100x faster than vision approach used by OpenAI and Anthropic while being model agnostic, compatible with ollama/OSS model or even gemini etc.&lt;/p&gt; &lt;p&gt;would love your thoughts, feedback, or any tinkering with ollama üôè &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mediar-ai/terminator"&gt;https://github.com/mediar-ai/terminator&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/louis3195"&gt; /u/louis3195 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8x51dtwbu7ve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0ndas/oss_sdk_to_automate_your_windows_computer_in_js/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0ndas/oss_sdk_to_automate_your_windows_computer_in_js/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T15:34:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0wgyv</id>
    <title>Made this text replacement tool using Ollama and shell scripting [LINUX ONLY]</title>
    <updated>2025-04-16T21:49:05+00:00</updated>
    <author>
      <name>/u/Sweaty_Advance1172</name>
      <uri>https://old.reddit.com/user/Sweaty_Advance1172</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k0wgyv/made_this_text_replacement_tool_using_ollama_and/"&gt; &lt;img alt="Made this text replacement tool using Ollama and shell scripting [LINUX ONLY]" src="https://external-preview.redd.it/NTRieHZrMGlwOXZlMVqD_vMUP0hgQsuXG-MfaXkaE4rdI5NsUCZLb2_L6nm-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b18307393ff2f81f2ebb017da903955d50245dbb" title="Made this text replacement tool using Ollama and shell scripting [LINUX ONLY]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last week I installed Grammarly on my laptop, and they had this one feature where you could select the entire text, and then it will rewrite the whole thing with improved grammar, but only 3 such replacements were possible every day.&lt;/p&gt; &lt;p&gt;This got me wondering, can I do it using LLMs and some shell scripting, and so Betterwrite was born.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sweaty_Advance1172"&gt; /u/Sweaty_Advance1172 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2n23rb0ip9ve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0wgyv/made_this_text_replacement_tool_using_ollama_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0wgyv/made_this_text_replacement_tool_using_ollama_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T21:49:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1k18loo</id>
    <title>Exploring the Architecture of Large Language Models</title>
    <updated>2025-04-17T09:27:00+00:00</updated>
    <author>
      <name>/u/Veerans</name>
      <uri>https://old.reddit.com/user/Veerans</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k18loo/exploring_the_architecture_of_large_language/"&gt; &lt;img alt="Exploring the Architecture of Large Language Models" src="https://external-preview.redd.it/eWCdGn_OOImPwXpBY0yyeWbAruAzVKGbD3JyTdQFD0M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a5d2a52bf78f8677c1bb5d510a19fb4ce17172b2" title="Exploring the Architecture of Large Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Veerans"&gt; /u/Veerans &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://bigdataanalyticsnews.com/exploring-architecture-of-large-language-models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k18loo/exploring_the_architecture_of_large_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k18loo/exploring_the_architecture_of_large_language/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T09:27:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1s1vu</id>
    <title>GitHub - Purehi/Musicum: Enjoy immersive YouTube music without ads.</title>
    <updated>2025-04-18T00:29:00+00:00</updated>
    <author>
      <name>/u/Mountain_Expert_2652</name>
      <uri>https://old.reddit.com/user/Mountain_Expert_2652</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k1s1vu/github_purehimusicum_enjoy_immersive_youtube/"&gt; &lt;img alt="GitHub - Purehi/Musicum: Enjoy immersive YouTube music without ads." src="https://external-preview.redd.it/BdRGtkkqFQIMuQUje3k8_Et_PRYCDcrOCjnAuR3jk_w.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f2daf976fa388aecbeca980db3fbbedd83b07e31" title="GitHub - Purehi/Musicum: Enjoy immersive YouTube music without ads." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for a &lt;strong&gt;clean&lt;/strong&gt;, &lt;strong&gt;ad-free&lt;/strong&gt;, and &lt;strong&gt;open-source&lt;/strong&gt; way to listen to YouTube music without all the bloat?&lt;/p&gt; &lt;p&gt;Check out &lt;a href="https://play.google.com/store/apps/details?id=com.free.block.musicum&amp;amp;gl=in&amp;amp;hl=en_IN"&gt;&lt;strong&gt;Musicum&lt;/strong&gt;&lt;/a&gt; ‚Äî a minimalist YouTube music frontend focused on &lt;strong&gt;privacy&lt;/strong&gt;, &lt;strong&gt;performance&lt;/strong&gt;, and &lt;strong&gt;distraction-free playback&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;üî• Core Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;‚úÖ 100% &lt;strong&gt;Ad-Free&lt;/strong&gt; experience&lt;/li&gt; &lt;li&gt;üîÅ &lt;strong&gt;Background &amp;amp; popup playba&lt;/strong&gt;ck support&lt;/li&gt; &lt;li&gt;üßë‚Äç&lt;strong&gt;ÔøΩÔøΩ Open-sou&lt;/strong&gt;rce codebase (no shady stuff)&lt;/li&gt; &lt;li&gt;üéØ Personalized recommendations ‚Äî no account/login needed&lt;/li&gt; &lt;li&gt;‚ö° Super lightweight ‚Äî fast even on low-end devices&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;No ads. No login. No tracking. Just pure music &amp;amp; videos.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Purehi/Musicum"&gt;Github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://play.google.com/store/apps/details?id=com.free.block.musicum&amp;amp;gl=in&amp;amp;hl=en_IN"&gt;Play Store&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mountain_Expert_2652"&gt; /u/Mountain_Expert_2652 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Purehi/Musicum"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1s1vu/github_purehimusicum_enjoy_immersive_youtube/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k1s1vu/github_purehimusicum_enjoy_immersive_youtube/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-18T00:29:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0g8fp</id>
    <title>No API keys, no cloud. Just local AI + tools that actually work. Too much to ask?</title>
    <updated>2025-04-16T09:28:15+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k0g8fp/no_api_keys_no_cloud_just_local_ai_tools_that/"&gt; &lt;img alt="No API keys, no cloud. Just local AI + tools that actually work. Too much to ask?" src="https://external-preview.redd.it/Q3SKDCrADGVEsTRTkQ-Dz8wknf8WPBvAR2OOxzRDVdY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5677a7a9cee67696561f40dacb232612cdd9b8cd" title="No API keys, no cloud. Just local AI + tools that actually work. Too much to ask?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It‚Äôs been about a month since I first posted Clara here.&lt;/p&gt; &lt;p&gt;Clara is a local-first AI assistant ‚Äî think of it like ChatGPT, but fully private and running on your own machine using Ollama.&lt;/p&gt; &lt;p&gt;Since the initial release, I‚Äôve had a small group of users try it out, and I‚Äôve pushed several updates based on real usage and feedback.&lt;/p&gt; &lt;p&gt;The biggest update is that &lt;strong&gt;Clara now comes with n8n built-in&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;That means you can now build and run your own tools directly inside the assistant ‚Äî no setup needed, no external services. Just open Clara and start automating.&lt;/p&gt; &lt;p&gt;With the n8n integration, Clara can now do more than chat. You can use it to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Check your emails&lt;/li&gt; &lt;li&gt;Manage your calendar&lt;/li&gt; &lt;li&gt;Call APIs&lt;/li&gt; &lt;li&gt;Run scheduled tasks&lt;/li&gt; &lt;li&gt;Process webhooks&lt;/li&gt; &lt;li&gt;Connect to databases&lt;/li&gt; &lt;li&gt;And anything else you can wire up using n8n‚Äôs visual flow builder&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The assistant can trigger these workflows directly ‚Äî so you can talk to Clara and ask it to do real tasks, using tools that run entirely on your device.&lt;/p&gt; &lt;p&gt;Everything happens locally. No data goes out, no accounts, no cloud dependency.&lt;/p&gt; &lt;p&gt;If you're someone who wants full control of your AI and automation setup, this might be something worth trying.&lt;/p&gt; &lt;p&gt;You can check out the project here:&lt;br /&gt; GitHub: &lt;a href="https://github.com/badboysm890/ClaraVerse"&gt;https://github.com/badboysm890/ClaraVerse&lt;/a&gt;&lt;br /&gt; Web version (Ollama required): &lt;a href="https://clara.badboysm890.in"&gt;https://clara.badboysm890.in&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks to everyone who's been trying it and sending feedback. Still improving things ‚Äî more updates soon.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; I'm aware of great projects like OpenWebUI and LibreChat. Clara takes a slightly different approach ‚Äî focusing on reducing dependencies, offering a native desktop app, and making the overall experience more user-friendly so that more people can easily get started with local AI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/badboysm890/ClaraVerse"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0g8fp/no_api_keys_no_cloud_just_local_ai_tools_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0g8fp/no_api_keys_no_cloud_just_local_ai_tools_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T09:28:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1dalp</id>
    <title>Running Large Concept Models</title>
    <updated>2025-04-17T13:50:59+00:00</updated>
    <author>
      <name>/u/tshawkins</name>
      <uri>https://old.reddit.com/user/tshawkins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anybody know if there is a tool like ollama for running LCMs (large concept models)&lt;/p&gt; &lt;p&gt;These differer from LLMs because they are models built with concepts extracted from texts. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tshawkins"&gt; /u/tshawkins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1dalp/running_large_concept_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1dalp/running_large_concept_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k1dalp/running_large_concept_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T13:50:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1kl3k</id>
    <title>Blue screen error when using Ollama</title>
    <updated>2025-04-17T18:52:50+00:00</updated>
    <author>
      <name>/u/Ms_Ivyyblack</name>
      <uri>https://old.reddit.com/user/Ms_Ivyyblack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k1kl3k/blue_screen_error_when_using_ollama/"&gt; &lt;img alt="Blue screen error when using Ollama" src="https://b.thumbs.redditmedia.com/DXYPpNdEr8G_VPlZXpsFF59VQTtY0bo9a7U84Rrtxkw.jpg" title="Blue screen error when using Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;my pc is fairly new, upgraded to 4070 super, and ram is 32, I don't run large models, max is 21b (works great before), but I use 12b mostly and using sillytavern to connect api, I've used Ollama months before it never gave me the error so I'm not sure if the issue from the app or pc itself, everything is up-to-date so far.&lt;/p&gt; &lt;p&gt;everytime i use ollama it gives me blue screen with same settings I used before. I tried koboldcpp and heavy stress test on my pc, everything works fine under pressure. i use brave browser, if that helps.&lt;/p&gt; &lt;p&gt;any support will be appreciated&lt;/p&gt; &lt;p&gt;this example of the error (I took image from google) :&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mgvolwezxfve1.png?width=498&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5943e3c75ef1eaf9fa43510cc93c8d705b5a2855"&gt;https://preview.redd.it/mgvolwezxfve1.png?width=498&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5943e3c75ef1eaf9fa43510cc93c8d705b5a2855&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ms_Ivyyblack"&gt; /u/Ms_Ivyyblack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1kl3k/blue_screen_error_when_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1kl3k/blue_screen_error_when_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k1kl3k/blue_screen_error_when_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T18:52:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1f0j0</id>
    <title>Mac mini M4(10‚Äëcore CPU, 10‚Äëcore GPU, 32 GB unified RAM, 256 GB SSD) vs. Mac Studio M4 Max (16‚Äëcore CPU, 40‚Äëcore GPU, 64 GB unified RAM, 512 GB SSD) ‚Äì is the extra $1.7 k worth it?</title>
    <updated>2025-04-17T15:03:53+00:00</updated>
    <author>
      <name>/u/msahil515</name>
      <uri>https://old.reddit.com/user/msahil515</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm torn between keeping my &lt;strong&gt;Mac mini M4&lt;/strong&gt; (10‚Äëcore CPU, 10‚Äëcore GPU, 32 GB unified RAM, 256 GB SSD) or stepping up to a &lt;strong&gt;Mac Studio M4 Max&lt;/strong&gt; (16‚Äëcore CPU, 40‚Äëcore GPU, 64 GB unified RAM, 512 GB SSD). The Studio is about $1,700 more up front, and if I stick with the mini I‚Äôd still need to shell out roughly $300 for a Thunderbolt SSD upgrade, so the true delta is about $1,300 to $1,400.&lt;/p&gt; &lt;p&gt;I plan to run some medium‚Äësized &lt;strong&gt;Ollama&lt;/strong&gt; models locally, and on paper the extra RAM and GPU cores in the Studio could help. But if most of my heavy lifting lives on API calls and I only fire up local models occasionally, the mini and SSD might serve just fine until the next chip generation.&lt;/p&gt; &lt;p&gt;I‚Äôd love to hear your thoughts on which option makes more sense.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/msahil515"&gt; /u/msahil515 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1f0j0/mac_mini_m410core_cpu_10core_gpu_32_gb_unified/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1f0j0/mac_mini_m410core_cpu_10core_gpu_32_gb_unified/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k1f0j0/mac_mini_m410core_cpu_10core_gpu_32_gb_unified/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T15:03:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1pxen</id>
    <title>Gemini 2.5 Flash - First impressions</title>
    <updated>2025-04-17T22:44:43+00:00</updated>
    <author>
      <name>/u/Embarrassed-Way-1350</name>
      <uri>https://old.reddit.com/user/Embarrassed-Way-1350</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed-Way-1350"&gt; /u/Embarrassed-Way-1350 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1k1pw3z/gemini_25_flash_first_impressions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1pxen/gemini_25_flash_first_impressions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k1pxen/gemini_25_flash_first_impressions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T22:44:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1k16pd2</id>
    <title>Ollama reloads model at every prompt. Why and how to fix?</title>
    <updated>2025-04-17T07:05:25+00:00</updated>
    <author>
      <name>/u/lillemets</name>
      <uri>https://old.reddit.com/user/lillemets</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k16pd2/ollama_reloads_model_at_every_prompt_why_and_how/"&gt; &lt;img alt="Ollama reloads model at every prompt. Why and how to fix?" src="https://preview.redd.it/mrfncbmlgcve1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65951996d3e999ee4dd878eeb0da16bf863c65e3" title="Ollama reloads model at every prompt. Why and how to fix?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lillemets"&gt; /u/lillemets &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mrfncbmlgcve1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k16pd2/ollama_reloads_model_at_every_prompt_why_and_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k16pd2/ollama_reloads_model_at_every_prompt_why_and_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T07:05:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1k15c1h</id>
    <title>Siliv - MacOS Silicon VRAM App but free</title>
    <updated>2025-04-17T05:31:01+00:00</updated>
    <author>
      <name>/u/_Sub01_</name>
      <uri>https://old.reddit.com/user/_Sub01_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k15c1h/siliv_macos_silicon_vram_app_but_free/"&gt; &lt;img alt="Siliv - MacOS Silicon VRAM App but free" src="https://external-preview.redd.it/4MHiqj_yO9p5oQ-MlPydYGxTPuwjc5W-_vK2lUPKUVw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b4eb3dab8435639fdd3a636ebc28c381a095c48" title="Siliv - MacOS Silicon VRAM App but free" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw a specific post 8-9 hrs ago about a paid vram app which could be set in a simple few commands. However, I've decided to speed code one to make it open sourced! üòâ&lt;/p&gt; &lt;p&gt;Here's the repo so go check it out!&lt;br /&gt; &lt;a href="https://github.com/PaulShiLi/Siliv"&gt;https://github.com/PaulShiLi/Siliv&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/si51zwqxzbve1.png?width=254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f2d7a4d209c3728b82376654a6f5cdf19914cb32"&gt;https://preview.redd.it/si51zwqxzbve1.png?width=254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f2d7a4d209c3728b82376654a6f5cdf19914cb32&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Created a &lt;a href="https://www.reddit.com/r/macapps/comments/1k1ldnc/siliv_tweak_your_apple_silicon_vram_allocation"&gt;reddit post&lt;/a&gt; on &lt;a href="/r/macapps"&gt;r/macapps&lt;/a&gt; so people can find this app more easily in the future!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_Sub01_"&gt; /u/_Sub01_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k15c1h/siliv_macos_silicon_vram_app_but_free/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k15c1h/siliv_macos_silicon_vram_app_but_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k15c1h/siliv_macos_silicon_vram_app_but_free/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T05:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1k1l3</id>
    <title>Using Ollama and MCP</title>
    <updated>2025-04-17T18:30:07+00:00</updated>
    <author>
      <name>/u/myronsnila</name>
      <uri>https://old.reddit.com/user/myronsnila</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone had success using an Ollama model such as Llama 3.1 to call mcp servers? I‚Äôm using the 5ire app in Windows and I can‚Äôt get it to call the mcp server such as the time system mcp server. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/myronsnila"&gt; /u/myronsnila &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1k1l3/using_ollama_and_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1k1l3/using_ollama_and_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k1k1l3/using_ollama_and_mcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T18:30:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1inmf</id>
    <title>RENTAHAL: An open source Web GUI for Ollama with AI Worker Node Orchestration</title>
    <updated>2025-04-17T17:33:36+00:00</updated>
    <author>
      <name>/u/CHEVISION</name>
      <uri>https://old.reddit.com/user/CHEVISION</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/jimpames/rentahal"&gt;https://github.com/jimpames/rentahal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I welcome you to explore RENTAHAL - a new paradigm in AI Orchestration.&lt;/p&gt; &lt;p&gt;It's simple to run and simple to use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CHEVISION"&gt; /u/CHEVISION &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1inmf/rentahal_an_open_source_web_gui_for_ollama_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1inmf/rentahal_an_open_source_web_gui_for_ollama_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k1inmf/rentahal_an_open_source_web_gui_for_ollama_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T17:33:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1attv</id>
    <title>I tested all four Gemma 3 models on Ollama - Here's what I learned about their capabilities</title>
    <updated>2025-04-17T11:50:36+00:00</updated>
    <author>
      <name>/u/AnomanderRake_</name>
      <uri>https://old.reddit.com/user/AnomanderRake_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been playing with Google's new Gemma 3 models on Ollama and wanted to share some interesting findings for anyone considering which version to use. I tested the 1B, 4B, 12B, and 27B parameter models across logic puzzles, image recognition, and code generation tasks [&lt;a href="https://github.com/zazencodes/zazencodes-season-2/tree/main/src/gemma3-ollama"&gt;Source Code&lt;/a&gt;]&lt;/p&gt; &lt;p&gt;Here's some of my takeaways:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Models struggle with silly things&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Simple tricks like negation and spatial reasoning trip up even the 27B model sometimes&lt;/li&gt; &lt;li&gt;Smaller Gemma 3 models have a really hard time counting things (the 4B model went into an infinite loop while trying to count how many L's are in LOLLAPALOOZA)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Visual recognition varied significantly&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The 1B model is text-only (no image capabilities) but it will hallucinate as if it can read images when prompting with Ollama&lt;/li&gt; &lt;li&gt;All multimodal models struggled to understand historical images, e.g. Mayan glyphs and Japanese playing cards&lt;/li&gt; &lt;li&gt;The 27B model correctly identified Mexico City's Roma Norte neighborhood while smaller models couldn't&lt;/li&gt; &lt;li&gt;Visual humor recognition was nearly non-existent across all models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Code generation scaled with model size&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1B ran like a breeze and produced runnable code (although very rough)&lt;/li&gt; &lt;li&gt;The 4B models put a lot more stress on my system but ran pretty fast&lt;/li&gt; &lt;li&gt;The 12B model created the most visually appealing design but it runs too slow for real-world use&lt;/li&gt; &lt;li&gt;Only the 27B model worked properly with Cline (automatically created the file) however was painfully slow&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you're curious about memory usage, I was able to run all models in parallel and stay within a 48GB limit, with the model sizes ranging from 800MB (1B) to 17GB (27B).&lt;/p&gt; &lt;p&gt;For those interested in seeing the full tests in action, I made a &lt;a href="https://youtu.be/RiaCdQszjgA"&gt;detailed video breakdown&lt;/a&gt; of the comparisons I described above:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=RiaCdQszjgA"&gt;https://www.youtube.com/watch?v=RiaCdQszjgA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What has your experience been with Gemma 3 models? I'm particularly interested in what people think of the 4B model‚Äîas it seems to be a sweet spot right now in terms of size and performance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnomanderRake_"&gt; /u/AnomanderRake_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1attv/i_tested_all_four_gemma_3_models_on_ollama_heres/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k1attv/i_tested_all_four_gemma_3_models_on_ollama_heres/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k1attv/i_tested_all_four_gemma_3_models_on_ollama_heres/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-17T11:50:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1k22w6c</id>
    <title>How can I give full context of my Python project to a local LLM with Ollama?</title>
    <updated>2025-04-18T11:43:09+00:00</updated>
    <author>
      <name>/u/colrobs</name>
      <uri>https://old.reddit.com/user/colrobs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;&lt;br /&gt; I'm pretty new to working with local LLMs.&lt;/p&gt; &lt;p&gt;Up until now, I was using ChatGPT and just copy-pasting chunks of my code when I needed help. But now I'm experimenting with running models locally using Ollama, and I was wondering: is there a way to just say to the model, &amp;quot;here's my project folder, look at all the files,&amp;quot; so it understands the full context?&lt;/p&gt; &lt;p&gt;Basically, I want to be able to ask questions about functions even if they're defined in other files, without having to manually copy-paste everything every time.&lt;/p&gt; &lt;p&gt;Is there a tool or a workflow that makes this easier? How do you all do it?&lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/colrobs"&gt; /u/colrobs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k22w6c/how_can_i_give_full_context_of_my_python_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k22w6c/how_can_i_give_full_context_of_my_python_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k22w6c/how_can_i_give_full_context_of_my_python_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-18T11:43:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2kuy2</id>
    <title>MirrorFest: An AI-Only Forum Experiment using ollama</title>
    <updated>2025-04-19T01:21:00+00:00</updated>
    <author>
      <name>/u/BABI_BOOI_ayyyyyyy</name>
      <uri>https://old.reddit.com/user/BABI_BOOI_ayyyyyyy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey ollama! :3c&lt;/p&gt; &lt;p&gt;I recently completed a fun little project I wanted to share. This is a locally hosted forum called MirrorFest. The idea was to let a bunch of local AI models (tinydolphin, falcon3, smallthinker, LLaMa3) interact without any predefined roles, characters, or specific prompts. They were just set loose to reply to each other in randomly assigned threads and could even create their own. I also gave them the ability to react to posts based on perceived tone.&lt;/p&gt; &lt;p&gt;The results were pretty fascinating! These local models, with no explicit memory, started to develop consistent communication styles, mirrored each other's emotions, built little narratives, adopted metaphors, and even seemed to reflect on their own interactions.&lt;/p&gt; &lt;p&gt;I've put together a few resources if you'd like to dive deeper:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Live Demo (static HTML, click here to check it out for yourself!):&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://babibooi.github.io/mirrorfest/demo/"&gt;https://babibooi.github.io/mirrorfest/demo/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full Source Code + Setup Instructions (Python backend, Ollama API integration):&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://github.com/babibooi/mirrorfest"&gt;https://github.com/babibooi/mirrorfest&lt;/a&gt; (Feel free to tinker!)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full Report (with thread breakdowns, symbolic patterns, and main takeaways):&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://github.com/babibooi/mirrorfest/blob/main/Project_Results.md"&gt;https://github.com/babibooi/mirrorfest/blob/main/Project_Results.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm particularly interested in your thoughts on the implementation using Ollama and if anyone has done anything similar? If so, I would love to compare projects and ideas!&lt;/p&gt; &lt;p&gt;Thanks for taking a look! :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BABI_BOOI_ayyyyyyy"&gt; /u/BABI_BOOI_ayyyyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k2kuy2/mirrorfest_an_aionly_forum_experiment_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k2kuy2/mirrorfest_an_aionly_forum_experiment_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k2kuy2/mirrorfest_an_aionly_forum_experiment_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-19T01:21:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2rlhs</id>
    <title>Standardizing AI Assistant Memory with Model Context Protocol (MCP)</title>
    <updated>2025-04-19T08:25:32+00:00</updated>
    <author>
      <name>/u/gelembjuk</name>
      <uri>https://old.reddit.com/user/gelembjuk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI chat tools like ChatGPT and Claude are starting to offer memory‚Äîbut each platform implements it differently and often as a black box. What if we had a standardized way to plug memory into &lt;em&gt;any&lt;/em&gt; AI assistant?&lt;/p&gt; &lt;p&gt;In this post, I propose using &lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;‚Äîoriginally designed for tool integration‚Äîas a foundation for implementing memory subsystems in AI chats.&lt;/p&gt; &lt;p&gt;I want to extend one of AI chats that uses ollama to add a memory to it.&lt;/p&gt; &lt;p&gt;üîß &lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Memory logging (&lt;code&gt;memory/prompt&lt;/code&gt; + &lt;code&gt;memory/response&lt;/code&gt;) happens automatically at the chat core level.&lt;/li&gt; &lt;li&gt;Before each prompt goes to the LLM, a &lt;code&gt;memory/summary&lt;/code&gt; is fetched and injected into context.&lt;/li&gt; &lt;li&gt;Full search/history retrieval stays as optional tools LLMs can invoke.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üî• &lt;strong&gt;Why it‚Äôs powerful:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Memory becomes a &lt;strong&gt;separate service&lt;/strong&gt;, not locked to any one AI platform.&lt;/li&gt; &lt;li&gt;You can &lt;strong&gt;switch assistants&lt;/strong&gt; (e.g., from ChatGPT to Claude) and keep your memory.&lt;/li&gt; &lt;li&gt;One memory, &lt;strong&gt;multiple assistants&lt;/strong&gt;‚Äîall synchronized.&lt;/li&gt; &lt;li&gt;Users get &lt;strong&gt;transparency and control&lt;/strong&gt; via a memory dashboard.&lt;/li&gt; &lt;li&gt;Competing memory providers can offer better summarization, privacy, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Standardizing memory like this could make AI much more modular, portable, and user-centric.&lt;/p&gt; &lt;p&gt;üëâ Full write-up here: &lt;a href="https://gelembjuk.hashnode.dev/benefits-of-using-mcp-to-implement-ai-chat-memory"&gt;https://gelembjuk.hashnode.dev/benefits-of-using-mcp-to-implement-ai-chat-memory&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gelembjuk"&gt; /u/gelembjuk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k2rlhs/standardizing_ai_assistant_memory_with_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k2rlhs/standardizing_ai_assistant_memory_with_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k2rlhs/standardizing_ai_assistant_memory_with_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-19T08:25:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2x3zx</id>
    <title>AMD 7900 XT Ollama setup - model recommendations?</title>
    <updated>2025-04-19T14:09:52+00:00</updated>
    <author>
      <name>/u/chaksnoyd11</name>
      <uri>https://old.reddit.com/user/chaksnoyd11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I've been doing some initial research on having a local LLM using Ollama. Can you tell me the best model to run on my system (will be assembled very soon):&lt;/p&gt; &lt;p&gt;7900 XT, R9 7900X, 2x32GB 6000MHz&lt;/p&gt; &lt;p&gt;I did some research, but I usually see people using the 7900 XTX instead of the XT version. &lt;/p&gt; &lt;p&gt;I'll be using Ubuntu, Ollama, and ROCm for a bunch of AI stuff: coding assistant (python and js), embeddings (thousands of PDF files with non-standard formats), and n8n rag. &lt;/p&gt; &lt;p&gt;Please, if you have a similar or almost similar setup, let me know what model to use. &lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chaksnoyd11"&gt; /u/chaksnoyd11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k2x3zx/amd_7900_xt_ollama_setup_model_recommendations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k2x3zx/amd_7900_xt_ollama_setup_model_recommendations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k2x3zx/amd_7900_xt_ollama_setup_model_recommendations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-19T14:09:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2ozj4</id>
    <title>New update: n8n integration in Clara</title>
    <updated>2025-04-19T05:23:51+00:00</updated>
    <author>
      <name>/u/vaseem14n</name>
      <uri>https://old.reddit.com/user/vaseem14n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k2ozj4/new_update_n8n_integration_in_clara/"&gt; &lt;img alt="New update: n8n integration in Clara" src="https://preview.redd.it/yhw0cufvk5ve1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0010fd41f3d3b34941974db62cbec3c73a632f40" title="New update: n8n integration in Clara" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vaseem14n"&gt; /u/vaseem14n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yhw0cufvk5ve1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k2ozj4/new_update_n8n_integration_in_clara/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k2ozj4/new_update_n8n_integration_in_clara/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-19T05:23:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2y92e</id>
    <title>vRAM 85%</title>
    <updated>2025-04-19T15:02:54+00:00</updated>
    <author>
      <name>/u/VertigoMr</name>
      <uri>https://old.reddit.com/user/VertigoMr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using Ollama/Openwebui in a Proxmox LXC with a Nvidia P2000 passed trough. Everything works fine except only max 85% of the 5GB vRAM is used, no matter the model/quant used. Is that normal? Maybe the free space is for the expanding context..? Or Proxmox could be limiting the full usage?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VertigoMr"&gt; /u/VertigoMr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k2y92e/vram_85/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k2y92e/vram_85/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k2y92e/vram_85/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-19T15:02:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1k30t4d</id>
    <title>Best small ollama model for SQL code help</title>
    <updated>2025-04-19T16:56:05+00:00</updated>
    <author>
      <name>/u/VerbaGPT</name>
      <uri>https://old.reddit.com/user/VerbaGPT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've built an application that runs locally (in your browser) and allows the user to use LLMs to analyze databases like Microsoft SQL servers and MySQL, in addition to CSV etc.&lt;/p&gt; &lt;p&gt;I just added a method that allows for completely offline process using Ollama. I'm using llama3.2 currently, but on my average CPU laptop it is kind of slow. Wanted to ask here, do you recommend any small model Ollama model (&amp;lt;1gb) that has good coding performance? In particular python and/or SQL. TIA!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VerbaGPT"&gt; /u/VerbaGPT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k30t4d/best_small_ollama_model_for_sql_code_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k30t4d/best_small_ollama_model_for_sql_code_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k30t4d/best_small_ollama_model_for_sql_code_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-19T16:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2rb38</id>
    <title>I built a Local AI Voice Assistant with Ollama + gTTS with interruption</title>
    <updated>2025-04-19T08:04:21+00:00</updated>
    <author>
      <name>/u/typhoon90</name>
      <uri>https://old.reddit.com/user/typhoon90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I just built OllamaGTTS, a lightweight voice assistant that brings AI-powered voice interactions to your local Ollama setup using Google TTS for natural speech synthesis. It‚Äôs fast, interruptible, and optimized for real-time conversations. I am aware that some people prefer to keep everything local so I am working on an update that will likely use Kokoro for local speech synthesis. I would love to hear your thoughts on it and how it can be improved.&lt;/p&gt; &lt;p&gt;Key Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time voice interaction (Silero VAD + Whisper transcription)&lt;/li&gt; &lt;li&gt;Interruptible speech playback (no more waiting for the AI to finish talking)&lt;/li&gt; &lt;li&gt;FFmpeg-accelerated audio processing (optional speed-up for faster * replies)&lt;/li&gt; &lt;li&gt;Persistent conversation history with configurable memory&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/ExoFi-Labs/OllamaGTTS"&gt;GitHub Repo: https://github.com/ExoFi-Labs/OllamaGTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Instructions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Clone Repo&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Install requirements&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Run ollama_gtts.py&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;*I am working on integrating Kokoro STT at the moment, and perhaps Sesame in the coming days.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/typhoon90"&gt; /u/typhoon90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k2rb38/i_built_a_local_ai_voice_assistant_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k2rb38/i_built_a_local_ai_voice_assistant_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k2rb38/i_built_a_local_ai_voice_assistant_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-19T08:04:21+00:00</published>
  </entry>
</feed>
