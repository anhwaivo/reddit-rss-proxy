<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-21T09:34:47+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1i48lo8</id>
    <title>OLLAMA_MAX_QUEUE and OLLAMA_NUM_PARALLEL</title>
    <updated>2025-01-18T14:25:24+00:00</updated>
    <author>
      <name>/u/Comfortable_Bend_819</name>
      <uri>https://old.reddit.com/user/Comfortable_Bend_819</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I can't seem to find specifics about these.&lt;/p&gt; &lt;p&gt;OLLAMA_MAX_QUEUE and OLLAMA_NUM_PARALLEL. I know what the defaults are, but I don't know what is reasonable.&lt;/p&gt; &lt;p&gt;What are the &amp;quot;guidelines&amp;quot; for these settings? What impacts them and what do they impact? &lt;/p&gt; &lt;p&gt;We have a number of Ollama servers sitting behind a proxy, some have a single GPU, some have multiple GPUs. They all serve the same single model. It would be GREAT to span the work across the multiple GPUs on those machines, but I am not sure this is going to happen, no matter the settings.&lt;/p&gt; &lt;p&gt;So my questions are: what determines the best settings for these? I need to get as much out of these machines as possible, and I think currently we aren't even close to this. Is it Vram, the GPU itself, machine RAM? I have no idea.&lt;/p&gt; &lt;p&gt;And yes, these things are being hammered with constant traffic. Thanks so much. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable_Bend_819"&gt; /u/Comfortable_Bend_819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i48lo8/ollama_max_queue_and_ollama_num_parallel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i48lo8/ollama_max_queue_and_ollama_num_parallel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i48lo8/ollama_max_queue_and_ollama_num_parallel/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-18T14:25:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4kace</id>
    <title>Conda - Ollama</title>
    <updated>2025-01-18T23:20:57+00:00</updated>
    <author>
      <name>/u/No-Structure4733</name>
      <uri>https://old.reddit.com/user/No-Structure4733</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Noob question: I am trying to play with Ollama and want to install it into my anaconda environment. Can someone tell me if the following is a genuine package from ollama&lt;/p&gt; &lt;p&gt;&lt;a href="https://anaconda.org/conda-forge/ollama-python"&gt;https://anaconda.org/conda-forge/ollama-python&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just trying to make sure I am not downloading anything malicious onto my system&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Structure4733"&gt; /u/No-Structure4733 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i4kace/conda_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i4kace/conda_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i4kace/conda_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-18T23:20:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4f0oa</id>
    <title>My Biggest Accomplishment So Far.</title>
    <updated>2025-01-18T19:17:55+00:00</updated>
    <author>
      <name>/u/Darkroomist</name>
      <uri>https://old.reddit.com/user/Darkroomist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i4f0oa/my_biggest_accomplishment_so_far/"&gt; &lt;img alt="My Biggest Accomplishment So Far." src="https://preview.redd.it/868a015ussde1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d5f6b9da38c2a9f0fb58df175024bd5be29f7089" title="My Biggest Accomplishment So Far." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Darkroomist"&gt; /u/Darkroomist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/868a015ussde1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i4f0oa/my_biggest_accomplishment_so_far/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i4f0oa/my_biggest_accomplishment_so_far/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-18T19:17:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4b9b0</id>
    <title>I run small stuff on my RPi5 and find it impressive</title>
    <updated>2025-01-18T16:31:19+00:00</updated>
    <author>
      <name>/u/tecneeq</name>
      <uri>https://old.reddit.com/user/tecneeq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i4b9b0/i_run_small_stuff_on_my_rpi5_and_find_it/"&gt; &lt;img alt="I run small stuff on my RPi5 and find it impressive" src="https://b.thumbs.redditmedia.com/HQgtPhc1taUU7ChSounCT2taAC0UuQEp5UJzfphzbJI.jpg" title="I run small stuff on my RPi5 and find it impressive" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/56mcrgvk4sde1.png?width=1443&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=056b91b2b1d8743b4083928b4e9ed927c35adf1b"&gt;https://preview.redd.it/56mcrgvk4sde1.png?width=1443&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=056b91b2b1d8743b4083928b4e9ed927c35adf1b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tecneeq"&gt; /u/tecneeq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i4b9b0/i_run_small_stuff_on_my_rpi5_and_find_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i4b9b0/i_run_small_stuff_on_my_rpi5_and_find_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i4b9b0/i_run_small_stuff_on_my_rpi5_and_find_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-18T16:31:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5cb76</id>
    <title>ollama is running locally or remotely?</title>
    <updated>2025-01-19T23:26:14+00:00</updated>
    <author>
      <name>/u/Fluid_Task9506</name>
      <uri>https://old.reddit.com/user/Fluid_Task9506</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I asked ollama (locally) the following question: &amp;quot;&lt;strong&gt;are those prompts in the meta servers or locally&lt;/strong&gt;?&amp;quot;. The answer was &lt;/p&gt; &lt;p&gt;&lt;em&gt;&amp;quot;The prompts I respond to are stored and processed on remote servers, often referred&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;to as &amp;quot;cloud&amp;quot; services. These servers are typically managed by the company that&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;developed the AI model I'm based on (in this case, Meta AI).&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;When you interact with me, your input is sent to these remote servers, where it's&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;processed and analyzed using complex algorithms and natural language processing&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;techniques. The responses I generate are then sent back to your device from those&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;servers.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;So, to answer your question, the prompts are not stored locally on your device or&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;mine (since I'm a cloud-based service). Instead, they're stored and processed&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;remotely on Meta's servers.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I then asked, &lt;strong&gt;&amp;quot;i am not connected to the internet now, how can you answer then?&amp;quot;&lt;/strong&gt; to which it replied:&lt;/p&gt; &lt;p&gt;&lt;em&gt;&amp;quot;I'm a large language model, I don't require an active internet connection to&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;function in real-time. However, my training data is based on a massive dataset that&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;was created by leveraging the internet.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;When you interact with me earlier, your device (e.g., laptop, mobile phone) sent a&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;request to Meta's servers, which then responded with my answers. But since you're&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;not connected to the internet now, it's not possible for me to receive new requests&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;or send responses in real-time.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;So, how can I answer your question? The truth is, \&lt;/em&gt;*I'm not actually answering your* &lt;/p&gt; &lt;p&gt;&lt;em&gt;question in real-time\&lt;/em&gt;*. My previous response was likely cached or stored locally on* &lt;/p&gt; &lt;p&gt;&lt;em&gt;your device.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;When you initially interacted with me, the conversation was facilitated through&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;Meta's servers. However, once the initial connection was established, some of my&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;responses might have been cached or stored locally on your device to improve&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;performance and reduce latency.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Now that you're offline, it's possible that your device is using this cached data to&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;display my previous responses, including the one I provided earlier. This means that&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;while I'm not actually &amp;quot;answering&amp;quot; your question in real-time, my pre-computed&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;responses are still being displayed on your device.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Keep in mind that this caching mechanism has limitations, and you won't be able to&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;receive new or updated responses from me until you're reconnected to the internet.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I suppose it's wrong because it's contradicting the official documentation. Correct?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluid_Task9506"&gt; /u/Fluid_Task9506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5cb76/ollama_is_running_locally_or_remotely/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5cb76/ollama_is_running_locally_or_remotely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i5cb76/ollama_is_running_locally_or_remotely/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-19T23:26:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4hk9y</id>
    <title>LLama Index Documentation Assistant with phi4 and ollama</title>
    <updated>2025-01-18T21:13:24+00:00</updated>
    <author>
      <name>/u/oridnary_artist</name>
      <uri>https://old.reddit.com/user/oridnary_artist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i4hk9y/llama_index_documentation_assistant_with_phi4_and/"&gt; &lt;img alt="LLama Index Documentation Assistant with phi4 and ollama" src="https://external-preview.redd.it/MzFoNmV2N3ppdGRlMQpxhBzZcKM0vjh7QfEh6AQM_565qKsHsYnJbi1X94T-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1007630a8f41c9e23a085b997da0f3bc2e7d0f71" title="LLama Index Documentation Assistant with phi4 and ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oridnary_artist"&gt; /u/oridnary_artist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0dd2tv7zitde1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i4hk9y/llama_index_documentation_assistant_with_phi4_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i4hk9y/llama_index_documentation_assistant_with_phi4_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-18T21:13:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4ye4x</id>
    <title>Is my B580 working?</title>
    <updated>2025-01-19T13:26:46+00:00</updated>
    <author>
      <name>/u/Ejo2001</name>
      <uri>https://old.reddit.com/user/Ejo2001</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ejo2001"&gt; /u/Ejo2001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/IntelArc/comments/1i4ydu8/is_my_b580_working/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i4ye4x/is_my_b580_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i4ye4x/is_my_b580_working/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-19T13:26:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4cttd</id>
    <title>PHI-4: Quantized Q4 vs Q8 on My Nvidia RTX 3060 12GB System</title>
    <updated>2025-01-18T17:40:41+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i4cttd/phi4_quantized_q4_vs_q8_on_my_nvidia_rtx_3060/"&gt; &lt;img alt="PHI-4: Quantized Q4 vs Q8 on My Nvidia RTX 3060 12GB System" src="https://external-preview.redd.it/xOIjwljoqcqAtLtEZzFIsx3i8OS8PGRXeXH3XVBboAw.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=752b0929d7ac91161885bfb8583051422a20f9d3" title="PHI-4: Quantized Q4 vs Q8 on My Nvidia RTX 3060 12GB System" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/mxNEQ53K7QQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i4cttd/phi4_quantized_q4_vs_q8_on_my_nvidia_rtx_3060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i4cttd/phi4_quantized_q4_vs_q8_on_my_nvidia_rtx_3060/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-18T17:40:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i54pqv</id>
    <title>Need help choosing/fine-tuning LLM for structured HTML content extraction to JSON</title>
    <updated>2025-01-19T18:08:01+00:00</updated>
    <author>
      <name>/u/KledMainSG</name>
      <uri>https://old.reddit.com/user/KledMainSG</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! 👋 I'm working on a project to extract structured content from HTML pages into JSON, and I'm running into issues with Mistral via Ollama. Here's what I'm trying to do:&lt;/p&gt; &lt;p&gt;I have HTML pages with various sections, lists, and text content that I want to extract into a clean, structured JSON format. Currently using Crawl4AI with Mistral, but getting inconsistent results - sometimes it just repeats my instructions back, other times gives partial data.&lt;/p&gt; &lt;p&gt;Here's my current setup (simplified):&lt;br /&gt; ```&lt;br /&gt; import asyncio&lt;/p&gt; &lt;p&gt;from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig&lt;/p&gt; &lt;p&gt;from crawl4ai.extraction_strategy import LLMExtractionStrategy&lt;/p&gt; &lt;p&gt;async def extract_structured_content():&lt;/p&gt; &lt;p&gt;strategy = LLMExtractionStrategy(&lt;/p&gt; &lt;p&gt;provider=&amp;quot;ollama/mistral&amp;quot;,&lt;/p&gt; &lt;p&gt;api_token=&amp;quot;no-token&amp;quot;,&lt;/p&gt; &lt;p&gt;extraction_type=&amp;quot;block&amp;quot;,&lt;/p&gt; &lt;p&gt;chunk_token_threshold=2000,&lt;/p&gt; &lt;p&gt;overlap_rate=0.1,&lt;/p&gt; &lt;p&gt;apply_chunking=True,&lt;/p&gt; &lt;p&gt;extra_args={&lt;/p&gt; &lt;p&gt;&amp;quot;temperature&amp;quot;: 0.0,&lt;/p&gt; &lt;p&gt;&amp;quot;timeout&amp;quot;: 300&lt;/p&gt; &lt;p&gt;},&lt;/p&gt; &lt;p&gt;instruction=&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;Convert this HTML content into a structured JSON object.&lt;/p&gt; &lt;p&gt;Guidelines:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create logical objects for main sections&lt;/li&gt; &lt;li&gt;Convert lists/bullet points into arrays&lt;/li&gt; &lt;li&gt;Preserve ALL text exactly as written&lt;/li&gt; &lt;li&gt;Don't summarize or truncate content&lt;/li&gt; &lt;li&gt;Maintain natural content hierarchy&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;browser_cfg = BrowserConfig(headless=True)&lt;/p&gt; &lt;p&gt;async with AsyncWebCrawler(config=browser_cfg) as crawler:&lt;/p&gt; &lt;p&gt;result = await crawler.arun(&lt;/p&gt; &lt;p&gt;url=&amp;quot;[my_url]&amp;quot;,&lt;/p&gt; &lt;p&gt;config=CrawlerRunConfig(&lt;/p&gt; &lt;p&gt;extraction_strategy=strategy,&lt;/p&gt; &lt;p&gt;cache_mode=&amp;quot;BYPASS&amp;quot;,&lt;/p&gt; &lt;p&gt;wait_for=&amp;quot;css:.content-area&amp;quot;&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;if result.success:&lt;/p&gt; &lt;p&gt;return json.loads(result.extracted_content)&lt;/p&gt; &lt;p&gt;return None&lt;/p&gt; &lt;p&gt;asyncio.run(extract_structured_content())&lt;br /&gt; ```&lt;/p&gt; &lt;p&gt;Questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Which model would you recommend for this kind of structured extraction? I need something that can:- Understand HTML content structure- Reliably output valid JSON- Handle long-ish content (few pages worth)- Run locally (prefer not to use OpenAI/Claude)&lt;/li&gt; &lt;li&gt;Should I fine-tune a model for this? If so:- What base model would you recommend?- Any tips on creating training data?- Recommended training approach?&lt;/li&gt; &lt;li&gt;Are there any prompt engineering tricks I should try before going the fine-tuning route?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Budget isn't a huge concern, but I'd prefer local models for latency/privacy reasons. Any suggestions much appreciated! 🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KledMainSG"&gt; /u/KledMainSG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i54pqv/need_help_choosingfinetuning_llm_for_structured/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i54pqv/need_help_choosingfinetuning_llm_for_structured/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i54pqv/need_help_choosingfinetuning_llm_for_structured/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-19T18:08:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4xx5h</id>
    <title>Kokoro-82M on Ollama?</title>
    <updated>2025-01-19T13:00:57+00:00</updated>
    <author>
      <name>/u/usernameIsRand0m</name>
      <uri>https://old.reddit.com/user/usernameIsRand0m</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not sure if this is even possible or not. But, will there be a Kokoro-82M TTS model which is able to run on Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/usernameIsRand0m"&gt; /u/usernameIsRand0m &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i4xx5h/kokoro82m_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i4xx5h/kokoro82m_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i4xx5h/kokoro82m_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-19T13:00:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i56ym2</id>
    <title>What prompts do you use to test a model's censorship limits?</title>
    <updated>2025-01-19T19:40:06+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i56ym2/what_prompts_do_you_use_to_test_a_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i56ym2/what_prompts_do_you_use_to_test_a_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i56ym2/what_prompts_do_you_use_to_test_a_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-19T19:40:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1i58ftc</id>
    <title>Ollama server limitation.</title>
    <updated>2025-01-19T20:41:07+00:00</updated>
    <author>
      <name>/u/JV_info</name>
      <uri>https://old.reddit.com/user/JV_info</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a question for you and would be more than happy if you could answer.&lt;br /&gt; I want to run a local AI (Ollama + docker + OpenwebUI) for our company and for its server, I wanted to build a machine with the below specs:&lt;br /&gt; AMD Ryzen Threadripper PRO 7965WX (24 cores/48 threads)&lt;br /&gt; ASUS Pro WS WRX80E-SAGE SE WIFI&lt;br /&gt; two MSI GeForce RTX 4090 SUPRIM LIQUID X 24G Hybrid Cooling 24GB&lt;br /&gt; 8x 16GB DDR5 ECC&lt;br /&gt; Corsair AX1600i 1600 Watt 80 Plus Titanium ATX Fully Modular Power Supply&lt;br /&gt; Noctua NH-U14S TR4-SP3&lt;br /&gt; 2T SSD storage &lt;/p&gt; &lt;p&gt;If I purchase and run this machine, can eg, 1000 people join and use the Ollama at the same time? or will there be a delay or is there a limitation etc,...&lt;br /&gt; Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JV_info"&gt; /u/JV_info &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i58ftc/ollama_server_limitation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i58ftc/ollama_server_limitation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i58ftc/ollama_server_limitation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-19T20:41:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5s4yf</id>
    <title>Status of current testing for AMD Instinct Mi60 AI Servers</title>
    <updated>2025-01-20T15:04:28+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalAIServers/comments/1i5s42v/status_of_current_testing_for_amd_instinct_mi60/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5s4yf/status_of_current_testing_for_amd_instinct_mi60/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i5s4yf/status_of_current_testing_for_amd_instinct_mi60/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-20T15:04:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5te2j</id>
    <title>How to set up windows as a ollama server?</title>
    <updated>2025-01-20T15:57:35+00:00</updated>
    <author>
      <name>/u/deeeeranged</name>
      <uri>https://old.reddit.com/user/deeeeranged</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a complete noob, I'm only a web developer.&lt;/p&gt; &lt;p&gt;I have ollama installed and running, and I want to setup this windows PC as a ollama server on my network so that I can ping it from my phones and other computers. &lt;/p&gt; &lt;p&gt;What would you recommend I do to get this setup? &lt;/p&gt; &lt;p&gt;I have no idea where to start with this and all the big three AIs have been useless. I still want to use the PC from time to time to game on and such...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/deeeeranged"&gt; /u/deeeeranged &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5te2j/how_to_set_up_windows_as_a_ollama_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5te2j/how_to_set_up_windows_as_a_ollama_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i5te2j/how_to_set_up_windows_as_a_ollama_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-20T15:57:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5ikds</id>
    <title>Created a tiny rust app that let’s you run terminal commands through natural English language using ollama and llama-3.2:3b</title>
    <updated>2025-01-20T04:56:24+00:00</updated>
    <author>
      <name>/u/deba2012ddx</name>
      <uri>https://old.reddit.com/user/deba2012ddx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/FrankenDeba/termino-x"&gt;https://github.com/FrankenDeba/termino-x&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/deba2012ddx"&gt; /u/deba2012ddx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5ikds/created_a_tiny_rust_app_that_lets_you_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5ikds/created_a_tiny_rust_app_that_lets_you_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i5ikds/created_a_tiny_rust_app_that_lets_you_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-20T04:56:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5kpjm</id>
    <title>Speech to 3D Model</title>
    <updated>2025-01-20T07:15:16+00:00</updated>
    <author>
      <name>/u/ConsultingJoe</name>
      <uri>https://old.reddit.com/user/ConsultingJoe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i5kpjm/speech_to_3d_model/"&gt; &lt;img alt="Speech to 3D Model" src="https://external-preview.redd.it/FCugAp2XsrtRKrRdQFiqY85v5aE1-IkFf45owu34XRo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9bad2747aed01a62f4720354a4583f67df98c0b2" title="Speech to 3D Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ConsultingJoe"&gt; /u/ConsultingJoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jsammarco/Speech2Model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5kpjm/speech_to_3d_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i5kpjm/speech_to_3d_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-20T07:15:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5dvzk</id>
    <title>Find and run Linux commands using Ollama</title>
    <updated>2025-01-20T00:42:20+00:00</updated>
    <author>
      <name>/u/regnull</name>
      <uri>https://old.reddit.com/user/regnull</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you are anything like me, you keep forgetting the useful linux commands all the time. I made a little script that makes it easy to find and execute them using Ollama. For example&lt;/p&gt; &lt;p&gt;```bash $ ./how.sh find and delete files older than 30 days&lt;/p&gt; &lt;p&gt;Generated command: find . -type f -mtime +30 -exec rm {} \;&lt;/p&gt; &lt;p&gt;Do you want to execute this command? (y/n): ```&lt;/p&gt; &lt;p&gt;If you feel adventurous, add -y to execute the command without confirmation. You can also specify the model with the optional -m flag.&lt;/p&gt; &lt;p&gt;Here's the repo: &lt;a href="https://github.com/regnull/how.sh"&gt;https://github.com/regnull/how.sh&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/regnull"&gt; /u/regnull &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5dvzk/find_and_run_linux_commands_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5dvzk/find_and_run_linux_commands_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i5dvzk/find_and_run_linux_commands_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-20T00:42:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5w6py</id>
    <title>Anyone using tool call api?</title>
    <updated>2025-01-20T17:51:23+00:00</updated>
    <author>
      <name>/u/gibriyagi</name>
      <uri>https://old.reddit.com/user/gibriyagi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using the golang client for tool calling but models seems to get stuck when they are provided with tool options. I was able to make them suggest a tool but they never reply when the role is &amp;quot;tool&amp;quot;.&lt;/p&gt; &lt;p&gt;Anyone had any luck with this? Is it just me?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gibriyagi"&gt; /u/gibriyagi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5w6py/anyone_using_tool_call_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5w6py/anyone_using_tool_call_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i5w6py/anyone_using_tool_call_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-20T17:51:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5kewj</id>
    <title>How to run (any) open LLM with Ollama on Google Cloud Run [Step-by-step]</title>
    <updated>2025-01-20T06:54:45+00:00</updated>
    <author>
      <name>/u/geshan</name>
      <uri>https://old.reddit.com/user/geshan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i5kewj/how_to_run_any_open_llm_with_ollama_on_google/"&gt; &lt;img alt="How to run (any) open LLM with Ollama on Google Cloud Run [Step-by-step]" src="https://external-preview.redd.it/rQbdXtC-XE7nsAc6p9ZyiTxARnjyYfbBKXv5EQ5bnoI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aeefb0e29f2291c669a3b3c15a27b1c2677020fe" title="How to run (any) open LLM with Ollama on Google Cloud Run [Step-by-step]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geshan"&gt; /u/geshan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://geshan.com.np/blog/2025/01/ollama-google-cloud-run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5kewj/how_to_run_any_open_llm_with_ollama_on_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i5kewj/how_to_run_any_open_llm_with_ollama_on_google/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-20T06:54:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5ydg8</id>
    <title>Three Exciting Projects Using Ollama's Vision Models and Structured Output</title>
    <updated>2025-01-20T19:17:37+00:00</updated>
    <author>
      <name>/u/Special_Community179</name>
      <uri>https://old.reddit.com/user/Special_Community179</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i5ydg8/three_exciting_projects_using_ollamas_vision/"&gt; &lt;img alt="Three Exciting Projects Using Ollama's Vision Models and Structured Output" src="https://external-preview.redd.it/xz4afcMfKjaSxxDudCSOnrrnp0uydjHX2MGZwQf0z2Y.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b36af53b64b6a6672e9750aa340d9563b149796f" title="Three Exciting Projects Using Ollama's Vision Models and Structured Output" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special_Community179"&gt; /u/Special_Community179 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=ZZHWLXyZHlA&amp;amp;list=PLp01ObP3udmq2quR-RfrX4zNut_t_kNot"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5ydg8/three_exciting_projects_using_ollamas_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i5ydg8/three_exciting_projects_using_ollamas_vision/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-20T19:17:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i63vh5</id>
    <title>Deepseek-r1:8b distilled model, success?</title>
    <updated>2025-01-20T23:01:52+00:00</updated>
    <author>
      <name>/u/b61nukejustice</name>
      <uri>https://old.reddit.com/user/b61nukejustice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The model get stuck for me when writing code endlessly repeating the same &amp;quot;thoughts&amp;quot;. Any success from anyone else?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/b61nukejustice"&gt; /u/b61nukejustice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i63vh5/deepseekr18b_distilled_model_success/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i63vh5/deepseekr18b_distilled_model_success/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i63vh5/deepseekr18b_distilled_model_success/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-20T23:01:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6b6o8</id>
    <title>DeepSeek-R1-8B-FP16 + vLLM + 4x AMD Instinct Mi60 Server</title>
    <updated>2025-01-21T05:06:24+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1is01zns4aee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6b6o8/deepseekr18bfp16_vllm_4x_amd_instinct_mi60_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6b6o8/deepseekr18bfp16_vllm_4x_amd_instinct_mi60_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T05:06:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6a6jn</id>
    <title>Quen2.5-Coder-32B-Instruct-FP16 + 4x AMD Instinct Mi60 Server</title>
    <updated>2025-01-21T04:09:43+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/f86dbcjxu9ee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6a6jn/quen25coder32binstructfp16_4x_amd_instinct_mi60/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6a6jn/quen25coder32binstructfp16_4x_amd_instinct_mi60/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T04:09:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1i65qzs</id>
    <title>built a local AI that watches your screen &amp; mic &amp; writes your obsidian notes (ollama-first, open source)</title>
    <updated>2025-01-21T00:25:59+00:00</updated>
    <author>
      <name>/u/louis3195</name>
      <uri>https://old.reddit.com/user/louis3195</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i65qzs/built_a_local_ai_that_watches_your_screen_mic/"&gt; &lt;img alt="built a local AI that watches your screen &amp;amp; mic &amp;amp; writes your obsidian notes (ollama-first, open source) " src="https://external-preview.redd.it/M2dqaXpjaTNyOGVlMX3UEqsouCoXDcMV9Txg0fwqmyMDrKts8K5P20rCv2xc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c9226415249c790e831bee93fb02fae91202e0f" title="built a local AI that watches your screen &amp;amp; mic &amp;amp; writes your obsidian notes (ollama-first, open source) " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/louis3195"&gt; /u/louis3195 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4slw6bi3r8ee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i65qzs/built_a_local_ai_that_watches_your_screen_mic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i65qzs/built_a_local_ai_that_watches_your_screen_mic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T00:25:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i66o1d</id>
    <title>deepseek-r1 is now in Ollama's Models library</title>
    <updated>2025-01-21T01:09:24+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i66o1d/deepseekr1_is_now_in_ollamas_models_library/"&gt; &lt;img alt="deepseek-r1 is now in Ollama's Models library" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="deepseek-r1 is now in Ollama's Models library" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/library/deepseek-r1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i66o1d/deepseekr1_is_now_in_ollamas_models_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i66o1d/deepseekr1_is_now_in_ollamas_models_library/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T01:09:24+00:00</published>
  </entry>
</feed>
