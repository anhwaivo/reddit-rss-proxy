<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-28T13:48:54+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ibtp06</id>
    <title>How to know LLM model can run in my computer?</title>
    <updated>2025-01-28T04:37:53+00:00</updated>
    <author>
      <name>/u/iNdramal</name>
      <uri>https://old.reddit.com/user/iNdramal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are many LLM models available in Ollama. Also, available in various sizes. How to know which LLM models can run on my computer? does it give an issues when running the model?&lt;/p&gt; &lt;p&gt;My computer specifications:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPU: Nvidia GetForce RTX 1050&lt;/li&gt; &lt;li&gt;Ram: 12GB&lt;/li&gt; &lt;li&gt;Processor: Intel(R) Core(TM) i7-8750H CPU @ 2.20GHz 2.21 GHz&lt;/li&gt; &lt;li&gt;OS: Ubuntu&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iNdramal"&gt; /u/iNdramal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibtp06/how_to_know_llm_model_can_run_in_my_computer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibtp06/how_to_know_llm_model_can_run_in_my_computer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibtp06/how_to_know_llm_model_can_run_in_my_computer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T04:37:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibx81l</id>
    <title>GPU working ok from Open WebUI, but not from Python?</title>
    <updated>2025-01-28T08:40:07+00:00</updated>
    <author>
      <name>/u/old-mike</name>
      <uri>https://old.reddit.com/user/old-mike</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. When using Ollama from Open WebUI, I can see how the GPU is used in NVTOP. Calling Ollama from a Python script does not use GPU. What am I doing wrong?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama.Client(host='http://192.168.100.61:11434') response = client.generate(model_name, prompt, options={&amp;quot;num_ctx&amp;quot;: 32000,&amp;quot;stream&amp;quot;: &amp;quot;false&amp;quot;} &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/old-mike"&gt; /u/old-mike &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibx81l/gpu_working_ok_from_open_webui_but_not_from_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibx81l/gpu_working_ok_from_open_webui_but_not_from_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibx81l/gpu_working_ok_from_open_webui_but_not_from_python/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T08:40:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibu574</id>
    <title>Model for content moderation?</title>
    <updated>2025-01-28T05:04:04+00:00</updated>
    <author>
      <name>/u/HackTheDev</name>
      <uri>https://old.reddit.com/user/HackTheDev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used openi's api to do automated content moderation which got expensive. today i heard about deepseek but it out right refuses requests if they &amp;quot;get too spicy&amp;quot; making it useless.&lt;/p&gt; &lt;p&gt;I then tried &amp;quot;closex/neuraldaredevil-8b-abliterated:latest&amp;quot; which worked better but i think its a bit dumb, or i am maybe. A simplified prompt like &amp;quot;detect insults against people or groups but allow non directed profanity&amp;quot; will always be flagged where openai handles this just fine&lt;/p&gt; &lt;p&gt;currently im downloading &amp;quot;jean-luc/big-tiger-gemma:27b-v1c-Q3_K_M&amp;quot; and hope this one will be better and be able to the job.&lt;/p&gt; &lt;p&gt;im using ollama on windows with a ryzen 9 5900x and a nvidia rtx 3080&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HackTheDev"&gt; /u/HackTheDev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibu574/model_for_content_moderation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibu574/model_for_content_moderation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibu574/model_for_content_moderation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T05:04:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibmf8q</id>
    <title>[v1.0.8] Notate - New Reasoning Layer Added (Agent actions will be added) | UI/UX Improvements &amp; Deepseek Integration - Have suggestions? love to hear em!</title>
    <updated>2025-01-27T22:41:52+00:00</updated>
    <author>
      <name>/u/Hairetsu</name>
      <uri>https://old.reddit.com/user/Hairetsu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ibmf8q/v108_notate_new_reasoning_layer_added_agent/"&gt; &lt;img alt="[v1.0.8] Notate - New Reasoning Layer Added (Agent actions will be added) | UI/UX Improvements &amp;amp; Deepseek Integration - Have suggestions? love to hear em!" src="https://external-preview.redd.it/mLmb6PM3DcPIkGqR7oQ2EX6QH1e3GfDbIdNH1M_vKQw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=769808d36520d2e157bfc35efdd17fa7406c869f" title="[v1.0.8] Notate - New Reasoning Layer Added (Agent actions will be added) | UI/UX Improvements &amp;amp; Deepseek Integration - Have suggestions? love to hear em!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hairetsu"&gt; /u/Hairetsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/CNTRLAI/Notate"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibmf8q/v108_notate_new_reasoning_layer_added_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibmf8q/v108_notate_new_reasoning_layer_added_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T22:41:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibmlty</id>
    <title>Can someone suggest me a model to use for a personal project</title>
    <updated>2025-01-27T22:49:49+00:00</updated>
    <author>
      <name>/u/AVBGaming</name>
      <uri>https://old.reddit.com/user/AVBGaming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am interested in implementing AI/LLM into my note taking software (Obsidian). What i’m envisioning is a tool i can use which i can enter text prompts (like asking where a certain note is, or a general question that could be answered with something in my notes) and get a response. Any pointers on where i could start? i’m not sure how many parameters would suffice for this task, and im not sure if i need to look exclusively at models i can run locally vs on the cloud.&lt;/p&gt; &lt;p&gt;I have a 4070 8GB VRAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AVBGaming"&gt; /u/AVBGaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibmlty/can_someone_suggest_me_a_model_to_use_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibmlty/can_someone_suggest_me_a_model_to_use_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibmlty/can_someone_suggest_me_a_model_to_use_for_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T22:49:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1iby4np</id>
    <title>Anyone got GEITje-7B?</title>
    <updated>2025-01-28T09:51:56+00:00</updated>
    <author>
      <name>/u/ScrapEngineer_</name>
      <uri>https://old.reddit.com/user/ScrapEngineer_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Dutch copyright troll Brein has taken offline GEITje-7B.&lt;/p&gt; &lt;p&gt;I had previously downloaded this model, but unfortunately lost it.&lt;/p&gt; &lt;p&gt;So does anyone had downloaded GEITje-7B before it was taken down and is willing to share it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ScrapEngineer_"&gt; /u/ScrapEngineer_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iby4np/anyone_got_geitje7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iby4np/anyone_got_geitje7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iby4np/anyone_got_geitje7b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T09:51:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iby6rq</id>
    <title>Running deepseek-r1:1.5b on Apple Silicon GPU</title>
    <updated>2025-01-28T09:56:26+00:00</updated>
    <author>
      <name>/u/qwpajrty</name>
      <uri>https://old.reddit.com/user/qwpajrty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running DeepSeek R1 (1.5B) on a MacBook M1 Pro with 16GB RAM. While monitoring performance, I noticed that GPU usage remained at 0% throughout the Ollama process, while CPU usage spiked to 500%.&lt;/p&gt; &lt;p&gt;How can I configure this model to utilize the Apple Silicon GPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/qwpajrty"&gt; /u/qwpajrty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iby6rq/running_deepseekr115b_on_apple_silicon_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iby6rq/running_deepseekr115b_on_apple_silicon_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iby6rq/running_deepseekr115b_on_apple_silicon_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T09:56:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibhxvm</id>
    <title>Guide to Installing and Locally Running Ollama LLM models in Comfy (ELI5 Level)</title>
    <updated>2025-01-27T19:38:29+00:00</updated>
    <author>
      <name>/u/GreyScope</name>
      <uri>https://old.reddit.com/user/GreyScope</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ibhxvm/guide_to_installing_and_locally_running_ollama/"&gt; &lt;img alt="Guide to Installing and Locally Running Ollama LLM models in Comfy (ELI5 Level)" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="Guide to Installing and Locally Running Ollama LLM models in Comfy (ELI5 Level)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Firstly, due diligence still applies to checking out any security issues to all models and software.&lt;/p&gt; &lt;p&gt;Secondly, this is written in the (kiss) style of all my guides : simple steps, it is not a technical paper, nor is it written for people who have greater technical knowledge, they are written as best I can in ELI5 style .&lt;/p&gt; &lt;h1&gt;Pre-requisites&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;A (quick) internet connection (if downloading large models&lt;/li&gt; &lt;li&gt;A working install of ComfyUI&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Usage Case:&lt;/h1&gt; &lt;p&gt;1. For Stable Diffusion purposes it’s for writing or expanding prompts, ie to make descriptions or make them more detailed / refined for a purpose (eg like a video) if used on an existing bare bones prompt .&lt;/p&gt; &lt;p&gt;2. If the LLM is used to describe an existing image, it can help replicate the style or substance of it.&lt;/p&gt; &lt;p&gt;3. Use it as a Chat bot or as a LLM front end for whatever you want (eg coding)&lt;/p&gt; &lt;h1&gt;Basic Steps to carry out (Part 1):&lt;/h1&gt; &lt;p&gt;1. Download Ollama itself&lt;/p&gt; &lt;p&gt;2. Turn off Ollama’s Autostart entry (&amp;amp; start when needed) or leave it&lt;/p&gt; &lt;p&gt;3. Set the Ollama ENV in Windows – to set where it saves the models that it uses&lt;/p&gt; &lt;p&gt;4. Run Ollama in a CMD window and download a model&lt;/p&gt; &lt;p&gt;5. Run Ollama with the model you just downloaded&lt;/p&gt; &lt;h1&gt;Basic Steps to carry out (Part 2):&lt;/h1&gt; &lt;p&gt;1. For use within Comfy download/install nodes for its use&lt;/p&gt; &lt;p&gt;2. Setup nodes within your own flow or download a flow with them in&lt;/p&gt; &lt;p&gt;3. Setup the settings within the LLM node to use Ollama &lt;/p&gt; &lt;h1&gt;Basic Explanation of Terms&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;An &lt;strong&gt;LLM (Large Language Model)&lt;/strong&gt; is an AI system trained on vast amounts of text data to understand, generate, and manipulate human-like language for various tasks - like coding, describing images, writing text etc&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt; is a tool that allows users to easily download, run, and manage open-source large language models (LLMs) locally on their own hardware.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;---------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Part 1 - Ollama&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;DownLoad Ollama&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Download Ollama and install from - &lt;a href="https://ollama.com/"&gt;https://ollama.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You will see nothing after it installs but if you go down the bottom right of the taskbar in the Notification section, you'll see it is active (running a background server).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/omup1pj0nkfe1.png?width=453&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c5484788c42d83c84cb53f38a5143afc62d9155"&gt;https://preview.redd.it/omup1pj0nkfe1.png?width=453&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c5484788c42d83c84cb53f38a5143afc62d9155&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Ollama and Autostart&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Be aware that Ollama autoruns on your PC’s startup, if you don’t want that then turn off its Autostart on (Ctrl -Alt-Del to start the Task Manager and then click on Startup Apps and lastly just right clock on its entry on the list and select ‘Disabled’)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Set Ollama's ENV settings&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Now setup where you want Ollama to save its models (eg your hard drive with your SD installs on or the one with the most space)&lt;/p&gt; &lt;p&gt;Type ‘ENV’ into search box on your taskbar&lt;/p&gt; &lt;p&gt;Select &amp;quot;Edit the System Environment Variables&amp;quot; (part of Windows Control Panel) , see below&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y03ubm8dnkfe1.png?width=393&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=179daa73ce2009eb1ea2343dd225a07c7a772455"&gt;https://preview.redd.it/y03ubm8dnkfe1.png?width=393&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=179daa73ce2009eb1ea2343dd225a07c7a772455&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On the newly opened ‘System Properties‘ window, click on &amp;quot;Environment Variables&amp;quot; (bottom right on pic below)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f2tv16minkfe1.png?width=283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=919004d372bd169d80d4a0378be070962a79f78d"&gt;https://preview.redd.it/f2tv16minkfe1.png?width=283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=919004d372bd169d80d4a0378be070962a79f78d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;System Variables are split into two sections of User and System - click on New under &amp;quot;User Variables&amp;quot; (top section on pic below)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zhbudfjnnkfe1.png?width=414&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef70cb35da716b05a365d8eab65ca7406eef7fdb"&gt;https://preview.redd.it/zhbudfjnnkfe1.png?width=414&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef70cb35da716b05a365d8eab65ca7406eef7fdb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On the new input window, input the following -&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Variable name:&lt;/strong&gt; OLLAMA_MODELS&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Variable value:&lt;/strong&gt; (input directory path you wish to save models to. Make your folder structure as you wish ( eg H:\Ollama\Models).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4xitvv0ynkfe1.png?width=535&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e07784593316bd1147b7b02547eece860a49fb0c"&gt;https://preview.redd.it/4xitvv0ynkfe1.png?width=535&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e07784593316bd1147b7b02547eece860a49fb0c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NB&lt;/strong&gt; Don’t change the ‘Variable name’ or Ollama will not save to the directory you wish.&lt;/p&gt; &lt;p&gt;Click OK on each screen until the Environment Variables windows and then the System Properties windows close down (the variables are not saved until they're all closed)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open a CMD window and type 'Ollama' it will return its commands that you can use (see pic below)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nazdqgraokfe1.png?width=353&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=badd182f68014c2e38d69cfa06e8eeecbec96763"&gt;https://preview.redd.it/nazdqgraokfe1.png?width=353&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=badd182f68014c2e38d69cfa06e8eeecbec96763&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here’s a list of popular Large Language Models (LLMs) available on &lt;strong&gt;Ollama&lt;/strong&gt;, categorized by their simplified use cases. These models can be downloaded and run locally using Ollama or any others that are available (due diligence required) :&lt;/p&gt; &lt;h1&gt;A. Chat Models&lt;/h1&gt; &lt;p&gt;These models are optimized for conversational AI and interactive chat applications.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Llama 2 (7B, 13B, 70B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: General-purpose chat, conversational AI, and answering questions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run llama2&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mistral (7B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Lightweight and efficient chat model for conversational tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run mistral&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;B. Text Generation Models&lt;/h1&gt; &lt;p&gt;These models excel at generating coherent and creative text for various purposes.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OpenLLaMA (7B, 13B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Open-source alternative for text generation and summarization.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run openllama&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;C. Coding Models&lt;/h1&gt; &lt;p&gt;These models are specialized for code generation, debugging, and programming assistance.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CodeLlama (7B, 13B, 34B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Code generation, debugging, and programming assistance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run codellama&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;C. Image Description Models&lt;/h1&gt; &lt;p&gt;These models are designed to generate text descriptions of images (multimodal capabilities).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LLaVA (7B, 13B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Image captioning, visual question answering, and multimodal tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run llava&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;D. Multimodal Models&lt;/h1&gt; &lt;p&gt;These models combine text and image understanding for advanced tasks.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fuyu (8B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Multimodal tasks, including image understanding and text generation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run fuyu&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;E. Specialized Models&lt;/h1&gt; &lt;p&gt;These models are fine-tuned for specific tasks or domains.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;WizardCoder (15B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Specialized in coding tasks and programming assistance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run wizardcoder&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Alpaca (7B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Instruction-following tasks and fine-tuned conversational AI.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run alpaca&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Model Strengths&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;As you can see above, an LLM is focused to a particular strength, it's for the best to expect a Coding biased LLM to provide a good description of an image.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Size&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Go into the Ollama website and pick a variant (noted by the number and followed by a B in brackets after each model) to fit into your graphics cards VRAM.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Downloading a model - When you have decided which model you want, say the Gemma 2 model in its smallest 2b variant at 1.6G (pic below). The arrow shows the command to put into the CMD window to download and run it (it autodownloads and then runs)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gvb8dq6rukfe1.png?width=504&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16ad8f0c116f45cbdff67179599af06ffedefa6a"&gt;https://preview.redd.it/gvb8dq6rukfe1.png?width=504&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16ad8f0c116f45cbdff67179599af06ffedefa6a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9cutop61wkfe1.png?width=429&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b406060c153a068c78537275b70193f71cf6976"&gt;Models downloads and then runs - I asked it what an LLM is. Typing 'ollama list' tells you the models you have.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;-------------------------------------------------------.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Part 2 - Comfy&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I prefer a working workflow to have everything in a state where you can work on and adjust it to your needs / interests.&lt;/p&gt; &lt;p&gt;This is a great example from a user here &lt;a href="/u/EnragedAntelope"&gt;u/EnragedAntelope&lt;/a&gt; posted on Civitai - its for a workflow that uses LLMs in picture description for Cosmos I2V.&lt;/p&gt; &lt;p&gt;&lt;a href="https://civitai.com/models/1145020/cosmos-automated-image-to-video-i2v-enragedantelope"&gt;Cosmos AUTOMATED Image to Video (I2V) - EnragedAntelope - v1.2 | Other Workflows | Civitai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The initial LLM (Florence2) auto-downloads and installs itself , it then carries out the initial Image description (bottom right text box)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jh8rvq4oxkfe1.png?width=1117&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35f30f6ea6bd6a318c6ec75414400f30006175aa"&gt;https://preview.redd.it/jh8rvq4oxkfe1.png?width=1117&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35f30f6ea6bd6a318c6ec75414400f30006175aa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The text in the initial description is then passed to the second LLM module (within the Plush nodes) , this is initially set to use bigger internet based LLMs.&lt;/p&gt; &lt;p&gt;From everything carried out above, this can be changed to use your local Ollama install. Ensure the server is running (Llama in the notification area) - note the settings in the Advanced Prompt Enhancer node in the pic below.&lt;/p&gt; &lt;p&gt;That node is from the &lt;a href="https://github.com/glibsonoran/Plush-for-ComfyUI"&gt;https://github.com/glibsonoran/Plush-for-ComfyUI&lt;/a&gt; , let manager sort it all out for you.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qzl0bt0fykfe1.png?width=1190&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b2a9a26ba3e39ee647171b937735d969812ca37"&gt;Advanced Prompt Generator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You select the Ollama model from your downloads with a simple click on the box (see pic below) .&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5zva3rqnykfe1.png?width=468&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=93e02f32d2ba61cb00b0c470b8a06fc16e4dc4a5"&gt;Ollama Model selection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the context of this workflow, the added second LLM is given the purpose of rewriting the prompt for a video to increase the quality.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ibhxvm/video/44vn5inmzkfe1/player"&gt;https://reddit.com/link/1ibhxvm/video/44vn5inmzkfe1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ibhxvm/video/3conlucvzkfe1/player"&gt;https://reddit.com/link/1ibhxvm/video/3conlucvzkfe1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreyScope"&gt; /u/GreyScope &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibhxvm/guide_to_installing_and_locally_running_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibhxvm/guide_to_installing_and_locally_running_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibhxvm/guide_to_installing_and_locally_running_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T19:38:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibybpt</id>
    <title>GPU question</title>
    <updated>2025-01-28T10:06:42+00:00</updated>
    <author>
      <name>/u/Primary_Arm_1175</name>
      <uri>https://old.reddit.com/user/Primary_Arm_1175</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ok here is the question. suppose I bought 2 6900 or 6800 AMD GPUs. will I be able to run something like 32B deep seek r1?&lt;br /&gt; will it be slow? will it be reasonably slow?&lt;br /&gt; also what if I bought a really good CPU with 128 GB of RAM? has anyone here has any idea what each setup can run? can anyone share their setup and what it's capable of? I have a 3050 8 GB with 32 GB RAM and a 12400f and it can run the 1.5B easily it DOES run the 8B, slowly but it sometimes crashes. can people here share what they have and how well it runs? thank you &amp;lt;3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Primary_Arm_1175"&gt; /u/Primary_Arm_1175 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibybpt/gpu_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibybpt/gpu_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibybpt/gpu_question/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T10:06:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibvoyt</id>
    <title>Looking For GPU Help</title>
    <updated>2025-01-28T06:44:11+00:00</updated>
    <author>
      <name>/u/thedayzed</name>
      <uri>https://old.reddit.com/user/thedayzed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys I am looking for some ideas on how to move on GPUs. currently I have a 4070ti but want to move to a larger Vram card. so was looking at the 7900xtx or a 3090. my board is only dual spaced so if I were to do a 2 card setup both cards would need to be only two slot but still as powerful as my 4070ti as I game also. any recommendations would be helpful as I have not decided on what way to go. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thedayzed"&gt; /u/thedayzed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibvoyt/looking_for_gpu_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibvoyt/looking_for_gpu_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibvoyt/looking_for_gpu_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T06:44:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibyn8k</id>
    <title>Is anyone running LLM on a Radeon Instinct Mi50?</title>
    <updated>2025-01-28T10:30:54+00:00</updated>
    <author>
      <name>/u/East-Engineering-653</name>
      <uri>https://old.reddit.com/user/East-Engineering-653</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently bought a used Radeon Instinct Mi50 and I'm going to try running LLM with it.&lt;/p&gt; &lt;p&gt;Since the VRAM is 16GB, the model size that can be run seems quite large, but I'm curious about the running speed. Also, I know that it's been a while since this card was released, so I'm curious if the latest ROCm supports this card.&lt;/p&gt; &lt;p&gt;Lastly, the seller said that he changed the graphics card BIOS to Radeon 7. Will this affect software for LLM such as ROCm?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/East-Engineering-653"&gt; /u/East-Engineering-653 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibyn8k/is_anyone_running_llm_on_a_radeon_instinct_mi50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibyn8k/is_anyone_running_llm_on_a_radeon_instinct_mi50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibyn8k/is_anyone_running_llm_on_a_radeon_instinct_mi50/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T10:30:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibz1i7</id>
    <title>Can anyone help regarding this error?</title>
    <updated>2025-01-28T10:59:44+00:00</updated>
    <author>
      <name>/u/SnooGiraffes4275</name>
      <uri>https://old.reddit.com/user/SnooGiraffes4275</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ibz1i7/can_anyone_help_regarding_this_error/"&gt; &lt;img alt="Can anyone help regarding this error?" src="https://preview.redd.it/oae86xlmupfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25023ba8cbe81cec5094cfe83595e47900fd6856" title="Can anyone help regarding this error?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My net speed is 300 mbps. I’m pretty sure something else is wrong but I can’t figure out what&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooGiraffes4275"&gt; /u/SnooGiraffes4275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oae86xlmupfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibz1i7/can_anyone_help_regarding_this_error/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibz1i7/can_anyone_help_regarding_this_error/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T10:59:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibadzz</id>
    <title>LLM powered scraping with ollama</title>
    <updated>2025-01-27T14:31:29+00:00</updated>
    <author>
      <name>/u/Financial-Article-12</name>
      <uri>https://old.reddit.com/user/Financial-Article-12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been developing Parsera, a simple yet powerful Python library that leverages LLMs for web scraping. Many users requested Ollama support, and now that I’ve added it, I wanted to share it with the Ollama community.&lt;/p&gt; &lt;p&gt;If you are looking for a way to extract data from websites (especially when dealing with multiple websites), Parsera lets you do it with just a few lines of code, without the need to develop custom scrapers.&lt;/p&gt; &lt;p&gt;Give it a try and let me know what you think. Feedback is always welcome!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/raznem/parsera"&gt;github.com/raznem/parsera&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial-Article-12"&gt; /u/Financial-Article-12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibadzz/llm_powered_scraping_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibadzz/llm_powered_scraping_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibadzz/llm_powered_scraping_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T14:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibzoat</id>
    <title>Why Ollama uses 90% GPU with both Deepseek r1 1.5b and 7b on MacbookPro M1Pro?</title>
    <updated>2025-01-28T11:41:24+00:00</updated>
    <author>
      <name>/u/sP0re90</name>
      <uri>https://old.reddit.com/user/sP0re90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; It's the first time I use Ollama. I'm running it with Open Web UI on Macbook Pro M1 Pro.&lt;br /&gt; I'm wondering why Ollama process in Activity Monitor shows 90% GPU usage with both 1.5b and 7b Deepseek models while waiting for an answer?&lt;br /&gt; If I type in terminal Ollama ps I see the models and both shows PROCESSOR 100% GPU. &lt;/p&gt; &lt;p&gt;They both works, not super fast answers but they do their job.&lt;br /&gt; But I'm trying to understand how Ollama/models resources consumption works.&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sP0re90"&gt; /u/sP0re90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibzoat/why_ollama_uses_90_gpu_with_both_deepseek_r1_15b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibzoat/why_ollama_uses_90_gpu_with_both_deepseek_r1_15b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibzoat/why_ollama_uses_90_gpu_with_both_deepseek_r1_15b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T11:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic0m0t</id>
    <title>Can I Use Ollama to automate repetitive tasks in third party apps like camtasia, adobe premiere pro, capcut etc.?</title>
    <updated>2025-01-28T12:38:26+00:00</updated>
    <author>
      <name>/u/Zacky96</name>
      <uri>https://old.reddit.com/user/Zacky96</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can i Ollama to automate repetitive tasks in third party apps like camtasia, adobe premiere pro, capcut etc.?&lt;/p&gt; &lt;p&gt;For Example: Capcut doesn't have a Batch Export feature for projects like other video editing tools; So, I have to sit down and export each project one by one? &lt;/p&gt; &lt;p&gt;So, Can I Use Ollama (any other alternative) to automate such repetitive tasks in 3rd party apps?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zacky96"&gt; /u/Zacky96 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic0m0t/can_i_use_ollama_to_automate_repetitive_tasks_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic0m0t/can_i_use_ollama_to_automate_repetitive_tasks_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic0m0t/can_i_use_ollama_to_automate_repetitive_tasks_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T12:38:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic0v0b</id>
    <title>Where to see System Requirements of models ?</title>
    <updated>2025-01-28T12:52:57+00:00</updated>
    <author>
      <name>/u/RedditNoobie777</name>
      <uri>https://old.reddit.com/user/RedditNoobie777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RedditNoobie777"&gt; /u/RedditNoobie777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic0v0b/where_to_see_system_requirements_of_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic0v0b/where_to_see_system_requirements_of_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic0v0b/where_to_see_system_requirements_of_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T12:52:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic0v0t</id>
    <title>Where to see System Requirements of models ?</title>
    <updated>2025-01-28T12:52:58+00:00</updated>
    <author>
      <name>/u/RedditNoobie777</name>
      <uri>https://old.reddit.com/user/RedditNoobie777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RedditNoobie777"&gt; /u/RedditNoobie777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic0v0t/where_to_see_system_requirements_of_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic0v0t/where_to_see_system_requirements_of_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic0v0t/where_to_see_system_requirements_of_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T12:52:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic0wm1</id>
    <title>Difference between deepseek-r1 models ?</title>
    <updated>2025-01-28T12:55:24+00:00</updated>
    <author>
      <name>/u/RedditNoobie777</name>
      <uri>https://old.reddit.com/user/RedditNoobie777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;llama-distill&lt;/li&gt; &lt;li&gt;qwen-distill&lt;/li&gt; &lt;li&gt;fp&lt;/li&gt; &lt;li&gt;q4&lt;/li&gt; &lt;li&gt;a8&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RedditNoobie777"&gt; /u/RedditNoobie777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic0wm1/difference_between_deepseekr1_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic0wm1/difference_between_deepseekr1_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic0wm1/difference_between_deepseekr1_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T12:55:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibuk3j</id>
    <title>Has anyone used the "distilled" versions of deepseek?</title>
    <updated>2025-01-28T05:28:43+00:00</updated>
    <author>
      <name>/u/daHsu</name>
      <uri>https://old.reddit.com/user/daHsu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for advice on model selection. I was looking for a DeepSeek model to download, and it turns out that not only are there 5 different sizes of the model, but they also went and fine-tuned the model on a bunch of different versions of qwen/llama as well!&lt;/p&gt; &lt;p&gt;I couldn't find any recommendations/info on these, other than their explanation&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1. We slightly change their configs and tokenizers. Please use our setting to run these models.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Does anyone have experience with them, or have any tips on when to use them? I'm not sure what a fine-tune here would really mean--like a DeepSeek that &amp;quot;talks like&amp;quot; qwen?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/daHsu"&gt; /u/daHsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibuk3j/has_anyone_used_the_distilled_versions_of_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibuk3j/has_anyone_used_the_distilled_versions_of_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibuk3j/has_anyone_used_the_distilled_versions_of_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T05:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic19hd</id>
    <title>everytime i try to install a llm it installs incorrectly</title>
    <updated>2025-01-28T13:14:11+00:00</updated>
    <author>
      <name>/u/lilgooberj</name>
      <uri>https://old.reddit.com/user/lilgooberj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ic19hd/everytime_i_try_to_install_a_llm_it_installs/"&gt; &lt;img alt="everytime i try to install a llm it installs incorrectly" src="https://a.thumbs.redditmedia.com/DAx2Z6q5J9ghvgv7zN-Ty6xVt5nXj5-JFos5j3OOde0.jpg" title="everytime i try to install a llm it installs incorrectly" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/885ffflliqfe1.png?width=1085&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52d7963007dab48e4bb076c6907f5f07742c6eae"&gt;https://preview.redd.it/885ffflliqfe1.png?width=1085&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52d7963007dab48e4bb076c6907f5f07742c6eae&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lilgooberj"&gt; /u/lilgooberj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic19hd/everytime_i_try_to_install_a_llm_it_installs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic19hd/everytime_i_try_to_install_a_llm_it_installs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic19hd/everytime_i_try_to_install_a_llm_it_installs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T13:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic1qxe</id>
    <title>First-time user</title>
    <updated>2025-01-28T13:38:50+00:00</updated>
    <author>
      <name>/u/Mighty_Mite_C</name>
      <uri>https://old.reddit.com/user/Mighty_Mite_C</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! Was so impressed by DeepSeek taking over the US news outlets etc. by storm I decided to give it a try locally on my personal (Mac) machine. I followed a guide from another redditor. Downloaded Ollama, Chatbox and then went with the smallest LMM; deepseek-r1:1.5b. &lt;/p&gt; &lt;p&gt;Gave it a spin, asked a question and got so freaked out that I —hopefully, removed all of it. Should I be worried? I am so worried that my laptop is infected. Though not noticing anything unusual. What about keyloggers? Is it usual for viruses to manifest like this? Is this a real fear or am I being paranoid? &lt;/p&gt; &lt;p&gt;Thank you community!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mighty_Mite_C"&gt; /u/Mighty_Mite_C &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic1qxe/firsttime_user/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic1qxe/firsttime_user/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic1qxe/firsttime_user/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T13:38:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1iblmdi</id>
    <title>Is Deepseek R-1 overthinking everything?</title>
    <updated>2025-01-27T22:08:10+00:00</updated>
    <author>
      <name>/u/nPrevail</name>
      <uri>https://old.reddit.com/user/nPrevail</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I like the critical thinking that Deep Seek R-1 (reason) has, but I also think that it's been overthinking a lot, and giving me too many alternatives and options, and it keeps running ideas beyond reason (ha ha ha).&lt;/p&gt; &lt;p&gt;At worst, it'll give me an incorrect answer, and will talk to itself, saying things like &amp;quot;Oh, that not the right answer. Let me try it again&amp;quot; type of conversation with itself, but it will keep going until it's satisfied with its own answer.&lt;/p&gt; &lt;p&gt;Has anyone else experienced this?&lt;/p&gt; &lt;p&gt;I almost want to shift back to using llama3 as my main LLM. It was pretty straightforward, despite not giving me any critical responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nPrevail"&gt; /u/nPrevail &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iblmdi/is_deepseek_r1_overthinking_everything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iblmdi/is_deepseek_r1_overthinking_everything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iblmdi/is_deepseek_r1_overthinking_everything/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T22:08:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibngl3</id>
    <title>Qwen2.5-VL just released</title>
    <updated>2025-01-27T23:26:33+00:00</updated>
    <author>
      <name>/u/numinouslymusing</name>
      <uri>https://old.reddit.com/user/numinouslymusing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5"&gt;https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/numinouslymusing"&gt; /u/numinouslymusing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibngl3/qwen25vl_just_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibngl3/qwen25vl_just_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibngl3/qwen25vl_just_released/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T23:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibulvz</id>
    <title>These random accounts have been showing up ever since I started using ollama. Should I be worried?</title>
    <updated>2025-01-28T05:31:49+00:00</updated>
    <author>
      <name>/u/Liquidmesh</name>
      <uri>https://old.reddit.com/user/Liquidmesh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ibulvz/these_random_accounts_have_been_showing_up_ever/"&gt; &lt;img alt="These random accounts have been showing up ever since I started using ollama. Should I be worried?" src="https://preview.redd.it/lqatzv6y7ofe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=21464609696b2ac648efbca375cb46f4d41f5c57" title="These random accounts have been showing up ever since I started using ollama. Should I be worried?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Liquidmesh"&gt; /u/Liquidmesh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lqatzv6y7ofe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibulvz/these_random_accounts_have_been_showing_up_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibulvz/these_random_accounts_have_been_showing_up_ever/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T05:31:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibwdvx</id>
    <title>Ollama enjoying the Chinese New Year! Open Source FTW 🚀</title>
    <updated>2025-01-28T07:34:16+00:00</updated>
    <author>
      <name>/u/eternviking</name>
      <uri>https://old.reddit.com/user/eternviking</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ibwdvx/ollama_enjoying_the_chinese_new_year_open_source/"&gt; &lt;img alt="Ollama enjoying the Chinese New Year! Open Source FTW 🚀" src="https://preview.redd.it/nv34uyjqtofe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9be62292f2e57c4882e28f231b0a82d1126bdabd" title="Ollama enjoying the Chinese New Year! Open Source FTW 🚀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eternviking"&gt; /u/eternviking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nv34uyjqtofe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibwdvx/ollama_enjoying_the_chinese_new_year_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibwdvx/ollama_enjoying_the_chinese_new_year_open_source/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T07:34:16+00:00</published>
  </entry>
</feed>
