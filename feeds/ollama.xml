<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-07-30T08:57:44+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mau97k</id>
    <title>CoexistAI – LLM-Powered Research Assistant (Now with MCP, Vision, Local File Chat, and More)</title>
    <updated>2025-07-27T18:50:44+00:00</updated>
    <author>
      <name>/u/Optimalutopic</name>
      <uri>https://old.reddit.com/user/Optimalutopic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mau97k/coexistai_llmpowered_research_assistant_now_with/"&gt; &lt;img alt="CoexistAI – LLM-Powered Research Assistant (Now with MCP, Vision, Local File Chat, and More)" src="https://external-preview.redd.it/VutiA7Y74mT9dz-x7z-SbUgl4169mRXlKlUpavRehWA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93cf06d43693ab16b6e5f10b12c5132279ba9f1b" title="CoexistAI – LLM-Powered Research Assistant (Now with MCP, Vision, Local File Chat, and More)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, thanks for showing love to CoexistAI 1.0.&lt;/p&gt; &lt;p&gt;I have just released a new version of &lt;strong&gt;CoexistAI v2.0&lt;/strong&gt;, a modular framework to search, summarize, and automate research using LLMs. Works with web, Reddit, YouTube, GitHub, maps, and local files/folders/codes/documentations.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What’s new:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vision support: explore images (&lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.svg&lt;/code&gt;, etc.)&lt;/li&gt; &lt;li&gt;Chat with local files and folders (PDFs, excels, csvs, ppts, code, images,etc)&lt;/li&gt; &lt;li&gt;Location + POI search (not just routes)&lt;/li&gt; &lt;li&gt;Smarter Reddit and YouTube tools (BM25, custom prompts)&lt;/li&gt; &lt;li&gt;Full &lt;strong&gt;MCP support&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Integrate with LM Studio, &lt;strong&gt;Ollama&lt;/strong&gt;, and other local and proprietary LLM tools&lt;/li&gt; &lt;li&gt;Supports Gemini, OpenAI, and any open source or self-hosted models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Python + API. Async.&lt;/p&gt; &lt;p&gt;Always open to feedback&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Optimalutopic"&gt; /u/Optimalutopic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/SPThole/CoexistAI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mau97k/coexistai_llmpowered_research_assistant_now_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mau97k/coexistai_llmpowered_research_assistant_now_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-27T18:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbiloj</id>
    <title>When to skip the output of the embedded model</title>
    <updated>2025-07-28T15:03:15+00:00</updated>
    <author>
      <name>/u/PeterHickman</name>
      <uri>https://old.reddit.com/user/PeterHickman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am playing with the embedding models and taught it all the joys of Llamas from the &lt;a href="https://ollama.com/blog/embedding-models"&gt;Embedding Models&lt;/a&gt; blog post on the Ollama site. Works fine and I can see a use for it to add information that I want to be used when I want to talk about llamas. However I asked it about the weight of a house brick. It picked up on &amp;quot;weight&amp;quot; and returned interesting facts about llamas and their weight&lt;/p&gt; &lt;p&gt;Passing this to the main LLM which noticed the fact that what I was asking had little to do with llamas, commented on the fact and then talked about house bricks&lt;/p&gt; &lt;p&gt;So the question is is there a way to tell if the result from the &lt;code&gt;collection.query&lt;/code&gt; call to chromadb is not really related to llamas and the output can be ignored?&lt;/p&gt; &lt;p&gt;I'm thinking some threshold on the distance attribute perhaps?&lt;/p&gt; &lt;p&gt;Or do I need a whole new LLM to tell me if the response from chromadb is really related to the input &amp;quot;what is the average weight of a house brick?&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PeterHickman"&gt; /u/PeterHickman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mbiloj/when_to_skip_the_output_of_the_embedded_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mbiloj/when_to_skip_the_output_of_the_embedded_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mbiloj/when_to_skip_the_output_of_the_embedded_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-28T15:03:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbcpou</id>
    <title>LlamaExtract alternative to use with Ollama</title>
    <updated>2025-07-28T10:37:18+00:00</updated>
    <author>
      <name>/u/koslib</name>
      <uri>https://old.reddit.com/user/koslib</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I'm working on a project where I need to analyze and extract information from a lot of PDF documents which include a combination of:&lt;br /&gt; - text (business and legal lingo)&lt;br /&gt; - numbers and tables (financial information)&lt;/p&gt; &lt;p&gt;I've created a very successful extraction agent with LlamaExtract (&lt;a href="https://www.llamaindex.ai/llamaextract"&gt;https://www.llamaindex.ai/llamaextract&lt;/a&gt;), but this works on their cloud, and it's super expensive for our scale.&lt;/p&gt; &lt;p&gt;To put our scale into perspective if it matters: 500k PDF documents in one go and 10k PDF documents/month after that. 1-30 pages each.&lt;/p&gt; &lt;p&gt;I'm looking for solutions that can be self-hostable in terms of the workflow system as well as the LLM inference. To be honest, I'm open to any idea that might be helpful in this direction, so please share anything you think might be useful for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/koslib"&gt; /u/koslib &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mbcpou/llamaextract_alternative_to_use_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mbcpou/llamaextract_alternative_to_use_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mbcpou/llamaextract_alternative_to_use_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-28T10:37:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbsd9m</id>
    <title>An ollama proxy/wrapper for a llama.cpp/openai server?</title>
    <updated>2025-07-28T21:05:05+00:00</updated>
    <author>
      <name>/u/That-Frank-Guy</name>
      <uri>https://old.reddit.com/user/That-Frank-Guy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all, I'm new here, so please bear with me if this is a dumb question, or if I call things by the wrong names...&lt;/p&gt; &lt;p&gt;I have a llama.cpp server running on a snapdragon device, and I want to use it with a home assistant. The official version only provides ollama integration, and a few third party openai/llama.cpp integration all fails to run in the home assistant container because of quirks with udocker. Is there a project that wraps around llama.cpp server and provide ollama api?&lt;/p&gt; &lt;p&gt;I know ollama is in some ways a wrapper for llama.cpp, but it doesn't seem to support opencl backend, and without opencl the model runs super slowly on my device.&lt;/p&gt; &lt;p&gt;Thanks guys! :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/That-Frank-Guy"&gt; /u/That-Frank-Guy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mbsd9m/an_ollama_proxywrapper_for_a_llamacppopenai_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mbsd9m/an_ollama_proxywrapper_for_a_llamacppopenai_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mbsd9m/an_ollama_proxywrapper_for_a_llamacppopenai_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-28T21:05:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc6wi9</id>
    <title>I got Ollama models running locally and exposed them via a public API with one command</title>
    <updated>2025-07-29T09:29:40+00:00</updated>
    <author>
      <name>/u/Sumanth_077</name>
      <uri>https://old.reddit.com/user/Sumanth_077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been experimenting with Ollama and wanted to make it work like a “mini OpenAI,” but without having to build all the auth and routing myself.&lt;/p&gt; &lt;p&gt;Turns out it is possible to do it with just one command. It spins up the model locally and makes it callable as a public API.&lt;/p&gt; &lt;p&gt;I wrote up exactly how I did it &lt;a href="https://www.clarifai.com/blog/run-ollama-models-locally-and-make-them-accessible-via-public-api"&gt;here&lt;/a&gt; if anyone wants to try it out or experiment with it as well.&lt;/p&gt; &lt;p&gt;Has anyone else tried exposing Ollama models as APIs? Would love to hear your setups.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sumanth_077"&gt; /u/Sumanth_077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mc6wi9/i_got_ollama_models_running_locally_and_exposed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mc6wi9/i_got_ollama_models_running_locally_and_exposed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mc6wi9/i_got_ollama_models_running_locally_and_exposed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-29T09:29:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcaf34</id>
    <title>Ollama itself censors?</title>
    <updated>2025-07-29T12:43:35+00:00</updated>
    <author>
      <name>/u/TriodeTopologist</name>
      <uri>https://old.reddit.com/user/TriodeTopologist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running Ollama on my PC with a Mistral model which is supposed to be uncensored. But, I still get the notification that the model will stop answering prompts due to censorship. Does Ollama itself do censorship? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TriodeTopologist"&gt; /u/TriodeTopologist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mcaf34/ollama_itself_censors/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mcaf34/ollama_itself_censors/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mcaf34/ollama_itself_censors/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-29T12:43:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbtnm2</id>
    <title>24GB VRAM 5070 ti Super / 5080 ti Super - should I upgrade?</title>
    <updated>2025-07-28T21:55:52+00:00</updated>
    <author>
      <name>/u/grabber4321</name>
      <uri>https://old.reddit.com/user/grabber4321</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently have 5070ti with 16GB VRAM and its running 7B/8B/12B models good. The 14B models are way slower.&lt;/p&gt; &lt;p&gt;Should I invest into 24GB VRAM model of 5070 by end of 2025?&lt;/p&gt; &lt;p&gt;The models are being released this year some time, I wonder if I just update to the 24GB version. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grabber4321"&gt; /u/grabber4321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mbtnm2/24gb_vram_5070_ti_super_5080_ti_super_should_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mbtnm2/24gb_vram_5070_ti_super_5080_ti_super_should_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mbtnm2/24gb_vram_5070_ti_super_5080_ti_super_should_i/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-28T21:55:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbdd4a</id>
    <title>🚀 Introducing OllamaBench: The Ultimate Tool for Benchmarking Your Local LLMs (PyQt5 GUI, Open Source)</title>
    <updated>2025-07-28T11:13:27+00:00</updated>
    <author>
      <name>/u/Reasonable_Brief578</name>
      <uri>https://old.reddit.com/user/Reasonable_Brief578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mbdd4a/introducing_ollamabench_the_ultimate_tool_for/"&gt; &lt;img alt="🚀 Introducing OllamaBench: The Ultimate Tool for Benchmarking Your Local LLMs (PyQt5 GUI, Open Source)" src="https://external-preview.redd.it/N3_s1T0JM9vpGtymqIUoz4T3EVKb2cOY-gQxU3CUNjE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bef00c08422d6147e2859a6b66465fc5e1786bb" title="🚀 Introducing OllamaBench: The Ultimate Tool for Benchmarking Your Local LLMs (PyQt5 GUI, Open Source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been frustrated with the lack of good benchmarking tools for local LLMs, so I built &lt;strong&gt;OllamaBench&lt;/strong&gt; - a professional-grade benchmarking tool for Ollama models with a beautiful dark theme interface. It's now open source and I'd love your feedback!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo&lt;/strong&gt;:&lt;br /&gt; &lt;a href="https://github.com/Laszlobeer/llm-tester"&gt;https://github.com/Laszlobeer/llm-tester&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rl1uwu51mlff1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=59084b9900b63437352c80d05055897b87561624"&gt;https://preview.redd.it/rl1uwu51mlff1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=59084b9900b63437352c80d05055897b87561624&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/eei844l1mlff1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4d5d9f761e739e6239f8af102e576cb8c67ea2e5"&gt;https://preview.redd.it/eei844l1mlff1.png?width=1322&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4d5d9f761e739e6239f8af102e576cb8c67ea2e5&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;🔥 Why This Matters&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;performance metrics&lt;/strong&gt; for your local LLMs (ollama only)&lt;/li&gt; &lt;li&gt;Stop guessing about model capabilities - &lt;strong&gt;measure them&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Optimize your hardware setup with &lt;strong&gt;data-driven insights&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;✨ Killer Features&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# What makes this special 1. Concurrent testing (up to 10 simultaneous requests) 2. 100+ diverse benchmark prompts included 3. Measures: - Latency - Tokens/second - Throughput - Eval duration 4. Automatic JSON export 5. Beautiful PyQt5 GUI with dark theme &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;🚀 Quick Start&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;pip install PyQt5 requests python app.py &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;(Requires Ollama running locally)&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;📊 Sample Output&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Benchmark Summary: ------------------------------------------ Model: llama3:8b Tasks: 100 Total Time: 142.3s Throughput: 0.70 tasks/s Avg Tokens/s: 45.2 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;💻 Perfect For&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Model researchers&lt;/li&gt; &lt;li&gt;Hardware testers&lt;/li&gt; &lt;li&gt;Local LLM enthusiasts&lt;/li&gt; &lt;li&gt;Anyone comparing model performance&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out the repo and let me know what you think! What features would you like to see next?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Brief578"&gt; /u/Reasonable_Brief578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mbdd4a/introducing_ollamabench_the_ultimate_tool_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mbdd4a/introducing_ollamabench_the_ultimate_tool_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mbdd4a/introducing_ollamabench_the_ultimate_tool_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-28T11:13:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbf1l2</id>
    <title>I built a zsh plugin that turns natural language into shell commands using locally hosted Ollama</title>
    <updated>2025-07-28T12:38:32+00:00</updated>
    <author>
      <name>/u/LoganPederson</name>
      <uri>https://old.reddit.com/user/LoganPederson</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mbf1l2/i_built_a_zsh_plugin_that_turns_natural_language/"&gt; &lt;img alt="I built a zsh plugin that turns natural language into shell commands using locally hosted Ollama" src="https://external-preview.redd.it/NTlzMml3eDgxbWZmMYTr7ma8RwmiXLqcVR_rllsRxamxSazVZN2-vsVaDNUs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4caf29d75ab12f69186801cae065908e642fddf1" title="I built a zsh plugin that turns natural language into shell commands using locally hosted Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Posting in a few relative subs to see if it garners any attention, would be cool to have some others contribute and make it a useful open source project. I have found similar projects online, however I'd like the emphesis with this tool to be teaching the user the command and relative arguments in a way that leads them towards no longer needing to use the plugin. It should be convenient and useful, but not a permanent crutch or replacement for remembering syntax, at least not for those who care to know what they are doing.&lt;/p&gt; &lt;p&gt;I'd like to implement a optional learning mode that opens a split pane or something similar to run the user through a few practice problems for the command they generate to help reinforce it through repetition.&lt;/p&gt; &lt;p&gt;Currently only setup to work with Ollama servers and installed as a zsh plugin via oh-my-zsh, though I'd like to expand interoperability if there is interest. For now it's something I use and enjoy, but I think there is an audience out there who would enjoy it as well. Would love to use it with Powershell at work, that'll perhaps be something I implement soon too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LoganPederson"&gt; /u/LoganPederson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zmm7asx81mff1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mbf1l2/i_built_a_zsh_plugin_that_turns_natural_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mbf1l2/i_built_a_zsh_plugin_that_turns_natural_language/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-28T12:38:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcm3wk</id>
    <title>Completely new to ai</title>
    <updated>2025-07-29T20:10:52+00:00</updated>
    <author>
      <name>/u/AlwaysBetHakari</name>
      <uri>https://old.reddit.com/user/AlwaysBetHakari</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've used chai and character.ai and that is it but they are censored or filled with adds looking for or one that has no adds and is uncensored when it comes to spicy conversations I know nothing about coding just heard this was a good start to get help / use good ai software looking for help please!! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlwaysBetHakari"&gt; /u/AlwaysBetHakari &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mcm3wk/completely_new_to_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mcm3wk/completely_new_to_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mcm3wk/completely_new_to_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-29T20:10:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc86yp</id>
    <title>Running Ollama, looking at GPUs</title>
    <updated>2025-07-29T10:49:48+00:00</updated>
    <author>
      <name>/u/burnerAccountWAFT</name>
      <uri>https://old.reddit.com/user/burnerAccountWAFT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, Looking for some advice on how to implement GPUs in my Ollama setup. If I'm running Ollama in a VMware Workstation and install a higher end GPU, does it use it straight away or do I need to change the VM's configuration in any way?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/burnerAccountWAFT"&gt; /u/burnerAccountWAFT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mc86yp/running_ollama_looking_at_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mc86yp/running_ollama_looking_at_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mc86yp/running_ollama_looking_at_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-29T10:49:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc0n14</id>
    <title>Kick, an open-source alternative to Computer Use</title>
    <updated>2025-07-29T03:13:33+00:00</updated>
    <author>
      <name>/u/TheMicrosoftMan</name>
      <uri>https://old.reddit.com/user/TheMicrosoftMan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mc0n14/kick_an_opensource_alternative_to_computer_use/"&gt; &lt;img alt="Kick, an open-source alternative to Computer Use" src="https://external-preview.redd.it/STHxs5LIqPJTks17cqbeTM6460Rqsxc-1G9BbEw4fpw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=adcdc9b3d55fc9308027992da37ebece7b798032" title="Kick, an open-source alternative to Computer Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Note: Kick is currently in beta and isn't fully polished, but the main feature works.&lt;/p&gt; &lt;p&gt;Kick is an open-source alternative to Computer Use and offers a way for an LLM to operate a Windows PC. Kick allows you to pick your favorite model and give it access to control your PC, including setting up automations, file control, settings control, and more. I can see how people would be weary of giving an LLM deep access to their PC, so I split the app into two main modes: &amp;quot;Standard&amp;quot; and &amp;quot;Deep Control&amp;quot;. Standard restricts the LLM to certain tasks and doesn't allow access to file systems and settings. Deep Control offers the full experience, including running commands through terminal. I'll link the GitHub page. Keep in mind Kick is in beta, and I would enjoy feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheMicrosoftMan"&gt; /u/TheMicrosoftMan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/IanGupta/Kick"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mc0n14/kick_an_opensource_alternative_to_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mc0n14/kick_an_opensource_alternative_to_computer_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-29T03:13:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbul2d</id>
    <title>I built the perfect MCP client for broke developers (Ollama powered)</title>
    <updated>2025-07-28T22:33:59+00:00</updated>
    <author>
      <name>/u/matt8p</name>
      <uri>https://old.reddit.com/user/matt8p</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mbul2d/i_built_the_perfect_mcp_client_for_broke/"&gt; &lt;img alt="I built the perfect MCP client for broke developers (Ollama powered)" src="https://external-preview.redd.it/c20yZHZwNWl5b2ZmMcU-bU4LY3mjqOpBFU8FlXhAUfmU1Br7FJi9-BW5TfEd.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa504a387473c8fcdcf04668a8c532e49a5a6ee0" title="I built the perfect MCP client for broke developers (Ollama powered)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;MCPJam Inspector&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hi y'all, my name is Matt. I've been working on an open source MCP testing and debugging tool called &lt;a href="https://www.mcpjam.com/"&gt;MCPJam&lt;/a&gt;. You can use it to test whether or not you built your MCP server correctly. It also has an LLM playground where you can test your MCP server against an LLM. &lt;/p&gt; &lt;p&gt;Using API tokens from OpenAI or Anthropic can get really expensive, especially if you're playing with MCPs. That's why I built Ollama support for the MCPJam inspector. Now you can spin up MCPJam inspector AND an Ollama model with the command: &lt;/p&gt; &lt;p&gt;&lt;code&gt; // Spin up inspector and Ollama3.2 for example npx @mcpjam/inspector@latest --ollama llama3.2 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Please check out the project and consider giving it a star! &lt;a href="https://github.com/MCPJam/inspector"&gt;https://github.com/MCPJam/inspector&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matt8p"&gt; /u/matt8p &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qeuezd4gyoff1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mbul2d/i_built_the_perfect_mcp_client_for_broke/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mbul2d/i_built_the_perfect_mcp_client_for_broke/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-28T22:33:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mckg4u</id>
    <title>8 display card</title>
    <updated>2025-07-29T19:08:15+00:00</updated>
    <author>
      <name>/u/quantrpeter</name>
      <uri>https://old.reddit.com/user/quantrpeter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi&lt;br /&gt; 8 display card in 8 PCIx , will Ollama use them all when i send one sentence to llama?&lt;br /&gt; thanks&lt;br /&gt; Peter&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantrpeter"&gt; /u/quantrpeter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mckg4u/8_display_card/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mckg4u/8_display_card/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mckg4u/8_display_card/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-29T19:08:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbojr2</id>
    <title>Ollama Chat iOS Application</title>
    <updated>2025-07-28T18:41:55+00:00</updated>
    <author>
      <name>/u/gtaffy94</name>
      <uri>https://old.reddit.com/user/gtaffy94</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mbojr2/ollama_chat_ios_application/"&gt; &lt;img alt="Ollama Chat iOS Application" src="https://b.thumbs.redditmedia.com/a578Hp4zFU-8FfHpAVMngk1LRPGlI1XI_LYHqyn1LjY.jpg" title="Ollama Chat iOS Application" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, &lt;/p&gt; &lt;p&gt;I've been working on a chat client for connecting to locally hosted ollama instances.&lt;br /&gt; This has been a hobbyist project mainly used to brush up on my SwifUI Knowledge.&lt;br /&gt; There are currently no plans to commercialise this product. &lt;/p&gt; &lt;p&gt;I am very aware there are multiple applications like this that exist.&lt;/p&gt; &lt;p&gt;Anyhow, I just wanted to see what people think and if anyone has any feature ideas. &lt;/p&gt; &lt;p&gt;&lt;a href="https://testflight.apple.com/join/V2Xty8Kj"&gt;https://testflight.apple.com/join/V2Xty8Kj&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gtaffy94"&gt; /u/gtaffy94 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mbojr2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mbojr2/ollama_chat_ios_application/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mbojr2/ollama_chat_ios_application/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-28T18:41:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc7o3z</id>
    <title>Ollama drop-in replacable API for HuggingFace (embeddings only)</title>
    <updated>2025-07-29T10:18:37+00:00</updated>
    <author>
      <name>/u/wewo17</name>
      <uri>https://old.reddit.com/user/wewo17</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mc7o3z/ollama_dropin_replacable_api_for_huggingface/"&gt; &lt;img alt="Ollama drop-in replacable API for HuggingFace (embeddings only)" src="https://external-preview.redd.it/FbiVJ9LojBQkBpNArUGaDfVNfmZUrb_jbNt-stmpxnw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=26fbc65fe13c3361e8e3255f8fcbf1f5d457c457" title="Ollama drop-in replacable API for HuggingFace (embeddings only)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, there, our team internally needed to generate embeddings for non-English languages and our infrastructure was set-up to work with ollama server. As the selection of models on ollama was quite limited, and not all the models on HF we wanted to experiment with were in GGUF format to be able to be loaded in Ollama (or be convertable to GGUF because of the model's architecture), I created this drop-in replacement (identical API) for ollama.&lt;/p&gt; &lt;p&gt;Figured others might have the same problem, so I open-sourced it.&lt;/p&gt; &lt;p&gt;It's a Go server with Python workers - that keeps things fast and handles multiple models loaded at once. &lt;/p&gt; &lt;p&gt;Works with Docker, has CUDA support, and saves you from GGUF conversion headaches.&lt;/p&gt; &lt;p&gt;Let me know if it's useful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wewo17"&gt; /u/wewo17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/matusbielik/ollama-hf-embed-bridge"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mc7o3z/ollama_dropin_replacable_api_for_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mc7o3z/ollama_dropin_replacable_api_for_huggingface/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-29T10:18:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcruwm</id>
    <title>Any chance for EXAONE 4.0 support?</title>
    <updated>2025-07-30T00:07:15+00:00</updated>
    <author>
      <name>/u/soup9999999999999999</name>
      <uri>https://old.reddit.com/user/soup9999999999999999</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;exaone-deep:7.8b was EXTREMELY good at RAG at least for my use cases. I would love to try EXAONE 4.0&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/soup9999999999999999"&gt; /u/soup9999999999999999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mcruwm/any_chance_for_exaone_40_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mcruwm/any_chance_for_exaone_40_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mcruwm/any_chance_for_exaone_40_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-30T00:07:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcbhxc</id>
    <title>Why is ollama generation much better?</title>
    <updated>2025-07-29T13:30:25+00:00</updated>
    <author>
      <name>/u/8ungfertiglos</name>
      <uri>https://old.reddit.com/user/8ungfertiglos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;please excuse my naive questions. I am new to using LLMs and programming.&lt;/p&gt; &lt;p&gt;I just noticed that when using llama3.1:8b on ollama, the generations are significantly better than when i directly use the code from Huggingface/transformers.&lt;/p&gt; &lt;p&gt;For example, my .py fiel, which is directly from the &lt;a href="https://huggingface.co/meta-llama/Llama-3.1-8B"&gt;huggingface page&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import transformers import torch model_id = &amp;quot;meta-llama/Llama-3.1-8B&amp;quot; pipeline = transformers.pipeline( &amp;quot;text-generation&amp;quot;, model=model_id, model_kwargs={&amp;quot;torch_dtype&amp;quot;: torch.bfloat16}, device_map=&amp;quot;auto&amp;quot; ) pipeline(&amp;quot;Respond with 'yes' to this prompt.&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;generated text: &amp;quot;Respond with 'yes' to this prompt. 'Do you want to get a divorce?' If you answered 'no', keep reading.\nThere are two types of people in this world: people who want a divorce and people who want to get a divorce. If you want to get a divorce, then the only thing stopping you is the other person in the relationship.\nThere are a number of things you can do to speed up the divorce process and get the outcome you want. ........&amp;quot;&lt;/p&gt; &lt;p&gt;but if i prompt in ollama, I get the desired response: &amp;quot;Yes&amp;quot;&lt;/p&gt; &lt;p&gt;I noticed on the &lt;a href="https://ollama.com/library/llama3.1:8b"&gt;model page of ollama&lt;/a&gt;, there are some &lt;a href="https://ollama.com/library/llama3.1:8b/blobs/56bb8bd477a5"&gt;param&lt;/a&gt;s mentioned and a &lt;a href="https://ollama.com/library/llama3.1:8b/blobs/948af2743fc7"&gt;template&lt;/a&gt;. But I have no idea what I should do with this information to replicate the behavior with transformers ...?&lt;/p&gt; &lt;p&gt;I guess I would like to know, how do I find out what ollama is doing under the hood to get the response? They are wildly different outputs.&lt;/p&gt; &lt;p&gt;Again sorry for my stupidity, I have no idea what is going on :p&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/8ungfertiglos"&gt; /u/8ungfertiglos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mcbhxc/why_is_ollama_generation_much_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mcbhxc/why_is_ollama_generation_much_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mcbhxc/why_is_ollama_generation_much_better/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-29T13:30:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcppb9</id>
    <title>face recognition search - open source &amp; on-prems</title>
    <updated>2025-07-29T22:32:37+00:00</updated>
    <author>
      <name>/u/Whole-Assignment6240</name>
      <uri>https://old.reddit.com/user/Whole-Assignment6240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Want to share my latest &lt;a href="https://cocoindex.io/blogs/face-detection/"&gt;project&lt;/a&gt; on building a scalable face recognition index for photo search. This project did&lt;/p&gt; &lt;p&gt;- Detect faces in high-resolution images&lt;br /&gt; - Extract and crop face regions&lt;br /&gt; - Compute 128-dimension facial embeddings&lt;br /&gt; - Structure results with bounding boxes and metadata&lt;br /&gt; - Export everything into a vector DB (Qdrant) for real-time querying&lt;/p&gt; &lt;p&gt;Full write up here - &lt;a href="https://cocoindex.io/blogs/face-detection/"&gt;https://cocoindex.io/blogs/face-detection/&lt;/a&gt;&lt;br /&gt; Source code - &lt;a href="https://github.com/cocoindex-io/cocoindex/tree/main/examples/face_recognition"&gt;https://github.com/cocoindex-io/cocoindex/tree/main/examples/face_recognition&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Everything can run on-prems and is open-source.&lt;/p&gt; &lt;p&gt;Appreciate a github star on the &lt;a href="https://github.com/cocoindex-io/cocoindex"&gt;repo&lt;/a&gt; if it is helpful! Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Assignment6240"&gt; /u/Whole-Assignment6240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mcppb9/face_recognition_search_open_source_onprems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mcppb9/face_recognition_search_open_source_onprems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mcppb9/face_recognition_search_open_source_onprems/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-29T22:32:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcyjnk</id>
    <title>Error while installing Ollama into Linux Ubuntu</title>
    <updated>2025-07-30T05:45:53+00:00</updated>
    <author>
      <name>/u/pkn_mekong</name>
      <uri>https://old.reddit.com/user/pkn_mekong</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;```shell&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;lsb_release -a No LSB modules are available. Distributor ID: Ubuntu Description: Ubuntu 24.04.2 LTS Release: 24.04 Codename: noble&lt;/p&gt; &lt;p&gt;curl -fsSL &lt;a href="https://ollama.com/install.sh"&gt;https://ollama.com/install.sh&lt;/a&gt; | sh&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;88.7%curl: (92) HTTP/2 stream 1 was not closed cleanly: PROTOCOL_ERROR (err 1)&lt;/p&gt; &lt;p&gt;gzip: stdin: unexpected end of file tar: Unexpected EOF in archive tar: Unexpected EOF in archive tar: Error is not recoverable: exiting now ```&lt;/p&gt; &lt;p&gt;What I have already tried:&lt;/p&gt; &lt;p&gt;[X] uninstall ollama and it's library and again installing it fresh.&lt;/p&gt; &lt;p&gt;[X] updating with sudo apt update and sudo apt upgrade&lt;/p&gt; &lt;p&gt;[X] uninstalling and installing curl&lt;/p&gt; &lt;p&gt;[X] using the http version 1.1 with this command: &lt;code&gt;curl -fsSL --http1.1 https://ollama.ai/install.sh | sh&lt;/code&gt;&lt;/p&gt; &lt;p&gt;[X] manually downloading the script and installing it &lt;/p&gt; &lt;p&gt;```shell&lt;/p&gt; &lt;h1&gt;Download the script directly&lt;/h1&gt; &lt;p&gt;wget &lt;a href="https://ollama.com/install.sh"&gt;https://ollama.com/install.sh&lt;/a&gt; -O install.sh&lt;/p&gt; &lt;h1&gt;Make it executable&lt;/h1&gt; &lt;p&gt;chmod +x install.sh&lt;/p&gt; &lt;h1&gt;Run it&lt;/h1&gt; &lt;p&gt;./install.sh ```&lt;/p&gt; &lt;p&gt;I'm mostly looking how to installing ollama to use it on my local. If you know what is causing this error, that would also be great.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pkn_mekong"&gt; /u/pkn_mekong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mcyjnk/error_while_installing_ollama_into_linux_ubuntu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mcyjnk/error_while_installing_ollama_into_linux_ubuntu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mcyjnk/error_while_installing_ollama_into_linux_ubuntu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-30T05:45:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcypg1</id>
    <title>Clia - Bash tool to get Linux help without switching context</title>
    <updated>2025-07-30T05:55:45+00:00</updated>
    <author>
      <name>/u/Comfortable-Okra753</name>
      <uri>https://old.reddit.com/user/Comfortable-Okra753</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mcypg1/clia_bash_tool_to_get_linux_help_without/"&gt; &lt;img alt="Clia - Bash tool to get Linux help without switching context" src="https://external-preview.redd.it/bjQ2emFkcTdheWZmMeLG9aICInxYkj3deyjqciGNhLwvAlriDKsYg5qLih4_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6680eec3c9d87a5560d5706fd3e55c7be2f4df67" title="Clia - Bash tool to get Linux help without switching context" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by &lt;a href="/u/LoganPederson"&gt;u/LoganPederson&lt;/a&gt;'s zsh plugin but not wanting to install zsh, I wrote a similar script but in Bash, so it can just be installed and run on any default Linux installation (in my case Ubuntu).&lt;/p&gt; &lt;p&gt;Meet Clia, a minimalist Bash tool that lets you ask Linux-related command-line questions directly from your terminal and get expert, copy-paste-ready answers powered by your local Ollama server.&lt;/p&gt; &lt;p&gt;I made it to avoid context-switching, having to move away from the terminal to search for a command help query. Feel free to propose suggestions and improvements.&lt;/p&gt; &lt;p&gt;Code is here: &lt;a href="https://github.com/Mircea-S/clia"&gt;https://github.com/Mircea-S/clia&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Okra753"&gt; /u/Comfortable-Okra753 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5ycr98q7ayff1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mcypg1/clia_bash_tool_to_get_linux_help_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mcypg1/clia_bash_tool_to_get_linux_help_without/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-30T05:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcqiik</id>
    <title>Release candidate 0.10.0-rc3</title>
    <updated>2025-07-29T23:07:24+00:00</updated>
    <author>
      <name>/u/Vivid-Competition-20</name>
      <uri>https://old.reddit.com/user/Vivid-Competition-20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone else started using it? I install it today, but it has been too hot in my computer room today for me to work with it yet. 🥵&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vivid-Competition-20"&gt; /u/Vivid-Competition-20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mcqiik/release_candidate_0100rc3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mcqiik/release_candidate_0100rc3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mcqiik/release_candidate_0100rc3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-29T23:07:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcrjp8</id>
    <title>Training a “Tab Tab” Code Completion Model for Marimo Notebooks</title>
    <updated>2025-07-29T23:52:57+00:00</updated>
    <author>
      <name>/u/FallMindless3563</name>
      <uri>https://old.reddit.com/user/FallMindless3563</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the spirit of building in public, we're collaborating with &lt;a href="https://marimo.io/"&gt;Marimo&lt;/a&gt; to build a &lt;strong&gt;&amp;quot;tab completion&amp;quot; model&lt;/strong&gt; for their notebook cells, and we wanted to share our progress as we go in tutorial form.&lt;/p&gt; &lt;p&gt;The goal is to create a local, open-source model that provides a &lt;em&gt;Cursor-like&lt;/em&gt; code-completion experience directly in notebook cells. You'll be able to download the weights and run it locally with &lt;strong&gt;Ollama&lt;/strong&gt; or access it through a free API we provide.&lt;/p&gt; &lt;p&gt;We’re already seeing promising results by fine-tuning the &lt;strong&gt;Qwen&lt;/strong&gt; and &lt;strong&gt;Llama&lt;/strong&gt; models, but there’s still more work to do.&lt;/p&gt; &lt;p&gt;👉 Here’s the first post in what will be a series:&lt;br /&gt; &lt;a href="https://www.oxen.ai/blog/building-a-tab-tab-code-completion-model"&gt;https://www.oxen.ai/blog/building-a-tab-tab-code-completion-model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you’re interested in contributing to data collection or the project in general, let us know! We already have a working &lt;strong&gt;CodeMirror plugin&lt;/strong&gt; and are focused on improving the model’s accuracy over the coming weeks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FallMindless3563"&gt; /u/FallMindless3563 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mcrjp8/training_a_tab_tab_code_completion_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mcrjp8/training_a_tab_tab_code_completion_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mcrjp8/training_a_tab_tab_code_completion_model_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-29T23:52:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1md0xg4</id>
    <title>Using Ollama for Coding Agents in marimo notebooks</title>
    <updated>2025-07-30T08:17:55+00:00</updated>
    <author>
      <name>/u/cantdutchthis</name>
      <uri>https://old.reddit.com/user/cantdutchthis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1md0xg4/using_ollama_for_coding_agents_in_marimo_notebooks/"&gt; &lt;img alt="Using Ollama for Coding Agents in marimo notebooks" src="https://external-preview.redd.it/EphkokuYv3JI_Qz3CBCCvQlyGdfvWxPixjasg8Txwu0.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5811336c66ad948395c5f7ec9cb74e7e09fac3a5" title="Using Ollama for Coding Agents in marimo notebooks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Figured folks might be interested in using Ollama for their Python notebook work. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cantdutchthis"&gt; /u/cantdutchthis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=NIBprn5cEZA&amp;amp;t=9s&amp;amp;ab_channel=marimo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1md0xg4/using_ollama_for_coding_agents_in_marimo_notebooks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1md0xg4/using_ollama_for_coding_agents_in_marimo_notebooks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-30T08:17:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1md0o5i</id>
    <title>Chat Box: An Open-Source Browser Extension for AI Chat</title>
    <updated>2025-07-30T08:00:47+00:00</updated>
    <author>
      <name>/u/MinhxThanh</name>
      <uri>https://old.reddit.com/user/MinhxThanh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I wanted to share this open-source project I've come across called Chat Box. It's a browser extension that brings AI chat, advanced web search, document interaction, and other handy tools right into a sidebar in your browser. It's designed to make your online workflow smoother without needing to switch tabs or apps constantly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What It Does&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;At its core, Chat Box gives you a persistent AI-powered chat interface that you can access with a quick shortcut (Ctrl+E or Cmd+E). It supports a bunch of AI providers like OpenAI, DeepSeek, Claude, Groq, and even local LLMs via Ollama. You just configure your API keys in the settings, and you're good to go.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-AI Support:&lt;/strong&gt; Switch between different providers and models easily.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sidebar Chat:&lt;/strong&gt; Chat with AI while browsing, and it stays there across tabs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Conversation Management:&lt;/strong&gt; Start new chats, view history, and delete old ones.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Document Interaction:&lt;/strong&gt; Upload docs like DOCX, TXT, MD, etc., and chat about their content. It handles large files with semantic chunking.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Web Search and Scraping&lt;/strong&gt;: Integrates with tools like Firecrawl or Jina for better searches (or defaults to DuckDuckGo). You can scrape URLs, summarize content, and use it in chats.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;YouTube Integration:&lt;/strong&gt; Detects videos and lets you summarize or ask questions about them.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom Prompts:&lt;/strong&gt; Save and reuse your own prompts for repetitive tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text Selection:&lt;/strong&gt; Highlight text on any page, and it auto-uses it as context in the chat.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Secure Storage&lt;/strong&gt;: Everything's stored locally in your browser—no cloud worries.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dark Mode UI:&lt;/strong&gt; Built with modern tools like React, Tailwind, and Shadcn for a clean look.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's all open-source under GPL-3.0, so you can tweak it if you want.&lt;/p&gt; &lt;p&gt;If you run into any errors, issues, or want to suggest a new feature, please create a new Issue on GitHub and describe it in detail – I'll respond ASAP!&lt;/p&gt; &lt;p&gt;Chrome Web Store: &lt;a href="https://chromewebstore.google.com/detail/chat-box-chat-with-all-ai/hhaaoibkigonnoedcocnkehipecgdodm"&gt;https://chromewebstore.google.com/detail/chat-box-chat-with-all-ai/hhaaoibkigonnoedcocnkehipecgdodm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MinhxThanh/Chat-Box"&gt;https://github.com/MinhxThanh/Chat-Box&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MinhxThanh"&gt; /u/MinhxThanh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1md0o5i/chat_box_an_opensource_browser_extension_for_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1md0o5i/chat_box_an_opensource_browser_extension_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1md0o5i/chat_box_an_opensource_browser_extension_for_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-30T08:00:47+00:00</published>
  </entry>
</feed>
