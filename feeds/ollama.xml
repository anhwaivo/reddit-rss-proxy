<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-05-23T18:41:00+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1kqo8ze</id>
    <title>Observer Micro Agents with Ollama demo!</title>
    <updated>2025-05-19T21:48:02+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kqo8ze/observer_micro_agents_with_ollama_demo/"&gt; &lt;img alt="Observer Micro Agents with Ollama demo!" src="https://external-preview.redd.it/Y3hqdjV4aDY3dDFmMUseoVY8fQTbYJfjqlW4w2NBhsFRYZKCiBtmbkUNYsUI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3a649ad3d73eb21cc664740a837af5243bd5ff2" title="Observer Micro Agents with Ollama demo!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/slzlvvh67t1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqo8ze/observer_micro_agents_with_ollama_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kqo8ze/observer_micro_agents_with_ollama_demo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-19T21:48:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1krk2ur</id>
    <title>Also new to OLLAMA .... have installed msty 19.2.9 not working</title>
    <updated>2025-05-20T23:59:30+00:00</updated>
    <author>
      <name>/u/Reasonable-Watch-497</name>
      <uri>https://old.reddit.com/user/Reasonable-Watch-497</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have installed msty x64 19.2.0 first use even though I told it local model would not let me do any work and wanted an authorization key. the next set of installs the icons are created but no gui screen comes up. OS is windows 10 (I will update soon)...really need help on this issue....thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable-Watch-497"&gt; /u/Reasonable-Watch-497 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1krk2ur/also_new_to_ollama_have_installed_msty_1929_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1krk2ur/also_new_to_ollama_have_installed_msty_1929_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1krk2ur/also_new_to_ollama_have_installed_msty_1929_not/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-20T23:59:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1krgihs</id>
    <title>ClipAI: connect your clipboard to Ollama</title>
    <updated>2025-05-20T21:18:30+00:00</updated>
    <author>
      <name>/u/Comprehensive-Bird59</name>
      <uri>https://old.reddit.com/user/Comprehensive-Bird59</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1krgihs/clipai_connect_your_clipboard_to_ollama/"&gt; &lt;img alt="ClipAI: connect your clipboard to Ollama" src="https://a.thumbs.redditmedia.com/I2aeHPP-fP7SsR_oc_u_JXSDUVljd1JeHx3YG6gwct4.jpg" title="ClipAI: connect your clipboard to Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;CLAIM:&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;ClipAI&lt;/strong&gt; is a simple but powerful utility to connect your &lt;strong&gt;clipboard&lt;/strong&gt; üìã directly to a &lt;strong&gt;Local LLM&lt;/strong&gt; ü§ñ (Ollama-based) such as &lt;strong&gt;Gemma 3&lt;/strong&gt;, &lt;strong&gt;Phi 4&lt;/strong&gt;, &lt;strong&gt;Deepseek-V3&lt;/strong&gt;, &lt;strong&gt;Qwen&lt;/strong&gt;, &lt;strong&gt;Llama 3.x&lt;/strong&gt;, etc. It is a &lt;strong&gt;clipboard viewer&lt;/strong&gt; and &lt;strong&gt;text transformer&lt;/strong&gt; application built using &lt;strong&gt;Python&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It is your &lt;strong&gt;daily companion&lt;/strong&gt; for any &lt;strong&gt;writing-related job&lt;/strong&gt; ‚úèÔ∏èüìÑ. Easy peasy.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/91btjuf7802f1.png?width=204&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c9c86ccbc98810e91d85b75d530e67c6f1f51e9a"&gt;https://preview.redd.it/91btjuf7802f1.png?width=204&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c9c86ccbc98810e91d85b75d530e67c6f1f51e9a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;REALITY:&lt;/strong&gt;&lt;br /&gt; So, it‚Äôs the 100th application that implements a chat/interaction with an LLM, but I aimed for something really simple to &amp;quot;drag and drop&amp;quot; while working, obviously focused on writing.&lt;/p&gt; &lt;p&gt;I was having trouble translating text on the fly and now I use ClipAI, which is working well for me. It‚Äôs at least solved one of my problems!&lt;/p&gt; &lt;p&gt;Feedback are appreciated.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/markod0925/ClipAI"&gt;https://github.com/markod0925/ClipAI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pi0mjx3f802f1.png?width=702&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0589c3969f9381e6bdeb21fb65cec54a82266390"&gt;https://preview.redd.it/pi0mjx3f802f1.png?width=702&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0589c3969f9381e6bdeb21fb65cec54a82266390&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comprehensive-Bird59"&gt; /u/Comprehensive-Bird59 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1krgihs/clipai_connect_your_clipboard_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1krgihs/clipai_connect_your_clipboard_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1krgihs/clipai_connect_your_clipboard_to_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-20T21:18:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1krbg6i</id>
    <title>IDEA: Record your voice prompts, copy them straight into Ollama (100% local)</title>
    <updated>2025-05-20T17:54:03+00:00</updated>
    <author>
      <name>/u/lukerm_zl</name>
      <uri>https://old.reddit.com/user/lukerm_zl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've integrated a simple voice recorder with Ollama.&lt;/p&gt; &lt;p&gt;Hopefully useful. Let me know if you have any ideas to improve. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lukerm_zl"&gt; /u/lukerm_zl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/lukerm/vosk-dictation?tab=readme-ov-file#ollama-"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1krbg6i/idea_record_your_voice_prompts_copy_them_straight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1krbg6i/idea_record_your_voice_prompts_copy_them_straight/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-20T17:54:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1krnzxi</id>
    <title>Improvement in the ollama-python tool system: refactoring, organization and better support for AI context</title>
    <updated>2025-05-21T03:20:35+00:00</updated>
    <author>
      <name>/u/chavomodder</name>
      <uri>https://old.reddit.com/user/chavomodder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1krnzxi/improvement_in_the_ollamapython_tool_system/"&gt; &lt;img alt="Improvement in the ollama-python tool system: refactoring, organization and better support for AI context" src="https://external-preview.redd.it/d0pyMuGXhi1u8xGfnFw3LaCoEAGKKUnQIwS1Kup-IcU.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4761175bca0491db83b3f8f2f316c63c1e171664" title="Improvement in the ollama-python tool system: refactoring, organization and better support for AI context" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys!&lt;/p&gt; &lt;p&gt;Previously, I took the initiative to create decorators to facilitate tool registration in ollama-python, but I realized that some parts of the system were still poorly organized or unclear. So I decided to refactor and improve several points. Here are the main changes:&lt;/p&gt; &lt;p&gt;I created the _tools.py module to centralize everything related to tools&lt;/p&gt; &lt;p&gt;I renamed functions to clearer names&lt;/p&gt; &lt;p&gt;Fixed bugs and improved registration and tool search&lt;/p&gt; &lt;p&gt;I added support for extracting the name and description of tools, useful for the AI ‚Äã‚Äãcontext (example: you are an assistant and have access to the following tools {get_ollama_tool_description})&lt;/p&gt; &lt;p&gt;Docstrings are now used as description automatically&lt;/p&gt; &lt;p&gt;It will return something like: ({ &amp;quot;Calculator&amp;quot;: &amp;quot;calculates numbers&amp;quot; &amp;quot;search_web&amp;quot;: Performs searches on the web })&lt;/p&gt; &lt;p&gt;More modular and tested code with new test suite&lt;/p&gt; &lt;p&gt;These changes make the use of tools simpler and more efficient for those who develop with the library.&lt;/p&gt; &lt;p&gt;commit link: &lt;a href="https://github.com/ollama/ollama-python/pull/516/commits/49ed36bf4789c754102fc05d2f911bbec5ea9cc6"&gt;https://github.com/ollama/ollama-python/pull/516/commits/49ed36bf4789c754102fc05d2f911bbec5ea9cc6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chavomodder"&gt; /u/chavomodder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ollama/ollama-python/pull/516/commits/49ed36bf4789c754102fc05d2f911bbec5ea9cc6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1krnzxi/improvement_in_the_ollamapython_tool_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1krnzxi/improvement_in_the_ollamapython_tool_system/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-21T03:20:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqz8gn</id>
    <title>I trapped LLama3.2B into an art installation and made it question its own existence endlessly</title>
    <updated>2025-05-20T07:48:24+00:00</updated>
    <author>
      <name>/u/Dull-Pressure9628</name>
      <uri>https://old.reddit.com/user/Dull-Pressure9628</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kqz8gn/i_trapped_llama32b_into_an_art_installation_and/"&gt; &lt;img alt="I trapped LLama3.2B into an art installation and made it question its own existence endlessly" src="https://preview.redd.it/gnnlh2fh6w1f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbc376b11d27165a314895138fd608df2741cb0e" title="I trapped LLama3.2B into an art installation and made it question its own existence endlessly" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dull-Pressure9628"&gt; /u/Dull-Pressure9628 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gnnlh2fh6w1f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kqz8gn/i_trapped_llama32b_into_an_art_installation_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kqz8gn/i_trapped_llama32b_into_an_art_installation_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-20T07:48:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ks1358</id>
    <title>Anyone else getting garbage output from models after updating to 0.7?</title>
    <updated>2025-05-21T15:43:51+00:00</updated>
    <author>
      <name>/u/aaronr_90</name>
      <uri>https://old.reddit.com/user/aaronr_90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am on Ubuntu 22.04 and was using Codestral, Mistral Small and Qwen 2.5. All models responded as if a large needy can was prancing all over the keyboard. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aaronr_90"&gt; /u/aaronr_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ks1358/anyone_else_getting_garbage_output_from_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ks1358/anyone_else_getting_garbage_output_from_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ks1358/anyone_else_getting_garbage_output_from_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-21T15:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ks5860</id>
    <title>Advice on the AI/LLM "GPU triangle" - the tradeoffs between Price/Cost, Size (VRAM), and Speed</title>
    <updated>2025-05-21T18:28:23+00:00</updated>
    <author>
      <name>/u/TorrentRover</name>
      <uri>https://old.reddit.com/user/TorrentRover</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To begin with, I'm poor. I'm running a Lenovo PowerStation P520 with Xeon W-2145 and 1000w power supply with 2x PCIe x16 slots and 2x GPU (or EPS 12v) power drops.&lt;/p&gt; &lt;p&gt;Here are my current options:&lt;/p&gt; &lt;p&gt;2x RTX 3060 12GB cards (newish, lower spec, 24GB VRAM total)&lt;/p&gt; &lt;p&gt;or&lt;/p&gt; &lt;p&gt;2x Tesla K80 cards (old, low spec, 48GB VRAM total)&lt;/p&gt; &lt;p&gt;The tradeoffs are pretty obvious here. I have tested both. The 3060s gives me better inference speed but limit what models I can run due to lower VRAM. The K80s allow me to run larger models, but the performance is abismal.&lt;/p&gt; &lt;p&gt;Oh, and the power draw on the K80s is pretty insane. Resting with no model(s) loaded has 4x dies/chips (2x per card) hovering around 20-30w each (up to 120w) just idling. When a model is held in RAM, it can easily be 50-70w per chip/die. When running inference, it does hit the TDP of 149w each (nearly 600w total).&lt;/p&gt; &lt;p&gt;What would you choose? Why? Are there any similarly priced options I should be considering?&lt;/p&gt; &lt;p&gt;EDIT: I should have mentioned the software environment. I'm running Proxmox, and my ollama/Open Webui system is setup as a VM with Ubuntu 24.04.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TorrentRover"&gt; /u/TorrentRover &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ks5860/advice_on_the_aillm_gpu_triangle_the_tradeoffs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ks5860/advice_on_the_aillm_gpu_triangle_the_tradeoffs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ks5860/advice_on_the_aillm_gpu_triangle_the_tradeoffs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-21T18:28:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1krkhr7</id>
    <title>Parking Analysis with Object Detection and Ollama models for Report Generation</title>
    <updated>2025-05-21T00:19:22+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1krkhr7/parking_analysis_with_object_detection_and_ollama/"&gt; &lt;img alt="Parking Analysis with Object Detection and Ollama models for Report Generation" src="https://external-preview.redd.it/N2J0OW5oMjkzMTJmMeMfnSo893myclMRvg1dOF4kmROzcG9sBbtv4hMJoM_m.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2887d668e49b0e0dd586dbd35f4bab59f2c79dd4" title="Parking Analysis with Object Detection and Ollama models for Report Generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Reddit!&lt;/p&gt; &lt;p&gt;Been tinkering with a fun project combining computer vision and LLMs, and wanted to share the progress.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The gist:&lt;/strong&gt;&lt;br /&gt; It uses a YOLO model (via Roboflow) to do real-time object detection on a video feed of a parking lot, figuring out which spots are taken and which are free. You can see the little red/green boxes doing their thing in the video.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;But here's the (IMO) coolest part:&lt;/strong&gt; The system then takes that occupancy data and feeds it to an open-source LLM (running locally with Ollama, tried models like Phi-3 for this). The LLM then generates a surprisingly detailed &amp;quot;Parking Lot Analysis Report&amp;quot; in Markdown.&lt;/p&gt; &lt;p&gt;This report isn't just &amp;quot;X spots free.&amp;quot; It calculates occupancy percentages, assesses current demand (e.g., &amp;quot;moderately utilized&amp;quot;), flags potential risks (like overcrowding if it gets too full), and even suggests actionable improvements like dynamic pricing strategies or better signage.&lt;/p&gt; &lt;p&gt;It's all automated ‚Äì from seeing the car park to getting a mini-management consultant report.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech Stack Snippets:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CV:&lt;/strong&gt; YOLO model from Roboflow for spot detection.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM:&lt;/strong&gt; Ollama for local LLM inference (e.g., Phi-3).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output:&lt;/strong&gt; Markdown reports.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The video shows it in action, including the report being generated.&lt;/p&gt; &lt;p&gt;Github Code: &lt;a href="https://github.com/Pavankunchala/LLM-Learn-PK/tree/main/ollama/parking_analysis"&gt;https://github.com/Pavankunchala/LLM-Learn-PK/tree/main/ollama/parking_analysis&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also if in this code you have to draw the polygons manually I built a separate app for it you can check that code here: &lt;a href="https://github.com/Pavankunchala/LLM-Learn-PK/tree/main/polygon-zone-app"&gt;https://github.com/Pavankunchala/LLM-Learn-PK/tree/main/polygon-zone-app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Self-promo note: If you find the code useful, a star on GitHub would be awesome!)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I'm thinking next:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time alerts for lot managers.&lt;/li&gt; &lt;li&gt;Predictive analysis for peak hours.&lt;/li&gt; &lt;li&gt;Maybe a simple web dashboard.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; On a related note, I'm actively looking for new opportunities in Computer Vision and LLM engineering. If your team is hiring or you know of any openings, I'd be grateful if you'd reach out!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Email:&lt;/strong&gt; [&lt;a href="mailto:pavankunchalaofficial@gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;](mailto:&lt;a href="mailto:pavankunchalaofficial@gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My other projects on GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Resume:&lt;/strong&gt; &lt;a href="https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view"&gt;https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i9tkek29312f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1krkhr7/parking_analysis_with_object_detection_and_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1krkhr7/parking_analysis_with_object_detection_and_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-21T00:19:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kru7b4</id>
    <title>12-&gt;16GB VRAM worth the upgrade?</title>
    <updated>2025-05-21T10:11:14+00:00</updated>
    <author>
      <name>/u/rddz48</name>
      <uri>https://old.reddit.com/user/rddz48</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is an upgrade to f.i. RTX2000ADA with 16GB VRAM from RTX4070 with 12GB worth the money?&lt;/p&gt; &lt;p&gt;Just asking because from models available for download only a few more seem to fit in the extra 4GB, a couple of 24b models to be specific. &lt;/p&gt; &lt;p&gt;If a model is only a bit bigger than available VRAM Ollama will fall back to CPU/RAM from CUDA/VRAM I think...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rddz48"&gt; /u/rddz48 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kru7b4/1216gb_vram_worth_the_upgrade/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kru7b4/1216gb_vram_worth_the_upgrade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kru7b4/1216gb_vram_worth_the_upgrade/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-21T10:11:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1krywit</id>
    <title>Did llama3.2-vision:11b go blind?</title>
    <updated>2025-05-21T14:14:57+00:00</updated>
    <author>
      <name>/u/Comfortable_Ad_8117</name>
      <uri>https://old.reddit.com/user/Comfortable_Ad_8117</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1krywit/did_llama32vision11b_go_blind/"&gt; &lt;img alt="Did llama3.2-vision:11b go blind?" src="https://preview.redd.it/fqme1xjy752f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb2b9b1187f5501e79b343474c72d2f55771cd26" title="Did llama3.2-vision:11b go blind?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable_Ad_8117"&gt; /u/Comfortable_Ad_8117 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fqme1xjy752f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1krywit/did_llama32vision11b_go_blind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1krywit/did_llama32vision11b_go_blind/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-21T14:14:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksiywq</id>
    <title>Feedback from Anyone Running RTX 4000 SFF Ada vs Dual RTXA2000 SFF Ada?</title>
    <updated>2025-05-22T05:24:11+00:00</updated>
    <author>
      <name>/u/PocketMartyr</name>
      <uri>https://old.reddit.com/user/PocketMartyr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I‚Äôm trying to decide between two GPU setups for running Ollama and would love to hear from anyone who‚Äôs tested either config in the wild. &lt;/p&gt; &lt;p&gt;Space and power consumption are &lt;em&gt;not&lt;/em&gt; flexible, so my options are literally between the 2 I have outlined below. Cards must be half height, single slot, and run only on the power supplied by PCIE.&lt;/p&gt; &lt;p&gt;Option 1: ‚Ä¢ Single RTX 4000 SFF Ada (20GB VRAM)&lt;/p&gt; &lt;p&gt;Option 2: ‚Ä¢ Dual RTX A2000 SFF (16GB each, 32GB combined VRAM)&lt;/p&gt; &lt;p&gt;I‚Äôll primarily be running local LLMs and possibly experimenting with RAG and fine tuning. &lt;/p&gt; &lt;p&gt;I‚Äôve been running small models off the Ryzen 5600x with 64gb memory. I‚Äôm just not sure whether the total combined vram or faster single you with lower vram will yield the best overall experience. &lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PocketMartyr"&gt; /u/PocketMartyr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ksiywq/feedback_from_anyone_running_rtx_4000_sff_ada_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ksiywq/feedback_from_anyone_running_rtx_4000_sff_ada_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ksiywq/feedback_from_anyone_running_rtx_4000_sff_ada_vs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-22T05:24:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kshx5c</id>
    <title>I built an Open-Source AI Resume Tailoring App with LangChain &amp; Ollama - Looking for feedback &amp; my next CV/GenAI role!</title>
    <updated>2025-05-22T04:18:41+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kshx5c/i_built_an_opensource_ai_resume_tailoring_app/"&gt; &lt;img alt="I built an Open-Source AI Resume Tailoring App with LangChain &amp;amp; Ollama - Looking for feedback &amp;amp; my next CV/GenAI role!" src="https://external-preview.redd.it/dDNqOW1qcXllOTJmMfQDJh93iwJo5nXUO4UpJpk3IDUw8nDIGOAeiz1giQIg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de3c1ba102c71e04d5ad47e0c84338e5b5bca867" title="I built an Open-Source AI Resume Tailoring App with LangChain &amp;amp; Ollama - Looking for feedback &amp;amp; my next CV/GenAI role!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been diving deep into the LLM world lately and wanted to share a project I've been tinkering with: an &lt;strong&gt;AI-powered Resume Tailoring application&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Gist:&lt;/strong&gt; You feed it your current resume and a job description, and it tries to tweak your resume's keywords to better align with what the job posting is looking for. We all know how much of a pain manual tailoring can be, so I wanted to see if I could automate parts of it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech Stack Under the Hood:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; LangChain is the star here, using hybrid retrieval (BM25 for sparse, and a dense model for semantic search). I'm running language models locally using Ollama, which has been a fun experience.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; Good ol' React.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Current Status &amp;amp; What's Next:&lt;/strong&gt;&lt;br /&gt; It's definitely not perfect yet ‚Äì more of a proof-of-concept at this stage. I'm planning to spend this weekend refining the code, improving the prompting, and maybe making the UI a bit slicker.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I'd love your thoughts!&lt;/strong&gt; If you're into RAG, LangChain, or just resume tech, I'd appreciate any suggestions, feedback, or even contributions. The code is open source:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Project Repo:&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala/LLM-Learn-PK/tree/main/resume-tailor"&gt;https://github.com/Pavankunchala/LLM-Learn-PK/tree/main/resume-tailor&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;On a related note (and the other reason for this post!):&lt;/strong&gt; I'm actively on the hunt for new opportunities, specifically in &lt;strong&gt;Computer Vision and Generative AI / LLM domains&lt;/strong&gt;. Building this project has only fueled my passion for these areas. If your team is hiring, or you know someone who might be interested in a profile like mine, I'd be thrilled if you reached out.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;My Email:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=mailto%3Apavankunchalaofficial%40gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My GitHub Profile (for more projects):&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My Resume:&lt;/strong&gt; &lt;a href="https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view"&gt;https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for reading this far! Looking forward to any discussions or leads.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c0bhdjqye92f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kshx5c/i_built_an_opensource_ai_resume_tailoring_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kshx5c/i_built_an_opensource_ai_resume_tailoring_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-22T04:18:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksw8o4</id>
    <title>Is a NVIDIA Jetson AGX Orin 64GB enough to run 32b q4 models comfortably?</title>
    <updated>2025-05-22T17:14:20+00:00</updated>
    <author>
      <name>/u/cyuhat</name>
      <uri>https://old.reddit.com/user/cyuhat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am new to this topic.&lt;/p&gt; &lt;p&gt;I have currently a computer with a NVIDIA GeForce RTX 3060. It can run Qwen2.5:32b at 2.35 tokens/s. I want to run it at least 3 times faster. So is a Nvidia Jetson AGX Orin 64GB good enough for that, or do you have better recommendations?&lt;/p&gt; &lt;p&gt;Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cyuhat"&gt; /u/cyuhat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ksw8o4/is_a_nvidia_jetson_agx_orin_64gb_enough_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ksw8o4/is_a_nvidia_jetson_agx_orin_64gb_enough_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ksw8o4/is_a_nvidia_jetson_agx_orin_64gb_enough_to_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-22T17:14:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksqejy</id>
    <title>I added Ollama support to AI Runner</title>
    <updated>2025-05-22T13:14:51+00:00</updated>
    <author>
      <name>/u/w00fl35</name>
      <uri>https://old.reddit.com/user/w00fl35</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ksqejy/i_added_ollama_support_to_ai_runner/"&gt; &lt;img alt="I added Ollama support to AI Runner" src="https://external-preview.redd.it/cjY4OW1kZG4yYzJmMdr0bmklJbYVf9evqj64tkFRNulqvAaIZm1K71UFaRqZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3bce072873d981f96feaba7286a439baa426c095" title="I added Ollama support to AI Runner" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w00fl35"&gt; /u/w00fl35 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t3szu8st1c2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ksqejy/i_added_ollama_support_to_ai_runner/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ksqejy/i_added_ollama_support_to_ai_runner/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-22T13:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kt0pxi</id>
    <title>FireBird-Technologies/Auto-Analyst: Open-source AI-powered data science platform. Can be used locally via Ollama</title>
    <updated>2025-05-22T20:15:10+00:00</updated>
    <author>
      <name>/u/phicreative1997</name>
      <uri>https://old.reddit.com/user/phicreative1997</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kt0pxi/firebirdtechnologiesautoanalyst_opensource/"&gt; &lt;img alt="FireBird-Technologies/Auto-Analyst: Open-source AI-powered data science platform. Can be used locally via Ollama" src="https://external-preview.redd.it/B84SGSysHAPCM8nv1VtMQe9RGypuIRY5ptrEhECfLDg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e94b1779aeebd5419f0e64b0911cd6f44fc0ae27" title="FireBird-Technologies/Auto-Analyst: Open-source AI-powered data science platform. Can be used locally via Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phicreative1997"&gt; /u/phicreative1997 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/FireBird-Technologies/Auto-Analyst"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kt0pxi/firebirdtechnologiesautoanalyst_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kt0pxi/firebirdtechnologiesautoanalyst_opensource/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-22T20:15:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktaeue</id>
    <title>Want help in retrieving links from DB</title>
    <updated>2025-05-23T04:02:35+00:00</updated>
    <author>
      <name>/u/420Deku</name>
      <uri>https://old.reddit.com/user/420Deku</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I made a chatbot using a model from Ollama, everything is working fine but now I want to make changes. I have cloud where I am dumped my resources, and each resource I have its link to be accessed. Now I have stored this links in a database where I have stored it as title/name of the resource and corresponding link to the resource. Whenever I ask something related to any of the topic present in the DB, I want the model to fetch me the link of the relevant topic. Incase that topic is not there then it should create a ticket/do something which can call the admin of the llm for manual intervention. However to get the links is the tricky part for me. Please help&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/420Deku"&gt; /u/420Deku &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ktaeue/want_help_in_retrieving_links_from_db/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ktaeue/want_help_in_retrieving_links_from_db/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ktaeue/want_help_in_retrieving_links_from_db/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-23T04:02:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kteptv</id>
    <title>What is the most powerful model one can run on NVIDIA T4 GPU (Standard NC4as T4 v3 VM)?</title>
    <updated>2025-05-23T08:49:53+00:00</updated>
    <author>
      <name>/u/Joh1011100</name>
      <uri>https://old.reddit.com/user/Joh1011100</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I have NC4as T4 v3 VM in Azure I ran some models with ollama on it. I'm curious what is the most powerful mmodel that it can handle.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Joh1011100"&gt; /u/Joh1011100 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kteptv/what_is_the_most_powerful_model_one_can_run_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kteptv/what_is_the_most_powerful_model_one_can_run_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kteptv/what_is_the_most_powerful_model_one_can_run_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-23T08:49:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksmmwi</id>
    <title>Translate an entire book with Ollama</title>
    <updated>2025-05-22T09:42:40+00:00</updated>
    <author>
      <name>/u/hydropix</name>
      <uri>https://old.reddit.com/user/hydropix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've developed a Python script to translate large amounts of text, like entire books, using Ollama. Here‚Äôs how it works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Smart Chunking:&lt;/strong&gt; The script breaks down the text into smaller paragraphs, ensuring that lines are not awkwardly cut off to preserve meaning.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Contextual Continuity:&lt;/strong&gt; To maintain translation coherence, it feeds context from the previously translated segment into the next one.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompt Injection &amp;amp; Extraction:&lt;/strong&gt; It then uses a customizable translation prompt and retrieves the translated text from between specific tags (e.g., &lt;code&gt;&amp;lt;translate&amp;gt;&lt;/code&gt;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Performance:&lt;/strong&gt; As a benchmark, an entire book can be translated in just over an hour on an RTX 4090.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Usage Tips:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Feel free to adjust the prompt within the script if your content has specific requirements (tone, style, terminology).&lt;/li&gt; &lt;li&gt;It's also recommended to experiment with different LLM models depending on the source and target languages.&lt;/li&gt; &lt;li&gt;Based on my tests, models that explicitly use a &amp;quot;chain-of-thought&amp;quot; approach don't seem to perform best for this direct translation task.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can find the script on &lt;a href="https://github.com/hydropix/TranslateBookWithLLM"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy translating!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hydropix"&gt; /u/hydropix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ksmmwi/translate_an_entire_book_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ksmmwi/translate_an_entire_book_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ksmmwi/translate_an_entire_book_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-22T09:42:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktcooa</id>
    <title>Rocm or vulkan support for AMD Radeon 780M?</title>
    <updated>2025-05-23T06:25:50+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I've installed ollama on a machine with an AMD 7040U series processor + radeon 780M igpu, I've seen a message about the gpu being detected and rocm being supported, but then ollama only runs models on the CPU. &lt;/p&gt; &lt;p&gt;If I compile llama.cpp + vulkan and directly run models through llama.cpp, they are about 2x a fast as on the CPU via ollama.&lt;/p&gt; &lt;p&gt;Is there any trick to get ollama+rocm working on the 780M? Or instead to use ollama with vulkan?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ktcooa/rocm_or_vulkan_support_for_amd_radeon_780m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ktcooa/rocm_or_vulkan_support_for_amd_radeon_780m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ktcooa/rocm_or_vulkan_support_for_amd_radeon_780m/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-23T06:25:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktm2uo</id>
    <title>Right model for M1 Pro MacBook with 16 GB of RAM</title>
    <updated>2025-05-23T15:14:36+00:00</updated>
    <author>
      <name>/u/dfalidas</name>
      <uri>https://old.reddit.com/user/dfalidas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a M1 Pro MacBook with 16 GB of RAM. What would be a model that I could run with decent results? I am interested to try the new Raycast local models AI and for querying my Obsidian vault&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dfalidas"&gt; /u/dfalidas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ktm2uo/right_model_for_m1_pro_macbook_with_16_gb_of_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ktm2uo/right_model_for_m1_pro_macbook_with_16_gb_of_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ktm2uo/right_model_for_m1_pro_macbook_with_16_gb_of_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-23T15:14:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksy9tu</id>
    <title>How do you guys learn to train AI</title>
    <updated>2025-05-22T18:35:32+00:00</updated>
    <author>
      <name>/u/ARNAVRANJAN</name>
      <uri>https://old.reddit.com/user/ARNAVRANJAN</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm just a 20 year old college student right now. I've tons of ideas that I want to implement. But I have to first learn a lot of stuff to actually begin my journey, and to do that I need money. I think I need better hardwares better gpus if I really get into AI stuff. Yes I feel like money is holding me back (I might be wrong). But really want to start training models and research on LLMs, but all I have is a gaming laptop and AI is really resource heavy topic. What should I do ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ARNAVRANJAN"&gt; /u/ARNAVRANJAN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ksy9tu/how_do_you_guys_learn_to_train_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ksy9tu/how_do_you_guys_learn_to_train_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ksy9tu/how_do_you_guys_learn_to_train_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-22T18:35:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kt8da9</id>
    <title>üéôÔ∏è Offline Speech-to-Text with NVIDIA Parakeet-TDT 0.6B v2</title>
    <updated>2025-05-23T02:11:35+00:00</updated>
    <author>
      <name>/u/srireddit2020</name>
      <uri>https://old.reddit.com/user/srireddit2020</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kt8da9/offline_speechtotext_with_nvidia_parakeettdt_06b/"&gt; &lt;img alt="üéôÔ∏è Offline Speech-to-Text with NVIDIA Parakeet-TDT 0.6B v2" src="https://b.thumbs.redditmedia.com/5CxgKmXHxuluVozjHgyWjkVQi9nC2lVQ4y7_0aAWR2Y.jpg" title="üéôÔ∏è Offline Speech-to-Text with NVIDIA Parakeet-TDT 0.6B v2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! üëã&lt;/p&gt; &lt;p&gt;I recently built a fully local speech-to-text system using &lt;strong&gt;NVIDIA‚Äôs Parakeet-TDT 0.6B v2&lt;/strong&gt; ‚Äî a 600M parameter ASR model capable of transcribing real-world audio &lt;strong&gt;entirely offline with GPU acceleration&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;üí° &lt;strong&gt;Why this matters&lt;/strong&gt;:&lt;br /&gt; Most ASR tools rely on cloud APIs and miss crucial formatting like punctuation or timestamps. This setup works offline, includes segment-level timestamps, and handles a range of real-world audio inputs ‚Äî like news, lyrics, and conversations.&lt;/p&gt; &lt;p&gt;üìΩÔ∏è &lt;strong&gt;Demo Video&lt;/strong&gt;:&lt;br /&gt; &lt;em&gt;Shows transcription of 3 samples ‚Äî financial news, a song, and a conversation between Jensen Huang &amp;amp; Satya Nadella.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1kt8da9/video/o39gm571xf2f1/player"&gt;A full walkthrough of the local ASR system built with Parakeet-TDT 0.6B. Includes architecture overview and transcription demos for financial news, song lyrics, and a tech dialogue.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Processing video...A full walkthrough of the local ASR system built with Parakeet-TDT 0.6B. Includes architecture overview and transcription demos for financial news, song lyrics, and a tech dialogue.&lt;/p&gt; &lt;p&gt;üß™ &lt;strong&gt;Tested On&lt;/strong&gt;:&lt;br /&gt; ‚úÖ Stock market commentary with spoken numbers&lt;br /&gt; ‚úÖ Song lyrics with punctuation and rhyme&lt;br /&gt; ‚úÖ Multi-speaker tech conversation on AI and silicon innovation&lt;/p&gt; &lt;p&gt;üõ†Ô∏è &lt;strong&gt;Tech Stack&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;NVIDIA Parakeet-TDT 0.6B v2 (ASR model)&lt;/li&gt; &lt;li&gt;NVIDIA NeMo Toolkit&lt;/li&gt; &lt;li&gt;PyTorch + CUDA 11.8&lt;/li&gt; &lt;li&gt;Streamlit (for local UI)&lt;/li&gt; &lt;li&gt;FFmpeg + Pydub (preprocessing)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hs0ngks3xf2f1.png?width=1862&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=882aa1639df33417961ff092475fa097b15e2ea9"&gt;Flow diagram showing Local ASR using NVIDIA Parakeet-TDT with Streamlit UI, audio preprocessing, and model inference pipeline&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Flow diagram showing Local ASR using NVIDIA Parakeet-TDT with Streamlit UI, audio preprocessing, and model inference pipeline&lt;/p&gt; &lt;p&gt;üß† &lt;strong&gt;Key Features&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Runs 100% offline (no cloud APIs required)&lt;/li&gt; &lt;li&gt;Accurate punctuation + capitalization&lt;/li&gt; &lt;li&gt;Word + segment-level timestamp support&lt;/li&gt; &lt;li&gt;Works on my local RTX 3050 Laptop GPU with CUDA 11.8&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üìå &lt;strong&gt;Full blog + code + architecture + demo screenshots&lt;/strong&gt;:&lt;br /&gt; üîó &lt;a href="https://medium.com/towards-artificial-intelligence/%EF%B8%8F-building-a-local-speech-to-text-system-with-parakeet-tdt-0-6b-v2-ebd074ba8a4c"&gt;https://medium.com/towards-artificial-intelligence/Ô∏è-building-a-local-speech-to-text-system-with-parakeet-tdt-0-6b-v2-ebd074ba8a4c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üñ•Ô∏è &lt;strong&gt;Tested locally on&lt;/strong&gt;:&lt;br /&gt; NVIDIA RTX 3050 Laptop GPU + CUDA 11.8 + PyTorch&lt;/p&gt; &lt;p&gt;Would love to hear your feedback ‚Äî or if you‚Äôve tried ASR models like Whisper, how it compares for you! üôå&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/srireddit2020"&gt; /u/srireddit2020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kt8da9/offline_speechtotext_with_nvidia_parakeettdt_06b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kt8da9/offline_speechtotext_with_nvidia_parakeettdt_06b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kt8da9/offline_speechtotext_with_nvidia_parakeettdt_06b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-23T02:11:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktjind</id>
    <title>Ollama is running on AMD GPU, despite ROCM not being installed</title>
    <updated>2025-05-23T13:25:31+00:00</updated>
    <author>
      <name>/u/Xatraxalian</name>
      <uri>https://old.reddit.com/user/Xatraxalian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I've started to experiment with running local LLM's. It seems Ollama runs on the AMD GPU even without ROCM installed. This is what I did:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPU: AMD RX 6750 XT&lt;/li&gt; &lt;li&gt;OS: Debian Trixie 13 (currently testing)&lt;/li&gt; &lt;li&gt;Kernel: 6.14.x, Xanmod&lt;/li&gt; &lt;li&gt;Installed the Debian Trixie ROCM 6.1 libraries (bear with me here)&lt;/li&gt; &lt;li&gt;Set: HSA_OVERRIDE_GFX_VERSION=10.3.0 (in the systemd unit file)&lt;/li&gt; &lt;li&gt;Installed Ollama, and have it started with Systemd.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It ran, and it ran the models on the GPU, as 'ollama ps' said &amp;quot;100% GPU&amp;quot;. I can see the GPU being fully loaded when Ollama is doing something like generating code.&lt;/p&gt; &lt;p&gt;Then I wanted to install the latest version of ROCM from AMD, but it doesn't support Debian Trixie 13 yet. So I did this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Quit everything&lt;/li&gt; &lt;li&gt;Removed Ollama from my host system &lt;a href="https://github.com/ollama/ollama/issues/986"&gt;see here&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Installed Distrobox.&lt;/li&gt; &lt;li&gt;Created a box running Debian 12&lt;/li&gt; &lt;li&gt;Installed Ollama in it and 'exported' the binary to the host system&lt;/li&gt; &lt;li&gt;Had the box and the ollama server started by systemd&lt;/li&gt; &lt;li&gt;I still set HSA_OVERRIDE_GFX_VERSION=10.3.0&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything works: The ollama box and the server starts, and I can use the exported binary to control ollama within the distrobox. It still runs 100% on the GPU, probably because ROCM is installed on the host. (Distrobox first uses libraries in the box; if they're not there, it uses the system libraries, as far as I understand.)&lt;/p&gt; &lt;p&gt;Then I removed all the rocm libraries from my host system and rebooted the system, intending to re-install ROCM 6.4.1 in the distrobox. However, I first ran Ollama, expecting it to now run 100% on the CPU.&lt;/p&gt; &lt;p&gt;But surprise... when I restarted and then fired up a model, it was STILL running 100% on the GPU. All the ROCM libraries on the host are gone, and they where never installed in the distrobox. When grepping for 'rocm' in the 'dpkg --list' output, no ROCM packages are found; not in the host, not in the distrobox.&lt;/p&gt; &lt;p&gt;How's that possible? Does Ollama not actually require ROCM to just run the model, and only needs it to train new models? Does Ollama now include its own ROCM when installing on Linux? Is it able to run on the GPU all by itself if it detects it correctly?&lt;/p&gt; &lt;p&gt;Can anyone enlighten me here? Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xatraxalian"&gt; /u/Xatraxalian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ktjind/ollama_is_running_on_amd_gpu_despite_rocm_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ktjind/ollama_is_running_on_amd_gpu_despite_rocm_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ktjind/ollama_is_running_on_amd_gpu_despite_rocm_not/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-23T13:25:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktiap6</id>
    <title>32GB vs 48GB RAM MBP for local LLM experimentation - real world experiences?</title>
    <updated>2025-05-23T12:27:26+00:00</updated>
    <author>
      <name>/u/SampleSalty</name>
      <uri>https://old.reddit.com/user/SampleSalty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently torn between two MacBook Pro M4 configs at the same price (‚Ç¨2850):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option A:&lt;/strong&gt; M4 + 32GB RAM + 2TB storage&lt;br /&gt; &lt;strong&gt;Option B:&lt;/strong&gt; M4 Pro + 48GB RAM + 1TB storage&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My use case:&lt;/strong&gt; Web research, development POCs, and increasingly interested in local LLM experimentation. I know 64GB+ is ideal for the biggest models, but that's ‚Ç¨4500+ which is out of budget.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What's the largest/most useful model you've successfully run on 32GB vs 48GB?&lt;/li&gt; &lt;li&gt;Does the extra 16GB make a meaningful difference in your day-to-day LLM usage?&lt;/li&gt; &lt;li&gt;Any M4 vs M4 Pro performance differences you've noticed with inference?&lt;/li&gt; &lt;li&gt;Is 1TB enough storage for model experimentation, or do you find yourself constantly managing space?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm particularly interested in hearing from anyone who's made a similar choice or upgraded from 32GB to 48GB. I am between the chairs, because I also value the better efficiency of the normal M4, otherwise choice would be much easier.&lt;/p&gt; &lt;p&gt;What would you do?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SampleSalty"&gt; /u/SampleSalty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ktiap6/32gb_vs_48gb_ram_mbp_for_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ktiap6/32gb_vs_48gb_ram_mbp_for_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ktiap6/32gb_vs_48gb_ram_mbp_for_local_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-23T12:27:26+00:00</published>
  </entry>
</feed>
