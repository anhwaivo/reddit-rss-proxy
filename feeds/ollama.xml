<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-18T15:38:51+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mrawvt</id>
    <title>Open Moxie - Fully Offline (ollama option) and XAI Grok API</title>
    <updated>2025-08-15T21:26:23+00:00</updated>
    <author>
      <name>/u/Over-Mix7071</name>
      <uri>https://old.reddit.com/user/Over-Mix7071</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mrawvt/open_moxie_fully_offline_ollama_option_and_xai/"&gt; &lt;img alt="Open Moxie - Fully Offline (ollama option) and XAI Grok API" src="https://external-preview.redd.it/a1zSbUhyWKbtVHaGz5nbMQPuqamAd8HDk_nVmyFL_-4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4aae6540bec8fcaa94d2dcf38e781bec38212860" title="Open Moxie - Fully Offline (ollama option) and XAI Grok API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Over-Mix7071"&gt; /u/Over-Mix7071 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/l7mikg3609jf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mrawvt/open_moxie_fully_offline_ollama_option_and_xai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mrawvt/open_moxie_fully_offline_ollama_option_and_xai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-15T21:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrwsyx</id>
    <title>Smart assist Home Assistant.</title>
    <updated>2025-08-16T14:27:50+00:00</updated>
    <author>
      <name>/u/Original-Chapter-112</name>
      <uri>https://old.reddit.com/user/Original-Chapter-112</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Original-Chapter-112"&gt; /u/Original-Chapter-112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/homeassistant/comments/1mrws8z/smart_assist_home_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mrwsyx/smart_assist_home_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mrwsyx/smart_assist_home_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T14:27:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms59k7</id>
    <title>A Guide to GRPO Fine-Tuning on Windows Using the TRL Library</title>
    <updated>2025-08-16T19:33:04+00:00</updated>
    <author>
      <name>/u/oridnary_artist</name>
      <uri>https://old.reddit.com/user/oridnary_artist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ms59k7/a_guide_to_grpo_finetuning_on_windows_using_the/"&gt; &lt;img alt="A Guide to GRPO Fine-Tuning on Windows Using the TRL Library" src="https://preview.redd.it/3o9tbazgofjf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ecf02796e74bfbbb6661da8cb781ff654a0af2a2" title="A Guide to GRPO Fine-Tuning on Windows Using the TRL Library" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wrote a hands-on guide for fine-tuning LLMs with GRPO (Group-Relative PPO) locally on Windows, using Hugging Face's TRL library. My goal was to create a practical workflow that doesn't require Colab or Linux.&lt;/p&gt; &lt;p&gt;The guide and the accompanying script focus on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;A TRL-based implementation&lt;/strong&gt; that runs on consumer GPUs (with LoRA and optional 4-bit quantization).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A verifiable reward system&lt;/strong&gt; that uses numeric, format, and boilerplate checks to create a more reliable training signal.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Automatic data mapping&lt;/strong&gt; for most Hugging Face datasets to simplify preprocessing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Practical troubleshooting&lt;/strong&gt; and configuration notes for local setups.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is for anyone looking to experiment with reinforcement learning techniques on their own machine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Read the blog post:&lt;/strong&gt; &lt;a href="https://pavankunchalapk.medium.com/windows-friendly-grpo-fine-tuning-with-trl-from-zero-to-verifiable-rewards-f28008c89323"&gt;&lt;code&gt;https://pavankunchalapk.medium.com/windows-friendly-grpo-fine-tuning-with-trl-from-zero-to-verifiable-rewards-f28008c89323&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Get the code:&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/trl-ppo-fine-tuning"&gt;Reinforcement-learning-with-verifable-rewards-Learnings/projects/trl-ppo-fine-tuning at main · Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm open to any feedback. Thanks!&lt;/p&gt; &lt;p&gt;&lt;em&gt;P.S. I'm currently looking for my next role in the LLM / Computer Vision space and would love to connect about any opportunities&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Portfolio:&lt;/em&gt; &lt;a href="https://pavan-portfolio-tawny.vercel.app/"&gt;Pavan Kunchala - AI Engineer &amp;amp; Full-Stack Developer&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oridnary_artist"&gt; /u/oridnary_artist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3o9tbazgofjf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ms59k7/a_guide_to_grpo_finetuning_on_windows_using_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ms59k7/a_guide_to_grpo_finetuning_on_windows_using_the/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T19:33:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms5cjg</id>
    <title>🚀 New Feature in RAGLight: Effortless MCP Integration for Agentic RAG Pipelines! 🔌</title>
    <updated>2025-08-16T19:36:10+00:00</updated>
    <author>
      <name>/u/Labess40</name>
      <uri>https://old.reddit.com/user/Labess40</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I just shipped a new feature in &lt;a href="https://github.com/Bessouat40/RAGLight"&gt;RAGLight&lt;/a&gt;, my lightweight and modular Python framework for Retrieval-Augmented Generation, and it's a big one: &lt;strong&gt;easy MCP Server integration&lt;/strong&gt; for Agentic RAG workflows. 🧠💻&lt;/p&gt; &lt;h1&gt;What's new?&lt;/h1&gt; &lt;p&gt;You can now plug in external tools directly into your agent's reasoning process using an MCP server. No boilerplate required. Whether you're building code assistants, tool-augmented LLM agents, or just want your LLM to interact with a live backend, it's now just a few lines of config.&lt;/p&gt; &lt;h1&gt;Example:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;config = AgenticRAGConfig( provider = Settings.OPENAI, model = &amp;quot;gpt-4o&amp;quot;, k = 10, mcp_config = [ {&amp;quot;url&amp;quot;: &amp;quot;http://127.0.0.1:8001/sse&amp;quot;} # Your MCP server URL ], ... ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This automatically injects all MCP tools into the agent's toolset.&lt;/p&gt; &lt;p&gt;📚 If you're curious how to write your own MCP tool or server, you can check the &lt;code&gt;MCPClient.server_parameters&lt;/code&gt; doc from &lt;a href="https://huggingface.co/docs/smolagents/en/reference/tools#smolagents.MCPClient.server_parameters"&gt;smolagents&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;👉 Try it out and let me know what you think: &lt;a href="https://github.com/Bessouat40/RAGLight"&gt;https://github.com/Bessouat40/RAGLight&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Labess40"&gt; /u/Labess40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ms5cjg/new_feature_in_raglight_effortless_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ms5cjg/new_feature_in_raglight_effortless_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ms5cjg/new_feature_in_raglight_effortless_mcp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T19:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1msclnz</id>
    <title>Phi 3 Mini needing 50gb??</title>
    <updated>2025-08-17T00:22:40+00:00</updated>
    <author>
      <name>/u/Fun-Tangerine5264</name>
      <uri>https://old.reddit.com/user/Fun-Tangerine5264</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1msclnz/phi_3_mini_needing_50gb/"&gt; &lt;img alt="Phi 3 Mini needing 50gb??" src="https://a.thumbs.redditmedia.com/ITn7NYWmJ_MOxmF9GA6WyOx8eGtT06QxvF8ZTULN108.jpg" title="Phi 3 Mini needing 50gb??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/64fvg8f44hjf1.png?width=1825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e38d036375ba95767abce394acc2cef1eeb7aff8"&gt;https://preview.redd.it/64fvg8f44hjf1.png?width=1825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e38d036375ba95767abce394acc2cef1eeb7aff8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Erm, I don't think this is supposed to happen on a mini model no?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Tangerine5264"&gt; /u/Fun-Tangerine5264 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msclnz/phi_3_mini_needing_50gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msclnz/phi_3_mini_needing_50gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msclnz/phi_3_mini_needing_50gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T00:22:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mso19g</id>
    <title>How to maximize qwen-coder-30b TPS on a 4060 Ti (8 GB)?</title>
    <updated>2025-08-17T11:04:52+00:00</updated>
    <author>
      <name>/u/Overall-Branch-1496</name>
      <uri>https://old.reddit.com/user/Overall-Branch-1496</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Overall-Branch-1496"&gt; /u/Overall-Branch-1496 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1msnxpy/how_to_maximize_qwencoder30b_tps_on_a_4060_ti_8_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mso19g/how_to_maximize_qwencoder30b_tps_on_a_4060_ti_8_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mso19g/how_to_maximize_qwencoder30b_tps_on_a_4060_ti_8_gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T11:04:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1msflr9</id>
    <title>Anyone have tokens per second results for gpt-oss 20-b on an ada 2000?</title>
    <updated>2025-08-17T02:50:33+00:00</updated>
    <author>
      <name>/u/RasPiBuilder</name>
      <uri>https://old.reddit.com/user/RasPiBuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for something with relatively low power draw and decent inference speeds. I don't need it to be blazing fast, but it does need to be responsive at reasonable speeds (hoping for around 7-10t/s). &lt;/p&gt; &lt;p&gt;For this particular setup power draw is the bottleneck, where my absolute max is 100w. Cost is less of an issue, though I'd lean towards the least expensive on comparable speed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RasPiBuilder"&gt; /u/RasPiBuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msflr9/anyone_have_tokens_per_second_results_for_gptoss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msflr9/anyone_have_tokens_per_second_results_for_gptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msflr9/anyone_have_tokens_per_second_results_for_gptoss/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T02:50:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1msarft</id>
    <title>Why does gpt-oss 120b run slower in ollama than in LM Studio in my setup?</title>
    <updated>2025-08-16T23:03:33+00:00</updated>
    <author>
      <name>/u/Southern-Chain-6485</name>
      <uri>https://old.reddit.com/user/Southern-Chain-6485</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My hardware is an RTX 3090 + 64gb of ddr4 ram. LM Studio runs it at something about 10-12 tokens per second (I don't have the actual measure at hand) while ollama runs it at half the speed, at best. I'm using the lm studio community version in LM Studio and the version downloaded from ollama's site with ollama - basically, the recommended versions. Are there flags that need to be run in Ollama to match LM Studio performance?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Southern-Chain-6485"&gt; /u/Southern-Chain-6485 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msarft/why_does_gptoss_120b_run_slower_in_ollama_than_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msarft/why_does_gptoss_120b_run_slower_in_ollama_than_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msarft/why_does_gptoss_120b_run_slower_in_ollama_than_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T23:03:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms1v2p</id>
    <title>Ollama interface with memory</title>
    <updated>2025-08-16T17:31:04+00:00</updated>
    <author>
      <name>/u/mrdougwright</name>
      <uri>https://old.reddit.com/user/mrdougwright</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;Ollama is so cool, it inspired me to do some open source!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mrdougwright/yak"&gt;https://github.com/mrdougwright/yak&lt;/a&gt;&lt;br /&gt; &lt;a href="https://www.npmjs.com/package/yak-llm"&gt;https://www.npmjs.com/package/yak-llm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Yak is a CLI interface with persistent chat sessions for local LLMs. Instead of losing context every time you restart, it remembers your conversations across sessions and lets you organize them by topic. &lt;/p&gt; &lt;p&gt;Key features:&lt;br /&gt; - Multiple chat sessions (work, personal, coding help, etc.)&lt;br /&gt; - Persistent memory using simple JSONL files&lt;br /&gt; - Auto-starts Ollama if needed&lt;br /&gt; - Switch models from the CLI&lt;br /&gt; - Zero config for new users &lt;/p&gt; &lt;p&gt;Install: `npm install -g yak-llm`&lt;br /&gt; Usage: `yak start` &lt;/p&gt; &lt;p&gt;Built this because I wanted something lightweight that actually remembers context and doesn't slow down with long conversations. Plus you can directly edit the chat files if needed! &lt;/p&gt; &lt;p&gt;Would love feedback from the Ollama community! 🦧&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrdougwright"&gt; /u/mrdougwright &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ms1v2p/ollama_interface_with_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ms1v2p/ollama_interface_with_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ms1v2p/ollama_interface_with_memory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T17:31:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1msvir4</id>
    <title>Getting a memory error when downloading a model</title>
    <updated>2025-08-17T16:32:23+00:00</updated>
    <author>
      <name>/u/LaPreparando</name>
      <uri>https://old.reddit.com/user/LaPreparando</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1msvir4/getting_a_memory_error_when_downloading_a_model/"&gt; &lt;img alt="Getting a memory error when downloading a model" src="https://a.thumbs.redditmedia.com/NzO-T4fMtBWXQHMWdkyvR2WoSj2Ico-GoNBllptue00.jpg" title="Getting a memory error when downloading a model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made sure that the folder for the models is under D:, and it really downloads it to D:. But then I get a memory error, basically because C: doesn't have enough space (which I am aware of). Any idea how to avoid/fix that? &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/iyku9u96xljf1.png?width=813&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa19a05fd216ceaf378f045d736e8c4f1d418a5d"&gt;https://preview.redd.it/iyku9u96xljf1.png?width=813&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa19a05fd216ceaf378f045d736e8c4f1d418a5d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LaPreparando"&gt; /u/LaPreparando &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msvir4/getting_a_memory_error_when_downloading_a_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msvir4/getting_a_memory_error_when_downloading_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msvir4/getting_a_memory_error_when_downloading_a_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T16:32:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1msw3k2</id>
    <title>how to force qwen3 to stop thinking? or even just how to limit the thinking effort?</title>
    <updated>2025-08-17T16:55:15+00:00</updated>
    <author>
      <name>/u/YaBoiGPT</name>
      <uri>https://old.reddit.com/user/YaBoiGPT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey yall so im using qwen3 0.6b and it seems to ignore my no_think tags and idk how to control the reasoning effort. any tips? thx&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YaBoiGPT"&gt; /u/YaBoiGPT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msw3k2/how_to_force_qwen3_to_stop_thinking_or_even_just/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msw3k2/how_to_force_qwen3_to_stop_thinking_or_even_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msw3k2/how_to_force_qwen3_to_stop_thinking_or_even_just/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T16:55:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1msy9w5</id>
    <title>Looking for most optimal llms for ollama</title>
    <updated>2025-08-17T18:17:25+00:00</updated>
    <author>
      <name>/u/biggerbuiltbody</name>
      <uri>https://old.reddit.com/user/biggerbuiltbody</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just downloaded Ollama yesterday, and the list of all the models is a bit overwhelming, lol. i got a 300gb hard drive and an RTX 3060, and i am looking for an llm to help with some coding, general questions, maybe some math, idek, but if anyone's got any recs or even a google drive or something, I'd appreciate any help&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/biggerbuiltbody"&gt; /u/biggerbuiltbody &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msy9w5/looking_for_most_optimal_llms_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msy9w5/looking_for_most_optimal_llms_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msy9w5/looking_for_most_optimal_llms_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T18:17:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms8ghv</id>
    <title>I made a no-install-needed web-GUI for Ollama</title>
    <updated>2025-08-16T21:32:51+00:00</updated>
    <author>
      <name>/u/DarkTom21</name>
      <uri>https://old.reddit.com/user/DarkTom21</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ms8ghv/i_made_a_noinstallneeded_webgui_for_ollama/"&gt; &lt;img alt="I made a no-install-needed web-GUI for Ollama" src="https://b.thumbs.redditmedia.com/fUb2ZooOQRQelk4szZCBNb_brztbxvoPSTkbeSXzDhk.jpg" title="I made a no-install-needed web-GUI for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;For the last while I've been working on a solution to a problem I've had ever since getting into Ollama, that being a GUI that is both powerful and easy to set up across multiple devices. Firstly I tried using OpenWebUI, however quickly dropped it due to needing to install either Python or Docker just to run it, and I didn't want to install more runtimes just to run a GUI. I looked at alternatives, but none seemed to quite fit what I wanted.&lt;/p&gt; &lt;p&gt;That's why I decided to make LlamaPen, LlamaPen is an open-source, no-download-required web app/GUI that lets you easily interface with your local instance of Ollama without needing to download anything extra. It contains all the basics you would expect from a GUI such as chats, conversations, and model selection, but also contains additional features, such as model management, downloading, mobile, PWA &amp;amp; offline support, formatting markdown and think text, icons for each model, and more, all without needing to go through a lengthy download and setup process.&lt;/p&gt; &lt;p&gt;It is currently available live at &lt;a href="https://llamapen.app/"&gt;https://llamapen.app/&lt;/a&gt; with a GitHub repo going further into the specifics and features at &lt;a href="https://github.com/ImDarkTom/LlamaPen"&gt;https://github.com/ImDarkTom/LlamaPen&lt;/a&gt;. If you have any questions or would like to know more feel free to leave a comment here and I will try to reply as soon as possible, and if you encounter any issues you can either comment here or I recommend opening an issue on the GitHub repo for faster support.&lt;/p&gt; &lt;p&gt;Thanks for reading and I hope at least one other person than me finds this useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkTom21"&gt; /u/DarkTom21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ms8ghv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ms8ghv/i_made_a_noinstallneeded_webgui_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ms8ghv/i_made_a_noinstallneeded_webgui_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T21:32:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1msobrw</id>
    <title>Chat Box: Open-Source Browser Extension</title>
    <updated>2025-08-17T11:21:41+00:00</updated>
    <author>
      <name>/u/MinhxThanh</name>
      <uri>https://old.reddit.com/user/MinhxThanh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1msobrw/chat_box_opensource_browser_extension/"&gt; &lt;img alt="Chat Box: Open-Source Browser Extension" src="https://external-preview.redd.it/eGN6eTcxOG9ka2pmMVQLTLeRVkAMK-J22uN-yMath0VT_Z9kdFkV696qxezn.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f15eeef2e915c27a053a58c45bde7ef6fa8155a9" title="Chat Box: Open-Source Browser Extension" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I wanted to share this open-source project I've come across called Chat Box. It's a browser extension that brings AI chat, advanced web search, document interaction, and other handy tools right into a sidebar in your browser. It's designed to make your online workflow smoother without needing to switch tabs or apps constantly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What It Does&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;At its core, Chat Box gives you a persistent AI-powered chat interface that you can access with a quick shortcut (Ctrl+E or Cmd+E). It supports a bunch of AI providers like OpenAI, DeepSeek, Claude, and even local LLMs via Ollama. You just configure your API keys in the settings, and you're good to go.&lt;/p&gt; &lt;p&gt;It's all open-source under GPL-3.0, so you can tweak it if you want.&lt;/p&gt; &lt;p&gt;If you run into any errors, issues, or want to suggest a new feature, please create a new Issue on GitHub and describe it in detail – I'll respond ASAP!&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/MinhxThanh/Chat-Box"&gt;https://github.com/MinhxThanh/Chat-Box&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chrome Web Store: &lt;a href="https://chromewebstore.google.com/detail/chat-box-chat-with-all-ai/hhaaoibkigonnoedcocnkehipecgdodm"&gt;https://chromewebstore.google.com/detail/chat-box-chat-with-all-ai/hhaaoibkigonnoedcocnkehipecgdodm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Firefox Add-Ons: &lt;a href="https://addons.mozilla.org/en-US/firefox/addon/chat-box-chat-with-all-ai/"&gt;https://addons.mozilla.org/en-US/firefox/addon/chat-box-chat-with-all-ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MinhxThanh"&gt; /u/MinhxThanh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uv7l508odkjf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msobrw/chat_box_opensource_browser_extension/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msobrw/chat_box_opensource_browser_extension/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T11:21:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt4fjp</id>
    <title>Connected Chrome's AI API to ollama, enabling ANY web app built for Chrome's local Gemini to seamlessly work with open-source LLMs</title>
    <updated>2025-08-17T22:19:07+00:00</updated>
    <author>
      <name>/u/andrei0david</name>
      <uri>https://old.reddit.com/user/andrei0david</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andrei0david"&gt; /u/andrei0david &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/AndreiDavid/status/1956692594878038046"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mt4fjp/connected_chromes_ai_api_to_ollama_enabling_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mt4fjp/connected_chromes_ai_api_to_ollama_enabling_any/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T22:19:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1msjayu</id>
    <title>Knowledge graph using gemma3</title>
    <updated>2025-08-17T06:13:22+00:00</updated>
    <author>
      <name>/u/cython_boy</name>
      <uri>https://old.reddit.com/user/cython_boy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1msjayu/knowledge_graph_using_gemma3/"&gt; &lt;img alt="Knowledge graph using gemma3" src="https://preview.redd.it/fviiip42uijf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1890c5ec0cd777145b90a7dff123fc028801135" title="Knowledge graph using gemma3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;strong&gt;Streamlit web app&lt;/strong&gt; that generates interactive &lt;strong&gt;knowledge graphs&lt;/strong&gt; from plain text using &lt;strong&gt;Ollama's open sourcw models.(geema3 , grnaite , llama3 , gpt-oss ,....)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Two input methods&lt;/strong&gt;: Upload &lt;code&gt;.txt&lt;/code&gt; file or paste text directly.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama model integration&lt;/strong&gt;: Select from available local models (e.g., Gemma, Mistral, LLaMA).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Automatic graph storage&lt;/strong&gt;: Generated graphs are saved and can be reloaded anytime.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interactive visualization&lt;/strong&gt;: Zoom, drag, and explore relationships between concepts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optimized for speed&lt;/strong&gt;: Uses hashed filenames to prevent regenerating the same graph.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/ganeshnikhil/Kgraph"&gt;kgraph&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cython_boy"&gt; /u/cython_boy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fviiip42uijf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msjayu/knowledge_graph_using_gemma3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msjayu/knowledge_graph_using_gemma3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T06:13:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtclvn</id>
    <title>Qwen3 models - cannot disable thinking now?</title>
    <updated>2025-08-18T04:47:15+00:00</updated>
    <author>
      <name>/u/derSchwamm11</name>
      <uri>https://old.reddit.com/user/derSchwamm11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried a few Qwen3 models like 14b with ollama. Using /no_think in the system or user prompt has no effect, neither does setting &amp;quot;think&amp;quot;: false in the JSON payload to /chat/completions. Firing up the model in the terminal and using /set nothink also does nothing.&lt;/p&gt; &lt;p&gt;This all seems to fly in the face of documentation in the model and from ollama and I am going crazy here. Am I missing something? I am on ollama 0.11.4, linux&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/derSchwamm11"&gt; /u/derSchwamm11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtclvn/qwen3_models_cannot_disable_thinking_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtclvn/qwen3_models_cannot_disable_thinking_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtclvn/qwen3_models_cannot_disable_thinking_now/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T04:47:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mslsvy</id>
    <title>Qwen 4B on iPhone Neural Engine runs at 20t/s</title>
    <updated>2025-08-17T08:46:29+00:00</updated>
    <author>
      <name>/u/Glad-Speaker3006</name>
      <uri>https://old.reddit.com/user/Glad-Speaker3006</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mslsvy/qwen_4b_on_iphone_neural_engine_runs_at_20ts/"&gt; &lt;img alt="Qwen 4B on iPhone Neural Engine runs at 20t/s" src="https://external-preview.redd.it/cHhxMXV5aTJtampmMdanLIb7eGjfE0jGaRLozRDSee5XNaS_ENo8Z9uouVRD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ee59b94928cc5ff9aff58431a6e1d4fbdc8158b" title="Qwen 4B on iPhone Neural Engine runs at 20t/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am excited to finally bring 4B models to iPhone!&lt;/p&gt; &lt;p&gt;Vector Space is a framework that makes it possible to run LLM on iPhones &lt;strong&gt;locally on the Neural Engine&lt;/strong&gt;. This translates to:&lt;/p&gt; &lt;p&gt;⚡️Faster inference. Qwen 4B runs at &lt;strong&gt;~20 token/s&lt;/strong&gt; in short context.&lt;/p&gt; &lt;p&gt;🔋 Low Energy. Energy consumption is 1/5 compared to CPU, which means your iPhone will stay cool and it will not drain your battery. &lt;/p&gt; &lt;p&gt;Vector Space also comes with an app 📲 that allows you to download models and try out the framework with 0 code. Try it now on TestFlight:&lt;/p&gt; &lt;p&gt;&lt;a href="https://testflight.apple.com/join/HXyt2bjU"&gt;https://testflight.apple.com/join/HXyt2bjU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fine prints: 1. The app all does not guarantee the persistence of data. 2. Currently only supports hardware released on or after 2022 (&amp;gt;= iPhone 14) 3. First time model compilation will take several minutes. Subsequent loads will be instant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glad-Speaker3006"&gt; /u/Glad-Speaker3006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xewf9vn2mjjf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mslsvy/qwen_4b_on_iphone_neural_engine_runs_at_20ts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mslsvy/qwen_4b_on_iphone_neural_engine_runs_at_20ts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T08:46:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtjmdz</id>
    <title>Best GUI for mac</title>
    <updated>2025-08-18T11:41:27+00:00</updated>
    <author>
      <name>/u/Adventurous-Hunter98</name>
      <uri>https://old.reddit.com/user/Adventurous-Hunter98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, Im trying to find the best GUI for mac that offers good features. Currently there is this new gui comes with ollama but I dont want this and want to remove it. Anyone can help me with these?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Hunter98"&gt; /u/Adventurous-Hunter98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtjmdz/best_gui_for_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtjmdz/best_gui_for_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtjmdz/best_gui_for_mac/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T11:41:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt9fop</id>
    <title>Serene Pub 0.4 Release - Ollama Manager, Accessability &amp; Tags</title>
    <updated>2025-08-18T02:07:10+00:00</updated>
    <author>
      <name>/u/doolijb</name>
      <uri>https://old.reddit.com/user/doolijb</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doolijb"&gt; /u/doolijb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mt8hd1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mt9fop/serene_pub_04_release_ollama_manager/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mt9fop/serene_pub_04_release_ollama_manager/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T02:07:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtog0w</id>
    <title>RAG with Ollama &amp; OPENWEBUI</title>
    <updated>2025-08-18T14:59:42+00:00</updated>
    <author>
      <name>/u/KookyExtension6513</name>
      <uri>https://old.reddit.com/user/KookyExtension6513</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been trying for the last 2 weeks to build my own LLM with RAG for school, but I cant get the RAG part to work properly. It just doesnt give me an answer on OPENWEBUI or Terminal. Does anyone have a good tutorial that actually works or a solution that i can try?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KookyExtension6513"&gt; /u/KookyExtension6513 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtog0w/rag_with_ollama_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtog0w/rag_with_ollama_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtog0w/rag_with_ollama_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T14:59:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mte3wt</id>
    <title>RPG Game engine running on qwen3</title>
    <updated>2025-08-18T06:14:21+00:00</updated>
    <author>
      <name>/u/Humbrol2</name>
      <uri>https://old.reddit.com/user/Humbrol2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Building an rpg game engine built on ollama models, use qwen3 with 128k tokens in settings but its running a tad slow. ALways looking for feedback&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/humbrol2/RPG-Assistant"&gt;humbrol2/RPG-Assistant: An AI assisted RPG game engine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;RPG Assistant&lt;/p&gt; &lt;p&gt;An AI-powered RPG Game Master using Ollama local models for immersive tabletop gaming experiences.&lt;/p&gt; &lt;h1&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-Agent System&lt;/strong&gt;: Game Master, World Builder, Character Manager, and Narrative Engine&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Persistent Storage&lt;/strong&gt;: JSON-based world and character data management&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Command-Line Interface&lt;/strong&gt;: Easy-to-use CLI for game management&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open World Sandbox&lt;/strong&gt;: Dynamic world generation and storytelling&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Humbrol2"&gt; /u/Humbrol2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mte3wt/rpg_game_engine_running_on_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mte3wt/rpg_game_engine_running_on_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mte3wt/rpg_game_engine_running_on_qwen3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T06:14:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtk646</id>
    <title>Presenton now supports presentation generation via MCP</title>
    <updated>2025-08-18T12:07:45+00:00</updated>
    <author>
      <name>/u/goodboydhrn</name>
      <uri>https://old.reddit.com/user/goodboydhrn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mtk646/presenton_now_supports_presentation_generation/"&gt; &lt;img alt="Presenton now supports presentation generation via MCP" src="https://external-preview.redd.it/a3dmbmRvNThxcmpmMROeZeQh15N-GStyHCbN5mnRNX7F3fY722SokL4VMsiB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f06ca9eed421bc468417ca1b178b3dc8c509e26" title="Presenton now supports presentation generation via MCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Presenton, an open source AI presentation tool now supports presentation generation via MCP. &lt;/p&gt; &lt;p&gt;Simply connect to MCP and let you model or agent calls for you to generate presentation. &lt;/p&gt; &lt;p&gt;Documentation: &lt;a href="https://docs.presenton.ai/generate-presentation-over-mcp"&gt;https://docs.presenton.ai/generate-presentation-over-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/presenton/presenton"&gt;https://github.com/presenton/presenton&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goodboydhrn"&gt; /u/goodboydhrn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uf6stn58qrjf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtk646/presenton_now_supports_presentation_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtk646/presenton_now_supports_presentation_generation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T12:07:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtjbfj</id>
    <title>Olla v0.0.16 - Lightweight LLM Proxy for Homelab &amp; OnPrem AI Inference (Failover, Model-Aware Routing, Model unification &amp; monitoring)</title>
    <updated>2025-08-18T11:26:12+00:00</updated>
    <author>
      <name>/u/2shanigans</name>
      <uri>https://old.reddit.com/user/2shanigans</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mtjbfj/olla_v0016_lightweight_llm_proxy_for_homelab/"&gt; &lt;img alt="Olla v0.0.16 - Lightweight LLM Proxy for Homelab &amp;amp; OnPrem AI Inference (Failover, Model-Aware Routing, Model unification &amp;amp; monitoring)" src="https://external-preview.redd.it/2hgDQzEGicIBvEiFXP41uo4_tgisCN7G65jKz963z60.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fbf307001e63575064b40dd940e53827c468636" title="Olla v0.0.16 - Lightweight LLM Proxy for Homelab &amp;amp; OnPrem AI Inference (Failover, Model-Aware Routing, Model unification &amp;amp; monitoring)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’ve been running distributed LLM infrastructure at work for a while and over time we’ve built a few tools to make it easier to manage them. &lt;strong&gt;Olla&lt;/strong&gt; is the latest iteration - smaller, faster and we think better at handling multiple inference endpoints without the headaches.&lt;/p&gt; &lt;p&gt;The problems we kept hitting without these tools:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;One endpoint dies &amp;gt; workflows stall&lt;/li&gt; &lt;li&gt;No model unification so routing isn't great&lt;/li&gt; &lt;li&gt;No unified load balancing across boxes&lt;/li&gt; &lt;li&gt;Limited visibility into what’s actually healthy&lt;/li&gt; &lt;li&gt;Failures when querying because of it&lt;/li&gt; &lt;li&gt;We'd love to merge all them into OpenAI queryable endpoints&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Olla fixes that - or tries to. It’s a lightweight Go proxy that sits in front of Ollama, LM Studio, vLLM or OpenAI-compatible backends (or endpoints) and:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Auto-failover with health checks (transparent to callers)&lt;/li&gt; &lt;li&gt;Model-aware routing (knows what’s available where)&lt;/li&gt; &lt;li&gt;Priority-based, round-robin, or least-connections balancing&lt;/li&gt; &lt;li&gt;Normalises model names for the same provider so it's seen as one big list say in OpenWebUI&lt;/li&gt; &lt;li&gt;Safeguards like circuit breakers, rate limits, size caps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’ve been running it in production for months now, and a few other large orgs are using it too for local inference via on prem MacStudios, RTX 6000 rigs.&lt;/p&gt; &lt;p&gt;A few folks that use &lt;a href="https://thushan.github.io/olla/usage/#development-tools-junie"&gt;JetBrains Junie just use Olla&lt;/a&gt; in the middle so they can work from home or work without configuring each time (and possibly cursor etc).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/thushan/olla"&gt;https://github.com/thushan/olla&lt;/a&gt;&lt;br /&gt; Docs: &lt;a href="https://thushan.github.io/olla/"&gt;https://thushan.github.io/olla/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Next up: auth support so it can also proxy to OpenRouter, GroqCloud, etc.&lt;/p&gt; &lt;p&gt;If you give it a spin, let us know how it goes (and what breaks). Oh yes, &lt;a href="https://thushan.github.io/olla/about/#the-name-olla"&gt;Olla does mean other things&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/2shanigans"&gt; /u/2shanigans &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/thushan/olla"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtjbfj/olla_v0016_lightweight_llm_proxy_for_homelab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtjbfj/olla_v0016_lightweight_llm_proxy_for_homelab/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T11:26:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtm2b4</id>
    <title>Why does Qwen3 (8B &amp; 14B) have a smaller context window than Qwen3 (4B)?</title>
    <updated>2025-08-18T13:28:42+00:00</updated>
    <author>
      <name>/u/arush1836</name>
      <uri>https://old.reddit.com/user/arush1836</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mtm2b4/why_does_qwen3_8b_14b_have_a_smaller_context/"&gt; &lt;img alt="Why does Qwen3 (8B &amp;amp; 14B) have a smaller context window than Qwen3 (4B)?" src="https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417" title="Why does Qwen3 (8B &amp;amp; 14B) have a smaller context window than Qwen3 (4B)?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed that Qwen3 (8B) and Qwen3 (14B) as shown here appear to have a smaller context window compared to Qwen3 (4B). Can someone clarify why this is the case? Also, is this information even accurate?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bhogzh245sjf1.png?width=871&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=07257d631cb170d622d462d6604d4c88dda9458e"&gt;https://preview.redd.it/bhogzh245sjf1.png?width=871&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=07257d631cb170d622d462d6604d4c88dda9458e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/qwen3"&gt;https://ollama.com/library/qwen3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arush1836"&gt; /u/arush1836 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtm2b4/why_does_qwen3_8b_14b_have_a_smaller_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtm2b4/why_does_qwen3_8b_14b_have_a_smaller_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtm2b4/why_does_qwen3_8b_14b_have_a_smaller_context/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T13:28:42+00:00</published>
  </entry>
</feed>
