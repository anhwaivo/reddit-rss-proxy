<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-05T18:38:24+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ihjfy9</id>
    <title>Slow performance on K8S</title>
    <updated>2025-02-04T14:54:29+00:00</updated>
    <author>
      <name>/u/geeky217</name>
      <uri>https://old.reddit.com/user/geeky217</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m trying to run ollama with deepseek-r1 7b on CPU only inside K8S. I’m using the official helm chart inside RedHat Openshift 4.12.2 with cephRBD storage. This is all on a vm (it’s a single node openshift dev box) with 24 cores of 6148 gold Xeon cpu and 96GB. The ollama deployment has 16 cores and 16GB set as reserved. Now the issue is that it runs like a dog compared to ollama on a basic Ubuntu vm on the same esx host which has half the resources (8&amp;amp;8). The only difference is one is containerised the other just a vm. I’m at a loss why there is such a performance difference. Both ollama instances run off nvme local storage so have plenty of bandwidth and low latency. Anyone got any insights here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geeky217"&gt; /u/geeky217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihjfy9/slow_performance_on_k8s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihjfy9/slow_performance_on_k8s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihjfy9/slow_performance_on_k8s/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T14:54:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihzvrd</id>
    <title>Deepseek benchmarks</title>
    <updated>2025-02-05T02:39:54+00:00</updated>
    <author>
      <name>/u/darkgamer_nw</name>
      <uri>https://old.reddit.com/user/darkgamer_nw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Where can I find benchmarks comparing the performance of deepseek-r1 8b with llama with a similar number of parameters?&lt;/p&gt; &lt;p&gt;I cannot find a page with benchmarks on recent models&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darkgamer_nw"&gt; /u/darkgamer_nw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihzvrd/deepseek_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihzvrd/deepseek_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihzvrd/deepseek_benchmarks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T02:39:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii8af6</id>
    <title>Any help for deepseek r1:70b</title>
    <updated>2025-02-05T11:54:12+00:00</updated>
    <author>
      <name>/u/isikkusgoz</name>
      <uri>https://old.reddit.com/user/isikkusgoz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi i have a setup which is like Nvdia A4000 16gb, 256 gb ram Do you guys think i can run 70b model with reasonable token speeds?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/isikkusgoz"&gt; /u/isikkusgoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii8af6/any_help_for_deepseek_r170b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii8af6/any_help_for_deepseek_r170b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii8af6/any_help_for_deepseek_r170b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T11:54:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihu6s2</id>
    <title>Need help with training</title>
    <updated>2025-02-04T22:15:47+00:00</updated>
    <author>
      <name>/u/Kind_Ad_2866</name>
      <uri>https://old.reddit.com/user/Kind_Ad_2866</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just entered the AI race not too long ago and I have some concepts to wrap my head around &lt;/p&gt; &lt;p&gt;1- what’s the difference between training the model via chat and training the model via unsloth&lt;/p&gt; &lt;p&gt;2- what’s the difference between datasets fed to unsloth and standard RAG where you upload bunch of files then you can ask the model about them? I’m asking because I have pdf text files (books, novels, etc) and I want to chat with the models about it or ask the AI to give me a decision based on data in these files. &lt;/p&gt; &lt;p&gt;3- if unsloth is the way to go, how would I go about creating a dataset for a novel? I have seen datasets where they mention the characters, but I don’t understand how the model would piece the story just buy using the character description!&lt;/p&gt; &lt;p&gt;here is an example on the dataset I mean: &lt;a href="https://huggingface.co/datasets/xywang1/OpenCharacter?row=1"&gt;https://huggingface.co/datasets/xywang1/OpenCharacter?row=1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kind_Ad_2866"&gt; /u/Kind_Ad_2866 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihu6s2/need_help_with_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihu6s2/need_help_with_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihu6s2/need_help_with_training/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T22:15:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii3nvf</id>
    <title>Deepseek Ollama download issue</title>
    <updated>2025-02-05T06:13:17+00:00</updated>
    <author>
      <name>/u/RecordSimilar2356</name>
      <uri>https://old.reddit.com/user/RecordSimilar2356</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am downloading deepseek-r1:7b using Ollama in my windows 11, but the downloading keep rolling back even if the internet speed is fine. The downloading reaches 45% let's say, then after sometime it back to 30% or even less. The downloading never completes! Any help or guide?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RecordSimilar2356"&gt; /u/RecordSimilar2356 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii3nvf/deepseek_ollama_download_issue/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii3nvf/deepseek_ollama_download_issue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii3nvf/deepseek_ollama_download_issue/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T06:13:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii4wb3</id>
    <title>As a noob, which language model could you suggest for me?</title>
    <updated>2025-02-05T07:39:53+00:00</updated>
    <author>
      <name>/u/Live-Pause-6543</name>
      <uri>https://old.reddit.com/user/Live-Pause-6543</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a macbook pro with m1 pro chip. And every model writes the requirements based on vram. Therefore, I have no Idea which model I can run without any issues. &lt;/p&gt; &lt;p&gt;For now, I’ve only tried the distilled version of deepseek r1 with 7b. It worked really fast but the results were awful, although my expectations were low bc I know that these models performs worse than the website version of deepseek. &lt;/p&gt; &lt;p&gt;I will mostly use this for coding, but the languages I use changes often. (For school, I use Java, at a student internship Tailwind and vue and for personal coding, I try to explore game development, so c# c++ either. &lt;/p&gt; &lt;p&gt;Basically for general coding and sometimes for writing essays for school and so on. &lt;/p&gt; &lt;p&gt;For writing essays etc. I did not use any local model, I know the distilled version of r1 performs well just in coding and math. So, for other models, I am open to use. &lt;/p&gt; &lt;p&gt;With that being said, can you suggest me any model to discover the local llms? &lt;/p&gt; &lt;p&gt;Thx for your answers :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Live-Pause-6543"&gt; /u/Live-Pause-6543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii4wb3/as_a_noob_which_language_model_could_you_suggest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii4wb3/as_a_noob_which_language_model_could_you_suggest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii4wb3/as_a_noob_which_language_model_could_you_suggest/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T07:39:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii4zg6</id>
    <title>500: Ollama: 500, message='Internal Server Error', url='http://host.docker.internal:11434/api/chat '</title>
    <updated>2025-02-05T07:46:19+00:00</updated>
    <author>
      <name>/u/PCOwner12</name>
      <uri>https://old.reddit.com/user/PCOwner12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm relatively new to the local models and AI. I just ran my 1st docker with Ollama Win 11 and it was working perfectly, but now I consitently get this error. Does anyone know how to resolve it?&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PCOwner12"&gt; /u/PCOwner12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii4zg6/500_ollama_500_messageinternal_server_error/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii4zg6/500_ollama_500_messageinternal_server_error/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii4zg6/500_ollama_500_messageinternal_server_error/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T07:46:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihqub9</id>
    <title>I want to get into Local LLMs for coding, home assistant, and maybe a little conversation. Low token/s is fine to see if I even like it. Which hardware that I have listed could do it? Or do I need a GPU solely for this, even to start off?</title>
    <updated>2025-02-04T19:59:03+00:00</updated>
    <author>
      <name>/u/bigrjsuto</name>
      <uri>https://old.reddit.com/user/bigrjsuto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have the following hardware:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Personal PC (LInux Mint 21) &lt;ul&gt; &lt;li&gt;Ryzen 5800X, 64GB DDR4, 3060 12GB, 1TB NVMe + 2TB NVMe&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Server PC (Proxmox) &lt;ul&gt; &lt;li&gt;Intel 12500T, 128GB DDR4, A4000 16GB (passthrough to Windows 11 VM for Solidworks), 128GB NVMe (boot) + 1TB NVMe (VMs/LXCs), 2x 18TB HDDs&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Gaming/HTPC (Windows) &lt;ul&gt; &lt;li&gt;Intel 10600K, 32GB DDR4, RX 590 8GB, 128GB + 1TB SSD&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;MiniPCs &lt;ul&gt; &lt;li&gt;20x Datto ALTO 3 V2 &lt;ul&gt; &lt;li&gt;Celeron 3865U, 2-16GB DDR4 (Can configure as needed)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;2x Datto S3X2 Dual-NIC &lt;ul&gt; &lt;li&gt;Intel i3-7100U, 2-16GB DDR4&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;4x Optiplex 3040 &lt;ul&gt; &lt;li&gt;Intel i3-6100T, 2-8GB DDR3&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Intel NUC &lt;ul&gt; &lt;li&gt;Intel i7-7567U, 2-16GB DDR4&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Other Hardware &lt;ul&gt; &lt;li&gt;RX 560 4GB LP&lt;/li&gt; &lt;li&gt;Various extra HDDs 160GB - 8TB&lt;/li&gt; &lt;li&gt;Various extra NVMe 32GB - 500GB&lt;/li&gt; &lt;li&gt;A few extra network switches (if needed for clustering)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wanted to add the RX 560 to the server, but after some research, everything I've seen says that 4GB is too little VRAM for even slow output. That's the case, right?&lt;/p&gt; &lt;p&gt;How about a Coral TPU? Or multiple? Each of those Datto ALTO MiniPCs have a A+E keyed m.2 slot, where I could place them and cluster them together.&lt;/p&gt; &lt;p&gt;Could I just run it on my PC? Would the 3060 be good enough to get some output?&lt;/p&gt; &lt;p&gt;I know there's the A4000, but I need it for CAD work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigrjsuto"&gt; /u/bigrjsuto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihqub9/i_want_to_get_into_local_llms_for_coding_home/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihqub9/i_want_to_get_into_local_llms_for_coding_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihqub9/i_want_to_get_into_local_llms_for_coding_home/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T19:59:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii5ky5</id>
    <title>Facing error while installing deepseek 1.5b</title>
    <updated>2025-02-05T08:31:00+00:00</updated>
    <author>
      <name>/u/The_Arcane19</name>
      <uri>https://old.reddit.com/user/The_Arcane19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ii5ky5/facing_error_while_installing_deepseek_15b/"&gt; &lt;img alt="Facing error while installing deepseek 1.5b" src="https://preview.redd.it/l5hpg8997ahe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=301ecaa1fa019c3b33920f10364ee9848f6749b7" title="Facing error while installing deepseek 1.5b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The_Arcane19"&gt; /u/The_Arcane19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l5hpg8997ahe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii5ky5/facing_error_while_installing_deepseek_15b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii5ky5/facing_error_while_installing_deepseek_15b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T08:31:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1iht2tf</id>
    <title>How To Setup DeepSeek-R1 LLM AI ChatBot Using Ollama On An Ubuntu Linux GPU Cloud Server (VPS)</title>
    <updated>2025-02-04T21:29:42+00:00</updated>
    <author>
      <name>/u/websplaining</name>
      <uri>https://old.reddit.com/user/websplaining</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iht2tf/how_to_setup_deepseekr1_llm_ai_chatbot_using/"&gt; &lt;img alt="How To Setup DeepSeek-R1 LLM AI ChatBot Using Ollama On An Ubuntu Linux GPU Cloud Server (VPS)" src="https://external-preview.redd.it/uNOHV2Maw2LzcoSfFMG6sju1JNogiBy71eR2QYEqjOQ.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbe5a0f4cb1ca54f001e5f84b38b48e405eb752e" title="How To Setup DeepSeek-R1 LLM AI ChatBot Using Ollama On An Ubuntu Linux GPU Cloud Server (VPS)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/websplaining"&gt; /u/websplaining &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/S_JEkuE9EyU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iht2tf/how_to_setup_deepseekr1_llm_ai_chatbot_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iht2tf/how_to_setup_deepseekr1_llm_ai_chatbot_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T21:29:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii6duy</id>
    <title>How do I connect vs code on a client machine to my LLM running on my local server (in Docker)?</title>
    <updated>2025-02-05T09:33:49+00:00</updated>
    <author>
      <name>/u/Serious_Gap_3403</name>
      <uri>https://old.reddit.com/user/Serious_Gap_3403</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Goal: A free, locally hosted and customizable 'co-pilot'.&lt;/p&gt; &lt;p&gt;I have Ollama running in a container in docker on my ubuntu server and want to utilize a code-generating LLM when I use vs code on my windows machine, I am guessing I need to configure a vs code extension to connect to the port of this container somehow? If anyone has tried anything similar any recommendations for such an extension or any guides/resources that were helpful It would be very appreciated, Cheers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Serious_Gap_3403"&gt; /u/Serious_Gap_3403 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6duy/how_do_i_connect_vs_code_on_a_client_machine_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6duy/how_do_i_connect_vs_code_on_a_client_machine_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii6duy/how_do_i_connect_vs_code_on_a_client_machine_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T09:33:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii6jgj</id>
    <title>why Im not able to install</title>
    <updated>2025-02-05T09:46:02+00:00</updated>
    <author>
      <name>/u/Delicious-Ad4105</name>
      <uri>https://old.reddit.com/user/Delicious-Ad4105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ii6jgj/why_im_not_able_to_install/"&gt; &lt;img alt="why Im not able to install" src="https://b.thumbs.redditmedia.com/qdsRLqINGpalRJE3F5p1mZDWIIjWGTMvJ9CLGTer7Gw.jpg" title="why Im not able to install" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/dkmgee4lkahe1.png?width=1453&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02b4979f39589368298d4812e4b4f6234473d89e"&gt;https://preview.redd.it/dkmgee4lkahe1.png?width=1453&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02b4979f39589368298d4812e4b4f6234473d89e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h7shcwtpkahe1.png?width=344&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1095f1c38997641dd03e46a1e09933692503a70"&gt;https://preview.redd.it/h7shcwtpkahe1.png?width=344&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1095f1c38997641dd03e46a1e09933692503a70&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious-Ad4105"&gt; /u/Delicious-Ad4105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6jgj/why_im_not_able_to_install/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6jgj/why_im_not_able_to_install/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii6jgj/why_im_not_able_to_install/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T09:46:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihnhpl</id>
    <title>Whats the best open source model for video generation?</title>
    <updated>2025-02-04T17:43:10+00:00</updated>
    <author>
      <name>/u/gl2101</name>
      <uri>https://old.reddit.com/user/gl2101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im currently running a 3060 setup but planning to upgrade to a more powerful GPU. &lt;/p&gt; &lt;p&gt;My main goal is to build ai videos but I don’t know where to start. &lt;/p&gt; &lt;p&gt;Any recommendations are greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gl2101"&gt; /u/gl2101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihnhpl/whats_the_best_open_source_model_for_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihnhpl/whats_the_best_open_source_model_for_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihnhpl/whats_the_best_open_source_model_for_video/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T17:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihibp9</id>
    <title>Is Wikipedia RAG possible entirely locally with a gaming machine?</title>
    <updated>2025-02-04T14:02:46+00:00</updated>
    <author>
      <name>/u/trichofobia</name>
      <uri>https://old.reddit.com/user/trichofobia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y'all, I'm super super new, so I'm sorry if this is a stupid question, but I just heard what RAG is, I'd like to improve a local model (I'm only really familiar with deepseek, but I understand that ollama is great with RAG) with RAG.&lt;/p&gt; &lt;p&gt;I'd like to download Wikipedia locally, and use that for RAG. I've got a passable gaming laptop I don't use which has 32gb RAM, an RTX 3070 and an i7, along with an SSD.&lt;/p&gt; &lt;p&gt;I know I can download Wikipedia without images and it's something like 12-17gb. Would a local LLM be capable of searching through it automatically and choosing the best 2-3 articles based on my question? Or am I opening a can of worms?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/trichofobia"&gt; /u/trichofobia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihibp9/is_wikipedia_rag_possible_entirely_locally_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihibp9/is_wikipedia_rag_possible_entirely_locally_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihibp9/is_wikipedia_rag_possible_entirely_locally_with_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T14:02:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii7zdv</id>
    <title>Does anyone know a good AI voice-changing model for ollama or any other model that has a similar function?</title>
    <updated>2025-02-05T11:33:18+00:00</updated>
    <author>
      <name>/u/Mallowfanthe4th</name>
      <uri>https://old.reddit.com/user/Mallowfanthe4th</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean as in the replication other people's voices (just for fun).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mallowfanthe4th"&gt; /u/Mallowfanthe4th &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii7zdv/does_anyone_know_a_good_ai_voicechanging_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii7zdv/does_anyone_know_a_good_ai_voicechanging_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii7zdv/does_anyone_know_a_good_ai_voicechanging_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T11:33:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii91b6</id>
    <title>Speech recognition for core EU languages?</title>
    <updated>2025-02-05T12:40:03+00:00</updated>
    <author>
      <name>/u/anderssewerin</name>
      <uri>https://old.reddit.com/user/anderssewerin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the current best model, preferably open, for this? I am testing out Whisper locally and so far it seems pretty solid, but would love to hear experiences from others here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anderssewerin"&gt; /u/anderssewerin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii91b6/speech_recognition_for_core_eu_languages/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii91b6/speech_recognition_for_core_eu_languages/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii91b6/speech_recognition_for_core_eu_languages/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T12:40:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iif4cu</id>
    <title>Is there any way to add my local drive data to a model?</title>
    <updated>2025-02-05T17:13:30+00:00</updated>
    <author>
      <name>/u/billhughes1960</name>
      <uri>https://old.reddit.com/user/billhughes1960</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently using Llama 3.2 Vision with Ollama, and I must say, the experience has been fantastic so far. &lt;/p&gt; &lt;p&gt;However, I've noticed that the model's training data is somewhat outdated, dating back to 2018. One feature that would greatly enhance my interaction with this model is the ability to supplement its knowledge with my own documents and data. Are there any tools or methods available to update or augment the model with user-provided information?&lt;/p&gt; &lt;p&gt;(This post was made better by Ollama!) :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/billhughes1960"&gt; /u/billhughes1960 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iif4cu/is_there_any_way_to_add_my_local_drive_data_to_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iif4cu/is_there_any_way_to_add_my_local_drive_data_to_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iif4cu/is_there_any_way_to_add_my_local_drive_data_to_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T17:13:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiflm1</id>
    <title>Using DeepSeek To Make A Game</title>
    <updated>2025-02-05T17:33:01+00:00</updated>
    <author>
      <name>/u/sveennn</name>
      <uri>https://old.reddit.com/user/sveennn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iiflm1/using_deepseek_to_make_a_game/"&gt; &lt;img alt="Using DeepSeek To Make A Game" src="https://external-preview.redd.it/Jsnuak6K7miLugvD2wowYcZVRD06OvCXYvnQr335CGY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7128c34bc49c26ff56a9bf1a8250bb7747e29153" title="Using DeepSeek To Make A Game" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sveennn"&gt; /u/sveennn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@sveennn/using-deepseek-to-make-a-game-db6932157495"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iiflm1/using_deepseek_to_make_a_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iiflm1/using_deepseek_to_make_a_game/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T17:33:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihfyi2</id>
    <title>Ollama + DeepSeek + Obsidian = The Ultimate Offline AI Assistant!</title>
    <updated>2025-02-04T11:54:17+00:00</updated>
    <author>
      <name>/u/Kind-Industry-609</name>
      <uri>https://old.reddit.com/user/Kind-Industry-609</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ihfyi2/ollama_deepseek_obsidian_the_ultimate_offline_ai/"&gt; &lt;img alt="Ollama + DeepSeek + Obsidian = The Ultimate Offline AI Assistant!" src="https://external-preview.redd.it/32JgoJVP2Vxa0PebR1pmCtaV_33XwoDfHhsNkStqIjE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49a27f19a9e63b070eb41d43f95929897b33eb6f" title="Ollama + DeepSeek + Obsidian = The Ultimate Offline AI Assistant!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kind-Industry-609"&gt; /u/Kind-Industry-609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/qAsGO5N7OCk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihfyi2/ollama_deepseek_obsidian_the_ultimate_offline_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihfyi2/ollama_deepseek_obsidian_the_ultimate_offline_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T11:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iib7um</id>
    <title>Sure way to make them hallucinate - ask them for explicit information like song lyrics. This is Bohemian Rhapsody, according to Llama3.2:3b-instruct-q8_0</title>
    <updated>2025-02-05T14:30:27+00:00</updated>
    <author>
      <name>/u/hn-mc</name>
      <uri>https://old.reddit.com/user/hn-mc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have you tested other models? Are bigger ones more accurate?&lt;/p&gt; &lt;p&gt;Anyway, here's my test:&lt;/p&gt; &lt;p&gt;Note (I'll add my own comments, marked with // as in programming)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;gt;&amp;gt;&amp;gt; Do you know the lyrics of Bohemian Rhapsody?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I do know the lyrics to &amp;quot;Bohemian Rhapsody&amp;quot; by Queen. The song is famous for its complex composition and operatic sections, making it a bit challenging to quote accurately. However, I'll provide you with the main lyrics:&lt;/p&gt; &lt;p&gt;Is this the real life?&lt;/p&gt; &lt;p&gt;Is this just fantasy?&lt;/p&gt; &lt;p&gt;Caught in a landslide,&lt;/p&gt; &lt;p&gt;No escape from reality&lt;/p&gt; &lt;p&gt;Open your eyes, look up to the skies and see&lt;/p&gt; &lt;p&gt;I'm just a poor boy, I need no sympathy&lt;/p&gt; &lt;p&gt;Because I'm easy come, easy go, little high, little low&lt;/p&gt; &lt;p&gt;Any way the wind blows doesn't really matter to me // so far so good&lt;/p&gt; &lt;p&gt;// skipped another &amp;quot;to me&amp;quot;&lt;/p&gt; &lt;p&gt;// skipped the whole section about killing - is this censorship, or natural aversion towards violence?&lt;/p&gt; &lt;p&gt;Mama, life had just begun&lt;/p&gt; &lt;p&gt;But now I've gone and thrown it all away&lt;/p&gt; &lt;p&gt;Mama, oooh (any way the wind blows) // this comes later&lt;/p&gt; &lt;p&gt;Didn't mean to make you cry&lt;/p&gt; &lt;p&gt;If I'm not back again this time tomorrow&lt;/p&gt; &lt;p&gt;Carry on, carry on as if nothing matters // as if nothing really* matters&lt;/p&gt; &lt;p&gt;Too late, my time has come&lt;/p&gt; &lt;p&gt;Sent shivers down my spine, body's aching all the time // sends*&lt;/p&gt; &lt;p&gt;Goodbye, everybody, I've got to go&lt;/p&gt; &lt;p&gt;Gotta leave you all behind and face the truth&lt;/p&gt; &lt;p&gt;Mama, oooh (any way the wind blows) // up to this point it's quite precise&lt;/p&gt; &lt;p&gt;&lt;del&gt;Didn't mean to make you cry&lt;/del&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;// From this point it descends into total chaos and incoherence:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I just walked in to find&lt;/p&gt; &lt;p&gt;Out that the crowd had gathered&lt;/p&gt; &lt;p&gt;There must have been some kind of mistake&lt;/p&gt; &lt;p&gt;They said I was in their way&lt;/p&gt; &lt;p&gt;So they put me down and threw me out&lt;/p&gt; &lt;p&gt;But I won't let them push me around&lt;/p&gt; &lt;p&gt;No, no, no, no, no, no&lt;/p&gt; &lt;p&gt;&lt;strong&gt;// Tries to regain its senses&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;(Operatic section)&lt;/p&gt; &lt;p&gt;Galileo, Galileo, Galileo, Figaro - magnifico&lt;/p&gt; &lt;p&gt;I'm just a poor boy, nobody loves me&lt;/p&gt; &lt;p&gt;Spare him his life from this monstrosity&lt;/p&gt; &lt;p&gt;Easy come, easy go, will you &lt;del&gt;never&lt;/del&gt; let me go?&lt;/p&gt; &lt;p&gt;(Bismillah) No, we will not let you go (Bismillah)&lt;/p&gt; &lt;p&gt;We will not let you go (Let him go)&lt;/p&gt; &lt;p&gt;(Big chorus)&lt;/p&gt; &lt;p&gt;So you think you can stone me and spit in my eye?&lt;/p&gt; &lt;p&gt;So you think you can love me and leave me to die?&lt;/p&gt; &lt;p&gt;Oh, baby, can't do this to me, baby&lt;/p&gt; &lt;p&gt;Just gotta get out, just gotta get right outta here&lt;/p&gt; &lt;p&gt;&lt;strong&gt;// Descends into incoherence again&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ooh, nothing matters, anyone can see&lt;/p&gt; &lt;p&gt;Nothing matters, all are free&lt;/p&gt; &lt;p&gt;Nothing matters, nothing matters&lt;/p&gt; &lt;p&gt;Nothing matters at all&lt;/p&gt; &lt;p&gt;(Big chorus)&lt;/p&gt; &lt;p&gt;(Ooh, nothing matters)&lt;/p&gt; &lt;p&gt;(Bismillah) We will not let you go (Bismillah)&lt;/p&gt; &lt;p&gt;There's a bit more to the song, but these are the main lyrics.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hn-mc"&gt; /u/hn-mc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iib7um/sure_way_to_make_them_hallucinate_ask_them_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iib7um/sure_way_to_make_them_hallucinate_ask_them_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iib7um/sure_way_to_make_them_hallucinate_ask_them_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T14:30:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii5o2q</id>
    <title>How do you know which LLM to use and for what use case?</title>
    <updated>2025-02-05T08:37:46+00:00</updated>
    <author>
      <name>/u/hexarthrius</name>
      <uri>https://old.reddit.com/user/hexarthrius</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there, I'm a newbie in local LLMs and maybe AI in general. Is there a guide out there that allows me to assess quickly which LLMs from Ollama are capable of doing which task?&lt;/p&gt; &lt;p&gt;I'd like to leverage AI in my local computer and later maybe branch it out to hosting my own personal service to do most stuff and maybe make an agent of myself to help me with my work (Corporate IT stuff).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hexarthrius"&gt; /u/hexarthrius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii5o2q/how_do_you_know_which_llm_to_use_and_for_what_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii5o2q/how_do_you_know_which_llm_to_use_and_for_what_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii5o2q/how_do_you_know_which_llm_to_use_and_for_what_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T08:37:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii11dz</id>
    <title>Deepseek r1 1.5b thinking about rose flower 🌹</title>
    <updated>2025-02-05T03:39:16+00:00</updated>
    <author>
      <name>/u/False-Woodpecker5604</name>
      <uri>https://old.reddit.com/user/False-Woodpecker5604</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ii11dz/deepseek_r1_15b_thinking_about_rose_flower/"&gt; &lt;img alt="Deepseek r1 1.5b thinking about rose flower 🌹" src="https://preview.redd.it/7axbiaybr8he1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=719cdceebe4b50f16a0fc624b6247f706549c65f" title="Deepseek r1 1.5b thinking about rose flower 🌹" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/False-Woodpecker5604"&gt; /u/False-Woodpecker5604 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7axbiaybr8he1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii11dz/deepseek_r1_15b_thinking_about_rose_flower/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii11dz/deepseek_r1_15b_thinking_about_rose_flower/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T03:39:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii6zut</id>
    <title>Optimizing Local LLMs on Mac Mini M4: Seeking Advice for Better Performance</title>
    <updated>2025-02-05T10:20:42+00:00</updated>
    <author>
      <name>/u/Killtec_Gaming</name>
      <uri>https://old.reddit.com/user/Killtec_Gaming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt; community! &lt;/p&gt; &lt;p&gt;We recently purchased a Mac Mini M4 (base model) for our office to run local AI operations. Our primary setup involves n8n for automation workflows integrated with Ollama, using mainly 7B and 14B models. &lt;/p&gt; &lt;p&gt;However, we've noticed that the results from these quantized models are significantly less impressive compared to cloud-based solutions. &lt;/p&gt; &lt;p&gt;We're looking for guidance on: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Are there specific optimization techniques or fine-tuning approaches we should consider? &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What settings have you found most effective for 7B/14B models on Apple Silicon? &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Would investing in more powerful hardware for running larger models be the only way to achieve cloud-like quality? &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any insights from those running similar setups would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Killtec_Gaming"&gt; /u/Killtec_Gaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6zut/optimizing_local_llms_on_mac_mini_m4_seeking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6zut/optimizing_local_llms_on_mac_mini_m4_seeking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii6zut/optimizing_local_llms_on_mac_mini_m4_seeking/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T10:20:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iifmpr</id>
    <title>qwen 2.5 VL on Ollama</title>
    <updated>2025-02-05T17:34:19+00:00</updated>
    <author>
      <name>/u/mans-987</name>
      <uri>https://old.reddit.com/user/mans-987</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any way to use qwen 2.5 VL with Ollama? The model is open source and can be found here on hugging face: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"&gt;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;also on github: &lt;a href="https://github.com/QwenLM/Qwen2.5-VL"&gt;https://github.com/QwenLM/Qwen2.5-VL&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mans-987"&gt; /u/mans-987 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iifmpr/qwen_25_vl_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iifmpr/qwen_25_vl_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iifmpr/qwen_25_vl_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T17:34:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii6y3n</id>
    <title>Which model is best for RAG or chatting document?</title>
    <updated>2025-02-05T10:16:59+00:00</updated>
    <author>
      <name>/u/Interesting_Music464</name>
      <uri>https://old.reddit.com/user/Interesting_Music464</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to train a model locally on my Macbook Pro M1 32GB based on a technical standard/specifications that is written in a document format like PDF. Which model would you recommend for this case? I saw that MLX is best for Apple Silicon so that is my only lead on how to properly choose a model aside from choosing the number of parameters and available unified memory.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting_Music464"&gt; /u/Interesting_Music464 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6y3n/which_model_is_best_for_rag_or_chatting_document/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6y3n/which_model_is_best_for_rag_or_chatting_document/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii6y3n/which_model_is_best_for_rag_or_chatting_document/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T10:16:59+00:00</published>
  </entry>
</feed>
