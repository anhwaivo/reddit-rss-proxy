<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-28T20:23:51+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ibadzz</id>
    <title>LLM powered scraping with ollama</title>
    <updated>2025-01-27T14:31:29+00:00</updated>
    <author>
      <name>/u/Financial-Article-12</name>
      <uri>https://old.reddit.com/user/Financial-Article-12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been developing Parsera, a simple yet powerful Python library that leverages LLMs for web scraping. Many users requested Ollama support, and now that Iâ€™ve added it, I wanted to share it with the Ollama community.&lt;/p&gt; &lt;p&gt;If you are looking for a way to extract data from websites (especially when dealing with multiple websites), Parsera lets you do it with just a few lines of code, without the need to develop custom scrapers.&lt;/p&gt; &lt;p&gt;Give it a try and let me know what you think. Feedback is always welcome!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/raznem/parsera"&gt;github.com/raznem/parsera&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial-Article-12"&gt; /u/Financial-Article-12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibadzz/llm_powered_scraping_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibadzz/llm_powered_scraping_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibadzz/llm_powered_scraping_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T14:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic0m0t</id>
    <title>Can I Use Ollama to automate repetitive tasks in third party apps like camtasia, adobe premiere pro, capcut etc.?</title>
    <updated>2025-01-28T12:38:26+00:00</updated>
    <author>
      <name>/u/Zacky96</name>
      <uri>https://old.reddit.com/user/Zacky96</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can i Ollama to automate repetitive tasks in third party apps like camtasia, adobe premiere pro, capcut etc.?&lt;/p&gt; &lt;p&gt;For Example: Capcut doesn't have a Batch Export feature for projects like other video editing tools; So, I have to sit down and export each project one by one? &lt;/p&gt; &lt;p&gt;So, Can I Use Ollama (any other alternative) to automate such repetitive tasks in 3rd party apps?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zacky96"&gt; /u/Zacky96 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic0m0t/can_i_use_ollama_to_automate_repetitive_tasks_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic0m0t/can_i_use_ollama_to_automate_repetitive_tasks_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic0m0t/can_i_use_ollama_to_automate_repetitive_tasks_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T12:38:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic0v0b</id>
    <title>Where to see System Requirements of models ?</title>
    <updated>2025-01-28T12:52:57+00:00</updated>
    <author>
      <name>/u/RedditNoobie777</name>
      <uri>https://old.reddit.com/user/RedditNoobie777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RedditNoobie777"&gt; /u/RedditNoobie777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic0v0b/where_to_see_system_requirements_of_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic0v0b/where_to_see_system_requirements_of_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic0v0b/where_to_see_system_requirements_of_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T12:52:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic0v0t</id>
    <title>Where to see System Requirements of models ?</title>
    <updated>2025-01-28T12:52:58+00:00</updated>
    <author>
      <name>/u/RedditNoobie777</name>
      <uri>https://old.reddit.com/user/RedditNoobie777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RedditNoobie777"&gt; /u/RedditNoobie777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic0v0t/where_to_see_system_requirements_of_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic0v0t/where_to_see_system_requirements_of_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic0v0t/where_to_see_system_requirements_of_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T12:52:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic0wm1</id>
    <title>Difference between deepseek-r1 models ?</title>
    <updated>2025-01-28T12:55:24+00:00</updated>
    <author>
      <name>/u/RedditNoobie777</name>
      <uri>https://old.reddit.com/user/RedditNoobie777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;llama-distill&lt;/li&gt; &lt;li&gt;qwen-distill&lt;/li&gt; &lt;li&gt;fp&lt;/li&gt; &lt;li&gt;q4&lt;/li&gt; &lt;li&gt;a8&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RedditNoobie777"&gt; /u/RedditNoobie777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic0wm1/difference_between_deepseekr1_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic0wm1/difference_between_deepseekr1_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic0wm1/difference_between_deepseekr1_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T12:55:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic19hd</id>
    <title>everytime i try to install a llm it installs incorrectly</title>
    <updated>2025-01-28T13:14:11+00:00</updated>
    <author>
      <name>/u/lilgooberj</name>
      <uri>https://old.reddit.com/user/lilgooberj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ic19hd/everytime_i_try_to_install_a_llm_it_installs/"&gt; &lt;img alt="everytime i try to install a llm it installs incorrectly" src="https://a.thumbs.redditmedia.com/DAx2Z6q5J9ghvgv7zN-Ty6xVt5nXj5-JFos5j3OOde0.jpg" title="everytime i try to install a llm it installs incorrectly" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/885ffflliqfe1.png?width=1085&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52d7963007dab48e4bb076c6907f5f07742c6eae"&gt;https://preview.redd.it/885ffflliqfe1.png?width=1085&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52d7963007dab48e4bb076c6907f5f07742c6eae&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lilgooberj"&gt; /u/lilgooberj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic19hd/everytime_i_try_to_install_a_llm_it_installs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic19hd/everytime_i_try_to_install_a_llm_it_installs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic19hd/everytime_i_try_to_install_a_llm_it_installs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T13:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic6yj3</id>
    <title>How do I connect Deepseek to the internet?</title>
    <updated>2025-01-28T17:28:40+00:00</updated>
    <author>
      <name>/u/Mrhi2u</name>
      <uri>https://old.reddit.com/user/Mrhi2u</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm really new with these kinds of things, and I was able to get Deepseek running on my computer locally, but I also want it to be able to access the net and watch videos for me, so how do I do that? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mrhi2u"&gt; /u/Mrhi2u &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic6yj3/how_do_i_connect_deepseek_to_the_internet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic6yj3/how_do_i_connect_deepseek_to_the_internet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic6yj3/how_do_i_connect_deepseek_to_the_internet/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T17:28:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic22v1</id>
    <title>Powershell - Scrolling while waiting for the answer</title>
    <updated>2025-01-28T13:55:59+00:00</updated>
    <author>
      <name>/u/majkhul</name>
      <uri>https://old.reddit.com/user/majkhul</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, i'm using Ollama on Windows with Powershell.&lt;br /&gt; I like using it like this and prefer it over using a browser.&lt;/p&gt; &lt;p&gt;But my problem is that i cannot scroll through the generated text as long as the response is still being written.&lt;br /&gt; It will always update the screen to the last write position of its answer.&lt;/p&gt; &lt;p&gt;Is there a way to adress this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/majkhul"&gt; /u/majkhul &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic22v1/powershell_scrolling_while_waiting_for_the_answer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic22v1/powershell_scrolling_while_waiting_for_the_answer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic22v1/powershell_scrolling_while_waiting_for_the_answer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T13:55:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic51h8</id>
    <title>Best model for 16GB GPU</title>
    <updated>2025-01-28T16:09:53+00:00</updated>
    <author>
      <name>/u/Flaky_Shame6323</name>
      <uri>https://old.reddit.com/user/Flaky_Shame6323</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, My professor managed to lend me a 32 GB ram PC (of which 16 are VRAM). It's an Intel NUC. I wanted to test using ollama locally to set up a endpoint for our humanities PhD group. I'm fairly new to the local setups. I was looking into DeepSeek 14B since it should fit, do you think it's a good idea or is there anything else better? I mostly need it for information extraction and JSON output. Thanks and sorry for the naiveness of the question :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flaky_Shame6323"&gt; /u/Flaky_Shame6323 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic51h8/best_model_for_16gb_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic51h8/best_model_for_16gb_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic51h8/best_model_for_16gb_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T16:09:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1iby4np</id>
    <title>Anyone got GEITje-7B?</title>
    <updated>2025-01-28T09:51:56+00:00</updated>
    <author>
      <name>/u/ScrapEngineer_</name>
      <uri>https://old.reddit.com/user/ScrapEngineer_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Dutch copyright troll Brein has taken offline GEITje-7B.&lt;/p&gt; &lt;p&gt;I had previously downloaded this model, but unfortunately lost it.&lt;/p&gt; &lt;p&gt;So does anyone had downloaded GEITje-7B before it was taken down and is willing to share it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ScrapEngineer_"&gt; /u/ScrapEngineer_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iby4np/anyone_got_geitje7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iby4np/anyone_got_geitje7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iby4np/anyone_got_geitje7b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T09:51:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iblmdi</id>
    <title>Is Deepseek R-1 overthinking everything?</title>
    <updated>2025-01-27T22:08:10+00:00</updated>
    <author>
      <name>/u/nPrevail</name>
      <uri>https://old.reddit.com/user/nPrevail</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I like the critical thinking that Deep Seek R-1 (reason) has, but I also think that it's been overthinking a lot, and giving me too many alternatives and options, and it keeps running ideas beyond reason (ha ha ha).&lt;/p&gt; &lt;p&gt;At worst, it'll give me an incorrect answer, and will talk to itself, saying things like &amp;quot;Oh, that not the right answer. Let me try it again&amp;quot; type of conversation with itself, but it will keep going until it's satisfied with its own answer.&lt;/p&gt; &lt;p&gt;Has anyone else experienced this?&lt;/p&gt; &lt;p&gt;I almost want to shift back to using llama3 as my main LLM. It was pretty straightforward, despite not giving me any critical responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nPrevail"&gt; /u/nPrevail &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iblmdi/is_deepseek_r1_overthinking_everything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iblmdi/is_deepseek_r1_overthinking_everything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iblmdi/is_deepseek_r1_overthinking_everything/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T22:08:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibzoat</id>
    <title>Why Ollama uses 90% GPU with both Deepseek r1 1.5b and 7b on MacbookPro M1Pro?</title>
    <updated>2025-01-28T11:41:24+00:00</updated>
    <author>
      <name>/u/sP0re90</name>
      <uri>https://old.reddit.com/user/sP0re90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; It's the first time I use Ollama. I'm running it with Open Web UI on Macbook Pro M1 Pro.&lt;br /&gt; I'm wondering why Ollama process in Activity Monitor shows 90% GPU usage with both 1.5b and 7b Deepseek models while waiting for an answer?&lt;br /&gt; If I type in terminal Ollama ps I see the models and both shows PROCESSOR 100% GPU. &lt;/p&gt; &lt;p&gt;They both works, not super fast answers but they do their job.&lt;br /&gt; But I'm trying to understand how Ollama/models resources consumption works.&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sP0re90"&gt; /u/sP0re90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibzoat/why_ollama_uses_90_gpu_with_both_deepseek_r1_15b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibzoat/why_ollama_uses_90_gpu_with_both_deepseek_r1_15b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibzoat/why_ollama_uses_90_gpu_with_both_deepseek_r1_15b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T11:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic78v8</id>
    <title>Is it possible to edit a thought in a &lt;think&gt; tag and regenerate the rest of a response from deepseek?</title>
    <updated>2025-01-28T17:40:25+00:00</updated>
    <author>
      <name>/u/faceplanted</name>
      <uri>https://old.reddit.com/user/faceplanted</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please correct me if I'm misunderstanding the underlying mechanism here, but I've been experimenting with deepseek and having the thinking process visible has made me wonder what would happen if I were simply to change some of the thoughts as it's having them or has just had them and then continue the inference from there.&lt;/p&gt; &lt;p&gt;Theoretically it seems reasonable as LLM's work token by token so changing something in the context window shouldn't be any different from making a prompt. So the only question is whether it's possible in ollama or if I'd have to try via something more low level.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/faceplanted"&gt; /u/faceplanted &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic78v8/is_it_possible_to_edit_a_thought_in_a_think_tag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic78v8/is_it_possible_to_edit_a_thought_in_a_think_tag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic78v8/is_it_possible_to_edit_a_thought_in_a_think_tag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T17:40:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic8da5</id>
    <title>Deepseek model for n8n tools?</title>
    <updated>2025-01-28T18:26:02+00:00</updated>
    <author>
      <name>/u/Negative_Place_2071</name>
      <uri>https://old.reddit.com/user/Negative_Place_2071</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to use &amp;quot;Ollama Chat Model&amp;quot; node for an AI Agent in n8n connected to my local running Deepseek model. But when I try to use a tool in the AI Agent it always throw an error message like this:&lt;/p&gt; &lt;p&gt;&lt;a href="http://registry.ollama.ai/library/deepseek-coder-v2:16b"&gt;registry.ollama.ai/library/deepseek-coder-v2:16b&lt;/a&gt; does not support tools&lt;/p&gt; &lt;p&gt;I tried with:&lt;br /&gt; - deepseek-r1:latest&lt;br /&gt; - deepseek-coder-v2:16b&lt;/p&gt; &lt;p&gt;I have RTX 4060 and 32GB RAM, but I'm aiming to use my RIG of 4 RTX 4090 and 256GB RAM for production &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Negative_Place_2071"&gt; /u/Negative_Place_2071 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic8da5/deepseek_model_for_n8n_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic8da5/deepseek_model_for_n8n_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic8da5/deepseek_model_for_n8n_tools/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T18:26:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic8uxw</id>
    <title>Who quantizes ollama models? Is the process transparent?</title>
    <updated>2025-01-28T18:45:51+00:00</updated>
    <author>
      <name>/u/Dogeboja</name>
      <uri>https://old.reddit.com/user/Dogeboja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There seem to be competing techniques and calibration datasets for quantization. Bartowski and Unsloth for example release GGUF models with different techniques. I am wondering how do the ollama ones compare? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dogeboja"&gt; /u/Dogeboja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic8uxw/who_quantizes_ollama_models_is_the_process/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic8uxw/who_quantizes_ollama_models_is_the_process/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic8uxw/who_quantizes_ollama_models_is_the_process/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T18:45:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic92ep</id>
    <title>Quantized Models - Base / Instruct , where are the non base / instruct options?</title>
    <updated>2025-01-28T18:54:16+00:00</updated>
    <author>
      <name>/u/vertigo235</name>
      <uri>https://old.reddit.com/user/vertigo235</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have been running models, with just the parameter tag, like 3b, 7b, 14b, etc. I know these are all quantized at Q4, and I wanted to try some of the larger quants. &lt;/p&gt; &lt;p&gt;So I look at the view all, and I see a bunch of uptions, however they all have instruct or base in the name as well. &lt;/p&gt; &lt;p&gt;I know what instruct is, but I don't want that, I tried the &amp;quot;base&amp;quot; versions, but they seem to only allow me to do context lengths of 2048 (ollama ignores my context input from Open WebUI, it took me a while to find that this was the case with these &amp;quot;base&amp;quot; models, so now I don't pull those)&lt;/p&gt; &lt;p&gt;So how do I pull a Quantized version that is just like the one without the :7b etc?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vertigo235"&gt; /u/vertigo235 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic92ep/quantized_models_base_instruct_where_are_the_non/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic92ep/quantized_models_base_instruct_where_are_the_non/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic92ep/quantized_models_base_instruct_where_are_the_non/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T18:54:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic9jlz</id>
    <title>CPU and GPU utilization less than 50%</title>
    <updated>2025-01-28T19:13:21+00:00</updated>
    <author>
      <name>/u/infinity6570</name>
      <uri>https://old.reddit.com/user/infinity6570</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am not at all experienced with running llms, I just wanted torun deepseek on my pc. I installed ollama and ran deepseek r1 32b. My pc is Ryzen 7 5700g, rtc 3070ti, 32gb ddr4 3200MHz cl16 ram, 3TB storage. When I run the model, it doesn't fully use my gou or cou, but it does run slowly. Is there any way to make it use more processors?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/infinity6570"&gt; /u/infinity6570 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic9jlz/cpu_and_gpu_utilization_less_than_50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic9jlz/cpu_and_gpu_utilization_less_than_50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic9jlz/cpu_and_gpu_utilization_less_than_50/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T19:13:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibuk3j</id>
    <title>Has anyone used the "distilled" versions of deepseek?</title>
    <updated>2025-01-28T05:28:43+00:00</updated>
    <author>
      <name>/u/daHsu</name>
      <uri>https://old.reddit.com/user/daHsu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for advice on model selection. I was looking for a DeepSeek model to download, and it turns out that not only are there 5 different sizes of the model, but they also went and fine-tuned the model on a bunch of different versions of qwen/llama as well!&lt;/p&gt; &lt;p&gt;I couldn't find any recommendations/info on these, other than their explanation&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1. We slightly change their configs and tokenizers. Please use our setting to run these models.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Does anyone have experience with them, or have any tips on when to use them? I'm not sure what a fine-tune here would really mean--like a DeepSeek that &amp;quot;talks like&amp;quot; qwen?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/daHsu"&gt; /u/daHsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibuk3j/has_anyone_used_the_distilled_versions_of_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibuk3j/has_anyone_used_the_distilled_versions_of_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibuk3j/has_anyone_used_the_distilled_versions_of_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T05:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ica4no</id>
    <title>Activate AMD Radeon Pro 5300M of Mac Pro for using DeepSeek</title>
    <updated>2025-01-28T19:36:29+00:00</updated>
    <author>
      <name>/u/AI-Ahmed</name>
      <uri>https://old.reddit.com/user/AI-Ahmed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am new using Mac Pro 2019 intell chip. Ollama doesn't recognise the GPU, I searched a lot, but I was not able to find a way to activate the GPU while using Ollama. Do anyone has an idea about that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AI-Ahmed"&gt; /u/AI-Ahmed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ica4no/activate_amd_radeon_pro_5300m_of_mac_pro_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ica4no/activate_amd_radeon_pro_5300m_of_mac_pro_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ica4no/activate_amd_radeon_pro_5300m_of_mac_pro_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T19:36:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic26pt</id>
    <title>Mollama</title>
    <updated>2025-01-28T14:01:12+00:00</updated>
    <author>
      <name>/u/No_Step2454</name>
      <uri>https://old.reddit.com/user/No_Step2454</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I have a small LLM model and I would like to deploy it locally on my smartphone is it possible to do it or I need to push it in ollama server first &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Step2454"&gt; /u/No_Step2454 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic26pt/mollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic26pt/mollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic26pt/mollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T14:01:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1icaxwm</id>
    <title>Transcription tools</title>
    <updated>2025-01-28T20:08:52+00:00</updated>
    <author>
      <name>/u/azimuth79b</name>
      <uri>https://old.reddit.com/user/azimuth79b</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Things move so fast since several weeks since I was last reading . Work &amp;amp; personal issues :/&lt;/p&gt; &lt;p&gt;So, any free tools to do video to text using Ollama etc? Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/azimuth79b"&gt; /u/azimuth79b &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icaxwm/transcription_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1icaxwm/transcription_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1icaxwm/transcription_tools/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T20:08:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibngl3</id>
    <title>Qwen2.5-VL just released</title>
    <updated>2025-01-27T23:26:33+00:00</updated>
    <author>
      <name>/u/numinouslymusing</name>
      <uri>https://old.reddit.com/user/numinouslymusing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5"&gt;https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/numinouslymusing"&gt; /u/numinouslymusing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibngl3/qwen25vl_just_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibngl3/qwen25vl_just_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibngl3/qwen25vl_just_released/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T23:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic7lt5</id>
    <title>Which Ollama local UI for Windows is the lightest and fastest?</title>
    <updated>2025-01-28T17:55:06+00:00</updated>
    <author>
      <name>/u/mazapo101</name>
      <uri>https://old.reddit.com/user/mazapo101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Through command line I can run ollama with deepseek-r1:32b and it works, it types the response a bit slow, but it works fine.&lt;/p&gt; &lt;p&gt;I tried installing Open WebUI through Docker, but it takes almost 3 minutes to start typing the thinking. I also tried AnythingLLM, but the same thing happens.&lt;/p&gt; &lt;p&gt;I just want an UI to have a more comfortable chat and to be able to keep my chat history. What options are there?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mazapo101"&gt; /u/mazapo101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic7lt5/which_ollama_local_ui_for_windows_is_the_lightest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ic7lt5/which_ollama_local_ui_for_windows_is_the_lightest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ic7lt5/which_ollama_local_ui_for_windows_is_the_lightest/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T17:55:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibulvz</id>
    <title>These random accounts have been showing up ever since I started using ollama. Should I be worried?</title>
    <updated>2025-01-28T05:31:49+00:00</updated>
    <author>
      <name>/u/Liquidmesh</name>
      <uri>https://old.reddit.com/user/Liquidmesh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ibulvz/these_random_accounts_have_been_showing_up_ever/"&gt; &lt;img alt="These random accounts have been showing up ever since I started using ollama. Should I be worried?" src="https://preview.redd.it/lqatzv6y7ofe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=21464609696b2ac648efbca375cb46f4d41f5c57" title="These random accounts have been showing up ever since I started using ollama. Should I be worried?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Liquidmesh"&gt; /u/Liquidmesh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lqatzv6y7ofe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibulvz/these_random_accounts_have_been_showing_up_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibulvz/these_random_accounts_have_been_showing_up_ever/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T05:31:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibwdvx</id>
    <title>Ollama enjoying the Chinese New Year! Open Source FTW ðŸš€</title>
    <updated>2025-01-28T07:34:16+00:00</updated>
    <author>
      <name>/u/eternviking</name>
      <uri>https://old.reddit.com/user/eternviking</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ibwdvx/ollama_enjoying_the_chinese_new_year_open_source/"&gt; &lt;img alt="Ollama enjoying the Chinese New Year! Open Source FTW ðŸš€" src="https://preview.redd.it/nv34uyjqtofe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9be62292f2e57c4882e28f231b0a82d1126bdabd" title="Ollama enjoying the Chinese New Year! Open Source FTW ðŸš€" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eternviking"&gt; /u/eternviking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nv34uyjqtofe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibwdvx/ollama_enjoying_the_chinese_new_year_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibwdvx/ollama_enjoying_the_chinese_new_year_open_source/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T07:34:16+00:00</published>
  </entry>
</feed>
