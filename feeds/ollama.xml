<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-06-28T21:35:21+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1lm0ib3</id>
    <title>Master LLMs in 5 minutes</title>
    <updated>2025-06-27T17:56:35+00:00</updated>
    <author>
      <name>/u/Substantial_Gain5838</name>
      <uri>https://old.reddit.com/user/Substantial_Gain5838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lm0ib3/master_llms_in_5_minutes/"&gt; &lt;img alt="Master LLMs in 5 minutes" src="https://external-preview.redd.it/YaJAyCn3RfH18pMB3LttrmTsD19Awx1vOaSTWgJAF18.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e1d490c64b2bcd2076cc47f9998dbac528762b1" title="Master LLMs in 5 minutes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please Like share and subscribe &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Gain5838"&gt; /u/Substantial_Gain5838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/khPcUhfEmxQ?si=LSlmHK4hcGua8gHl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lm0ib3/master_llms_in_5_minutes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lm0ib3/master_llms_in_5_minutes/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T17:56:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll4us5</id>
    <title>Beautify Ollama</title>
    <updated>2025-06-26T16:38:25+00:00</updated>
    <author>
      <name>/u/falkon2112</name>
      <uri>https://old.reddit.com/user/falkon2112</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1ll4us5/video/5zt9ljutua9f1/player"&gt;https://reddit.com/link/1ll4us5/video/5zt9ljutua9f1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I got tired of the basic Ollama interfaces out there and decided to build something that looks like it belongs in 2025. Meet &lt;strong&gt;BeautifyOllama&lt;/strong&gt; - a modern web interface that makes chatting with your local AI models actually enjoyable.&lt;/p&gt; &lt;h1&gt;What it does:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Animated shine borders&lt;/strong&gt; that cycle through colors (because why not make AI conversations pretty?)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time streaming&lt;/strong&gt; responses that feel snappy&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dark/light themes&lt;/strong&gt; that follow your system preferences&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mobile-responsive&lt;/strong&gt; so you can chat with AI on the toilet (we've all been there)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Glassmorphism effects&lt;/strong&gt; and smooth animations everywhere&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Tech stack (for the nerds):&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Next.js 15 + React 19 (bleeding edge stuff)&lt;/li&gt; &lt;li&gt;TypeScript (because I like my code to not break)&lt;/li&gt; &lt;li&gt;TailwindCSS 4 (utility classes go brrr)&lt;/li&gt; &lt;li&gt;Framer Motion (for those buttery smooth animations)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Demo &amp;amp; Code:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Live demo:&lt;/strong&gt; &lt;a href="https://beautifyollama.vercel.app/"&gt;https://beautifyollama.vercel.app/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/falkon2/BeautifyOllama"&gt;https://github.com/falkon2/BeautifyOllama&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What's coming next:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;File uploads (drag &amp;amp; drop your docs)&lt;/li&gt; &lt;li&gt;Conversation history that doesn't disappear&lt;/li&gt; &lt;li&gt;Plugin system for extending functionality&lt;/li&gt; &lt;li&gt;Maybe a mobile app if people actually use this thing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup is stupid simple:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Have Ollama running (&lt;code&gt;ollama serve&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Clone the repo&lt;/li&gt; &lt;li&gt;&lt;code&gt;npm install &amp;amp;&amp;amp; npm run dev&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Profit&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I would appreciate any and all feedback as well as criticism.&lt;/p&gt; &lt;p&gt;The project is early-stage but functional. I'm actively working on it and would love feedback, contributions, or just general roasting of my code.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question for the community:&lt;/strong&gt; What features would you actually want in a local AI interface? I'm building this for real use,.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/falkon2112"&gt; /u/falkon2112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ll4us5/beautify_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ll4us5/beautify_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ll4us5/beautify_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T16:38:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1llxeye</id>
    <title>[DEV] AgentTip ‚Äì trigger your OpenAI assistants or Ollama models from any macOS app (one-time $4.99)</title>
    <updated>2025-06-27T15:54:02+00:00</updated>
    <author>
      <name>/u/Brazilgs</name>
      <uri>https://old.reddit.com/user/Brazilgs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1llxeye/dev_agenttip_trigger_your_openai_assistants_or/"&gt; &lt;img alt="[DEV] AgentTip ‚Äì trigger your OpenAI assistants or Ollama models from any macOS app (one-time $4.99)" src="https://external-preview.redd.it/ejlhMXEwM3ZyaDlmMYYoLEsPwZFbst0w5wBXHUH9dFkt8rq8VyC4VGq0TReR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05e1dc68750abef1492e17db06338a1adc89de4d" title="[DEV] AgentTip ‚Äì trigger your OpenAI assistants or Ollama models from any macOS app (one-time $4.99)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks üëã I‚Äôm the dev behind AgentTip.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.agenttip.xyz/"&gt;https://www.agenttip.xyz/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Problem: jumping to a browser or separate window every time you want LLM kills flow.&lt;/p&gt; &lt;p&gt;Fix: type @idea brainstorm an onboarding flow, hit ‚èé, and AgentTip swaps the trigger for the assistant‚Äôs reply‚Äîright where you were typing. No context-switch, no copy-paste.&lt;/p&gt; &lt;p&gt;‚Ä¢ Instant trigger recognition ‚Äì define @writer, @code, anything you like.&lt;/p&gt; &lt;p&gt;‚Ä¢ Works system-wide ‚Äì TextEdit ‚Üí VS Code ‚Üí Safari, you name it.&lt;/p&gt; &lt;p&gt;‚Ä¢ Unlimited assistants ‚Äì connect every OpenAI Assistant or Ollama model you‚Äôve avaiable.&lt;/p&gt; &lt;p&gt;‚Ä¢ Unlimited use ‚Äì connect every Ollama model you‚Äôve in your local machine. - TOTAL privacy, using Ollama, your data never goes online. &lt;/p&gt; &lt;p&gt;‚Ä¢ Your own API key, stored in macOS Keychain ‚Äì pay OpenAI directly; we never see your data.&lt;/p&gt; &lt;p&gt;‚Ä¢ One-time purchase, $4.99 lifetime licence ‚Äì no subscriptions.&lt;/p&gt; &lt;p&gt;Mac App Store: &lt;a href="https://apps.apple.com/app/agenttip/id6747261813?utm_source=reddit&amp;amp;utm_campaign=macapps_launch"&gt;https://apps.apple.com/app/agenttip/id6747261813?utm_source=reddit&amp;amp;utm_campaign=macapps_launch&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brazilgs"&gt; /u/Brazilgs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3qqih57vrh9f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llxeye/dev_agenttip_trigger_your_openai_assistants_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1llxeye/dev_agenttip_trigger_your_openai_assistants_or/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T15:54:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lly7a4</id>
    <title>Best models a macbook can support</title>
    <updated>2025-06-27T16:25:13+00:00</updated>
    <author>
      <name>/u/lrshaid</name>
      <uri>https://old.reddit.com/user/lrshaid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! &lt;/p&gt; &lt;p&gt;I'm doing my first baby steps in runnning LLMs locally. I have a M4 16gb macbook air. Based on your experience, what do you recommend to run? I mean, probably you can run a lot of stuff but with big waiting times. Nothing in particular, just want to read your experiences!&lt;/p&gt; &lt;p&gt;Thanks in advance :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lrshaid"&gt; /u/lrshaid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lly7a4/best_models_a_macbook_can_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lly7a4/best_models_a_macbook_can_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lly7a4/best_models_a_macbook_can_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T16:25:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1llyj6o</id>
    <title>Issues with Tools via OW UI hitting Ollama via Tools/Filters</title>
    <updated>2025-06-27T16:38:07+00:00</updated>
    <author>
      <name>/u/matty990</name>
      <uri>https://old.reddit.com/user/matty990</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1llyj6o/issues_with_tools_via_ow_ui_hitting_ollama_via/"&gt; &lt;img alt="Issues with Tools via OW UI hitting Ollama via Tools/Filters" src="https://preview.redd.it/jhfkd85bzh9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c6e756d8c50683bf9aae2bae6a78a2006813640d" title="Issues with Tools via OW UI hitting Ollama via Tools/Filters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When using Open Web I have no issues with them speaking. It appears when trying to use a Memory Tool to connect it throws up 405s. &lt;/p&gt; &lt;p&gt;The network is all good as they are on the same docker stack. &lt;/p&gt; &lt;p&gt;Any advice would be amazing as this is the last step for me to get this fully setup. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matty990"&gt; /u/matty990 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jhfkd85bzh9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llyj6o/issues_with_tools_via_ow_ui_hitting_ollama_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1llyj6o/issues_with_tools_via_ow_ui_hitting_ollama_via/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T16:38:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1llzypj</id>
    <title>Am I realistic? Academic summarising question</title>
    <updated>2025-06-27T17:34:57+00:00</updated>
    <author>
      <name>/u/strangerweather</name>
      <uri>https://old.reddit.com/user/strangerweather</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking for a language model that can accurately summarise philosophy and literature academic articles. I have just done it using Claude on the web so I know it is possible for AI to do a good job with complex arguments. The reason I would like to do it locally is that some of these articles are my own work and I am concerned about privacy. I have an M4 MacBookPro with 24GB Unified Memory and I have tried granite 3.3 and llama 3.2, and several other models that I have since deleted. They all come up with complete nonsense. Is it realistic to want a good quality summary on 24GB? If so, which model should I use? If not, I'll forget about the idea lol. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/strangerweather"&gt; /u/strangerweather &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llzypj/am_i_realistic_academic_summarising_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llzypj/am_i_realistic_academic_summarising_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1llzypj/am_i_realistic_academic_summarising_question/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T17:34:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1llv752</id>
    <title>Anyone else experiencing extreme slowness with Gemma 3n on Ollama?</title>
    <updated>2025-06-27T14:24:15+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded Genma3n FP16 off of Ollama‚Äôs official repository and I‚Äôm running it on an H100 and it‚Äôs running at like hot garbage (like 2 tokens/s). I‚Äôve tried it on both 0.9.3 and pre-release of 0.9.4. Anymore else encountered this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llv752/anyone_else_experiencing_extreme_slowness_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llv752/anyone_else_experiencing_extreme_slowness_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1llv752/anyone_else_experiencing_extreme_slowness_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T14:24:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1llst8n</id>
    <title>How do I force Ollama to exclusively use GPU</title>
    <updated>2025-06-27T12:37:50+00:00</updated>
    <author>
      <name>/u/RadiantPermission513</name>
      <uri>https://old.reddit.com/user/RadiantPermission513</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay so I have a bit of an interesting situation. The computer I have running my Ollama LLMs is kind of a potato, it's running an older Ryzen CPU I don't remember the model off the top of my head and 32gb DDR3 RAM. It was my old Proxmox server I have since upgraded. However I upgraded my GPU in my gaming rig a while back and have an Nvidia 3050 that wasn't being used. So I put the 3050 in the rig and decided to make a dedicated LLM server running Open Web UI on it as well. Yes I recognize I put a sports car engine in a potato. However the issue I am having is Ollama can decide to use the sports car engine which runs 8b models like a champ or the potato which locks up with 3b models. I regularly have to restart it and flip a coin which it'll use, if it decides to us the GPU it'll run great for a few days then decide to give Llama3.1 8b a good college try on the CPU and lock out once the CPU starts running at 450%. Is there a way to convince Ollama to only use GPU and forget about the CPU? It won't even try to offload, it's 100% one or the other.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RadiantPermission513"&gt; /u/RadiantPermission513 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llst8n/how_do_i_force_ollama_to_exclusively_use_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llst8n/how_do_i_force_ollama_to_exclusively_use_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1llst8n/how_do_i_force_ollama_to_exclusively_use_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T12:37:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1llt7gr</id>
    <title>gemma3n not working with pictures</title>
    <updated>2025-06-27T12:56:58+00:00</updated>
    <author>
      <name>/u/Fun_Librarian_7699</name>
      <uri>https://old.reddit.com/user/Fun_Librarian_7699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tested gemma3n and it's really fast, but I looks like ollama doesn't support images (yet). According to their &lt;a href="https://ollama.com/library/gemma3n"&gt;webseite&lt;/a&gt;, gemma3n should support images and also audio. I've never used a model that supports audio with ollama before, looking forward to trying it when it's working. By the way, I updated ollama today and am now using version 0.9.3.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;(base) PS C:\Users\andre&amp;gt; ollama run gemma3:12b-it-q4_K_M &amp;gt;&amp;gt;&amp;gt; Describe the picture in one sentence &amp;quot;C:\Users\andre\Desktop\picture.jpg&amp;quot; Added image 'C:\Users\andre\Desktop\picture.jpg' A fluffy, orange and white cat is sprawled out and relaxing on a colorful patterned blanket with its paws extended. &amp;gt;&amp;gt;&amp;gt; (base) PS C:\Users\andre&amp;gt; ollama run gemma3n:e4b-it-q8_0 &amp;gt;&amp;gt;&amp;gt; Describe the picture in one sentence &amp;quot;C:\Users\andre\Desktop\picture.jpg&amp;quot; I am unable to access local files or URLs, so I cannot describe the picture at the given file path. Therefore, I can't fulfill your request. To get a description, you would need to: 1. **Describe the picture to me:** Tell me what you see in the image. 2. **Use an image recognition service:** Upload the image to a service like Google Lens, Amazon Rekognition, or Clarifai, which can analyze the image and provide a description. &amp;gt;&amp;gt;&amp;gt; (base) PS C:\Users\andre&amp;gt; ollama -v ollama version is 0.9.3 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun_Librarian_7699"&gt; /u/Fun_Librarian_7699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llt7gr/gemma3n_not_working_with_pictures/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1llt7gr/gemma3n_not_working_with_pictures/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1llt7gr/gemma3n_not_working_with_pictures/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T12:56:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm6ex3</id>
    <title>Looking for LLM</title>
    <updated>2025-06-27T22:01:23+00:00</updated>
    <author>
      <name>/u/Devve2kcccc</name>
      <uri>https://old.reddit.com/user/Devve2kcccc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;br /&gt; I'm looking for a simple, small-to-medium-sized language model that I can integrate as an agent into my SaaS platform. The goal is to automate repetitive tasks within an ERP system‚Äîranging from basic operations to more complex analyses.&lt;/p&gt; &lt;p&gt;Ideally, the model should be able to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Read and interpret documents (such as invoices);&lt;/li&gt; &lt;li&gt;Detect inconsistencies or irregularities (e.g., mismatched values);&lt;/li&gt; &lt;li&gt;Perform calculations and accurately understand numerical data;&lt;/li&gt; &lt;li&gt;Provide high precision in its analysis.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I would prefer a model that can run comfortably &lt;strong&gt;locally during the development phase&lt;/strong&gt;, and possibly be used later via services like &lt;strong&gt;OpenRouter&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It should be resource-efficient and reliable enough to be used in a production environment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Devve2kcccc"&gt; /u/Devve2kcccc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lm6ex3/looking_for_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lm6ex3/looking_for_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lm6ex3/looking_for_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T22:01:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll5h5p</id>
    <title>gemma3n is out</title>
    <updated>2025-06-26T17:02:40+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma 3n models are designed for efficient execution on everyday devices such as laptops, tablets or phones.&lt;/p&gt; &lt;p&gt;Gemma 3n models are designed for efficient execution on everyday devices such as laptops, tablets or phones. These models were trained with data in over 140 spoken languages.&lt;/p&gt; &lt;p&gt;Gemma 3n models use selective parameter activation technology to reduce resource requirements. This technique allows the models to operate at an effective size of 2B and 4B parameters, which is lower than the total number of parameters they contain.&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/gemma3n"&gt;https://ollama.com/library/gemma3n&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Upd: ollama 0.9.3 required&lt;/p&gt; &lt;p&gt;Upd2: official post &lt;a href="https://www.reddit.com/r/LocalLLaMA/s/0nLcE3wzA1"&gt;https://www.reddit.com/r/LocalLLaMA/s/0nLcE3wzA1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ll5h5p/gemma3n_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ll5h5p/gemma3n_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ll5h5p/gemma3n_is_out/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-26T17:02:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm1ojg</id>
    <title>Runs slowly migrate to CPU</title>
    <updated>2025-06-27T18:43:55+00:00</updated>
    <author>
      <name>/u/mlt-</name>
      <uri>https://old.reddit.com/user/mlt-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lm1ojg/runs_slowly_migrate_to_cpu/"&gt; &lt;img alt="Runs slowly migrate to CPU" src="https://external-preview.redd.it/9lo4tg3zx-vJGfFjJ34hPXpbnLBo7OTGBWhTVhrEA1Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d39c54b54ea752bbab2f5e7bb32aca7f8be2f9af" title="Runs slowly migrate to CPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlt-"&gt; /u/mlt- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lm1ojg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lm1ojg/runs_slowly_migrate_to_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lm1ojg/runs_slowly_migrate_to_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T18:43:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmd42b</id>
    <title>Questions from a noob</title>
    <updated>2025-06-28T03:35:08+00:00</updated>
    <author>
      <name>/u/-how-about-69-</name>
      <uri>https://old.reddit.com/user/-how-about-69-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I am totally new to this and really just wanted to experiment to see the difference between models &amp;amp; model sizes and how that impacts the quality of the response. I downloaded the 671b deepseek r-1 model and got hit with the not enough system memory. Which leads me to a few questions.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Is there a way to run a larger model off a hard drive instead of ram to bypass the not enough system memory issue? Would manually changing the paging file size on an external SSD to something like 1tb bypass this issue? My research showed me this isn't how Ollama works but figured id ask given speed isn't a parameter I currently value as I am just brainstorming uses at this time. Im only looking for the absolute highest quality answers from the various models.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If the answer to number 1 is no then what kind of models can I run with my pc? I have a 7800x3d w/ 64gb ram &amp;amp; a 1080ti 11gb. Is there a chart that breaks down how much ram each model would need?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I have a M2 MacBook Air with 8gb of ram. Since I know macOS uses swap does that theoretically mean I could bypass this error on my MacBook?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks in advance for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-how-about-69-"&gt; /u/-how-about-69- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lmd42b/questions_from_a_noob/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lmd42b/questions_from_a_noob/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lmd42b/questions_from_a_noob/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-28T03:35:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lly9ii</id>
    <title>Recommend me the best model for coding</title>
    <updated>2025-06-27T16:27:34+00:00</updated>
    <author>
      <name>/u/mo7akh</name>
      <uri>https://old.reddit.com/user/mo7akh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running a beefy GTX 1650 4gb and a whopping 16gb of ram. Recommend me the best coding model for this hardware, and thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mo7akh"&gt; /u/mo7akh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lly9ii/recommend_me_the_best_model_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lly9ii/recommend_me_the_best_model_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lly9ii/recommend_me_the_best_model_for_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T16:27:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmhd70</id>
    <title>On-Premise AI Assistant for Customer Care (Ollama, Hardware, Alternatives)</title>
    <updated>2025-06-28T07:56:42+00:00</updated>
    <author>
      <name>/u/Worth_Rabbit_6262</name>
      <uri>https://old.reddit.com/user/Worth_Rabbit_6262</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;A while ago I posted here asking for advice on building an on-premise AI assistant for our Customer Care team in a medium-to-large telecommunications company. I received some very interesting replies ‚Äî but they went in many different directions, so I‚Äôd like to clarify the use case and ask for more focused feedback.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;üéØ The Real Goal&lt;/p&gt; &lt;p&gt;We want to assist human operators in opening ‚ÄúAssurance‚Äù tickets (for service disruptions or degradations) with complete and accurate information.&lt;/p&gt; &lt;p&gt;Here‚Äôs how the initial workflow is designed:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;The operator is on a call with the customer and writes a brief summary of the issue into our internal CRM.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Before hanging up, they press a button in the system.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The written text is sent to the AI ‚Äî along with network diagnostics and device status, pulled in real time via our internal monitoring APIs.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The AI checks whether all the key info is present to correctly open a support ticket.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If anything is missing, it returns specific questions or actions the operator should ask or perform before ending the call.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;p&gt;üß† Example Outputs&lt;/p&gt; &lt;p&gt;FTTH down ‚Üí Ask to check ONT status&lt;/p&gt; &lt;p&gt;Radio bridge unreachable ‚Üí Restart router + IDU&lt;/p&gt; &lt;p&gt;No browsing, LAN port down ‚Üí Ask to check Ethernet cable&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;‚ö†Ô∏è Important Scope Note&lt;/p&gt; &lt;p&gt;At this stage:&lt;/p&gt; &lt;p&gt;No audio transcription&lt;/p&gt; &lt;p&gt;No chatbot interaction&lt;/p&gt; &lt;p&gt;No full conversation processing&lt;/p&gt; &lt;p&gt;We're simply analyzing short operator-written text, combined with real-time network and device status data from internal APIs. The aim is to help operators avoid missing information and improve ticket quality ‚Äî without slowing down the workflow.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;‚ùì What I‚Äôd Love Your Input On&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Do we actually need an LLM for this use case? Or could a simpler approach (e.g., classification, rules, smaller model) be more appropriate?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If an LLM makes sense:&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Which model would you recommend for on-premise use with this kind of input?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Is Ollama a viable solution for low-latency, production-grade inference?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What kind of hardware would realistically be needed for this workload?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We need low latency&lt;/p&gt; &lt;p&gt;And support for concurrent usage (many operators may trigger the system at the same time)&lt;/p&gt; &lt;p&gt;We want to keep everything on-prem for privacy and security reasons&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;üôè Final Note&lt;/p&gt; &lt;p&gt;Thanks again to everyone who replied to my original post ‚Äî your thoughts helped us move forward, even if I walked away with more questions than answers üòÖ I hope this version better explains the scope and helps spark more targeted insights.&lt;/p&gt; &lt;p&gt;Happy to clarify any details or discuss further if needed. Thanks again!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worth_Rabbit_6262"&gt; /u/Worth_Rabbit_6262 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lmhd70/onpremise_ai_assistant_for_customer_care_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lmhd70/onpremise_ai_assistant_for_customer_care_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lmhd70/onpremise_ai_assistant_for_customer_care_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-28T07:56:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lls5os</id>
    <title>Arch-Router 1.5B - The world's fast and first LLM router that can align to your usage preferences.</title>
    <updated>2025-06-27T12:04:55+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lls5os/archrouter_15b_the_worlds_fast_and_first_llm/"&gt; &lt;img alt="Arch-Router 1.5B - The world's fast and first LLM router that can align to your usage preferences." src="https://preview.redd.it/7u00gzrxkg9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c39772460819d003fdebb4e72fe5e3ae7b1b9377" title="Arch-Router 1.5B - The world's fast and first LLM router that can align to your usage preferences." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited to share Arch-Router, our research and model for LLM routing. Routing to the right LLM is still an elusive problem, riddled with nuance and blindspots. For example:&lt;/p&gt; &lt;p&gt;‚ÄúEmbedding-based‚Äù (or simple intent-classifier) routers sound good on paper‚Äîlabel each prompt via embeddings as ‚Äúsupport,‚Äù ‚ÄúSQL,‚Äù ‚Äúmath,‚Äù then hand it to the matching model‚Äîbut real chats don‚Äôt stay in their lanes. Users bounce between topics, task boundaries blur, and any new feature means retraining the classifier. The result is brittle routing that can‚Äôt keep up with multi-turn conversations or fast-moving product scopes.&lt;/p&gt; &lt;p&gt;Performance-based routers swing the other way, picking models by benchmark or cost curves. They rack up points on MMLU or MT-Bench yet miss the human tests that matter in production: ‚ÄúWill Legal accept this clause?‚Äù ‚ÄúDoes our support tone still feel right?‚Äù Because these decisions are subjective and domain-specific, benchmark-driven black-box routers often send the wrong model when it counts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Arch-Router skips both pitfalls by routing on&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;preferences you write in plain language&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt; Drop rules like ‚Äúcontract clauses ‚Üí GPT-4o‚Äù or ‚Äúquick travel tips ‚Üí Gemini-Flash,‚Äù and our 1.5B auto-regressive router model maps prompt along with the context to your routing policies‚Äîno retraining, no sprawling rules that are encoded in if/else statements. Co-designed with Twilio and Atlassian, it adapts to intent drift, lets you swap in new models with a one-liner, and keeps routing logic in sync with the way you actually judge quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Specs&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tiny footprint&lt;/strong&gt; ‚Äì 1.5 B params ‚Üí runs on one modern GPU (or CPU while you play).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Plug-n-play&lt;/strong&gt; ‚Äì points at any mix of LLM endpoints; adding models needs &lt;em&gt;zero&lt;/em&gt; retraining.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SOTA query-to-policy matching&lt;/strong&gt; ‚Äì beats bigger closed models on conversational datasets.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost / latency smart&lt;/strong&gt; ‚Äì push heavy stuff to premium models, everyday queries to the fast ones.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Exclusively available in Arch (the AI-native proxy for agents): &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;br /&gt; üîó Model + code: &lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;https://huggingface.co/katanemo/Arch-Router-1.5B&lt;/a&gt;&lt;br /&gt; üìÑ Paper / longer read: &lt;a href="https://arxiv.org/abs/2506.16655"&gt;https://arxiv.org/abs/2506.16655&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7u00gzrxkg9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lls5os/archrouter_15b_the_worlds_fast_and_first_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lls5os/archrouter_15b_the_worlds_fast_and_first_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T12:04:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmjbxj</id>
    <title>Can I combine 2x M4 Pro MacBooks for LLMs? And what Ollama models can I run?</title>
    <updated>2025-06-28T10:10:20+00:00</updated>
    <author>
      <name>/u/esraaatmeh</name>
      <uri>https://old.reddit.com/user/esraaatmeh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm thinking of buying &lt;strong&gt;two MacBooks&lt;/strong&gt;, each with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Apple M4 Pro chip (12-core CPU, 16-core GPU)&lt;/li&gt; &lt;li&gt;24GB unified memory&lt;/li&gt; &lt;li&gt;512GB SSD&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I have two questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Can I combine both devices&lt;/strong&gt; to run larger LLMs ‚Äî similar to multi-GPU setups on PCs? Or is this not possible with Apple Silicon?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What size Ollama models&lt;/strong&gt; (e.g., LLaMA 7B/13B, Mistral, Gemma, Phi) can I realistically run &lt;strong&gt;professionally&lt;/strong&gt; on this setup?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/esraaatmeh"&gt; /u/esraaatmeh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lmjbxj/can_i_combine_2x_m4_pro_macbooks_for_llms_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lmjbxj/can_i_combine_2x_m4_pro_macbooks_for_llms_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lmjbxj/can_i_combine_2x_m4_pro_macbooks_for_llms_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-28T10:10:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmjc7e</id>
    <title>The Impact Of Cybercrime On Digital Innovation And Cybersecurity.</title>
    <updated>2025-06-28T10:10:52+00:00</updated>
    <author>
      <name>/u/Curious_Candy851</name>
      <uri>https://old.reddit.com/user/Curious_Candy851</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lmjc7e/the_impact_of_cybercrime_on_digital_innovation/"&gt; &lt;img alt="The Impact Of Cybercrime On Digital Innovation And Cybersecurity." src="https://external-preview.redd.it/Zirp1RFJY6JRbsRyUoBLCtlSM427DlBLx641L4bW1U0.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb7e079f829ec4a68d963fad8867417ba9d46b42" title="The Impact Of Cybercrime On Digital Innovation And Cybersecurity." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is video, presented by Frederick Wakulyaka, discusses the significant impact of cybercrime on e-commerce and digital innovation [&lt;a href="http://www.youtube.com/watch?v=6JLByP-UTLA&amp;amp;t=38"&gt;00:38&lt;/a&gt;]. It defines cybercrime as illegal activities in the digital realm, including identity theft, online fraud, and hacking, and emphasizes the importance of addressing it [&lt;a href="http://www.youtube.com/watch?v=6JLByP-UTLA&amp;amp;t=121"&gt;02:01&lt;/a&gt;].&lt;/p&gt; &lt;p&gt;The video highlights how cybercrime increases risks in e-commerce by compromising transaction security and stifles digital innovation as businesses prioritize damage control [&lt;a href="http://www.youtube.com/watch?v=6JLByP-UTLA&amp;amp;t=216"&gt;03:36&lt;/a&gt;]. It also covers online purchase vulnerabilities, customer and business risks, and cites the 2019 Hot Topic data breach as an example [&lt;a href="http://www.youtube.com/watch?v=6JLByP-UTLA&amp;amp;t=251"&gt;04:11&lt;/a&gt;].&lt;/p&gt; &lt;p&gt;Furthermore, the video explains how advancements in technology create new vulnerabilities, with cybercriminals exploiting emerging technologies like AI, blockchain, and IoT [&lt;a href="http://www.youtube.com/watch?v=6JLByP-UTLA&amp;amp;t=400"&gt;06:40&lt;/a&gt;]. It stresses the importance of strategic investment in cybersecurity as a fundamental business component [&lt;a href="http://www.youtube.com/watch?v=6JLByP-UTLA&amp;amp;t=512"&gt;08:32&lt;/a&gt;] and notes that small and medium businesses (SMBs) are particularly susceptible to cyberattacks [&lt;a href="http://www.youtube.com/watch?v=6JLByP-UTLA&amp;amp;t=583"&gt;09:43&lt;/a&gt;]. The video concludes by emphasizing the need for continuous investment, proactive steps, and collaboration for a secure digital future [&lt;a href="http://www.youtube.com/watch?v=6JLByP-UTLA&amp;amp;t=636"&gt;10:36&lt;/a&gt;]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Curious_Candy851"&gt; /u/Curious_Candy851 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=6JLByP-UTLA&amp;amp;si=GQOplXDYRRxVrC1Z"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lmjc7e/the_impact_of_cybercrime_on_digital_innovation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lmjc7e/the_impact_of_cybercrime_on_digital_innovation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-28T10:10:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm9b32</id>
    <title>Best model a RTX 5070ti can handle well?</title>
    <updated>2025-06-28T00:13:06+00:00</updated>
    <author>
      <name>/u/pducharme</name>
      <uri>https://old.reddit.com/user/pducharme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for the holy grail of model that will max out my RTX 5070ti and maximize the GPU. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pducharme"&gt; /u/pducharme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lm9b32/best_model_a_rtx_5070ti_can_handle_well/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lm9b32/best_model_a_rtx_5070ti_can_handle_well/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lm9b32/best_model_a_rtx_5070ti_can_handle_well/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-28T00:13:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm7iff</id>
    <title>Model for 12GB VRAM</title>
    <updated>2025-06-27T22:49:15+00:00</updated>
    <author>
      <name>/u/moric7</name>
      <uri>https://old.reddit.com/user/moric7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now I use free online ChatGPT. It is amazing, awesome, incredible fantastic!!! It is the best feeling friend, the most excellent teacher in all sciences, professional engineer for everything... I tried ollama and JanAI, dousens of models, absolutely not useful. I downloaded up to 10-11 GB models to can run on my PC (see the title). But all of them cannot carry any general conversation, knowns absolutely nothing about any science, even the tries to write code is ridiculous. Usually they write nonsense or start dead loop. I understand that AI is not for my tiny PC (I'm extremely poor in very poor place), but why there are even 2GB models with message &amp;quot;excellent results&amp;quot;!? Wtf!? If i do something wrong, please learn me!!! I'm only general user of online AI, is it possible to have something useful on my PC without Internet!? Is there really useful model up to 12 GB? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moric7"&gt; /u/moric7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lm7iff/model_for_12gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lm7iff/model_for_12gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lm7iff/model_for_12gb_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T22:49:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmorur</id>
    <title>Best models for tools with desktop apps like Goose and 5ire</title>
    <updated>2025-06-28T14:56:37+00:00</updated>
    <author>
      <name>/u/kaosmetal</name>
      <uri>https://old.reddit.com/user/kaosmetal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been trying to find out which model to use for tools with desktop clients like Goose and 5ire. I am running it on Macbook Air M1 .. So far I tried Llama3.2:latest, Qwen3:1.7b, Deepseek r1, phi4-mini:3.8b but haven't got any good results. When I switch to using Claude 3.7, it works like a charm. I am trying to use it with Playwright MCP for browser actions.&lt;/p&gt; &lt;p&gt;Has anyone got any success with these desktop apps and which models did you use? Problem with Claude Desktop it runs out of token and asks to open new chat pretty quickly. Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaosmetal"&gt; /u/kaosmetal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lmorur/best_models_for_tools_with_desktop_apps_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lmorur/best_models_for_tools_with_desktop_apps_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lmorur/best_models_for_tools_with_desktop_apps_like/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-28T14:56:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lma0hk</id>
    <title>I built the first open source Ollama MCP client (sneak peak)</title>
    <updated>2025-06-28T00:48:18+00:00</updated>
    <author>
      <name>/u/matt8p</name>
      <uri>https://old.reddit.com/user/matt8p</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lma0hk/i_built_the_first_open_source_ollama_mcp_client/"&gt; &lt;img alt="I built the first open source Ollama MCP client (sneak peak)" src="https://external-preview.redd.it/MWE0czlnczNmazlmMePD01GQiA2-17o3pDd3lEuJj5F9rySNpUtXl4qBF3I9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5d42f8af2cd2fa7e86289caa4452a5a1f2fb140e" title="I built the first open source Ollama MCP client (sneak peak)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm building MCPJam, Postman for MCP. It‚Äôs an open source tool to help test and debug your MCP server.&lt;/p&gt; &lt;p&gt;We are close to launching support for Ollama in our LLM playground. Now you can test your MCP server against an LLM, and choose between Anthropic, OpenAI, and now local Ollama servers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Release timeline&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The changes are already in the repo, but I‚Äôm doing an official launch and push to npm on Monday. Will be polishing up this feature over the weekend.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Support the project!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you find this project useful, please consider giving the repo a star.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/MCPJam/inspector"&gt;https://github.com/MCPJam/inspector&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The MCPJam dev community is also very active on Discord, please join&lt;/p&gt; &lt;p&gt;&lt;a href="https://discord.com/invite/Gpv7AmrRc4"&gt;https://discord.com/invite/Gpv7AmrRc4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matt8p"&gt; /u/matt8p &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gyduxes3fk9f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lma0hk/i_built_the_first_open_source_ollama_mcp_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lma0hk/i_built_the_first_open_source_ollama_mcp_client/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-28T00:48:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lms1zz</id>
    <title>Uploading files to open web ui</title>
    <updated>2025-06-28T17:14:42+00:00</updated>
    <author>
      <name>/u/pangmaster0</name>
      <uri>https://old.reddit.com/user/pangmaster0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have open web UI running in docker and a local installation of ollama. Basic installation into Mac following the setup guides. &lt;/p&gt; &lt;p&gt;It seems to have trouble reading uploaded files to process. I have many been uploading c# code files and 8/10 times it just fails and says I‚Äôm read please upload the files to be scanned. &lt;/p&gt; &lt;p&gt;Is there some setup I‚Äôm missing ?&lt;/p&gt; &lt;p&gt;I‚Äôve been using qwen2.5 coder 7B ,14b models&lt;/p&gt; &lt;p&gt;File size are like 20KB to 500KB&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pangmaster0"&gt; /u/pangmaster0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lms1zz/uploading_files_to_open_web_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lms1zz/uploading_files_to_open_web_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lms1zz/uploading_files_to_open_web_ui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-28T17:14:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmscz4</id>
    <title>My last post‚Ä¶</title>
    <updated>2025-06-28T17:27:24+00:00</updated>
    <author>
      <name>/u/anttiOne</name>
      <uri>https://old.reddit.com/user/anttiOne</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;‚Ä¶for a while. It‚Äòs part 3/3 of the Privacy AI article series.&lt;/p&gt; &lt;p&gt;The setup is in PROD for a whole month now and except some slight tweaking and testing, I won‚Äôt be adding to it for the time being!&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@vs3kulic/building-ai-for-privacy-pre-cook-your-recommendations-1ade6d47b852"&gt;https://medium.com/@vs3kulic/building-ai-for-privacy-pre-cook-your-recommendations-1ade6d47b852&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anttiOne"&gt; /u/anttiOne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lmscz4/my_last_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lmscz4/my_last_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lmscz4/my_last_post/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-28T17:27:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmp9er</id>
    <title>Best models tuned for coding</title>
    <updated>2025-06-28T15:17:01+00:00</updated>
    <author>
      <name>/u/Fragrant-Review-5055</name>
      <uri>https://old.reddit.com/user/Fragrant-Review-5055</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which are the best models that have been tuned for programming. &lt;/p&gt; &lt;p&gt;For GPUs with 12gb, 16gb and 24gb vram?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fragrant-Review-5055"&gt; /u/Fragrant-Review-5055 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lmp9er/best_models_tuned_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lmp9er/best_models_tuned_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lmp9er/best_models_tuned_for_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-28T15:17:01+00:00</published>
  </entry>
</feed>
