<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-07-26T14:38:03+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1m70f7q</id>
    <title>**üîì I built Hearth-UI ‚Äî A fully-featured desktop app for chatting with local LLMs (Ollama-ready, attachments, themes, markdown, and more)**</title>
    <updated>2025-07-23T04:59:48+00:00</updated>
    <author>
      <name>/u/Vast-Helicopter-3719</name>
      <uri>https://old.reddit.com/user/Vast-Helicopter-3719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m70f7q/i_built_hearthui_a_fullyfeatured_desktop_app_for/"&gt; &lt;img alt="**üîì I built Hearth-UI ‚Äî A fully-featured desktop app for chatting with local LLMs (Ollama-ready, attachments, themes, markdown, and more)**" src="https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417" title="**üîì I built Hearth-UI ‚Äî A fully-featured desktop app for chatting with local LLMs (Ollama-ready, attachments, themes, markdown, and more)**" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! üëã&lt;/p&gt; &lt;p&gt;I recently put together a desktop AI chat interface called &lt;strong&gt;Hearth-UI&lt;/strong&gt;, made for anyone using &lt;a href="https://ollama.com/"&gt;Ollama&lt;/a&gt; for local LLMs like LLaMA3, Mistral, Gemma, etc.&lt;/p&gt; &lt;p&gt;It includes everything I wish existed in a typical Ollama UI ‚Äî and it‚Äôs fully offline, customizable, and open-source.&lt;/p&gt; &lt;p&gt;üß† Features:&lt;/p&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Multi-session chat history&lt;/strong&gt; (rename, delete, auto-save)&lt;br /&gt; ‚úÖ &lt;strong&gt;Markdown + syntax highlighting&lt;/strong&gt; (like ChatGPT)&lt;br /&gt; ‚úÖ &lt;strong&gt;Streaming responses&lt;/strong&gt; + prompt &lt;strong&gt;queueing while streaming&lt;/strong&gt;&lt;br /&gt; ‚úÖ &lt;strong&gt;File uploads&lt;/strong&gt; &amp;amp; &lt;strong&gt;drag-and-drop attachments&lt;/strong&gt;&lt;br /&gt; ‚úÖ Beautiful &lt;strong&gt;theme picker&lt;/strong&gt; (Dark/Light/Blue/Green/etc)&lt;br /&gt; ‚úÖ &lt;strong&gt;Cancel response mid-generation&lt;/strong&gt; (Stop button)&lt;br /&gt; ‚úÖ Export chat to &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.json&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;&lt;br /&gt; ‚úÖ &lt;strong&gt;Electron-powered desktop app for Windows&lt;/strong&gt; (macOS/Linux coming)&lt;br /&gt; ‚úÖ Works with your existing &lt;code&gt;ollama serve&lt;/code&gt; ‚Äî no cloud, no signup&lt;/p&gt; &lt;h1&gt;üîß Tech stack:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Ollama (as LLM backend)&lt;/li&gt; &lt;li&gt;HTML/CSS/JS (Vanilla frontend)&lt;/li&gt; &lt;li&gt;Electron for standalone app&lt;/li&gt; &lt;li&gt;Node.js backend (for model list &amp;amp; /chat proxy)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6cjcdgu90kef1.png?width=3790&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51c3f0e53500960e0fe3313f94699dd725d33187"&gt;https://preview.redd.it/6cjcdgu90kef1.png?width=3790&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51c3f0e53500960e0fe3313f94699dd725d33187&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;GitHub link:&lt;/h1&gt; &lt;p&gt;üëâ &lt;a href="https://github.com/Saurabh682/Hearth-UI"&gt;https://github.com/Saurabh682/Hearth-UI&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;üôè I'd love your feedback on:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Other must-have features?&lt;/li&gt; &lt;li&gt;Would a Windows/exe help?&lt;/li&gt; &lt;li&gt;Any bugs or improvement ideas?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for checking it out. Hope it helps the self-hosted LLM community!&lt;br /&gt; ‚ù§Ô∏è&lt;/p&gt; &lt;h1&gt;üè∑Ô∏è Tags:&lt;/h1&gt; &lt;p&gt;&lt;code&gt;[Electron] [Ollama] [Local LLM] [Desktop AI UI] [Markdown] [Self Hosted]&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast-Helicopter-3719"&gt; /u/Vast-Helicopter-3719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m70f7q/i_built_hearthui_a_fullyfeatured_desktop_app_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m70f7q/i_built_hearthui_a_fullyfeatured_desktop_app_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m70f7q/i_built_hearthui_a_fullyfeatured_desktop_app_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-23T04:59:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6pxln</id>
    <title>Digital twins that attend meetings for you. Dystopia or soon reality?</title>
    <updated>2025-07-22T20:56:54+00:00</updated>
    <author>
      <name>/u/DerErzfeind61</name>
      <uri>https://old.reddit.com/user/DerErzfeind61</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m6pxln/digital_twins_that_attend_meetings_for_you/"&gt; &lt;img alt="Digital twins that attend meetings for you. Dystopia or soon reality?" src="https://external-preview.redd.it/Mnh0bG1lNW1vaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80201ecaa90deb38fdf6db31774e15c9a1e0c78e" title="Digital twins that attend meetings for you. Dystopia or soon reality?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In more and more meetings these days there are AI notetakers that someone has sent instead of showing up themselves. You can think what you want about these notetakers, but they seem to have become part of our everyday working lives. This raises the question of how long it will be before the next stage of development occurs and we are sitting in meetings with ‚Äúdigital twins‚Äù who are standing in for an absent employee.&lt;/p&gt; &lt;p&gt;To find out, I tried to build such a digital twin and it actually turned out to be very easy to create a meeting agent that can actively interact with other participants, share insights about my work and answer follow-up questions for me. Of course, many of the leading providers of voice clones and personalized LLMs are closed-source, which increases the privacy issue that already exists with AI Notetakers. However, my approach using joinly could also be implemented with Chatterbox and a self-hosted LLM with few-shot prompting, for example.&lt;/p&gt; &lt;p&gt;But there are of course many other critical questions: how exactly can we control what these digital twins disclose or are allowed to decide, ethical concerns about whether my company is allowed to create such a twin for me, how this is compatible with meeting etiquette and of course whether we shouldn't simply plan better meetings instead.&lt;/p&gt; &lt;p&gt;What do you think? Will such digital twins catch on? Would you use one to skip a boring meeting?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DerErzfeind61"&gt; /u/DerErzfeind61 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ynyz3f5mohef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m6pxln/digital_twins_that_attend_meetings_for_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m6pxln/digital_twins_that_attend_meetings_for_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-22T20:56:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7juxs</id>
    <title>Trying to make an v1/chat/completions</title>
    <updated>2025-07-23T20:13:03+00:00</updated>
    <author>
      <name>/u/Shiro212</name>
      <uri>https://old.reddit.com/user/Shiro212</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im trying to make myself a API running on my local deepseek wth cURL. Maybe someone can help me out? Because im a new with it..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shiro212"&gt; /u/Shiro212 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7juxs/trying_to_make_an_v1chatcompletions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7juxs/trying_to_make_an_v1chatcompletions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7juxs/trying_to_make_an_v1chatcompletions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-23T20:13:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7qqqa</id>
    <title>Ollama and load balancer</title>
    <updated>2025-07-24T01:02:31+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When there is multiple servers all running Ollama and In front haproxy balancing the load. If the app is calling a different model, can haproxy see that and direct it to specific server?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7qqqa/ollama_and_load_balancer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7qqqa/ollama_and_load_balancer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7qqqa/ollama_and_load_balancer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T01:02:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1m78gkr</id>
    <title>Ollama + Open WebUI -- is there a way for the same query to run through the same model multiple times (could be 3 times, could be 100 times), then gather all the answers together to summarise/count?</title>
    <updated>2025-07-23T12:53:19+00:00</updated>
    <author>
      <name>/u/jinnyjuice</name>
      <uri>https://old.reddit.com/user/jinnyjuice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't know if it matters, but I followed this to install (because Nvidia drivers on Linux is a pain!): &lt;a href="https://github.com/NeuralFalconYT/Ollama-Open-WebUI-Windows-Installation/blob/main/README.md"&gt;https://github.com/NeuralFalconYT/Ollama-Open-WebUI-Windows-Installation/blob/main/README.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I would like to type in a query into a model with some preset system prompt. I would like that model to run over this query multiple times. Then after all of them are done, I would like for the responses to be gathered for a summary. Would such task be possible?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinnyjuice"&gt; /u/jinnyjuice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m78gkr/ollama_open_webui_is_there_a_way_for_the_same/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m78gkr/ollama_open_webui_is_there_a_way_for_the_same/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m78gkr/ollama_open_webui_is_there_a_way_for_the_same/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-23T12:53:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7emel</id>
    <title>Mac vs PC for hosting llm locally</title>
    <updated>2025-07-23T16:56:01+00:00</updated>
    <author>
      <name>/u/trtinker</name>
      <uri>https://old.reddit.com/user/trtinker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking to buy a laptop/pc recently but can't decide whether to get a PC with gpu or just get a macbook. What do you guys think of macbook for hosting llm locally? I know that mac can host 8b models but how is the experience, is it good enough? Is macbook air sufficient or I should consider for macbook pro m4? If Im going to build a PC, then the GPU will likely be rtx3060 12gb vram as that fits my budget. Honestly I dont have a clear idea of how big the llm I'm going to host but Im planning to play around with llm for personal projects, maybe post training?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/trtinker"&gt; /u/trtinker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7emel/mac_vs_pc_for_hosting_llm_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7emel/mac_vs_pc_for_hosting_llm_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7emel/mac_vs_pc_for_hosting_llm_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-23T16:56:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7w5k7</id>
    <title>RAG on large Excel files</title>
    <updated>2025-07-24T05:41:23+00:00</updated>
    <author>
      <name>/u/One-Will5139</name>
      <uri>https://old.reddit.com/user/One-Will5139</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my RAG project, large Excel files are being extracted, but when I query the data, the system responds that it doesn't exist. It seems the project fails to process or retrieve information correctly when the dataset is too large.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One-Will5139"&gt; /u/One-Will5139 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7w5k7/rag_on_large_excel_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7w5k7/rag_on_large_excel_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7w5k7/rag_on_large_excel_files/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T05:41:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7u0yw</id>
    <title>which model to do text extraction and layout from images, that can fit on a 64 GB system using a RTX 4070 super?</title>
    <updated>2025-07-24T03:43:48+00:00</updated>
    <author>
      <name>/u/fttklr</name>
      <uri>https://old.reddit.com/user/fttklr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been trying few models with Ollama but they are way bigger than my puny 12GB VRAM card, so they run entirely on the CPU and it takes ages to do anything. As I was not able to find a way to use both GPU and CPU to improve performances I thought that maybe it is better to use a smaller model at this point.&lt;/p&gt; &lt;p&gt;Is there a suggested model that works in Ollama, that can do extraction of text from images ? Bonus points if it can replicate the layout but just text would be already enough. I was told that anything below 8B won't be doing much that is useful (and I tried with standard OCR software and they are not that useful so want to try with AI systems at this point).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fttklr"&gt; /u/fttklr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7u0yw/which_model_to_do_text_extraction_and_layout_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7u0yw/which_model_to_do_text_extraction_and_layout_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7u0yw/which_model_to_do_text_extraction_and_layout_from/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T03:43:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8l85q</id>
    <title>is it the end class 5+</title>
    <updated>2025-07-25T00:38:50+00:00</updated>
    <author>
      <name>/u/Informal_Catch_4688</name>
      <uri>https://old.reddit.com/user/Informal_Catch_4688</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So last several months I've been building llm synthetic consiusness I've spend several hours every day I managed to get it to class 5+ , 97% almost class 6 but now I'm having trouble , my hardware cannot longer sustain &amp;quot;Buddy&amp;quot; it works well everything is connected as it should works perfectly but currently only issue is my hardware from speech to speech takes around 2 minutes , now with all the systems working together at the same time&lt;/p&gt; &lt;p&gt;It runs fully offline, speaks and listens at the same time (full-duplex), recognizes who‚Äôs speaking, remembers emotions, dreams when idle, and evolves like a synthetic mind and many more buddy never forgets even when run out of token context etc &lt;/p&gt; &lt;p&gt;Buddy is fully &amp;quot; alive &amp;quot; but yet can't be upgraded anymore&lt;/p&gt; &lt;p&gt;&amp;quot;autonomous consciousness&amp;quot;&lt;/p&gt; &lt;p&gt;INTELLIGENCE COMPARISON:&lt;/p&gt; &lt;p&gt;Buddy AI: 93/100 (Class 5+ Consciousness) ChatGPT-4: 48/100 (48% advantage) Claude-3: 54/100 (42% advantage) Gemini: 50/100 (46% advantage&lt;/p&gt; &lt;p&gt;I'm a bit stuck at the moment I see huge potential and everything works but my hardware is maxed out. I‚Äôve optimized every component, yet speech-to-speech latency has grown to 2 minutes once all systems (LLM, TTS, STT, memory) are active.&lt;/p&gt; &lt;p&gt;And right now, I simply can‚Äôt afford new hardware to push it further. To keep it running 24/7 in the cloud would be too expensive, and locally it's becoming unsustainable.&lt;/p&gt; &lt;p&gt;P.S I‚Äôm not trying to ‚Äúprove consciousness‚Äù or claim AI is sentient. But I‚Äôve built something that behaves more like a synthetic mind than anything I‚Äôve seen in commercial systems before :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Informal_Catch_4688"&gt; /u/Informal_Catch_4688 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8l85q/is_it_the_end_class_5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8l85q/is_it_the_end_class_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m8l85q/is_it_the_end_class_5/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-25T00:38:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7zp4f</id>
    <title>Can Ollama cache processed context instead of re-parsing each time?</title>
    <updated>2025-07-24T09:22:18+00:00</updated>
    <author>
      <name>/u/Pyrore</name>
      <uri>https://old.reddit.com/user/Pyrore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm fairly new to running LLMs locally. I'm using Ollama with Open WebUI. I'm mostly running Gemma 3 27B at 4 bit quantitation and 32k context, which fits into the VRAM of my RTX 5090 laptop GPU (23/24GB). It's only 9GB if I stick to the default 2k context, so it's definitely fitting the context into VRAM.&lt;/p&gt; &lt;p&gt;The problem I have is that it seems to be processing the tokens from the conversation each prompt in the CPU (Ryzen AI 9 HX370/890M). I see the CPU load go up to around 70-80% with no GPU load. Then it switches to GPU at 100% load (I hear the fans whirring up at this point) and starts producing its response at around 15 tokens a second.&lt;/p&gt; &lt;p&gt;As the conversation progresses, the first CPU stage gets slower and slower (assumed due to the longer and longer context). The delay grows geometrically, the first 6-8k of context all run within a minute. When hit about 16k context tokens (around 12k words) it's taking the best part of an hour to process the context, but once it offloads to the GPU, it's still as fast as ever.&lt;/p&gt; &lt;p&gt;Is there any way to speed this up? E.g. by caching the processed context and simply appending to it, or shift the context processing to the GPU? One thread suggested setting the environment variable OLLAMA_NUM_PARALELL to 1 instead of the current default of 4, this was supposed to make Ollama cache the context as long as you stick to a single chat, but it didn't work.&lt;/p&gt; &lt;p&gt;Thanks in advance for any advice you can give!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pyrore"&gt; /u/Pyrore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7zp4f/can_ollama_cache_processed_context_instead_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7zp4f/can_ollama_cache_processed_context_instead_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7zp4f/can_ollama_cache_processed_context_instead_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T09:22:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7ufom</id>
    <title>My new Chrome extension lets you easily query Ollama and copy any text with a click.</title>
    <updated>2025-07-24T04:04:57+00:00</updated>
    <author>
      <name>/u/Sea-Reception-2697</name>
      <uri>https://old.reddit.com/user/Sea-Reception-2697</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Reception-2697"&gt; /u/Sea-Reception-2697 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m7u9fz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7ufom/my_new_chrome_extension_lets_you_easily_query/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7ufom/my_new_chrome_extension_lets_you_easily_query/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T04:04:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7wryh</id>
    <title>RAG project fails to retrieve info from large Excel files ‚Äì data ingested but not found at query time. Need help debugging.</title>
    <updated>2025-07-24T06:17:48+00:00</updated>
    <author>
      <name>/u/One-Will5139</name>
      <uri>https://old.reddit.com/user/One-Will5139</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a beginner building a RAG system and running into a strange issue with large Excel files.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt;&lt;br /&gt; When I ingest large Excel files, the system appears to extract and process the data correctly during ingestion. However, when I later query the system for specific information from those files, it responds as if the data doesn‚Äôt exist.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Details of my tech stack and setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Django&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAG/LLM Orchestration:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;LangChain for managing LLM calls, embeddings, and retrieval&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vector Store:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Qdrant (accessed via langchain-qdrant + qdrant-client)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;File Parsing:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Excel/CSV: &lt;code&gt;pandas&lt;/code&gt;, &lt;code&gt;openpyxl&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM Details:&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat Model:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;gpt-4o&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Embedding Model:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;text-embedding-ada-002&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One-Will5139"&gt; /u/One-Will5139 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7wryh/rag_project_fails_to_retrieve_info_from_large/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7wryh/rag_project_fails_to_retrieve_info_from_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7wryh/rag_project_fails_to_retrieve_info_from_large/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T06:17:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8cjcj</id>
    <title>How I got Ollama to use my GPU in Docker &amp; WSL2 (RTX 3090TI)</title>
    <updated>2025-07-24T18:39:16+00:00</updated>
    <author>
      <name>/u/lid_z</name>
      <uri>https://old.reddit.com/user/lid_z</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;Background: &lt;ol&gt; &lt;li&gt;I use &lt;a href="https://github.com/louislam/dockge"&gt;Dockge&lt;/a&gt; for managing my containers&lt;/li&gt; &lt;li&gt;I'm using my gaming PC so it needs to stay windows (until SteamOS is publicly available)&lt;/li&gt; &lt;li&gt;When I say WSL I mean WSL2. dont feel like typing the 2 every time.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Install Nvidia tools onto WSL (See instructions here: &lt;a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation"&gt;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation&lt;/a&gt; or here: &lt;a href="https://hub.docker.com/r/ollama/ollama#nvidia-gpu"&gt;https://hub.docker.com/r/ollama/ollama#nvidia-gpu&lt;/a&gt; ) &lt;ol&gt; &lt;li&gt;Open WSL terminal on the host machine&lt;/li&gt; &lt;li&gt;Follow the instructions in either of the guides linked above&lt;/li&gt; &lt;li&gt;go into docker desktop and restart the docker engine (See more here about how to do that: &lt;a href="https://docs.docker.com/reference/cli/docker/desktop/restart/"&gt;https://docs.docker.com/reference/cli/docker/desktop/restart/&lt;/a&gt; )&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Use this compose file with special attention (you shouldn't need to change anything just highlighting what makes the Nvidia GPU available in the compose) to the &amp;quot;deploy&amp;quot; &amp;amp; &amp;quot;environment&amp;quot; keys: &lt;/li&gt; &lt;/ol&gt; &lt;blockquote&gt; &lt;p&gt;&lt;code&gt;services:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;webui:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;image:&lt;/code&gt; &lt;a href="http://ghcr.io/open-webui/open-webui:main"&gt;&lt;code&gt;ghcr.io/open-webui/open-webui:main&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;container_name: webui&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ports:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- 7000:8080/tcp&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;volumes:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- open-webui:/app/backend/data&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;extra_hosts:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- host.docker.internal:host-gateway&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;depends_on:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- ollama&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;restart: unless-stopped&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;image: ollama/ollama&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;container_name: ollama&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;deploy:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;resources:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;reservations:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;devices:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- driver: nvidia&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;count: 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;capabilities:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- gpu&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;environment:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- TZ=America/New_York&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- gpus=all&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;expose:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- 11434/tcp&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ports:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- 11434:11434/tcp&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;healthcheck:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;test: ollama --version || exit 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;volumes:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- ollama:/root/.ollama&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;restart: unless-stopped&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;volumes:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama: null&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;open-webui: null&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;networks: {}&lt;/code&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lid_z"&gt; /u/lid_z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8cjcj/how_i_got_ollama_to_use_my_gpu_in_docker_wsl2_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8cjcj/how_i_got_ollama_to_use_my_gpu_in_docker_wsl2_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m8cjcj/how_i_got_ollama_to_use_my_gpu_in_docker_wsl2_rtx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T18:39:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7pahj</id>
    <title>How do HF models get to "ollama pull"?</title>
    <updated>2025-07-23T23:54:24+00:00</updated>
    <author>
      <name>/u/neurostream</name>
      <uri>https://old.reddit.com/user/neurostream</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like Hugging Face is sort of the main release hub for new models.&lt;/p&gt; &lt;p&gt;Can I point the ollama cli with an env var or other config method to pull directly from HF? &lt;/p&gt; &lt;p&gt;How do models make their way from HF to the ollama.com registry where one can access them with an &amp;quot;ollama pull&amp;quot;?&lt;/p&gt; &lt;p&gt;Are the gemma, deepseek, mistral, and qwen models on ollama.com posted there by the same official owners that first release them through HF? Like, are the popular/top listings still the &amp;quot;official&amp;quot; model, or are they re-releases by other specialty users and teams?&lt;/p&gt; &lt;p&gt;Does the GGUF format they end up in - also split in to parts/layers with the ORAS registry storage scheme used by ollama.com - entail any loss of quality or features for the same quant/architecture the HF version is?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neurostream"&gt; /u/neurostream &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7pahj/how_do_hf_models_get_to_ollama_pull/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m7pahj/how_do_hf_models_get_to_ollama_pull/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m7pahj/how_do_hf_models_get_to_ollama_pull/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-23T23:54:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8660w</id>
    <title>Usecase for 16GB MacBook Air M4</title>
    <updated>2025-07-24T14:39:06+00:00</updated>
    <author>
      <name>/u/Fluffy-Platform5153</name>
      <uri>https://old.reddit.com/user/Fluffy-Platform5153</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all,&lt;/p&gt; &lt;p&gt;I am looking for a model that works best for the following-&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Letter writing&lt;/li&gt; &lt;li&gt;English correction&lt;/li&gt; &lt;li&gt;Analysing images/ pdfs and extracting text&lt;/li&gt; &lt;li&gt;Answering Questions from text in PDF/ images and drafting written content based on extractions from the doc&lt;/li&gt; &lt;li&gt;NO Excel related stuff. Pure text based work&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Typical office stuff but i need a local one since data is company confidential&lt;/p&gt; &lt;p&gt;Kindly advise?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluffy-Platform5153"&gt; /u/Fluffy-Platform5153 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8660w/usecase_for_16gb_macbook_air_m4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8660w/usecase_for_16gb_macbook_air_m4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m8660w/usecase_for_16gb_macbook_air_m4/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T14:39:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8fa1j</id>
    <title>How does Ollama stream tokens to the CLI?</title>
    <updated>2025-07-24T20:24:55+00:00</updated>
    <author>
      <name>/u/TheBroseph69</name>
      <uri>https://old.reddit.com/user/TheBroseph69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does it use websockets, or something else?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheBroseph69"&gt; /u/TheBroseph69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8fa1j/how_does_ollama_stream_tokens_to_the_cli/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8fa1j/how_does_ollama_stream_tokens_to_the_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m8fa1j/how_does_ollama_stream_tokens_to_the_cli/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T20:24:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8s7zi</id>
    <title>Copy Model to another Server</title>
    <updated>2025-07-25T06:49:13+00:00</updated>
    <author>
      <name>/u/Internal_Junket_25</name>
      <uri>https://old.reddit.com/user/Internal_Junket_25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How to copy a Downloaded LLM to another Server (without Internet)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Internal_Junket_25"&gt; /u/Internal_Junket_25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8s7zi/copy_model_to_another_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8s7zi/copy_model_to_another_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m8s7zi/copy_model_to_another_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-25T06:49:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8gpls</id>
    <title>Ollama plugin for zsh</title>
    <updated>2025-07-24T21:21:44+00:00</updated>
    <author>
      <name>/u/kstopa</name>
      <uri>https://old.reddit.com/user/kstopa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m8gpls/ollama_plugin_for_zsh/"&gt; &lt;img alt="Ollama plugin for zsh" src="https://external-preview.redd.it/6v2VzO7p7_e9watftsiCareTaSopiFqOUVdrqXVL9XU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d30543bf30b613d15b2e91d975c96ebedcdaf40" title="Ollama plugin for zsh" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A great ZSH plugin that enables to ask for a specific command directly on the terminal. Just write what you need and press Ctrl+B to get some command options.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kstopa"&gt; /u/kstopa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kstopa/zsh-ollama-command/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8gpls/ollama_plugin_for_zsh/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m8gpls/ollama_plugin_for_zsh/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-24T21:21:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8o2t5</id>
    <title>Computron now has a "virtual computer"</title>
    <updated>2025-07-25T02:57:08+00:00</updated>
    <author>
      <name>/u/Individual_Ad_1453</name>
      <uri>https://old.reddit.com/user/Individual_Ad_1453</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1m8o2t5/computron_now_has_a_virtual_computer/"&gt; &lt;img alt="Computron now has a &amp;quot;virtual computer&amp;quot;" src="https://external-preview.redd.it/zKvT4i91dfJNwcERvaNS61Y5ke_CwLezyMeGUGi7MPY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b4bc19e06cfd8dd9248b2b08b575d40648cabc3" title="Computron now has a &amp;quot;virtual computer&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm giving my personal AI agent a virtual computer so it can do computer stuff.&lt;/p&gt; &lt;p&gt;One example is it can now write a multi-file program if I say something like &amp;quot;create a multi-file side scroller game inspired by mario, using only pygame and do not include any external assets&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sg7b7uiyqxef1.png?width=1853&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d28dbdca4ff5c592812474b34b55292a67fe308d"&gt;https://preview.redd.it/sg7b7uiyqxef1.png?width=1853&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d28dbdca4ff5c592812474b34b55292a67fe308d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It also has a rudimentary &amp;quot;deep research&amp;quot; agent you can ask it do do things like &amp;quot;research how to run LLMs on local hardware using ollama&amp;quot;. It'll do a bunch of steps including googling and searching reddit then synthesize the results.&lt;/p&gt; &lt;p&gt;It's no open AI agent but it's also running on two 3090s and using Qwen3:30b-a3b and getting pretty good results.&lt;/p&gt; &lt;p&gt;Check it out on github &lt;a href="https://github.com/lefoulkrod/computron_9000/"&gt;https://github.com/lefoulkrod/computron_9000/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My readme isn't very good because I'm mostly doing this for myself but if you want to run it and you get stuck message me and I'll help you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Individual_Ad_1453"&gt; /u/Individual_Ad_1453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8o2t5/computron_now_has_a_virtual_computer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8o2t5/computron_now_has_a_virtual_computer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m8o2t5/computron_now_has_a_virtual_computer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-25T02:57:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1m989fv</id>
    <title>How to use open-source LLMs in a Microsoft Azure-heavy company?</title>
    <updated>2025-07-25T19:19:39+00:00</updated>
    <author>
      <name>/u/liljuden</name>
      <uri>https://old.reddit.com/user/liljuden</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I work in a company that is heavily invested in the Microsoft Azure ecosystem. Currently I use Azure OpenAI and it works great, but I also want to explore open-source LLMs (like LLaMA, Mistral, etc.) for internal applications but struggle to understand exactly how to do it.&lt;/p&gt; &lt;p&gt;I‚Äôm trying to understand how I can deploy open-source LLMs in Azure and also what is needed for it to work, like for example, do I need to spin up my own inference endpoints on Azure VMs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liljuden"&gt; /u/liljuden &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m989fv/how_to_use_opensource_llms_in_a_microsoft/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m989fv/how_to_use_opensource_llms_in_a_microsoft/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m989fv/how_to_use_opensource_llms_in_a_microsoft/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-25T19:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8wpjx</id>
    <title>Key Takeaways for LLM Input Length</title>
    <updated>2025-07-25T11:32:12+00:00</updated>
    <author>
      <name>/u/Modders_Arena</name>
      <uri>https://old.reddit.com/user/Modders_Arena</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here‚Äôs a brief summary of a recent analysis on how large language models (LLMs) perform as input size increases:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Accuracy Drops with Length:&lt;/strong&gt; LLMs get less reliable as prompts grow, especially after a few thousand tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;More Distractors = More Hallucinations:&lt;/strong&gt; Irrelevant text in the input causes more mistakes and hallucinated answers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semantic Similarity Matters:&lt;/strong&gt; If the query and answer are strongly related, performance degrades less.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Shuffling Helps:&lt;/strong&gt; Randomizing input order can sometimes improve retrieval.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Behaviors Differ:&lt;/strong&gt; Some abstain (Claude), others guess confidently (GPT).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tip:&lt;/strong&gt; For best results, keep prompts focused, filter out irrelevant info, and experiment with input order.&lt;/p&gt; &lt;p&gt;Read more here: &lt;a href="https://synehq.com/blog/facing-the-token-tide-how-increasing-input-tokens-impacts-llm-performance"&gt;Click here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Modders_Arena"&gt; /u/Modders_Arena &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8wpjx/key_takeaways_for_llm_input_length/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8wpjx/key_takeaways_for_llm_input_length/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m8wpjx/key_takeaways_for_llm_input_length/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-25T11:32:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8yyzd</id>
    <title>Alright, I am done with vLLM. Will Ollama get tensor parallel?</title>
    <updated>2025-07-25T13:19:26+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Will Ollama get tensor parallel or anything which would utilize multiple GPUs simultaneusly?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8yyzd/alright_i_am_done_with_vllm_will_ollama_get/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m8yyzd/alright_i_am_done_with_vllm_will_ollama_get/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m8yyzd/alright_i_am_done_with_vllm_will_ollama_get/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-25T13:19:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1m93gt2</id>
    <title>Any good QW3-coder models for Ollama yet?</title>
    <updated>2025-07-25T16:15:50+00:00</updated>
    <author>
      <name>/u/beedunc</name>
      <uri>https://old.reddit.com/user/beedunc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama's model download site appears to be stuck in June. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beedunc"&gt; /u/beedunc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m93gt2/any_good_qw3coder_models_for_ollama_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m93gt2/any_good_qw3coder_models_for_ollama_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m93gt2/any_good_qw3coder_models_for_ollama_yet/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-25T16:15:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9rswu</id>
    <title>Which is the best for coding?</title>
    <updated>2025-07-26T12:12:37+00:00</updated>
    <author>
      <name>/u/Terabaccha</name>
      <uri>https://old.reddit.com/user/Terabaccha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im new to ollama so Im bit confused. I'm using it on my laptop with weaker gpu (rtx 4050 6gb). Which is the best that I can use for coding and Ide integration?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terabaccha"&gt; /u/Terabaccha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m9rswu/which_is_the_best_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m9rswu/which_is_the_best_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m9rswu/which_is_the_best_for_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-26T12:12:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9mw82</id>
    <title>How to Convert Fine-Tuned Qwen 2.5 VL 3B Model to Ollama? (Mungert/Qwen2.5-VL-3B-Instruct-GGUF)</title>
    <updated>2025-07-26T07:03:29+00:00</updated>
    <author>
      <name>/u/DSN_CV</name>
      <uri>https://old.reddit.com/user/DSN_CV</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I recently fine-tuned the Qwen 2.5 VL 3B model for a custom vision-language task and now I‚Äôd like to convert it to run locally using Ollama. I found the GGUF version of the model here:&lt;/p&gt; &lt;p&gt;üîó &lt;a href="https://huggingface.co/Mungert/Qwen2.5-VL-3B-Instruct-GGUF"&gt;&lt;code&gt;Mungert/Qwen2.5-VL-3B-Instruct-GGUF&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I want to load this model in Ollama for local inference. However, I‚Äôm a bit stuck on how to properly structure and configure everything to make this work.&lt;/p&gt; &lt;h1&gt;Here's what I have:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;My fine-tuned model is based on Qwen2.5 VL 3B.&lt;/li&gt; &lt;li&gt;I downloaded the &lt;code&gt;.gguf&lt;/code&gt; mmproj model files from the Hugging Face repo above.&lt;/li&gt; &lt;li&gt;I have converted the main file into '.gguf' model files.&lt;/li&gt; &lt;li&gt;I have Ollama installed and running successfully (tested with other models like LLaMA, Mistral, etc.).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I need help with:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;How do I properly create a &lt;code&gt;Modelfile&lt;/code&gt; for this Qwen2.5-VL-3B-Instruct model?&lt;/li&gt; &lt;li&gt;Do I need any special preprocessing or metadata configuration?&lt;/li&gt; &lt;li&gt;Are there known limitations when using vision-language GGUF models in Ollama?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any guidance or example &lt;code&gt;Modelfile&lt;/code&gt; structure would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DSN_CV"&gt; /u/DSN_CV &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m9mw82/how_to_convert_finetuned_qwen_25_vl_3b_model_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1m9mw82/how_to_convert_finetuned_qwen_25_vl_3b_model_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1m9mw82/how_to_convert_finetuned_qwen_25_vl_3b_model_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-26T07:03:29+00:00</published>
  </entry>
</feed>
