<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-07-03T20:24:40+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1lpaoz1</id>
    <title>Seeking Advice on Building a Personal ChatGPT/You.com Replica Using Ollama and Open Web UI</title>
    <updated>2025-07-01T19:17:46+00:00</updated>
    <author>
      <name>/u/EntertainmentOk5540</name>
      <uri>https://old.reddit.com/user/EntertainmentOk5540</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm reaching out to the community for some advice on how to use **ollama** with **Open Web UI** to build a personal ChatGPT/You.com replica at home.&lt;/p&gt; &lt;p&gt;My wife and I both rely on AI for our day-to-day work. She uses it primarily for crafting new emails and brainstorming processes, as well as generating graphics and handling various miscellaneous tasks. I, on the other hand, utilize AI for researching IT infrastructure, working with Linux, creating general IoT guides, and troubleshooting/support. Over the past several months, I‚Äôve found myself heavily dependent on the smart search feature within You.com.&lt;/p&gt; &lt;p&gt;The reason I‚Äôm posting is that my subscription‚Äîwhich I bought at a heavily discounted price several months ago‚Äîis coming to an end soon. I‚Äôm hoping to use ollama locally as a replacement to avoid the high renewal costs. I plan to run this on my gaming computer, which is already on 24/7. The specs are a **Ryzen 9 5900X** with an **RTX 3060 12GB GPU**.&lt;/p&gt; &lt;p&gt;I would greatly appreciate any guidance on how to set up the environment correctly, what models to use, and any additional advice so that we can maintain the functionality we currently enjoy, especially since we leverage several of the ChatGPT AI models.&lt;/p&gt; &lt;p&gt;Thanks in advance for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EntertainmentOk5540"&gt; /u/EntertainmentOk5540 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpaoz1/seeking_advice_on_building_a_personal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpaoz1/seeking_advice_on_building_a_personal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lpaoz1/seeking_advice_on_building_a_personal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-01T19:17:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp08de</id>
    <title>Ollama GPU Underutilization (RTX 2070) - CPU Overload?</title>
    <updated>2025-07-01T12:21:56+00:00</updated>
    <author>
      <name>/u/alchemistST</name>
      <uri>https://old.reddit.com/user/alchemistST</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; ,&lt;/p&gt; &lt;p&gt;I'm trying to optimize my local LLM setup with Ollama and Open WebUI, and I'm encountering some odd GPU usage. I'm hoping someone with similar hardware or more experience can shed some light on this.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Ryzen 5 3600&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 16GB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; RTX 2070 (8GB VRAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama &amp;amp; Open WebUI:&lt;/strong&gt; Running directly on Archlinux (no Docker virtualization)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm running models like &lt;code&gt;mistral:7b-instruct-q4&lt;/code&gt; and &lt;code&gt;gemma3:4b&lt;/code&gt; and finding them quite slow. Fine, reasonable, my hardware specs are tight, but being this the case, I would expect GPU working hard, but my monitoring tools show otherwise:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;nvtop&lt;/code&gt;: GPU usage rarely exceeds 25%, and only for brief spikes. VRAM usage doesn't exceed 20%.&lt;/li&gt; &lt;li&gt;&lt;code&gt;btop&lt;/code&gt;: My CPU (Ryzen 5 3600) is heavily utilized, frequently peaking above 50% with multiple cores hitting 100%.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I've Checked (and why I'm confused):&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Ollama GPU Detection:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;ollama ps&lt;/code&gt; shows the active model indicating &amp;quot;100% GPU&amp;quot; under the &lt;code&gt;PROCESSOR&lt;/code&gt; column.&lt;/li&gt; &lt;li&gt;Ollama logs confirm CUDA detection and identify my RTX 2070 (example log snippet below for context).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;My Question:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is this level of GPU utilization (under 25%) normal when running these types of models locally on the GPU, or is there something that might make my models not run on the GPU and run on the CPU, instead?&lt;/li&gt; &lt;li&gt;Is there anything else I could do to ensure the models run on the GPU, or any other way to debug why there might not be running on the GPU?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any insights or suggestions would be greatly appreciated! Thanks in advance!&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Jul 01 13:24:41 archlinux ollama[90528]: CUDA driver version: 12.8 Jul 01 13:24:41 archlinux ollama[90528]: calling cuDeviceGetCount Jul 01 13:24:41 archlinux ollama[90528]: device count 1 Jul 01 13:24:41 archlinux ollama[90528]: time=2025-07-01T13:24:41.344+02:00 level=DEBUG source =gpu.go:125 msg=&amp;quot;detected GPUs&amp;quot; count=1 library=/usr/lib/libcuda.so.570.153.02 Jul 01 13:24:41 archlinux ollama[90528]: [GPU-bcba49f7-d2eb-7e44-e137-5b623c16e047] CUDA total Mem 7785mb Jul 01 13:24:41 archlinux ollama[90528]: [GPU-bcba49f7-d2eb-7e44-e137-5b623c16e047] CUDA freeM em 7343mb Jul 01 13:24:41 archlinux ollama[90528]: [GPU-bcba49f7-d2eb-7e44-e137-5b623c16e047] Compute Ca pability 7.5 Jul 01 13:24:41 archlinux ollama[90528]: time=2025-07-01T13:24:41.610+02:00 level=DEBUG source =amd_linux.go:419 msg=&amp;quot;amdgpu driver not detected /sys/module/amdgpu&amp;quot; Jul 01 13:24:41 archlinux ollama[90528]: releasing cuda driver library Jul 01 13:24:41 archlinux ollama[90528]: time=2025-07-01T13:24:41.610+02:00 level=INFO source= types.go:130 msg=&amp;quot;inference compute&amp;quot; id=GPU-bcba49f7-d2eb-7e44-e137-5b623c16e047 library=cuda variant=v12 compute=7.5 driver=12.8 name=&amp;quot;NVIDIA GeForce RTX 2070&amp;quot; total=&amp;quot;7.6 GiB&amp;quot; available=&amp;quot; 7.2 GiB&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alchemistST"&gt; /u/alchemistST &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lp08de/ollama_gpu_underutilization_rtx_2070_cpu_overload/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lp08de/ollama_gpu_underutilization_rtx_2070_cpu_overload/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lp08de/ollama_gpu_underutilization_rtx_2070_cpu_overload/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-01T12:21:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lph1ke</id>
    <title>Need guidance on windows vs windows wsl2 for local llm based RAG.</title>
    <updated>2025-07-01T23:39:45+00:00</updated>
    <author>
      <name>/u/CantaloupeBubbly3706</name>
      <uri>https://old.reddit.com/user/CantaloupeBubbly3706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a minisforum X1 A1(AMD ryzen) pro with 96 GB RAM. I want to create a production grade RAG using ollama+Mixtral-8x7b. Eventually for my RAG I want to integrate it with langchain/llanaindex, qdrant( for vector databas), litellm etc. I am trying to figure out the right approach in terms of performance, future support etc. I am reading conflicting information where one says native windows is faster and all these mentioned tools provide good support and other information says wsl2 is more optimized and will provide better inference speeds and ecosystem support. I looked directly into the website but found no information conclusively pointing in either direction. So finally reaching out to community for support and guidance. Have you tried something similar and based on your experience what option should I go with? Thanks in advance üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CantaloupeBubbly3706"&gt; /u/CantaloupeBubbly3706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lph1ke/need_guidance_on_windows_vs_windows_wsl2_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lph1ke/need_guidance_on_windows_vs_windows_wsl2_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lph1ke/need_guidance_on_windows_vs_windows_wsl2_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-01T23:39:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp9u1d</id>
    <title>LiteChat : A web UI for all your LLM you can run with a simple http server</title>
    <updated>2025-07-01T18:45:18+00:00</updated>
    <author>
      <name>/u/dbuildofficial</name>
      <uri>https://old.reddit.com/user/dbuildofficial</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lp9u1d/litechat_a_web_ui_for_all_your_llm_you_can_run/"&gt; &lt;img alt="LiteChat : A web UI for all your LLM you can run with a simple http server" src="https://b.thumbs.redditmedia.com/Bg9X3SJYYVI4nx5WGr_sAAicUbnUb8VzxpgsVjVy_cg.jpg" title="LiteChat : A web UI for all your LLM you can run with a simple http server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I am the creator of &lt;a href="https://litechat.dev/"&gt;https://litechat.dev/&lt;/a&gt; .&lt;br /&gt; repo : &lt;a href="https://github.com/DimitriGilbert/LiteChat"&gt;https://github.com/DimitriGilbert/LiteChat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g5moa13fcbaf1.jpg?width=1914&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=153d16b11e52e96d51bb3f205f5468dcf0d1f85f"&gt;https://preview.redd.it/g5moa13fcbaf1.jpg?width=1914&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=153d16b11e52e96d51bb3f205f5468dcf0d1f85f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is a an AI chat I created to be able to use both local and served LLM all in your browser.&lt;br /&gt; It is local first and only needs an HTTP serve to run, everything stay in your browser !&lt;br /&gt; Data is saved in an IndexeDB database and you can synchronize your conversations using git.&lt;/p&gt; &lt;p&gt;Yes, in the browser ( &lt;a href="https://isomorphic-git.org/"&gt;https://isomorphic-git.org/&lt;/a&gt; ) :P To do that I had to also implement a virtual file system (in the browser using &lt;a href="https://github.com/zen-fs"&gt;https://github.com/zen-fs&lt;/a&gt; ).&lt;br /&gt; So you have access to both ! you can clone a repo and join files from the vfs in your conversations !&lt;/p&gt; &lt;p&gt;But because manually selecting files was a chore, i have built in tool support for the vfs and git !&lt;/p&gt; &lt;p&gt;The basic architecture being there for tools, I added support for HTTP MCP servers, but missing stdio stuff was annoying, so you also have a bridge rewrote by AI from &lt;a href="https://github.com/sparfenyuk/mcp-proxy"&gt;https://github.com/sparfenyuk/mcp-proxy&lt;/a&gt; to use them (you can deploy it where ever you fancy but it is not secured !)&lt;/p&gt; &lt;p&gt;That said I was a bit bored by the text only output, so I added support for mermaid diagrams and html form (so the AI can gather specific information when needed without you having to think what ^^ ). Mermaid diagrams were a bit old fashion, and because I added a workflow module with &lt;a href="https://reactflow.dev/"&gt;https://reactflow.dev/&lt;/a&gt; vizualisations, I also added a way for LLM to create you one !&lt;/p&gt; &lt;p&gt;As always typing the same prompts with just a few difference was also annoying (and because I needed that for workflows !) I have a prompt library module with templates so you can just fill up a form ;)&lt;/p&gt; &lt;p&gt;And what are Agents but a system prompt, tools and specific prompts for tasks ? Yup ! it's the same, so you have that to !&lt;/p&gt; &lt;p&gt;Prompts and agents can integrate into workflows (duh, they were meant for that !) but you also have &amp;quot;transform&amp;quot;/user code execution/&amp;quot;custom prompt&amp;quot; steps to help you chain things together nicely !&lt;/p&gt; &lt;p&gt;As you might have guess, if I have some form of code execution for workflows, can't I have that for AI generated code ?&lt;br /&gt; Yes, yes you can ! either python with &lt;a href="https://pyodide.org/"&gt;https://pyodide.org/&lt;/a&gt; or javascript using &lt;a href="https://github.com/justjake/quickjs-emscripten"&gt;https://github.com/justjake/quickjs-emscripten&lt;/a&gt; .&lt;br /&gt; If you are feeling adventurous, you have an &amp;quot;unsafe&amp;quot; (eval and yolo XD) mod for js execution that can produce stuff (like that one shot threejs scroll shooter &lt;a href="https://dimitrigilbert.github.io/racebench/scroller/claude-sonnet-4.html"&gt;https://dimitrigilbert.github.io/racebench/scroller/claude-sonnet-4.html&lt;/a&gt; ) that you can export in 1 click (template is ugly but I'll be working on that !)&lt;/p&gt; &lt;p&gt;In order not to destroy the system prompt, all these custom UI block can be &amp;quot;activated&amp;quot; (more like suggested ^^) using rules. You can of course add you own rules and you have an AI selector for the best fitting rules for your current prompt.&lt;/p&gt; &lt;p&gt;Of course you have the usual regen (with a different model if you'd like) and forking, but you can also edit a response manually if you want (trim the fat or fix dumbness more easily !). Code block can also be edited manually with syntax coloration for the most common language but no fancy auto complete or what not !). You can also summarize a conversation with one click if needed !&lt;/p&gt; &lt;p&gt;To cap things off but maybe not needed (or practically implemented is more true) for local llms, you can race your model against one another with an unlimited number of participants.&lt;br /&gt; It is nice to benchmark things or when you want to have multiple takes on a prompt without having to copy paste.&lt;br /&gt; I even made a small tool that take an exported race conversation and create a benchmark like recap (more targeted at the js execution block for now &lt;a href="https://dimitrigilbert.github.io/racebench/scroller/index.html"&gt;https://dimitrigilbert.github.io/racebench/scroller/index.html&lt;/a&gt; for the &amp;quot;game&amp;quot; of earlier)&lt;/p&gt; &lt;p&gt;I am most certainly forgetting a few bits and bobs but you got the gist of it ^^&lt;br /&gt; Bit of warning though, I did not try with Ollama (it runs like scrap on my system :( ) so I migth need to cook a few tweaks to support models capabilities.&lt;/p&gt; &lt;p&gt;The hosted version is on github pages and there is no tracking, no account required ! you bring your own API keys !&lt;br /&gt; You probably wont be able to use the hosted version for you local llm because of https/http restriction, but as I said, you can download &lt;a href="https://github.com/DimitriGilbert/LiteChat/releases"&gt;https://github.com/DimitriGilbert/LiteChat/releases&lt;/a&gt; and host with a simple http server.&lt;br /&gt; You even have localized version for French, Italian, German and Spanish.&lt;br /&gt; A small (highly incomplete) playlist of tutorial if you are feeling a bit lost &lt;a href="https://www.youtube.com/playlist?list=PL5Doe56gCsNRdNyfetOYPQw_JkPHO3XVh"&gt;https://www.youtube.com/playlist?list=PL5Doe56gCsNRdNyfetOYPQw_JkPHO3XVh&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I hope you'll enjoy and constructive feedback greatly appreciated :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dbuildofficial"&gt; /u/dbuildofficial &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lp9u1d/litechat_a_web_ui_for_all_your_llm_you_can_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lp9u1d/litechat_a_web_ui_for_all_your_llm_you_can_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lp9u1d/litechat_a_web_ui_for_all_your_llm_you_can_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-01T18:45:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpnp5b</id>
    <title>Guidance</title>
    <updated>2025-07-02T05:24:23+00:00</updated>
    <author>
      <name>/u/barrulus</name>
      <uri>https://old.reddit.com/user/barrulus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all&lt;/p&gt; &lt;p&gt;I am running a rather lacklustre RTX 3070 locally with my ollama setup and was wondering what models you‚Äôve had success with n that sort of GPU range? 8GB RAM)&lt;/p&gt; &lt;p&gt;Even small models like qwen3:4b unpack too large to fit in the 8GB.&lt;/p&gt; &lt;p&gt;I am looking for a model that can do role play and creative world building - doesn‚Äôt have to be lightening fast but it would be nicer than taking minutes to do anything‚Ä¶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/barrulus"&gt; /u/barrulus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpnp5b/guidance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpnp5b/guidance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lpnp5b/guidance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T05:24:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp35d3</id>
    <title>Ollama Dev Companion v0.2.0 - Major overhaul based on your feedback! üöÄ</title>
    <updated>2025-07-01T14:30:01+00:00</updated>
    <author>
      <name>/u/StayHigh24-7</name>
      <uri>https://old.reddit.com/user/StayHigh24-7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent the last few weeks completely rewriting the extension from the ground up. Here's what's new in v0.2.0:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;üèóÔ∏è Complete Architecture Overhaul&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Rewrote everything with proper dependency injection&lt;/li&gt; &lt;li&gt;Fixed all the security vulnerabilities (yes, there were XSS issues üòÖ)&lt;/li&gt; &lt;li&gt;Added comprehensive error handling and recovery&lt;/li&gt; &lt;li&gt;Implemented proper memory management&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am thinking to add MCP support for better tool integration for extending the power of LocalLLMs&lt;/p&gt; &lt;p&gt;Here is the extension url:&lt;br /&gt; &lt;strong&gt;MarketPlace&lt;/strong&gt;: &lt;a href="https://marketplace.visualstudio.com/items?itemName=Gnana997.ollama-dev-companion"&gt;https://marketplace.visualstudio.com/items?itemName=Gnana997.ollama-dev-companion&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href="https://github.com/gnana997/ollama-copilot"&gt;https://github.com/gnana997/ollama-copilot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would love to hear some feedback and What features would you like to see next? I'm particularly excited about the MCP integration - imagine having your local AI access your development tools!&lt;/p&gt; &lt;p&gt;Thanks!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StayHigh24-7"&gt; /u/StayHigh24-7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lp35d3/ollama_dev_companion_v020_major_overhaul_based_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lp35d3/ollama_dev_companion_v020_major_overhaul_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lp35d3/ollama_dev_companion_v020_major_overhaul_based_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-01T14:30:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpi6jc</id>
    <title>Is Mac Mini M4 Pro Good Enough for Local Models Like Ollama?</title>
    <updated>2025-07-02T00:33:21+00:00</updated>
    <author>
      <name>/u/connectome16</name>
      <uri>https://old.reddit.com/user/connectome16</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm considering getting a Mac Mini M4 for my wife, and we're both interested in exploring local AI model, specifically language models through tools like Ollama.&lt;/p&gt; &lt;p&gt;The configuration I‚Äôm looking at is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;M4 Pro chip&lt;/li&gt; &lt;li&gt;12-core CPU&lt;/li&gt; &lt;li&gt;16-core GPU&lt;/li&gt; &lt;li&gt;16-core Neural Engine&lt;/li&gt; &lt;li&gt;48GB unified memory&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Before finalizing the purchase, I have a few questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Would this be sufficient to run llms locally?&lt;/li&gt; &lt;li&gt;Would Ollama run smoothly on this spec?&lt;/li&gt; &lt;li&gt;If performance is a concern, is it more helpful to upgrade to the 14-core CPU / 20-core GPU, or should I focus on increasing the RAM to 64GB?&lt;/li&gt; &lt;li&gt;Has anyone here run language models successfully on an M4 Mac Mini or other Apple Silicon machines?&lt;/li&gt; &lt;li&gt;Any known performance limitations or workarounds on macOS?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I‚Äôve seen some people recommend avoiding Macs for image generation due to lack of NVIDIA GPU support, but I‚Äôm curious how well the current Apple Silicon + Ollama setup performs in practice. A Mac Studio is likely out of budget, so I‚Äôd love to hear whether the M4 Mini is a viable middle ground.&lt;/p&gt; &lt;p&gt;Thanks so much for your help and insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/connectome16"&gt; /u/connectome16 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpi6jc/is_mac_mini_m4_pro_good_enough_for_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpi6jc/is_mac_mini_m4_pro_good_enough_for_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lpi6jc/is_mac_mini_m4_pro_good_enough_for_local_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T00:33:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpq2uw</id>
    <title>LLM classification for taxonomy</title>
    <updated>2025-07-02T07:59:13+00:00</updated>
    <author>
      <name>/u/420Deku</name>
      <uri>https://old.reddit.com/user/420Deku</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have data which consists of lots of rows maybe in millions. It has columns like description, now I want to use each description and classify them into categories. Now the main problem is I have categorical hierarchy into 3 parts like category-&amp;gt; sub category -&amp;gt; sub of sub category and I have pre defined categories and combination which goes around 1000 values. I am not sure which method will give me the highest accuracy. I have used embedding and etc but there are evident flaws. I want to use LLM on a good scale to give maximum accuracy. I have lots of data to even fine tune also but I want a straight plan and best approach. Please help me understand the best way to get maximum accuracy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/420Deku"&gt; /u/420Deku &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpq2uw/llm_classification_for_taxonomy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpq2uw/llm_classification_for_taxonomy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lpq2uw/llm_classification_for_taxonomy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T07:59:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpkgzt</id>
    <title>Apologies for the basic question‚Äîjust starting out and very curious about local LLMs</title>
    <updated>2025-07-02T02:27:39+00:00</updated>
    <author>
      <name>/u/connectome16</name>
      <uri>https://old.reddit.com/user/connectome16</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; I‚Äôm fairly new to the world of local LLMs, so apologies in advance if this is a very basic question. I‚Äôve been searching through forums and documentation, but I figured I‚Äôd get better insights by asking directly here.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Why do people use local LLMs?&lt;br /&gt; With powerful models like ChatGPT, Gemini, and Perplexity available online (trained on massive datasets) what‚Äôs the benefit of running a smaller model locally? Since local PCs can‚Äôt usually run the biggest models due to hardware limits, what‚Äôs the appeal beyond just privacy?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I‚Äôve started exploring local image generation (using FLUX.1), and I get that local setups allow for more customization. Even with FLUX.1, it feels like we're still tapping into a model trained on a large dataset (via API or downloaded weights). So I can see some benefits there. But when it comes to language models, what are the real advantages of running them locally besides privacy and offline access?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I‚Äôm an academic researcher, mainly looking for reasoning and writing support (e.g., manuscript drafts or exploring research ideas). Would I actually benefit from using a local LLM in this case? I imagine training or fine-tuning on specific journal articles could help match academic tone, but wouldn‚Äôt platforms like ChatGPT or Gemini still perform better for these kinds of tasks?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I‚Äôd love to hear how others are using their local LLMs to get some insight on how to use it. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/connectome16"&gt; /u/connectome16 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpkgzt/apologies_for_the_basic_questionjust_starting_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpkgzt/apologies_for_the_basic_questionjust_starting_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lpkgzt/apologies_for_the_basic_questionjust_starting_out/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T02:27:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq2pma</id>
    <title>Gemini CLI executes commads in deepseek LLM (via Ollama in Termux)</title>
    <updated>2025-07-02T17:59:32+00:00</updated>
    <author>
      <name>/u/apravint</name>
      <uri>https://old.reddit.com/user/apravint</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lq2pma/gemini_cli_executes_commads_in_deepseek_llm_via/"&gt; &lt;img alt="Gemini CLI executes commads in deepseek LLM (via Ollama in Termux)" src="https://external-preview.redd.it/xF-vpfQr66ZOyLXu-bPweVxMku12asgGtpSRuc5Jh3Y.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f502b820e299f2f7aa9736b606d821995f069170" title="Gemini CLI executes commads in deepseek LLM (via Ollama in Termux)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/apravint"&gt; /u/apravint &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/shorts/8X1Lh-t1gLI?si=_N_Eis7EMVPD-er_"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lq2pma/gemini_cli_executes_commads_in_deepseek_llm_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lq2pma/gemini_cli_executes_commads_in_deepseek_llm_via/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T17:59:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpign4</id>
    <title>Why Do AI Models Default to Python Code in Their Responses?</title>
    <updated>2025-07-02T00:47:27+00:00</updated>
    <author>
      <name>/u/GloriousLion18</name>
      <uri>https://old.reddit.com/user/GloriousLion18</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why do many AI models (like gemma, Lama, qwen, etc) often include Python code in their responses by default?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GloriousLion18"&gt; /u/GloriousLion18 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpign4/why_do_ai_models_default_to_python_code_in_their/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpign4/why_do_ai_models_default_to_python_code_in_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lpign4/why_do_ai_models_default_to_python_code_in_their/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T00:47:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpchao</id>
    <title>TimeCapsule-SLM - Open Source AI Deep Research Platform That Runs 100% in Your Browser!</title>
    <updated>2025-07-01T20:27:45+00:00</updated>
    <author>
      <name>/u/adssidhu86</name>
      <uri>https://old.reddit.com/user/adssidhu86</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lpchao/timecapsuleslm_open_source_ai_deep_research/"&gt; &lt;img alt="TimeCapsule-SLM - Open Source AI Deep Research Platform That Runs 100% in Your Browser!" src="https://preview.redd.it/ma9l20u8obaf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e9c105132c47fdebf61ac5c089af6603184d7003" title="TimeCapsule-SLM - Open Source AI Deep Research Platform That Runs 100% in Your Browser!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Heyüëã&lt;br /&gt; Just launched &lt;a href="https://timecapsule.bubblspace.com/"&gt;TimeCapsule-SLM&lt;/a&gt; - an open source AI research platform that I think you'll find interesting. The key differentiator? Everything runs locally in your browser with complete privacy.üî• What it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In-Browser RAG: Upload PDFs/documents, get AI insights without sending data to servers&lt;/li&gt; &lt;li&gt;TimeCapsule Sharing: Export/import complete research sessions as .timecapsule.json files&lt;/li&gt; &lt;li&gt;Multi-LLM Support: Works with Ollama, LM Studio, OpenAI APIs&lt;/li&gt; &lt;li&gt;Two main tools: DeepResearch (for novel idea generation) + Playground (for visual coding)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üîí Privacy Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Zero server dependency after initial load&lt;/li&gt; &lt;li&gt;All processing happens locally&lt;/li&gt; &lt;li&gt;Your data never leaves your device&lt;/li&gt; &lt;li&gt;Works offline once models are loaded&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üéØ Perfect for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Researchers who need privacy-first AI tools&lt;/li&gt; &lt;li&gt;Teams wanting to share research sessions&lt;/li&gt; &lt;li&gt;Anyone building local AI workflows&lt;/li&gt; &lt;li&gt;People tired of cloud-dependent tools&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Live Demo: &lt;a href="https://timecapsule.bubblspace.com"&gt;https://timecapsule.bubblspace.com&lt;/a&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/thefirehacker/TimeCapsule-SLM"&gt;https://github.com/thefirehacker/TimeCapsule-SLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Ollama integration is particularly smooth - just enable CORS and you're ready to go with local models like qwen3:0.6b.Would love to hear your thoughts and feedback! Also happy to answer any technical questions about the implementation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adssidhu86"&gt; /u/adssidhu86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ma9l20u8obaf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpchao/timecapsuleslm_open_source_ai_deep_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lpchao/timecapsuleslm_open_source_ai_deep_research/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-01T20:27:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpxsxk</id>
    <title>DeepSeek R1 8b: was it supposed to support tools?</title>
    <updated>2025-07-02T14:48:45+00:00</updated>
    <author>
      <name>/u/Effective_Head_5020</name>
      <uri>https://old.reddit.com/user/Effective_Head_5020</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to use DeepSeek R1 8b through the HTTP API, but it says that it does not support tools. Is that correct? Or am I doing something wrong? Let me know and I can share more details&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective_Head_5020"&gt; /u/Effective_Head_5020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpxsxk/deepseek_r1_8b_was_it_supposed_to_support_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lpxsxk/deepseek_r1_8b_was_it_supposed_to_support_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lpxsxk/deepseek_r1_8b_was_it_supposed_to_support_tools/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T14:48:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq09qy</id>
    <title>Hardware advice?</title>
    <updated>2025-07-02T16:24:41+00:00</updated>
    <author>
      <name>/u/Glittering-Role3913</name>
      <uri>https://old.reddit.com/user/Glittering-Role3913</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone, i hope this is the right place to ask this. &lt;/p&gt; &lt;p&gt;Recently I've gotten into using local llms and I foresee myself getting alot of utility out of local llms. With that said, I want to upgrade my rig to be able to run models like deepseek r1 32b with 8-bit quantization locally inside of a vm. &lt;/p&gt; &lt;p&gt;My setup is: Ryzen 5 7600 (6 core, 12 thread) 2x8gb ddr5 ram (4800mhz at cl40) rx 7800 xt (16gb gddr6) Rtx 3060 (12gb gddr6) Powered by a 1000w psu OS: debian 12 (server)&lt;/p&gt; &lt;p&gt;Because I run the llms in a vm, I allocate 6 threads to the llms with 8gb of memory (i have other vms that require the other 8gb). &lt;/p&gt; &lt;p&gt;Total RAM - 28gb gddr6 + 8gb ddr5&lt;/p&gt; &lt;p&gt;Due to limited system resources, I realize that I need more system RAM or more VRAM. Ram will cost me $250 CAD after tax (2x32gb ddr5, 6000mhz cl30) whereas I can spend $300 CAD and get another 3060 (12gb gddr6). &lt;/p&gt; &lt;p&gt;Option A - 40gb gddr6 + 8gb ddr5 (cl40, 4800mhz) Option B - 28gb gddr6 + 64gb ddr5 (cl30, 6000 mhz)&lt;/p&gt; &lt;p&gt;My question is which one should I go with? Given my requirements, which one makes more sense? Are my requirements too intense, would it require too much VRAM? What models will provide similar performance or atleast really good performance given my setup in your opinion. Advice is greatly appreciated. &lt;/p&gt; &lt;p&gt;As long as I can get around 4 tokens per second under 8-bit quantization with an accurate model, id say im pretty satisfied. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glittering-Role3913"&gt; /u/Glittering-Role3913 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lq09qy/hardware_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lq09qy/hardware_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lq09qy/hardware_advice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T16:24:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqaff2</id>
    <title>Dumb question, but how do you choose an LLM that's most appropriate for your system in the event of restrictions (no / lightweight GPU, limited RAM, etc)?</title>
    <updated>2025-07-02T23:18:02+00:00</updated>
    <author>
      <name>/u/-ThatGingerKid-</name>
      <uri>https://old.reddit.com/user/-ThatGingerKid-</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-ThatGingerKid-"&gt; /u/-ThatGingerKid- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqaff2/dumb_question_but_how_do_you_choose_an_llm_thats/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqaff2/dumb_question_but_how_do_you_choose_an_llm_thats/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqaff2/dumb_question_but_how_do_you_choose_an_llm_thats/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T23:18:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqe0ap</id>
    <title>Gemma3 e series</title>
    <updated>2025-07-03T02:14:08+00:00</updated>
    <author>
      <name>/u/wahnsinnwanscene</name>
      <uri>https://old.reddit.com/user/wahnsinnwanscene</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can anyone give some insight on the new gemma3 with the matroshka learning model? It sounds like a highly powered network in network NIN &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wahnsinnwanscene"&gt; /u/wahnsinnwanscene &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqe0ap/gemma3_e_series/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqe0ap/gemma3_e_series/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqe0ap/gemma3_e_series/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T02:14:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqnwac</id>
    <title>Help!! Ollama on AMD</title>
    <updated>2025-07-03T12:07:40+00:00</updated>
    <author>
      <name>/u/nqdat1995</name>
      <uri>https://old.reddit.com/user/nqdat1995</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Could someone help me run Ollama on my AMD Radeon 6800 GPU. I run Ollama but it always runs on CPU instead :((&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nqdat1995"&gt; /u/nqdat1995 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqnwac/help_ollama_on_amd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqnwac/help_ollama_on_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqnwac/help_ollama_on_amd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T12:07:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqphsw</id>
    <title>What's the difference between ollama.embeddings() and ollama.embed() ? Why do the methods return different embeddings for the same model (code in description)?</title>
    <updated>2025-07-03T13:23:21+00:00</updated>
    <author>
      <name>/u/LordTerminator</name>
      <uri>https://old.reddit.com/user/LordTerminator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am calling both methods to compare the embeddings they return.&lt;/p&gt; &lt;p&gt;&lt;code&gt;ll = ollama.embeddings(model='llama3.2',&lt;/code&gt;&lt;br /&gt; &lt;code&gt;prompt = 'The sky is blue because of rayleigh scattering'&lt;/code&gt;&lt;br /&gt; &lt;code&gt;)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;llm = dict(ll)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;llm['embedding']&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ll = ollama.embed(model='llama3.2',&lt;/code&gt;&lt;br /&gt; &lt;code&gt;input = 'The sky is blue because of rayleigh scattering'&lt;/code&gt;&lt;br /&gt; &lt;code&gt;)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;llm = dict(ll)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;llm['embeddings'][0]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;They return different embeddings for the same model. Why is that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LordTerminator"&gt; /u/LordTerminator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqphsw/whats_the_difference_between_ollamaembeddings_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqphsw/whats_the_difference_between_ollamaembeddings_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqphsw/whats_the_difference_between_ollamaembeddings_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T13:23:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq8yx7</id>
    <title>Best lightweight model for running on CPU with low RAM?</title>
    <updated>2025-07-02T22:13:36+00:00</updated>
    <author>
      <name>/u/-ThatGingerKid-</name>
      <uri>https://old.reddit.com/user/-ThatGingerKid-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've got an unRAID server and I've set up Open WebUI and Ollama on it. Problem is, I've only got 16gb of RAM and no GPU... I plan to upgrade eventually, but can't afford that right now. As a beginner, the sheer mass of options in Ollama is a bit overwhelming. What options would you recommend for lightweight hardware?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-ThatGingerKid-"&gt; /u/-ThatGingerKid- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lq8yx7/best_lightweight_model_for_running_on_cpu_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lq8yx7/best_lightweight_model_for_running_on_cpu_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lq8yx7/best_lightweight_model_for_running_on_cpu_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T22:13:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqfy1w</id>
    <title>A little project to analyze stock trends and explain major movements</title>
    <updated>2025-07-03T03:55:32+00:00</updated>
    <author>
      <name>/u/m19990328</name>
      <uri>https://old.reddit.com/user/m19990328</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lqfy1w/a_little_project_to_analyze_stock_trends_and/"&gt; &lt;img alt="A little project to analyze stock trends and explain major movements" src="https://b.thumbs.redditmedia.com/-tJiAQz3zixSDJrySlOjJVjaLSvDEuXQ8cBOZ83hpYo.jpg" title="A little project to analyze stock trends and explain major movements" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a tool that tries to explain the market movements to better understand the risk of investing in any stocks. I'd love to hear your opinion.&lt;/p&gt; &lt;p&gt;üëâ&lt;a href="https://github.com/CyrusCKF/stock-gone-wrong"&gt;https://github.com/CyrusCKF/stock-gone-wrong&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m19990328"&gt; /u/m19990328 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lqfy1w"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqfy1w/a_little_project_to_analyze_stock_trends_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqfy1w/a_little_project_to_analyze_stock_trends_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T03:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqor90</id>
    <title>Build a Multi-Agent AI Investment Advisor using Ollama, LangGraph, and Streamlit</title>
    <updated>2025-07-03T12:49:41+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lqor90/build_a_multiagent_ai_investment_advisor_using/"&gt; &lt;img alt="Build a Multi-Agent AI Investment Advisor using Ollama, LangGraph, and Streamlit" src="https://external-preview.redd.it/jrYQtvyJEJq6ekkw4MqwVzItNTy5yAMF5kFGArliMc8.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10054d9e3a79cf2b91c7dfa3d5941441e8535236" title="Build a Multi-Agent AI Investment Advisor using Ollama, LangGraph, and Streamlit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/FXPYOq63eWY?si=W7L7eCU1Ad3mOUd3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqor90/build_a_multiagent_ai_investment_advisor_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqor90/build_a_multiagent_ai_investment_advisor_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T12:49:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq6a33</id>
    <title>It‚Äôs finally here. Thanks to the Ollama community, I'm launching Observer AI v1.0 this Friday üöÄ ‚Äì the open-source agent builder you helped shape.</title>
    <updated>2025-07-02T20:22:22+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Ollama community!,&lt;/p&gt; &lt;p&gt;Some of you might remember my earlier posts about a project I was building‚Äîan open-source way to create local AI agents. I've been tinkering, coding, and taking in all your amazing feedback for months. Today, I'm incredibly excited (and a little nervous!) to announce that &lt;strong&gt;Observer AI v1.0 is officially launching this Friday!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For anyone who missed it, &lt;strong&gt;Observer AI üëÅÔ∏è&lt;/strong&gt; is a privacy-first platform for building your own micro-agents that run locally on your machine.&lt;/p&gt; &lt;p&gt;The whole idea started because, like many of you, I was blown away by the power of local models but wanted a simple, powerful way to connect them to my own computer‚Äîto let them see my screen, react to events, and automate tasks without sending my screen data to cloud providers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This Project is a Love Letter to Ollama and This Community&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Observer AI would not exist without Ollama.&lt;/strong&gt; The sheer accessibility and power of what the Ollama team has built was what gave me the vision of this project.&lt;/p&gt; &lt;p&gt;And more importantly, it wouldn't be what it is today without &lt;strong&gt;YOU&lt;/strong&gt;. Every comment, suggestion, and bit of encouragement I've received from this community has directly shaped the features and direction of Observer. You told me what you wanted to see in a local agent platform, and I did my best to build it. So, from the bottom of my heart, &lt;strong&gt;thank you.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Launch This Friday&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The core Observer AI platform is, and will always be, &lt;strong&gt;free and open-source&lt;/strong&gt;. That's non-negotiable.&lt;/p&gt; &lt;p&gt;To help support the project's future development (I'm a solo dev, so server costs and coffee are my main fuel!), I'm also introducing an optional &lt;strong&gt;Observer Pro&lt;/strong&gt; subscription. This will give users unlimited access to the hosted Ob-Server models for those who might not be running a local instance 24/7. It's my way of trying to make the project sustainable long-term.&lt;/p&gt; &lt;p&gt;I'd be incredibly grateful if you'd take a look. Star the repo if you think it's cool, try building an agent, and let me know what you think. I'm building this for you, and your feedback is what will guide v1.1 and beyond.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;App Link:&lt;/strong&gt; &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GitHub (all the code is here!):&lt;/strong&gt; &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Twitter/X:&lt;/strong&gt; &lt;a href="https://x.com/AppObserverAI"&gt;https://x.com/AppObserverAI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord:&lt;/strong&gt; &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll be hanging out here all day to answer any questions. Let's build some cool stuff together!&lt;/p&gt; &lt;p&gt;Cheers,&lt;br /&gt; Roy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lq6a33/its_finally_here_thanks_to_the_ollama_community/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lq6a33/its_finally_here_thanks_to_the_ollama_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lq6a33/its_finally_here_thanks_to_the_ollama_community/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-02T20:22:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqlwli</id>
    <title>Best light llm for ocr summarize chat</title>
    <updated>2025-07-03T10:13:04+00:00</updated>
    <author>
      <name>/u/SuperMindHero</name>
      <uri>https://old.reddit.com/user/SuperMindHero</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I would like to run a local model 32 ram i7 12g. The goal is OCR for small pdf files max 2pages, summarize of text, chat with limited context and rag logic for specialized knowedge&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuperMindHero"&gt; /u/SuperMindHero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqlwli/best_light_llm_for_ocr_summarize_chat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqlwli/best_light_llm_for_ocr_summarize_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqlwli/best_light_llm_for_ocr_summarize_chat/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T10:13:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqqocr</id>
    <title>Ollama Local AI Journaling App.</title>
    <updated>2025-07-03T14:14:51+00:00</updated>
    <author>
      <name>/u/Frosty-Cap-4282</name>
      <uri>https://old.reddit.com/user/Frosty-Cap-4282</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This was born out of a personal need ‚Äî I journal daily , and I didn‚Äôt want to upload my thoughts to some cloud server and also wanted to use AI. So I built Vinaya to be:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Private&lt;/strong&gt;: Everything stays on your device. No servers, no cloud, no trackers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simple&lt;/strong&gt;: Clean UI built with Electron + React. No bloat, just journaling.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Insightful&lt;/strong&gt;: Semantic search, mood tracking, and AI-assisted reflections (all offline).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Link to the app: &lt;a href="https://vinaya-journal.vercel.app/"&gt;https://vinaya-journal.vercel.app/&lt;/a&gt;&lt;br /&gt; Github: &lt;a href="https://github.com/BarsatKhadka/Vinaya-Journal"&gt;https://github.com/BarsatKhadka/Vinaya-Journal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm not trying to build a SaaS or chase growth metrics. I just wanted something I could trust and use daily. If this resonates with anyone else, I‚Äôd love feedback or thoughts.&lt;/p&gt; &lt;p&gt;If you like the idea or find it useful and want to encourage me to consistently refine it but don‚Äôt know me personally and feel shy to say it ‚Äî just drop a ‚≠ê on GitHub. That‚Äôll mean a lot :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Frosty-Cap-4282"&gt; /u/Frosty-Cap-4282 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqqocr/ollama_local_ai_journaling_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqqocr/ollama_local_ai_journaling_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqqocr/ollama_local_ai_journaling_app/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T14:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqpsjh</id>
    <title>Ollama based AI presentation generator and API - Gamma Alternative</title>
    <updated>2025-07-03T13:36:35+00:00</updated>
    <author>
      <name>/u/goodboydhrn</name>
      <uri>https://old.reddit.com/user/goodboydhrn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lqpsjh/ollama_based_ai_presentation_generator_and_api/"&gt; &lt;img alt="Ollama based AI presentation generator and API - Gamma Alternative" src="https://preview.redd.it/awcrxuqjwnaf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=aa54d3b167b836a81007137b327da2d5800fd272" title="Ollama based AI presentation generator and API - Gamma Alternative" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Me and my roommates are building Presenton, which is an AI presentation generator that can run entirely on your own device. It has Ollama built in so, all you need is add Pexels (free image provider) API Key and start generating high quality presentations which can be exported to PPTX and PDF. It even works on CPU(can generate professional presentation with as small as 3b models)!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Presentation Generation UI&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It has beautiful user-interface which can be used to create presentations.&lt;/li&gt; &lt;li&gt;7+ beautiful themes to choose from.&lt;/li&gt; &lt;li&gt;Can choose number of slides, languages and themes.&lt;/li&gt; &lt;li&gt;Can create presentation from PDF, PPTX, DOCX, etc files directly.&lt;/li&gt; &lt;li&gt;Export to PPTX, PDF.&lt;/li&gt; &lt;li&gt;Share presentation link.(if you host on public IP)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Presentation Generation over API&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can even host the instance to generation presentation over API. (1 endpoint for all above features)&lt;/li&gt; &lt;li&gt;All above features supported over API&lt;/li&gt; &lt;li&gt;You'll get two links; first the static presentation file (pptx/pdf) which you requested and editable link through which you can edit the presentation and export the file.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love for you to try it out! Very easy docker based setup and deployment.&lt;/p&gt; &lt;p&gt;Here's the github link: &lt;a href="https://github.com/presenton/presenton"&gt;https://github.com/presenton/presenton&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Also check out the docs here: &lt;a href="https://docs.presenton.ai/"&gt;https://docs.presenton.ai&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Feedbacks are very appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goodboydhrn"&gt; /u/goodboydhrn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/awcrxuqjwnaf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lqpsjh/ollama_based_ai_presentation_generator_and_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lqpsjh/ollama_based_ai_presentation_generator_and_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-03T13:36:35+00:00</published>
  </entry>
</feed>
