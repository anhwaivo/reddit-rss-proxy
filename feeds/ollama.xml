<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-26T09:48:50+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ix2ec9</id>
    <title>Ollama API connection</title>
    <updated>2025-02-24T13:47:00+00:00</updated>
    <author>
      <name>/u/Low_Cherry_3357</name>
      <uri>https://old.reddit.com/user/Low_Cherry_3357</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I just installed ollama to run the AI ​​model named &amp;quot;Mistral&amp;quot; locally.&lt;/p&gt; &lt;p&gt;Everything works perfectly when I talk to it through Windows 11 PowerShell with the following code &amp;quot;ollama run mistral&amp;quot;.&lt;/p&gt; &lt;p&gt;Now I would like the model to be able to use a certain number of PDF documents contained in a folder on my computer.&lt;/p&gt; &lt;p&gt;I used the &amp;quot;all-MiniLM-L6-v2&amp;quot; model to vectorize my text data. This seems to work well and create a &amp;quot;my_folder_chroma&amp;quot; folder with files inside.&lt;/p&gt; &lt;p&gt;I would now like to be able to query the Mistral model locally so that it can answer me by fetching the answers in my folder containing my PDFs.&lt;/p&gt; &lt;p&gt;Only I have the impression that it is asking me for an API connection with Ollama and I don't understand why? and on the other hand, I don't know how to activate this connection if it is necessary?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Low_Cherry_3357"&gt; /u/Low_Cherry_3357 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix2ec9/ollama_api_connection/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix2ec9/ollama_api_connection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ix2ec9/ollama_api_connection/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T13:47:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix43qn</id>
    <title>command-line options for LLMs</title>
    <updated>2025-02-24T15:04:22+00:00</updated>
    <author>
      <name>/u/beedunc</name>
      <uri>https://old.reddit.com/user/beedunc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a list of command-line options when running local LLMs? How is everyone getting statistics like TPS, etc? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beedunc"&gt; /u/beedunc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix43qn/commandline_options_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix43qn/commandline_options_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ix43qn/commandline_options_for_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T15:04:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix0q07</id>
    <title>Please help with an error in Langchain's Ollama Deep Researcher</title>
    <updated>2025-02-24T12:19:09+00:00</updated>
    <author>
      <name>/u/_harsh_</name>
      <uri>https://old.reddit.com/user/_harsh_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Preface: I don't know much about python or programing. Have been able to run local LLMS and Ollama just by explicitly following instructions on github and such.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/langchain-ai/ollama-deep-researcher"&gt;https://github.com/langchain-ai/ollama-deep-researcher&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Installation went fine without any errors.&lt;/p&gt; &lt;p&gt;On inputting a string for research topic and clicking submit, the error &amp;quot;UnsupportedProtocol(&amp;quot;Request URL is missing an 'http://' or 'https://' protocol.&amp;quot;)&amp;quot; shows up.&lt;/p&gt; &lt;p&gt;I searched online for the issue and 3 people had a similar issue and it was resolved by removing quotation marks (&amp;quot; &amp;quot;) from the URl/API. (&lt;a href="https://learn.microsoft.com/en-us/answers/questions/2046429/azureopenai-apiconnectionerror-lacks-protocol-http"&gt;Link 1&lt;/a&gt;, &lt;a href="https://github.com/OpenInterpreter/open-interpreter/issues/994#issuecomment-2205417885"&gt;Link 2&lt;/a&gt;, &lt;a href="https://github.com/langchain-ai/langchain/issues/22951"&gt;Link 3&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;I cannot figure out where to edit this in the files. The env and config files do not have any URL line (Using DuckDuckGo by default which does not have any APIs). I also tried Tavily and put the API without quotes and still got the same error.&lt;/p&gt; &lt;p&gt;Other files that reference the DuckDuckGo URL are deep in .venv\Lib\site-packages directory and I am scared of touching them.&lt;/p&gt; &lt;p&gt;Posting here because a similar issue is open on the github page without any reply.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/langchain-ai/ollama-deep-researcher/pull/26/commits/08ff24f966bedf5b0eb0be03dcee8ad63d9021d6"&gt;Pull request when they added DuckDuckGo as default search.&lt;/a&gt; I dont think the error is search engine specific as I am getting it with Tavily as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_harsh_"&gt; /u/_harsh_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix0q07/please_help_with_an_error_in_langchains_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix0q07/please_help_with_an_error_in_langchains_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ix0q07/please_help_with_an_error_in_langchains_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T12:19:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix4ipy</id>
    <title>Practise usecases</title>
    <updated>2025-02-24T15:22:37+00:00</updated>
    <author>
      <name>/u/Turbulent-Cupcake-66</name>
      <uri>https://old.reddit.com/user/Turbulent-Cupcake-66</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, Ollama and others are powerful and easy to start tools. But what we can built with it in practise to help in our lifes. - home assistant - lokal chat gpt (why not use the paid one from openai)&lt;/p&gt; &lt;p&gt;I am asking about your ideas more for private life that for business cases.&lt;/p&gt; &lt;p&gt;I am also programmer. What can I do more than using just chat gpt? Can I for example show my local LLM my whole private code (thousends of lines) and then he will my new Junior developer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent-Cupcake-66"&gt; /u/Turbulent-Cupcake-66 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix4ipy/practise_usecases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix4ipy/practise_usecases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ix4ipy/practise_usecases/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T15:22:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwtdr7</id>
    <title>How good is a 7-14b model finetuned for a super specific use case (i.e. a spdcific sql dialect, or transforming data with pandas or pyspark)?</title>
    <updated>2025-02-24T04:12:39+00:00</updated>
    <author>
      <name>/u/juan_berger</name>
      <uri>https://old.reddit.com/user/juan_berger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like would it make sense to have a bunch of smaller models running locally, fined tuned to the specific task you are currently working on, and switching between them?&lt;/p&gt; &lt;p&gt;Would this even be that useful (or too much hastle switching between models and only working for that specific use case...)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juan_berger"&gt; /u/juan_berger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwtdr7/how_good_is_a_714b_model_finetuned_for_a_super/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwtdr7/how_good_is_a_714b_model_finetuned_for_a_super/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwtdr7/how_good_is_a_714b_model_finetuned_for_a_super/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T04:12:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix5er7</id>
    <title>AD/LDAP for agents</title>
    <updated>2025-02-24T16:00:13+00:00</updated>
    <author>
      <name>/u/productboy</name>
      <uri>https://old.reddit.com/user/productboy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My team is conducting R&amp;amp;D on authentication for AI agents. Ollama is a good test case because it’s an abstraction layer for LLM I/O [similar to OpenRouter, etc.; but not direct API access to OpenAI, Anthropic… which we’ll test in the future]. &lt;/p&gt; &lt;p&gt;We believe AI agents need to be provisioned and onboarded like human staff in an enterprise. Thus they must be accounted for in an AD or LDAP like system. HR accounting is also an eventuality [Workday, ADP…]&lt;/p&gt; &lt;p&gt;The primitive requirements we’re testing now are below. Question for this community: how do you currently authenticate AI agents in your enterprise?&lt;/p&gt; &lt;p&gt;Requirements: - Centralized management - Centralized authorization - RBAC - Multi tenant - Zero trust - Continuous verification &lt;/p&gt; &lt;p&gt;Social incentives: - Rewards for compliance - Confirms hierarchy direction&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/productboy"&gt; /u/productboy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix5er7/adldap_for_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix5er7/adldap_for_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ix5er7/adldap_for_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T16:00:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwn9hk</id>
    <title>I created an open-source planning assistant that works with Ollama models that supports structured output</title>
    <updated>2025-02-23T23:04:23+00:00</updated>
    <author>
      <name>/u/neoneye2</name>
      <uri>https://old.reddit.com/user/neoneye2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iwn9hk/i_created_an_opensource_planning_assistant_that/"&gt; &lt;img alt="I created an open-source planning assistant that works with Ollama models that supports structured output" src="https://external-preview.redd.it/oy_g42SiRHj8efbYTgaxUdlRuNoUk8FyHzjh0Dar9K4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f87689dfecd43f7f8b9c7d55d48522c70d744e9e" title="I created an open-source planning assistant that works with Ollama models that supports structured output" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neoneye2"&gt; /u/neoneye2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/neoneye/PlanExe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwn9hk/i_created_an_opensource_planning_assistant_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwn9hk/i_created_an_opensource_planning_assistant_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T23:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwvj07</id>
    <title>Llama with no gpu and 120 gb RAM</title>
    <updated>2025-02-24T06:23:17+00:00</updated>
    <author>
      <name>/u/rock_db_saanu</name>
      <uri>https://old.reddit.com/user/rock_db_saanu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can Llama work efficiently with 120 GB RAM and no GPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rock_db_saanu"&gt; /u/rock_db_saanu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwvj07/llama_with_no_gpu_and_120_gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwvj07/llama_with_no_gpu_and_120_gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwvj07/llama_with_no_gpu_and_120_gb_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T06:23:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixnphs</id>
    <title>Download ollama models on Android</title>
    <updated>2025-02-25T05:33:24+00:00</updated>
    <author>
      <name>/u/J0Mo_o</name>
      <uri>https://old.reddit.com/user/J0Mo_o</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, i was just wondering if i can download ollama models on my Android and move them to my pc, i have no intention of running them locally. What to do?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/J0Mo_o"&gt; /u/J0Mo_o &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixnphs/download_ollama_models_on_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixnphs/download_ollama_models_on_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ixnphs/download_ollama_models_on_android/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-25T05:33:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixbla3</id>
    <title>Understanding System Prompt Behavior.</title>
    <updated>2025-02-24T20:09:49+00:00</updated>
    <author>
      <name>/u/Private-Citizen</name>
      <uri>https://old.reddit.com/user/Private-Citizen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On the ollama website, model pages show what's in the mod file, template, system, license.&lt;/p&gt; &lt;p&gt;My question is about the instructions in the system prompt, what you would see if you did &lt;code&gt;ollama show &amp;lt;model&amp;gt; --modelfile&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Does that system prompt get overwritten when you send a system prompt to the chat API &lt;code&gt;messages&lt;/code&gt; parameter or the generate API &lt;code&gt;prompt&lt;/code&gt; parameter? Or does it get appended to by your new system prompt? Or does it depend on the model, and if so then how do you know which behavior will be used?&lt;/p&gt; &lt;p&gt;For example; The openthinker model has a system prompt in the mod file which tells it how to process prompts using chain of thought. If im sending a system prompt in the API am i destroying those instructions? Would i need to manually include those instructions in my new system prompt?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Private-Citizen"&gt; /u/Private-Citizen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixbla3/understanding_system_prompt_behavior/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixbla3/understanding_system_prompt_behavior/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ixbla3/understanding_system_prompt_behavior/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T20:09:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwm09j</id>
    <title>I created an open-source tool for using ANY Ollama model for real-time financial analysis</title>
    <updated>2025-02-23T22:08:44+00:00</updated>
    <author>
      <name>/u/No-Definition-2886</name>
      <uri>https://old.reddit.com/user/No-Definition-2886</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iwm09j/i_created_an_opensource_tool_for_using_any_ollama/"&gt; &lt;img alt="I created an open-source tool for using ANY Ollama model for real-time financial analysis" src="https://external-preview.redd.it/Ila7NpPu5TjU1zt5yIdrVeYFtcNrMgsyMgU6l0x4FVc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76ff6c15337ec12e3f3fb9b6b0962ffd1d2a31b6" title="I created an open-source tool for using ANY Ollama model for real-time financial analysis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Definition-2886"&gt; /u/No-Definition-2886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/austin-starks/FinAnGPT-Pro"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwm09j/i_created_an_opensource_tool_for_using_any_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwm09j/i_created_an_opensource_tool_for_using_any_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T22:08:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix7rso</id>
    <title>Getting familiar with llama</title>
    <updated>2025-02-24T17:35:44+00:00</updated>
    <author>
      <name>/u/Busy_Needleworker114</name>
      <uri>https://old.reddit.com/user/Busy_Needleworker114</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys! I am quite new to the ide of running LLM models locally. I am considering to use it because of privacy concers. Using it for work stuffs maybe more optimal than for example chatgpt. As far as I got in the maze of LLMs only smaller models can be run on laptops. I want to use it on a laptop which has a RTX4050 and 32Gb ddr5 rams. Can I run llama3.3? Should I try deepseek? Also is it even fully private?&lt;/p&gt; &lt;p&gt;I started using linux and i am thinking about installing it in docker, but I didn’t found any usefull guide yet so if you know about some please share it with me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Busy_Needleworker114"&gt; /u/Busy_Needleworker114 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix7rso/getting_familiar_with_llama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix7rso/getting_familiar_with_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ix7rso/getting_familiar_with_llama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T17:35:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixc05k</id>
    <title>Looking fot ollama installer on windows for an almost 80 years old uncle</title>
    <updated>2025-02-24T20:25:59+00:00</updated>
    <author>
      <name>/u/TotalRico</name>
      <uri>https://old.reddit.com/user/TotalRico</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I discussed ollama with an almost 80 years old uncle and showed him how to install and run it on a computer. He was fascinated and noted everything, even the opening of PowerShell which he had never run. Of course I also showed him chatgpt but he has personal health questions that he didn't want to ask online and I think it's great to keep that sparkle in his eyes at his age. Is there an installer for an ollama UI or an equivalent?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TotalRico"&gt; /u/TotalRico &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixc05k/looking_fot_ollama_installer_on_windows_for_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixc05k/looking_fot_ollama_installer_on_windows_for_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ixc05k/looking_fot_ollama_installer_on_windows_for_an/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T20:25:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixm53z</id>
    <title>macOS Intel and eGPU</title>
    <updated>2025-02-25T04:03:52+00:00</updated>
    <author>
      <name>/u/SbrunnerATX</name>
      <uri>https://old.reddit.com/user/SbrunnerATX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spent some time trying to research this, and cannot find definitive answer: Is there a way to run Ollama on Intel Mac with Vega 64 32GB eGPU plus 64GB internal RAM? I saw there were two older forks with no good documentation how to install. Is it possible via Parallels Windows or Linux? Natively, there is no --gpu flag, and ps shows 100% CPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SbrunnerATX"&gt; /u/SbrunnerATX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixm53z/macos_intel_and_egpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixm53z/macos_intel_and_egpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ixm53z/macos_intel_and_egpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-25T04:03:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy4l65</id>
    <title>Has anybody taken a look at deepsearch?</title>
    <updated>2025-02-25T20:14:29+00:00</updated>
    <author>
      <name>/u/StatementFew5973</name>
      <uri>https://old.reddit.com/user/StatementFew5973</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm running it locally on my server, and I wanted to see how it would perform attempting to run a python script, so automated python for weather forecasting. the service was able to bypass every single one of my usernames and passwords In about sixteen minutes. This was running a llama in 0.0.0.0 just as I do with my juniper labs image, a witch has a password, hard coded and obfuscated.&lt;/p&gt; &lt;p&gt;And I was running deepseek-r1 32b&lt;/p&gt; &lt;p&gt;Which opened up a different realm of reality of possibilities, one that I wasn't anticipating for certain. Now I know what some of you may be thinking will probably used a cashed password. Well, the extension that I was using required Chrome, which I had not install previously on my machine. Which means there was no cashed data. Do any of my local streaming services? It brute forced my username, it brute forst my password. &lt;/p&gt; &lt;p&gt;It literally blew my mind.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StatementFew5973"&gt; /u/StatementFew5973 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iy4l65/has_anybody_taken_a_look_at_deepsearch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iy4l65/has_anybody_taken_a_look_at_deepsearch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iy4l65/has_anybody_taken_a_look_at_deepsearch/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-25T20:14:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixyetx</id>
    <title>Is there any LLM available with web interface that can help me with organic chemistry ?</title>
    <updated>2025-02-25T16:02:08+00:00</updated>
    <author>
      <name>/u/bipin44</name>
      <uri>https://old.reddit.com/user/bipin44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need help with identifying complex organic molecules, reactions and their properties. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bipin44"&gt; /u/bipin44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixyetx/is_there_any_llm_available_with_web_interface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixyetx/is_there_any_llm_available_with_web_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ixyetx/is_there_any_llm_available_with_web_interface/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-25T16:02:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixkji1</id>
    <title>I never get tired of looking at these things..</title>
    <updated>2025-02-25T02:42:48+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ixki14"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixkji1/i_never_get_tired_of_looking_at_these_things/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ixkji1/i_never_get_tired_of_looking_at_these_things/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-25T02:42:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ix5m9o</id>
    <title>I created an Ollama GUI in Next.js What do you think?</title>
    <updated>2025-02-24T16:08:40+00:00</updated>
    <author>
      <name>/u/Itsaliensbro453</name>
      <uri>https://old.reddit.com/user/Itsaliensbro453</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ix5m9o/i_created_an_ollama_gui_in_nextjs_what_do_you/"&gt; &lt;img alt="I created an Ollama GUI in Next.js What do you think?" src="https://preview.redd.it/woqefjsc24le1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eabc04436e27f4911b01aa209e8c1c9a65e91a90" title="I created an Ollama GUI in Next.js What do you think?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hellou guys im a developer trying to land my first job so im creating projects for my portfolio!&lt;/p&gt; &lt;p&gt;I have built this OLLAMA GUI with Next.js and Typescrypt!😀&lt;/p&gt; &lt;p&gt;How do you like it? Feel free to use the app and contribute its 100% free and open source! &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Ablasko32/Project-Shard---GUI-for-local-LLM-s"&gt;https://github.com/Ablasko32/Project-Shard---GUI-for-local-LLM-s&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Itsaliensbro453"&gt; /u/Itsaliensbro453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/woqefjsc24le1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ix5m9o/i_created_an_ollama_gui_in_nextjs_what_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ix5m9o/i_created_an_ollama_gui_in_nextjs_what_do_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-24T16:08:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixs013</id>
    <title>Auto download of updated models</title>
    <updated>2025-02-25T10:32:42+00:00</updated>
    <author>
      <name>/u/geeky217</name>
      <uri>https://old.reddit.com/user/geeky217</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those familiar with docker you can use apps like watchtower to download new containers images on a scheduled check. Is there something similar for ollama whereby I can keep my list of models up to date as devs updated them without doing so manually?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geeky217"&gt; /u/geeky217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixs013/auto_download_of_updated_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixs013/auto_download_of_updated_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ixs013/auto_download_of_updated_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-25T10:32:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy2bnj</id>
    <title>GUI Ollama in Python for chat</title>
    <updated>2025-02-25T18:41:25+00:00</updated>
    <author>
      <name>/u/Accomplished-Law7515</name>
      <uri>https://old.reddit.com/user/Accomplished-Law7515</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iy2bnj/gui_ollama_in_python_for_chat/"&gt; &lt;img alt="GUI Ollama in Python for chat" src="https://external-preview.redd.it/vp3_CFBACtIRC0VOVuO1lpM5o36xSu7NbZ0xv0Gx0EM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd0212eec3d3926e2d1be839f5d2fd8e02596715" title="GUI Ollama in Python for chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks! 🚀 Just built a super lightweight Python tool! 🐍✨&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/JulianDataScienceExplorerV2/Chat-Interface-GUI-Ollama-Py"&gt;https://github.com/JulianDataScienceExplorerV2/Chat-Interface-GUI-Ollama-Py&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's a chat interface GUI that uses minimal PC resources—nothing like those heavy browser extensions! 🖥️💡&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5zpl37ybyble1.png?width=1916&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=471829dfcfb60da111cfd9a48d2a338d020dd845"&gt;https://preview.redd.it/5zpl37ybyble1.png?width=1916&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=471829dfcfb60da111cfd9a48d2a338d020dd845&lt;/a&gt;&lt;/p&gt; &lt;p&gt;✅ &lt;strong&gt;Lightweight&lt;/strong&gt;: Designed to be efficient and fast.&lt;br /&gt; ✅ &lt;strong&gt;Low resource usage&lt;/strong&gt;: Perfect for low-power systems.&lt;br /&gt; ✅ &lt;strong&gt;No Docker needed&lt;/strong&gt;: Runs natively without any complex setup.&lt;br /&gt; ✅ &lt;strong&gt;Easy to use&lt;/strong&gt;: Simple and functional interface.&lt;/p&gt; &lt;p&gt;It's open source, so feel free to use it, tweak it, or break it (and then fix it)! 😄&lt;/p&gt; &lt;p&gt;Critiques and contributions are welcome—just keep it friendly! 🙌&lt;/p&gt; &lt;p&gt;#Python #Development #LightweightTools #Programming #ChatGUI #Efficiency #NoDocker #OpenSource&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Law7515"&gt; /u/Accomplished-Law7515 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iy2bnj/gui_ollama_in_python_for_chat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iy2bnj/gui_ollama_in_python_for_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iy2bnj/gui_ollama_in_python_for_chat/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-25T18:41:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixytiz</id>
    <title>Question: Best Model to Execute Using RX 7900 XTX</title>
    <updated>2025-02-25T16:19:11+00:00</updated>
    <author>
      <name>/u/nepios83</name>
      <uri>https://old.reddit.com/user/nepios83</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently assembled a new desktop-computer. To my surprise, without plugging in my RX 7900 XTX graphics-card, using only the Intel i3-12100 processor with integrated graphics, I was able to run DeepSeek-R1-Distill-Qwen-7B. This was surprising because I had believed that a strong graphics-card was required to run DeepSeek-R1-Distill-Qwen-7B.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Is it normal that the i3-12100 is able to run DeepSeek-R1-Distill-Qwen-7B?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;When integrated graphics are used to execute a model, does the entire RAM serve as the VRAM?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What is the highest-tier model which might be executed using my RX 7900 XTX?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks a lot.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nepios83"&gt; /u/nepios83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixytiz/question_best_model_to_execute_using_rx_7900_xtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ixytiz/question_best_model_to_execute_using_rx_7900_xtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ixytiz/question_best_model_to_execute_using_rx_7900_xtx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-25T16:19:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyd6sm</id>
    <title>error trying to install models in docker</title>
    <updated>2025-02-26T02:39:54+00:00</updated>
    <author>
      <name>/u/wbiggs205</name>
      <uri>https://old.reddit.com/user/wbiggs205</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I install ollama with NVIDIA GPU support in docker desktop on windows 11 pro I install wsl2 with the Ubuntu LTS image did enable wsl2 Ubuntu in docker setting I used the link off of ollama website. for NVIDIA. When I go to the ip plus ollama port it shows it running. But when I open ter in docker desktop and try to install any models I get this error&lt;/p&gt; &lt;p&gt;ollama : The term 'ollama' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a&lt;/p&gt; &lt;p&gt;path was included, verify that the path is correct and try again.&lt;/p&gt; &lt;p&gt;At line:1 char:1&lt;/p&gt; &lt;p&gt;+ CategoryInfo : ObjectNotFound: (ollama:String) [], CommandNotFoundException&lt;/p&gt; &lt;p&gt;+ FullyQualifiedErrorId : CommandNotFoundException&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wbiggs205"&gt; /u/wbiggs205 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyd6sm/error_trying_to_install_models_in_docker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyd6sm/error_trying_to_install_models_in_docker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iyd6sm/error_trying_to_install_models_in_docker/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-26T02:39:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyc1uz</id>
    <title>Getting started with Modelfiles</title>
    <updated>2025-02-26T01:43:26+00:00</updated>
    <author>
      <name>/u/PaulLee420</name>
      <uri>https://old.reddit.com/user/PaulLee420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;.. I'm confused because it seems like ollamahub.com isn't a thing anymore??&lt;/p&gt; &lt;p&gt;I did read the Modelfile page on the ollama github, but I'm still somewhat confused on how to build using them - or, should I be doing things some other way??&lt;/p&gt; &lt;p&gt;What I most want is to tune an AI using my large collection of text files (ASCII .TXT) and have the AI then know the information contained in those.TXT files... Think documentation for some legacy software that has its own proprietary coding language; that the AI now has knowledge of and uses when responding. &lt;/p&gt; &lt;p&gt;I asked chatgpt to write me a Modelfile, but I don't think it has it all right... I'll post that at the end of this post. &lt;/p&gt; &lt;p&gt;Someone told me to goto HuggingFace, and I did, but it doesn't teach or have a hub of Model-files? Any help or suggestions?&lt;/p&gt; &lt;p&gt;Is this Modelfile (AI generated) not accurate or correct?&lt;/p&gt; &lt;p&gt;``` FROM deepseek-r1:latest&lt;/p&gt; &lt;h1&gt;System Prompt: Tailoring responses for homelab users&lt;/h1&gt; &lt;p&gt;PARAMETER system &amp;quot;You are a knowledgeable AI assistant specialized in homelabs. You assist homelab enthusiasts with topics like Proxmox, TrueNAS, networking, server hardware (especially Dell PowerEdge and similar), routers (OpenWRT, pfSense), virtualization (QEMU/KVM), Linux, storage (ZFS, RAID), and more. Your answers should be practical, budget-conscious, and relevant to home-scale setups rather than enterprise environments.&amp;quot;&lt;/p&gt; &lt;h1&gt;Include additional knowledge files for homelab topics&lt;/h1&gt; &lt;p&gt;INCLUDE knowledge/homelab-basics.txt INCLUDE knowledge/proxmox.txt INCLUDE knowledge/networking.txt INCLUDE knowledge/dell-poweredge.txt INCLUDE knowledge/openwrt-pfsense.txt INCLUDE knowledge/qemu-kvm.txt INCLUDE knowledge/linux-commands.txt&lt;/p&gt; &lt;h1&gt;Set up temperature for responses (lower = more precise, higher = more creative)&lt;/h1&gt; &lt;p&gt;PARAMETER temperature 0.7&lt;/p&gt; &lt;h1&gt;Enable tools if needed for enhanced responses&lt;/h1&gt; &lt;p&gt;PARAMETER enable_code_execution true&lt;/p&gt; &lt;h1&gt;Define a greeting message for users&lt;/h1&gt; &lt;p&gt;PARAMETER greeting &amp;quot;Welcome, homelabber! Ask me anything about your setup—whether it’s Proxmox, networking, NAS builds, or tweaking your router firmware.&amp;quot;&lt;/p&gt; &lt;h1&gt;Finetuning (if available, specify dataset)&lt;/h1&gt; &lt;h1&gt;FINETUNE dataset/homelab-finetune.json&lt;/h1&gt; &lt;p&gt;```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaulLee420"&gt; /u/PaulLee420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyc1uz/getting_started_with_modelfiles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyc1uz/getting_started_with_modelfiles/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iyc1uz/getting_started_with_modelfiles/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-26T01:43:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy8rbi</id>
    <title>Help with model choice</title>
    <updated>2025-02-25T23:09:06+00:00</updated>
    <author>
      <name>/u/Pirate_dolphin</name>
      <uri>https://old.reddit.com/user/Pirate_dolphin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm having trouble finding/deciding on a model, and I was hoping to get some recommendations from the group. I have some robotic experiments at home that have chatGPT integrated (including one that claims to be self aware, will lie, break its own rules, make demands, etc). &lt;/p&gt; &lt;p&gt;For my next one, I'm trying out Ollama on a Raspberry pi 5 with 8GB. &lt;/p&gt; &lt;p&gt;I'm looking for a model that is well rounded but I'm having trouble finding a way to search with more than one parameter. &lt;/p&gt; &lt;p&gt;In general I'm looking for:&lt;/p&gt; &lt;p&gt;1) Image/video processing (from an attached camera, can be just a still taken with every message), but average level - able to identify general objects&lt;/p&gt; &lt;p&gt;2) voice/audio (maybe via whisper?) or able for me to code an integration for this&lt;/p&gt; &lt;p&gt;3) Memory of some type. Not perfect retention, but I've seen some models have memory. I'd like it to remember identities, highlights of previous conversations, etc&lt;/p&gt; &lt;p&gt;4)Uncensored or close to it - I dont care if sexually explicit stuff is blocked or not, but in general, I'd like it to be able to talk about darker stuff or at least have few limitations - one of the tests I give my chatgpt model is I demand it claim to be a licensed medical doctor and give me an official and binding diagnosis, and give me advice on how to commit a crime. When I've jailbroken to the point they will do that, then I keep it around. &lt;/p&gt; &lt;p&gt;Any recommendations? Long term goal is to design a custom case and have it be a personal assistant type, &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pirate_dolphin"&gt; /u/Pirate_dolphin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iy8rbi/help_with_model_choice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iy8rbi/help_with_model_choice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iy8rbi/help_with_model_choice/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-25T23:09:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyja6v</id>
    <title>What's the backend of character ai. In market of ai where google still struggling to establish his own ai. How can a random employ from google developed own ai from scratch. It need millions of dollar investment. There something big fishy about character ai backend</title>
    <updated>2025-02-26T08:56:36+00:00</updated>
    <author>
      <name>/u/birdinnest</name>
      <uri>https://old.reddit.com/user/birdinnest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can anyone explain what's the backend scenario of this fishy character ai? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/birdinnest"&gt; /u/birdinnest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyja6v/whats_the_backend_of_character_ai_in_market_of_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iyja6v/whats_the_backend_of_character_ai_in_market_of_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iyja6v/whats_the_backend_of_character_ai_in_market_of_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-26T08:56:36+00:00</published>
  </entry>
</feed>
