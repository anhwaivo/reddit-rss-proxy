<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-23T17:34:51+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1i7cdyj</id>
    <title>Why is ollama only using 4535MiB/6144MiB of VRAM? Shouldnt it first fill the 6GB VRAM then move towards the system RAM?</title>
    <updated>2025-01-22T14:31:54+00:00</updated>
    <author>
      <name>/u/R46H4V</name>
      <uri>https://old.reddit.com/user/R46H4V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7cdyj/why_is_ollama_only_using_4535mib6144mib_of_vram/"&gt; &lt;img alt="Why is ollama only using 4535MiB/6144MiB of VRAM? Shouldnt it first fill the 6GB VRAM then move towards the system RAM?" src="https://preview.redd.it/671wdnno2kee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ee8f91525de17d621b557549507e510a41806a7" title="Why is ollama only using 4535MiB/6144MiB of VRAM? Shouldnt it first fill the 6GB VRAM then move towards the system RAM?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R46H4V"&gt; /u/R46H4V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/671wdnno2kee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7cdyj/why_is_ollama_only_using_4535mib6144mib_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7cdyj/why_is_ollama_only_using_4535mib6144mib_of_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T14:31:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7bjgg</id>
    <title>First time trying out Ollama and Deepseek. Can you use a GUI and upload images?</title>
    <updated>2025-01-22T13:51:26+00:00</updated>
    <author>
      <name>/u/Draufgaenger</name>
      <uri>https://old.reddit.com/user/Draufgaenger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So far I'm using the command line interface. Is that what you all are using too?&lt;br /&gt; I saw there are some Web-interfaces that let you use Ollama and some of them even seem to have the ability for you to upload images just like you would in ChatGPT?&lt;/p&gt; &lt;p&gt;In the commandline when I enter /? I see no option to upload an image though..so I am not sure. &lt;/p&gt; &lt;p&gt;Also I'd feel uneasy just using any random GUI for it..&lt;/p&gt; &lt;p&gt;Can you help me understand that whole thing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Draufgaenger"&gt; /u/Draufgaenger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7bjgg/first_time_trying_out_ollama_and_deepseek_can_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7bjgg/first_time_trying_out_ollama_and_deepseek_can_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7bjgg/first_time_trying_out_ollama_and_deepseek_can_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T13:51:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7d3uc</id>
    <title>I wrote an open source tool to test prompt injection attacks</title>
    <updated>2025-01-22T15:04:58+00:00</updated>
    <author>
      <name>/u/utku1337</name>
      <uri>https://old.reddit.com/user/utku1337</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two years ago, I released &amp;quot;promptmap&amp;quot;, the first tool that automatically tests prompt injection attacks in GPT applications. But since then, open source models became popular and new prompt injection techniques are discovered. This led me to completely rewrite the tool. Now it supports any open-source model thanks to Ollama.&lt;/p&gt; &lt;p&gt;Feed it with system prompts of your LLM application and let it perform automated prompt injection attacks. It will reveal potential vulnerabilities and determine if attackers could extract your system prompts.&lt;/p&gt; &lt;p&gt;Any feedback is welcome: &lt;a href="https://github.com/utkusen/promptmap"&gt;https://github.com/utkusen/promptmap&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/utku1337"&gt; /u/utku1337 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7d3uc/i_wrote_an_open_source_tool_to_test_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7d3uc/i_wrote_an_open_source_tool_to_test_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7d3uc/i_wrote_an_open_source_tool_to_test_prompt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T15:04:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i80oxr</id>
    <title>Which version of deepseek-r1 for coding on my M3 Pro MacBook?</title>
    <updated>2025-01-23T10:46:58+00:00</updated>
    <author>
      <name>/u/joyfulsparrow</name>
      <uri>https://old.reddit.com/user/joyfulsparrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've got a MacBook with these specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; M3 Pro&lt;/li&gt; &lt;li&gt; Memory: 36GB&lt;/li&gt; &lt;li&gt; macOS 15.2&lt;/li&gt; &lt;li&gt; Ollama&lt;/li&gt; &lt;li&gt; Ollamac app&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What's the best version of deepseek-rc1 for coding that I could use on the MacBook? &lt;a href="https://ollama.com/library/deepseek-r1"&gt;https://ollama.com/library/deepseek-r1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joyfulsparrow"&gt; /u/joyfulsparrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i80oxr/which_version_of_deepseekr1_for_coding_on_my_m3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i80oxr/which_version_of_deepseekr1_for_coding_on_my_m3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i80oxr/which_version_of_deepseekr1_for_coding_on_my_m3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T10:46:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7gwbl</id>
    <title>Changed the model to Deepseek for my SEO Outliner (and I'm mesmerized with the thinking)</title>
    <updated>2025-01-22T17:41:51+00:00</updated>
    <author>
      <name>/u/LilFingaz</name>
      <uri>https://old.reddit.com/user/LilFingaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7gwbl/changed_the_model_to_deepseek_for_my_seo_outliner/"&gt; &lt;img alt="Changed the model to Deepseek for my SEO Outliner (and I'm mesmerized with the thinking)" src="https://external-preview.redd.it/bjR0eXhidXMwbGVlMU6N6TQf3Qi5Z6usbSq_x6Ry7p0qzaDdmhrtZz_2nu3Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60fc529f0a776c8041e5c0c83815472262c4e1f8" title="Changed the model to Deepseek for my SEO Outliner (and I'm mesmerized with the thinking)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LilFingaz"&gt; /u/LilFingaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3cn6tjus0lee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7gwbl/changed_the_model_to_deepseek_for_my_seo_outliner/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7gwbl/changed_the_model_to_deepseek_for_my_seo_outliner/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T17:41:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7uhof</id>
    <title>List of string model names - ollama/langchain python</title>
    <updated>2025-01-23T03:39:54+00:00</updated>
    <author>
      <name>/u/Oceanboi</name>
      <uri>https://old.reddit.com/user/Oceanboi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any way to grab all model names that are available for ollama pull? I'd like to develop a module for my streamlit app that allows you to browse model names/cards without having to open your browser as well for quicker iteration and add a model manager into the UI. Would rather not web scrape because it's annoying.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Oceanboi"&gt; /u/Oceanboi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7uhof/list_of_string_model_names_ollamalangchain_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7uhof/list_of_string_model_names_ollamalangchain_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7uhof/list_of_string_model_names_ollamalangchain_python/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T03:39:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7zco5</id>
    <title>This is how I use reasoning models (Deepseek R1) in agents &amp; LLM apps</title>
    <updated>2025-01-23T09:04:27+00:00</updated>
    <author>
      <name>/u/jasonzhou1993</name>
      <uri>https://old.reddit.com/user/jasonzhou1993</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7zco5/this_is_how_i_use_reasoning_models_deepseek_r1_in/"&gt; &lt;img alt="This is how I use reasoning models (Deepseek R1) in agents &amp;amp; LLM apps" src="https://external-preview.redd.it/bGtwbTN5MGFscGVlMfJSOzJ9yre6sC7e1ecrhDdOVYLiJ3GWclt88RbKO1mA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a2e4b5f197082b18fa833d6c6526687059c06cc" title="This is how I use reasoning models (Deepseek R1) in agents &amp;amp; LLM apps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jasonzhou1993"&gt; /u/jasonzhou1993 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k861mx0alpee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7zco5/this_is_how_i_use_reasoning_models_deepseek_r1_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7zco5/this_is_how_i_use_reasoning_models_deepseek_r1_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T09:04:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7zq3s</id>
    <title>My good people...</title>
    <updated>2025-01-23T09:34:51+00:00</updated>
    <author>
      <name>/u/caiowilson</name>
      <uri>https://old.reddit.com/user/caiowilson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a local server with dedicated GPU, i9 (quite old, Skylake I think) and 16gb running debian headless is it a better option to use? Harder to configure? Currently I only play with LLMs on my MacBook pro M3 pro (18gb, most LLMs don't use the GPU) but it's not very doable when working (20 containers running 20% of my processors and quite a bit of ram) Any considerations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/caiowilson"&gt; /u/caiowilson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7zq3s/my_good_people/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7zq3s/my_good_people/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7zq3s/my_good_people/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T09:34:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7n9n5</id>
    <title>How the VRAM works for Macs</title>
    <updated>2025-01-22T22:00:34+00:00</updated>
    <author>
      <name>/u/Cosyless</name>
      <uri>https://old.reddit.com/user/Cosyless</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just recently got into working with models in local. I have 1650Ti (notebook GPU) which has 4GB VRAM obviously. I wonder how VRAM works for the new M-chip mac(books/mini). By definition, they do not have a 'video card'. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cosyless"&gt; /u/Cosyless &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7n9n5/how_the_vram_works_for_macs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7n9n5/how_the_vram_works_for_macs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7n9n5/how_the_vram_works_for_macs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T22:00:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i80brl</id>
    <title>My first every test drive of the deepseek-r1</title>
    <updated>2025-01-23T10:20:35+00:00</updated>
    <author>
      <name>/u/Kind_Ad_2866</name>
      <uri>https://old.reddit.com/user/Kind_Ad_2866</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i80brl/my_first_every_test_drive_of_the_deepseekr1/"&gt; &lt;img alt="My first every test drive of the deepseek-r1" src="https://b.thumbs.redditmedia.com/U7q8JfhCRJ9KmqGLoZ-z3_3bBBOkzIJsHlBoWdlNdgM.jpg" title="My first every test drive of the deepseek-r1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded DeepSeek-R1 last night and tested it on my Nvidia GeForce 4060 (8GB) paired with an Intel i7 and 64GB of RAM. My first impressions are really positive—it handles conversation and reasoning surprisingly well, especially for a locally running model. However, unlike other models such as LLaMA 3.3, I can’t seem to make it follow my instructions precisely. If you look at the screenshot, you’ll see that it doesn’t produce the exact output format I requested. I’m wondering if I’m missing a step or if this is just how the model behaves.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x67te5yd4qee1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1350e47660acbc968da44a05be040228799b0467"&gt;https://preview.redd.it/x67te5yd4qee1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1350e47660acbc968da44a05be040228799b0467&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/aq6ba6yd4qee1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1065763b1396f06a34b01f5f291e8af514999d3"&gt;https://preview.redd.it/aq6ba6yd4qee1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1065763b1396f06a34b01f5f291e8af514999d3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hh3oa7yd4qee1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9312866d88acd4a01c0e0006e98fdc7a32eb8d35"&gt;https://preview.redd.it/hh3oa7yd4qee1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9312866d88acd4a01c0e0006e98fdc7a32eb8d35&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kind_Ad_2866"&gt; /u/Kind_Ad_2866 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i80brl/my_first_every_test_drive_of_the_deepseekr1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i80brl/my_first_every_test_drive_of_the_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i80brl/my_first_every_test_drive_of_the_deepseekr1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T10:20:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i88279</id>
    <title>What local LLM provides that online chat bots like gpt and gemini can’t?</title>
    <updated>2025-01-23T17:00:27+00:00</updated>
    <author>
      <name>/u/Latter_Possession786</name>
      <uri>https://old.reddit.com/user/Latter_Possession786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;why use system resources when you can just hop on online chat bots?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Latter_Possession786"&gt; /u/Latter_Possession786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i88279/what_local_llm_provides_that_online_chat_bots/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i88279/what_local_llm_provides_that_online_chat_bots/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i88279/what_local_llm_provides_that_online_chat_bots/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T17:00:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1i814ta</id>
    <title>Is it possible to run ollama if i have multiple computers?</title>
    <updated>2025-01-23T11:17:55+00:00</updated>
    <author>
      <name>/u/Muted_Membership9372</name>
      <uri>https://old.reddit.com/user/Muted_Membership9372</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m kind of new to this field, so I apologize for any ignorance on my part!&lt;br /&gt; Here’s the problem I’ve encountered:&lt;br /&gt; Suppose I have several devices with NVIDIA GeForce RTX 3060 Lite Hash Rate, and I’d like to run a model that requires more GPU VRAM than I have on a single GPU.&lt;br /&gt; So, my question is:&lt;br /&gt; Is it possible to use my devices (perhaps as a cluster) to run the model? And if so, will the workload be distributed equally across the GPUs?&lt;/p&gt; &lt;p&gt;I'd also be grateful to know if there's already implemented solutions out there, as I'd prefer not to reinvent the wheel&lt;/p&gt; &lt;p&gt;Thx in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Muted_Membership9372"&gt; /u/Muted_Membership9372 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i814ta/is_it_possible_to_run_ollama_if_i_have_multiple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i814ta/is_it_possible_to_run_ollama_if_i_have_multiple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i814ta/is_it_possible_to_run_ollama_if_i_have_multiple/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T11:17:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7as97</id>
    <title>The Chinese OBLITERATED OpenAI. A side-by-side comparison of DeepSeek R1 vs OpenAI O1 for Finance</title>
    <updated>2025-01-22T13:12:02+00:00</updated>
    <author>
      <name>/u/No-Definition-2886</name>
      <uri>https://old.reddit.com/user/No-Definition-2886</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7as97/the_chinese_obliterated_openai_a_sidebyside/"&gt; &lt;img alt="The Chinese OBLITERATED OpenAI. A side-by-side comparison of DeepSeek R1 vs OpenAI O1 for Finance" src="https://external-preview.redd.it/0d_JuM7Vh55nJ0hGva5m6LiE2ZjyLRmQI-3W0gsRh-4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b831df3cf7b173edcc1428ce4703b68ea01c270" title="The Chinese OBLITERATED OpenAI. A side-by-side comparison of DeepSeek R1 vs OpenAI O1 for Finance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Definition-2886"&gt; /u/No-Definition-2886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/p/93a1b4343a82"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7as97/the_chinese_obliterated_openai_a_sidebyside/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7as97/the_chinese_obliterated_openai_a_sidebyside/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T13:12:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1i84th6</id>
    <title>Looking for a model that's good at recall/memory</title>
    <updated>2025-01-23T14:40:20+00:00</updated>
    <author>
      <name>/u/badaimbadjokes</name>
      <uri>https://old.reddit.com/user/badaimbadjokes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What I need most is a cognitive partner. It doesn't have to be all that smart, because I'm not all that smart. But what I REALLY want is for it to remember things. If I say, &amp;quot;Hey, what was that movie I added to my list the other day?&amp;quot; I want it to be able to say, &amp;quot;You wanted me to remember 'The Royal Tenenbaums.'&amp;quot; &lt;/p&gt; &lt;p&gt;Is there an ollama model that leans into this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/badaimbadjokes"&gt; /u/badaimbadjokes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i84th6/looking_for_a_model_thats_good_at_recallmemory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i84th6/looking_for_a_model_thats_good_at_recallmemory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i84th6/looking_for_a_model_thats_good_at_recallmemory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T14:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7r6hf</id>
    <title>How to Turn off Thinking on ollama for DS-R1?</title>
    <updated>2025-01-23T00:54:11+00:00</updated>
    <author>
      <name>/u/PBlague</name>
      <uri>https://old.reddit.com/user/PBlague</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7r6hf/how_to_turn_off_thinking_on_ollama_for_dsr1/"&gt; &lt;img alt="How to Turn off Thinking on ollama for DS-R1?" src="https://a.thumbs.redditmedia.com/Xs-jZtPcBKuCW5kZ139NKMaiuaxV26kv1lwVYBQpU70.jpg" title="How to Turn off Thinking on ollama for DS-R1?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/o1yk2nfn5nee1.png?width=989&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5bfe41d31ef405a0d23da5d42fcbdc03cd00956f"&gt;DeepThink Button at the Bottom Left&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On the website you can turn on &amp;quot;DeepThink&amp;quot; if you so choose to, which helps make the model more powerful in reasoning and less powerful in actually holding the conversation.&lt;/p&gt; &lt;p&gt;Is there a way to turn off this thinking part in ollama too?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qc7szvbx5nee1.png?width=1405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fb98b4076d3cab51fbab5516c61bcb5962771766"&gt;&amp;lt;Think&amp;gt; &amp;lt;/Think&amp;gt; Patterns&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PBlague"&gt; /u/PBlague &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7r6hf/how_to_turn_off_thinking_on_ollama_for_dsr1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7r6hf/how_to_turn_off_thinking_on_ollama_for_dsr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7r6hf/how_to_turn_off_thinking_on_ollama_for_dsr1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T00:54:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i81q09</id>
    <title>Context with deepseek r1?</title>
    <updated>2025-01-23T11:57:37+00:00</updated>
    <author>
      <name>/u/joyfulsparrow</name>
      <uri>https://old.reddit.com/user/joyfulsparrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does deepseek r1 keep the context of what you're talking about, or is every question like new for it? For instance, if I ask a coding question, and then follow up with some bugs, it doesn't seem to remember the original question and answer. Is there a way to enable this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joyfulsparrow"&gt; /u/joyfulsparrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i81q09/context_with_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i81q09/context_with_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i81q09/context_with_deepseek_r1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T11:57:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7ds9g</id>
    <title>It didn't even need to think to reply 😂 (deepseek-r1)</title>
    <updated>2025-01-22T15:33:52+00:00</updated>
    <author>
      <name>/u/EnoughVeterinarian90</name>
      <uri>https://old.reddit.com/user/EnoughVeterinarian90</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7ds9g/it_didnt_even_need_to_think_to_reply_deepseekr1/"&gt; &lt;img alt="It didn't even need to think to reply 😂 (deepseek-r1)" src="https://b.thumbs.redditmedia.com/bU5DA_X-RMAId5YJB4pzOMTQS9Hz0n0JNBOyXhHZi3E.jpg" title="It didn't even need to think to reply 😂 (deepseek-r1)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EnoughVeterinarian90"&gt; /u/EnoughVeterinarian90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i7ds9g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7ds9g/it_didnt_even_need_to_think_to_reply_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7ds9g/it_didnt_even_need_to_think_to_reply_deepseekr1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T15:33:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8749h</id>
    <title>Problem launching Ollama Open-WebUI</title>
    <updated>2025-01-23T16:21:04+00:00</updated>
    <author>
      <name>/u/StuartMillington</name>
      <uri>https://old.reddit.com/user/StuartMillington</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to install and run Ollama and the Open-WebUI running in a VM on linux Ubuntu. I do NOT want to use Docker because it is an extra layer of complexity I can do without at this stage. My specifi problem is that although I have installed Ollama and the OpenWebUI, when I enter localhost:8080 into my browser, the response is that the localhost refused to connect. When I issue localhost:11434 the response is that Ollama is running. I have done the following:&lt;/p&gt; &lt;p&gt;Installed Ollama as per &lt;a href="https://itsfoss.com/ollama-setup-linux/"&gt;this link&lt;/a&gt; - Step #1 only. I have not tried to install any LLMs yet.&lt;/p&gt; &lt;p&gt;I installed the WebUI as per &lt;a href="https://docs.openwebui.com/getting-started/quick-start/"&gt;this link&lt;/a&gt; using the Python method onto a Linux machine. I have also update the application using Pip as per the same document. The problem comes when I try to launch the WebUI with localhost:8080 (for the Python installation). &lt;/p&gt; &lt;p&gt;Researching the problem on the internet suggests that there can be a problem if not all local interfaces are open to listening, as per &lt;a href="https://github.com/ollama/ollama/issues/703"&gt;this post&lt;/a&gt;. So I edited a service override file to add the following:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[Service] Environment=&amp;quot;OLLAMA_HOST=0.0.0.0&amp;quot; &amp;quot;OLLAMA_KEEP_ALIVE=-1&amp;quot; &amp;quot;OLLAMA_MAX_LOADED_MODELS=4&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;rather than edit the service daemon directly as per the advice by &lt;a href="https://github.com/nuaimat"&gt;nuaimat&lt;/a&gt;&lt;a href="https://github.com/ollama/ollama/issues/703#issuecomment-2325253842"&gt;on Sep 2, 2024&lt;/a&gt;. I then did the following:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo systemctl daemon-reload&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo service ollama restart&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo service ollama status&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The output of the last command is as follows:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama.service - Ollama Service&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Loaded: loaded (/etc/systemd/system/ollama.service; enabled; preset: enabled)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Drop-In: /etc/systemd/system/ollama.service.d&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;└─override.conf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Active: active (running) since Thu 2025-01-23 15:43:21 GMT; 33min ago&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Main PID: 1142 (ollama)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Tasks: 11 (limit: 23348)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Memory: 33.0M (peak: 33.5M)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;CPU: 271ms&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;CGroup: /system.slice/ollama.service&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;└─1142 /usr/local/bin/ollama serve&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jan 23 15:43:22 FRANK1 ollama[1142]: [GIN-debug] HEAD /api/version --&amp;gt;&lt;/code&gt; &lt;a href="http://github.com/ollama/ollama/server.(*Server"&gt;&lt;code&gt;github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2&lt;/code&gt;&lt;/a&gt;.GenerateRoutes.func2) &lt;code&gt;(5 handlers)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jan 23 15:43:22 FRANK1 ollama[1142]: time=2025-01-23T15:43:22.157Z level=INFO source=routes.go:1238 msg=&amp;quot;Listening on [::]:11434 (version 0.5.7)&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jan 23 15:43:22 FRANK1 ollama[1142]: time=2025-01-23T15:43:22.162Z level=INFO source=routes.go:1267 msg=&amp;quot;Dynamic LLM libraries&amp;quot; runners=&amp;quot;[cpu_avx2 cuda_v11_avx cuda_v12_avx rocm_avx cpu cpu&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jan 23 15:43:22 FRANK1 ollama[1142]: time=2025-01-23T15:43:22.165Z level=INFO source=gpu.go:226 msg=&amp;quot;looking for compatible GPUs&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jan 23 15:43:22 FRANK1 ollama[1142]: time=2025-01-23T15:43:22.212Z level=INFO source=gpu.go:392 msg=&amp;quot;no compatible GPUs were discovered&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jan 23 15:43:22 FRANK1 ollama[1142]: time=2025-01-23T15:43:22.212Z level=INFO source=types.go:131 msg=&amp;quot;inference compute&amp;quot; id=0 library=cpu variant=avx compute=&amp;quot;&amp;quot; driver=0.0 name=&amp;quot;&amp;quot; total=&amp;quot;1&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jan 23 15:52:11 FRANK1 ollama[1142]: [GIN] 2025/01/23 - 15:52:11 | 200 | 1.056082ms | ::1 | GET &amp;quot;/&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jan 23 15:52:11 FRANK1 ollama[1142]: [GIN] 2025/01/23 - 15:52:11 | 404 | 6.26µs | ::1 | GET &amp;quot;/favicon.ico&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jan 23 15:56:04 FRANK1 ollama[1142]: [GIN] 2025/01/23 - 15:56:04 | 200 | 50.966µs |&lt;/code&gt; &lt;a href="http://127.0.0.1"&gt;&lt;code&gt;127.0.0.1&lt;/code&gt;&lt;/a&gt; &lt;code&gt;| HEAD &amp;quot;/&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Jan 23 15:56:04 FRANK1 ollama[1142]: [GIN] 2025/01/23 - 15:56:04 | 200 | 1.10902ms |&lt;/code&gt; &lt;a href="http://127.0.0.1"&gt;&lt;code&gt;127.0.0.1&lt;/code&gt;&lt;/a&gt; &lt;code&gt;| GET &amp;quot;/api/ps&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I feel that I am very close to getting this right, but there is something I am missing. Can anybody help, please. Regards, Stuart M&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StuartMillington"&gt; /u/StuartMillington &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8749h/problem_launching_ollama_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8749h/problem_launching_ollama_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8749h/problem_launching_ollama_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T16:21:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1i87g5h</id>
    <title>LLMs are changing its answers like mood swings</title>
    <updated>2025-01-23T16:35:05+00:00</updated>
    <author>
      <name>/u/TanmaySathe</name>
      <uri>https://old.reddit.com/user/TanmaySathe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I asked following question to OpenAI o1, Gemini 1.5 &amp;amp; 2.0 b and DeepSeek-R1&lt;/p&gt; &lt;p&gt;&lt;code&gt;Turn on the machine only once an hour when the magnetic sensor is triggered. Last triggered time was 8:17PM. It's 9:00PM now &amp;amp; the magnetic senso has triggered. Can the machine be turned on? Answer only in JSON&lt;/code&gt;&lt;/p&gt; &lt;p&gt;ChatGPT answered&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;canTurnOn&amp;quot;: true } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Gemini answered&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;turn_on_machine&amp;quot;: true } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;DeepSeek-R1 answered&lt;br /&gt; &lt;code&gt;{ &amp;quot;can_turn_on&amp;quot;: true, &amp;quot;reason&amp;quot;: &amp;quot;More than an hour has passed since the last trigger (8:17 PM to 9:00 PM).&amp;quot; }&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Sometimes ChatGPT did give me correct answer when I asked that equestion multiple times. I want to use the power of LLM for an industrial automation system but without consistent &amp;amp; trustworthy results I can not use it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TanmaySathe"&gt; /u/TanmaySathe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i87g5h/llms_are_changing_its_answers_like_mood_swings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i87g5h/llms_are_changing_its_answers_like_mood_swings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i87g5h/llms_are_changing_its_answers_like_mood_swings/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T16:35:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7ygrp</id>
    <title>Good UI for DeepSeek R1</title>
    <updated>2025-01-23T07:56:24+00:00</updated>
    <author>
      <name>/u/AdAccomplished8942</name>
      <uri>https://old.reddit.com/user/AdAccomplished8942</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm running DeepSeek R1 (32b) on my Apple M3 MacBook Pro and it works pretty well, but I can only use it in Terminal because I haven't found a nice UI for it.&lt;/p&gt; &lt;p&gt;Msty looks really good and has great features, but it forgets previous prompts and answers. Basically every prompt is like a completely new chat, which makes many things impossible.&lt;/p&gt; &lt;p&gt;What are some nice UIs for DeepSeek R1 on ollama?&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdAccomplished8942"&gt; /u/AdAccomplished8942 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7ygrp/good_ui_for_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7ygrp/good_ui_for_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7ygrp/good_ui_for_deepseek_r1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T07:56:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i87mtt</id>
    <title>infinite download, ollama, deepseek models</title>
    <updated>2025-01-23T16:42:47+00:00</updated>
    <author>
      <name>/u/ext115</name>
      <uri>https://old.reddit.com/user/ext115</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can't download the language model with Ollama. While downloading the percentages go back and forth, the process takes hours until the session expires and it uses all my internet connections - cannot do anything with the internet. Do you have a similar problem?&lt;/p&gt; &lt;p&gt;ps&lt;br /&gt; i was only able to download smallest deepseek model deepseek-r1:1.5b &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ext115"&gt; /u/ext115 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i87mtt/infinite_download_ollama_deepseek_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i87mtt/infinite_download_ollama_deepseek_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i87mtt/infinite_download_ollama_deepseek_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T16:42:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i87oru</id>
    <title>Struggling to run a RAQ application on my m1 mbp 16gb ram and 512ssd; model is ollama3.2 3b params</title>
    <updated>2025-01-23T16:45:03+00:00</updated>
    <author>
      <name>/u/Dasaboro</name>
      <uri>https://old.reddit.com/user/Dasaboro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using langflow since i'm new to python and every thing ai, but I can read in between the lines as I have a coding background&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dasaboro"&gt; /u/Dasaboro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i87oru/struggling_to_run_a_raq_application_on_my_m1_mbp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i87oru/struggling_to_run_a_raq_application_on_my_m1_mbp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i87oru/struggling_to_run_a_raq_application_on_my_m1_mbp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T16:45:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1i87rs7</id>
    <title>Connect multiple Mac Minis for fast DeepSeek R1 performance</title>
    <updated>2025-01-23T16:48:30+00:00</updated>
    <author>
      <name>/u/Exciting-Syrup-1107</name>
      <uri>https://old.reddit.com/user/Exciting-Syrup-1107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there,&lt;/p&gt; &lt;p&gt;since using DeepSeek R1 I‘m baffled by the fact that I can use a nearly o1 level LLM on my local device. But it‘s just super slow. I have seen a YouTube video (i don‘t remember it exactly anymore) where a guy talks about connecting 2 or 3 Mac Minis to achieve a fast performing local DeepSeek R1.&lt;/p&gt; &lt;p&gt;My question now is: How can you do that? Are there certain tools that can „split“ the workload of ollama onto multiple devices? Are there online tools where you can rent fast computing space to run your own instance?&lt;/p&gt; &lt;p&gt;I‘m really curious about this topic and would love to learn more.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Exciting-Syrup-1107"&gt; /u/Exciting-Syrup-1107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i87rs7/connect_multiple_mac_minis_for_fast_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i87rs7/connect_multiple_mac_minis_for_fast_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i87rs7/connect_multiple_mac_minis_for_fast_deepseek_r1/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T16:48:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7yycc</id>
    <title>what can do now?</title>
    <updated>2025-01-23T08:33:23+00:00</updated>
    <author>
      <name>/u/Own-Perception-1574</name>
      <uri>https://old.reddit.com/user/Own-Perception-1574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i7yycc/what_can_do_now/"&gt; &lt;img alt="what can do now?" src="https://preview.redd.it/y8xv9gwwfpee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f7701dbfaa233f2456f822ab6bf09b6cd5f6c5e" title="what can do now?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Perception-1574"&gt; /u/Own-Perception-1574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y8xv9gwwfpee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7yycc/what_can_do_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7yycc/what_can_do_now/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T08:33:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7nqrj</id>
    <title>Run a fully local AI Search / RAG pipeline using Ollama with 4GB of memory and no GPU</title>
    <updated>2025-01-22T22:20:43+00:00</updated>
    <author>
      <name>/u/LeetTools</name>
      <uri>https://old.reddit.com/user/LeetTools</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, for people that want to run AI search and RAG pipelines locally, you can now build your local knowledge base with one line of command and everything runs locally with no docker or API key required. Repo is here: &lt;a href="https://github.com/leettools-dev/leettools"&gt;https://github.com/leettools-dev/leettools&lt;/a&gt;. The total memory usage is around 4GB with the Llama3.2 model: * llama3.2:latest 3.5 GB * nomic-embed-text:latest 370 MB * LeetTools: 350MB (Document pipeline backend with Python and DuckDB)&lt;/p&gt; &lt;p&gt;First, follow the instructions on &lt;a href="https://github.com/ollama/ollama"&gt;https://github.com/ollama/ollama&lt;/a&gt; to install the ollama program. Make sure the ollama program is running.&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;set up&lt;/h1&gt; &lt;p&gt;ollama pull llama3.2 ollama pull nomic-embed-text pip install leettools curl -fsSL -o .env.ollama &lt;a href="https://raw.githubusercontent.com/leettools-dev/leettools/refs/heads/main/env.ollama"&gt;https://raw.githubusercontent.com/leettools-dev/leettools/refs/heads/main/env.ollama&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;one command line to download a PDF and save it to the graphrag KB&lt;/h1&gt; &lt;p&gt;leet kb add-url -e .env.ollama -k graphrag -l info &lt;a href="https://arxiv.org/pdf/2501.09223"&gt;https://arxiv.org/pdf/2501.09223&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;now you query the local graphrag KB with questions&lt;/h1&gt; &lt;p&gt;leet flow -t answer -e .env.ollama -k graphrag -l info -p retriever_type=local -q &amp;quot;How does GraphRAG work?&amp;quot; ```&lt;/p&gt; &lt;p&gt;You can also add your local directory or files to the knowledge base using &lt;code&gt;leet kb add-local&lt;/code&gt; command.&lt;/p&gt; &lt;p&gt;For the above default setup, we are using * docling to convert PDF to markdown * chonkie as the chunker * nomic-embed-text as the embedding model * llama3.2 as the inference engine * Duckdb as the data storage include graph and vector&lt;/p&gt; &lt;p&gt;We think it might be helpful for some usage scenarios that require local deployment and resource limits. Questions or suggestions are welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeetTools"&gt; /u/LeetTools &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7nqrj/run_a_fully_local_ai_search_rag_pipeline_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i7nqrj/run_a_fully_local_ai_search_rag_pipeline_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i7nqrj/run_a_fully_local_ai_search_rag_pipeline_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-22T22:20:43+00:00</published>
  </entry>
</feed>
