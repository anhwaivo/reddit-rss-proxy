<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-24T19:05:33+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1i8qav3</id>
    <title>A list of all the top Open Source Chat UI for ollama/any LLM in general. (community edition)</title>
    <updated>2025-01-24T07:43:57+00:00</updated>
    <author>
      <name>/u/VisibleLawfulness246</name>
      <uri>https://old.reddit.com/user/VisibleLawfulness246</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's my list right now&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open Web UI&lt;/li&gt; &lt;li&gt;LibreChat&lt;/li&gt; &lt;li&gt;anythingLLM&lt;/li&gt; &lt;li&gt;GPT4all&lt;/li&gt; &lt;li&gt;oobabooga&lt;/li&gt; &lt;li&gt;verba&lt;/li&gt; &lt;li&gt;dify&lt;/li&gt; &lt;li&gt;SillyTavern&lt;/li&gt; &lt;li&gt;Danswer&lt;/li&gt; &lt;li&gt;Lobe Ui&lt;/li&gt; &lt;li&gt;hugging face chat-Ui&lt;/li&gt; &lt;li&gt;kobold Cpp/ for from llama cpp&lt;/li&gt; &lt;li&gt;private gpt&lt;/li&gt; &lt;li&gt;serge chat&lt;/li&gt; &lt;li&gt;JanHQ&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What am I missing from this list? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VisibleLawfulness246"&gt; /u/VisibleLawfulness246 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8qav3/a_list_of_all_the_top_open_source_chat_ui_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8qav3/a_list_of_all_the_top_open_source_chat_ui_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8qav3/a_list_of_all_the_top_open_source_chat_ui_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T07:43:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8h7sd</id>
    <title>Param Count vs FP Precision</title>
    <updated>2025-01-23T23:23:25+00:00</updated>
    <author>
      <name>/u/PBlague</name>
      <uri>https://old.reddit.com/user/PBlague</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this is not an exact science and that models with the same amount of parameters and same floating point precision can have vastly different results based on the benchmark and how they've been trained and the model architecture. But I wanna know which one is gonna be generally better...&lt;/p&gt; &lt;p&gt;For example is the fp16 llama3.2 8b gonna be better than the q_2 llama3.2 70b? They have comparable amounts of data, somewhere around 25gbs.&lt;/p&gt; &lt;p&gt;I can't run the 70b with my current hardware and I doubt I will be able to run the fp16 8b either, but it seems more possible, so I was wondering if it's going to be comparable in accuracy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PBlague"&gt; /u/PBlague &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8h7sd/param_count_vs_fp_precision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8h7sd/param_count_vs_fp_precision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8h7sd/param_count_vs_fp_precision/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T23:23:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8re2x</id>
    <title>Ghif: llm-friendly GitHub issue fetcher</title>
    <updated>2025-01-24T09:08:18+00:00</updated>
    <author>
      <name>/u/kiwami_zamurai</name>
      <uri>https://old.reddit.com/user/kiwami_zamurai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/kiwamizamurai/ghif"&gt;https://github.com/kiwamizamurai/ghif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I developed a way to bring repository issues to a local environment, hoping it might make AI development a bit more enjoyable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kiwami_zamurai"&gt; /u/kiwami_zamurai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8re2x/ghif_llmfriendly_github_issue_fetcher/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8re2x/ghif_llmfriendly_github_issue_fetcher/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8re2x/ghif_llmfriendly_github_issue_fetcher/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T09:08:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8f0cm</id>
    <title>I'm new to local LLMs but this is so inaccurate</title>
    <updated>2025-01-23T21:46:50+00:00</updated>
    <author>
      <name>/u/vsurresh</name>
      <uri>https://old.reddit.com/user/vsurresh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm new to running local LLMs and I'm only running the 8b version. I decided to try 'deepseek-r1:8b' and decided to ask a simple question. Looks like the training ended before the F1 championship was over but it says Lewis Hamilton has retired which is completely inaccurate. Is this type of response normal for locally running small LLMs? (screenshot attached) Thanks in advance&lt;/p&gt; &lt;p&gt;Edit - Adding the output&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; Who won 2024 F1 championship? &amp;lt;think&amp;gt; Okay, the user just asked, &amp;quot;Who won 2024 F1 championship?&amp;quot; Hmm, wait a minute. I remember that as of my last update in July 2024, the 2024 Formula 1 season was still ongoing. The races hadn't concluded yet, so it's not possible to determine the winner at that point. I should check if there have been any changes or updates since then. Let me thinkâ€”did Lewis Hamilton win another championship? No, he retired from F1 in 2023. Max Verstappen won the 2023 championship but was involved in an incident at the 2024 Spanish Grand Prix where he crashed heavily. Wait, I'm getting confused because the user is asking about 2024, and my knowledge cutoff is July 2024. So, as of now, the championship is still being competed for, and the season hasn't ended yet. Therefore, the winner isn't determined until the end of the season. I should make sure to inform the user that the season is ongoing and the winner will be known after all races are completed. Also, I need to mention that Lewis Hamilton retired, so he's not competing anymore. It's important to provide accurate information without speculation since the championship isn't decided yet. &amp;lt;/think&amp;gt; As of my last update in July 2024, the 2024 Formula 1 season is still ongoing, and the champion has not yet been determined. The season concludes after all races are completed, so the winner will be known by the end of the year. Keep in mind that Lewis Hamilton retired from Formula 1 at the end of the 2023 season, so he is no longer competing. Max Verstappen won the 2023 championship but faced a challenging defense in 2024. The 2024 championship is still up for grabs, and it will be decided after the final race of the season. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vsurresh"&gt; /u/vsurresh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8f0cm/im_new_to_local_llms_but_this_is_so_inaccurate/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8f0cm/im_new_to_local_llms_but_this_is_so_inaccurate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8f0cm/im_new_to_local_llms_but_this_is_so_inaccurate/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T21:46:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8spym</id>
    <title>Recovering lost model files due to forced windows update</title>
    <updated>2025-01-24T10:48:26+00:00</updated>
    <author>
      <name>/u/justquacks</name>
      <uri>https://old.reddit.com/user/justquacks</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i8spym/recovering_lost_model_files_due_to_forced_windows/"&gt; &lt;img alt="Recovering lost model files due to forced windows update" src="https://b.thumbs.redditmedia.com/froMZXhP8yqyX2g1t-7Ts1xdg_M6eJB3_wSLVYePBKs.jpg" title="Recovering lost model files due to forced windows update" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just spent 14 hours downloading a 400 gigabyte model.&lt;/p&gt; &lt;p&gt;I got about 60% of the way through when windows update force restarted the computer. &lt;/p&gt; &lt;p&gt;Sometimes, ollama has end of file errors or the connection times out. Many times my downloads were able to be resumed and there were zero issues.&lt;/p&gt; &lt;p&gt;I dont know what caused this, but ALL of the files from that download are gone. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Does anyone know of any recovery software that doesnt overwrite data? Software that just makes data available again?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As far as I know, recuva is absolutely NOT the tool for this-&lt;/p&gt; &lt;p&gt;I was writing this post when I was again prompted to update windows on the server pc- I cannot express how full of rage I am after losing 14 hours of download.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/far7dj7p7xee1.png?width=714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=662c07ebe2c446dc655fc71c9e8b68191ae11f1e"&gt;https://preview.redd.it/far7dj7p7xee1.png?width=714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=662c07ebe2c446dc655fc71c9e8b68191ae11f1e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/justquacks"&gt; /u/justquacks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8spym/recovering_lost_model_files_due_to_forced_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8spym/recovering_lost_model_files_due_to_forced_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8spym/recovering_lost_model_files_due_to_forced_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T10:48:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8trrd</id>
    <title>ollama local models disappeared on my linux machine</title>
    <updated>2025-01-24T11:59:43+00:00</updated>
    <author>
      <name>/u/wo-tatatatatata</name>
      <uri>https://old.reddit.com/user/wo-tatatatatata</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;~/.ollama only has a 'history' file, ollama models are nowhere under /, other than the app binary, here is the result after running: ollama list&lt;/p&gt; &lt;p&gt;NAME ID SIZE MODIFIED&lt;/p&gt; &lt;p&gt;llama3.3:latest a6eb4748fd29 42 GB 3 weeks ago&lt;/p&gt; &lt;p&gt;incept5/llama3.1-claude:latest 4ba850d59c62 4.7 GB 3 weeks ago&lt;/p&gt; &lt;p&gt;codellama:13b-instruct 9f438cb9cd58 7.4 GB 3 weeks ago&lt;/p&gt; &lt;p&gt;mistral:latest f974a74358d6 4.1 GB 3 weeks ago&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wo-tatatatatata"&gt; /u/wo-tatatatatata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8trrd/ollama_local_models_disappeared_on_my_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8trrd/ollama_local_models_disappeared_on_my_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8trrd/ollama_local_models_disappeared_on_my_linux/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T11:59:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8uejq</id>
    <title>Ollama model file creation running for very long time.</title>
    <updated>2025-01-24T12:37:11+00:00</updated>
    <author>
      <name>/u/RageshAntony</name>
      <uri>https://old.reddit.com/user/RageshAntony</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I downloaded the &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-Q8_0"&gt;https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-Q8_0&lt;/a&gt; folder.&lt;/p&gt; &lt;p&gt;Then tried executing the command inside the folder :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama create deepseek-r1 -f Modelfile &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;the process message &amp;quot;gathering model components&amp;quot; was running for more than 40mins. I am using 4x AMD MI300X and finally stopped the pod due to pricing. &lt;/p&gt; &lt;p&gt;Why this happening ? &lt;/p&gt; &lt;p&gt;And another question?&lt;/p&gt; &lt;p&gt;If I download from Ollama Model Hub that is &amp;quot;ollama run deepseek-r1:671b-q8_0&amp;quot; to a custom directory and reuse the disk in another pod with new Ollama installation, can I directly run from the custom directory ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RageshAntony"&gt; /u/RageshAntony &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8uejq/ollama_model_file_creation_running_for_very_long/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8uejq/ollama_model_file_creation_running_for_very_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8uejq/ollama_model_file_creation_running_for_very_long/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T12:37:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8uiiq</id>
    <title>What models and for what task do you use with OLLaMa</title>
    <updated>2025-01-24T12:43:12+00:00</updated>
    <author>
      <name>/u/Lines25</name>
      <uri>https://old.reddit.com/user/Lines25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title.&lt;/p&gt; &lt;p&gt;I'm using models for: code completions (qwen2.5-code:3b), code generation (Codestral, I use mistral server vesrion, it's free, but I have downloaded it and I have ran it like 15-20 times, yeah), SillyTavern (MythoMax:13B, it really grate for both SFW and NSFW, for now, using the openrouter version, cuz it free and have downloaded it too on OLLaMa, btw)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lines25"&gt; /u/Lines25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8uiiq/what_models_and_for_what_task_do_you_use_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8uiiq/what_models_and_for_what_task_do_you_use_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8uiiq/what_models_and_for_what_task_do_you_use_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T12:43:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8lza7</id>
    <title>AI-Powered Bot for Automated Job Applications on LinkedIn</title>
    <updated>2025-01-24T03:15:40+00:00</updated>
    <author>
      <name>/u/Own-Perception-1574</name>
      <uri>https://old.reddit.com/user/Own-Perception-1574</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to develop an AI-powered bot capable of automating the job application process on LinkedIn. This bot would analyze job descriptions (JDs), customize the CV or resume based on the requirements, and apply to relevant positions automatically. Does a similar AI solution already exist in the market?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Perception-1574"&gt; /u/Own-Perception-1574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8lza7/aipowered_bot_for_automated_job_applications_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8lza7/aipowered_bot_for_automated_job_applications_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8lza7/aipowered_bot_for_automated_job_applications_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T03:15:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8fg88</id>
    <title>I Built an Open-Source RAG API for Docs, GitHub Issues and READMEs</title>
    <updated>2025-01-23T22:05:41+00:00</updated>
    <author>
      <name>/u/eleven-five</name>
      <uri>https://old.reddit.com/user/eleven-five</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve been working on &lt;strong&gt;Ragpi&lt;/strong&gt;, an open-source AI assistant that builds knowledge bases from docs, GitHub Issues, and READMEs. It uses Redis Stack as a vector DB and leverages RAG to answer technical questions through an API.&lt;/p&gt; &lt;p&gt;Some things it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Creates knowledge bases from documentation websites, GitHub Issues, and READMEs&lt;/li&gt; &lt;li&gt;Uses hybrid search (semantic + keyword) for retrieval&lt;/li&gt; &lt;li&gt;Uses tool calling to dynamically search and retrieve relevant information during conversations&lt;/li&gt; &lt;li&gt;Works with OpenAI or Ollama&lt;/li&gt; &lt;li&gt;Provides a simple REST API for querying and managing sources&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Built with:&lt;/strong&gt; FastAPI, Redis Stack, and Celery.&lt;/p&gt; &lt;p&gt;Itâ€™s still a work in progress, but Iâ€™d love some feedback!&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/ragpi/ragpi"&gt;https://github.com/ragpi/ragpi&lt;/a&gt;&lt;br /&gt; API Reference: &lt;a href="https://docs.ragpi.io/"&gt;https://docs.ragpi.io&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eleven-five"&gt; /u/eleven-five &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8fg88/i_built_an_opensource_rag_api_for_docs_github/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8fg88/i_built_an_opensource_rag_api_for_docs_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8fg88/i_built_an_opensource_rag_api_for_docs_github/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T22:05:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8r79t</id>
    <title>How to run models that are not on ollama website? Especially uncensored ones</title>
    <updated>2025-01-24T08:53:40+00:00</updated>
    <author>
      <name>/u/discoveringnature12</name>
      <uri>https://old.reddit.com/user/discoveringnature12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;trying to figure out how to run models not listed &lt;a href="https://ollama.com/library"&gt;https://ollama.com/library&lt;/a&gt;. Want to run uncensored models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/discoveringnature12"&gt; /u/discoveringnature12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8r79t/how_to_run_models_that_are_not_on_ollama_website/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8r79t/how_to_run_models_that_are_not_on_ollama_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8r79t/how_to_run_models_that_are_not_on_ollama_website/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T08:53:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8m6ls</id>
    <title>Llama 3.1 405B + 8x AMD Instinct Mi60 AI Server - Shockingly Good!</title>
    <updated>2025-01-24T03:26:28+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xzhz003o1vee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8m6ls/llama_31_405b_8x_amd_instinct_mi60_ai_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8m6ls/llama_31_405b_8x_amd_instinct_mi60_ai_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T03:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8k8u7</id>
    <title>I added ollama support for an open-source operator agent</title>
    <updated>2025-01-24T01:47:28+00:00</updated>
    <author>
      <name>/u/Swimming_Driver4974</name>
      <uri>https://old.reddit.com/user/Swimming_Driver4974</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created a similar Operator Agent framework like OpenAI's one, and made it open-source. Just added support for Ollama so it can be run completely locally using text and vision models. It's not perfect, but has potential: &lt;a href="https://github.com/GPT-Protocol/007-agent"&gt;https://github.com/GPT-Protocol/007-agent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Swimming_Driver4974"&gt; /u/Swimming_Driver4974 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8k8u7/i_added_ollama_support_for_an_opensource_operator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8k8u7/i_added_ollama_support_for_an_opensource_operator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8k8u7/i_added_ollama_support_for_an_opensource_operator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T01:47:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8zs2g</id>
    <title>Locally hosted deepseek-r1 and OpenWebUI</title>
    <updated>2025-01-24T16:45:35+00:00</updated>
    <author>
      <name>/u/robonova-1</name>
      <uri>https://old.reddit.com/user/robonova-1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to use deepseek-r1 with OpenWebUI. When I select that model and ask a question I can see the model loading in VRAM but the UI never loads any response. I have the latest version of OpenWebUI docker version installed. Other models work fine. Is there some config that I need to change in the settings for OpenWebUI for it to work with deepseek-r1? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robonova-1"&gt; /u/robonova-1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8zs2g/locally_hosted_deepseekr1_and_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8zs2g/locally_hosted_deepseekr1_and_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8zs2g/locally_hosted_deepseekr1_and_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T16:45:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8q0gk</id>
    <title>LLM website that lets you use any model and pay as you go. I canâ€™t remember what itâ€™s called.</title>
    <updated>2025-01-24T07:22:08+00:00</updated>
    <author>
      <name>/u/opelly</name>
      <uri>https://old.reddit.com/user/opelly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The title says it all. I came across a website a while ago that has a ChatGPT like interface, but lets you use all kinds of different models, and pay by the token. I specifically remember a leaderboard that ranks the models by popularity and even showed the number of tokens that each model had generated among all users on the website. I canâ€™t find it for the life of me. Please let me know what site this is if it rings a bell. Thank you!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/opelly"&gt; /u/opelly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8q0gk/llm_website_that_lets_you_use_any_model_and_pay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8q0gk/llm_website_that_lets_you_use_any_model_and_pay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8q0gk/llm_website_that_lets_you_use_any_model_and_pay/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T07:22:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1i90dm0</id>
    <title>Import a prompt/export response with python?</title>
    <updated>2025-01-24T17:09:59+00:00</updated>
    <author>
      <name>/u/Fervolts</name>
      <uri>https://old.reddit.com/user/Fervolts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im quite new using this, I'm new in python too lmao (I mostly program with JS) but I'm trying to use Ollama to make a chatbot for my streams, I use streamer.bot and that's able to run python code so I've been working in a way to get the prompt from a twitch message and then export the response, any ideas or examples I could use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fervolts"&gt; /u/Fervolts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i90dm0/import_a_promptexport_response_with_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i90dm0/import_a_promptexport_response_with_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i90dm0/import_a_promptexport_response_with_python/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T17:09:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8qckb</id>
    <title>List of top Open Source Chat UI for ollama/any LLM in general. (community edition)</title>
    <updated>2025-01-24T07:47:35+00:00</updated>
    <author>
      <name>/u/VisibleLawfulness246</name>
      <uri>https://old.reddit.com/user/VisibleLawfulness246</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey community, I am trying to compile a list of all the open-source ChatGPT UI. Here is the list from my research. Let's make this thread helpful. tell me- what do you use? and what are the pros and cons along with alternatives your tool of choice.&lt;/p&gt; &lt;p&gt;personally I'm a big fan of Open WebUI but I'm looking to try out what all is new in the community,&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open WebUI&lt;/li&gt; &lt;li&gt;LibreChat&lt;/li&gt; &lt;li&gt;anythingLLM&lt;/li&gt; &lt;li&gt;GPT4all&lt;/li&gt; &lt;li&gt;oobabooga&lt;/li&gt; &lt;li&gt;verba&lt;/li&gt; &lt;li&gt;dify&lt;/li&gt; &lt;li&gt;SillyTavern&lt;/li&gt; &lt;li&gt;Danswer&lt;/li&gt; &lt;li&gt;Lobe Ui&lt;/li&gt; &lt;li&gt;hugging face chat-Ui&lt;/li&gt; &lt;li&gt;kobold Cpp/ for from llama cpp&lt;/li&gt; &lt;li&gt;private gpt&lt;/li&gt; &lt;li&gt;serge chat&lt;/li&gt; &lt;li&gt;JanHQ&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What am I missing from this list? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VisibleLawfulness246"&gt; /u/VisibleLawfulness246 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8qckb/list_of_top_open_source_chat_ui_for_ollamaany_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8qckb/list_of_top_open_source_chat_ui_for_ollamaany_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8qckb/list_of_top_open_source_chat_ui_for_ollamaany_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T07:47:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8bhf3</id>
    <title>Upgraded!</title>
    <updated>2025-01-23T19:19:46+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i8bhf3/upgraded/"&gt; &lt;img alt="Upgraded!" src="https://preview.redd.it/8qdjrpbxmsee1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e72aeebb6bad9946b60f373d5e38bdfa97f84a8" title="Upgraded!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8qdjrpbxmsee1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8bhf3/upgraded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8bhf3/upgraded/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-23T19:19:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8r971</id>
    <title>I want to try to replicate the RAG functionality similar to LM Studio using open source tools. Any ideas on where to start?</title>
    <updated>2025-01-24T08:57:53+00:00</updated>
    <author>
      <name>/u/ikmalsaid</name>
      <uri>https://old.reddit.com/user/ikmalsaid</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ikmalsaid"&gt; /u/ikmalsaid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8r971/i_want_to_try_to_replicate_the_rag_functionality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8r971/i_want_to_try_to_replicate_the_rag_functionality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8r971/i_want_to_try_to_replicate_the_rag_functionality/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T08:57:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8yiae</id>
    <title>Lovable, Boltâ€¦ self-host alternatives</title>
    <updated>2025-01-24T15:51:56+00:00</updated>
    <author>
      <name>/u/productboy</name>
      <uri>https://old.reddit.com/user/productboy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Before I build it myself &lt;em&gt;please&lt;/em&gt; tell me someone here has an open source alternative to Lovable or Bolt thatâ€™s as good, that I can self-host.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/productboy"&gt; /u/productboy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8yiae/lovable_bolt_selfhost_alternatives/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8yiae/lovable_bolt_selfhost_alternatives/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8yiae/lovable_bolt_selfhost_alternatives/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T15:51:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i900ls</id>
    <title>Would 2 x 3090 double my speed?</title>
    <updated>2025-01-24T16:55:23+00:00</updated>
    <author>
      <name>/u/PositiveEnergyMatter</name>
      <uri>https://old.reddit.com/user/PositiveEnergyMatter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 3090 now, and 32b models work fine with it. My question is would two 3090s double my speed? I know it makes it so i can load large models, but i am more interested in speed boosts. IS it worth getting a 4090 or a 5090, or am i better off just getting another 3090?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PositiveEnergyMatter"&gt; /u/PositiveEnergyMatter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i900ls/would_2_x_3090_double_my_speed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i900ls/would_2_x_3090_double_my_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i900ls/would_2_x_3090_double_my_speed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T16:55:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i92man</id>
    <title>Llama-Vision training on CPU - I know but has things changed yet?</title>
    <updated>2025-01-24T18:41:55+00:00</updated>
    <author>
      <name>/u/groovy_mentor</name>
      <uri>https://old.reddit.com/user/groovy_mentor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using Llama3.2Vision11B LLM on Ollama to analyze my farm indoor plant growth. I run it in Debian 12 on a HP mini desktop with i5 8500 chip, 32GB RAM with 512 SSD. As my plants grow, I need to train the model to infer better. I have not seen ANY videos or posts on how to train to do this using my CPU based computer. I don't care about time it takes to train, let it be 48 hours. I don't have a lot of pics either to train from, worst case 500 pics. Please help me! Should I look at anything other than Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/groovy_mentor"&gt; /u/groovy_mentor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i92man/llamavision_training_on_cpu_i_know_but_has_things/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i92man/llamavision_training_on_cpu_i_know_but_has_things/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i92man/llamavision_training_on_cpu_i_know_but_has_things/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T18:41:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8yp5m</id>
    <title>Use RAG to chat with PDFs using Deepseek, Langchain and Streamlit</title>
    <updated>2025-01-24T16:00:22+00:00</updated>
    <author>
      <name>/u/Special_Community179</name>
      <uri>https://old.reddit.com/user/Special_Community179</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i8yp5m/use_rag_to_chat_with_pdfs_using_deepseek/"&gt; &lt;img alt="Use RAG to chat with PDFs using Deepseek, Langchain and Streamlit" src="https://external-preview.redd.it/Kb3DxleokIDZE5pe9gpJx8YJHS033-33bKrc01NRHGA.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db0276a28b5d9b2ec24cad65f6aa21e323d49169" title="Use RAG to chat with PDFs using Deepseek, Langchain and Streamlit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special_Community179"&gt; /u/Special_Community179 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=M6vZ6b75p9k&amp;amp;list=PLp01ObP3udmq2quR-RfrX4zNut_t_kNot"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8yp5m/use_rag_to_chat_with_pdfs_using_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8yp5m/use_rag_to_chat_with_pdfs_using_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T16:00:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8qav7</id>
    <title>A list of all the top Open Source Chat UI for ollama/any LLM in general. (community edition)</title>
    <updated>2025-01-24T07:43:58+00:00</updated>
    <author>
      <name>/u/VisibleLawfulness246</name>
      <uri>https://old.reddit.com/user/VisibleLawfulness246</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i8qav7/a_list_of_all_the_top_open_source_chat_ui_for/"&gt; &lt;img alt="A list of all the top Open Source Chat UI for ollama/any LLM in general. (community edition)" src="https://b.thumbs.redditmedia.com/2mlU8Ofw8DVHHHmMnu33kSfWqc1LdBv2Whegj1zwhkY.jpg" title="A list of all the top Open Source Chat UI for ollama/any LLM in general. (community edition)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's my list right now&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open Web UI&lt;/li&gt; &lt;li&gt;LibreChat&lt;/li&gt; &lt;li&gt;anythingLLM&lt;/li&gt; &lt;li&gt;GPT4all&lt;/li&gt; &lt;li&gt;oobabooga&lt;/li&gt; &lt;li&gt;verba&lt;/li&gt; &lt;li&gt;dify&lt;/li&gt; &lt;li&gt;SillyTavern&lt;/li&gt; &lt;li&gt;Danswer&lt;/li&gt; &lt;li&gt;Lobe Ui&lt;/li&gt; &lt;li&gt;hugging face chat-Ui&lt;/li&gt; &lt;li&gt;kobold Cpp/ for from llama cpp&lt;/li&gt; &lt;li&gt;private gpt&lt;/li&gt; &lt;li&gt;serge chat&lt;/li&gt; &lt;li&gt;JanHQ&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What am I missing from this list?&lt;/p&gt; &lt;p&gt;adding this image from my research&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VisibleLawfulness246"&gt; /u/VisibleLawfulness246 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8qav7/a_list_of_all_the_top_open_source_chat_ui_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8qav7/a_list_of_all_the_top_open_source_chat_ui_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8qav7/a_list_of_all_the_top_open_source_chat_ui_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T07:43:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8n38z</id>
    <title>Coming soon: 100% Local Video Understanding Engine (an open-source project that can classify, caption, transcribe, and understand any video on your local device)</title>
    <updated>2025-01-24T04:17:00+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i8n38z/coming_soon_100_local_video_understanding_engine/"&gt; &lt;img alt="Coming soon: 100% Local Video Understanding Engine (an open-source project that can classify, caption, transcribe, and understand any video on your local device)" src="https://external-preview.redd.it/MDIwNzQ0ZzNidmVlMe21Biif0sGFU8GTsH3N7D_CJugYvIxsEVZ-nvrUed0U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8fa43786541e784dc36e99b0ae3b5e02c8a90ea9" title="Coming soon: 100% Local Video Understanding Engine (an open-source project that can classify, caption, transcribe, and understand any video on your local device)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4sh274g3bvee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i8n38z/coming_soon_100_local_video_understanding_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i8n38z/coming_soon_100_local_video_understanding_engine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-24T04:17:00+00:00</published>
  </entry>
</feed>
