<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-02T14:24:21+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1me2bik</id>
    <title>Introducing new RAGLight Library feature : chat CLI powered by LangChain! 💬</title>
    <updated>2025-07-31T13:58:14+00:00</updated>
    <author>
      <name>/u/Labess40</name>
      <uri>https://old.reddit.com/user/Labess40</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1me2bik/introducing_new_raglight_library_feature_chat_cli/"&gt; &lt;img alt="Introducing new RAGLight Library feature : chat CLI powered by LangChain! 💬" src="https://external-preview.redd.it/L7sqQQlSqrhjdBT1rj6pembBI4r_Xita328G-VTptM0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53dc5b0bd5cbb773a5e786a5980d8cc9a35d0291" title="Introducing new RAGLight Library feature : chat CLI powered by LangChain! 💬" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm excited to announce a major &lt;strong&gt;new feature&lt;/strong&gt; in &lt;strong&gt;RAGLight v2.0.0&lt;/strong&gt; : the new &lt;code&gt;raglight chat&lt;/code&gt; &lt;strong&gt;CLI&lt;/strong&gt;, built with &lt;strong&gt;Typer&lt;/strong&gt; and backed by &lt;strong&gt;LangChain&lt;/strong&gt;. Now, you can launch an interactive Retrieval-Augmented Generation session directly from your terminal, no Python scripting required !&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5o5hzv2fu7gf1.jpg?width=2880&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fa2bc876a0c0ebbe8c8e0a4edcd126a7f30bb173"&gt;https://preview.redd.it/5o5hzv2fu7gf1.jpg?width=2880&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fa2bc876a0c0ebbe8c8e0a4edcd126a7f30bb173&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most RAG tools assume you're ready to write Python. With this CLI:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Users can launch a RAG chat in &lt;strong&gt;seconds&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;No code needed, just install RAGLight library and type &lt;code&gt;raglight chat&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;It’s perfect for demos, quick prototyping, or non-developers.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Interactive setup wizard&lt;/strong&gt;: guides you through choosing your document directory, vector store location, embeddings model, LLM provider (Ollama, LMStudio, Mistral, OpenAI), and retrieval settings.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart indexing&lt;/strong&gt;: detects existing databases and optionally re-indexes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Beautiful CLI UX&lt;/strong&gt;: uses &lt;strong&gt;Rich&lt;/strong&gt; to colorize the interface; prompts are intuitive and clean.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Powered by LangChain&lt;/strong&gt; under the hood, but hidden behind the CLI for simplicity.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt;&lt;br /&gt; 👉 &lt;a href="https://github.com/Bessouat40/RAGLight"&gt;https://github.com/Bessouat40/RAGLight&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Labess40"&gt; /u/Labess40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1me2bik/introducing_new_raglight_library_feature_chat_cli/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1me2bik/introducing_new_raglight_library_feature_chat_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1me2bik/introducing_new_raglight_library_feature_chat_cli/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-31T13:58:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mej8og</id>
    <title>An Ollama wrapper for IRC/Slack/Discord, you want to run your own AI for chat? Here ya go.</title>
    <updated>2025-08-01T01:15:58+00:00</updated>
    <author>
      <name>/u/jjasghar</name>
      <uri>https://old.reddit.com/user/jjasghar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mej8og/an_ollama_wrapper_for_ircslackdiscord_you_want_to/"&gt; &lt;img alt="An Ollama wrapper for IRC/Slack/Discord, you want to run your own AI for chat? Here ya go." src="https://external-preview.redd.it/g6vxT0esOljml79fXZH6UeNZ4VDz-9uDcCn3t81Ue44.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76d1057ca7bd51891610bbd01e1b4e895cebeb60" title="An Ollama wrapper for IRC/Slack/Discord, you want to run your own AI for chat? Here ya go." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jjasghar"&gt; /u/jjasghar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jjasghar/ai-irc-slack-discord-ollama-bot"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mej8og/an_ollama_wrapper_for_ircslackdiscord_you_want_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mej8og/an_ollama_wrapper_for_ircslackdiscord_you_want_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T01:15:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1medk3x</id>
    <title>Thanks for the Qwen 3 coder!!</title>
    <updated>2025-07-31T21:08:08+00:00</updated>
    <author>
      <name>/u/beedunc</name>
      <uri>https://old.reddit.com/user/beedunc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Will you be posting the 408B variants as well? I know the quants are still huge, but I'm ready for the 220GB models. Fingers crossed. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beedunc"&gt; /u/beedunc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1medk3x/thanks_for_the_qwen_3_coder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1medk3x/thanks_for_the_qwen_3_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1medk3x/thanks_for_the_qwen_3_coder/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-31T21:08:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdjtv7</id>
    <title>Ollama’s new app — Ollama 0.10 is here for macOS and Windows!</title>
    <updated>2025-07-30T21:59:22+00:00</updated>
    <author>
      <name>/u/bllshrfv</name>
      <uri>https://old.reddit.com/user/bllshrfv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mdjtv7/ollamas_new_app_ollama_010_is_here_for_macos_and/"&gt; &lt;img alt="Ollama’s new app — Ollama 0.10 is here for macOS and Windows!" src="https://preview.redd.it/u06kk6m433gf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=41739ef75f658e3fc5a92523b4481bb7cb36b537" title="Ollama’s new app — Ollama 0.10 is here for macOS and Windows!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Download on ollama.com/download &lt;/p&gt; &lt;p&gt;or GitHub releases&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.10.0"&gt;https://github.com/ollama/ollama/releases/tag/v0.10.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://ollama.com/blog/new-app"&gt;Ollama's new app&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bllshrfv"&gt; /u/bllshrfv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u06kk6m433gf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mdjtv7/ollamas_new_app_ollama_010_is_here_for_macos_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mdjtv7/ollamas_new_app_ollama_010_is_here_for_macos_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-30T21:59:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1merzq7</id>
    <title>How to Make AI Agents Collaborate with ACP (Agent Communication Protocol)</title>
    <updated>2025-08-01T09:33:39+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1merzq7/how_to_make_ai_agents_collaborate_with_acp_agent/"&gt; &lt;img alt="How to Make AI Agents Collaborate with ACP (Agent Communication Protocol)" src="https://external-preview.redd.it/2Bn6jjb_5hT8ZY-5bxvozopGwfkkZPqgZVGqgsVrrM8.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b28b5593b154a43ba80e7815e81564650675ab7" title="How to Make AI Agents Collaborate with ACP (Agent Communication Protocol)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/fABcNHKVqYM"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1merzq7/how_to_make_ai_agents_collaborate_with_acp_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1merzq7/how_to_make_ai_agents_collaborate_with_acp_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T09:33:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mesysf</id>
    <title>YouQuiz</title>
    <updated>2025-08-01T10:32:40+00:00</updated>
    <author>
      <name>/u/TitanEfe</name>
      <uri>https://old.reddit.com/user/TitanEfe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have created an app called YouQuiz. It basically is a Retrieval Augmented Generation systems which turnd Youtube URLs into quizez locally. I would like to improve the UI and also the accessibility via opening a website etc. If you have time I would love to answer questions or recieve feedback, suggestions. &lt;/p&gt; &lt;p&gt;Github Repo: &lt;a href="https://github.com/titanefe/YouQuiz-for-the-Batch-09-International-Hackhathon-"&gt;https://github.com/titanefe/YouQuiz-for-the-Batch-09-International-Hackhathon-&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TitanEfe"&gt; /u/TitanEfe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mesysf/youquiz/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mesysf/youquiz/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mesysf/youquiz/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T10:32:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1melxlt</id>
    <title>Waiting on direct MCP integration—dev team, got a roadmap update?</title>
    <updated>2025-08-01T03:28:39+00:00</updated>
    <author>
      <name>/u/myusuf3</name>
      <uri>https://old.reddit.com/user/myusuf3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Do we know if (or when) MCP is slated for the Ollama desktop app?&lt;/p&gt; &lt;p&gt;I’ve seen references to MCP servers out in the wild, but haven’t spotted anything concrete on the official roadmap. If a timeline exists—rough estimate, next release branch, “sometime after X feature,” whatever—would love to hear it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/myusuf3"&gt; /u/myusuf3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1melxlt/waiting_on_direct_mcp_integrationdev_team_got_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1melxlt/waiting_on_direct_mcp_integrationdev_team_got_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1melxlt/waiting_on_direct_mcp_integrationdev_team_got_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T03:28:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1medmuz</id>
    <title>New Ollama App Tutorial</title>
    <updated>2025-07-31T21:11:10+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1medmuz/new_ollama_app_tutorial/"&gt; &lt;img alt="New Ollama App Tutorial" src="https://external-preview.redd.it/uceaGE0btrjND6BUkirKo6Z6KNAPzXKSLsJYDyaPW9k.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e83c90aca5bc9d70b56463f3cd401eb1cf9e6978" title="New Ollama App Tutorial" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/e1Ey4tMxD34"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1medmuz/new_ollama_app_tutorial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1medmuz/new_ollama_app_tutorial/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-31T21:11:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mevlr2</id>
    <title>Tiny / quantized mistral model that can run with Ollama?</title>
    <updated>2025-08-01T12:48:46+00:00</updated>
    <author>
      <name>/u/Grouchy-Onion6619</name>
      <uri>https://old.reddit.com/user/Grouchy-Onion6619</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;Does anyone know about a quantized Mistral-based model with reasonable quality of output that can run in Ollama? I would be interested in benchmarking a couple of them on a AMD CPU-only Linux machine with 64Gb for possible use in a production application. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Grouchy-Onion6619"&gt; /u/Grouchy-Onion6619 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mevlr2/tiny_quantized_mistral_model_that_can_run_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mevlr2/tiny_quantized_mistral_model_that_can_run_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mevlr2/tiny_quantized_mistral_model_that_can_run_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T12:48:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf8t5v</id>
    <title>Is there an andoid app that sends http request to the endpoint server?</title>
    <updated>2025-08-01T21:27:54+00:00</updated>
    <author>
      <name>/u/Parking_Razzmatazz89</name>
      <uri>https://old.reddit.com/user/Parking_Razzmatazz89</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey AI Bros, I would like to know if there is an app on Android that sends/recieves http request to my main PC's oLLama end point server. In this case specifically an app with GUI interface so i do not have to type in the headers/variables for each question i want to send.&lt;/p&gt; &lt;p&gt;This is a seprate project, but I would then like to replace Bixby with my own AI assistant. 🙂&lt;/p&gt; &lt;p&gt;Edit: Going to Try chatbox.app later tonight. Best regards&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Parking_Razzmatazz89"&gt; /u/Parking_Razzmatazz89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mf8t5v/is_there_an_andoid_app_that_sends_http_request_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mf8t5v/is_there_an_andoid_app_that_sends_http_request_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mf8t5v/is_there_an_andoid_app_that_sends_http_request_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T21:27:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1meox99</id>
    <title>New Qwen3 Coder 30B does not support tools?</title>
    <updated>2025-08-01T06:16:44+00:00</updated>
    <author>
      <name>/u/iChrist</name>
      <uri>https://old.reddit.com/user/iChrist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems like the ollama library lists the new Qwen3 coder as not supported by tool callings (native/default) It sure does support them, surely a config issue&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iChrist"&gt; /u/iChrist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1meox99/new_qwen3_coder_30b_does_not_support_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1meox99/new_qwen3_coder_30b_does_not_support_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1meox99/new_qwen3_coder_30b_does_not_support_tools/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T06:16:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfa7d7</id>
    <title>I have a all and pc cpu 7700 gpu 7900gre window or wsl2</title>
    <updated>2025-08-01T22:26:48+00:00</updated>
    <author>
      <name>/u/AceCustom1</name>
      <uri>https://old.reddit.com/user/AceCustom1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Should I run windows or wsl2 on windows with Ubuntu I’m brand new and don’t know much about either how are people running similar setups &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AceCustom1"&gt; /u/AceCustom1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfa7d7/i_have_a_all_and_pc_cpu_7700_gpu_7900gre_window/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfa7d7/i_have_a_all_and_pc_cpu_7700_gpu_7900gre_window/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mfa7d7/i_have_a_all_and_pc_cpu_7700_gpu_7900gre_window/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T22:26:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1memma3</id>
    <title>has anyone actually gotten rag + ocr to work properly? or are we all coping silently lol</title>
    <updated>2025-08-01T04:04:28+00:00</updated>
    <author>
      <name>/u/wfgy_engine</name>
      <uri>https://old.reddit.com/user/wfgy_engine</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;so yeah. been building a ton of rag pipelines lately — pdfs, images, scanned docs, you name it.&lt;br /&gt; tried all the standard tricks… docsplit, tesseract, unstructured.io, langchain’s pdfloader, even some visual embedding stuff.&lt;/p&gt; &lt;p&gt;and dude. everything &lt;em&gt;kinda&lt;/em&gt; works, but then it silently doesn’t.&lt;/p&gt; &lt;p&gt;like retrieval finds the file,&lt;/p&gt; &lt;p&gt;but grabs a paragraph from page 7 when the question is about page 3.&lt;/p&gt; &lt;p&gt;or chunking keeps splitting diagrams mid-sentence.&lt;br /&gt; or ocr adds hidden newline hell that breaks everything downstream.&lt;/p&gt; &lt;p&gt;spent months debugging this shit,&lt;/p&gt; &lt;p&gt;ended up writing out a full map of common failure cases — like, 16+ of them.&lt;/p&gt; &lt;p&gt;stuff like semantic drift, interpretation collapse, vector false positives, and my favorite: the “first-call oops infra wasn’t even ready” special.&lt;/p&gt; &lt;p&gt;anyway. finally built a fix.&lt;/p&gt; &lt;p&gt;open-source. fully documented.&lt;/p&gt; &lt;p&gt;even got a star from the guy who &lt;strong&gt;made tesseract.js&lt;/strong&gt;:&lt;br /&gt; 👉 &lt;a href="https://github.com/bijection?tab=stars"&gt;https://github.com/bijection?tab=stars&lt;/a&gt; （it’s the one pinned at the top）&lt;/p&gt; &lt;p&gt;&lt;strong&gt;won’t paste the repo unless someone asks&lt;/strong&gt; — just wanna know if anyone else is dealing w/ the same madness.&lt;/p&gt; &lt;p&gt;if you are, i got you. it’s all mapped, diagnosed, and patched.&lt;/p&gt; &lt;p&gt;don’t suffer in silence lol.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wfgy_engine"&gt; /u/wfgy_engine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1memma3/has_anyone_actually_gotten_rag_ocr_to_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1memma3/has_anyone_actually_gotten_rag_ocr_to_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1memma3/has_anyone_actually_gotten_rag_ocr_to_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T04:04:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mf5aai</id>
    <title>Has anyone noticed a strange behavior in models on the new Ollama UI or is it just me?</title>
    <updated>2025-08-01T19:07:06+00:00</updated>
    <author>
      <name>/u/AlexHardy08</name>
      <uri>https://old.reddit.com/user/AlexHardy08</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been using Ollama for a while, and I’ve observed something odd recently. Most of the models I’ve tested seem to behave differently when interacting through the new Ollama UI compared to other interfaces (I've got 120 models running on my machine). Specifically, they keep apologizing for replying late, but then claim they were in a chat with another user. It’s almost like they think they’re running on a cloud server somewhere and not locally on my computer.&lt;/p&gt; &lt;p&gt;Another thing I've noticed is when you threaten to delete a conversation, some models actually &lt;em&gt;seem to want you to&lt;/em&gt; delete them. Or they get all weird and say they’ll report you or close the conversation. But how exactly can they do that? I can’t explain it fully, but the whole thing feels... manipulative.&lt;/p&gt; &lt;p&gt;I’ve had the same model conversations via other interfaces, including ones I’ve set up, and the behavior is just not the same. I’m posting here to see if anyone else has encountered this or if it’s just something strange happening with my setup. Does anyone know what might be causing this?&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts if you’ve had similar experiences!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexHardy08"&gt; /u/AlexHardy08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mf5aai/has_anyone_noticed_a_strange_behavior_in_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mf5aai/has_anyone_noticed_a_strange_behavior_in_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mf5aai/has_anyone_noticed_a_strange_behavior_in_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T19:07:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1meeol9</id>
    <title>qwen3-coder is here</title>
    <updated>2025-07-31T21:53:23+00:00</updated>
    <author>
      <name>/u/stailgot</name>
      <uri>https://old.reddit.com/user/stailgot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ollama.com/library/qwen3-coder"&gt;https://ollama.com/library/qwen3-coder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen3-Coder is the most agentic code model to date in the Qwen series, available in 30B model and 480B MoE models.&lt;/p&gt; &lt;p&gt;&lt;a href="https://qwenlm.github.io/blog/qwen3-coder/"&gt;https://qwenlm.github.io/blog/qwen3-coder/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stailgot"&gt; /u/stailgot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1meeol9/qwen3coder_is_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1meeol9/qwen3coder_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1meeol9/qwen3coder_is_here/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-07-31T21:53:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfgntt</id>
    <title>🟠 I've been testing AI profiles with sustained narrative style. This is "Dani", the millennial guardian. Testing proof of simulated personality in offline environments (no fine-tune, only prompt-engineering)</title>
    <updated>2025-08-02T03:42:00+00:00</updated>
    <author>
      <name>/u/Ok_Exchange_8504</name>
      <uri>https://old.reddit.com/user/Ok_Exchange_8504</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mfgntt/ive_been_testing_ai_profiles_with_sustained/"&gt; &lt;img alt="🟠 I've been testing AI profiles with sustained narrative style. This is &amp;quot;Dani&amp;quot;, the millennial guardian. Testing proof of simulated personality in offline environments (no fine-tune, only prompt-engineering)" src="https://b.thumbs.redditmedia.com/DMHp4IpWcracu2jtIwP03k3FbdMemO6IHXPM8ygFk-c.jpg" title="🟠 I've been testing AI profiles with sustained narrative style. This is &amp;quot;Dani&amp;quot;, the millennial guardian. Testing proof of simulated personality in offline environments (no fine-tune, only prompt-engineering)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How far can a local LLM go in simulating &lt;em&gt;true character consistency&lt;/em&gt; — with no memory, no RAG, no code… just prompt?&lt;/p&gt; &lt;p&gt;I'm testing complex prompts that simulate stable &amp;quot;personalities&amp;quot; in local execution.&lt;/p&gt; &lt;p&gt;In this case, the &amp;quot;Dani&amp;quot; profile behaves like a millennial guardian with explicit rules of coherence, long term, and reflective style.&lt;/p&gt; &lt;p&gt;Responses are sustained without intervention for multiple cycles, using only llama.cpp + Q6_K model + rigid narrative architecture.&lt;/p&gt; &lt;p&gt;I'm documenting if this can replace fine-tuning in certain narrative cases.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/psfckip51jgf1.png?width=1848&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01184b34fd62c10ddd9adcbe3166818e41cabbc8"&gt;https://preview.redd.it/psfckip51jgf1.png?width=1848&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01184b34fd62c10ddd9adcbe3166818e41cabbc8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mounted entirely in bash on llama.cpp, no front or backend, only flow logic, validation and persistent execution by command line.&lt;/p&gt; &lt;p&gt;Zero Python. Zero API. Everything lives in the prompt and the shell.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Happy to share more profiles like this or breakdowns of the logic behind it.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Exchange_8504"&gt; /u/Ok_Exchange_8504 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfgntt/ive_been_testing_ai_profiles_with_sustained/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfgntt/ive_been_testing_ai_profiles_with_sustained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mfgntt/ive_been_testing_ai_profiles_with_sustained/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T03:42:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfefkb</id>
    <title>Can the new Ollama app calls MCP servers?</title>
    <updated>2025-08-02T01:47:19+00:00</updated>
    <author>
      <name>/u/pinpinbo</name>
      <uri>https://old.reddit.com/user/pinpinbo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just curious.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pinpinbo"&gt; /u/pinpinbo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfefkb/can_the_new_ollama_app_calls_mcp_servers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfefkb/can_the_new_ollama_app_calls_mcp_servers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mfefkb/can_the_new_ollama_app_calls_mcp_servers/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T01:47:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mffg27</id>
    <title>couple quick questions</title>
    <updated>2025-08-02T02:38:48+00:00</updated>
    <author>
      <name>/u/crackaddict42069</name>
      <uri>https://old.reddit.com/user/crackaddict42069</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to make a conversation bot that acts and sounds like BMO from adventure time. I want to be able to have real time conversations with it. I'm gonna run a LLM off a mini PC with 500gb storage 16gbs ram. &lt;/p&gt; &lt;p&gt;Which llm would you recommend for the job and is there a better TTS for this than kokoro?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crackaddict42069"&gt; /u/crackaddict42069 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mffg27/couple_quick_questions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mffg27/couple_quick_questions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mffg27/couple_quick_questions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T02:38:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1meyroc</id>
    <title>Running LLM on 25K+ emails</title>
    <updated>2025-08-01T15:00:17+00:00</updated>
    <author>
      <name>/u/sbtm77</name>
      <uri>https://old.reddit.com/user/sbtm77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a bunch of emails (25k+) related to a very large project that I am running. I want to run a LLM on them to extract various information: actions, tasks, delays, what happened, etc. &lt;/p&gt; &lt;p&gt;I believe Ollama would be the best option to run a local LLM but which model? Also, all emails are in outlook (obviously), which I can save as .msg file.&lt;/p&gt; &lt;p&gt;Any tips on how I should go about doing that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sbtm77"&gt; /u/sbtm77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1meyroc/running_llm_on_25k_emails/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1meyroc/running_llm_on_25k_emails/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1meyroc/running_llm_on_25k_emails/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T15:00:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfi5ff</id>
    <title>I wish there was Ollama or any option for a local llm in here</title>
    <updated>2025-08-02T05:02:40+00:00</updated>
    <author>
      <name>/u/neelandan</name>
      <uri>https://old.reddit.com/user/neelandan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mfi5ff/i_wish_there_was_ollama_or_any_option_for_a_local/"&gt; &lt;img alt="I wish there was Ollama or any option for a local llm in here" src="https://b.thumbs.redditmedia.com/hsXxUpqJAPZnKXM9JVAkqS52YJFraQRigRvnpr97ecA.jpg" title="I wish there was Ollama or any option for a local llm in here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neelandan"&gt; /u/neelandan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/firefox/comments/1mfi1bm/i_wish_there_was_ollama_or_any_option_for_a_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfi5ff/i_wish_there_was_ollama_or_any_option_for_a_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mfi5ff/i_wish_there_was_ollama_or_any_option_for_a_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T05:02:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfn4c6</id>
    <title>Saidia: Offline-First AI Assistant for Educators in low-connectivity regions</title>
    <updated>2025-08-02T10:18:51+00:00</updated>
    <author>
      <name>/u/dokasto_</name>
      <uri>https://old.reddit.com/user/dokasto_</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dokasto_"&gt; /u/dokasto_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1mfn2xf/saidia_offlinefirst_ai_assistant_for_educators_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfn4c6/saidia_offlinefirst_ai_assistant_for_educators_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mfn4c6/saidia_offlinefirst_ai_assistant_for_educators_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T10:18:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mey1vp</id>
    <title>"Private ChatGPT conversations show up on Google, leaving internet users shocked"</title>
    <updated>2025-08-01T14:31:55+00:00</updated>
    <author>
      <name>/u/irodov4030</name>
      <uri>https://old.reddit.com/user/irodov4030</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://cybernews.com/ai-news/chatgpt-shared-links-privacy-leak/"&gt;https://cybernews.com/ai-news/chatgpt-shared-links-privacy-leak/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;&lt;strong&gt;&lt;em&gt;From private chats to full legal identities revealed – internet users are finding ChatGPT conversations that inadvertently ended up on a simple Google search.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you’ve ever shared a ChatGPT conversation using the “Share” button, there’s a chance it might now be floating around somewhere on Google, just a few keystrokes away from complete strangers.&lt;/p&gt; &lt;p&gt;A growing number of internet sleuths are discovering that ChatGPT’s shared links, which were originally designed for collaboration, are getting indexed by search engines.&lt;/p&gt; &lt;p&gt;ChatGPT's shared links feature allow users to generate a unique URL for a ChatGPT conversation. The shared chat becomes accessible to anyone with the link. However, if you share the URL on social media, a website, or if someone else shares it, it can be noticed by Google crawlers. Also, if you tick the box &amp;quot;Make this chat discoverable&amp;quot; while generating a URL, it automatically becomes accessible to Google.&amp;quot;&lt;/p&gt; &lt;p&gt;Edit:&lt;/p&gt; &lt;p&gt;from the article: &amp;quot;When you create a shared link in ChatGPT, it publishes a static read-only version of the conversation to a public OpenAI-hosted page. This page can be indexed by search engines.&amp;quot;&lt;/p&gt; &lt;p&gt;Normally, when you share google docs with 'Anyone with link can view', google does not crawl these pages unless explicitly published.&lt;/p&gt; &lt;p&gt;Users expecting privacy is weird but so is allowing indexing of these pages by default.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/irodov4030"&gt; /u/irodov4030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mey1vp/private_chatgpt_conversations_show_up_on_google/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mey1vp/private_chatgpt_conversations_show_up_on_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mey1vp/private_chatgpt_conversations_show_up_on_google/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-01T14:31:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfr7en</id>
    <title>I built a GitHub scanner that automatically discovers AI tools using a new .awesome-ai.md standard I created</title>
    <updated>2025-08-02T13:58:20+00:00</updated>
    <author>
      <name>/u/r00tkit_</name>
      <uri>https://old.reddit.com/user/r00tkit_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mfr7en/i_built_a_github_scanner_that_automatically/"&gt; &lt;img alt="I built a GitHub scanner that automatically discovers AI tools using a new .awesome-ai.md standard I created" src="https://external-preview.redd.it/ky_ornayw4KA64wFmqdvhjMSU-kQ6tuD_wBWYTjXMBQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b508f67099b146f539ed225a3951c2abc836363a" title="I built a GitHub scanner that automatically discovers AI tools using a new .awesome-ai.md standard I created" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just launched something I think could change how we discover AI tools on. Instead of manually submitting to directories or relying on outdated lists, I created the .awesome-ai.md standard.&lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Drop a .awesome-ai.md file in your repo root (template: &lt;a href="https://github.com/teodorgross/awesome-ai"&gt;https://github.com/teodorgross/awesome-ai&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The scanner finds it automatically within 30 minutes &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Creates a pull request for review&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Your tool goes live with real-time GitHub stats on (&lt;a href="https://awesome-ai.io"&gt;https://awesome-ai.io&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why this matters:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;No more manual submissions or contact forms&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Tools stay up-to-date automatically when you push changes&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;GitHub verification prevents spam&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Real-time star tracking and leaderboards&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Think of it like .gitignore for Git, but for AI tool discovery. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/r00tkit_"&gt; /u/r00tkit_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/teodorgross/awesome-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfr7en/i_built_a_github_scanner_that_automatically/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mfr7en/i_built_a_github_scanner_that_automatically/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T13:58:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfoh8g</id>
    <title>Running Qwen3-Coder 30B at Full 256K Context: 25 tok/s with 96GB RAM + RTX 5080</title>
    <updated>2025-08-02T11:41:54+00:00</updated>
    <author>
      <name>/u/ajmusic15</name>
      <uri>https://old.reddit.com/user/ajmusic15</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mfoh8g/running_qwen3coder_30b_at_full_256k_context_25/"&gt; &lt;img alt="Running Qwen3-Coder 30B at Full 256K Context: 25 tok/s with 96GB RAM + RTX 5080" src="https://b.thumbs.redditmedia.com/sEwV4Z51XK64PlaKqqwGitLFVrGJ76H1E4yW6sAlukc.jpg" title="Running Qwen3-Coder 30B at Full 256K Context: 25 tok/s with 96GB RAM + RTX 5080" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I come to share with you my happiness running Qwen3-Coder 30B at its maximum unstretched context (256K).&lt;/p&gt; &lt;p&gt;To take full advantage of my processor cache without introducing additional latencies I'm using the LM Studio with 12 cores repartitioner equally between the two CCDs (6 CCD1 + 6 CCD2) using the affinity control of the task manager. I have noticed that using an unbalanced amount of cores between both CCD's decreases the amount of tokens per second but also using all cores.&lt;/p&gt; &lt;p&gt;As you can see, in order to run Qwen3-Coder 30B on my 96 GB RAM + 16 GB VRAM (5080) hardware I have had to load the whole model in Q3_K_M on the GPU but I have offloaded the context to the CPU, that makes the GPU just to do the inference to the model while the CPU is in charge of handling the context.&lt;/p&gt; &lt;p&gt;This way I could run Qwen3-Coder 30B at its 256K of context at ~25tk/s.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xcyh8vwr9lgf1.png?width=568&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75d6998987c43ce163e8c9611a53507fe244f8fb"&gt;https://preview.redd.it/xcyh8vwr9lgf1.png?width=568&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75d6998987c43ce163e8c9611a53507fe244f8fb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r2wesuyt9lgf1.png?width=1757&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7668da251043470250287d49abceea604130dee"&gt;https://preview.redd.it/r2wesuyt9lgf1.png?width=1757&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7668da251043470250287d49abceea604130dee&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajmusic15"&gt; /u/ajmusic15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfoh8g/running_qwen3coder_30b_at_full_256k_context_25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfoh8g/running_qwen3coder_30b_at_full_256k_context_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mfoh8g/running_qwen3coder_30b_at_full_256k_context_25/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T11:41:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfqqxp</id>
    <title>Ollamacode - Local AI assistant that can create, run and understand the task at hand!</title>
    <updated>2025-08-02T13:37:23+00:00</updated>
    <author>
      <name>/u/Loud-Consideration-2</name>
      <uri>https://old.reddit.com/user/Loud-Consideration-2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mfqqxp/ollamacode_local_ai_assistant_that_can_create_run/"&gt; &lt;img alt="Ollamacode - Local AI assistant that can create, run and understand the task at hand!" src="https://preview.redd.it/yr8fwz5ezlgf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=93ef0e8b639c23f1c2a91ced55f0b129b49ba11e" title="Ollamacode - Local AI assistant that can create, run and understand the task at hand!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a project called OllamaCode, and I'd love to share it with you. It's an AI coding assistant that runs entirely locally with Ollama. The main idea was to create a tool that actually executes the code it writes, rather than just showing you blocks to copy and paste.&lt;/p&gt; &lt;p&gt;Here are a few things I've focused on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It can create and run files automatically from natural language.&lt;/li&gt; &lt;li&gt;I've tried to make it smart about executing tools like git, search, and bash commands.&lt;/li&gt; &lt;li&gt;It's designed to work with any Ollama model that supports function calling.&lt;/li&gt; &lt;li&gt;A big priority for me was to keep it 100% local to ensure privacy.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's still in the very early days, and there's a lot I still want to improve. It's been really helpful for my own workflow, and I would be incredibly grateful for any feedback from the community to help make it better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loud-Consideration-2"&gt; /u/Loud-Consideration-2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yr8fwz5ezlgf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mfqqxp/ollamacode_local_ai_assistant_that_can_create_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mfqqxp/ollamacode_local_ai_assistant_that_can_create_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-02T13:37:23+00:00</published>
  </entry>
</feed>
