<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-05-11T07:23:30+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1kgwapn</id>
    <title>Apple Silicon NPU / Ollama</title>
    <updated>2025-05-07T12:37:57+00:00</updated>
    <author>
      <name>/u/BoandlK</name>
      <uri>https://old.reddit.com/user/BoandlK</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;will it ever be possible to run a model like gemma3:12b on the Apple Silicon integrated NPUs (M1-4)?&lt;/p&gt; &lt;p&gt;Is an NPU even capable of running such a big LLM in theory?&lt;/p&gt; &lt;p&gt;Many thanks in advance.&lt;/p&gt; &lt;p&gt;Bastian&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoandlK"&gt; /u/BoandlK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgwapn/apple_silicon_npu_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kgwapn/apple_silicon_npu_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kgwapn/apple_silicon_npu_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-07T12:37:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1khvz93</id>
    <title>How to make an ai give me the answer i want</title>
    <updated>2025-05-08T17:31:52+00:00</updated>
    <author>
      <name>/u/Scariingella</name>
      <uri>https://old.reddit.com/user/Scariingella</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So i just downloaded a model on ollama and im using anythingllm for the ui. im giving it this prompt so i can create flashcards from a text:&lt;br /&gt; for each page write me flash cards, the flash cards must be like this and without writing question, answer or the page and take the information only from the text that I send you below and format md:&lt;/p&gt; &lt;p&gt;# &amp;quot;question&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;answer&amp;quot;&lt;/p&gt; &lt;p&gt;# &amp;quot;question&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;answer&amp;quot;&lt;/p&gt; &lt;p&gt;text.......&lt;/p&gt; &lt;p&gt;when i run it on claude ai i get the flashcards done correctly but when i do the same prompt in ollama i get bad responded like not all the pages i sent him or not creating question and getting pages wrong and mixing information, what is the problem? im happy to give more context.&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/F13huTaa"&gt;https://pastebin.com/F13huTaa&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Scariingella"&gt; /u/Scariingella &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1khvz93/how_to_make_an_ai_give_me_the_answer_i_want/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1khvz93/how_to_make_an_ai_give_me_the_answer_i_want/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1khvz93/how_to_make_an_ai_give_me_the_answer_i_want/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-08T17:31:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1khsrup</id>
    <title>Evaluating Models</title>
    <updated>2025-05-08T15:21:36+00:00</updated>
    <author>
      <name>/u/marketlurker</name>
      <uri>https://old.reddit.com/user/marketlurker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;with so many models available in &lt;strong&gt;Ollama&lt;/strong&gt;, I asked &lt;strong&gt;Gemini&lt;/strong&gt; the following, &amp;quot;can you contrast the top 15 models that ollama uses? I want to know what each is best at, why it is best and also any weaknesses. add mininum requirements for those models.&amp;quot; Here is what was returned.&lt;/p&gt; &lt;h1&gt;1. Falcon 40B&lt;/h1&gt; &lt;p&gt;· Best At: Multilingual capabilities and broad knowledge applications.&lt;/p&gt; &lt;p&gt;· Why: Large model with a comprehensive understanding of diverse languages.&lt;/p&gt; &lt;p&gt;· Weaknesses: Older architecture compared to newer models like Llama 3 or Mistral.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 40 GB&lt;/p&gt; &lt;h1&gt;2. Llama 3&lt;/h1&gt; &lt;p&gt;· Best At: Cutting-edge performance and efficiency.&lt;/p&gt; &lt;p&gt;· Why: Modern architecture that delivers superior results in various tasks.&lt;/p&gt; &lt;p&gt;· Weaknesses: May require more advanced hardware to operate optimally.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 30 GB&lt;/p&gt; &lt;h1&gt;3. Mistral&lt;/h1&gt; &lt;p&gt;· Best At: Speed and real-time applications.&lt;/p&gt; &lt;p&gt;· Why: Optimized for rapid inference and low latency.&lt;/p&gt; &lt;p&gt;· Weaknesses: Less suited for tasks requiring extensive knowledge bases.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 15 GB&lt;/p&gt; &lt;h1&gt;4. GPT-3&lt;/h1&gt; &lt;p&gt;· Best At: General-purpose tasks and conversational AI.&lt;/p&gt; &lt;p&gt;· Why: Extensive training data and versatile applications.&lt;/p&gt; &lt;p&gt;· Weaknesses: Larger model size can be a constraint in limited hardware environments.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 175 GB&lt;/p&gt; &lt;h1&gt;5. BERT&lt;/h1&gt; &lt;p&gt;· Best At: Natural language understanding.&lt;/p&gt; &lt;p&gt;· Why: Pre-trained on a vast corpus enabling deep comprehension capabilities.&lt;/p&gt; &lt;p&gt;· Weaknesses: Not optimized for generative tasks.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 11 GB&lt;/p&gt; &lt;h1&gt;6. RoBERTa&lt;/h1&gt; &lt;p&gt;· Best At: Enhanced natural language processing.&lt;/p&gt; &lt;p&gt;· Why: Fine-tuned improvements over BERT.&lt;/p&gt; &lt;p&gt;· Weaknesses: Similar to BERT, mainly focused on understanding rather than generation.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 12 GB&lt;/p&gt; &lt;h1&gt;7. XLNet&lt;/h1&gt; &lt;p&gt;· Best At: Contextual language modeling.&lt;/p&gt; &lt;p&gt;· Why: Autoregressive pre-training for better context handling.&lt;/p&gt; &lt;p&gt;· Weaknesses: Complex architecture that might require substantial computational power.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 14 GB&lt;/p&gt; &lt;h1&gt;8. T5&lt;/h1&gt; &lt;p&gt;· Best At: Text generation and transformation tasks.&lt;/p&gt; &lt;p&gt;· Why: Versatile model capable of handling multiple NLP tasks effectively.&lt;/p&gt; &lt;p&gt;· Weaknesses: Memory-intensive model requiring robust hardware.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 16 GB&lt;/p&gt; &lt;h1&gt;9. DistilBERT&lt;/h1&gt; &lt;p&gt;· Best At: Efficient NLP tasks with reduced model size.&lt;/p&gt; &lt;p&gt;· Why: Distilled version of BERT retaining performance with lesser computational demand.&lt;/p&gt; &lt;p&gt;· Weaknesses: Slightly less accurate compared to its larger counterpart.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 6 GB&lt;/p&gt; &lt;h1&gt;10. GPT-2&lt;/h1&gt; &lt;p&gt;· Best At: Generative text applications.&lt;/p&gt; &lt;p&gt;· Why: Well-known for its ability to produce coherent and contextually relevant text.&lt;/p&gt; &lt;p&gt;· Weaknesses: Lower performance compared to GPT-3.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 40 GB&lt;/p&gt; &lt;h1&gt;11. OpenAI Codex&lt;/h1&gt; &lt;p&gt;· Best At: Code generation and programming assistance.&lt;/p&gt; &lt;p&gt;· Why: Specifically trained on a diverse range of coding languages and tasks.&lt;/p&gt; &lt;p&gt;· Weaknesses: Limited in non-coding linguistic tasks.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 20 GB&lt;/p&gt; &lt;h1&gt;12. BART&lt;/h1&gt; &lt;p&gt;· Best At: Text summarization and generation.&lt;/p&gt; &lt;p&gt;· Why: Transformer-based model designed for sequence-to-sequence tasks.&lt;/p&gt; &lt;p&gt;· Weaknesses: Requires substantial computational resources.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 13 GB&lt;/p&gt; &lt;h1&gt;13. ALBERT&lt;/h1&gt; &lt;p&gt;· Best At: Efficient natural language understanding.&lt;/p&gt; &lt;p&gt;· Why: Lightweight model designed to mitigate BERT's limitations.&lt;/p&gt; &lt;p&gt;· Weaknesses: May have reduced performance in highly complex tasks.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 8 GB&lt;/p&gt; &lt;h1&gt;14. Electra&lt;/h1&gt; &lt;p&gt;· Best At: Pre-training efficiency.&lt;/p&gt; &lt;p&gt;· Why: Utilizes a novel approach to pre-training yielding high performance.&lt;/p&gt; &lt;p&gt;· Weaknesses: May require additional fine-tuning for specific tasks.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 10 GB&lt;/p&gt; &lt;h1&gt;15. GPT-Neo&lt;/h1&gt; &lt;p&gt;· Best At: Open-source generative modeling.&lt;/p&gt; &lt;p&gt;· Why: Provides flexibility and customization for various generative tasks.&lt;/p&gt; &lt;p&gt;· Weaknesses: Performance may vary compared to proprietary models.&lt;/p&gt; &lt;p&gt;· Minimum Memory: 12 GB&lt;/p&gt; &lt;p&gt;I would love to hear the thoughts of any of you. I am looking to hear your experience and what you would change.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marketlurker"&gt; /u/marketlurker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1khsrup/evaluating_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1khsrup/evaluating_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1khsrup/evaluating_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-08T15:21:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kht359</id>
    <title>Which Mac?</title>
    <updated>2025-05-08T15:34:34+00:00</updated>
    <author>
      <name>/u/jacob-indie</name>
      <uri>https://old.reddit.com/user/jacob-indie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What kind of (latest) Mac would you buy to run Ollama?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;best overall&lt;/li&gt; &lt;li&gt;best bang for buck - new?&lt;/li&gt; &lt;li&gt;best bang for buck - used?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My guess is it’s all about max ram, but is that true?&lt;/p&gt; &lt;p&gt;(I have lots of small local AI tasks and think about horizontal scaling)&lt;/p&gt; &lt;p&gt;(Bonus: if there is a superior PC option, maybe rack based… I may consider it; energy consumption is less of a concern thanks to lots of solar)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacob-indie"&gt; /u/jacob-indie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kht359/which_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kht359/which_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kht359/which_mac/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-08T15:34:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ki8za7</id>
    <title>Save or auto launch parameter</title>
    <updated>2025-05-09T03:29:09+00:00</updated>
    <author>
      <name>/u/lavoie005</name>
      <uri>https://old.reddit.com/user/lavoie005</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, i want to change the parameter of the ollama llm or launch it before every request&lt;br /&gt; i want to set the num_gpu and num_ctx.&lt;br /&gt; i have check a couple of video put i dont have any idea how to do it.&lt;/p&gt; &lt;p&gt;Thanks for your help&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lavoie005"&gt; /u/lavoie005 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ki8za7/save_or_auto_launch_parameter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ki8za7/save_or_auto_launch_parameter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ki8za7/save_or_auto_launch_parameter/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-09T03:29:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ki2atk</id>
    <title>open source local AI debugger</title>
    <updated>2025-05-08T21:50:43+00:00</updated>
    <author>
      <name>/u/AntelopeEntire9191</name>
      <uri>https://old.reddit.com/user/AntelopeEntire9191</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Ollama community,&lt;/p&gt; &lt;p&gt;I’m Gabriel Cha and an incoming data science @ coluimbia and just wanted to share what I've been building past 2 weeks with my friend Min Kim.&lt;/p&gt; &lt;p&gt;cloi is a local debugging agent that runs in your terminal.&lt;/p&gt; &lt;p&gt;We made cloi because every AI coding tool wants API keys, subscriptions, and your entire codebase uploaded to their servers. cloi, however, runs entirely on your machine. No cloud, no API keys, no subscriptions, no data leaving your system.&lt;/p&gt; &lt;p&gt;The tech is simple: it captures your error context, spins up Ollama locally, generates targeted fixes, and - only with your explicit permission - applies patches to your files. You can swap to any Ollama model you've got installed.&lt;/p&gt; &lt;p&gt;Install Globaly: &lt;code&gt;$ npm install -g&lt;/code&gt; @cloi-ai&lt;code&gt;/cloi&lt;/code&gt;&lt;/p&gt; &lt;p&gt;cloi is open source &lt;a href="https://github.com/cloi-ai/cloi"&gt;https://github.com/cloi-ai/cloi&lt;/a&gt; [243 stars in under 7 days] We want to build something actually helpful and not just another garbage npm package, but if you feel as tho it is, drop the feedback and roast it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AntelopeEntire9191"&gt; /u/AntelopeEntire9191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cloi-ai.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ki2atk/open_source_local_ai_debugger/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ki2atk/open_source_local_ai_debugger/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-08T21:50:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1khshnn</id>
    <title>Best way to run a model for local use? ~20 users at a time.</title>
    <updated>2025-05-08T15:10:06+00:00</updated>
    <author>
      <name>/u/Ttaywsenrak</name>
      <uri>https://old.reddit.com/user/Ttaywsenrak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is probably a question that has been asked before to some degree but here goes -&lt;/p&gt; &lt;p&gt;I am a high school comp-sci teacher, and I am looking to keep my kids as up to speed as possible by integrating AI into some of our projects next year. Mostly for simple things, but I think AI is one of the few things that excites students these days.&lt;/p&gt; &lt;p&gt;The trick is the relatively high cost of having enough tokens for this, and more importantly, the school district hates students having to have accounts for things, which is of course necessary for API keys (plus you have to be 18+ for most of the sign ups anyways).&lt;/p&gt; &lt;p&gt;Now, my classroom lab is pretty decent, all PCs could run a simple model no problem. But school IT has vetoed this because they don't have a way to log everything students ask, so they are worried about kids requesting how to make bombs etc. Compounding this is the fact that students can just download an uncensored model and do whatever they want.&lt;/p&gt; &lt;p&gt;Therefore, my potential requirements would be LAN API requests and logging. I don't necessarily need a GUI, though it would be a nice option as long as logging is available.&lt;/p&gt; &lt;p&gt;To be honest, I don't know a lot about running local LLMs yet, but I am a pretty quick study.&lt;/p&gt; &lt;p&gt;Thanks in advance for any help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ttaywsenrak"&gt; /u/Ttaywsenrak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1khshnn/best_way_to_run_a_model_for_local_use_20_users_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1khshnn/best_way_to_run_a_model_for_local_use_20_users_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1khshnn/best_way_to_run_a_model_for_local_use_20_users_at/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-08T15:10:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kifr4n</id>
    <title>Simple Gradio Chat UI for Ollama and OpenRouter with Streaming Support</title>
    <updated>2025-05-09T11:04:37+00:00</updated>
    <author>
      <name>/u/Illustrious_Low_3411</name>
      <uri>https://old.reddit.com/user/Illustrious_Low_3411</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kifr4n/simple_gradio_chat_ui_for_ollama_and_openrouter/"&gt; &lt;img alt="Simple Gradio Chat UI for Ollama and OpenRouter with Streaming Support" src="https://preview.redd.it/ar5lsxxbmqze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fdba69ba828fe48be844a242a8947faeb41a1629" title="Simple Gradio Chat UI for Ollama and OpenRouter with Streaming Support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m new to LLMs and made a simple Gradio chat UI. It works with local models using Ollama and cloud models via OpenRouter. Has streaming too.&lt;br /&gt; Supports streaming too.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/gurmessa/llm-gradio-chat"&gt;https://github.com/gurmessa/llm-gradio-chat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious_Low_3411"&gt; /u/Illustrious_Low_3411 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ar5lsxxbmqze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kifr4n/simple_gradio_chat_ui_for_ollama_and_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kifr4n/simple_gradio_chat_ui_for_ollama_and_openrouter/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-09T11:04:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kirezi</id>
    <title>Spent the last month building a platform to run visual browser agents with self-hosted models, what do you think?</title>
    <updated>2025-05-09T19:44:30+00:00</updated>
    <author>
      <name>/u/Capable_Cover6678</name>
      <uri>https://old.reddit.com/user/Capable_Cover6678</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently I built a meal assistant that used browser agents with VLM’s. &lt;/p&gt; &lt;p&gt;Getting set up with my models was so painful!! &lt;/p&gt; &lt;p&gt;Existing solutions forced me into their agent framework and didn’t integrate so easily with the code i had already built using my self-hosted models. The engineer in me decided to build a quick prototype. &lt;/p&gt; &lt;p&gt;The tool deploys your agent code when you `git push`, runs browsers concurrently, and passes in queries and env variables. &lt;/p&gt; &lt;p&gt;I showed it to an old coworker and he found it useful, so wanted to get feedback from other devs – anyone else have trouble setting up headful browser agents with their LLMs? Let me know in the comments! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Capable_Cover6678"&gt; /u/Capable_Cover6678 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kirezi/spent_the_last_month_building_a_platform_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kirezi/spent_the_last_month_building_a_platform_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kirezi/spent_the_last_month_building_a_platform_to_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-09T19:44:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kicje6</id>
    <title>Can we choose what to offload to GPU?</title>
    <updated>2025-05-09T07:16:24+00:00</updated>
    <author>
      <name>/u/ACheshirov</name>
      <uri>https://old.reddit.com/user/ACheshirov</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I like Ollama because it gives me an easy way to integrate LLMs into my tools, but sometimes more advanced settings could be really beneficial.&lt;/p&gt; &lt;p&gt;So, I came across this reddit post &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ki7tg7/dont_offload_gguf_layers_offload_tensors_200_gen/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ki7tg7/dont_offload_gguf_layers_offload_tensors_200_gen/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This guy shows how we can get a 200%+ performance boost by offloading only the &amp;quot;right&amp;quot; layers to the GPU. Basically, when we can't fit the whole model into GPU VRAM, part of it has to run from the CPU and RAM. The key point is which parts go to the CPU and which ones to the GPU.&lt;/p&gt; &lt;p&gt;The idea is: let the GPU handle all possible tensors, but leave the GGUF layers on the CPU. That way, the GPU does the heavy lifting, and the whole thing runs more efficiently - you get more tokens per second for free. :) &lt;/p&gt; &lt;p&gt;At least, that's what I understood from his post.&lt;/p&gt; &lt;p&gt;So… is there a flag in Ollama that lets us do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ACheshirov"&gt; /u/ACheshirov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kicje6/can_we_choose_what_to_offload_to_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kicje6/can_we_choose_what_to_offload_to_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kicje6/can_we_choose_what_to_offload_to_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-09T07:16:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1kiuz8q</id>
    <title>Create model for resume writing</title>
    <updated>2025-05-09T22:19:49+00:00</updated>
    <author>
      <name>/u/puckpuckgo</name>
      <uri>https://old.reddit.com/user/puckpuckgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my mind, this can work, but please correct me if I'm wrong. I'm not an expert.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;BACKGROUND:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I use Ollama/OpenWebUI to write different versions of my resume. I have a prompt and then I just upload my resume and the job description to have it write a resume for that job. The issue is that after it does its thing, I have to go in and fine tune because it fabricated stuff, got stuff wrong, etc. I want to improve this process so that I can tailor resumes quicker.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;IDEA:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create knowledge within OpenWebUI and upload every single &amp;quot;final&amp;quot; version of my resume that I've submitted. Eventually, I will end up with a vast collection of &amp;quot;approved&amp;quot; resumes that Ollama can use to tailor to each JD I provide it.&lt;/li&gt; &lt;li&gt;Create a model that uses that knowledge to scan for relevant pieces of the resumes in the knowledge collection and use those to better match previous, approved, snippets to new JDs.&lt;/li&gt; &lt;li&gt;Use the model and simply paste a JD in order to get a tailored version of my resume. The outcome should be way better than using a single resume to tailor to a JD, right?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Will this work? What would be the best model to use for this specific use case?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/puckpuckgo"&gt; /u/puckpuckgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kiuz8q/create_model_for_resume_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kiuz8q/create_model_for_resume_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kiuz8q/create_model_for_resume_writing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-09T22:19:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ki7x1s</id>
    <title>New very simple UI for Ollama</title>
    <updated>2025-05-09T02:30:13+00:00</updated>
    <author>
      <name>/u/rotgertesla</name>
      <uri>https://old.reddit.com/user/rotgertesla</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ki7x1s/new_very_simple_ui_for_ollama/"&gt; &lt;img alt="New very simple UI for Ollama" src="https://preview.redd.it/2xe9bpzq3oze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=530d0af0ac1c4e748f74446c02c26ecdc6ca03cb" title="New very simple UI for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created a very simple html UI for Ollama (single file).&lt;br /&gt; Probably the simplest UI you can find.&lt;/p&gt; &lt;p&gt;See github page here: &lt;a href="https://github.com/rotger/Simple-Ollama-Chatbot"&gt;https://github.com/rotger/Simple-Ollama-Chatbot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;support markdown, mathjax and code synthax highlighting&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rotgertesla"&gt; /u/rotgertesla &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2xe9bpzq3oze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ki7x1s/new_very_simple_ui_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ki7x1s/new_very_simple_ui_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-09T02:30:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjav4f</id>
    <title>Build Your Own Local AI Podcaster with Kokoro, LangChain, and Streamlit</title>
    <updated>2025-05-10T13:55:46+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kjav4f/build_your_own_local_ai_podcaster_with_kokoro/"&gt; &lt;img alt="Build Your Own Local AI Podcaster with Kokoro, LangChain, and Streamlit" src="https://external-preview.redd.it/rnDYHBsChhqE7qxoeEZMpL5k9tkUJsq7rkMab9x10R4.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=98f989263160bbccfa6371c10dfe0e4d57833671" title="Build Your Own Local AI Podcaster with Kokoro, LangChain, and Streamlit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=h5D4rDNe8xk&amp;amp;list=PLp01ObP3udmq2quR-RfrX4zNut_t_kNot&amp;amp;index=19"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjav4f/build_your_own_local_ai_podcaster_with_kokoro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kjav4f/build_your_own_local_ai_podcaster_with_kokoro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-10T13:55:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjfm7c</id>
    <title>stuck on pulling manifest</title>
    <updated>2025-05-10T17:31:02+00:00</updated>
    <author>
      <name>/u/No-Situation2445</name>
      <uri>https://old.reddit.com/user/No-Situation2445</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kjfm7c/stuck_on_pulling_manifest/"&gt; &lt;img alt="stuck on pulling manifest" src="https://external-preview.redd.it/YiaXi82cK_qRoVV_SK2m53-VRr5BGhO-sTxbF4aO2o8.png?width=140&amp;amp;height=77&amp;amp;crop=140:77,smart&amp;amp;auto=webp&amp;amp;s=8a2bf17a4bef6b3d5bdf052039958560de359bb1" title="stuck on pulling manifest" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0f4ll7napzze1.png?width=1097&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2b760f12f60ed59a6887581b25d5d68fc6759ed"&gt;https://preview.redd.it/0f4ll7napzze1.png?width=1097&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2b760f12f60ed59a6887581b25d5d68fc6759ed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Disabled Windows Firewall and Proxies&lt;br /&gt; Tried Google and Cloudflare DNS&lt;br /&gt; Tried installing it on a different drive&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Situation2445"&gt; /u/No-Situation2445 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjfm7c/stuck_on_pulling_manifest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjfm7c/stuck_on_pulling_manifest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kjfm7c/stuck_on_pulling_manifest/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-10T17:31:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjaods</id>
    <title>Which models and parameter is can use?</title>
    <updated>2025-05-10T13:46:40+00:00</updated>
    <author>
      <name>/u/QuarterOverall5966</name>
      <uri>https://old.reddit.com/user/QuarterOverall5966</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all I am a user I recently bought a macbook air 2017 (8db ram 128gb ssd ,used one) Could you guys tell me which models I can use and in that version how many parameter I can use using in ollama? Please help me with it .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuarterOverall5966"&gt; /u/QuarterOverall5966 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjaods/which_models_and_parameter_is_can_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjaods/which_models_and_parameter_is_can_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kjaods/which_models_and_parameter_is_can_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-10T13:46:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1kiqggq</id>
    <title>Vision models that work well with Ollama</title>
    <updated>2025-05-09T19:03:01+00:00</updated>
    <author>
      <name>/u/deeperexistence</name>
      <uri>https://old.reddit.com/user/deeperexistence</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone use a vision model that is not on the official list at &lt;a href="https://ollama.com/search?c=vision"&gt;https://ollama.com/search?c=vision&lt;/a&gt; ? The models listed there aren't quite suitable for a project I'm working on, I wonder if anyone has gotten any of the models on hugging face to work well with vision in Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/deeperexistence"&gt; /u/deeperexistence &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kiqggq/vision_models_that_work_well_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kiqggq/vision_models_that_work_well_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kiqggq/vision_models_that_work_well_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-09T19:03:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj0p40</id>
    <title>Building Helios: A Self-Hosted Platform to Supercharge Local LLMs (Ollama, HF) with Memory &amp; Management - Feedback Needed!</title>
    <updated>2025-05-10T03:17:55+00:00</updated>
    <author>
      <name>/u/Effective_Muscle_110</name>
      <uri>https://old.reddit.com/user/Effective_Muscle_110</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey r/Ollama, community!&lt;/p&gt; &lt;p&gt;I'm a big fan of running LLMs locally and I'm building a platform called &lt;strong&gt;Helios&lt;/strong&gt; to make it easier to manage and enhance these local models. I'd love your feedback.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Goal:&lt;/strong&gt;&lt;br /&gt; To provide a self-hosted backend that gives you:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Better Model Management:&lt;/strong&gt; Easily switch between different local models (from Ollama, local HuggingFace Hub caches) and even integrate cloud APIs (OpenAI, Anthropic) if you need to, all through one consistent interface. It also includes hardware detection to help pick suitable models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Persistent, Intelligent Memory:&lt;/strong&gt; Give your local LLMs long-term memory. Helios would handle semantic search over past interactions/data, summarize long conversations, and even help manage conflicting information.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benchmarking Tools:&lt;/strong&gt; Understand how different local models perform on your own hardware for specific tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A Simple UI:&lt;/strong&gt; For chatting, managing memories, and overseeing your local LLM setup.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why I'm Building This:&lt;/strong&gt;&lt;br /&gt; I find managing multiple local models, giving them effective context, and understanding their performance can be a bit of a pain. I'm aiming for Helios to be an integrated solution that sits on top of tools like Ollama or direct HuggingFace model usage.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Looking for Your Thoughts:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;As users of local LLMs, what are your biggest pain points in managing them and building applications with them?&lt;/li&gt; &lt;li&gt;Does the idea of an integrated platform with advanced memory and benchmarking specifically for local/hybrid setups appeal to you?&lt;/li&gt; &lt;li&gt;Which features (model management, memory, benchmarking) would be most useful in your workflow?&lt;/li&gt; &lt;li&gt;Are there specific challenges with Ollama or local HuggingFace models that a platform like Helios could help solve?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm keen to hear from the local LLM community. Any feedback, ideas, or &amp;quot;I wish I had X&amp;quot; comments would be amazing!&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective_Muscle_110"&gt; /u/Effective_Muscle_110 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kj0p40/building_helios_a_selfhosted_platform_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kj0p40/building_helios_a_selfhosted_platform_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kj0p40/building_helios_a_selfhosted_platform_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-10T03:17:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjjvdd</id>
    <title>HOW TO DOWNLOAD OLLAMA ON A DIFFERENT DRIVE</title>
    <updated>2025-05-10T20:43:40+00:00</updated>
    <author>
      <name>/u/LibraryRemarkable42</name>
      <uri>https://old.reddit.com/user/LibraryRemarkable42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;Find the Installer&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;First things first — you need to know where that &lt;code&gt;OllamaSetup.exe&lt;/code&gt; file is.&lt;/p&gt; &lt;p&gt;Let’s say you downloaded it and it’s just in your &lt;strong&gt;Downloads&lt;/strong&gt; folder.&lt;br /&gt; (RIGHT-CLICK the file and choose &lt;strong&gt;“Copy as path”&lt;/strong&gt; — it should look something like this):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;D:\Users\Administrator\Downloads\OllamaSetup.exe &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;2. Fire Up the Command Line as Admin&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Hit the &lt;strong&gt;Windows&lt;/strong&gt; key and type in &lt;code&gt;cmd&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;In the search results, &lt;strong&gt;right-click&lt;/strong&gt; on &lt;em&gt;Command Prompt&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;Choose &lt;strong&gt;“Run as administrator.”&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. Tell It Where to Go&lt;/h1&gt; &lt;p&gt;Now, in that black Command Prompt window, type in something like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;D:\Users\Administrator\Downloads\OllamaSetup.exe&amp;quot; /DIR=&amp;quot;D:\Users\Administrator\ollama&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;4. Let It Do Its Thing&lt;/h1&gt; &lt;p&gt;Once you press &lt;strong&gt;Enter&lt;/strong&gt;, the Ollama installer should launch. It might show a regular setup window — just follow the steps. It’ll install everything into the folder you specified (like &lt;code&gt;D:\Users\Administrator\ollama&lt;/code&gt;).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LibraryRemarkable42"&gt; /u/LibraryRemarkable42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjjvdd/how_to_download_ollama_on_a_different_drive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjjvdd/how_to_download_ollama_on_a_different_drive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kjjvdd/how_to_download_ollama_on_a_different_drive/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-10T20:43:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1kiw05t</id>
    <title>Built a simple way to one-click install and connect MCP servers to Ollama (Open source local LLM client)</title>
    <updated>2025-05-09T23:08:22+00:00</updated>
    <author>
      <name>/u/WalrusVegetable4506</name>
      <uri>https://old.reddit.com/user/WalrusVegetable4506</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kiw05t/built_a_simple_way_to_oneclick_install_and/"&gt; &lt;img alt="Built a simple way to one-click install and connect MCP servers to Ollama (Open source local LLM client)" src="https://external-preview.redd.it/NnVobDhibnU2dXplMSCrY1eLm44uy6JKxNLUNGEQmoO1qAaJ9AW8ntQj8l4s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4698c761ceb2a09ce353888a9a429e3dade4f7fb" title="Built a simple way to one-click install and connect MCP servers to Ollama (Open source local LLM client)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! &lt;a href="/u/TomeHanks"&gt;u/TomeHanks&lt;/a&gt;, &lt;a href="/u/_march"&gt;u/_march&lt;/a&gt; and I recently open sourced a local LLM client called Tome (&lt;a href="https://github.com/runebookai/tome"&gt;https://github.com/runebookai/tome&lt;/a&gt;) that lets you connect Ollama to MCP servers without having to manage uv/npm or any json configs.&lt;/p&gt; &lt;p&gt;It's a &amp;quot;technical preview&amp;quot; (aka it's only been out for a week or so) but here's what you can do today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;connect to Ollama&lt;/li&gt; &lt;li&gt;add an MCP server, you can either paste something like &amp;quot;uvx mcp-server-fetch&amp;quot; or you can use the Smithery registry integration to one-click install a local MCP server - Tome manages uv/npm and starts up/shuts down your MCP servers so you don't have to worry about it&lt;/li&gt; &lt;li&gt;chat with your model and watch it make tool calls!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The demo video is using Qwen3:14B and an MCP Server called desktop-commander that can execute terminal commands and edit files. I sped up through a lot of the thinking, smaller models aren't yet at &amp;quot;Claude Desktop + Sonnet 3.7&amp;quot; speed/efficiency, but we've got some fun ideas coming out in the next few months for how we can better utilize the lower powered models for local work.&lt;/p&gt; &lt;p&gt;Feel free to try it out, it's currently MacOS only but Windows is coming soon. If you have any questions throw them in here or feel free to &lt;a href="https://discord.gg/9CH6us29YA"&gt;join us on Discord&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;GitHub here: &lt;a href="https://github.com/runebookai/tome"&gt;https://github.com/runebookai/tome&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WalrusVegetable4506"&gt; /u/WalrusVegetable4506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kbpduwnu6uze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kiw05t/built_a_simple_way_to_oneclick_install_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kiw05t/built_a_simple_way_to_oneclick_install_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-09T23:08:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjacaf</id>
    <title>How to remove &lt;think&gt; tags in VS Code or Zed?</title>
    <updated>2025-05-10T13:29:54+00:00</updated>
    <author>
      <name>/u/redditemailorusernam</name>
      <uri>https://old.reddit.com/user/redditemailorusernam</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kjacaf/how_to_remove_think_tags_in_vs_code_or_zed/"&gt; &lt;img alt="How to remove &amp;lt;think&amp;gt; tags in VS Code or Zed?" src="https://preview.redd.it/285fljy5iyze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10de2be0e4cc790ad7412c195a7976ec45200926" title="How to remove &amp;lt;think&amp;gt; tags in VS Code or Zed?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who use AI in either code editor, please can you tell me how to hide the &amp;lt;think&amp;gt; part of the response from local LLMs? It's so cluttered currently in my editor&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redditemailorusernam"&gt; /u/redditemailorusernam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/285fljy5iyze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjacaf/how_to_remove_think_tags_in_vs_code_or_zed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kjacaf/how_to_remove_think_tags_in_vs_code_or_zed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-10T13:29:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjgo15</id>
    <title>ollama using system ram over vram</title>
    <updated>2025-05-10T18:17:25+00:00</updated>
    <author>
      <name>/u/Old_Guide627</name>
      <uri>https://old.reddit.com/user/Old_Guide627</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kjgo15/ollama_using_system_ram_over_vram/"&gt; &lt;img alt="ollama using system ram over vram" src="https://external-preview.redd.it/yeGlUSsSCNJk7ls9EWHm7kYoxiuziss5SeOGZwV2NE0.png?width=140&amp;amp;height=26&amp;amp;crop=140:26,smart&amp;amp;auto=webp&amp;amp;s=bf60bc6df88649fea4a5038665282ac90704cb0e" title="ollama using system ram over vram" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i dont know why it happens but my ollama seems to priorize system ram over vram in some cases. &amp;quot;small&amp;quot; llms run in vram just fine and if you increase context size its filling vram and the rest that is needed is system memory as it should be, but with qwen 3 its 100% cpu no matter what. any ideas what causes this and how i can fix it?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w5d1k2okxzze1.png?width=677&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63893e4015c547a19d7d336861042c0efaabc239"&gt;https://preview.redd.it/w5d1k2okxzze1.png?width=677&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63893e4015c547a19d7d336861042c0efaabc239&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old_Guide627"&gt; /u/Old_Guide627 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjgo15/ollama_using_system_ram_over_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjgo15/ollama_using_system_ram_over_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kjgo15/ollama_using_system_ram_over_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-10T18:17:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjt2d8</id>
    <title>Questions about Ollama (NSFW)</title>
    <updated>2025-05-11T04:53:45+00:00</updated>
    <author>
      <name>/u/MilaAmane</name>
      <uri>https://old.reddit.com/user/MilaAmane</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;I'm very new Ollama i have some questions regarding about it first can use it to write uncensored stories&lt;/li&gt; &lt;li&gt;Is like chatgpt with all the restrictions and risk of getting banned in the future with how picky their terms of service &lt;/li&gt; &lt;li&gt;Can you upload documents to ollama&lt;/li&gt; &lt;li&gt;Does it have problem writing fanfiction&lt;/li&gt; &lt;li&gt;Can it be put on your hard drive without internet&lt;/li&gt; &lt;li&gt;It's pretty much free to use&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any information would be absolutely great. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MilaAmane"&gt; /u/MilaAmane &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjt2d8/questions_about_ollama_nsfw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjt2d8/questions_about_ollama_nsfw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kjt2d8/questions_about_ollama_nsfw/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-11T04:53:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjijh6</id>
    <title>Would it be possible to create a robot powered by ollama/ai locally?</title>
    <updated>2025-05-10T19:42:45+00:00</updated>
    <author>
      <name>/u/Game-Lover44</name>
      <uri>https://old.reddit.com/user/Game-Lover44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tend to dream big, this may be one of those times. Im just curious but is it possible to make a small robot that can talk, see, as if in a conversation, something like that? Can this be done locally on something like a Raspberry Pi stuck in a robot? What type of specs would the robot need along with parts? what would you image this robot look like or do?&lt;/p&gt; &lt;p&gt;as i said i tend to dream big and this may stay a dream.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Game-Lover44"&gt; /u/Game-Lover44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjijh6/would_it_be_possible_to_create_a_robot_powered_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjijh6/would_it_be_possible_to_create_a_robot_powered_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kjijh6/would_it_be_possible_to_create_a_robot_powered_by/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-10T19:42:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjhul8</id>
    <title>how to image generate locally?</title>
    <updated>2025-05-10T19:10:36+00:00</updated>
    <author>
      <name>/u/Crafty-Teaching-9289</name>
      <uri>https://old.reddit.com/user/Crafty-Teaching-9289</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there a model that lets generating images without connecting to any external service on the internet? like i want it because i see much services for image generating like chatgpt, copilot... have limit of 5 images and 15 or so.&lt;/p&gt; &lt;p&gt;so thats why i want to locally host a image generator for me and my family.&lt;/p&gt; &lt;p&gt;if anyone can help i would appreciate&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Crafty-Teaching-9289"&gt; /u/Crafty-Teaching-9289 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjhul8/how_to_image_generate_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjhul8/how_to_image_generate_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kjhul8/how_to_image_generate_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-10T19:10:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjf4m5</id>
    <title>The era of local Computer-Use AI Agents is here.</title>
    <updated>2025-05-10T17:08:54+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kjf4m5/the_era_of_local_computeruse_ai_agents_is_here/"&gt; &lt;img alt="The era of local Computer-Use AI Agents is here." src="https://external-preview.redd.it/OWZwbHFibGdsenplMUyjGwnJS8rotX6d0qpdBh20m0kIIRiKyvfLkkchTDUy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e50d0cb8dc2f6a4bb011b4f080e82a7e4a363bd7" title="The era of local Computer-Use AI Agents is here." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The era of local Computer-Use AI Agents is here. Meet UI-TARS-1.5-7B-6bit, now running natively on Apple Silicon via MLX.&lt;/p&gt; &lt;p&gt;The video is of UI-TARS-1.5-7B-6bit completing the prompt &amp;quot;draw a line from the red circle to the green circle, then open reddit in a new tab&amp;quot; running entirely on MacBook. The video is just a replay, during actual usage it took between 15s to 50s per turn with 720p screenshots (on avg its ~30s per turn), this was also with many apps open so it had to fight for memory at times.&lt;/p&gt; &lt;p&gt;This is just the 7 Billion model.Expect much more with the 72 billion.The future is indeed here.&lt;/p&gt; &lt;p&gt;Try it now: &lt;a href="https://github.com/trycua/cua/tree/feature/agent/uitars-mlx"&gt;https://github.com/trycua/cua/tree/feature/agent/uitars-mlx&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Patch: &lt;a href="https://github.com/ddupont808/mlx-vlm/tree/fix/qwen2-position-id"&gt;https://github.com/ddupont808/mlx-vlm/tree/fix/qwen2-position-id&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Built using c/ua : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Join us making them here: &lt;a href="https://discord.gg/4fuebBsAUj"&gt;https://discord.gg/4fuebBsAUj&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x5uzqurglzze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kjf4m5/the_era_of_local_computeruse_ai_agents_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kjf4m5/the_era_of_local_computeruse_ai_agents_is_here/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-10T17:08:54+00:00</published>
  </entry>
</feed>
