<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-27T05:48:47+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jisfbt</id>
    <title>Just Built an Interactive AI-Powered CrewAI Documentation Assistant with Langchain and Ollama</title>
    <updated>2025-03-24T14:48:48+00:00</updated>
    <author>
      <name>/u/Maleficent-Penalty50</name>
      <uri>https://old.reddit.com/user/Maleficent-Penalty50</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jisfbt/just_built_an_interactive_aipowered_crewai/"&gt; &lt;img alt="Just Built an Interactive AI-Powered CrewAI Documentation Assistant with Langchain and Ollama" src="https://external-preview.redd.it/eXNyMnNyY2xobnFlMZe9ybkIgIdvv6PPmgWnShlnht23JofJEdJa4TdKHoaX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0fec3b3117d586dead26223ecc60881554fe0bf" title="Just Built an Interactive AI-Powered CrewAI Documentation Assistant with Langchain and Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent-Penalty50"&gt; /u/Maleficent-Penalty50 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/je2heyclhnqe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jisfbt/just_built_an_interactive_aipowered_crewai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jisfbt/just_built_an_interactive_aipowered_crewai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-24T14:48:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjciqk</id>
    <title>Creating an Ollama to Signal bridge</title>
    <updated>2025-03-25T05:48:36+00:00</updated>
    <author>
      <name>/u/asynchronous-x</name>
      <uri>https://old.reddit.com/user/asynchronous-x</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asynchronous-x"&gt; /u/asynchronous-x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://asynchronous.win/post/replacing-myself-with-an-llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjciqk/creating_an_ollama_to_signal_bridge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jjciqk/creating_an_ollama_to_signal_bridge/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-25T05:48:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jj1ukn</id>
    <title>ObserverAI demo video!</title>
    <updated>2025-03-24T21:01:45+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jj1ukn/observerai_demo_video/"&gt; &lt;img alt="ObserverAI demo video!" src="https://external-preview.redd.it/c2Z6czE5YWVicHFlMSzX04uPkutVqaVB3RNDmFdJfygXornQmolDDmaeMUuj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12c6e8b4c5e7f19d09d212fdcf6218c522b1f3d4" title="ObserverAI demo video!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey ollama community!&lt;/p&gt; &lt;p&gt;This is a better demo video than the one I uploaded a few days ago, it shows the flow of the application better!&lt;/p&gt; &lt;p&gt;The Observer AI agents can:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Observe your screen (via OCR or screenshots with vision models)&lt;/li&gt; &lt;li&gt;Process what they see with LLMs running locally through Ollama&lt;/li&gt; &lt;li&gt;Execute JS in the browser or Python code to perform actions on your system!!&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Looking for feedback:&lt;br /&gt; I'd love your thoughts on:&lt;br /&gt; * What kinds of agents would you build with Python execution capabilities?&lt;br /&gt; Examples:&lt;br /&gt; - Stock buying bot (would be very bad at it's job hahaha)&lt;br /&gt; - Dashboard watching agent with custom hooks to react to information&lt;br /&gt; - Process registration agent, (would describe step by step a process you do on your computer)(I can help you through discord or dm's)&lt;br /&gt; * Feature requests or improvements to the UX?&lt;/p&gt; &lt;p&gt;Observer AI remains 100% open source and local-first - try it at &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com&lt;/a&gt; or check out the code at &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;br /&gt; Thanks for all the support and feedback so far!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/allep4aebpqe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jj1ukn/observerai_demo_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jj1ukn/observerai_demo_video/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-24T21:01:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjc0n0</id>
    <title>Training LLM to assist me with running D&amp;D?</title>
    <updated>2025-03-25T05:13:45+00:00</updated>
    <author>
      <name>/u/SeriousLemur</name>
      <uri>https://old.reddit.com/user/SeriousLemur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Would it be possible to train an AI to reference .pdf files for a D&amp;amp;D campaign in order to assist me with dialogue, descriptions, running it, etc?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SeriousLemur"&gt; /u/SeriousLemur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjc0n0/training_llm_to_assist_me_with_running_dd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjc0n0/training_llm_to_assist_me_with_running_dd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jjc0n0/training_llm_to_assist_me_with_running_dd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-25T05:13:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjkbf1</id>
    <title>How to analyse codebase for technical auditory work with ollama (no code generation)</title>
    <updated>2025-03-25T14:13:19+00:00</updated>
    <author>
      <name>/u/Zestyclose-Proof9270</name>
      <uri>https://old.reddit.com/user/Zestyclose-Proof9270</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, &lt;/p&gt; &lt;p&gt;I am a (non-tech) founder of a company in a highly regulated field and want to help our dev team. &lt;/p&gt; &lt;p&gt;We are undergoing prep work for extensive regulatory certifications; in short our devs have to check our front- and backend codebase against over 500 very specific IT-regulatory criteria and provide evidence that we fulfill these criteria (or change the code). &lt;/p&gt; &lt;p&gt;Devs are fullstack without AI-background and I am trying to help setting up a local LLM that can help analyzing whether the code complies with these individual regulations or not. &lt;/p&gt; &lt;p&gt;We work with Kotlin and Dart and have about 90k lines of code, meaning even the largest context windows (128k etc.) are not enough. &lt;/p&gt; &lt;p&gt;I like Ollama and was wondering how a setup could like in which I can analyse the entire codebase in the current folder/filestructure with interdependencies. &lt;/p&gt; &lt;p&gt;Only selecting certain files to be analyzed does not make much sense as the point is for the LLM to identify the locations in the codebase in which the requirements are fulfilled. &lt;/p&gt; &lt;p&gt;If anyone can simply point me to other post / blogs / articles etc. I would be eternally grateful. &lt;/p&gt; &lt;p&gt;Thx!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zestyclose-Proof9270"&gt; /u/Zestyclose-Proof9270 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjkbf1/how_to_analyse_codebase_for_technical_auditory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjkbf1/how_to_analyse_codebase_for_technical_auditory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jjkbf1/how_to_analyse_codebase_for_technical_auditory/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-25T14:13:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jj60ng</id>
    <title>How I adapted a 1B function calling LLM for fast agent hand off and routing in a framework agnostic way</title>
    <updated>2025-03-24T23:57:17+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jj60ng/how_i_adapted_a_1b_function_calling_llm_for_fast/"&gt; &lt;img alt="How I adapted a 1B function calling LLM for fast agent hand off and routing in a framework agnostic way" src="https://preview.redd.it/fhb45akh7qqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66166e82aaf4bdf06504f8555ace369815775238" title="How I adapted a 1B function calling LLM for fast agent hand off and routing in a framework agnostic way" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You might have heard a thing or two about agents. Things that have high level goals and usually run in a loop to complete a said task - the trade off being latency for some powerful automation work&lt;/p&gt; &lt;p&gt;Well if you have been building with agents then you know that users can switch between them.Mid context and expect you to get the routing and agent hand off scenarios right. So now you are focused on not only working on the goals of your agent you are also working on thus pesky work on fast, contextual routing and hand off &lt;/p&gt; &lt;p&gt;Well I just adapted Arch-Function a SOTA function calling LLM that can make precise tools calls for common agentic scenarios to support routing to more coarse-grained or high-level agent definitions&lt;/p&gt; &lt;p&gt;The project can be found here: &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt; and the models are listed in the README. &lt;/p&gt; &lt;p&gt;Happy bulking 🛠️&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fhb45akh7qqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jj60ng/how_i_adapted_a_1b_function_calling_llm_for_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jj60ng/how_i_adapted_a_1b_function_calling_llm_for_fast/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-24T23:57:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjhzpk</id>
    <title>Better alternative to open webui on ollama for text uploading?</title>
    <updated>2025-03-25T12:18:52+00:00</updated>
    <author>
      <name>/u/CanAmDB7</name>
      <uri>https://old.reddit.com/user/CanAmDB7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running a few LLMs for text analysis in ollama, they are fine, but regularly I cant get the model to 'see' the attached documents. Sometimes I can, sometimes I cant. I dont see any errors or messages&lt;/p&gt; &lt;p&gt;sometimes uploading the file works and the model reads the text ok, others webui says the file is uploaded/attached but the model complains I haven't attached anything to the message. &lt;/p&gt; &lt;p&gt;Are there other solutions out there for locally running a chat session where uploading text files is more stable?&lt;/p&gt; &lt;p&gt;thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CanAmDB7"&gt; /u/CanAmDB7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjhzpk/better_alternative_to_open_webui_on_ollama_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjhzpk/better_alternative_to_open_webui_on_ollama_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jjhzpk/better_alternative_to_open_webui_on_ollama_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-25T12:18:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjdbuy</id>
    <title>GUIDE : run ollama on Radeon Pro W5700 in Ubuntu 24.10</title>
    <updated>2025-03-25T06:47:54+00:00</updated>
    <author>
      <name>/u/fantastic_mr_wolf</name>
      <uri>https://old.reddit.com/user/fantastic_mr_wolf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jjdbuy/guide_run_ollama_on_radeon_pro_w5700_in_ubuntu/"&gt; &lt;img alt="GUIDE : run ollama on Radeon Pro W5700 in Ubuntu 24.10" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="GUIDE : run ollama on Radeon Pro W5700 in Ubuntu 24.10" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hopefully this'll help other Navi 10 owners whose cards aren't officially supported by &lt;a href="https://ollama.com/blog/amd-preview"&gt;ollama&lt;/a&gt;, or &lt;a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html"&gt;rocm&lt;/a&gt; for that matter.&lt;/p&gt; &lt;p&gt;I kept seeing articles/posts (&lt;a href="https://www.linkedin.com/pulse/ollama-working-amd-rx-5700-xt-windows-robert-buccigrossi-tze0e"&gt;like this one&lt;/a&gt;) recommending custom git repos and modifying env variables to get ollama to recognize the old Radeon, but none worked for me. After much trial and error though, I finally got it running:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Clean install of&lt;/strong&gt; &lt;a href="https://releases.ubuntu.com/oracular/"&gt;&lt;strong&gt;Ubuntu 24.10&lt;/strong&gt;&lt;/a&gt; &lt;ul&gt; &lt;li&gt;The Radeon driver needed to run rocm wouldn't build/install correctly under 24.04 or 22.04, the two officially supported Ubuntu releases for rocm&lt;/li&gt; &lt;li&gt;Goes without saying, make sure to update all Ubuntu packages before the next step&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Install latest rocm 6.3.3 using AMD docs&lt;/strong&gt; &lt;ul&gt; &lt;li&gt; &lt;a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/detailed-install.html"&gt;https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/detailed-install.html&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Follow the instruction for Ubuntu 24.04, I used the Package Manager approach but if that's giving you trouble the AMD installer should also work&lt;/li&gt; &lt;li&gt;I recommend following the &amp;quot;Detailed Install&amp;quot; instead of the &amp;quot;Quick Start&amp;quot; instruction, and do all the pre- &amp;amp; post- install steps&lt;/li&gt; &lt;li&gt;Once that's done you can run &lt;code&gt;rocminfo&lt;/code&gt; in a terminal and you should get some output that identifies your GPU&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Install ollama&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;curl -fsSL&lt;/code&gt; &lt;a href="https://ollama.com/install.sh"&gt;&lt;code&gt;https://ollama.com/install.sh&lt;/code&gt;&lt;/a&gt; &lt;code&gt;| sh&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Personally I like to do this in using a dedicated conda env so I can mess with variables and packages down the line without messing up the rest of my system, but you do you&lt;/li&gt; &lt;li&gt;Also, I suggest installing &lt;code&gt;nvtop&lt;/code&gt; to monitor ollama is actually using your GPU&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;... and that's it. If all went well your text generation should be WAAAAY faster, assuming the model fits within the VRAM:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m8eshs48flqe1.png?width=1636&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=853c6a26b2d6b1ef6cf334b32675b161048c6794"&gt;https://preview.redd.it/m8eshs48flqe1.png?width=1636&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=853c6a26b2d6b1ef6cf334b32675b161048c6794&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A few other other notes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This also works for multi-gpu&lt;/li&gt; &lt;li&gt;Models seem to use more VRAM on AMD than Nvidia gpu's, I've seen anywhere from 10%-30% more but haven't had the time to properly test&lt;/li&gt; &lt;li&gt;If you're planning to use ollama w/Open-WebUI (which you probably are) you might run into problems installing it via &lt;code&gt;pip&lt;/code&gt;, so I suggest you use docker and refer to this page: &lt;a href="https://docs.openwebui.com/troubleshooting/connection-error/"&gt;https://docs.openwebui.com/troubleshooting/connection-error/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fantastic_mr_wolf"&gt; /u/fantastic_mr_wolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjdbuy/guide_run_ollama_on_radeon_pro_w5700_in_ubuntu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjdbuy/guide_run_ollama_on_radeon_pro_w5700_in_ubuntu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jjdbuy/guide_run_ollama_on_radeon_pro_w5700_in_ubuntu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-25T06:47:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jj6ddm</id>
    <title>I built a self-hosted, memory-aware AI node on Ollama—Pan-AI Seed Node is live and public</title>
    <updated>2025-03-25T00:13:23+00:00</updated>
    <author>
      <name>/u/GVDub2</name>
      <uri>https://old.reddit.com/user/GVDub2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;I’ve been experimenting with locally hosted models on my homelab setup and wanted something more than just a stateless chatbot.&lt;/p&gt; &lt;p&gt;So I built (with a little help from local AI) &lt;strong&gt;Pan-AI Seed Node&lt;/strong&gt;—a FastAPI wrapper around Ollama that gives each node:&lt;/p&gt; &lt;p&gt;• An identity (via panai.identity.json)&lt;/p&gt; &lt;p&gt;• A memory policy (via panai.memory.json)&lt;/p&gt; &lt;p&gt;• Markdown-based journaling of every interaction&lt;/p&gt; &lt;p&gt;• And soon: federation-ready peer configs and trust models&lt;/p&gt; &lt;p&gt;Everything is local. Everything is auditable. And it’s built for a future where we might need AI that &lt;strong&gt;remembers context&lt;/strong&gt;, &lt;strong&gt;reflects values&lt;/strong&gt;, and &lt;strong&gt;resists institutional forgetting&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;p&gt;✅ Runs on any Ollama model (I’m using llama3.2:latest)&lt;/p&gt; &lt;p&gt;✅ Logs are human-readable and timestamped&lt;/p&gt; &lt;p&gt;✅ Easy to fork, adapt, and expand&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href="https://github.com/GVDub/panai-seed-node"&gt;https://github.com/GVDub/panai-seed-node&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love your thoughts, forks, suggestions—or philosophical rants. Especially, I need your help making this an indispensable tool for all of us. This is only the beginning. &lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GVDub2"&gt; /u/GVDub2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jj6ddm/i_built_a_selfhosted_memoryaware_ai_node_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jj6ddm/i_built_a_selfhosted_memoryaware_ai_node_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jj6ddm/i_built_a_selfhosted_memoryaware_ai_node_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-25T00:13:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjkw3g</id>
    <title>Integrated graphics</title>
    <updated>2025-03-25T14:38:39+00:00</updated>
    <author>
      <name>/u/Da-real-admin</name>
      <uri>https://old.reddit.com/user/Da-real-admin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm on a laptop with an integrated graphics card. Will this help with AI at all? If so, how do I convince it to do that? All I know is that it's AMD Radeon (TM) Graphics.&lt;/p&gt; &lt;p&gt;I downloaded ROCm drivers from AMD. I also downloaded ollama-for-amd and am currently trying to figure out what drivers to get for that. I think I've figured out that my integrated graphics card is RDNA 2, but I don't know where to go from there.&lt;/p&gt; &lt;p&gt;Also, I'm trying to run llama3.2:3b, and task manager says I have 8.1gb of GPU memory.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Da-real-admin"&gt; /u/Da-real-admin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjkw3g/integrated_graphics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjkw3g/integrated_graphics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jjkw3g/integrated_graphics/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-25T14:38:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjvufl</id>
    <title>Need help choosing build</title>
    <updated>2025-03-25T22:07:44+00:00</updated>
    <author>
      <name>/u/khud_ki_talaash</name>
      <uri>https://old.reddit.com/user/khud_ki_talaash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I am thinking of getting MacBook Pro with the following configuration:&lt;/p&gt; &lt;p&gt;M4 Max, 14-Core CPU, 32-Core GPU, 36GB Unified Memory, 1TB SSD Storage, 16-core Neural Engine&lt;/p&gt; &lt;p&gt;Is this good enough for play around with &lt;strong&gt;small to medium models&lt;/strong&gt;? Say upto the 20B parameters?&lt;/p&gt; &lt;p&gt;I have always had an mac but OK to try a Lenovo too, in case options and cost are easier. But I really wouldn't have the time and patience to build one from scratch. Appreciate all the guidance and protips!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/khud_ki_talaash"&gt; /u/khud_ki_talaash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjvufl/need_help_choosing_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjvufl/need_help_choosing_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jjvufl/need_help_choosing_build/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-25T22:07:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjr959</id>
    <title>Best LLaMa model for software modeling task?</title>
    <updated>2025-03-25T19:00:39+00:00</updated>
    <author>
      <name>/u/ChampionshipSad2979</name>
      <uri>https://old.reddit.com/user/ChampionshipSad2979</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a masters student of software engineering and am trying to create a AI application to help me create design models from software requirements. I wanted to know if there is any model you suggest to use to achieve this task. My goal is to create an application that uses RAG techniques to improve the context of the prompt and create a plantUML code for the class diagram. Am relatively new to the LLaMa world! all the help i can get is welcome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChampionshipSad2979"&gt; /u/ChampionshipSad2979 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjr959/best_llama_model_for_software_modeling_task/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjr959/best_llama_model_for_software_modeling_task/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jjr959/best_llama_model_for_software_modeling_task/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-25T19:00:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jj7vzo</id>
    <title>Second Me: An open-source framework for creating autonomous AI identities</title>
    <updated>2025-03-25T01:26:02+00:00</updated>
    <author>
      <name>/u/TopRavenfruit</name>
      <uri>https://old.reddit.com/user/TopRavenfruit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found an interesting open-source AI project, &lt;a href="https://github.com/Mindverse/Second-Me"&gt;second-me&lt;/a&gt;. They are building a network of AI entities that everybody can train on their local devices.&lt;/p&gt; &lt;p&gt;Key innovations:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Me-alignment Structure - A system that transforms user data into personalized AI insights using reinforcement learning&lt;/li&gt; &lt;li&gt;Hierarchical Memory Modeling - A three-layer memory structure that evolves from concrete interactions to abstract understanding&lt;/li&gt; &lt;li&gt;A decentralized protocol (SMP) where these AI entities can interact independently while preserving user privacy.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any idea? Feel free to talk here🤩&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TopRavenfruit"&gt; /u/TopRavenfruit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jj7vzo/second_me_an_opensource_framework_for_creating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jj7vzo/second_me_an_opensource_framework_for_creating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jj7vzo/second_me_an_opensource_framework_for_creating/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-25T01:26:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjnzhf</id>
    <title>Cheapest Serverless Coding LLM or API</title>
    <updated>2025-03-25T16:48:58+00:00</updated>
    <author>
      <name>/u/juan_berger</name>
      <uri>https://old.reddit.com/user/juan_berger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the CHEAPEST serverless option to run an llm for coding (at least as good as qwen 32b).&lt;/p&gt; &lt;p&gt;Basically asking what is the cheapest way to use an llm through an api, not the web ui.&lt;/p&gt; &lt;p&gt;Open to ideas like: - Official APIs (if they are cheap) - Serverless (Modal, Lambda, etc...) - Spot GPU instance running ollama - Renting (Vast AI &amp;amp; Similar) - Services like Google Cloud Run&lt;/p&gt; &lt;p&gt;Basically curious what options people have tried.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juan_berger"&gt; /u/juan_berger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjnzhf/cheapest_serverless_coding_llm_or_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjnzhf/cheapest_serverless_coding_llm_or_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jjnzhf/cheapest_serverless_coding_llm_or_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-25T16:48:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jju2jy</id>
    <title>Create Your Personal AI Knowledge Assistant - No Coding Needed</title>
    <updated>2025-03-25T20:54:42+00:00</updated>
    <author>
      <name>/u/PeterHash</name>
      <uri>https://old.reddit.com/user/PeterHash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've just published a guide on building a personal AI assistant using Open WebUI that works with your own documents.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What You Can Do:&lt;/strong&gt; - Answer questions from personal notes - Search through research PDFs - Extract insights from web content - Keep all data private on your own machine&lt;/p&gt; &lt;p&gt;My tutorial walks you through: - Setting up a knowledge base - Creating a research companion - Lots of tips and trick for getting precise answers - All without any programming&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Might be helpful for:&lt;/strong&gt; - Students organizing research - Professionals managing information - Anyone wanting smarter document interactions&lt;/p&gt; &lt;p&gt;Upcoming articles will cover more advanced AI techniques like function calling and multi-agent systems.&lt;/p&gt; &lt;p&gt;Curious what knowledge base you're thinking of creating. Drop a comment!&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@hautel.alex2000/open-webui-tutorial-supercharging-your-local-ai-with-rag-and-custom-knowledge-bases-334d272c8c40"&gt;Open WebUI tutorial — Supercharge Your Local AI with RAG and Custom Knowledge Bases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PeterHash"&gt; /u/PeterHash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jju2jy/create_your_personal_ai_knowledge_assistant_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jju2jy/create_your_personal_ai_knowledge_assistant_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jju2jy/create_your_personal_ai_knowledge_assistant_no/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-25T20:54:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk9fab</id>
    <title>Ollama *always* summarizes a local text file</title>
    <updated>2025-03-26T11:22:33+00:00</updated>
    <author>
      <name>/u/ozaarmat</name>
      <uri>https://old.reddit.com/user/ozaarmat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OS : MacOS 15.3.2&lt;br /&gt; ollama : installed locally and as python module&lt;br /&gt; models : llama2, mistral&lt;br /&gt; language : python3&lt;br /&gt; issue : no matter what I prompt, the output is always a summary of the local text file. &lt;/p&gt; &lt;p&gt;I'd appreciate some tips if anyone has encountered this issue. &lt;/p&gt; &lt;p&gt;CLI PROMPT 1&lt;br /&gt; $python3 &lt;a href="http://promptfile2.py"&gt;promptfile2.py&lt;/a&gt; cinq_semaines.txt &amp;quot;Count the words in this text file&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt; The prompt is read correctly&lt;br /&gt; &amp;quot;Sending prompt: Count the number of words and characters in this file. &amp;quot; but&lt;br /&gt; &amp;gt;&amp;gt; I get a summary of the text file, irrespective of which model is selected (llama2 or mistral)&lt;/p&gt; &lt;p&gt;CLI PROMPT 2&lt;br /&gt; $ollama run mistral &amp;quot;Do not summarize. Return only the total number of words in this text as an integer, nothing else: Hello world, this is a test.&amp;quot;&lt;br /&gt; &amp;gt;&amp;gt; 15&lt;br /&gt; &amp;gt;&amp;gt; direct prompt returns the correct result. Counting words is for testing purposes, I know there are other ways to count words. &lt;/p&gt; &lt;p&gt;** ollama/mistral is able to understand the instruction when called directly, but not via the script.&lt;br /&gt; ** My text file is in French, but llama2 or mistral read it and give me a nice summary in English.&lt;br /&gt; ** I tried ollama.chat() and ollama.generate() &lt;/p&gt; &lt;p&gt;Code :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import ollama import os import sys # Check command-line arguments if len(sys.argv) &amp;lt; 2 or len(sys.argv) &amp;gt; 3: print(&amp;quot;Usage: python3 promptfileX.py &amp;lt;filename.txt&amp;gt; [prompt]&amp;quot;) print(&amp;quot; If no prompt is provided, defaults to 'Summarize'&amp;quot;) sys.exit(1) filename = sys.argv[1] prompt = sys.argv[2] # Check file validity if not filename.endswith(&amp;quot;.txt&amp;quot;) or not os.path.isfile(filename): print(&amp;quot;Error: Please provide a valid .txt file&amp;quot;) sys.exit(1) # Read the file def read_text_file(file_path): try: with open(file_path, 'r', encoding='utf-8') as file: return file.read() except Exception as e: return f&amp;quot;Error reading file: {str(e)}&amp;quot; # Use ollama.generate() def query_ollama_generate(content, prompt): full_prompt = f&amp;quot;{prompt}\n\n---\n\n{content}&amp;quot; print(f&amp;quot;Sending prompt: {prompt[:60]}...&amp;quot;) try: response = ollama.generate( model='mistral', # or 'mistral', whichever you want prompt=full_prompt ) return response['response'] except Exception as e: return f&amp;quot;Error from Ollama: {str(e)}&amp;quot; # Main content = read_text_file(filename) if &amp;quot;Error&amp;quot; in content: print(content) sys.exit(1) result = query_ollama_generate(content, prompt) print(&amp;quot;Ollama response:&amp;quot;) print(result) import ollama import os import sys # Check command-line arguments if len(sys.argv) &amp;lt; 2 or len(sys.argv) &amp;gt; 3: print(&amp;quot;Usage: python3 promptfileX.py &amp;lt;filename.txt&amp;gt; [prompt]&amp;quot;) print(&amp;quot; If no prompt is provided, defaults to 'Summarize'&amp;quot;) sys.exit(1) filename = sys.argv[1] prompt = sys.argv[2] # Check file validity if not filename.endswith(&amp;quot;.txt&amp;quot;) or not os.path.isfile(filename): print(&amp;quot;Error: Please provide a valid .txt file&amp;quot;) sys.exit(1) # Read the file def read_text_file(file_path): try: with open(file_path, 'r', encoding='utf-8') as file: return file.read() except Exception as e: return f&amp;quot;Error reading file: {str(e)}&amp;quot; # Use ollama.generate() def query_ollama_generate(content, prompt): full_prompt = f&amp;quot;{prompt}\n\n---\n\n{content}&amp;quot; print(f&amp;quot;Sending prompt: {prompt[:60]}...&amp;quot;) try: response = ollama.generate( model='mistral', # or 'mistral', whichever you want prompt=full_prompt ) return response['response'] except Exception as e: return f&amp;quot;Error from Ollama: {str(e)}&amp;quot; # Main content = read_text_file(filename) if &amp;quot;Error&amp;quot; in content: print(content) sys.exit(1) result = query_ollama_generate(content, prompt) print(&amp;quot;Ollama response:&amp;quot;) print(result) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ozaarmat"&gt; /u/ozaarmat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jk9fab/ollama_always_summarizes_a_local_text_file/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jk9fab/ollama_always_summarizes_a_local_text_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jk9fab/ollama_always_summarizes_a_local_text_file/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-26T11:22:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjsc2b</id>
    <title>I got Ollama working on my 9070xt - here's how (Windows)</title>
    <updated>2025-03-25T19:44:49+00:00</updated>
    <author>
      <name>/u/DegenerativePoop</name>
      <uri>https://old.reddit.com/user/DegenerativePoop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was struggling to get the official image of Ollama to work with my new 9070xt. It doesn't appear to natively support it yet. I was browsing and found &lt;a href="https://github.com/likelovewant/ollama-for-amd/releases/tag/v0.6.3"&gt;Ollama-For-AMD&lt;/a&gt;. I installed that version, and downloaded the &lt;a href="https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU/releases/tag/v0.6.2.4"&gt;ROCmLibs for 6.2.4&lt;/a&gt; (it would be the rocm gfx1201 file).&lt;/p&gt; &lt;p&gt;Find the &lt;code&gt;rocblas.dll&lt;/code&gt; file and the &lt;code&gt;rocblas/library&lt;/code&gt; folder within the Ollama installation folder (usually located at &lt;code&gt;C:\Users\usrname\AppData\Local\Programs\Ollama\lib\ollama\rocm&lt;/code&gt;). I am not sure where it is in linux, at least not until I get home and check)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Delete&lt;/strong&gt; the existing &lt;code&gt;rocblas/library&lt;/code&gt; folder.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Replace&lt;/strong&gt; it with the correct ROCm libraries.&lt;/li&gt; &lt;li&gt;Also replace the rocblas.dll file with the downloaded one&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That's it! It's working for me, and it works pretty well!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DegenerativePoop"&gt; /u/DegenerativePoop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjsc2b/i_got_ollama_working_on_my_9070xt_heres_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjsc2b/i_got_ollama_working_on_my_9070xt_heres_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jjsc2b/i_got_ollama_working_on_my_9070xt_heres_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-25T19:44:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk36h6</id>
    <title>Best small model to run without a gpu? (For coding and questions)</title>
    <updated>2025-03-26T04:00:00+00:00</updated>
    <author>
      <name>/u/Game-Lover44</name>
      <uri>https://old.reddit.com/user/Game-Lover44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a pretty good desktop but i want to test the limits of a laptop i have that im not sure what to do with but i want to be more productive on the go.&lt;/p&gt; &lt;p&gt;said laptop has 16 ram ddr4, 2 threads and 4 cores (intel i5 that is old), around 200 gb ssd, its a Lenovo ThinkPad T470 and it is possible i may have got something wrong.&lt;/p&gt; &lt;p&gt;would i be better of using a online ai, i just find myself in alot of places that dont have wifi for my laptop such as a waiting room.&lt;/p&gt; &lt;p&gt;i havent found a good small model yet and there no way im running anything big on this laptop.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Game-Lover44"&gt; /u/Game-Lover44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jk36h6/best_small_model_to_run_without_a_gpu_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jk36h6/best_small_model_to_run_without_a_gpu_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jk36h6/best_small_model_to_run_without_a_gpu_for_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-26T04:00:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkfsmr</id>
    <title>GPU Not Recognized in Ollama Running in LXC (Host: pve) – "cuda driver library init failure: 999" Error</title>
    <updated>2025-03-26T16:23:21+00:00</updated>
    <author>
      <name>/u/lowriskcork</name>
      <uri>https://old.reddit.com/user/lowriskcork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I’m encountering a persistent issue trying to enable GPU acceleration with Ollama within an LXC container on my host system. Although my host detects the GPU via PCI (and the appropriate kernel driver is in use), Ollama inside the container cannot initialize CUDA and falls back to CPU inference with the following error:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;unknown error initializing cuda driver library /usr/lib/x86_64-linux-gnu/nvidia/current/libcuda.so.535.216.01: cuda driver library init failure: 999. see https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md for more information &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Below I’ve included the diagnostic information I’ve gathered both from the container and the host.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Inside the Container:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;CUDA Library and NVIDIA Directory:&lt;/strong&gt;&lt;em&gt;Output snippet from the container:&lt;/em&gt;ls -l /lib/x86_64-linux-gnu/libcuda.so* ls -l /usr/lib/x86_64-linux-gnu/nvidia/current/ lrwxrwxrwx 1 root root 34 Mar 26 16:17 /lib/x86_64-linux-gnu/libcuda.so.535.216.01 -&amp;gt; /lib/x86_64-linux-gnu/libcuda.so.1 ... &lt;/li&gt; &lt;li&gt;&lt;strong&gt;LD_LIBRARY_PATH:&lt;/strong&gt;&lt;em&gt;Output:&lt;/em&gt;echo $LD_LIBRARY_PATH /usr/lib/x86_64-linux-gnu/nvidia/current:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/nvidia/current:/usr/lib/x86_64-linux-gnu: &lt;/li&gt; &lt;li&gt;&lt;strong&gt;NVIDIA GPU Details:&lt;/strong&gt;&lt;em&gt;Output from container:&lt;/em&gt;nvidia-smi Wed Mar 26 16:20:09 2025 +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 535.216.01 Driver Version: 535.216.01 CUDA Version: 12.2 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | |=========================================+======================+======================| | 0 Quadro P2000 On | 00000000:C1:00.0 Off | N/A | +-----------------------------------------+----------------------+----------------------+ &lt;/li&gt; &lt;li&gt;&lt;strong&gt;CUDA Compiler Version:&lt;/strong&gt;&lt;em&gt;Output snippet:&lt;/em&gt;nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Cuda compilation tools, release 11.8, V11.8.89 &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kernel Information:&lt;/strong&gt;&lt;em&gt;Output:&lt;/em&gt;uname -a Linux GPU 6.8.12-9-pve #1 SMP PREEMPT_DYNAMIC PMX 6.8.12-9 (2025-03-16T19:18Z) x86_64 GNU/Linux &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamic Linker Cache for CUDA:&lt;/strong&gt;&lt;em&gt;Output snippet:&lt;/em&gt;ldconfig -p | grep cuda libcuda.so.1 (libc6,x86-64) =&amp;gt; /lib/x86_64-linux-gnu/libcuda.so.1 &lt;a href="http://libcuda.so"&gt;libcuda.so&lt;/a&gt; (libc6,x86-64) =&amp;gt; /lib/x86_64-linux-gnu/libcuda.so &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Logs:&lt;/strong&gt;&lt;em&gt;Key Log Lines:&lt;/em&gt;ollama serve time=2025-03-26T16:20:41.525Z level=WARN source=gpu.go:605 msg=&amp;quot;unknown error initializing cuda driver library /usr/lib/x86_64-linux-gnu/nvidia/current/libcuda.so.535.216.01: cuda driver library init failure: 999...&amp;quot; time=2025-03-26T16:20:41.593Z level=INFO source=gpu.go:377 msg=&amp;quot;no compatible GPUs were discovered&amp;quot; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Container Environment Variables:&lt;/strong&gt;&lt;em&gt;Snippet of the output:&lt;/em&gt;cat /proc/1/environ | tr '\0' '\n' TERM=linux container=lxc &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;On the Host Machine:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I also gathered some details from the host, running on Proxmox Virtual Environment (pve):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Kernel Version and OS Info:&lt;/strong&gt;&lt;em&gt;Output:&lt;/em&gt;uname -a Linux pve 6.8.12-9-pve #1 SMP PREEMPT_DYNAMIC PMX 6.8.12-9 (2025-03-16T19:18Z) x86_64 &lt;/li&gt; &lt;li&gt;&lt;strong&gt;nvidia-smi:&lt;/strong&gt;When I ran &lt;code&gt;nvidia-smi&lt;/code&gt; on the host, I received:However, the GPU is visible via PCI later.-bash: nvidia-smi: command not found &lt;/li&gt; &lt;li&gt;&lt;strong&gt;PCI Device Listing:&lt;/strong&gt;&lt;em&gt;Output:&lt;/em&gt;lspci -nnk | grep -i nvidia c1:00.0 VGA compatible controller [0300]: NVIDIA Corporation GP106GL [Quadro P2000] [10de:1c30] (rev a1) Kernel driver in use: nvidia Kernel modules: nvidia c1:00.1 Audio device [0403]: NVIDIA Corporation GP106 High Definition Audio Controller [10de:10f1] (rev a1) &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Host Dynamic Linker Cache:&lt;/strong&gt;&lt;em&gt;Output snippet:&lt;/em&gt;ldconfig -p | grep cuda libcuda.so.1 (libc6,x86-64) =&amp;gt; /lib/x86_64-linux-gnu/libcuda.so.1 &lt;a href="http://libcuda.so"&gt;libcuda.so&lt;/a&gt; (libc6,x86-64) =&amp;gt; /lib/x86_64-linux-gnu/libcuda.so &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The Issue &amp;amp; My Questions:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Issue:&lt;/strong&gt; Despite detailed configuration inside the container, Ollama fails to initialize the CUDA driver (error 999) and falls back to CPU, even though the GPU is visible and the symlink adjustments seem correct.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Questions:&lt;/strong&gt; &lt;ol&gt; &lt;li&gt;Are there any known compatibility issues with Ollama, the specific NVIDIA driver/CUDA version, and running inside an LXC container?&lt;/li&gt; &lt;li&gt;Is there additional host-side configuration (perhaps re: GPU passthrough or container privileges) that I should check?&lt;/li&gt; &lt;li&gt;Should I provide or adjust any further details from the host (like installing or running &lt;code&gt;nvidia-smi&lt;/code&gt; on the host) to help diagnose this?&lt;/li&gt; &lt;li&gt;Are there additional debugging steps to force Ollama to successfully initialize the CUDA driver?&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any help or insights would be greatly appreciated. I’m happy to provide further logs or configuration details if needed.&lt;/p&gt; &lt;p&gt;Thanks in advance for your assistance!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Additional Note:&lt;/strong&gt;&lt;br /&gt; If anyone has suggestions for ensuring that the host’s NVIDIA tools (like &lt;code&gt;nvidia-smi&lt;/code&gt;) are available for deeper diagnostics from inside the host environment, please let me know.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lowriskcork"&gt; /u/lowriskcork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkfsmr/gpu_not_recognized_in_ollama_running_in_lxc_host/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkfsmr/gpu_not_recognized_in_ollama_running_in_lxc_host/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jkfsmr/gpu_not_recognized_in_ollama_running_in_lxc_host/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-26T16:23:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkgp0g</id>
    <title>Hardware Recommendations</title>
    <updated>2025-03-26T17:00:36+00:00</updated>
    <author>
      <name>/u/CorpusculantCortex</name>
      <uri>https://old.reddit.com/user/CorpusculantCortex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just that, I am looking for recommendations for what to prioritize hardware wise.&lt;/p&gt; &lt;p&gt;I am far overdue for a computer upgrade, current system: I7 9700kf 32gb ram RTX 2070 &lt;/p&gt; &lt;p&gt;And i have been thinking something like: I9 14900k 64g ddr5 RTX 5070TI (if ever available)&lt;/p&gt; &lt;p&gt;That was what I was thinking, but have gotten into the world of ollama relatively recently, specifically trying to host my own llm to drive my project goose ai agent. I tried a half dozen models on my current system, but as you can imagine they are either painfully slow, or painfully inadequate. So I am looking to upgrade with that as a dream, but it may be way out of reach.. the leader board for tool calling is topped by watt-tool 70B but i can't see how i could afford to run that with any efficiency. I also want to do more light /medium model training, but not llms really, I'm a data analyst/scientist/engineer and would be leveraging for optimization of work tasks. But I think anything that can handle a decent ollama instance can manage my needs there&lt;/p&gt; &lt;p&gt;The overall goal is to use this all for work tasks that I really can't send certain data offside. And or the sheer volume of frequency would make it prohibitive to go pay model.&lt;/p&gt; &lt;p&gt;Anyway my budget is ~$2000 USD and I don't have the bandwidth or trust to run down used parts right now.&lt;/p&gt; &lt;p&gt;What are your recommendations for what I should prioritize. I am very not up on the state of the art but am trying to get there quickly. Any special installations and approaches that I should learn about are also helpful! Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CorpusculantCortex"&gt; /u/CorpusculantCortex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkgp0g/hardware_recommendations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkgp0g/hardware_recommendations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jkgp0g/hardware_recommendations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-26T17:00:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkhtxd</id>
    <title>changelog for https://ollama.com/library/gemma3 ?</title>
    <updated>2025-03-26T17:46:03+00:00</updated>
    <author>
      <name>/u/caetydid</name>
      <uri>https://old.reddit.com/user/caetydid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw gemma3 got updated yesterday - is there a way to see changelogs for ollama model library updates?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/caetydid"&gt; /u/caetydid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkhtxd/changelog_for_httpsollamacomlibrarygemma3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkhtxd/changelog_for_httpsollamacomlibrarygemma3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jkhtxd/changelog_for_httpsollamacomlibrarygemma3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-26T17:46:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjtyrq</id>
    <title>Create Your Personal AI Knowledge Assistant - No Coding Needed</title>
    <updated>2025-03-25T20:50:15+00:00</updated>
    <author>
      <name>/u/PeterHash</name>
      <uri>https://old.reddit.com/user/PeterHash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've just published a guide on building a personal AI assistant using Open WebUI that works with your own documents.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What You Can Do:&lt;/strong&gt; - Answer questions from personal notes - Search through research PDFs - Extract insights from web content - Keep all data private on your own machine&lt;/p&gt; &lt;p&gt;My tutorial walks you through: - Setting up a knowledge base - Creating a research companion - Lots of tips and trick for getting precise answers - All without any programming&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Might be helpful for:&lt;/strong&gt; - Students organizing research - Professionals managing information - Anyone wanting smarter document interactions&lt;/p&gt; &lt;p&gt;Upcoming articles will cover more advanced AI techniques like function calling and multi-agent systems.&lt;/p&gt; &lt;p&gt;Curious what knowledge base you're thinking of creating. Drop a comment!&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@hautel.alex2000/open-webui-tutorial-supercharging-your-local-ai-with-rag-and-custom-knowledge-bases-334d272c8c40"&gt;Open WebUI tutorial — Supercharge Your Local AI with RAG and Custom Knowledge Bases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PeterHash"&gt; /u/PeterHash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjtyrq/create_your_personal_ai_knowledge_assistant_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jjtyrq/create_your_personal_ai_knowledge_assistant_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jjtyrq/create_your_personal_ai_knowledge_assistant_no/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-25T20:50:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkjttu</id>
    <title>Problems Using Vision Models</title>
    <updated>2025-03-26T19:07:28+00:00</updated>
    <author>
      <name>/u/GhostInThePudding</name>
      <uri>https://old.reddit.com/user/GhostInThePudding</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else having trouble with vision models from either Ollama or Huggingface? Gemma3 works fine, but I tried about 8 variants of it that are meant to be uncensored/abliterated and none of them work. For example:&lt;br /&gt; &lt;a href="https://ollama.com/huihui_ai/gemma3-abliterated"&gt;https://ollama.com/huihui_ai/gemma3-abliterated&lt;/a&gt;&lt;br /&gt; &lt;a href="https://ollama.com/nidumai/nidum-gemma-3-27b-instruct-uncensored"&gt;https://ollama.com/nidumai/nidum-gemma-3-27b-instruct-uncensored&lt;/a&gt;&lt;br /&gt; Both claim to support vision, and they run and work normally, but if you try and add an image, it simply doesn't add the image and will answers questions about the image with pure hallucinations.&lt;/p&gt; &lt;p&gt;I also tried a bunch from Huggingface, I got the GGUF version but they give errors when running. I've got plenty of Huggingface models running before, but the vision ones seem to require multiple files, but even when I create a model to load the files, I get various errors.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GhostInThePudding"&gt; /u/GhostInThePudding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkjttu/problems_using_vision_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkjttu/problems_using_vision_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jkjttu/problems_using_vision_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-26T19:07:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkth2i</id>
    <title>Has anybody gotten anything useful out of Exaone 32b?</title>
    <updated>2025-03-27T02:17:01+00:00</updated>
    <author>
      <name>/u/GVDub2</name>
      <uri>https://old.reddit.com/user/GVDub2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Installed it today, asked it to evaluate a short Python script to update restart policy on Docker containers, and it spent 10 minutes thinking, starting to seriously hallucinate halfway through. DeepSeekR1:32b (distill of Qwen2.5) thought of 45 seconds, and spit out improved streamlined code. I find it hard to believe the charts with with Ollama model that claim Exaone is all that. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GVDub2"&gt; /u/GVDub2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkth2i/has_anybody_gotten_anything_useful_out_of_exaone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jkth2i/has_anybody_gotten_anything_useful_out_of_exaone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jkth2i/has_anybody_gotten_anything_useful_out_of_exaone/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-27T02:17:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk7hh0</id>
    <title>Use Ollama to create your own AI Memory locally from 30+ types of data sources</title>
    <updated>2025-03-26T09:06:32+00:00</updated>
    <author>
      <name>/u/Short-Honeydew-7000</name>
      <uri>https://old.reddit.com/user/Short-Honeydew-7000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;We've just finished a small guide on how to set up Ollama with cognee, an open-source AI memory tool that will allow you to ingest your local data into graph/vector stores, enrich it and search it.&lt;/p&gt; &lt;p&gt;You can load all your codebase to cognee and enrich it with your README file and documentation or load images, video and audio data and merge different data sources.&lt;/p&gt; &lt;p&gt;And in the end you get to see and explore a nice looking graph.&lt;/p&gt; &lt;p&gt;Here is a short tutorial to set up Ollama with cognee:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=aZYRo-eXDzA&amp;amp;t=62s"&gt;https://www.youtube.com/watch?v=aZYRo-eXDzA&amp;amp;t=62s&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here is our Github: &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/topoteretes/cognee"&gt;https://github.com/topoteretes/cognee&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Short-Honeydew-7000"&gt; /u/Short-Honeydew-7000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jk7hh0/use_ollama_to_create_your_own_ai_memory_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jk7hh0/use_ollama_to_create_your_own_ai_memory_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jk7hh0/use_ollama_to_create_your_own_ai_memory_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-26T09:06:32+00:00</published>
  </entry>
</feed>
