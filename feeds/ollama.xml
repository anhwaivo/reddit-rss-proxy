<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-20T05:23:35+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1it28bf</id>
    <title>Running DeepSeek 70B</title>
    <updated>2025-02-19T10:16:02+00:00</updated>
    <author>
      <name>/u/Zockerdude15</name>
      <uri>https://old.reddit.com/user/Zockerdude15</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My current Setup is a System with an RTX 4090, a 7800X3D and 64 GB RAM. I can run anything up to 32B just fine with my 4090, however none of the 70B Models seem to utilize my GPU (Q4/Q2, even with gpu offloading Parameters set). Would it be possible to add something like a 4060Ti 16GB for it to work fully on gpu‘s ? Or would a dedicated system with something like 4 3060 12GB work better ? Current t/s on just CPU is about 1.2-1.5, which is too Slow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zockerdude15"&gt; /u/Zockerdude15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it28bf/running_deepseek_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it28bf/running_deepseek_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1it28bf/running_deepseek_70b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T10:16:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1it2ahn</id>
    <title>Bluetooth air quality analysis using Gemma AI ( source code available)</title>
    <updated>2025-02-19T10:20:13+00:00</updated>
    <author>
      <name>/u/bleuio</name>
      <uri>https://old.reddit.com/user/bleuio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1it2ahn/bluetooth_air_quality_analysis_using_gemma_ai/"&gt; &lt;img alt="Bluetooth air quality analysis using Gemma AI ( source code available)" src="https://external-preview.redd.it/UHpmaNAE0LOr_rVvObZX6INnvcJ0rTYBSLeN_WR3cBc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f7d44b7e7f75cb06354268111d61b224ef3b42b" title="Bluetooth air quality analysis using Gemma AI ( source code available)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bleuio"&gt; /u/bleuio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bleuio.com/blog/chat-with-hibouair-using-bleuio-smart-air-quality-analysis-with-google-technologies/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it2ahn/bluetooth_air_quality_analysis_using_gemma_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1it2ahn/bluetooth_air_quality_analysis_using_gemma_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T10:20:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1it2dag</id>
    <title>How to run llama3.1 on CPU only?</title>
    <updated>2025-02-19T10:25:31+00:00</updated>
    <author>
      <name>/u/Fantastic-Method2046</name>
      <uri>https://old.reddit.com/user/Fantastic-Method2046</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have latest ollama installed on a laptop with RTX3050 GPU. Now I'd like to run LLM inference (for example with previously downloaded llama3.1) on CPU only. Please help. I tried many things found on the internet. Some of them not works some of them runs with errors (for example Error: unknown flag: --num-gpu) etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fantastic-Method2046"&gt; /u/Fantastic-Method2046 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it2dag/how_to_run_llama31_on_cpu_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it2dag/how_to_run_llama31_on_cpu_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1it2dag/how_to_run_llama31_on_cpu_only/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T10:25:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1it4gxn</id>
    <title>Run LLM on 5090 vs 3090 - how the 5090 performs running deepseek-r1 using Ollama?</title>
    <updated>2025-02-19T12:36:14+00:00</updated>
    <author>
      <name>/u/chain-77</name>
      <uri>https://old.reddit.com/user/chain-77</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1it4gxn/run_llm_on_5090_vs_3090_how_the_5090_performs/"&gt; &lt;img alt="Run LLM on 5090 vs 3090 - how the 5090 performs running deepseek-r1 using Ollama?" src="https://external-preview.redd.it/Ou-vKSYlV1cbaUGSZy0hMZHyER-_tVW8jE9HgZJ1UnA.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4669ccb1227cf76d727ffbf02d810dd6ad02d58c" title="Run LLM on 5090 vs 3090 - how the 5090 performs running deepseek-r1 using Ollama?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chain-77"&gt; /u/chain-77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/YxJYhhrhrDk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it4gxn/run_llm_on_5090_vs_3090_how_the_5090_performs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1it4gxn/run_llm_on_5090_vs_3090_how_the_5090_performs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T12:36:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1itaci0</id>
    <title>Creating a model that will instrinsically behave the way I want no matter the prompt</title>
    <updated>2025-02-19T16:58:44+00:00</updated>
    <author>
      <name>/u/Nuvola_Rossa</name>
      <uri>https://old.reddit.com/user/Nuvola_Rossa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need to create a model that will behave the way I want (= talking in a certain way) without any prompt engineering, no matter what the user request is. I can achieve this using the modelfile and mofying the system prompt on ollama, but then I still don't have a GGUF file that I can export (this is mandatory for my study case)... &lt;/p&gt; &lt;p&gt;So I don't really need any training as any generic model (I'm using llama3.2) has all the knowledge I want already and I don't know what to do. Any advices?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nuvola_Rossa"&gt; /u/Nuvola_Rossa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itaci0/creating_a_model_that_will_instrinsically_behave/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itaci0/creating_a_model_that_will_instrinsically_behave/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1itaci0/creating_a_model_that_will_instrinsically_behave/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T16:58:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1it5szj</id>
    <title>How to build and run Ollama on PPC64LE systems</title>
    <updated>2025-02-19T13:45:08+00:00</updated>
    <author>
      <name>/u/icbts</name>
      <uri>https://old.reddit.com/user/icbts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1it5szj/how_to_build_and_run_ollama_on_ppc64le_systems/"&gt; &lt;img alt="How to build and run Ollama on PPC64LE systems" src="https://external-preview.redd.it/3XRNKRY7mw1IR8KHZ4Jh9lT_fAkGQsxOhoQnqIF98fQ.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a44da49e0908c1fdf43aef80ad4a34700ca7834c" title="How to build and run Ollama on PPC64LE systems" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/icbts"&gt; /u/icbts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=P4iEZiwfLm8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it5szj/how_to_build_and_run_ollama_on_ppc64le_systems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1it5szj/how_to_build_and_run_ollama_on_ppc64le_systems/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T13:45:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1isuyd4</id>
    <title>8x AMD Instinct Mi50 AI Server #1 is in Progress..</title>
    <updated>2025-02-19T02:45:22+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1isuyd4/8x_amd_instinct_mi50_ai_server_1_is_in_progress/"&gt; &lt;img alt="8x AMD Instinct Mi50 AI Server #1 is in Progress.." src="https://preview.redd.it/q7q2q1n970ke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=051ed274f0a297d2b44d83c02fd2cc80ce6d0b7f" title="8x AMD Instinct Mi50 AI Server #1 is in Progress.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q7q2q1n970ke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isuyd4/8x_amd_instinct_mi50_ai_server_1_is_in_progress/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isuyd4/8x_amd_instinct_mi50_ai_server_1_is_in_progress/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T02:45:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1isqhm1</id>
    <title>Ollama Shell -- improved Terminal app for using local models</title>
    <updated>2025-02-18T23:22:47+00:00</updated>
    <author>
      <name>/u/sunkencity999</name>
      <uri>https://old.reddit.com/user/sunkencity999</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1isqhm1/ollama_shell_improved_terminal_app_for_using/"&gt; &lt;img alt="Ollama Shell -- improved Terminal app for using local models" src="https://b.thumbs.redditmedia.com/i1OWSPjS_iJXLlQo0MDCNlUzL5p_Rm9K-O4rQsmb8NM.jpg" title="Ollama Shell -- improved Terminal app for using local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y'all,&lt;/p&gt; &lt;p&gt;I am personally a huge fan of working directly in the terminal; the existing terminal shell for Ollama, in my opinion, leaves much to be desired, functionality and aesthetics-wise. SO, I figured I would create a Shell application that allows you to work with Ollama and models in the terminal in a way that is practical and reasonably efficient. You can analyze documents by dragging-and-dropping them in the chat, manage models (pull and delete), have continuous chat history and save system prompts for use as necessary. If working in the terminal / shell is something you enjoy as well, please give it a shot. Free, and of course I welcome contributors.&lt;br /&gt; &lt;a href="https://github.com/sunkencity999/ollama_shell"&gt;Ollama Shell on Github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ym0w7cmtizje1.png?width=1071&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=427fbffce0d8ba45e5906e3342f5aa1028bcd959"&gt;Main Interface&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tpfh7k4vizje1.png?width=1059&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0fa8276a0e793475538033c655452323b68354bc"&gt;Prompt selection after Model selection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n2mhbjjxizje1.png?width=920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d99ceda3ceb10582f95be390b8bf8b12a3fc641e"&gt;Query answered by LLM and provided (deepseek-r1:14b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunkencity999"&gt; /u/sunkencity999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isqhm1/ollama_shell_improved_terminal_app_for_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isqhm1/ollama_shell_improved_terminal_app_for_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isqhm1/ollama_shell_improved_terminal_app_for_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-18T23:22:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1itbr79</id>
    <title>Optimal Hardware for Running Ollama Models with Marker for PDF to Markdown Conversion</title>
    <updated>2025-02-19T17:52:59+00:00</updated>
    <author>
      <name>/u/BigdadEdge</name>
      <uri>https://old.reddit.com/user/BigdadEdge</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I'm planning to convert large PDFs, like textbooks, into Markdown using the Marker tool in conjunction with Ollama's local LLMs. Given this setup, what hardware specifications would you recommend? Specifically, I'm interested in:&lt;/p&gt; &lt;p&gt;- The most suitable Ollama model for this task for the minimal hardware requirements, I still want the Ollama model to be fast, but I do not want to spend too much money on online computation when renting a server. &lt;/p&gt; &lt;p&gt;- Minimum and recommended CPU and RAM requirements&lt;/p&gt; &lt;p&gt;- The necessity and impact of a GPU on performance&lt;/p&gt; &lt;p&gt;Any insights or experiences would be greatly appreciated!&lt;/p&gt; &lt;p&gt;You can check out the [&lt;a href="https://github.com/VikParuchuri/marker"&gt;Marker GitHub repository&lt;/a&gt;] for more details on the project.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BigdadEdge"&gt; /u/BigdadEdge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itbr79/optimal_hardware_for_running_ollama_models_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itbr79/optimal_hardware_for_running_ollama_models_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1itbr79/optimal_hardware_for_running_ollama_models_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T17:52:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1itcvxk</id>
    <title>Seeking Recommendations on Open-Source RAG Frameworks</title>
    <updated>2025-02-19T18:37:39+00:00</updated>
    <author>
      <name>/u/BigdadEdge</name>
      <uri>https://old.reddit.com/user/BigdadEdge</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I’ve been exploring the &lt;a href="https://github.com/Mintplex-Labs/anything-llm"&gt;Anything LLM GitHub repository&lt;/a&gt; for LLM-based retrieval methods. However, it does not support advanced RAG techniques like Hybrid, Graph, or Agentic RAG. I'm looking for open-source frameworks or GitHub projects that implement these advanced methods. Any guidance on choosing the right tools for handling more complex data and tasks would be greatly appreciated.&lt;/p&gt; &lt;p&gt;Best regards,&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BigdadEdge"&gt; /u/BigdadEdge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itcvxk/seeking_recommendations_on_opensource_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itcvxk/seeking_recommendations_on_opensource_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1itcvxk/seeking_recommendations_on_opensource_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T18:37:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1it5ob9</id>
    <title>Model for object detection with bounding box</title>
    <updated>2025-02-19T13:38:44+00:00</updated>
    <author>
      <name>/u/PertinentOverthinker</name>
      <uri>https://old.reddit.com/user/PertinentOverthinker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there, i am newbie when it comes to computer vision and AI. I am wondering if there is AI model that can detect object of interest and draw the bounding box around it or give the coordinate of the bounding box to be plotted separately&lt;/p&gt; &lt;p&gt;thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PertinentOverthinker"&gt; /u/PertinentOverthinker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it5ob9/model_for_object_detection_with_bounding_box/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it5ob9/model_for_object_detection_with_bounding_box/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1it5ob9/model_for_object_detection_with_bounding_box/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T13:38:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1istkgr</id>
    <title>Does Ollama cache prompts?</title>
    <updated>2025-02-19T01:40:06+00:00</updated>
    <author>
      <name>/u/palaceofcesi</name>
      <uri>https://old.reddit.com/user/palaceofcesi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay I’m a little confused and freaked out right now but my first thought is that I didn’t read the documentation properly.&lt;/p&gt; &lt;p&gt;Does Ollama cache prompts? &lt;/p&gt; &lt;p&gt;I previously used the deepseek-r1:32B with Ollama to create a presentation about a business product, call it Product A.&lt;/p&gt; &lt;p&gt;Then I used deepseek to create a presentation about Product B. In my prompt “ollama run deepseek-r1:32b $prompt” I made no reference whatsoever to Product A. And yet, in its response, I received multiple references to Product A in my creating a presentation for Product B. &lt;/p&gt; &lt;p&gt;The model was praising how well these two products work together.&lt;/p&gt; &lt;p&gt;That’s great, but I was not aware of any prompt caching in Ollama. This has a huge security implication because I’m running Ollama on sensitive documents on internal networks of non-air-gapped systems so if Ollama is caching the prompts and/or outputs and potentially uploading them over the network that would be a huge security risk.&lt;/p&gt; &lt;p&gt;Can someone tell me what’s going on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/palaceofcesi"&gt; /u/palaceofcesi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1istkgr/does_ollama_cache_prompts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1istkgr/does_ollama_cache_prompts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1istkgr/does_ollama_cache_prompts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T01:40:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1itcpq4</id>
    <title>Specialized model without notice in Model desc on site.</title>
    <updated>2025-02-19T18:30:46+00:00</updated>
    <author>
      <name>/u/HeadGr</name>
      <uri>https://old.reddit.com/user/HeadGr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've downloaded &lt;strong&gt;3logic/llama-3.1-8b-instruct-phactual_fp16_he20&lt;/strong&gt; and found it was trained for assist patiens with single certain decease. When I asked &amp;quot;What d'you know about Alpha Centauri&amp;quot; model answered kinda &amp;quot;I know that Alpha Centauri is star system but I have no idea how it related to Diabetes type 2&amp;quot;. &lt;/p&gt; &lt;p&gt;Don't waste time and 16 Gb of disk space if you don't need exactly this assistant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HeadGr"&gt; /u/HeadGr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itcpq4/specialized_model_without_notice_in_model_desc_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itcpq4/specialized_model_without_notice_in_model_desc_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1itcpq4/specialized_model_without_notice_in_model_desc_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T18:30:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ite280</id>
    <title>Olmo2?</title>
    <updated>2025-02-19T19:23:24+00:00</updated>
    <author>
      <name>/u/GVDub2</name>
      <uri>https://old.reddit.com/user/GVDub2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been messing about with Olmo2:13.2b recently and I'm finding it reasonably decent for basic chat function and idea generation. Anybody else been diving into this one? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GVDub2"&gt; /u/GVDub2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ite280/olmo2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ite280/olmo2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ite280/olmo2/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T19:23:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1itq6hz</id>
    <title>What chars aren't allowed in Ollama Modelfile?</title>
    <updated>2025-02-20T04:34:20+00:00</updated>
    <author>
      <name>/u/Hot_Reputation_1421</name>
      <uri>https://old.reddit.com/user/Hot_Reputation_1421</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I can't seem to find what characters aren't allowed in the Model file. I am constantly getting an error, but it works when I have a little amount. What can I do about this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hot_Reputation_1421"&gt; /u/Hot_Reputation_1421 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itq6hz/what_chars_arent_allowed_in_ollama_modelfile/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itq6hz/what_chars_arent_allowed_in_ollama_modelfile/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1itq6hz/what_chars_arent_allowed_in_ollama_modelfile/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-20T04:34:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1itr0mu</id>
    <title>Don't be fooled: `ollama run llava "describe this image: ./art.jpg"`</title>
    <updated>2025-02-20T05:22:25+00:00</updated>
    <author>
      <name>/u/firesalamander</name>
      <uri>https://old.reddit.com/user/firesalamander</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel very silly.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The image you've provided appears to be a digital artwork or illustration featuring an abstract design with geometric shapes and forms. There are blocks of color in shades of blue, green, and brown, which may suggest some sort of landscape or natural scene. The shapes and lines seem to overlap and intersect in a way that creates depth and dimensionality. The overall style is somewhat reminiscent of cubism or other forms of abstract art, where the focus is on the relationship between shapes and colors rather than on realistic representation.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Awesome! Wait a minute...&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo find / -name art.jpg&lt;/code&gt; = no results. 😂. Dang thing was hallucinating.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/firesalamander"&gt; /u/firesalamander &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itr0mu/dont_be_fooled_ollama_run_llava_describe_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itr0mu/dont_be_fooled_ollama_run_llava_describe_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1itr0mu/dont_be_fooled_ollama_run_llava_describe_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-20T05:22:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ita4ph</id>
    <title>deepseek and ollama to create knowledge graphs</title>
    <updated>2025-02-19T16:50:05+00:00</updated>
    <author>
      <name>/u/Short-Honeydew-7000</name>
      <uri>https://old.reddit.com/user/Short-Honeydew-7000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ita4ph/deepseek_and_ollama_to_create_knowledge_graphs/"&gt; &lt;img alt="deepseek and ollama to create knowledge graphs" src="https://external-preview.redd.it/4ijyY7Qu-05QDvrnzoIwg803mKYJkGjXXn0YyImxyhE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6da44814a5c90a2b4de967ae82c0dd3e786dc7d2" title="deepseek and ollama to create knowledge graphs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Short-Honeydew-7000"&gt; /u/Short-Honeydew-7000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cognee.ai/blog/deep-dives/deepseek-ollama-and-graphs"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ita4ph/deepseek_and_ollama_to_create_knowledge_graphs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ita4ph/deepseek_and_ollama_to_create_knowledge_graphs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T16:50:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1it71ge</id>
    <title>Ollama Portable Zip for Intel GPU has now come to Linux</title>
    <updated>2025-02-19T14:41:50+00:00</updated>
    <author>
      <name>/u/bigbigmind</name>
      <uri>https://old.reddit.com/user/bigbigmind</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;a href="https://github.com/intel/ipex-llm/releases/tag/v2.2.0-nightly"&gt;Download &lt;/a&gt;and unzip&lt;/li&gt; &lt;li&gt;./start-ollama.sh&lt;/li&gt; &lt;li&gt;./ollama run deepseek-r1:7b&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;See the guide &lt;a href="https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portablze_zip_quickstart.md"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigbigmind"&gt; /u/bigbigmind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it71ge/ollama_portable_zip_for_intel_gpu_has_now_come_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1it71ge/ollama_portable_zip_for_intel_gpu_has_now_come_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1it71ge/ollama_portable_zip_for_intel_gpu_has_now_come_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T14:41:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1itowvo</id>
    <title>Ollama "No Modelfile or safetensors files found" Error Despite Pulling Mistral</title>
    <updated>2025-02-20T03:24:46+00:00</updated>
    <author>
      <name>/u/bobboganushed</name>
      <uri>https://old.reddit.com/user/bobboganushed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, super novice here!&lt;/p&gt; &lt;p&gt;Issue&lt;/p&gt; &lt;p&gt;I’m trying to create a custom AI model in Ollama using the following command:&lt;/p&gt; &lt;p&gt;ollama create my-ai -f &amp;quot;system: You are a personal AI assistant for Robert. Your tone is strategic. You remember past conversations.&amp;quot;&lt;/p&gt; &lt;p&gt;However, I keep getting this error: Error: no Modelfile or safetensors files found&lt;/p&gt; &lt;p&gt;What I’ve Tried&lt;/p&gt; &lt;p&gt;Ran ollama list, and it shows mistral:latest is installed.&lt;/p&gt; &lt;p&gt;Ran ollama pull mistral again, and it successfully downloaded 4.1GB.&lt;/p&gt; &lt;p&gt;Checked the C:\Users\rober.ollama\models\mistral directory, but only found a small latest file (1KB). &lt;/p&gt; &lt;p&gt;No Modelfile or safetensors files. Reinstalled Ollama twice—same issue persists.&lt;/p&gt; &lt;p&gt;System Details&lt;/p&gt; &lt;p&gt;OS: Windows 10/11 Python Version: 3.13.2&lt;/p&gt; &lt;p&gt;Question&lt;/p&gt; &lt;p&gt;Why is the model not creating properly?&lt;/p&gt; &lt;p&gt;Do I need to manually download additional files?&lt;/p&gt; &lt;p&gt;Is there a specific directory where safetensors should be?&lt;/p&gt; &lt;p&gt;Any help would be appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobboganushed"&gt; /u/bobboganushed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itowvo/ollama_no_modelfile_or_safetensors_files_found/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itowvo/ollama_no_modelfile_or_safetensors_files_found/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1itowvo/ollama_no_modelfile_or_safetensors_files_found/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-20T03:24:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1itgqm2</id>
    <title>Blind Guy Looking For Help -- Open Source Robust NLU Engine Cominb Shortly.</title>
    <updated>2025-02-19T21:11:31+00:00</updated>
    <author>
      <name>/u/mdizak</name>
      <uri>https://old.reddit.com/user/mdizak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;No idea if this is allowed, and although not Ollama specific, right in line with the self hosted, privacy focused AI stuff.&lt;/p&gt; &lt;p&gt;I'm lead on Cicero, mission statement at: &lt;a href="https://cicero.sh/mission"&gt;https://cicero.sh/mission&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just about to open source the newly upgraded NLU engine coined Sophia. Specs and demo you can play with at &lt;a href="https://cicero.sh/sophia"&gt;https://cicero.sh/sophia&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Great if you have AI agentic workflows of any kind, and within a day or two will be open sourced under dual license. If you play with that demo, you will realize the POS tagger is terribl and mixes up verbs / nouns all the time. This has been fully resolved in the latest version.&lt;/p&gt; &lt;p&gt;I want a nice release though, because I deserve something good to finally happen. Years ago life decided to have some fun with me and all in the span of about 16 months I went suddenly and totally blind within a few weeks, my primary business colleague of 9 years was murdered via professional hit, was forced by immigration to move back to Canada resulting in the loss of my fiance and dogs of 7 years, toxic family decided gas lighting me for a while was a good idea, and other challenges. It's been thrilling, I tell ya.&lt;/p&gt; &lt;p&gt;So now I got Cicero, and although I make bold claims, I have the skill and experience to see it through. Of course, will be doing the right thing and open sourcing everything.&lt;/p&gt; &lt;p&gt;I'd like a nice release in the coming days of the NLU engine, and could really use a helping hand from a kind soul here. Just two quick things:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Need a logo. Asked Chat GPT and got this: &lt;a href="https://cicero.sh/cicero.png"&gt;https://cicero.sh/cicero.png&lt;/a&gt; -- no idea, how does that look? Is it decent, good to go? If it's not good, can you throw a quick prompt or two out for me and get me something usable?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;With the release of the NLU engine, also want to release first episode of a faceless podcast. I have no idea about the various free / open source text 2 video services, because why would I? Any kind soul willing to throw a couple prompts in and just get me something decent and short that I can play on loop for the video part? &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Would want to stick with the ancient Rome theme, Cicero was a great statesman and senator, great rags to success story, and although controversial was definitely a man for the people. I have no idea, maybe a landscape of an ancient Roman village or market or something, and maybe some kind of modern twist added but don't know how so maybe just classical would be cool. I have no idea, just something along with a still image I can use for thumbnails would be greatly appreciated.&lt;/p&gt; &lt;p&gt;If anyone could help with those two above points, would really appreciate it. In exchange, will get that NLU engine open sourced right away here, and continue working on awesome open source software for everyone, as everything will be open sourced within Cicero...&lt;/p&gt; &lt;p&gt;I don't expect this as it's way too much of an ask, but if anyone is willing to play with oneof those web design AI things that would be awesome. That design you see is from &lt;a href="https://apexpl.io/"&gt;https://apexpl.io/&lt;/a&gt; another project I developed that went nowhere, and I kid you not, that was 18 months, $12k USD and 6 designers. Unreal, eh?&lt;/p&gt; &lt;p&gt;Nothing fancy, no Javascript frameworks, just simple, clean HTML / CSS code that conveys the information in a concise and professional format. If anyone's willing to do that, and pass a couple messages back and forth with me while I complete integration, would be beyond grateful. Happy to link you as sponsor, add &amp;quot;designed by&amp;quot; link in footer, and contract out to you for all future design and other related work I need in the future once I get money rolling again as I plan on launching loads of sites.&lt;/p&gt; &lt;p&gt;Wow, way too long, sorry about that. If interested in the NLU engine release, just keep your eye on &lt;a href="/r/opensource"&gt;/r/opensource&lt;/a&gt; in the next 2 or 3 days and it'll show up. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mdizak"&gt; /u/mdizak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itgqm2/blind_guy_looking_for_help_open_source_robust_nlu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itgqm2/blind_guy_looking_for_help_open_source_robust_nlu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1itgqm2/blind_guy_looking_for_help_open_source_robust_nlu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T21:11:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1itmqfo</id>
    <title>Run structured visual extraction locally with Ollama</title>
    <updated>2025-02-20T01:35:06+00:00</updated>
    <author>
      <name>/u/AutonomousScott</name>
      <uri>https://old.reddit.com/user/AutonomousScott</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/vlm-run/vlmrun-hub"&gt;https://github.com/vlm-run/vlmrun-hub&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AutonomousScott"&gt; /u/AutonomousScott &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itmqfo/run_structured_visual_extraction_locally_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itmqfo/run_structured_visual_extraction_locally_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1itmqfo/run_structured_visual_extraction_locally_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-20T01:35:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1itndet</id>
    <title>Rag With Your CSV File As A Script For Your Chatbot Using Ollama DeepSeekR1 and Llama Locally</title>
    <updated>2025-02-20T02:06:35+00:00</updated>
    <author>
      <name>/u/Spirited-Wind6803</name>
      <uri>https://old.reddit.com/user/Spirited-Wind6803</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone. I just want to share with you this tutorial that I am: &lt;/p&gt; &lt;p&gt;Creating a CSV file as a script for my bot to handle unique business questions like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;&amp;quot;Does your business ship internationally?&amp;quot;&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;&amp;quot;What is your return policy?&amp;quot;&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Link is here: &lt;a href="https://www.youtube.com/watch?v=fgEPXblIAcg"&gt;https://www.youtube.com/watch?v=fgEPXblIAcg&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I will show you how I use &lt;strong&gt;RAG (Retrieval-Augmented Generation)&lt;/strong&gt; and &lt;strong&gt;Ollama Deepseek-R1&lt;/strong&gt; to build a powerful chatbot backend that can answer customer queries efficiently and accurately tailor to your business policy. &lt;/p&gt; &lt;p&gt;I would love to hear your thoughts and feedback! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spirited-Wind6803"&gt; /u/Spirited-Wind6803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itndet/rag_with_your_csv_file_as_a_script_for_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itndet/rag_with_your_csv_file_as_a_script_for_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1itndet/rag_with_your_csv_file_as_a_script_for_your/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-20T02:06:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1itj1l9</id>
    <title>PyVisionAI: Instantly Extract &amp; Describe Content from Documents with Vision LLMs(Now with Claude and homebrew)</title>
    <updated>2025-02-19T22:46:44+00:00</updated>
    <author>
      <name>/u/Electrical-Two9833</name>
      <uri>https://old.reddit.com/user/Electrical-Two9833</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;If you deal with documents and images and want to save time on parsing, analyzing, or describing them, PyVisionAI is for you.&lt;/strong&gt; It unifies multiple Vision LLMs (GPT-4 Vision, Claude Vision, or local Llama2-based models) under one workflow, so you can extract text and images from PDF, DOCX, PPTX, and HTML—even capturing fully rendered web pages—and generate human-like explanations for images or diagrams.&lt;/p&gt; &lt;h1&gt;Why It’s Useful&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;All-in-One&lt;/strong&gt;: Handle text extraction and image description across various file types—no juggling separate scripts or libraries.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible&lt;/strong&gt;: Go with &lt;strong&gt;cloud-based&lt;/strong&gt; GPT-4/Claude for speed, or &lt;strong&gt;local&lt;/strong&gt; Llama models for privacy.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CLI &amp;amp; Python Library&lt;/strong&gt;: Use simple terminal commands or integrate PyVisionAI right into your Python projects.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple OS Support&lt;/strong&gt;: Works on macOS (via Homebrew), Windows, and Linux (via pip).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No More Dependency Hassles&lt;/strong&gt;: On macOS, just run one Homebrew command (plus a couple optional installs if you need advanced features).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Quick macOS Setup (Homebrew)&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;brew tap mdgrey33/pyvisionai brew install pyvisionai # Optional: Needed for dynamic HTML extraction playwright install chromium # Optional: For Office documents (DOCX, PPTX) brew install --cask libreoffice &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This leverages Python 3.11+ automatically (as required by the Homebrew formula). If you’re on Windows or Linux, you can install via &lt;code&gt;pip install pyvisionai&lt;/code&gt; (Python 3.8+).&lt;/p&gt; &lt;h1&gt;Core Features (Confirmed by the READMEs)&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Document Extraction&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;PDFs, DOCXs, PPTXs, HTML (with JS), and images are all fair game.&lt;/li&gt; &lt;li&gt;Extract text, tables, and even generate screenshots of HTML.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Image Description&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Analyze diagrams, charts, photos, or scanned pages using GPT-4, Claude, or a &lt;strong&gt;local&lt;/strong&gt; Llama model via &lt;a href="https://github.com/ollama/ollama"&gt;Ollama&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Customize your prompts to control the level of detail.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CLI &amp;amp; Python API&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CLI&lt;/strong&gt;: &lt;code&gt;file-extract&lt;/code&gt; for documents, &lt;code&gt;describe-image&lt;/code&gt; for images.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: &lt;code&gt;create_extractor(...)&lt;/code&gt; to handle large sets of files; &lt;code&gt;describe_image_*&lt;/code&gt; functions for quick references in code.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance &amp;amp; Reliability&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Parallel processing, thorough logging, and automatic retries for rate-limited APIs.&lt;/li&gt; &lt;li&gt;Test coverage sits above 80%, so it’s stable enough for production scenarios.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Sample Code&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;from pyvisionai import create_extractor, describe_image_claude # 1. Extract content from PDFs extractor = create_extractor(&amp;quot;pdf&amp;quot;, model=&amp;quot;gpt4&amp;quot;) # or &amp;quot;claude&amp;quot;, &amp;quot;llama&amp;quot; extractor.extract(&amp;quot;quarterly_reports/&amp;quot;, &amp;quot;analysis_out/&amp;quot;) # 2. Describe an image or diagram desc = describe_image_claude( &amp;quot;circuit.jpg&amp;quot;, prompt=&amp;quot;Explain what this circuit does, focusing on the components&amp;quot; ) print(desc) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Choose Your Model&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cloud&lt;/strong&gt;:export OPENAI_API_KEY=&amp;quot;your-openai-key&amp;quot; # GPT-4 Vision export ANTHROPIC_API_KEY=&amp;quot;your-anthropic-key&amp;quot; # Claude Vision &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local&lt;/strong&gt;:brew install ollama ollama pull llama2-vision # Then run: describe-image -i diagram.jpg -u llama &lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;System Requirements&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt; (Homebrew install): Python 3.11+&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Windows/Linux&lt;/strong&gt;: Python 3.8+ via &lt;code&gt;pip install pyvisionai&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1GB+ Free Disk Space&lt;/strong&gt; (local models may require more)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Want More?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Official Site&lt;/strong&gt;: &lt;a href="https://pyvisionai.com/"&gt;pyvisionai.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href="https://github.com/MDGrey33/pyvisionai"&gt;MDGrey33/pyvisionai&lt;/a&gt; – open issues or PRs if you spot bugs!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Docs&lt;/strong&gt;: &lt;a href="https://github.com/MDGrey33/pyvisionai#readme"&gt;Full README &amp;amp; Usage&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Homebrew Formula&lt;/strong&gt;: &lt;a href="https://github.com/mdgrey33/homebrew-pyvisionai"&gt;mdgrey33/homebrew-pyvisionai&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Help Shape the Future of PyVisionAI&lt;/h1&gt; &lt;p&gt;If there’s a feature you need—maybe specialized document parsing, new prompt templates, or deeper local model integration—&lt;strong&gt;please ask or open a feature request&lt;/strong&gt; on GitHub. I want PyVisionAI to fit right into your workflow, whether you’re doing academic research, business analysis, or general-purpose data wrangling.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Give it a try and share your ideas!&lt;/strong&gt; I’d love to know how PyVisionAI can make your work easier.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electrical-Two9833"&gt; /u/Electrical-Two9833 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itj1l9/pyvisionai_instantly_extract_describe_content/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itj1l9/pyvisionai_instantly_extract_describe_content/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1itj1l9/pyvisionai_instantly_extract_describe_content/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T22:46:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1isvd1h</id>
    <title>Ollama Deepseek-R1 AI writes my Obsidian notes by watching my screen (open source)</title>
    <updated>2025-02-19T03:05:26+00:00</updated>
    <author>
      <name>/u/louis3195</name>
      <uri>https://old.reddit.com/user/louis3195</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1isvd1h/ollama_deepseekr1_ai_writes_my_obsidian_notes_by/"&gt; &lt;img alt="Ollama Deepseek-R1 AI writes my Obsidian notes by watching my screen (open source)" src="https://external-preview.redd.it/NGI4NHl2aTBpMGtlMaEzGEUaUQonH8wQBp1lwNMA6DwRZxj8nxN2XpHfUUNy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6aa25cc908b1aaf94262c3fa735cebef775b5e9c" title="Ollama Deepseek-R1 AI writes my Obsidian notes by watching my screen (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/louis3195"&gt; /u/louis3195 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1eo4wui0i0ke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1isvd1h/ollama_deepseekr1_ai_writes_my_obsidian_notes_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1isvd1h/ollama_deepseekr1_ai_writes_my_obsidian_notes_by/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-19T03:05:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1itm5i4</id>
    <title>A fully local, cross-platform AI chat application powered by Ollama</title>
    <updated>2025-02-20T01:07:26+00:00</updated>
    <author>
      <name>/u/rayon_io</name>
      <uri>https://old.reddit.com/user/rayon_io</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1itm5i4/a_fully_local_crossplatform_ai_chat_application/"&gt; &lt;img alt="A fully local, cross-platform AI chat application powered by Ollama" src="https://external-preview.redd.it/OTZwbzlrZnQxN2tlMakES6FH2W8yfgBPhVVYUvoYQmRIwx5RUSE0YuLVDLKW.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da479c78d326f4b6ba4fa3fd39bd742837499c43" title="A fully local, cross-platform AI chat application powered by Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rayon_io"&gt; /u/rayon_io &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/06vaqjft17ke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1itm5i4/a_fully_local_crossplatform_ai_chat_application/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1itm5i4/a_fully_local_crossplatform_ai_chat_application/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-20T01:07:26+00:00</published>
  </entry>
</feed>
