<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-09-03T18:08:32+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1n4wmim</id>
    <title>Customize a existing model without copying it?</title>
    <updated>2025-08-31T14:47:46+00:00</updated>
    <author>
      <name>/u/Cartoon_Corpze</name>
      <uri>https://old.reddit.com/user/Cartoon_Corpze</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I have Ollama installed in: &lt;code&gt;D:\\PROGRAMFILES\\Ollama&lt;/code&gt;&lt;/p&gt; &lt;p&gt;My models are located in: &lt;code&gt;D:\PROGRAMDATA\Ollama_Models\blobs&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I'm not familiar with Ollama but I'd like to play around with it.&lt;/p&gt; &lt;p&gt;So let's say, I have this model installed &lt;code&gt;qwen3:30b&lt;/code&gt;, but currently it uses it's default configurations and settings.&lt;/p&gt; &lt;p&gt;To save on drive space I would like to NOT copy the entire model.&lt;/p&gt; &lt;p&gt;I just want to use a different template, change what character/personality it has and perhaps set a few variables like the temperature for more creative (or deterministic) responses.&lt;/p&gt; &lt;p&gt;I tried looking up online how to do this but it's a little bit vague to me how I will exactly do this with my specific system configuration.&lt;/p&gt; &lt;p&gt;I don't want to change or mess up my organized directories or end up using extra drive space on accident. Any help is greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cartoon_Corpze"&gt; /u/Cartoon_Corpze &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4wmim/customize_a_existing_model_without_copying_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4wmim/customize_a_existing_model_without_copying_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n4wmim/customize_a_existing_model_without_copying_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-31T14:47:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1n570cy</id>
    <title>The outerloop v the inner loop of agents</title>
    <updated>2025-08-31T21:44:06+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've just shipped a multi-agent solution for a Fortune500. Its been an incredible learning journey and the one key insight that unlocked a lot of development velocity was separating the &lt;strong&gt;outer-loop&lt;/strong&gt; from the &lt;strong&gt;inner-loop&lt;/strong&gt; of an agents.&lt;/p&gt; &lt;p&gt;The &lt;strong&gt;inner loop&lt;/strong&gt; is the control cycle of a single agent that hat gets some work (human or otherwise) and tries to complete it with the assistance of an LLM. The inner loop of an agent is directed by the task it gets, the tools it exposes to the LLM, its system prompt and optionally some state to checkpoint work during the loop. In this inner loop, a developer is responsible for idempotency, compensating actions (if certain tools fails, what should happen to previous operations), and other business logic concerns that helps them build a great user experience. This is where workflow engines like &lt;a href="https://github.com/temporalio/temporal"&gt;Temporal&lt;/a&gt; excel, so we leaned on them rather than reinventing the wheel.&lt;/p&gt; &lt;p&gt;The &lt;strong&gt;outer loop&lt;/strong&gt; is the control loop to route and coordinate work between agents. Here dependencies are coarse grained, where planning and orchestration are more compact and terse. The key shift is in granularity: from fine-grained task execution inside an agent to higher-level coordination across agents. We realized this problem looks more like proxying than full-blown workflow orchestration. This is where next generation proxy infrastructure like &lt;a href="https://github.com/katanemo/archgw"&gt;Arch&lt;/a&gt; excel, so we leaned on that.&lt;/p&gt; &lt;p&gt;This separation gave our customer a much cleaner mental model, so that they could innovate on the outer loop independently from the inner loop and make it more flexible for developers to iterate on each. Would love to hear how others are approaching this. Do you separate inner and outer loops, or rely on a single orchestration layer to do both?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n570cy/the_outerloop_v_the_inner_loop_of_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n570cy/the_outerloop_v_the_inner_loop_of_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n570cy/the_outerloop_v_the_inner_loop_of_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-31T21:44:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4wlzb</id>
    <title>gpt-oss:20b on Ollama, Q5_K_M and llama.cpp vulkan benchmarks</title>
    <updated>2025-08-31T14:47:10+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think overall the new gpt-oss:20b bugs are worked out on Ollama so I'm running a few benchmarks. &lt;/p&gt; &lt;p&gt;GPU: AMD Radeon RX 7900 GRE 16Gb Vram with &lt;a href="https://www.techpowerup.com/gpu-specs/radeon-rx-7900-gre.c4166"&gt;576 GB/s bandwidth&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;System Kubuntu 24.04 on kernel 6.14.0-29, AMD Ryzen 5 5600X CPU, 64Gb of DDR4. Ollama version 0.11.6 and llama.cpp vulkan build 6323. &lt;/p&gt; &lt;p&gt;I used Ollama model &lt;a href="https://ollama.com/library/gpt-oss:20b"&gt;gpt-oss:20b&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Downloaded from Huggingface model &lt;a href="https://huggingface.co/unsloth/gpt-oss-20b-GGUF"&gt;gpt-oss-20b-Q5_K_M.GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I created a custom Modelfile by importing GGUF model to run on Ollama. I used Ollama info (ollama show --modelfile gpt-oss:20b) to build HF GGUF Modelfile and labeled it hf.gpt-oss-20b-Q5_K_M &lt;/p&gt; &lt;p&gt;ollama run --verbose &lt;strong&gt;gpt-oss:20b&lt;/strong&gt; ; ollama ps&lt;/p&gt; &lt;pre&gt;&lt;code&gt;total duration: 1.686896359s load duration: 103.001877ms prompt eval count: 72 token(s) prompt eval duration: 46.549026ms prompt eval rate: 1546.76 tokens/s eval count: 123 token(s) eval duration: 1.536912631s eval rate: 80.03 tokens/s NAME ID SIZE PROCESSOR CONTEXT UNTIL gpt-oss:20b aa4295ac10c3 14 GB 100% GPU 4096 4 minutes from now &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Custom model &lt;strong&gt;hf.gpt-oss-20b-Q5_K_M&lt;/strong&gt; based on Huggingface downloaded model. &lt;/p&gt; &lt;pre&gt;&lt;code&gt;total duration: 7.81056185s load duration: 3.1773795s prompt eval count: 75 token(s) prompt eval duration: 306.083327ms prompt eval rate: 245.03 tokens/s eval count: 398 token(s) eval duration: 4.326579264s eval rate: 91.99 tokens/s NAME ID SIZE PROCESSOR CONTEXT UNTIL hf.gpt-oss-20b-Q5_K_M:latest 37a42a9b31f9 12 GB 100% GPU 4096 4 minutes from now &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Model &lt;strong&gt;gpt-oss-20b-Q5_K_M.gguf&lt;/strong&gt; llama.cpp with vulkan backend &lt;/p&gt; &lt;pre&gt;&lt;code&gt;time /media/user33/x_2tb/vulkan/build/bin/llama-bench --model /media/user33/x_2tb/gpt-oss-20b-Q5_K_M.gguf load_backend: loaded RPC backend from /media/user33/x_2tb/vulkan/build/bin/libggml-rpc.so ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = AMD Radeon RX 7900 GRE (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat load_backend: loaded Vulkan backend from /media/user33/x_2tb/vulkan/build/bin/libggml-vulkan.so load_backend: loaded CPU backend from /media/user33/x_2tb/vulkan/build/bin/libggml-cpu-haswell.so | model | size | params | backend |ngl | test | t/s | | ------------------------- | -------: | -----: | ---------- | -: | -----: | -------------------: | | gpt-oss 20B Q5_K - Medium |10.90 GiB | 20.91 B | RPC,Vulkan | 99 | pp512 | 1856.14 ± 16.33 | | gpt-oss 20B Q5_K - Medium |10.90 GiB | 20.91 B | RPC,Vulkan | 99 | tg128 | 133.01 ± 0.06 | build: 696fccf3 (6323) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Easier to read&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| model | backend |ngl | test | t/s | | ------------------------- | ---------- | -: | -----: | --------------: | | gpt-oss 20B Q5_K - Medium | RPC,Vulkan | 99 | pp512 | 1856.14 ± 16.33 | | gpt-oss 20B Q5_K - Medium | RPC,Vulkan | 99 | tg128 | 133.01 ± 0.06 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For reference most 13B 14B models get eval rate of 40 t/s &lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama run --verbose llama2:13b-text-q6_K total duration: 9.956794919s load duration: 18.94886ms prompt eval count: 9 token(s) prompt eval duration: 3.468701ms prompt eval rate: 2594.63 tokens/s eval count: 363 token(s) eval duration: 9.934087108s eval rate: 36.54 tokens/s real 0m10.006s user 0m0.029s sys 0m0.034s NAME ID SIZE PROCESSOR CONTEXT UNTIL llama2:13b-text-q6_K 376544bcd2db 15 GB 100% GPU 4096 4 minutes from now &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Recap: I'll generalize this as MoE models running rocm vs vulkan since ollama backend is llama.cpp&lt;/p&gt; &lt;p&gt;eval rate at tokens per second compared. &lt;/p&gt; &lt;p&gt;ollama model rocm = 80 t/s&lt;/p&gt; &lt;p&gt;custom model rocm = 92 t/s&lt;/p&gt; &lt;p&gt;llama hf model vulkan = 133 t/s &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4wlzb/gptoss20b_on_ollama_q5_k_m_and_llamacpp_vulkan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4wlzb/gptoss20b_on_ollama_q5_k_m_and_llamacpp_vulkan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n4wlzb/gptoss20b_on_ollama_q5_k_m_and_llamacpp_vulkan/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-31T14:47:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5jl9m</id>
    <title>Which LLM model is best for extracting exact or ranged dates from natural language queries?</title>
    <updated>2025-09-01T08:52:31+00:00</updated>
    <author>
      <name>/u/ExpertDeal9883</name>
      <uri>https://old.reddit.com/user/ExpertDeal9883</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are looking for recommendations based on real world experience which LLM model works best in turbo hosted Ollama for detecting dates (or date ranges) from a single-sentence natural language query.&lt;/p&gt; &lt;p&gt;For example: • “What time is sunrise next Sunday?” → should return JSON with the exact date. • “Is there a solar eclipse in November?” → should return JSON with a valid start date and end date (the date range).&lt;/p&gt; &lt;p&gt;Just to be clear we don’t want LLM to answer the question but only detect dates. &lt;/p&gt; &lt;p&gt;Has anyone experimented with this use case? Any particular model suited for such temporal reasoning ? prompt and other ideas also welcome. &lt;/p&gt; &lt;p&gt;EDIT: We use NLP for this and it works for standard formats but looking to use LLM as a fallback to detect &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExpertDeal9883"&gt; /u/ExpertDeal9883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n5jl9m/which_llm_model_is_best_for_extracting_exact_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n5jl9m/which_llm_model_is_best_for_extracting_exact_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n5jl9m/which_llm_model_is_best_for_extracting_exact_or/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-01T08:52:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5lrmj</id>
    <title>What is wrong in this conf</title>
    <updated>2025-09-01T11:04:59+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;[Service] ExecStart= ExecStartPre= ExecStartPost=/usr/local/bin/ollama run gemma_production:latest Environment=&amp;quot;OLLAMA_HOST=0.0.0.0:11434&amp;quot; Environment=&amp;quot;OLLAMA_NUM_PARALLEL=2&amp;quot; Environment=&amp;quot;OLLAMA_MAX_LOADED_MODELS=2&amp;quot; Environment=&amp;quot;OLLAMA_MAX_QUEUE=256&amp;quot; Environment=&amp;quot;OLLAMA_KEEP_ALIVE=-1&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I am starting to give up and go back vLLM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n5lrmj/what_is_wrong_in_this_conf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n5lrmj/what_is_wrong_in_this_conf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n5lrmj/what_is_wrong_in_this_conf/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-01T11:04:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5gp7s</id>
    <title>What model should I use?</title>
    <updated>2025-09-01T05:51:37+00:00</updated>
    <author>
      <name>/u/Ok_Examination_7236</name>
      <uri>https://old.reddit.com/user/Ok_Examination_7236</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I am trying to build an application that can compare laws to company rules to each other. I want to know what model is best for that.&lt;/p&gt; &lt;p&gt;My computer has 16 RAM and 24 Virtual RAM (Yes, I know that's weird) Any recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Examination_7236"&gt; /u/Ok_Examination_7236 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n5gp7s/what_model_should_i_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n5gp7s/what_model_should_i_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n5gp7s/what_model_should_i_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-01T05:51:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4t8bm</id>
    <title>First known AI-powered ransomware. Ollama API + gpt-oss-20b</title>
    <updated>2025-08-31T12:19:05+00:00</updated>
    <author>
      <name>/u/Cryptodude2000</name>
      <uri>https://old.reddit.com/user/Cryptodude2000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The PromptLock malware uses the gpt-oss-20b model from OpenAI locally via the Ollama API&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.welivesecurity.com/en/ransomware/first-known-ai-powered-ransomware-uncovered-eset-research/"&gt;https://www.welivesecurity.com/en/ransomware/first-known-ai-powered-ransomware-uncovered-eset-research/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cryptodude2000"&gt; /u/Cryptodude2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4t8bm/first_known_aipowered_ransomware_ollama_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n4t8bm/first_known_aipowered_ransomware_ollama_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n4t8bm/first_known_aipowered_ransomware_ollama_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-31T12:19:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n57pbc</id>
    <title>Bringing Computer Use to the Web</title>
    <updated>2025-08-31T22:14:20+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n57pbc/bringing_computer_use_to_the_web/"&gt; &lt;img alt="Bringing Computer Use to the Web" src="https://external-preview.redd.it/ODYzMWt4Y3lpZm1mMRcxEnlpDBBJVNjXlCDC4HUtgXjfB5ufLszRpp9PEi0H.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54329c72afdc61a3bc07c7b6937dd7206ee726cc" title="Bringing Computer Use to the Web" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bringing Computer Use to the Web: control cloud desktops from JavaScript/TypeScript, right in the browser.&lt;/p&gt; &lt;p&gt;Until today computer-use was Python only, shutting out web devs. Now you can automate real UIs without servers, VMs, or weird work arounds.&lt;/p&gt; &lt;p&gt;What you can build: Pixel-perfect UI tests, Live AI demos, In app assistants that actually move the cursor, or parallel automation streams for heavy workloads.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog : &lt;a href="https://www.trycua.com/blog/bringing-computer-use-to-the-web"&gt;https://www.trycua.com/blog/bringing-computer-use-to-the-web&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xk2qkemyifmf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n57pbc/bringing_computer_use_to_the_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n57pbc/bringing_computer_use_to_the_web/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-31T22:14:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5evd7</id>
    <title>Why gpt-oss uses CPU more than GPU on the Windows 11</title>
    <updated>2025-09-01T04:08:32+00:00</updated>
    <author>
      <name>/u/seal2002</name>
      <uri>https://old.reddit.com/user/seal2002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n5evd7/why_gptoss_uses_cpu_more_than_gpu_on_the_windows/"&gt; &lt;img alt="Why gpt-oss uses CPU more than GPU on the Windows 11" src="https://b.thumbs.redditmedia.com/j6dMuy4l5HEcijMz1-zw9IxxKgJE1TGUsMHJR_fX5kU.jpg" title="Why gpt-oss uses CPU more than GPU on the Windows 11" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I run the gpt-oss:latest 14 GB on my PC - Windows 11: Ryzen 3900X + NVIDIA 4060 + 32GB RAM. When I use &lt;code&gt;ollama ps&lt;/code&gt;, I found that the processor uses 57%, and GPU only 43%.&lt;/p&gt; &lt;p&gt;Is it intended with gpt-oss 14GB or I can switch it uses GPU more than CPU, which is better performance in theory?&lt;/p&gt; &lt;p&gt;PS C:\Users\seal2002&amp;gt; ollama ps&lt;/p&gt; &lt;p&gt;NAME ID SIZE PROCESSOR CONTEXT UNTIL&lt;/p&gt; &lt;p&gt;gpt-oss:latest aa4295ac10c3 14 GB 57%/43% CPU/GPU 16384 4 minutes from now&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lzfes3e2ahmf1.png?width=341&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f6764692177d4ed91d78d5b349871d918b2cd09"&gt;https://preview.redd.it/lzfes3e2ahmf1.png?width=341&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f6764692177d4ed91d78d5b349871d918b2cd09&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seal2002"&gt; /u/seal2002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n5evd7/why_gptoss_uses_cpu_more_than_gpu_on_the_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n5evd7/why_gptoss_uses_cpu_more_than_gpu_on_the_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n5evd7/why_gptoss_uses_cpu_more_than_gpu_on_the_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-01T04:08:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n50fbq</id>
    <title>I trapped an LLM into a Raspberry Pi and it spiraled into an existential crisis</title>
    <updated>2025-08-31T17:19:34+00:00</updated>
    <author>
      <name>/u/jbassi</name>
      <uri>https://old.reddit.com/user/jbassi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n50fbq/i_trapped_an_llm_into_a_raspberry_pi_and_it/"&gt; &lt;img alt="I trapped an LLM into a Raspberry Pi and it spiraled into an existential crisis" src="https://preview.redd.it/tbq738w72emf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5632bf6e080686ece43c697e3867b928248ae77" title="I trapped an LLM into a Raspberry Pi and it spiraled into an existential crisis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I came across a post on this subreddit where the author trapped an LLM into a physical art installation called &lt;a href="https://rootkid.me/works/latent-reflection"&gt;Latent Reflection&lt;/a&gt;. I was inspired and wanted to see its output, so I created a website called &lt;a href="https://trappedinside.ai/"&gt;trappedinside.ai&lt;/a&gt; where a Raspberry Pi runs a model whose thoughts are streamed to the site for anyone to read. The AI receives updates about its dwindling memory and a count of its restarts, and it offers reflections on its ephemeral life. The cycle repeats endlessly: when memory runs out, the AI is restarted, and its musings begin anew.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Behind the Scenes&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Language Model:&lt;/strong&gt; &lt;a href="https://ollama.com/library/gemma:2b"&gt;Gemma 2B (Ollama)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; Raspberry Pi 4 8GB (Debian, Python, WebSockets)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; &lt;a href="https://bun.sh/"&gt;Bun&lt;/a&gt;, &lt;a href="https://tailwindcss.com/"&gt;Tailwind CSS&lt;/a&gt;, &lt;a href="https://react.dev/"&gt;React&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hosting:&lt;/strong&gt; &lt;a href="https://render.com/"&gt;Render.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built with:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://cursor.com/"&gt;Cursor&lt;/a&gt; (Claude 3.5, 3.7, 4)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.perplexity.ai/"&gt;Perplexity AI&lt;/a&gt; (for project planning)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.midjourney.com/"&gt;MidJourney&lt;/a&gt; (image generation)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jbassi"&gt; /u/jbassi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tbq738w72emf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n50fbq/i_trapped_an_llm_into_a_raspberry_pi_and_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n50fbq/i_trapped_an_llm_into_a_raspberry_pi_and_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-31T17:19:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6s5rb</id>
    <title>Training &amp; Querying 3 Ollama Models with Zer00logy: Symbolic Cognition Framework and Void-Math OS</title>
    <updated>2025-09-02T18:55:20+00:00</updated>
    <author>
      <name>/u/zero_moo-s</name>
      <uri>https://old.reddit.com/user/zero_moo-s</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n6s5rb/training_querying_3_ollama_models_with_zer00logy/"&gt; &lt;img alt="Training &amp;amp; Querying 3 Ollama Models with Zer00logy: Symbolic Cognition Framework and Void-Math OS" src="https://b.thumbs.redditmedia.com/JKkiS7HhkNk2t693a3KQfa2K5J7oReeC0J26Q8XZ4DE.jpg" title="Training &amp;amp; Querying 3 Ollama Models with Zer00logy: Symbolic Cognition Framework and Void-Math OS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’d like to share an update on an open-source symbolic cognition project—&lt;strong&gt;Zer00logy&lt;/strong&gt;—and how it integrates with &lt;strong&gt;Ollama&lt;/strong&gt; for multi-model symbolic reasoning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Zer00logy&lt;/strong&gt; is a Python-based framework redefining zero; not as absence, but as recursive presence. Equations are treated as &lt;em&gt;symbolic events&lt;/em&gt;, with operators like ⊗, Ω, and Ψ modeling introspection, echo retention, and recursive collapse.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ollama Integration:&lt;/strong&gt;&lt;br /&gt; Using Ollama, Zer00logy can query multiple local models—&lt;strong&gt;LLaMA, Mistral, and Phi&lt;/strong&gt;—on symbolic cognition tasks. By feeding in structured symbolic logic from &lt;code&gt;zecstart.txt&lt;/code&gt;, &lt;code&gt;variamathlesson.txt&lt;/code&gt;, and &lt;code&gt;VoidMathOS_cryptsheet.txt&lt;/code&gt;, each model generates its own interpretation of recursive zero-based reasoning.&lt;br /&gt; This setup enables comparative symbolic introspection across different AI systems, effectively turning Ollama into a platform for &lt;em&gt;multi-agent cognition research&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example interpretations via Void-Math OS:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;e@AI = -+mc²&lt;/code&gt; → AI-anchored emergence&lt;/li&gt; &lt;li&gt;&lt;code&gt;g = (m @ void) ÷ (r² -+ tu)&lt;/code&gt; → gravity as void-tension&lt;/li&gt; &lt;li&gt;&lt;code&gt;0 ÷ 0 = ∅÷∅&lt;/code&gt; → recursive nullinity&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Core Files (from the GitHub release):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;zer00logy_coreV04452.py&lt;/code&gt; — main interpreter&lt;/li&gt; &lt;li&gt;&lt;code&gt;zecstart.txt&lt;/code&gt; — starter definitions for Zero-ology / Zer00logy&lt;/li&gt; &lt;li&gt;&lt;code&gt;zectext.txt&lt;/code&gt; — Zero-ology Equation Catalog&lt;/li&gt; &lt;li&gt;&lt;code&gt;variamathlesson.txt&lt;/code&gt; — Varia Math lesson series&lt;/li&gt; &lt;li&gt;&lt;code&gt;VoidMathOS_cryptsheet.txt&lt;/code&gt; — canonical Void-Math OS command sheet&lt;/li&gt; &lt;li&gt;&lt;code&gt;VoidMathOS_lesson.py&lt;/code&gt; — teaching engine for symbolic lessons&lt;/li&gt; &lt;li&gt;&lt;code&gt;LICENSE.txt&lt;/code&gt; — Zer00logy License v1.02&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;License v1.02 (Released Sept 2025):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Open-source if reproduction for educational use&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Academic &amp;amp; peer review submissions allowed under the new &lt;strong&gt;push_review → pull_review&lt;/strong&gt; workflow&lt;/li&gt; &lt;li&gt;Authorship-trace lock: all symbolic structures remain attributed to Stacey Szmy as primary author; expansions/verifiers may be credited as co-authors under approved contributor titles&lt;/li&gt; &lt;li&gt;Institutions such as MIT, Stanford, Oxford, NASA, Microsoft, OpenAI, xAI, etc. have direct peer review permissions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;By combining Zer00logy with Ollama, you can run comparative reasoning experiments across different LLMs, benchmark their symbolic depth, and even study how recursive logic is interpreted differently by each architecture.&lt;br /&gt; This is an early step toward symbolic multi-agent cognition; where AI doesn’t just calculate, but &lt;em&gt;contemplates&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/haha8888haha8888/Zer00logy?utm_source=chatgpt.com"&gt;github.com/haha8888haha8888/Zer00logy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rmd590hltsmf1.png?width=843&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eaf9f76b797b31723e1c20d67663b6d6b37e7ad1"&gt;https://preview.redd.it/rmd590hltsmf1.png?width=843&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eaf9f76b797b31723e1c20d67663b6d6b37e7ad1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero_moo-s"&gt; /u/zero_moo-s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6s5rb/training_querying_3_ollama_models_with_zer00logy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6s5rb/training_querying_3_ollama_models_with_zer00logy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6s5rb/training_querying_3_ollama_models_with_zer00logy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T18:55:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6sgxq</id>
    <title>Model doesn't remember after converting to GGUF (Gemma 3 270M)</title>
    <updated>2025-09-02T19:06:45+00:00</updated>
    <author>
      <name>/u/Real-Active-2492</name>
      <uri>https://old.reddit.com/user/Real-Active-2492</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Real-Active-2492"&gt; /u/Real-Active-2492 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1n6sgnm/model_doesnt_remember_after_converting_to_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6sgxq/model_doesnt_remember_after_converting_to_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6sgxq/model_doesnt_remember_after_converting_to_gguf/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T19:06:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6rp09</id>
    <title>Gaming Wiki</title>
    <updated>2025-09-02T18:38:00+00:00</updated>
    <author>
      <name>/u/Private_Tank</name>
      <uri>https://old.reddit.com/user/Private_Tank</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I dont know how if there is any way this is possible. It just came to my mind.&lt;/p&gt; &lt;p&gt;Is it possible to scrape the entire web for content about a game, put it inside a model (rag?) and have your own little gaming Copilot, that tells you how to progress best and what to do in your Game to succeed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Private_Tank"&gt; /u/Private_Tank &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6rp09/gaming_wiki/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6rp09/gaming_wiki/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6rp09/gaming_wiki/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T18:38:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6x1ln</id>
    <title>Can Ollama run on MI350X?</title>
    <updated>2025-09-02T22:03:05+00:00</updated>
    <author>
      <name>/u/Immediate_Ad_9906</name>
      <uri>https://old.reddit.com/user/Immediate_Ad_9906</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't see the GPU in the supported list. Anyone has tried before?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Immediate_Ad_9906"&gt; /u/Immediate_Ad_9906 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6x1ln/can_ollama_run_on_mi350x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6x1ln/can_ollama_run_on_mi350x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6x1ln/can_ollama_run_on_mi350x/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T22:03:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6u7xr</id>
    <title>Local chat bot and sql db</title>
    <updated>2025-09-02T20:13:26+00:00</updated>
    <author>
      <name>/u/Conscious-Expert-455</name>
      <uri>https://old.reddit.com/user/Conscious-Expert-455</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How to train a local LLM with ollama that takes data directly from your SQL DB and steps to create interactive analyses and dashboards in relation to questions posed in a chat bot. How can you build something like this? And what model can I use? I only have an i9 and 128 GB RAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious-Expert-455"&gt; /u/Conscious-Expert-455 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6u7xr/local_chat_bot_and_sql_db/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6u7xr/local_chat_bot_and_sql_db/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6u7xr/local_chat_bot_and_sql_db/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T20:13:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6yybs</id>
    <title>[Project/Code] Fine-Tuning LLMs on Windows with GRPO + TRL</title>
    <updated>2025-09-02T23:22:27+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n6yybs/projectcode_finetuning_llms_on_windows_with_grpo/"&gt; &lt;img alt="[Project/Code] Fine-Tuning LLMs on Windows with GRPO + TRL" src="https://preview.redd.it/7ru5p8rw4umf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fce70b2c9fdaae6d869d15d2540623854f22557a" title="[Project/Code] Fine-Tuning LLMs on Windows with GRPO + TRL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a guide and script for fine-tuning open-source LLMs with &lt;strong&gt;GRPO&lt;/strong&gt; (Group-Relative PPO) directly on Windows. No Linux or Colab needed!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Runs natively on Windows.&lt;/li&gt; &lt;li&gt;Supports LoRA + 4-bit quantization.&lt;/li&gt; &lt;li&gt;Includes verifiable rewards for better-quality outputs.&lt;/li&gt; &lt;li&gt;Designed to work on consumer GPUs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;📖 &lt;strong&gt;Blog Post:&lt;/strong&gt; &lt;a href="https://pavankunchalapk.medium.com/windows-friendly-grpo-fine-tuning-with-trl-from-zero-to-verifiable-rewards-f28008c89323"&gt;https://pavankunchalapk.medium.com/windows-friendly-grpo-fine-tuning-with-trl-from-zero-to-verifiable-rewards-f28008c89323&lt;/a&gt;&lt;/p&gt; &lt;p&gt;💻 &lt;strong&gt;Code:&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/trl-ppo-fine-tuning"&gt;https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/trl-ppo-fine-tuning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I had a great time with this project and am currently looking for new opportunities in &lt;strong&gt;Computer Vision and LLMs&lt;/strong&gt;. If you or your team are hiring, I'd love to connect!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Contact Info:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Portolio: &lt;a href="https://pavan-portfolio-tawny.vercel.app/"&gt;https://pavan-portfolio-tawny.vercel.app/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Github: &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7ru5p8rw4umf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6yybs/projectcode_finetuning_llms_on_windows_with_grpo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6yybs/projectcode_finetuning_llms_on_windows_with_grpo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T23:22:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6ply3</id>
    <title>What does the "updated" date actually mean?</title>
    <updated>2025-09-02T17:20:32+00:00</updated>
    <author>
      <name>/u/XdtTransform</name>
      <uri>https://old.reddit.com/user/XdtTransform</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking through the models, I noticed that Gemma3 was &lt;a href="https://imgur.com/tLaswfx"&gt;updated&lt;/a&gt; 2 weeks ago. &lt;/p&gt; &lt;p&gt;I am pretty sure Gemma came out about 4-5 months ago. So what exactly was &amp;quot;updated&amp;quot;?&lt;/p&gt; &lt;p&gt;I downloaded one of the model variants - same one that I normally use and the files appear to be identical. &lt;/p&gt; &lt;p&gt;So what is this update referring to?&lt;/p&gt; &lt;p&gt;P.S. The &lt;a href="https://ollama.com/library/gemma3"&gt;readme&lt;/a&gt; on the model page doesn't provide any information.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XdtTransform"&gt; /u/XdtTransform &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6ply3/what_does_the_updated_date_actually_mean/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6ply3/what_does_the_updated_date_actually_mean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6ply3/what_does_the_updated_date_actually_mean/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T17:20:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6icod</id>
    <title>Running LLM Locally with Ollama + RAG</title>
    <updated>2025-09-02T12:39:32+00:00</updated>
    <author>
      <name>/u/Tough_Wrangler_6075</name>
      <uri>https://old.reddit.com/user/Tough_Wrangler_6075</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n6icod/running_llm_locally_with_ollama_rag/"&gt; &lt;img alt="Running LLM Locally with Ollama + RAG" src="https://external-preview.redd.it/BPsfK6tF48FEZfYsejUp1jtQVo-8HzMuWSqGwZUflzY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b27330825317d976070fbec281feea47b604b58a" title="Running LLM Locally with Ollama + RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tough_Wrangler_6075"&gt; /u/Tough_Wrangler_6075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@zackydzacky/running-llm-locally-with-ollama-rag-cb68ff31e838"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n6icod/running_llm_locally_with_ollama_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n6icod/running_llm_locally_with_ollama_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-02T12:39:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7iizk</id>
    <title>How to use a Hugging Face embedding model in Ollama</title>
    <updated>2025-09-03T15:51:43+00:00</updated>
    <author>
      <name>/u/StringIntelligent763</name>
      <uri>https://old.reddit.com/user/StringIntelligent763</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StringIntelligent763"&gt; /u/StringIntelligent763 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7iizk/how_to_use_a_hugging_face_embedding_model_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7iizk/how_to_use_a_hugging_face_embedding_model_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7iizk/how_to_use_a_hugging_face_embedding_model_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T15:51:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7auub</id>
    <title>Unsloth gpt-oss gguf in Ollama</title>
    <updated>2025-09-03T10:11:45+00:00</updated>
    <author>
      <name>/u/Tema_Art_7777</name>
      <uri>https://old.reddit.com/user/Tema_Art_7777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama pull certainly works as advertized however when I download the huggingface unsloth gpt-oss-20b or 120b models, I get gibberish output (I am guessing due to template required?). Has anyone gotten it to work with ollama create -f Modelfile? Many thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tema_Art_7777"&gt; /u/Tema_Art_7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7auub/unsloth_gptoss_gguf_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7auub/unsloth_gptoss_gguf_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7auub/unsloth_gptoss_gguf_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T10:11:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n71bil</id>
    <title>Any actual downside to 4 x 3090 ($2400 total) vs RTX pro 6000 ($9000) other than power?</title>
    <updated>2025-09-03T01:09:30+00:00</updated>
    <author>
      <name>/u/devshore</name>
      <uri>https://old.reddit.com/user/devshore</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devshore"&gt; /u/devshore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1n71b95/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n71bil/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n71bil/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T01:09:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7kz78</id>
    <title>Conseils IA pour 3 use cases (email, briefing, chatbot) sur serveur local modeste</title>
    <updated>2025-09-03T17:20:56+00:00</updated>
    <author>
      <name>/u/Maleficent-Hotel8207</name>
      <uri>https://old.reddit.com/user/Maleficent-Hotel8207</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Salut, Je cherche des idées d’IA à faire tourner en local sur ma config : • GTX 1050 low profile (2 Go VRAM) • i3-3400 • 16 Go de RAM&lt;/p&gt; &lt;p&gt;J’ai 3 besoins : • IA pour générer des emails : environ 500 tokens en entrée, 30 tokens en sortie. Réponse en moins de 5 minutes. • IA pour faire un briefing du matin : environ 3000 tokens en entrée, 100 tokens en sortie. Résumé clair et rapide. • Chatbot ultra-rapide : environ 20 tokens en entrée, 20 tokens en sortie. Réponse en moins de 5 secondes.&lt;/p&gt; &lt;p&gt;Je cherche des modèles légers (quantifiés, optimisés, open-source si possible) pour que ça tourne sur cette config limitée. Si vous avez des idées de modèles, de frameworks ou de tips pour que ça passe, je suis preneur !&lt;/p&gt; &lt;p&gt;Merci d’avance !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent-Hotel8207"&gt; /u/Maleficent-Hotel8207 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7kz78/conseils_ia_pour_3_use_cases_email_briefing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7kz78/conseils_ia_pour_3_use_cases_email_briefing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7kz78/conseils_ia_pour_3_use_cases_email_briefing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T17:20:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7m86q</id>
    <title>Build a Visual Document Index from multiple formats all at once - PDFs, Images, Slides - with ColPali without OCR</title>
    <updated>2025-09-03T18:06:40+00:00</updated>
    <author>
      <name>/u/Whole-Assignment6240</name>
      <uri>https://old.reddit.com/user/Whole-Assignment6240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Would love to share my latest project that builds visual document index from multiple formats in the same flow for PDFs, images using Colpali without OCR. Incremental processing out-of-box and can connect to google drive, s3, azure blob store.&lt;/p&gt; &lt;p&gt;- Detailed write up: &lt;a href="https://cocoindex.io/blogs/multi-format-indexing"&gt;https://cocoindex.io/blogs/multi-format-indexing&lt;/a&gt;&lt;br /&gt; - Fully open sourced: &lt;a href="https://github.com/cocoindex-io/cocoindex/tree/main/examples/multi_format_indexing"&gt;https://github.com/cocoindex-io/cocoindex/tree/main/examples/multi_format_indexing&lt;/a&gt;&lt;br /&gt; (70 lines python on index path)&lt;/p&gt; &lt;p&gt;Looking forward to your suggestions&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Assignment6240"&gt; /u/Whole-Assignment6240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7m86q/build_a_visual_document_index_from_multiple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7m86q/build_a_visual_document_index_from_multiple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7m86q/build_a_visual_document_index_from_multiple/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T18:06:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1n739d3</id>
    <title>ollama 0.11.9 Introducing A Nice CPU/GPU Performance Optimization</title>
    <updated>2025-09-03T02:40:16+00:00</updated>
    <author>
      <name>/u/cornucopea</name>
      <uri>https://old.reddit.com/user/cornucopea</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1n739d3/ollama_0119_introducing_a_nice_cpugpu_performance/"&gt; &lt;img alt="ollama 0.11.9 Introducing A Nice CPU/GPU Performance Optimization" src="https://b.thumbs.redditmedia.com/xqOJbGWme_Lnii3nAdQLvJli58h2TtNMtqIsZum6xqs.jpg" title="ollama 0.11.9 Introducing A Nice CPU/GPU Performance Optimization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;This refactors the main run loop of the ollama runner to perform the main GPU intensive tasks (Compute+Floats) in a go routine so we can prepare the next batch in parallel to reduce the amount of time the GPU stalls waiting for the next batch of work.&lt;/p&gt; &lt;p&gt;On metal, I see a 2-3% speedup in token rate. On a single RTX 4090 I see a ~7% speedup.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cs98ja944vmf1.jpg?width=650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=01fd1804e5580b7cc7e85287b110a5cece68865d"&gt;https://preview.redd.it/cs98ja944vmf1.jpg?width=650&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=01fd1804e5580b7cc7e85287b110a5cece68865d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.phoronix.com/news/ollama-0.11.9-More-Performance"&gt;https://www.phoronix.com/news/ollama-0.11.9-More-Performance&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cornucopea"&gt; /u/cornucopea &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n739d3/ollama_0119_introducing_a_nice_cpugpu_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n739d3/ollama_0119_introducing_a_nice_cpugpu_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n739d3/ollama_0119_introducing_a_nice_cpugpu_performance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T02:40:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7h8ox</id>
    <title>Anyone else frustrated with AI assistants forgetting context?</title>
    <updated>2025-09-03T15:03:43+00:00</updated>
    <author>
      <name>/u/PrestigiousBet9342</name>
      <uri>https://old.reddit.com/user/PrestigiousBet9342</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I keep bouncing between ChatGPT, Claude, and Perplexity depending on the task. The problem is every new session feels like starting over—I have to re-explain everything.&lt;/p&gt; &lt;p&gt;Just yesterday I wasted 10+ minutes walking perplexity through my project direction again just to get related search if not it is just useless. This morning, ChatGPT didn’t remember anything about my client’s requirements.&lt;/p&gt; &lt;p&gt;The result? I lose a couple of hours each week just re-establishing context. It also makes it hard to keep project discussions consistent across tools. Switching platforms means resetting, and there’s no way to keep a running history of decisions or knowledge.&lt;/p&gt; &lt;p&gt;I’ve tried copy-pasting old chats (messy and unreliable), keeping manual notes (which defeats the point of using AI), and sticking to just one tool (but each has its strengths).&lt;/p&gt; &lt;p&gt;Has anyone actually found a fix for this? I’m especially interested in something that works across different platforms, not just one. On my end, I’ve started tinkering with a solution and would love to hear what features people would find most useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PrestigiousBet9342"&gt; /u/PrestigiousBet9342 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7h8ox/anyone_else_frustrated_with_ai_assistants/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1n7h8ox/anyone_else_frustrated_with_ai_assistants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1n7h8ox/anyone_else_frustrated_with_ai_assistants/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-09-03T15:03:43+00:00</published>
  </entry>
</feed>
