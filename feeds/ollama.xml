<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-21T18:07:40+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1i4hk9y</id>
    <title>LLama Index Documentation Assistant with phi4 and ollama</title>
    <updated>2025-01-18T21:13:24+00:00</updated>
    <author>
      <name>/u/oridnary_artist</name>
      <uri>https://old.reddit.com/user/oridnary_artist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i4hk9y/llama_index_documentation_assistant_with_phi4_and/"&gt; &lt;img alt="LLama Index Documentation Assistant with phi4 and ollama" src="https://external-preview.redd.it/MzFoNmV2N3ppdGRlMQpxhBzZcKM0vjh7QfEh6AQM_565qKsHsYnJbi1X94T-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1007630a8f41c9e23a085b997da0f3bc2e7d0f71" title="LLama Index Documentation Assistant with phi4 and ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oridnary_artist"&gt; /u/oridnary_artist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0dd2tv7zitde1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i4hk9y/llama_index_documentation_assistant_with_phi4_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i4hk9y/llama_index_documentation_assistant_with_phi4_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-18T21:13:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4ye4x</id>
    <title>Is my B580 working?</title>
    <updated>2025-01-19T13:26:46+00:00</updated>
    <author>
      <name>/u/Ejo2001</name>
      <uri>https://old.reddit.com/user/Ejo2001</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ejo2001"&gt; /u/Ejo2001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/IntelArc/comments/1i4ydu8/is_my_b580_working/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i4ye4x/is_my_b580_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i4ye4x/is_my_b580_working/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-19T13:26:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4cttd</id>
    <title>PHI-4: Quantized Q4 vs Q8 on My Nvidia RTX 3060 12GB System</title>
    <updated>2025-01-18T17:40:41+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i4cttd/phi4_quantized_q4_vs_q8_on_my_nvidia_rtx_3060/"&gt; &lt;img alt="PHI-4: Quantized Q4 vs Q8 on My Nvidia RTX 3060 12GB System" src="https://external-preview.redd.it/xOIjwljoqcqAtLtEZzFIsx3i8OS8PGRXeXH3XVBboAw.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=752b0929d7ac91161885bfb8583051422a20f9d3" title="PHI-4: Quantized Q4 vs Q8 on My Nvidia RTX 3060 12GB System" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/mxNEQ53K7QQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i4cttd/phi4_quantized_q4_vs_q8_on_my_nvidia_rtx_3060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i4cttd/phi4_quantized_q4_vs_q8_on_my_nvidia_rtx_3060/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-18T17:40:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i54pqv</id>
    <title>Need help choosing/fine-tuning LLM for structured HTML content extraction to JSON</title>
    <updated>2025-01-19T18:08:01+00:00</updated>
    <author>
      <name>/u/KledMainSG</name>
      <uri>https://old.reddit.com/user/KledMainSG</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! üëã I'm working on a project to extract structured content from HTML pages into JSON, and I'm running into issues with Mistral via Ollama. Here's what I'm trying to do:&lt;/p&gt; &lt;p&gt;I have HTML pages with various sections, lists, and text content that I want to extract into a clean, structured JSON format. Currently using Crawl4AI with Mistral, but getting inconsistent results - sometimes it just repeats my instructions back, other times gives partial data.&lt;/p&gt; &lt;p&gt;Here's my current setup (simplified):&lt;br /&gt; ```&lt;br /&gt; import asyncio&lt;/p&gt; &lt;p&gt;from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig&lt;/p&gt; &lt;p&gt;from crawl4ai.extraction_strategy import LLMExtractionStrategy&lt;/p&gt; &lt;p&gt;async def extract_structured_content():&lt;/p&gt; &lt;p&gt;strategy = LLMExtractionStrategy(&lt;/p&gt; &lt;p&gt;provider=&amp;quot;ollama/mistral&amp;quot;,&lt;/p&gt; &lt;p&gt;api_token=&amp;quot;no-token&amp;quot;,&lt;/p&gt; &lt;p&gt;extraction_type=&amp;quot;block&amp;quot;,&lt;/p&gt; &lt;p&gt;chunk_token_threshold=2000,&lt;/p&gt; &lt;p&gt;overlap_rate=0.1,&lt;/p&gt; &lt;p&gt;apply_chunking=True,&lt;/p&gt; &lt;p&gt;extra_args={&lt;/p&gt; &lt;p&gt;&amp;quot;temperature&amp;quot;: 0.0,&lt;/p&gt; &lt;p&gt;&amp;quot;timeout&amp;quot;: 300&lt;/p&gt; &lt;p&gt;},&lt;/p&gt; &lt;p&gt;instruction=&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;Convert this HTML content into a structured JSON object.&lt;/p&gt; &lt;p&gt;Guidelines:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create logical objects for main sections&lt;/li&gt; &lt;li&gt;Convert lists/bullet points into arrays&lt;/li&gt; &lt;li&gt;Preserve ALL text exactly as written&lt;/li&gt; &lt;li&gt;Don't summarize or truncate content&lt;/li&gt; &lt;li&gt;Maintain natural content hierarchy&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;browser_cfg = BrowserConfig(headless=True)&lt;/p&gt; &lt;p&gt;async with AsyncWebCrawler(config=browser_cfg) as crawler:&lt;/p&gt; &lt;p&gt;result = await crawler.arun(&lt;/p&gt; &lt;p&gt;url=&amp;quot;[my_url]&amp;quot;,&lt;/p&gt; &lt;p&gt;config=CrawlerRunConfig(&lt;/p&gt; &lt;p&gt;extraction_strategy=strategy,&lt;/p&gt; &lt;p&gt;cache_mode=&amp;quot;BYPASS&amp;quot;,&lt;/p&gt; &lt;p&gt;wait_for=&amp;quot;css:.content-area&amp;quot;&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;if result.success:&lt;/p&gt; &lt;p&gt;return json.loads(result.extracted_content)&lt;/p&gt; &lt;p&gt;return None&lt;/p&gt; &lt;p&gt;asyncio.run(extract_structured_content())&lt;br /&gt; ```&lt;/p&gt; &lt;p&gt;Questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Which model would you recommend for this kind of structured extraction? I need something that can:- Understand HTML content structure- Reliably output valid JSON- Handle long-ish content (few pages worth)- Run locally (prefer not to use OpenAI/Claude)&lt;/li&gt; &lt;li&gt;Should I fine-tune a model for this? If so:- What base model would you recommend?- Any tips on creating training data?- Recommended training approach?&lt;/li&gt; &lt;li&gt;Are there any prompt engineering tricks I should try before going the fine-tuning route?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Budget isn't a huge concern, but I'd prefer local models for latency/privacy reasons. Any suggestions much appreciated! üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KledMainSG"&gt; /u/KledMainSG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i54pqv/need_help_choosingfinetuning_llm_for_structured/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i54pqv/need_help_choosingfinetuning_llm_for_structured/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i54pqv/need_help_choosingfinetuning_llm_for_structured/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-19T18:08:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4xx5h</id>
    <title>Kokoro-82M on Ollama?</title>
    <updated>2025-01-19T13:00:57+00:00</updated>
    <author>
      <name>/u/usernameIsRand0m</name>
      <uri>https://old.reddit.com/user/usernameIsRand0m</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not sure if this is even possible or not. But, will there be a Kokoro-82M TTS model which is able to run on Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/usernameIsRand0m"&gt; /u/usernameIsRand0m &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i4xx5h/kokoro82m_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i4xx5h/kokoro82m_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i4xx5h/kokoro82m_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-19T13:00:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i56ym2</id>
    <title>What prompts do you use to test a model's censorship limits?</title>
    <updated>2025-01-19T19:40:06+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i56ym2/what_prompts_do_you_use_to_test_a_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i56ym2/what_prompts_do_you_use_to_test_a_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i56ym2/what_prompts_do_you_use_to_test_a_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-19T19:40:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1i58ftc</id>
    <title>Ollama server limitation.</title>
    <updated>2025-01-19T20:41:07+00:00</updated>
    <author>
      <name>/u/JV_info</name>
      <uri>https://old.reddit.com/user/JV_info</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a question for you and would be more than happy if you could answer.&lt;br /&gt; I want to run a local AI (Ollama + docker + OpenwebUI) for our company and for its server, I wanted to build a machine with the below specs:&lt;br /&gt; AMD Ryzen Threadripper PRO 7965WX (24 cores/48 threads)&lt;br /&gt; ASUS Pro WS WRX80E-SAGE SE WIFI&lt;br /&gt; two MSI GeForce RTX 4090 SUPRIM LIQUID X 24G Hybrid Cooling 24GB&lt;br /&gt; 8x 16GB DDR5 ECC&lt;br /&gt; Corsair AX1600i 1600 Watt 80 Plus Titanium ATX Fully Modular Power Supply&lt;br /&gt; Noctua NH-U14S TR4-SP3&lt;br /&gt; 2T SSD storage &lt;/p&gt; &lt;p&gt;If I purchase and run this machine, can eg, 1000 people join and use the Ollama at the same time? or will there be a delay or is there a limitation etc,...&lt;br /&gt; Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JV_info"&gt; /u/JV_info &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i58ftc/ollama_server_limitation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i58ftc/ollama_server_limitation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i58ftc/ollama_server_limitation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-19T20:41:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5s4yf</id>
    <title>Status of current testing for AMD Instinct Mi60 AI Servers</title>
    <updated>2025-01-20T15:04:28+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalAIServers/comments/1i5s42v/status_of_current_testing_for_amd_instinct_mi60/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5s4yf/status_of_current_testing_for_amd_instinct_mi60/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i5s4yf/status_of_current_testing_for_amd_instinct_mi60/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-20T15:04:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5te2j</id>
    <title>How to set up windows as a ollama server?</title>
    <updated>2025-01-20T15:57:35+00:00</updated>
    <author>
      <name>/u/deeeeranged</name>
      <uri>https://old.reddit.com/user/deeeeranged</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a complete noob, I'm only a web developer.&lt;/p&gt; &lt;p&gt;I have ollama installed and running, and I want to setup this windows PC as a ollama server on my network so that I can ping it from my phones and other computers. &lt;/p&gt; &lt;p&gt;What would you recommend I do to get this setup? &lt;/p&gt; &lt;p&gt;I have no idea where to start with this and all the big three AIs have been useless. I still want to use the PC from time to time to game on and such...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/deeeeranged"&gt; /u/deeeeranged &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5te2j/how_to_set_up_windows_as_a_ollama_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5te2j/how_to_set_up_windows_as_a_ollama_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i5te2j/how_to_set_up_windows_as_a_ollama_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-20T15:57:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5ikds</id>
    <title>Created a tiny rust app that let‚Äôs you run terminal commands through natural English language using ollama and llama-3.2:3b</title>
    <updated>2025-01-20T04:56:24+00:00</updated>
    <author>
      <name>/u/deba2012ddx</name>
      <uri>https://old.reddit.com/user/deba2012ddx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/FrankenDeba/termino-x"&gt;https://github.com/FrankenDeba/termino-x&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/deba2012ddx"&gt; /u/deba2012ddx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5ikds/created_a_tiny_rust_app_that_lets_you_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5ikds/created_a_tiny_rust_app_that_lets_you_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i5ikds/created_a_tiny_rust_app_that_lets_you_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-20T04:56:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5kpjm</id>
    <title>Speech to 3D Model</title>
    <updated>2025-01-20T07:15:16+00:00</updated>
    <author>
      <name>/u/ConsultingJoe</name>
      <uri>https://old.reddit.com/user/ConsultingJoe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i5kpjm/speech_to_3d_model/"&gt; &lt;img alt="Speech to 3D Model" src="https://external-preview.redd.it/FCugAp2XsrtRKrRdQFiqY85v5aE1-IkFf45owu34XRo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9bad2747aed01a62f4720354a4583f67df98c0b2" title="Speech to 3D Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ConsultingJoe"&gt; /u/ConsultingJoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jsammarco/Speech2Model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5kpjm/speech_to_3d_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i5kpjm/speech_to_3d_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-20T07:15:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5dvzk</id>
    <title>Find and run Linux commands using Ollama</title>
    <updated>2025-01-20T00:42:20+00:00</updated>
    <author>
      <name>/u/regnull</name>
      <uri>https://old.reddit.com/user/regnull</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you are anything like me, you keep forgetting the useful linux commands all the time. I made a little script that makes it easy to find and execute them using Ollama. For example&lt;/p&gt; &lt;p&gt;```bash $ ./how.sh find and delete files older than 30 days&lt;/p&gt; &lt;p&gt;Generated command: find . -type f -mtime +30 -exec rm {} \;&lt;/p&gt; &lt;p&gt;Do you want to execute this command? (y/n): ```&lt;/p&gt; &lt;p&gt;If you feel adventurous, add -y to execute the command without confirmation. You can also specify the model with the optional -m flag.&lt;/p&gt; &lt;p&gt;Here's the repo: &lt;a href="https://github.com/regnull/how.sh"&gt;https://github.com/regnull/how.sh&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/regnull"&gt; /u/regnull &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5dvzk/find_and_run_linux_commands_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5dvzk/find_and_run_linux_commands_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i5dvzk/find_and_run_linux_commands_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-20T00:42:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5w6py</id>
    <title>Anyone using tool call api?</title>
    <updated>2025-01-20T17:51:23+00:00</updated>
    <author>
      <name>/u/gibriyagi</name>
      <uri>https://old.reddit.com/user/gibriyagi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using the golang client for tool calling but models seems to get stuck when they are provided with tool options. I was able to make them suggest a tool but they never reply when the role is &amp;quot;tool&amp;quot;.&lt;/p&gt; &lt;p&gt;Anyone had any luck with this? Is it just me?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gibriyagi"&gt; /u/gibriyagi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5w6py/anyone_using_tool_call_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5w6py/anyone_using_tool_call_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i5w6py/anyone_using_tool_call_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-20T17:51:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5kewj</id>
    <title>How to run (any) open LLM with Ollama on Google Cloud Run [Step-by-step]</title>
    <updated>2025-01-20T06:54:45+00:00</updated>
    <author>
      <name>/u/geshan</name>
      <uri>https://old.reddit.com/user/geshan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i5kewj/how_to_run_any_open_llm_with_ollama_on_google/"&gt; &lt;img alt="How to run (any) open LLM with Ollama on Google Cloud Run [Step-by-step]" src="https://external-preview.redd.it/rQbdXtC-XE7nsAc6p9ZyiTxARnjyYfbBKXv5EQ5bnoI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aeefb0e29f2291c669a3b3c15a27b1c2677020fe" title="How to run (any) open LLM with Ollama on Google Cloud Run [Step-by-step]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geshan"&gt; /u/geshan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://geshan.com.np/blog/2025/01/ollama-google-cloud-run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5kewj/how_to_run_any_open_llm_with_ollama_on_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i5kewj/how_to_run_any_open_llm_with_ollama_on_google/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-20T06:54:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5ydg8</id>
    <title>Three Exciting Projects Using Ollama's Vision Models and Structured Output</title>
    <updated>2025-01-20T19:17:37+00:00</updated>
    <author>
      <name>/u/Special_Community179</name>
      <uri>https://old.reddit.com/user/Special_Community179</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i5ydg8/three_exciting_projects_using_ollamas_vision/"&gt; &lt;img alt="Three Exciting Projects Using Ollama's Vision Models and Structured Output" src="https://external-preview.redd.it/xz4afcMfKjaSxxDudCSOnrrnp0uydjHX2MGZwQf0z2Y.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b36af53b64b6a6672e9750aa340d9563b149796f" title="Three Exciting Projects Using Ollama's Vision Models and Structured Output" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special_Community179"&gt; /u/Special_Community179 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=ZZHWLXyZHlA&amp;amp;list=PLp01ObP3udmq2quR-RfrX4zNut_t_kNot"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i5ydg8/three_exciting_projects_using_ollamas_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i5ydg8/three_exciting_projects_using_ollamas_vision/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-20T19:17:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i63vh5</id>
    <title>Deepseek-r1:8b distilled model, success?</title>
    <updated>2025-01-20T23:01:52+00:00</updated>
    <author>
      <name>/u/b61nukejustice</name>
      <uri>https://old.reddit.com/user/b61nukejustice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The model get stuck for me when writing code endlessly repeating the same &amp;quot;thoughts&amp;quot;. Any success from anyone else?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/b61nukejustice"&gt; /u/b61nukejustice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i63vh5/deepseekr18b_distilled_model_success/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i63vh5/deepseekr18b_distilled_model_success/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i63vh5/deepseekr18b_distilled_model_success/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-20T23:01:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6hdjy</id>
    <title>Ollama on WSL2 not using GPU</title>
    <updated>2025-01-21T12:22:11+00:00</updated>
    <author>
      <name>/u/DiterKlein</name>
      <uri>https://old.reddit.com/user/DiterKlein</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i6hdjy/ollama_on_wsl2_not_using_gpu/"&gt; &lt;img alt="Ollama on WSL2 not using GPU " src="https://b.thumbs.redditmedia.com/3mpmghKhH3tLIU2FxpJEmBq3A1fAbMp7HM1piEUBoSg.jpg" title="Ollama on WSL2 not using GPU " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, If been trying to run ollama for a couple days now without success.&lt;br /&gt; (I'm new working with Linux or WSL, so I pretty much just started digging into this field.)&lt;/p&gt; &lt;p&gt;Currently Ollama works on WSL2 (using model llama3.3), but it takes ages just to generate the first word for a simple prompt. I managed to monitor that ollama is not using the GPU (NVIDIDA RTX 3060), instead it uses up my RAM (16GB) and CPU, which makes my default system (Windows 11) suffer a lot, by this i mean i can bearly interact with my Win11 compouter while ollama is processing a prompt. I already updated the NVIDIA drivers on Windows, I installed CUDA on WSL2 as well. I managed to update the .wslconfig to give the WSL as much memory as possible. This allows ollama to run in the first place, otherwise I can't even run ollama.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ok3su41l9cee1.png?width=758&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b35a2f731379e12dff739e74e95d019be0d96e0e"&gt;\&amp;quot;C:\Users\Diter\.wslconfig\&amp;quot; .wslconfig content&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Is this normal, am I just blue eyed and thought this would work super nice and fast?&lt;/p&gt; &lt;p&gt;For more context.&lt;/p&gt; &lt;p&gt;I made a &lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lxv3kq377cee1.png?width=729&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f26c02d12e21f6498eb693cd299ccc3e7bcfc0fa"&gt;WSL2 $ nvidia-smi&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The CUDA Version (as far as I read) on the Top right, is the required version, is it necessary to ensure CUDA is on exactly this version?&lt;/p&gt; &lt;p&gt;My CUDA version &lt;code&gt;$nvcc --version&lt;/code&gt; is currently V12.0.140&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/73wko7478cee1.png?width=390&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=79eff7752f8b4c7b9245cfcefac89eeda9329305"&gt;WSL2 $nvcc --version&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was unable to find a way to upgrade CUDA. but to be honest, I'm very confused right now. On the NVIDIA guide (&lt;a href="https://docs.nvidia.com/cuda/wsl-user-guide/index.html"&gt;https://docs.nvidia.com/cuda/wsl-user-guide/index.html&lt;/a&gt;) they especially point out, that I'm not supposed to install other drivers (2.1 Step1).&lt;/p&gt; &lt;p&gt;further context:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ouinimh49cee1.png?width=967&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e243d96e305247b8b0e207b13bdbe865a06425ce"&gt;TaskManager when running ollama prompts in WSL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here you can see, that ollama is using a lot of RAM. I'm curious if the GPU RAM is enough, or if it is used and i just don't notice because the difference is not that much?&lt;/p&gt; &lt;p&gt;Sorry for the mess, I know these are multiple questions, I'm just very confused on how to &amp;quot;fix&amp;quot; this from this point on.&lt;br /&gt; If more context is needed, pleas ask!&lt;/p&gt; &lt;p&gt;Any Idea, link to possible solutions or any explanation (which may help me better understand what I'm actually doing) on this topic is highly appreciated!&lt;/p&gt; &lt;p&gt;I already found this post, but this step-by-step guid didn't quit help me sandly :/&lt;br /&gt; (&lt;a href="https://www.reddit.com/r/ollama/comments/18oxvya/how%5C_i%5C_got%5C_ollama%5C_to%5C_use%5C_my%5C_gpu%5C_in%5C_wsl2%5C_rtx%5C_4090/"&gt;https://www.reddit.com/r/ollama/comments/18oxvya/how\_i\_got\_ollama\_to\_use\_my\_gpu\_in\_wsl2\_rtx\_4090/&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;NOTE: My windows system and NVIDIA drivers are UpToDate&lt;/p&gt; &lt;p&gt;Thanks in advance! :) - Diter&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DiterKlein"&gt; /u/DiterKlein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6hdjy/ollama_on_wsl2_not_using_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6hdjy/ollama_on_wsl2_not_using_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6hdjy/ollama_on_wsl2_not_using_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T12:22:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6b6o8</id>
    <title>DeepSeek-R1-8B-FP16 + vLLM + 4x AMD Instinct Mi60 Server</title>
    <updated>2025-01-21T05:06:24+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1is01zns4aee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6b6o8/deepseekr18bfp16_vllm_4x_amd_instinct_mi60_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6b6o8/deepseekr18bfp16_vllm_4x_amd_instinct_mi60_server/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T05:06:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6mei2</id>
    <title>How much VRAM might I need for my homelab? Which model(s)?</title>
    <updated>2025-01-21T16:23:55+00:00</updated>
    <author>
      <name>/u/--Tinman--</name>
      <uri>https://old.reddit.com/user/--Tinman--</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to add some AI to the homelab and am trying to work out requirements.&lt;/p&gt; &lt;p&gt;Mostly I would use it for search engine stuff or coding questions where i don't want to have to remove all work specific stuff. I do a lot of audiobook series, and it would be handy to feed it a book or two in a series like (Orphan X or Longmire) and get a &amp;quot;The road so far&amp;quot; synopsis. &lt;/p&gt; &lt;p&gt;I don't currently use speech in home assistant much, but its mostly due to it running on a thin client so the whisper piper combo is very slow.&lt;/p&gt; &lt;p&gt;I worked out what I think I would like to be able to do:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Whisper STT&lt;/li&gt; &lt;li&gt;Piper TTS&lt;/li&gt; &lt;li&gt;LLM for Home Assistant&lt;/li&gt; &lt;li&gt;LLM for using context documents so I can feed it a epub(or similar) and get a summary&lt;/li&gt; &lt;li&gt;General code assistant with or without &lt;strong&gt;continue&lt;/strong&gt; in vscode&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I hope there is some overlap like 3-5 can all be done on llama3.2:3b or similar.&lt;/p&gt; &lt;p&gt;I'd like to stick around $500 and I'm not sure what hardware will get me what I want.&lt;/p&gt; &lt;p&gt;I think I would be fine with anything 10+ tokens/s, not being power hungry would be a bonus.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Up to 2 jetson orin nanos? (power usage would be amazing) &lt;/li&gt; &lt;li&gt;Optiplex 5080 upgraded to 64gb of mem and a Nvidia P102-100 10GB? &lt;/li&gt; &lt;li&gt;Something I'm way to inexperienced to even know about?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to clarify any things I left out and to discuss possible variations I have not thought of&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/--Tinman--"&gt; /u/--Tinman-- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6mei2/how_much_vram_might_i_need_for_my_homelab_which/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6mei2/how_much_vram_might_i_need_for_my_homelab_which/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6mei2/how_much_vram_might_i_need_for_my_homelab_which/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T16:23:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6a6jn</id>
    <title>Quen2.5-Coder-32B-Instruct-FP16 + 4x AMD Instinct Mi60 Server</title>
    <updated>2025-01-21T04:09:43+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/f86dbcjxu9ee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6a6jn/quen25coder32binstructfp16_4x_amd_instinct_mi60/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6a6jn/quen25coder32binstructfp16_4x_amd_instinct_mi60/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T04:09:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6otb0</id>
    <title>Deepseek R1: Running Locally on a 3-Year-Old Laptop and a 3060 12GB Desktop</title>
    <updated>2025-01-21T18:03:09+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i6otb0/deepseek_r1_running_locally_on_a_3yearold_laptop/"&gt; &lt;img alt="Deepseek R1: Running Locally on a 3-Year-Old Laptop and a 3060 12GB Desktop" src="https://external-preview.redd.it/LGmhwPblu6uwtH8x-Pl3GRweuvk163J6xhkJZrNFkrQ.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24d30b51e4f51ee4ef722835bc143b035b3069de" title="Deepseek R1: Running Locally on a 3-Year-Old Laptop and a 3060 12GB Desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/hAqBEm4wRsk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6otb0/deepseek_r1_running_locally_on_a_3yearold_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6otb0/deepseek_r1_running_locally_on_a_3yearold_laptop/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T18:03:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6grhi</id>
    <title>Ollama-Powershell-Command-Generator</title>
    <updated>2025-01-21T11:44:25+00:00</updated>
    <author>
      <name>/u/admajic</name>
      <uri>https://old.reddit.com/user/admajic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on the idea of another user here I made a Windows and PowerShell Command toolll!!&lt;br /&gt; Give it a go here: &lt;a href="https://github.com/adamjen/Ollama_Shell_Commands.git"&gt;https://github.com/adamjen/Ollama_Shell_Commands.git&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Ollama-Powershell-Command-Generator A PowerShell script that leverages Ollama AI to generate and execute commands based on user questions. ## Description This script interacts with the Ollama AI platform to perform tasks in a Windows environment using PowerShell. It allows users to generate detailed command sequences or single commands by providing a natural language question. The script also supports automatic execution of generated commands. ## Features - **AI-Powered Command Generation**: Uses Ollama AI models to generate PowerShell commands based on user questions. - **Error Handling**: Checks for Ollama installation and validates inputs. - **Multiple Models Support**: Supports different AI models available in Ollama. - **User Interaction**: Provides options to execute commands automatically or receive explanations. ## Prerequisites 1. **Ollama Installed**: Ensure Ollama is installed on your system. If not, download it from [ollama.ai]( https://ollama.ai ). 2. **Powershell**: Windows PowerShell must be installed and configured on your system. ## Installation 1. Clone this repository or download the `how.ps1` script. 2. Place the script in a directory of your choice. 3. Open PowerShell as an administrator if necessary, depending on the tasks you plan to perform. ## Usage ### Basic Usage Run the script with a question: ```powershell .\how.ps1 &amp;quot;What is the current date?&amp;quot; ``` ### Command-Line Arguments - **`-y`**: Automatically execute the generated command without user confirmation. - **`-m &amp;lt;model_name&amp;gt;`**: Specify an Ollama model to use for generating commands. For example: ```powershell .\how.ps1 -m llama3.2:latest &amp;quot;How can I create a backup of my files?&amp;quot; - Use e at the end of your question to explain the command in details. ### Examples 1. Generate and execute a command: (Use with caution) ```powershell .\how.ps1 &amp;quot;List all running processes&amp;quot; -y ``` 2. Generate a command and receive an explanation: ```powershell .\how.ps1 &amp;quot;How can I create a system backup?&amp;quot; ``` ## Contributing Contributions are welcome! If you encounter issues or have suggestions, please open an issue on the GitHub repository or submit a pull request. ## License Go for it :) --- This README provides clear instructions for users and contributors, ensuring that anyone who downloads the script understands how to use it effectively. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/admajic"&gt; /u/admajic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6grhi/ollamapowershellcommandgenerator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6grhi/ollamapowershellcommandgenerator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6grhi/ollamapowershellcommandgenerator/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T11:44:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1i65qzs</id>
    <title>built a local AI that watches your screen &amp; mic &amp; writes your obsidian notes (ollama-first, open source)</title>
    <updated>2025-01-21T00:25:59+00:00</updated>
    <author>
      <name>/u/louis3195</name>
      <uri>https://old.reddit.com/user/louis3195</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i65qzs/built_a_local_ai_that_watches_your_screen_mic/"&gt; &lt;img alt="built a local AI that watches your screen &amp;amp; mic &amp;amp; writes your obsidian notes (ollama-first, open source) " src="https://external-preview.redd.it/M2dqaXpjaTNyOGVlMX3UEqsouCoXDcMV9Txg0fwqmyMDrKts8K5P20rCv2xc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c9226415249c790e831bee93fb02fae91202e0f" title="built a local AI that watches your screen &amp;amp; mic &amp;amp; writes your obsidian notes (ollama-first, open source) " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/louis3195"&gt; /u/louis3195 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4slw6bi3r8ee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i65qzs/built_a_local_ai_that_watches_your_screen_mic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i65qzs/built_a_local_ai_that_watches_your_screen_mic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T00:25:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i66o1d</id>
    <title>deepseek-r1 is now in Ollama's Models library</title>
    <updated>2025-01-21T01:09:24+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i66o1d/deepseekr1_is_now_in_ollamas_models_library/"&gt; &lt;img alt="deepseek-r1 is now in Ollama's Models library" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="deepseek-r1 is now in Ollama's Models library" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/library/deepseek-r1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i66o1d/deepseekr1_is_now_in_ollamas_models_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i66o1d/deepseekr1_is_now_in_ollamas_models_library/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T01:09:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6gmgq</id>
    <title>Got DeepSeek R1 running locally - Full setup guide and my personal review (Free OpenAI o1 alternative that runs locally??)</title>
    <updated>2025-01-21T11:35:03+00:00</updated>
    <author>
      <name>/u/sleepingbenb</name>
      <uri>https://old.reddit.com/user/sleepingbenb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1i6gmgq/got_deepseek_r1_running_locally_full_setup_guide/"&gt; &lt;img alt="Got DeepSeek R1 running locally - Full setup guide and my personal review (Free OpenAI o1 alternative that runs locally??)" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="Got DeepSeek R1 running locally - Full setup guide and my personal review (Free OpenAI o1 alternative that runs locally??)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Edit: I double-checked the model card on Ollama(&lt;a href="https://ollama.com/library/deepseek-r1"&gt;https://ollama.com/library/deepseek-r1&lt;/a&gt;), and it does mention DeepSeek R1 Distill Qwen 7B in the metadata. So this is actually a distilled model. But honestly, that still impresses me!&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Just discovered DeepSeek R1 and I'm pretty hyped about it. For those who don't know, it's a new &lt;strong&gt;open-source AI model that matches OpenAI o1 and Claude 3.5 Sonnet&lt;/strong&gt; in math, coding, and reasoning tasks.&lt;/p&gt; &lt;p&gt;You can check out Reddit to see what others are saying about DeepSeek R1 vs OpenAI o1 and Claude 3.5 Sonnet. For me it's really good - good enough to be compared with those top models.&lt;/p&gt; &lt;p&gt;And the best part? &lt;strong&gt;You can run it locally on your machine, with total privacy and 100% FREE!!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I've got it running locally and have been playing with it for a while. Here's my setup - super easy to follow:&lt;/p&gt; &lt;p&gt;&lt;em&gt;(Just a note: While I'm using a Mac,&lt;/em&gt; &lt;strong&gt;&lt;em&gt;this guide works exactly the same for Windows and Linux users&lt;/em&gt;&lt;/strong&gt;*! üëå)*&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1) Install Ollama&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Quick intro to Ollama: It's a tool for running AI models locally on your machine. Grab it here: &lt;a href="https://ollama.com/download"&gt;https://ollama.com/download&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vdmiiuw4vbee1.png?width=748&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e1efb91eee9cfd8c654ed3282154e92cbbcedad"&gt;https://preview.redd.it/vdmiiuw4vbee1.png?width=748&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e1efb91eee9cfd8c654ed3282154e92cbbcedad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2) Next, you'll need to pull and run the DeepSeek R1 model locally.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ollama offers different model sizes - basically, bigger models = smarter AI, but need better GPU. Here's the lineup:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1.5B version (smallest): ollama run deepseek-r1:1.5b 8B version: ollama run deepseek-r1:8b 14B version: ollama run deepseek-r1:14b 32B version: ollama run deepseek-r1:32b 70B version (biggest/smartest): ollama run deepseek-r1:70b &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Maybe start with a smaller model first to test the waters. Just open your terminal and run:&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama run deepseek-r1:8b&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Once it's pulled, the model will run locally on your machine. Simple as that!&lt;/p&gt; &lt;p&gt;&lt;em&gt;Note: The bigger versions (like 32B and 70B) need some serious GPU power. Start small and work your way up based on your hardware!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uk32frykvbee1.png?width=966&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df7a11a9b2c03e89b899b9aa3d9e1b62fd194197"&gt;https://preview.redd.it/uk32frykvbee1.png?width=966&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df7a11a9b2c03e89b899b9aa3d9e1b62fd194197&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3) Set up Chatbox - a powerful client for AI models&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Quick intro to Chatbox: a free, clean, and powerful desktop interface that works with most models. I started it as a side project for 2 years. It‚Äôs privacy-focused (all data stays local) and super easy to set up‚Äîno Docker or complicated steps. Download here: &lt;a href="https://chatboxai.app"&gt;https://chatboxai.app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In Chatbox, go to settings and switch the model provider to Ollama. Since you're running models locally, you can ignore the built-in cloud AI options - &lt;strong&gt;no license key or payment is needed!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ye2tfudmvbee1.png?width=1940&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2711854eb585e6940c8fa27fa0fdc6c0e656fd03"&gt;https://preview.redd.it/ye2tfudmvbee1.png?width=1940&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2711854eb585e6940c8fa27fa0fdc6c0e656fd03&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Then set up the Ollama API host - the default setting is &lt;a href="http://127.0.0.1:11434"&gt;&lt;code&gt;http://127.0.0.1:11434&lt;/code&gt;&lt;/a&gt;, which should work right out of the box. That's it! Just pick the model and hit save. Now you're all set and ready to chat with your locally running Deepseek R1! üöÄ&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vizcc81pvbee1.png?width=2238&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b80cb5066444203c85fd5d267b710e991df2381f"&gt;https://preview.redd.it/vizcc81pvbee1.png?width=2238&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b80cb5066444203c85fd5d267b710e991df2381f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope this helps! Let me know if you run into any issues.&lt;/p&gt; &lt;p&gt;---------------------&lt;/p&gt; &lt;p&gt;Here are a few tests I ran on my local DeepSeek R1 setup (loving Chatbox's &lt;strong&gt;artifact preview&lt;/strong&gt; feature btw!) üëá&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Explain TCP:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dqa138svvbee1.png?width=2268&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47c01c70f596a22e1c4cfb85878f2dd539a47824"&gt;https://preview.redd.it/dqa138svvbee1.png?width=2268&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47c01c70f596a22e1c4cfb85878f2dd539a47824&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Honestly, this looks pretty good, especially considering it's just an 8B model!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Make a Pac-Man game:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/iwjhq593zbee1.gif"&gt;https://i.redd.it/iwjhq593zbee1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It looks great, but I couldn‚Äôt actually play it. I feel like there might be a few small bugs that could be fixed with some tweaking. (Just to clarify, this wasn‚Äôt done on the local model ‚Äî my mac doesn‚Äôt have enough space for the largest deepseek R1 70b model, so I used the cloud model instead.)&lt;/p&gt; &lt;p&gt;---------------------&lt;/p&gt; &lt;p&gt;Honestly, I‚Äôve seen a lot of overhyped posts about models here lately, so I was a bit skeptical going into this. But after testing DeepSeek R1 myself, I think it‚Äôs actually really solid. It‚Äôs not some magic replacement for OpenAI or Claude, but it‚Äôs &lt;strong&gt;surprisingly capable&lt;/strong&gt; for something that runs locally. The fact that it‚Äôs free and works offline is a huge plus.&lt;/p&gt; &lt;p&gt;What do you guys think? Curious to hear your honest thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sleepingbenb"&gt; /u/sleepingbenb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6gmgq/got_deepseek_r1_running_locally_full_setup_guide/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1i6gmgq/got_deepseek_r1_running_locally_full_setup_guide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1i6gmgq/got_deepseek_r1_running_locally_full_setup_guide/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-21T11:35:03+00:00</published>
  </entry>
</feed>
