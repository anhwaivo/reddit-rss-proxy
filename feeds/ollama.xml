<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-03T05:48:50+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ifz7f5</id>
    <title>Running Mistral Large v2 on Lambda Labs for my dev team, good idea?</title>
    <updated>2025-02-02T15:09:14+00:00</updated>
    <author>
      <name>/u/SelectSpread</name>
      <uri>https://old.reddit.com/user/SelectSpread</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;During the weekend I played around with a lambda labs h200 machine (96 GB VRAM, 432 GB RAM and 64 x64 cores) I was running mistral large v2 (123B) which occupied around 75 GB of VRAM. I also deployed open web-ui and used &lt;a href="http://continue.dev"&gt;continue.dev&lt;/a&gt; in intellij and vscode. That was not perfect but still quite cood, compared to a webchat based approach with copy paste.&lt;br /&gt; As we mainly do Java, and Mistral has a high score in Java, I thought it might be a data privacy friendly approach to host such a system during office ours for my team (around 10 developers; startup and shutdown of the instance via lambda lab's API). Hardware (and electricity) are too expensive to self host. What do you think about that approach? What would you differently? Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SelectSpread"&gt; /u/SelectSpread &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifz7f5/running_mistral_large_v2_on_lambda_labs_for_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifz7f5/running_mistral_large_v2_on_lambda_labs_for_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifz7f5/running_mistral_large_v2_on_lambda_labs_for_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T15:09:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifa93h</id>
    <title>DeepSeek R1 Hardware Requirements Explained</title>
    <updated>2025-02-01T16:39:48+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifa93h/deepseek_r1_hardware_requirements_explained/"&gt; &lt;img alt="DeepSeek R1 Hardware Requirements Explained" src="https://external-preview.redd.it/bavhQxeXV5pgAp-fBIVoF8XffcIw7GN5u11i9CCbtIY.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2a472a729ce975767359b47dc4788711949124d4" title="DeepSeek R1 Hardware Requirements Explained" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/5RhPZgDoglE"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifa93h/deepseek_r1_hardware_requirements_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifa93h/deepseek_r1_hardware_requirements_explained/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T16:39:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifqglv</id>
    <title>Using Local LLMS to read text images, pdf, excel, programming scripts and also output modified text files and programming scripts</title>
    <updated>2025-02-02T05:44:33+00:00</updated>
    <author>
      <name>/u/sheik_ali</name>
      <uri>https://old.reddit.com/user/sheik_ali</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifqglv/using_local_llms_to_read_text_images_pdf_excel/"&gt; &lt;img alt="Using Local LLMS to read text images, pdf, excel, programming scripts and also output modified text files and programming scripts" src="https://a.thumbs.redditmedia.com/xHY4X2cd7XDsA6zaZTJJ-cfhkKfmWIaRxyRRqrfz894.jpg" title="Using Local LLMS to read text images, pdf, excel, programming scripts and also output modified text files and programming scripts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Guys,&lt;/p&gt; &lt;p&gt;I would like to ask, I am using Ollama to run local LLMs, for now as yall might know since Ollama is run on a command prompt, one can only send text to the LLM for it to read and interpret. I understand there is a way to convert the files to a text for the LLMs to read and interpret, but what if the files I want to modify with the LLM's help is too big to convert to text to send in the command prompt? Is there a similiar function/feature to attach files to send in a prompt to the local LLMs just like in chatgpt prompt UI as seen in the attached image/GIF?&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/5m1yvitwynge1.gif"&gt;https://i.redd.it/5m1yvitwynge1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xfxsiegxynge1.png?width=288&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cfc2fa0ae1763d86bbaec4803d5128f84a548058"&gt;https://preview.redd.it/xfxsiegxynge1.png?width=288&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cfc2fa0ae1763d86bbaec4803d5128f84a548058&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sheik_ali"&gt; /u/sheik_ali &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifqglv/using_local_llms_to_read_text_images_pdf_excel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifqglv/using_local_llms_to_read_text_images_pdf_excel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifqglv/using_local_llms_to_read_text_images_pdf_excel/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T05:44:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig17qx</id>
    <title>anybody tried rx 580 2048 tsb with 8gb of vram for deepseek r1 8b</title>
    <updated>2025-02-02T16:37:59+00:00</updated>
    <author>
      <name>/u/felix_ardyan</name>
      <uri>https://old.reddit.com/user/felix_ardyan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i just wonder the peformance of it with unnoficial amd for ollama anybody tried and if soo how good it is&lt;/p&gt; &lt;p&gt;note i know is very old performace but i want to buy it for my egpu for playing game just wondering if it can do funny ai&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/felix_ardyan"&gt; /u/felix_ardyan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig17qx/anybody_tried_rx_580_2048_tsb_with_8gb_of_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig17qx/anybody_tried_rx_580_2048_tsb_with_8gb_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig17qx/anybody_tried_rx_580_2048_tsb_with_8gb_of_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T16:37:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig19mg</id>
    <title>Sql generation with llama</title>
    <updated>2025-02-02T16:40:16+00:00</updated>
    <author>
      <name>/u/rock_db_saanu</name>
      <uri>https://old.reddit.com/user/rock_db_saanu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using llama 3 as local llm to generate sql from text. Using RAG with good enough sql statements and ddl for all relevant tables and table joins Llama is just good enough but slow and also sometimes not accurate. Do you recommend any other local LLM which can be better than this I have GPU and 32 GB RAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rock_db_saanu"&gt; /u/rock_db_saanu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig19mg/sql_generation_with_llama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig19mg/sql_generation_with_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig19mg/sql_generation_with_llama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T16:40:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig2g1f</id>
    <title>Permissions for Ollama and Chrome extensions</title>
    <updated>2025-02-02T17:29:33+00:00</updated>
    <author>
      <name>/u/rajatrocks</name>
      <uri>https://old.reddit.com/user/rajatrocks</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I want my Chrome extension to be able to interact with Ollama, from the command line I need to run:&lt;/p&gt; &lt;p&gt;&lt;code&gt;launchctl setenv OLLAMA_ORIGINS &amp;quot;chrome-extension://gldebcpkoojijledacjeboaehblhfbjg&amp;quot;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;or start ollama with &lt;a href="https://github.com/ollama/ollama/issues/2308"&gt;a special command line&lt;/a&gt;. This makes using Ollama with my extension harder than any of the other local model services that I've used. Are there any alternatives? Any guidance would be appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rajatrocks"&gt; /u/rajatrocks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig2g1f/permissions_for_ollama_and_chrome_extensions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig2g1f/permissions_for_ollama_and_chrome_extensions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig2g1f/permissions_for_ollama_and_chrome_extensions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T17:29:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1iffzhn</id>
    <title>I created a web UI for Ollama that lets you talk to your models and manage them</title>
    <updated>2025-02-01T20:48:26+00:00</updated>
    <author>
      <name>/u/Ok_Promotion_9578</name>
      <uri>https://old.reddit.com/user/Ok_Promotion_9578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iffzhn/i_created_a_web_ui_for_ollama_that_lets_you_talk/"&gt; &lt;img alt="I created a web UI for Ollama that lets you talk to your models and manage them" src="https://external-preview.redd.it/MbCNCUfEvON9rMG6_Ug9gghhk5NKAhUN5fohRsQ5Kk0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bca5176e36c712849de99c8520c0e7c3eaa142c4" title="I created a web UI for Ollama that lets you talk to your models and manage them" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I believe that are some excellent solutions for this problem, but I wanted to take a stab at making my own and focusing on making it super light weight, minimalistic, and clean.&lt;/p&gt; &lt;p&gt;It is still in the early stages, but I'd love some feedback on what I've built so far and to hear about what you'd like to see in a solution like this! &lt;/p&gt; &lt;p&gt;I'm thinking about really cool things in the roadmap down the line, but wanted to start simple and involve the community.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/majicmaj/aloha"&gt;https://github.com/majicmaj/aloha&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xu3brq22blge1.png?width=750&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78b7bbe65ab50348fe373ed1e35d6e80e51e5650"&gt;https://preview.redd.it/xu3brq22blge1.png?width=750&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78b7bbe65ab50348fe373ed1e35d6e80e51e5650&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Promotion_9578"&gt; /u/Ok_Promotion_9578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iffzhn/i_created_a_web_ui_for_ollama_that_lets_you_talk/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iffzhn/i_created_a_web_ui_for_ollama_that_lets_you_talk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iffzhn/i_created_a_web_ui_for_ollama_that_lets_you_talk/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T20:48:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig5v7z</id>
    <title>Which DeepSeek model is the "funny" one</title>
    <updated>2025-02-02T19:50:41+00:00</updated>
    <author>
      <name>/u/Accomplished_Smell53</name>
      <uri>https://old.reddit.com/user/Accomplished_Smell53</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like web version barely answers about China so I downloaded &lt;code&gt;deepseek-r1:8b&lt;/code&gt; locally and when I ask something about ccp, it just acts like chatgpt and tells everything as is. Some say it's because the model is altered little bit by ollama and some say I need to get the full 500gb model. Which one should I get or what should I do?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished_Smell53"&gt; /u/Accomplished_Smell53 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig5v7z/which_deepseek_model_is_the_funny_one/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig5v7z/which_deepseek_model_is_the_funny_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig5v7z/which_deepseek_model_is_the_funny_one/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T19:50:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifkdid</id>
    <title>Run DeepSeek-R1 Locally with Ollama and Open-WebUI (Docker Compose)</title>
    <updated>2025-02-02T00:10:22+00:00</updated>
    <author>
      <name>/u/ntalekt</name>
      <uri>https://old.reddit.com/user/ntalekt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deploy DeepSeek-R1 on your local machine using Ollama and Open-WebUI with this Docker Compose setup. Perfect for those without GPU hardware who want to experiment with AI models.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clone the repo&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;code&gt;git clone&lt;/code&gt; &lt;a href="https://github.com/ntalekt/deepseek-r1-docker-compose.git"&gt;&lt;code&gt;https://github.com/ntalekt/deepseek-r1-docker-compose.git&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Start services&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;code&gt;docker compose up -d&lt;/code&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Access Web UI: &lt;a href="http://localhost:3000/"&gt;http://localhost:3000&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;CPU-only setup (no GPU required)&lt;/li&gt; &lt;li&gt;Automatic download of deepseek-r1:8b model&lt;/li&gt; &lt;li&gt;Easy installation and management with Docker Compose&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Requirements:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Docker Engine v20.10.10+&lt;/li&gt; &lt;li&gt;Docker Compose v2.20.0+&lt;/li&gt; &lt;li&gt;8GB RAM (16GB recommended)&lt;/li&gt; &lt;li&gt;20GB+ free disk space&lt;/li&gt; &lt;li&gt;Linux/macOS/WSL2&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Note:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;CPU inference will be slower than GPU-accelerated setups. Consider GPU hardware for production use.&lt;/li&gt; &lt;li&gt;License: MIT&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full repository: &lt;a href="https://github.com/ntalekt/deepseek-r1-docker-compose"&gt;https://github.com/ntalekt/deepseek-r1-docker-compose&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ntalekt"&gt; /u/ntalekt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifkdid/run_deepseekr1_locally_with_ollama_and_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifkdid/run_deepseekr1_locally_with_ollama_and_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifkdid/run_deepseekr1_locally_with_ollama_and_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T00:10:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig81i0</id>
    <title>Can't get Ollama to use B580</title>
    <updated>2025-02-02T21:20:27+00:00</updated>
    <author>
      <name>/u/Ejo2001</name>
      <uri>https://old.reddit.com/user/Ejo2001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I recently picked up an Intel ARC B580 to run Ollama on, but I can't for the life of me get it to work. I have installed Conda, I have followed all official guides from Ollama, I've installed OneAPI, I got the latest B580 drivers, I've tried both Windows and Linux, I've followed a video tutorial, I've initialized ollama-init, but I just can't get it working.&lt;/p&gt; &lt;p&gt;Has anyone got it working? Can someone tell me what I do wrong? These are the guides I've followed so far: &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/intel/ipex-llm/blob/main/docs%2Fmddocs%2FQuickstart%2Follama_quickstart.md"&gt;https://github.com/intel/ipex-llm/blob/main/docs%2Fmddocs%2FQuickstart%2Follama_quickstart.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/intel/ipex-llm/blob/main/docs%2Fmddocs%2FQuickstart%2Fbmg_quickstart.md"&gt;https://github.com/intel/ipex-llm/blob/main/docs%2Fmddocs%2FQuickstart%2Fbmg_quickstart.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/dHgFl2ccq7k?si=NekwbHQ6Y0S2rgeH"&gt;https://youtu.be/dHgFl2ccq7k?si=NekwbHQ6Y0S2rgeH&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Does anyone have any idea how to get it running?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ejo2001"&gt; /u/Ejo2001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig81i0/cant_get_ollama_to_use_b580/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig81i0/cant_get_ollama_to_use_b580/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig81i0/cant_get_ollama_to_use_b580/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T21:20:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig4qrp</id>
    <title>Local Quantization Workflows: what tools do you use?</title>
    <updated>2025-02-02T19:03:59+00:00</updated>
    <author>
      <name>/u/SilentChip5913</name>
      <uri>https://old.reddit.com/user/SilentChip5913</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey everyone, quick question for those who work on quantizing models locally:&lt;/p&gt; &lt;p&gt;what tool or workflow do you currently use to keep track of different quantization iterations (e.g., tracking versions, comparing results, reverting changes)?&lt;/p&gt; &lt;p&gt;curious to hear what’s working (or not working) for you :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SilentChip5913"&gt; /u/SilentChip5913 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig4qrp/local_quantization_workflows_what_tools_do_you_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig4qrp/local_quantization_workflows_what_tools_do_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig4qrp/local_quantization_workflows_what_tools_do_you_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T19:03:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1if4p38</id>
    <title>Been messing around with DeepSeek R1 + Ollama, and honestly, it's kinda wild how much you can do locally with free open-source tools. No cloud, no API keys, just your machine and some cool AI magic.</title>
    <updated>2025-02-01T11:50:04+00:00</updated>
    <author>
      <name>/u/hasan_py</name>
      <uri>https://old.reddit.com/user/hasan_py</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;Page-Assist Chrome Extension - &lt;a href="https://github.com/n4ze3m/page-assist"&gt;https://github.com/n4ze3m/page-assist&lt;/a&gt; (any model with any params)&lt;/li&gt; &lt;li&gt;Open Web-UI LLM Wrapper - &lt;a href="https://github.com/open-webui/open-webui"&gt;https://github.com/open-webui/open-webui&lt;/a&gt; (any model with any params)&lt;/li&gt; &lt;li&gt;Browser use – &lt;a href="https://github.com/browser-use/browser-use"&gt;https://github.com/browser-use/browser-use&lt;/a&gt; (deepseek r1:14b or more params) &lt;/li&gt; &lt;li&gt;Roo-Code (VS Code Extension) – &lt;a href="https://github.com/RooVetGit/Roo-Code"&gt;https://github.com/RooVetGit/Roo-Code&lt;/a&gt; (deepseek coder)&lt;/li&gt; &lt;li&gt;n8n – &lt;a href="https://github.com/n8n-io/n8n"&gt;https://github.com/n8n-io/n8n&lt;/a&gt; (any model with any params)&lt;/li&gt; &lt;li&gt;A simple RAG app: &lt;a href="https://github.com/hasan-py/chat-with-pdf-RAG"&gt;https://github.com/hasan-py/chat-with-pdf-RAG&lt;/a&gt; (deepseek r1:8b)&lt;/li&gt; &lt;li&gt;Ai assistant Chrome extension: &lt;a href="https://github.com/hasan-py/Ai-Assistant-Chrome-Extension"&gt;https://github.com/hasan-py/Ai-Assistant-Chrome-Extension&lt;/a&gt; (GPT, Gemini, Grok Api, Ollama added recently)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Full installation video: &lt;a href="https://youtu.be/hjg9kJs8al8?si=rillpsKpjONYMDYW"&gt;https://youtu.be/hjg9kJs8al8?si=rillpsKpjONYMDYW&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone exploring something else? Please share- it would be highly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hasan_py"&gt; /u/hasan_py &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if4p38/been_messing_around_with_deepseek_r1_ollama_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if4p38/been_messing_around_with_deepseek_r1_ollama_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if4p38/been_messing_around_with_deepseek_r1_ollama_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T11:50:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1igcdh6</id>
    <title>Ollama.com (Down?)</title>
    <updated>2025-02-03T00:35:12+00:00</updated>
    <author>
      <name>/u/Bakkario</name>
      <uri>https://old.reddit.com/user/Bakkario</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Everyone...&lt;/p&gt; &lt;p&gt;Simple question, I am not able to get to &lt;a href="http://ollama.com"&gt;ollama.com&lt;/a&gt; and cant ping it from termina. I have tried even to change my ISPs DND to opendns, but still no joy.&lt;/p&gt; &lt;p&gt;Anyone has the same issue?&lt;/p&gt; &lt;p&gt;Also, I am trying to convert some GGUF for Deepseek r1 to ollama, the file converts just fine (as it does not complain or through any errors), but when I try to run it says `Error: exception done_getting_tensors: wrong number of tensors; expected 292, got 291` is this by any mean relatable to not being to reach ollama.com?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bakkario"&gt; /u/Bakkario &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igcdh6/ollamacom_down/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igcdh6/ollamacom_down/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igcdh6/ollamacom_down/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T00:35:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ige721</id>
    <title>Windows 10 GPU usage</title>
    <updated>2025-02-03T02:05:42+00:00</updated>
    <author>
      <name>/u/TwoWrongsAreSoRight</name>
      <uri>https://old.reddit.com/user/TwoWrongsAreSoRight</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know..I know..I should be using linux. I'm working on it. Question. What do I need to do to get Ollama to use my gpu? I have a Radeon 7800xt running deepseek-r1:7b. When I test in LM Studio, it uses my gpu with the same model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TwoWrongsAreSoRight"&gt; /u/TwoWrongsAreSoRight &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ige721/windows_10_gpu_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ige721/windows_10_gpu_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ige721/windows_10_gpu_usage/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T02:05:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1igepcr</id>
    <title>Install and run OpenWebUI without Docker</title>
    <updated>2025-02-03T02:32:01+00:00</updated>
    <author>
      <name>/u/Important_Fishing_73</name>
      <uri>https://old.reddit.com/user/Important_Fishing_73</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How do I install and run openwebui without docker?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Important_Fishing_73"&gt; /u/Important_Fishing_73 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igepcr/install_and_run_openwebui_without_docker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igepcr/install_and_run_openwebui_without_docker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igepcr/install_and_run_openwebui_without_docker/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T02:32:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifv422</id>
    <title>Can someone clarify the subtypes of models (quantization, text vs instruct, etc.)?</title>
    <updated>2025-02-02T11:13:29+00:00</updated>
    <author>
      <name>/u/hn-mc</name>
      <uri>https://old.reddit.com/user/hn-mc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed that models come in many versions, but I'm a little confused about it.&lt;/p&gt; &lt;p&gt;First there are &amp;quot;instruct&amp;quot; models and &amp;quot;text&amp;quot; models? What's the difference?&lt;/p&gt; &lt;p&gt;Second, I know that quantization is a type of compression, and the bigger the model in gigabytes, the less compression, and therefore higher quality, but at cost of hardware demands and speed. I know this general principle. But I don't know what exactly these quantization types mean. For example, I've seen all these types of quantization:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;fp16&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q2_K&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q3_K_L&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q3_K_M&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q3_K_S&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q4_0&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q4_1&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q4_K_M&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q4_K_S&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q5_0&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q5_1&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q5_K_M&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q5_K_S&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q6_K&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;q8_0&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;And they all come for TEXT models and INSTRUCT models?&lt;/p&gt; &lt;p&gt;How to make sense of all that mess?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hn-mc"&gt; /u/hn-mc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifv422/can_someone_clarify_the_subtypes_of_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifv422/can_someone_clarify_the_subtypes_of_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifv422/can_someone_clarify_the_subtypes_of_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T11:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig2lzo</id>
    <title>How would Macbook Pro M3 16gb perform?</title>
    <updated>2025-02-02T17:36:19+00:00</updated>
    <author>
      <name>/u/Adventurous-Hunter98</name>
      <uri>https://old.reddit.com/user/Adventurous-Hunter98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to try ollama on my macbook pro m3 16 gb, but Im curious about the performance. For coding and studying, will it perform same as chatgpt or worse because of ram? Anyone else tried it with the same hardware? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Hunter98"&gt; /u/Adventurous-Hunter98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig2lzo/how_would_macbook_pro_m3_16gb_perform/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig2lzo/how_would_macbook_pro_m3_16gb_perform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig2lzo/how_would_macbook_pro_m3_16gb_perform/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T17:36:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1igfyc9</id>
    <title>Anyone experimenting with The NVIDIA Jetson Orin Nano?</title>
    <updated>2025-02-03T03:37:17+00:00</updated>
    <author>
      <name>/u/TalkProfessional4911</name>
      <uri>https://old.reddit.com/user/TalkProfessional4911</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Interested in seeing some more about local AI performance on the NVIDIA Jetson Orin Nano. Has anyone here had a chance to run any local models on edge computing devices? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TalkProfessional4911"&gt; /u/TalkProfessional4911 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/shorts/ZphIUphDb_E?feature=shared"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igfyc9/anyone_experimenting_with_the_nvidia_jetson_orin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igfyc9/anyone_experimenting_with_the_nvidia_jetson_orin/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T03:37:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1igi3wt</id>
    <title>Spring AI has added support for DeepSeek AISpring AI has added support for DeepSeek AI - Integrating Spring AI with DeepSeek R1 locally using Ollama</title>
    <updated>2025-02-03T05:38:45+00:00</updated>
    <author>
      <name>/u/zarinfam</name>
      <uri>https://old.reddit.com/user/zarinfam</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1igi3wt/spring_ai_has_added_support_for_deepseek_aispring/"&gt; &lt;img alt="Spring AI has added support for DeepSeek AISpring AI has added support for DeepSeek AI - Integrating Spring AI with DeepSeek R1 locally using Ollama" src="https://external-preview.redd.it/xyBhXQrwKGI8bz0IOS9bJIklu6PdPlPZLGkYZI_phRw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f048f4c02cda32194ff3846c3c94d7218079beb" title="Spring AI has added support for DeepSeek AISpring AI has added support for DeepSeek AI - Integrating Spring AI with DeepSeek R1 locally using Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zarinfam"&gt; /u/zarinfam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://itnext.io/spring-ai-has-added-support-for-deepseek-ai-74d0834682a1?sk=6b62a49327a31ec861a446cfc0936b68"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igi3wt/spring_ai_has_added_support_for_deepseek_aispring/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igi3wt/spring_ai_has_added_support_for_deepseek_aispring/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T05:38:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifvbgp</id>
    <title>Can we really do something with deepseek-r1:1.5b?</title>
    <updated>2025-02-02T11:28:07+00:00</updated>
    <author>
      <name>/u/Inevitable-Judge2642</name>
      <uri>https://old.reddit.com/user/Inevitable-Judge2642</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifvbgp/can_we_really_do_something_with_deepseekr115b/"&gt; &lt;img alt="Can we really do something with deepseek-r1:1.5b?" src="https://external-preview.redd.it/eyfcbevjKo97vLjBfB0Fmj0NwFo3-R0O6txnn7zhQLY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb92958779590d19aea5be1f847281f6de982d04" title="Can we really do something with deepseek-r1:1.5b?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Judge2642"&gt; /u/Inevitable-Judge2642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://k33g.hashnode.dev/can-we-really-do-something-with-deepseek-r115b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifvbgp/can_we_really_do_something_with_deepseekr115b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifvbgp/can_we_really_do_something_with_deepseekr115b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T11:28:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig5xsu</id>
    <title>CAG with DataBridge - 6x your retrieval speed!</title>
    <updated>2025-02-02T19:53:43+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;Happy to announce that we've introduced Cache Augmented Generation to &lt;a href="https://github.com/databridge-org/databridge-core"&gt;DataBridge&lt;/a&gt;! Cache Augmented Generation essentially allows you to save the kv-cache of your model once it has processed a corpus of text (eg. a really long system prompt, or a large book). Next time you query your model, it doesn't have to process the entire text again, and only has to process your (presumably smaller) run-time query. This leads to increased speed and lower computation costs.&lt;/p&gt; &lt;p&gt;While it is up to you to decide how effective CAG can be for your use case (we've seen a lot of chatter in this subreddit about whether its beneficial or not) - we just wanted to share an easy to use implementation with you all!&lt;/p&gt; &lt;p&gt;Here's a simple code snippet showing how easy it is to use CAG with DataBridge:&lt;/p&gt; &lt;p&gt;Ingestion path: ``` from databridge import DataBridge db = DataBridge(os.getenv(&amp;quot;DB_URI&amp;quot;))&lt;/p&gt; &lt;p&gt;db.ingest_text(..., metadata={&amp;quot;category&amp;quot; : &amp;quot;db_demo&amp;quot;}) db.ingest_file(..., metadata={&amp;quot;category&amp;quot; : &amp;quot;db_demo&amp;quot;})&lt;/p&gt; &lt;p&gt;db.create_cache(name=&amp;quot;reddit_rag_demo_cache&amp;quot;, filters = {&amp;quot;category&amp;quot;:&amp;quot;db_demo&amp;quot;}) ```&lt;/p&gt; &lt;p&gt;Query path: &lt;code&gt; demo_cache = db.get_cache(&amp;quot;reddit_rag_demo_cache&amp;quot;) response = demo_cache.query(&amp;quot;Tell me more about cache augmented generation&amp;quot;) &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Let us know what you think! Would love some feedback, feature requests, and more!&lt;/p&gt; &lt;p&gt;(PS: apologies for the poor formatting, the reddit markdown editor is being incredibly buggy)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig5xsu/cag_with_databridge_6x_your_retrieval_speed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig5xsu/cag_with_databridge_6x_your_retrieval_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig5xsu/cag_with_databridge_6x_your_retrieval_speed/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T19:53:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig752h</id>
    <title>Ollama's DeepSeek Advanced RAG: Boost Your RAG Chatbot: Hybrid Retrieval (BM25 + FAISS) + Neural Reranking + HyDe🚀</title>
    <updated>2025-02-02T20:43:11+00:00</updated>
    <author>
      <name>/u/akhilpanja</name>
      <uri>https://old.reddit.com/user/akhilpanja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;🚀 DeepSeek's Supercharging RAG Chatbots with Hybrid Search, Reranking &amp;amp; Source Tracking&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Retrieval-Augmented Generation (&lt;strong&gt;RAG&lt;/strong&gt;) is revolutionizing &lt;strong&gt;AI-powered document search&lt;/strong&gt;, but &lt;strong&gt;pure vector search (FAISS)&lt;/strong&gt; isn’t always enough. What if you could &lt;strong&gt;combine keyword-based and semantic search&lt;/strong&gt; to get the &lt;strong&gt;best of both worlds&lt;/strong&gt;?&lt;/p&gt; &lt;p&gt;We just upgraded our &lt;strong&gt;DeepSeek RAG Chatbot&lt;/strong&gt; with:&lt;br /&gt; ✅ &lt;strong&gt;Hybrid Retrieval (BM25 + FAISS)&lt;/strong&gt; for better &lt;strong&gt;keyword &amp;amp; semantic matching&lt;/strong&gt;&lt;br /&gt; ✅ &lt;strong&gt;Cross-Encoder Reranking&lt;/strong&gt; to sort &lt;strong&gt;results by relevance&lt;/strong&gt;&lt;br /&gt; ✅ &lt;strong&gt;Query Expansion (HyDE)&lt;/strong&gt; to &lt;strong&gt;retrieve more accurate results&lt;/strong&gt;&lt;br /&gt; ✅ &lt;strong&gt;Document Source Tracking&lt;/strong&gt; so you know &lt;strong&gt;where answers come from&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here’s &lt;strong&gt;how we did it&lt;/strong&gt; &amp;amp; how you can try it on your own &lt;strong&gt;100% local RAG chatbot&lt;/strong&gt;! 🚀&lt;/p&gt; &lt;h1&gt;🔹 Why Hybrid Retrieval Matters&lt;/h1&gt; &lt;p&gt;Most &lt;strong&gt;RAG chatbots rely only on FAISS&lt;/strong&gt;, a &lt;strong&gt;semantic search engine&lt;/strong&gt; that finds &lt;strong&gt;similar embeddings&lt;/strong&gt; but &lt;strong&gt;ignores exact keyword matches&lt;/strong&gt;. This leads to:&lt;br /&gt; ❌ &lt;strong&gt;Missing relevant sections&lt;/strong&gt; in the documents&lt;br /&gt; ❌ &lt;strong&gt;Returning vague or unrelated answers&lt;/strong&gt;&lt;br /&gt; ❌ &lt;strong&gt;Struggling with domain-specific terminology&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;🔹 &lt;strong&gt;Solution? Combine BM25 (keyword search) with FAISS (semantic search)!&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;🛠️ Before vs. After Hybrid Retrieval&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;Old Version&lt;/th&gt; &lt;th align="left"&gt;New Version&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Retrieval Method&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;FAISS-only&lt;/td&gt; &lt;td align="left"&gt;BM25 + FAISS (Hybrid)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Document Ranking&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;No reranking&lt;/td&gt; &lt;td align="left"&gt;Cross-Encoder Reranking&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Query Expansion&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Basic queries only&lt;/td&gt; &lt;td align="left"&gt;HyDE Query Expansion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Search Accuracy&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Moderate&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;High&lt;/strong&gt; (Hybrid + Reranking)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;🔹 How We Improved It&lt;/h1&gt; &lt;h1&gt;1️⃣ Hybrid Retrieval (BM25 + FAISS)&lt;/h1&gt; &lt;p&gt;Instead of using &lt;strong&gt;only FAISS&lt;/strong&gt;, we:&lt;br /&gt; ✅ &lt;strong&gt;Added BM25 (lexical search)&lt;/strong&gt; for &lt;strong&gt;keyword-based relevance&lt;/strong&gt;&lt;br /&gt; ✅ &lt;strong&gt;Weighted BM25 &amp;amp; FAISS&lt;/strong&gt; to &lt;strong&gt;combine both retrieval strategies&lt;/strong&gt;&lt;br /&gt; ✅ &lt;strong&gt;Used&lt;/strong&gt; &lt;code&gt;EnsembleRetriever&lt;/code&gt; to get &lt;strong&gt;higher-quality results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;💡 &lt;strong&gt;Example:&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;User Query:&lt;/strong&gt; &lt;em&gt;&amp;quot;What is the eligibility for student loans?&amp;quot;&lt;/em&gt;&lt;br /&gt; 🔹 &lt;strong&gt;FAISS-only:&lt;/strong&gt; Might retrieve a &lt;strong&gt;general finance policy&lt;/strong&gt;&lt;br /&gt; 🔹 &lt;strong&gt;BM25-only:&lt;/strong&gt; Might &lt;strong&gt;match a keyword&lt;/strong&gt; but miss the context&lt;br /&gt; 🔹 &lt;strong&gt;Hybrid:&lt;/strong&gt; Finds &lt;strong&gt;exact terms (BM25) + meaning-based context (FAISS)&lt;/strong&gt; ✅&lt;/p&gt; &lt;h1&gt;2️⃣ Neural Reranking with Cross-Encoder&lt;/h1&gt; &lt;p&gt;Even after &lt;strong&gt;retrieval&lt;/strong&gt;, we needed a &lt;strong&gt;smarter way to rank results&lt;/strong&gt;. &lt;strong&gt;Cross-Encoder (&lt;/strong&gt;&lt;code&gt;ms-marco-MiniLM-L-6-v2&lt;/code&gt;&lt;strong&gt;)&lt;/strong&gt; ranks retrieved documents by:&lt;br /&gt; ✅ &lt;strong&gt;Analyzing how well they match the query&lt;/strong&gt;&lt;br /&gt; ✅ &lt;strong&gt;Sorting results by highest probability of relevance&lt;/strong&gt;&lt;br /&gt; ✅ **Utilizing GPU for &lt;strong&gt;fast reranking&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;💡 &lt;strong&gt;Example:&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Query:&lt;/strong&gt; &lt;em&gt;&amp;quot;Eligibility for student loans?&amp;quot;&lt;/em&gt;&lt;br /&gt; 🔹 Without reranking → &lt;strong&gt;Might rank an unrelated finance doc higher&lt;/strong&gt;&lt;br /&gt; 🔹 With reranking → &lt;strong&gt;Ranks the best answer at the top!&lt;/strong&gt; ✅&lt;/p&gt; &lt;h1&gt;3️⃣ Query Expansion with HyDE&lt;/h1&gt; &lt;p&gt;Some &lt;strong&gt;queries don’t retrieve enough documents&lt;/strong&gt; because the &lt;strong&gt;exact wording doesn’t match&lt;/strong&gt;. &lt;strong&gt;HyDE (Hypothetical Document Embeddings)&lt;/strong&gt; fixes this by:&lt;br /&gt; ✅ &lt;strong&gt;Generating a “fake” answer first&lt;/strong&gt;&lt;br /&gt; ✅ &lt;strong&gt;Using this expanded query&lt;/strong&gt; to find &lt;strong&gt;better results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;💡 &lt;strong&gt;Example:&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Query:&lt;/strong&gt; &lt;em&gt;&amp;quot;Who can apply for educational assistance?&amp;quot;&lt;/em&gt;&lt;br /&gt; 🔹 Without HyDE → Might &lt;strong&gt;miss relevant pages&lt;/strong&gt;&lt;br /&gt; 🔹 With HyDE → Expands into &lt;em&gt;&amp;quot;Students, parents, and veterans may apply for financial aid and scholarships...&amp;quot;&lt;/em&gt; ✅&lt;/p&gt; &lt;h1&gt;🛠️ How to Try It on Your Own RAG Chatbot&lt;/h1&gt; &lt;h1&gt;1️⃣ Install Dependencies&lt;/h1&gt; &lt;p&gt;git clone &lt;a href="https://github.com/SaiAkhil066/DeepSeek-RAG-Chatbot.git"&gt;https://github.com/SaiAkhil066/DeepSeek-RAG-Chatbot.git&lt;/a&gt; cd DeepSeek-RAG-Chatbot python -m venv venv venv/Scripts/activate pip install -r requirements.txt&lt;/p&gt; &lt;h1&gt;2️⃣ Download &amp;amp; Set Up Ollama&lt;/h1&gt; &lt;p&gt;&lt;a href="https://ollama.com/"&gt;🔗 Download Ollama&lt;/a&gt; &amp;amp; pull the required models:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama pull deepseek-r1:7b ollama pull nomic-embed-text &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;3️⃣ Run the Chatbot&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;streamlit run app.py &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;🚀 &lt;strong&gt;Upload PDFs, DOCX, TXT, and start chatting!&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;📌 Summary of Upgrades&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;Old Version&lt;/th&gt; &lt;th align="left"&gt;New Version&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Retrieval&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;FAISS-only&lt;/td&gt; &lt;td align="left"&gt;BM25 + FAISS (Hybrid)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Ranking&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;No reranking&lt;/td&gt; &lt;td align="left"&gt;Cross-Encoder Reranking&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Query Expansion&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;No query expansion&lt;/td&gt; &lt;td align="left"&gt;HyDE Query Expansion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Performance&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Moderate&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Fast &amp;amp; GPU-accelerated&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;🚀 Final Thoughts&lt;/h1&gt; &lt;p&gt;By &lt;strong&gt;combining lexical search, semantic retrieval, and neural reranking&lt;/strong&gt;, this update &lt;strong&gt;drastically improves&lt;/strong&gt; the quality of document-based AI search.&lt;/p&gt; &lt;p&gt;🔹 &lt;strong&gt;More accurate answers&lt;/strong&gt;&lt;br /&gt; 🔹 &lt;strong&gt;Better ranking of retrieved documents&lt;/strong&gt;&lt;br /&gt; 🔹 &lt;strong&gt;Clickable sources for verification&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Try it out &amp;amp; let me know your thoughts! 🚀💡&lt;/p&gt; &lt;p&gt;🔗 &lt;a href="https://github.com/SaiAkhil066/DeepSeek-RAG-Chatbot"&gt;&lt;strong&gt;GitHub Repo&lt;/strong&gt;&lt;/a&gt; | 💬 &lt;strong&gt;Drop your feedback in the comments!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akhilpanja"&gt; /u/akhilpanja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig752h/ollamas_deepseek_advanced_rag_boost_your_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig752h/ollamas_deepseek_advanced_rag_boost_your_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig752h/ollamas_deepseek_advanced_rag_boost_your_rag/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T20:43:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig3axm</id>
    <title>🔥 Chipper RAG Toolbox 2.2 is Here! (Ollama API Reflection, DeepSeek, Haystack, Python)</title>
    <updated>2025-02-02T18:04:51+00:00</updated>
    <author>
      <name>/u/Alarming_Divide_1339</name>
      <uri>https://old.reddit.com/user/Alarming_Divide_1339</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ig3axm/chipper_rag_toolbox_22_is_here_ollama_api/"&gt; &lt;img alt="🔥 Chipper RAG Toolbox 2.2 is Here! (Ollama API Reflection, DeepSeek, Haystack, Python)" src="https://external-preview.redd.it/EDUvohhVb5xr-KRAeuTg8gg3QUUyDGnrLD58QPihBNs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=811d33bdc4c31e21572a972d29ec5913e016dd45" title="🔥 Chipper RAG Toolbox 2.2 is Here! (Ollama API Reflection, DeepSeek, Haystack, Python)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Big news for all &lt;strong&gt;Ollama&lt;/strong&gt; and &lt;strong&gt;RAG&lt;/strong&gt; enthusiasts – &lt;strong&gt;Chipper 2.2&lt;/strong&gt; is out, and it's packing some serious upgrades!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Chipper Chains,&lt;/strong&gt; you can now link multiple Chipper instances together, distributing workloads across servers and pushing the ultimate context boundary. Just set your &lt;code&gt;OLLAMA_URL&lt;/code&gt; to another Chipper instance, and lets go. &lt;/p&gt; &lt;p&gt;💡 &lt;strong&gt;What's new?&lt;/strong&gt;&lt;br /&gt; - &lt;strong&gt;Full Ollama API Reflection&lt;/strong&gt; – Chipper is now a seamless drop-in service that fully mirrors the &lt;strong&gt;Ollama Chat API&lt;/strong&gt;, integrating &lt;strong&gt;RAG capabilities&lt;/strong&gt; without breaking existing workflows.&lt;br /&gt; - &lt;strong&gt;API Proxy &amp;amp; Security&lt;/strong&gt; – Reflects &amp;amp; proxies &lt;strong&gt;non-RAG pipeline calls&lt;/strong&gt;, with &lt;strong&gt;bearer token support&lt;/strong&gt; for a &lt;strong&gt;more secure&lt;/strong&gt; Ollama setup.&lt;br /&gt; - &lt;strong&gt;Daisy-Chaining&lt;/strong&gt; – Connect multiple &lt;strong&gt;Chipper&lt;/strong&gt; instances to extend processing across multiple nodes.&lt;br /&gt; - &lt;strong&gt;Middleware&lt;/strong&gt; – Chipper now acts as an &lt;strong&gt;Ollama middleware&lt;/strong&gt;, also enabling &lt;strong&gt;client-side query parameters&lt;/strong&gt; for fine-tuned responses or server side overrides.&lt;br /&gt; - &lt;strong&gt;DeepSeek R1 Support&lt;/strong&gt; - The Chipper web UI does now supports &amp;lt;think&amp;gt; tags.&lt;/p&gt; &lt;p&gt;⚡ &lt;strong&gt;Why this matters?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Easily add &lt;strong&gt;shared RAG capabilities&lt;/strong&gt; to your favourite &lt;strong&gt;Ollama Client&lt;/strong&gt; with &lt;strong&gt;little extra complexity&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Securely expose your &lt;strong&gt;Ollama&lt;/strong&gt; server to desktop clients (like &lt;strong&gt;Enchanted&lt;/strong&gt;) with bearer token support.&lt;/li&gt; &lt;li&gt;Run multi-instance &lt;strong&gt;RAG pipelines&lt;/strong&gt; to augment requests with distributed knowledge bases or services.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you find Chipper useful or exciting, &lt;strong&gt;leaving a star would be lovely&lt;/strong&gt; and will help others discover Chipper too ✨. I am working on many more ideas and occasionally want to share my progress here with you.&lt;/p&gt; &lt;p&gt;For everyone upgrading to version 2.2, please regenerate your &lt;code&gt;.env&lt;/code&gt; files using the &lt;code&gt;run&lt;/code&gt; tool, and don't forget to regenerate your images.&lt;/p&gt; &lt;p&gt;🔗 &lt;strong&gt;Check it out &amp;amp; demo it yourself:&lt;/strong&gt;&lt;br /&gt; 👉 &lt;a href="https://github.com/TilmanGriesel/chipper"&gt;https://github.com/TilmanGriesel/chipper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;👉 &lt;a href="https://chipper.tilmangriesel.com/"&gt;https://chipper.tilmangriesel.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt; &lt;a href="https://chipper.tilmangriesel.com/get-started.html"&gt;https://chipper.tilmangriesel.com/get-started.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/y8kq2y36lrge1.gif"&gt;https://i.redd.it/y8kq2y36lrge1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/6j46hz77lrge1.gif"&gt;https://i.redd.it/6j46hz77lrge1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/o9cfokr7lrge1.gif"&gt;https://i.redd.it/o9cfokr7lrge1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alarming_Divide_1339"&gt; /u/Alarming_Divide_1339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig3axm/chipper_rag_toolbox_22_is_here_ollama_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig3axm/chipper_rag_toolbox_22_is_here_ollama_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig3axm/chipper_rag_toolbox_22_is_here_ollama_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T18:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig7uen</id>
    <title>Testing Uncensored DeepSeek-R1-Distill-Llama-70B-abliterated FP16</title>
    <updated>2025-02-02T21:11:59+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/012ebmi7ksge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ig7uen/testing_uncensored/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ig7uen/testing_uncensored/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T21:11:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ighr82</id>
    <title>Customizable GUI for ollama (less than 1MB)</title>
    <updated>2025-02-03T05:17:25+00:00</updated>
    <author>
      <name>/u/A8LR</name>
      <uri>https://old.reddit.com/user/A8LR</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ighr82/customizable_gui_for_ollama_less_than_1mb/"&gt; &lt;img alt="Customizable GUI for ollama (less than 1MB)" src="https://preview.redd.it/vyq6efv0zuge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fdbf99e396bc2b32ada4b4be7b12700a7f25056" title="Customizable GUI for ollama (less than 1MB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A barebones chat interface for Ollama in 4 files; HTML, CSS, JS and Python.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/qusaismael/localllm"&gt;https://github.com/qusaismael/localllm&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Why post: seeing people struggle with over-engineered examples. MIT licensed = modify freely. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A8LR"&gt; /u/A8LR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vyq6efv0zuge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ighr82/customizable_gui_for_ollama_less_than_1mb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ighr82/customizable_gui_for_ollama_less_than_1mb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T05:17:25+00:00</published>
  </entry>
</feed>
