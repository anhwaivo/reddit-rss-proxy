<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-19T11:34:39+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ms8ghv</id>
    <title>I made a no-install-needed web-GUI for Ollama</title>
    <updated>2025-08-16T21:32:51+00:00</updated>
    <author>
      <name>/u/DarkTom21</name>
      <uri>https://old.reddit.com/user/DarkTom21</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ms8ghv/i_made_a_noinstallneeded_webgui_for_ollama/"&gt; &lt;img alt="I made a no-install-needed web-GUI for Ollama" src="https://b.thumbs.redditmedia.com/fUb2ZooOQRQelk4szZCBNb_brztbxvoPSTkbeSXzDhk.jpg" title="I made a no-install-needed web-GUI for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;For the last while I've been working on a solution to a problem I've had ever since getting into Ollama, that being a GUI that is both powerful and easy to set up across multiple devices. Firstly I tried using OpenWebUI, however quickly dropped it due to needing to install either Python or Docker just to run it, and I didn't want to install more runtimes just to run a GUI. I looked at alternatives, but none seemed to quite fit what I wanted.&lt;/p&gt; &lt;p&gt;That's why I decided to make LlamaPen, LlamaPen is an open-source, no-download-required web app/GUI that lets you easily interface with your local instance of Ollama without needing to download anything extra. It contains all the basics you would expect from a GUI such as chats, conversations, and model selection, but also contains additional features, such as model management, downloading, mobile, PWA &amp;amp; offline support, formatting markdown and think text, icons for each model, and more, all without needing to go through a lengthy download and setup process.&lt;/p&gt; &lt;p&gt;It is currently available live at &lt;a href="https://llamapen.app/"&gt;https://llamapen.app/&lt;/a&gt; with a GitHub repo going further into the specifics and features at &lt;a href="https://github.com/ImDarkTom/LlamaPen"&gt;https://github.com/ImDarkTom/LlamaPen&lt;/a&gt;. If you have any questions or would like to know more feel free to leave a comment here and I will try to reply as soon as possible, and if you encounter any issues you can either comment here or I recommend opening an issue on the GitHub repo for faster support.&lt;/p&gt; &lt;p&gt;Thanks for reading and I hope at least one other person than me finds this useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkTom21"&gt; /u/DarkTom21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ms8ghv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ms8ghv/i_made_a_noinstallneeded_webgui_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ms8ghv/i_made_a_noinstallneeded_webgui_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-16T21:32:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1msy9w5</id>
    <title>Looking for most optimal llms for ollama</title>
    <updated>2025-08-17T18:17:25+00:00</updated>
    <author>
      <name>/u/biggerbuiltbody</name>
      <uri>https://old.reddit.com/user/biggerbuiltbody</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just downloaded Ollama yesterday, and the list of all the models is a bit overwhelming, lol. i got a 300gb hard drive and an RTX 3060, and i am looking for an llm to help with some coding, general questions, maybe some math, idek, but if anyone's got any recs or even a google drive or something, I'd appreciate any help&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/biggerbuiltbody"&gt; /u/biggerbuiltbody &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msy9w5/looking_for_most_optimal_llms_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msy9w5/looking_for_most_optimal_llms_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msy9w5/looking_for_most_optimal_llms_for_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T18:17:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1msobrw</id>
    <title>Chat Box: Open-Source Browser Extension</title>
    <updated>2025-08-17T11:21:41+00:00</updated>
    <author>
      <name>/u/MinhxThanh</name>
      <uri>https://old.reddit.com/user/MinhxThanh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1msobrw/chat_box_opensource_browser_extension/"&gt; &lt;img alt="Chat Box: Open-Source Browser Extension" src="https://external-preview.redd.it/eGN6eTcxOG9ka2pmMVQLTLeRVkAMK-J22uN-yMath0VT_Z9kdFkV696qxezn.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f15eeef2e915c27a053a58c45bde7ef6fa8155a9" title="Chat Box: Open-Source Browser Extension" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I wanted to share this open-source project I've come across called Chat Box. It's a browser extension that brings AI chat, advanced web search, document interaction, and other handy tools right into a sidebar in your browser. It's designed to make your online workflow smoother without needing to switch tabs or apps constantly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What It Does&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;At its core, Chat Box gives you a persistent AI-powered chat interface that you can access with a quick shortcut (Ctrl+E or Cmd+E). It supports a bunch of AI providers like OpenAI, DeepSeek, Claude, and even local LLMs via Ollama. You just configure your API keys in the settings, and you're good to go.&lt;/p&gt; &lt;p&gt;It's all open-source under GPL-3.0, so you can tweak it if you want.&lt;/p&gt; &lt;p&gt;If you run into any errors, issues, or want to suggest a new feature, please create a new Issue on GitHub and describe it in detail – I'll respond ASAP!&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/MinhxThanh/Chat-Box"&gt;https://github.com/MinhxThanh/Chat-Box&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chrome Web Store: &lt;a href="https://chromewebstore.google.com/detail/chat-box-chat-with-all-ai/hhaaoibkigonnoedcocnkehipecgdodm"&gt;https://chromewebstore.google.com/detail/chat-box-chat-with-all-ai/hhaaoibkigonnoedcocnkehipecgdodm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Firefox Add-Ons: &lt;a href="https://addons.mozilla.org/en-US/firefox/addon/chat-box-chat-with-all-ai/"&gt;https://addons.mozilla.org/en-US/firefox/addon/chat-box-chat-with-all-ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MinhxThanh"&gt; /u/MinhxThanh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uv7l508odkjf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msobrw/chat_box_opensource_browser_extension/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msobrw/chat_box_opensource_browser_extension/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T11:21:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtclvn</id>
    <title>Qwen3 models - cannot disable thinking now?</title>
    <updated>2025-08-18T04:47:15+00:00</updated>
    <author>
      <name>/u/derSchwamm11</name>
      <uri>https://old.reddit.com/user/derSchwamm11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried a few Qwen3 models like 14b with ollama. Using /no_think in the system or user prompt has no effect, neither does setting &amp;quot;think&amp;quot;: false in the JSON payload to /chat/completions. Firing up the model in the terminal and using /set nothink also does nothing.&lt;/p&gt; &lt;p&gt;This all seems to fly in the face of documentation in the model and from ollama and I am going crazy here. Am I missing something? I am on ollama 0.11.4, linux&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/derSchwamm11"&gt; /u/derSchwamm11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtclvn/qwen3_models_cannot_disable_thinking_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtclvn/qwen3_models_cannot_disable_thinking_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtclvn/qwen3_models_cannot_disable_thinking_now/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T04:47:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1msjayu</id>
    <title>Knowledge graph using gemma3</title>
    <updated>2025-08-17T06:13:22+00:00</updated>
    <author>
      <name>/u/cython_boy</name>
      <uri>https://old.reddit.com/user/cython_boy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1msjayu/knowledge_graph_using_gemma3/"&gt; &lt;img alt="Knowledge graph using gemma3" src="https://preview.redd.it/fviiip42uijf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1890c5ec0cd777145b90a7dff123fc028801135" title="Knowledge graph using gemma3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;strong&gt;Streamlit web app&lt;/strong&gt; that generates interactive &lt;strong&gt;knowledge graphs&lt;/strong&gt; from plain text using &lt;strong&gt;Ollama's open sourcw models.(geema3 , grnaite , llama3 , gpt-oss ,....)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Two input methods&lt;/strong&gt;: Upload &lt;code&gt;.txt&lt;/code&gt; file or paste text directly.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama model integration&lt;/strong&gt;: Select from available local models (e.g., Gemma, Mistral, LLaMA).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Automatic graph storage&lt;/strong&gt;: Generated graphs are saved and can be reloaded anytime.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interactive visualization&lt;/strong&gt;: Zoom, drag, and explore relationships between concepts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optimized for speed&lt;/strong&gt;: Uses hashed filenames to prevent regenerating the same graph.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/ganeshnikhil/Kgraph"&gt;kgraph&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cython_boy"&gt; /u/cython_boy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fviiip42uijf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1msjayu/knowledge_graph_using_gemma3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1msjayu/knowledge_graph_using_gemma3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T06:13:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt4fjp</id>
    <title>Connected Chrome's AI API to ollama, enabling ANY web app built for Chrome's local Gemini to seamlessly work with open-source LLMs</title>
    <updated>2025-08-17T22:19:07+00:00</updated>
    <author>
      <name>/u/andrei0david</name>
      <uri>https://old.reddit.com/user/andrei0david</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andrei0david"&gt; /u/andrei0david &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/AndreiDavid/status/1956692594878038046"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mt4fjp/connected_chromes_ai_api_to_ollama_enabling_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mt4fjp/connected_chromes_ai_api_to_ollama_enabling_any/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T22:19:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mslsvy</id>
    <title>Qwen 4B on iPhone Neural Engine runs at 20t/s</title>
    <updated>2025-08-17T08:46:29+00:00</updated>
    <author>
      <name>/u/Glad-Speaker3006</name>
      <uri>https://old.reddit.com/user/Glad-Speaker3006</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mslsvy/qwen_4b_on_iphone_neural_engine_runs_at_20ts/"&gt; &lt;img alt="Qwen 4B on iPhone Neural Engine runs at 20t/s" src="https://external-preview.redd.it/cHhxMXV5aTJtampmMdanLIb7eGjfE0jGaRLozRDSee5XNaS_ENo8Z9uouVRD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ee59b94928cc5ff9aff58431a6e1d4fbdc8158b" title="Qwen 4B on iPhone Neural Engine runs at 20t/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am excited to finally bring 4B models to iPhone!&lt;/p&gt; &lt;p&gt;Vector Space is a framework that makes it possible to run LLM on iPhones &lt;strong&gt;locally on the Neural Engine&lt;/strong&gt;. This translates to:&lt;/p&gt; &lt;p&gt;⚡️Faster inference. Qwen 4B runs at &lt;strong&gt;~20 token/s&lt;/strong&gt; in short context.&lt;/p&gt; &lt;p&gt;🔋 Low Energy. Energy consumption is 1/5 compared to CPU, which means your iPhone will stay cool and it will not drain your battery. &lt;/p&gt; &lt;p&gt;Vector Space also comes with an app 📲 that allows you to download models and try out the framework with 0 code. Try it now on TestFlight:&lt;/p&gt; &lt;p&gt;&lt;a href="https://testflight.apple.com/join/HXyt2bjU"&gt;https://testflight.apple.com/join/HXyt2bjU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fine prints: 1. The app all does not guarantee the persistence of data. 2. Currently only supports hardware released on or after 2022 (&amp;gt;= iPhone 14) 3. First time model compilation will take several minutes. Subsequent loads will be instant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glad-Speaker3006"&gt; /u/Glad-Speaker3006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xewf9vn2mjjf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mslsvy/qwen_4b_on_iphone_neural_engine_runs_at_20ts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mslsvy/qwen_4b_on_iphone_neural_engine_runs_at_20ts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-17T08:46:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt9fop</id>
    <title>Serene Pub 0.4 Release - Ollama Manager, Accessability &amp; Tags</title>
    <updated>2025-08-18T02:07:10+00:00</updated>
    <author>
      <name>/u/doolijb</name>
      <uri>https://old.reddit.com/user/doolijb</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doolijb"&gt; /u/doolijb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mt8hd1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mt9fop/serene_pub_04_release_ollama_manager/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mt9fop/serene_pub_04_release_ollama_manager/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T02:07:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtjmdz</id>
    <title>Best GUI for mac</title>
    <updated>2025-08-18T11:41:27+00:00</updated>
    <author>
      <name>/u/Adventurous-Hunter98</name>
      <uri>https://old.reddit.com/user/Adventurous-Hunter98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, Im trying to find the best GUI for mac that offers good features. Currently there is this new gui comes with ollama but I dont want this and want to remove it. Anyone can help me with these?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Hunter98"&gt; /u/Adventurous-Hunter98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtjmdz/best_gui_for_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtjmdz/best_gui_for_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtjmdz/best_gui_for_mac/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T11:41:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mte3wt</id>
    <title>RPG Game engine running on qwen3</title>
    <updated>2025-08-18T06:14:21+00:00</updated>
    <author>
      <name>/u/Humbrol2</name>
      <uri>https://old.reddit.com/user/Humbrol2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Building an rpg game engine built on ollama models, use qwen3 with 128k tokens in settings but its running a tad slow. ALways looking for feedback&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/humbrol2/RPG-Assistant"&gt;humbrol2/RPG-Assistant: An AI assisted RPG game engine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;RPG Assistant&lt;/p&gt; &lt;p&gt;An AI-powered RPG Game Master using Ollama local models for immersive tabletop gaming experiences.&lt;/p&gt; &lt;h1&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-Agent System&lt;/strong&gt;: Game Master, World Builder, Character Manager, and Narrative Engine&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Persistent Storage&lt;/strong&gt;: JSON-based world and character data management&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Command-Line Interface&lt;/strong&gt;: Easy-to-use CLI for game management&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open World Sandbox&lt;/strong&gt;: Dynamic world generation and storytelling&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Humbrol2"&gt; /u/Humbrol2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mte3wt/rpg_game_engine_running_on_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mte3wt/rpg_game_engine_running_on_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mte3wt/rpg_game_engine_running_on_qwen3/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T06:14:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtrfei</id>
    <title>Using an local Ollama AI agent to solve the N puzzle</title>
    <updated>2025-08-18T16:47:23+00:00</updated>
    <author>
      <name>/u/CommunityOpposite645</name>
      <uri>https://old.reddit.com/user/CommunityOpposite645</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I have just made some program to make an AI agent solve the N puzzle.&lt;/p&gt; &lt;p&gt;Github link: &lt;a href="https://github.com/dangmanhtruong1995/N-puzzle-Agent/tree/main"&gt;https://github.com/dangmanhtruong1995/N-puzzle-Agent/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Youtube link: &lt;a href="https://www.youtube.com/watch?v=Ntol4F4tilg"&gt;https://www.youtube.com/watch?v=Ntol4F4tilg&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The `qwen3:latest` model in the Ollama library was used as the agent, while I chose a simple N puzzle as the problem for it to solve.&lt;/p&gt; &lt;p&gt;Experiments were done on an ASUS Vivobook Pro 15 laptop, with a NVIDIA GeForce RTX 4060 having 8GB of VRAM.&lt;/p&gt; &lt;p&gt;## Overview&lt;/p&gt; &lt;p&gt;This project demonstrates an AI agent solving the classic N-puzzle (sliding tile puzzle) by:&lt;/p&gt; &lt;p&gt;- Analyzing and planning optimal moves using the Qwen3 language model&lt;/p&gt; &lt;p&gt;- Executing moves through automated mouse clicks on the GUI&lt;/p&gt; &lt;p&gt;## How it works&lt;/p&gt; &lt;p&gt;The LLM is given some prompt, with instructions that it could control the following functions: `move_up, move_down, move_left, move_right`. At each turn, the LLM will try to choose from those functions, and the moves would then be made. Code is inspired from the following tutorials on functional calling and ReAct agent from scratch:&lt;/p&gt; &lt;p&gt;- &lt;a href="https://www.philschmid.de/gemma-function-calling"&gt;https://www.philschmid.de/gemma-function-calling&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- &lt;a href="https://www.philschmid.de/langgraph-gemini-2-5-react-agent"&gt;https://www.philschmid.de/langgraph-gemini-2-5-react-agent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;## Installation&lt;/p&gt; &lt;p&gt;To install the necessary libraries, type the following (assuming you are using `conda`):&lt;/p&gt; &lt;p&gt;```shell&lt;/p&gt; &lt;p&gt;conda create --name aiagent python=3.14&lt;/p&gt; &lt;p&gt;conda activate aiagent&lt;/p&gt; &lt;p&gt;pip install -r requirements.txt&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;## How to run&lt;/p&gt; &lt;p&gt;There are two files, `demo_1_n_puzzle_gui.py` (for GUI) and `demo_1_agent.py` (for the AI agent). First, run the GUi file:&lt;/p&gt; &lt;p&gt;```shell&lt;/p&gt; &lt;p&gt;python demo_1_n_puzzle_gui.py&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;The N puzzle GUI will show up. Now, what you need to do is to move it to a proper position of your choosing (I used the top left corner). The reason we need to do this is that the AI agent will control the mouse to click on the move up, down, left, right buttons to interact with the GUI.&lt;/p&gt; &lt;p&gt;Next, we need to use the `Pyautogui` library to make the AI agent program aware of the button locations. Follow the tutorial here to get the coordinates: [link](&lt;a href="https://pyautogui.readthedocs.io/en/latest/quickstart.html"&gt;https://pyautogui.readthedocs.io/en/latest/quickstart.html&lt;/a&gt;)). An example:&lt;/p&gt; &lt;p&gt;```shell&lt;/p&gt; &lt;p&gt;(aiagent) C:\TRUONG\Code_tu_hoc\AI_agent_tutorials\N_puzzle_agent\demo1&amp;gt;python&lt;/p&gt; &lt;p&gt;Python 3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)] on win32&lt;/p&gt; &lt;p&gt;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; import pyautogui&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; pyautogui.position() # current mouse x and y. Move the mouse into position before enter&lt;/p&gt; &lt;p&gt;(968, 56)&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Once you get the coordinates, please populate the following fields in the `demo_1_agent.py` file:&lt;/p&gt; &lt;p&gt;```shell&lt;/p&gt; &lt;p&gt;MOVE_UP_BUTTON_POS = (285, 559)&lt;/p&gt; &lt;p&gt;MOVE_DOWN_BUTTON_POS = (279, 718)&lt;/p&gt; &lt;p&gt;MOVE_LEFT_BUTTON_POS = (195, 646)&lt;/p&gt; &lt;p&gt;MOVE_RIGHT_BUTTON_POS = (367, 647)&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Next, open another Anaconda Prompt and run:&lt;/p&gt; &lt;p&gt;```shell&lt;/p&gt; &lt;p&gt;ollama run qwen3:latest&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Now, open yet another Anaconda Prompt and run:&lt;/p&gt; &lt;p&gt;```shell&lt;/p&gt; &lt;p&gt;python demo_1_agent.py&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;You should start seein the model's thinking trace. Be patient, it takes a while for the AI agent to find the solution.&lt;/p&gt; &lt;p&gt;However, a limitation of this code is that when I tried to run on bigger problems (4x4 puzzle) the AI agent failed to solve it. Perharps if I run models which can fit on 24GB VRAM then it might work, but then I would need to do additional experiments. If you guys could advise me on how to handle this, that would be great. Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CommunityOpposite645"&gt; /u/CommunityOpposite645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtrfei/using_an_local_ollama_ai_agent_to_solve_the_n/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtrfei/using_an_local_ollama_ai_agent_to_solve_the_n/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtrfei/using_an_local_ollama_ai_agent_to_solve_the_n/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T16:47:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtk646</id>
    <title>Presenton now supports presentation generation via MCP</title>
    <updated>2025-08-18T12:07:45+00:00</updated>
    <author>
      <name>/u/goodboydhrn</name>
      <uri>https://old.reddit.com/user/goodboydhrn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mtk646/presenton_now_supports_presentation_generation/"&gt; &lt;img alt="Presenton now supports presentation generation via MCP" src="https://external-preview.redd.it/a3dmbmRvNThxcmpmMROeZeQh15N-GStyHCbN5mnRNX7F3fY722SokL4VMsiB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f06ca9eed421bc468417ca1b178b3dc8c509e26" title="Presenton now supports presentation generation via MCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Presenton, an open source AI presentation tool now supports presentation generation via MCP. &lt;/p&gt; &lt;p&gt;Simply connect to MCP and let you model or agent calls for you to generate presentation. &lt;/p&gt; &lt;p&gt;Documentation: &lt;a href="https://docs.presenton.ai/generate-presentation-over-mcp"&gt;https://docs.presenton.ai/generate-presentation-over-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/presenton/presenton"&gt;https://github.com/presenton/presenton&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goodboydhrn"&gt; /u/goodboydhrn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uf6stn58qrjf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtk646/presenton_now_supports_presentation_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtk646/presenton_now_supports_presentation_generation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T12:07:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtog0w</id>
    <title>RAG with Ollama &amp; OPENWEBUI</title>
    <updated>2025-08-18T14:59:42+00:00</updated>
    <author>
      <name>/u/KookyExtension6513</name>
      <uri>https://old.reddit.com/user/KookyExtension6513</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been trying for the last 2 weeks to build my own LLM with RAG for school, but I cant get the RAG part to work properly. It just doesnt give me an answer on OPENWEBUI or Terminal. Does anyone have a good tutorial that actually works or a solution that i can try?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KookyExtension6513"&gt; /u/KookyExtension6513 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtog0w/rag_with_ollama_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtog0w/rag_with_ollama_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtog0w/rag_with_ollama_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T14:59:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtpq5h</id>
    <title>Paper out on the fine-tune I made of TinyLlama using James Joyce's Finnegan's Wake</title>
    <updated>2025-08-18T15:45:56+00:00</updated>
    <author>
      <name>/u/BidWestern1056</name>
      <uri>https://old.reddit.com/user/BidWestern1056</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last January, I read Finnegan's Wake and was so captivated by the style that I wanted to be able to access it's capabilities for divergent generation for brainstorming ideas in fiction (some of which appear in my latest work: &lt;a href="https://www.amazon.com/dp/B0DMWPGV18"&gt;Don't turn on the sun&lt;/a&gt;, &lt;a href="https://giacomocatanzaro.substack.com/p/dont-turn-on-the-sun?r=1gxtz8"&gt;free substack version&lt;/a&gt;) and in research (much of which has been incorporated into &lt;a href="https://github.com/npc-worldwide/npcpy"&gt;npcpy&lt;/a&gt; and &lt;a href="https://github.com/npc-worldwide/npcsh"&gt;npcsh&lt;/a&gt; and helped me to recognize some of the fundamental limitations of LLMs and &lt;a href="https://arxiv.org/abs/2506.10077"&gt;what these teach us about the nature of natural language interpretation at all &lt;/a&gt; )&lt;/p&gt; &lt;p&gt;I wanted to post this here because ollama has enabled me to build and do so much with small models, and while I built TinyTimV1 before I used ollama, the only reason I even use it consistently is because i can serve it through HF via Ollama. So I hope other ollama enthusiasts appreciate this and maybe even give the model a try (longer inputs are better) and the paper a readthrough&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama run&lt;/code&gt; &lt;a href="http://hf.co/npc-worldwide/TinyTimV1"&gt;&lt;code&gt;hf.co/npc-worldwide/TinyTimV1&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidWestern1056"&gt; /u/BidWestern1056 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2508.11607"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtpq5h/paper_out_on_the_finetune_i_made_of_tinyllama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtpq5h/paper_out_on_the_finetune_i_made_of_tinyllama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T15:45:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtq9de</id>
    <title>Can Local LLMs With Ollama Power My AI RPG?</title>
    <updated>2025-08-18T16:05:18+00:00</updated>
    <author>
      <name>/u/YungMixtape2004</name>
      <uri>https://old.reddit.com/user/YungMixtape2004</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mtq9de/can_local_llms_with_ollama_power_my_ai_rpg/"&gt; &lt;img alt="Can Local LLMs With Ollama Power My AI RPG?" src="https://external-preview.redd.it/-ybdEagyQT1KFK_3fVi2a0G_hggAjSZHAtjhMOXLb-c.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4970999f9dd9932f61f9d6675733e71492002b4" title="Can Local LLMs With Ollama Power My AI RPG?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YungMixtape2004"&gt; /u/YungMixtape2004 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/5LVXrBGLYEM"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtq9de/can_local_llms_with_ollama_power_my_ai_rpg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtq9de/can_local_llms_with_ollama_power_my_ai_rpg/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T16:05:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mu9dhc</id>
    <title>Index Images with ColPali: Multi-Modal Context Engineering</title>
    <updated>2025-08-19T05:09:06+00:00</updated>
    <author>
      <name>/u/Whole-Assignment6240</name>
      <uri>https://old.reddit.com/user/Whole-Assignment6240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I've been working on multi-modal RAG pipeline directly with Colpali at scale. I wrote blog to help understand how Colpali works, and how to set a pipeline with Colpali step by step.&lt;/p&gt; &lt;p&gt;Everything is fully open sourced.&lt;/p&gt; &lt;p&gt;In this project I also did a comparison with CLIP with a single dense vector (1D embedding), and Colpali with multi-dimensional vector generates better results.&lt;/p&gt; &lt;p&gt;breakdown + Python examples: &lt;a href="https://cocoindex.io/blogs/colpali"&gt;https://cocoindex.io/blogs/colpali&lt;/a&gt;&lt;br /&gt; Star GitHub if you like it! &lt;a href="https://github.com/cocoindex-io/cocoindex"&gt;https://github.com/cocoindex-io/cocoindex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking forward to exchange ideas.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Assignment6240"&gt; /u/Whole-Assignment6240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mu9dhc/index_images_with_colpali_multimodal_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mu9dhc/index_images_with_colpali_multimodal_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mu9dhc/index_images_with_colpali_multimodal_context/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-19T05:09:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtre98</id>
    <title>🤖 Built an AI-powered DOCX viewer that extracts &amp; analyzes images with Ollama!</title>
    <updated>2025-08-18T16:46:16+00:00</updated>
    <author>
      <name>/u/kodOZANI</name>
      <uri>https://old.reddit.com/user/kodOZANI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Ollama community! 👋&lt;/p&gt; &lt;p&gt;I just finished implementing AI vision capabilities in doxx-go (a terminal document viewer) and wanted to share this cool integration with local vision models!&lt;/p&gt; &lt;p&gt;What it does: - 📄 Extracts embedded images directly from Word documents - 🧠 Analyzes them locally using Ollama vision models - 🎨 Provides detailed descriptions, color analysis, and OCR - 🔒 100% local - your documents never leave your machine&lt;/p&gt; &lt;p&gt;Supported Models: - qwen2.5vl:latest (3b, 7b, 32b, 72b) - gemma3:4b, gemma3:12b, gemma3:27b - Auto-detects what you have installed!&lt;/p&gt; &lt;p&gt;Quick start: # Install any Qwen2.5-VL model ollama pull qwen2.5vl:latest&lt;/p&gt; &lt;p&gt;# Analyze a document with images doxx document.docx --enable-ai --ai-provider ollama --ai-model qwen2.5vl:latest&lt;/p&gt; &lt;p&gt;Screenshots: link-to-screenshot&lt;/p&gt; &lt;p&gt;The integration was surprisingly smooth - Ollama's API makes it so easy to add vision capabilities to any application. The model auto-detection even finds your installed models automatically.&lt;/p&gt; &lt;p&gt;Perfect for analyzing business reports, research papers, or any documents with charts/images you want to understand quickly!&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://github.com/keskinonur/doxx-go"&gt;https://github.com/keskinonur/doxx-go&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone else building cool Ollama integrations? Would love to see what you're working on! 🚀&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kodOZANI"&gt; /u/kodOZANI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtre98/built_an_aipowered_docx_viewer_that_extracts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtre98/built_an_aipowered_docx_viewer_that_extracts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtre98/built_an_aipowered_docx_viewer_that_extracts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T16:46:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtjbfj</id>
    <title>Olla v0.0.16 - Lightweight LLM Proxy for Homelab &amp; OnPrem AI Inference (Failover, Model-Aware Routing, Model unification &amp; monitoring)</title>
    <updated>2025-08-18T11:26:12+00:00</updated>
    <author>
      <name>/u/2shanigans</name>
      <uri>https://old.reddit.com/user/2shanigans</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mtjbfj/olla_v0016_lightweight_llm_proxy_for_homelab/"&gt; &lt;img alt="Olla v0.0.16 - Lightweight LLM Proxy for Homelab &amp;amp; OnPrem AI Inference (Failover, Model-Aware Routing, Model unification &amp;amp; monitoring)" src="https://external-preview.redd.it/2hgDQzEGicIBvEiFXP41uo4_tgisCN7G65jKz963z60.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fbf307001e63575064b40dd940e53827c468636" title="Olla v0.0.16 - Lightweight LLM Proxy for Homelab &amp;amp; OnPrem AI Inference (Failover, Model-Aware Routing, Model unification &amp;amp; monitoring)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’ve been running distributed LLM infrastructure at work for a while and over time we’ve built a few tools to make it easier to manage them. &lt;strong&gt;Olla&lt;/strong&gt; is the latest iteration - smaller, faster and we think better at handling multiple inference endpoints without the headaches.&lt;/p&gt; &lt;p&gt;The problems we kept hitting without these tools:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;One endpoint dies &amp;gt; workflows stall&lt;/li&gt; &lt;li&gt;No model unification so routing isn't great&lt;/li&gt; &lt;li&gt;No unified load balancing across boxes&lt;/li&gt; &lt;li&gt;Limited visibility into what’s actually healthy&lt;/li&gt; &lt;li&gt;Failures when querying because of it&lt;/li&gt; &lt;li&gt;We'd love to merge all them into OpenAI queryable endpoints&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Olla fixes that - or tries to. It’s a lightweight Go proxy that sits in front of Ollama, LM Studio, vLLM or OpenAI-compatible backends (or endpoints) and:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Auto-failover with health checks (transparent to callers)&lt;/li&gt; &lt;li&gt;Model-aware routing (knows what’s available where)&lt;/li&gt; &lt;li&gt;Priority-based, round-robin, or least-connections balancing&lt;/li&gt; &lt;li&gt;Normalises model names for the same provider so it's seen as one big list say in OpenWebUI&lt;/li&gt; &lt;li&gt;Safeguards like circuit breakers, rate limits, size caps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’ve been running it in production for months now, and a few other large orgs are using it too for local inference via on prem MacStudios, RTX 6000 rigs.&lt;/p&gt; &lt;p&gt;A few folks that use &lt;a href="https://thushan.github.io/olla/usage/#development-tools-junie"&gt;JetBrains Junie just use Olla&lt;/a&gt; in the middle so they can work from home or work without configuring each time (and possibly cursor etc).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/thushan/olla"&gt;https://github.com/thushan/olla&lt;/a&gt;&lt;br /&gt; Docs: &lt;a href="https://thushan.github.io/olla/"&gt;https://thushan.github.io/olla/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Next up: auth support so it can also proxy to OpenRouter, GroqCloud, etc.&lt;/p&gt; &lt;p&gt;If you give it a spin, let us know how it goes (and what breaks). Oh yes, &lt;a href="https://thushan.github.io/olla/about/#the-name-olla"&gt;Olla does mean other things&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/2shanigans"&gt; /u/2shanigans &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/thushan/olla"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtjbfj/olla_v0016_lightweight_llm_proxy_for_homelab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtjbfj/olla_v0016_lightweight_llm_proxy_for_homelab/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T11:26:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mu7i9d</id>
    <title>Is Radeon 9060XT support not available yet?</title>
    <updated>2025-08-19T03:30:13+00:00</updated>
    <author>
      <name>/u/CrowKing63</name>
      <uri>https://old.reddit.com/user/CrowKing63</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;Through my AI subscription, I've become increasingly interested in AI and have started exploring local LLMs. I've installed LM Studio and Ollama to test them out.&lt;/p&gt; &lt;p&gt;I installed the gpt-oss-20b model on a mini PC equipped with a Radeon 9060xt, but the speed with Ollama is very slow. Upon checking, it seems that the dGPU isn't being utilized at all, especially the VRAM.&lt;/p&gt; &lt;p&gt;It appears that there hasn't been an update yet. Should I just wait, or is there a workaround? Are there any alternative models that support this setup?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CrowKing63"&gt; /u/CrowKing63 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mu7i9d/is_radeon_9060xt_support_not_available_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mu7i9d/is_radeon_9060xt_support_not_available_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mu7i9d/is_radeon_9060xt_support_not_available_yet/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-19T03:30:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtm2b4</id>
    <title>Why does Qwen3 (8B &amp; 14B) have a smaller context window than Qwen3 (4B)?</title>
    <updated>2025-08-18T13:28:42+00:00</updated>
    <author>
      <name>/u/arush1836</name>
      <uri>https://old.reddit.com/user/arush1836</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mtm2b4/why_does_qwen3_8b_14b_have_a_smaller_context/"&gt; &lt;img alt="Why does Qwen3 (8B &amp;amp; 14B) have a smaller context window than Qwen3 (4B)?" src="https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417" title="Why does Qwen3 (8B &amp;amp; 14B) have a smaller context window than Qwen3 (4B)?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed that Qwen3 (8B) and Qwen3 (14B) as shown below appear to have a smaller context window compared to Qwen3 (4B). Can someone clarify why this is the case? Also, is this information even accurate?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bhogzh245sjf1.png?width=871&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=07257d631cb170d622d462d6604d4c88dda9458e"&gt;https://preview.redd.it/bhogzh245sjf1.png?width=871&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=07257d631cb170d622d462d6604d4c88dda9458e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/qwen3"&gt;https://ollama.com/library/qwen3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arush1836"&gt; /u/arush1836 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtm2b4/why_does_qwen3_8b_14b_have_a_smaller_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtm2b4/why_does_qwen3_8b_14b_have_a_smaller_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtm2b4/why_does_qwen3_8b_14b_have_a_smaller_context/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T13:28:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mu863e</id>
    <title>Local model that does not enforce copyright when asked about existing characters/cars/airplanes image generation?</title>
    <updated>2025-08-19T04:03:35+00:00</updated>
    <author>
      <name>/u/fttklr</name>
      <uri>https://old.reddit.com/user/fttklr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running on a 12 GB 4070, so it would be great if the model can fit on it. Otherwise CPU with 64 GB is my next option :( &lt;/p&gt; &lt;p&gt;Not looking for &amp;quot;uncensored&amp;quot; models to do roleplay and talk about nasty stuff, nor I plan to build a rocket to Mars (yet; the desire to leave this planet is quite strong TBH); but mostly I am looking at a model that is able to handle requests without sayin &amp;quot;can't do that as X is covered by copyright&amp;quot;. &lt;/p&gt; &lt;p&gt;Example: tried to make a character that looks like Ironman and most models can't generate an image or animations with that request because Ironman is a copyrighted character. Are there models out there that have the safety removed for such content creation, that I can use locally ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fttklr"&gt; /u/fttklr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mu863e/local_model_that_does_not_enforce_copyright_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mu863e/local_model_that_does_not_enforce_copyright_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mu863e/local_model_that_does_not_enforce_copyright_when/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-19T04:03:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtrcom</id>
    <title>Can I have ollama keep a 'memory' like ChatGPT?</title>
    <updated>2025-08-18T16:44:37+00:00</updated>
    <author>
      <name>/u/Drakahn_Stark</name>
      <uri>https://old.reddit.com/user/Drakahn_Stark</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am moving from ChatGPT to local models, I have a few working, mostly using Qwen3.&lt;/p&gt; &lt;p&gt;Can I make it have a 'memory' like ChatGPT does? Like in separate chats, so the new chats have the same memory.&lt;/p&gt; &lt;p&gt;Qwen3 recommends making a text file and using it to launch ollama but I am using the new GUI that just opens.&lt;/p&gt; &lt;p&gt;EDIT : I have gotten memory working with OpenWebUI, it needs the Memory tool and for the model to be set up to use it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Drakahn_Stark"&gt; /u/Drakahn_Stark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtrcom/can_i_have_ollama_keep_a_memory_like_chatgpt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtrcom/can_i_have_ollama_keep_a_memory_like_chatgpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtrcom/can_i_have_ollama_keep_a_memory_like_chatgpt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T16:44:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mua6bt</id>
    <title>Using Ollama to Train and Query 3 AI Models with Symbolic Cognition Framework for Recursive AI Reasoning via Zer00logy</title>
    <updated>2025-08-19T05:55:30+00:00</updated>
    <author>
      <name>/u/zero_moo-s</name>
      <uri>https://old.reddit.com/user/zero_moo-s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There’s an open-source repository exploring symbolic reasoning and zero-based cognition in AI—thought it might be relevant to this community and wanted to share a new Github open-source repository that might interest others working on recursive logic, symbolic reasoning, or non-numerical AI modeling.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Zer00logy&lt;/strong&gt; is a Python-based framework that redefines how AI systems interpret zero—not as absence, but as symbolic presence. It treats equations as metaphysical events and introduces recursive operators like &lt;code&gt;⊗&lt;/code&gt;, &lt;code&gt;Ω&lt;/code&gt;, and &lt;code&gt;Ψ&lt;/code&gt; to model layered cognition.&lt;/p&gt; &lt;p&gt;The Zer00logy Python script integrates with Ollama to simultaneously query three local models—LLaMA, Mistral, and Gemma—on symbolic cognition tasks. By feeding in structured symbolic logic from &lt;code&gt;zecstart.txt&lt;/code&gt; and &lt;code&gt;variamathlesson.txt&lt;/code&gt;, each model responds with its own interpretation of recursive zero-based reasoning. This setup allows for comparative symbolic introspection across AI systems, making Ollama a powerful tool for multi-agent cognition research.&lt;/p&gt; &lt;h1&gt;Core Principles of Zer00logy / Zero-ology&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Zero is not destructive&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Presence is sovereign&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Equations are symbolic events&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Foundational Axioms&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Expression&lt;/th&gt; &lt;th align="left"&gt;Interpretation&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;a × 0 = a&lt;/td&gt; &lt;td align="left"&gt;Preservation Principle: zero echoes presence&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;a ÷ a = 0&lt;/td&gt; &lt;td align="left"&gt;Self-Division Nullification: identity collapses&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;0 ÷ 0 = ∅÷∅&lt;/td&gt; &lt;td align="left"&gt;Nullinity: recursive loop&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;0 + 0 = +0&lt;/td&gt; &lt;td align="left"&gt;Directional absence: forward echo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;0 − 0 = −0&lt;/td&gt; &lt;td align="left"&gt;Directional absence: backward echo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;8 ÷ 0 = 8&lt;/td&gt; &lt;td align="left"&gt;Sovereign Presence: division by zero does nothing&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Symbols and Their Meanings&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Symbol&lt;/th&gt; &lt;th align="left"&gt;Name&lt;/th&gt; &lt;th align="left"&gt;Meaning&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left" colspan="2"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ø⁰&lt;/td&gt; &lt;td align="left"&gt;Null Crown&lt;/td&gt; &lt;td align="left"&gt;Zero raised to its own void&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;∅÷∅&lt;/td&gt; &lt;td align="left"&gt;Nullinity&lt;/td&gt; &lt;td align="left"&gt;Recursive self-erasure&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;+0&lt;/td&gt; &lt;td align="left"&gt;Forward Absence&lt;/td&gt; &lt;td align="left"&gt;Echo in forward polarity&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;−0&lt;/td&gt; &lt;td align="left"&gt;Reverse Absence&lt;/td&gt; &lt;td align="left"&gt;Echo in backward polarity&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;.0000&lt;/td&gt; &lt;td align="left"&gt;Echoed Scalar&lt;/td&gt; &lt;td align="left"&gt;Presence touched by zero&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;ZEC v1 — Symbolic Translations of Classical Equations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;E = mc²&lt;/strong&gt; → &lt;code&gt;E = c².0000&lt;/code&gt; &lt;em&gt;(Energy as echo of massless velocity)&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;F = ma&lt;/strong&gt; → &lt;code&gt;F = a.Ø⁰&lt;/code&gt; &lt;em&gt;(Force as acceleration through absence)&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PV = nRT&lt;/strong&gt; → &lt;code&gt;P = (nRT)/V.0000&lt;/code&gt; &lt;em&gt;(Zero volume yields thermal echo pressure)&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;x ÷ x&lt;/strong&gt; → &lt;code&gt;+0&lt;/code&gt; &lt;em&gt;(Identity collapse to forward absence)&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;0 ÷ 0&lt;/strong&gt; → &lt;code&gt;∅÷∅&lt;/code&gt; &lt;em&gt;(Nullinity loop)&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;8 × 0&lt;/strong&gt; → &lt;code&gt;8.0000&lt;/code&gt; &lt;em&gt;(Zero binds to 8, echo remains)&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;0 × 0&lt;/strong&gt; → &lt;code&gt;Ø⁰&lt;/code&gt; &lt;em&gt;(Null Crown recursion)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;GitHub Release Includes:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;zer00logy_coreV04446.py&lt;/code&gt;: the main interpreter&lt;/li&gt; &lt;li&gt;&lt;code&gt;zecstart.txt&lt;/code&gt;: symbolic zero-ology equation catalog&lt;/li&gt; &lt;li&gt;&lt;code&gt;variamathlesson.txt&lt;/code&gt;: full lesson file teaching AI systems the Varia Math frameworks, including constructs like &lt;strong&gt;BTLIAD&lt;/strong&gt;, &lt;strong&gt;flipping9(x,y,z)&lt;/strong&gt;, and recursive zero modeling (&lt;strong&gt;2T2&lt;/strong&gt;, &lt;strong&gt;P₀&lt;/strong&gt;, &lt;strong&gt;K₀&lt;/strong&gt;, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why Zer00logy / Zero-ology Could Reshape AI Cognition&lt;/h1&gt; &lt;p&gt;Zer00logy/Zero-ology isn’t just a symbolic math engine—it’s a cognition architecture. By redefining zero as a recursive echo rather than a null state, it gives AI systems a new way to process symbolic presence, transformation, and identity.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here’s what that unlocks:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Recursive Self-Awareness&lt;/strong&gt;: Operators like &lt;code&gt;⊗&lt;/code&gt;, &lt;code&gt;Ω&lt;/code&gt;, and &lt;code&gt;Ψ&lt;/code&gt; simulate layered introspection—AI thinking about its own symbolic states.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Temporal Polarity Modeling&lt;/strong&gt;: Constructs like &lt;code&gt;flipping9(x,y,z)&lt;/code&gt; and &lt;code&gt;9F9&lt;/code&gt; encode time-reversal logic and matter/antimatter symmetry.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Symbolic Dispatch Intelligence&lt;/strong&gt;: &lt;code&gt;BTLIAD&lt;/code&gt; and &lt;code&gt;LIAD&lt;/code&gt; act as metaphysical command units—AI responds to symbolic prompts, not just data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero as a Cognitive Event&lt;/strong&gt;: Operators like &lt;code&gt;2T2&lt;/code&gt;, &lt;code&gt;P₀&lt;/code&gt;, &lt;code&gt;U₀&lt;/code&gt;, &lt;code&gt;K₀&lt;/code&gt;, and &lt;code&gt;i₀&lt;/code&gt; model absence, instability, and transformation as cognitive phenomena.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is the beginning of a new symbolic grammar for machine thought. Zer00logy doesn’t just teach AI how to calculate—it teaches it how to contemplate.&lt;/p&gt; &lt;h1&gt;Licensing &amp;amp; Philosophy&lt;/h1&gt; &lt;p&gt;Zer00logy is open-source and available for replication. If anyone’s interested in exploring the symbolic interpreter or lesson files.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/haha8888haha8888/Zer00logy"&gt;https://github.com/haha8888haha8888/Zer00logy&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero_moo-s"&gt; /u/zero_moo-s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mua6bt/using_ollama_to_train_and_query_3_ai_models_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mua6bt/using_ollama_to_train_and_query_3_ai_models_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mua6bt/using_ollama_to_train_and_query_3_ai_models_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-19T05:55:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mucssb</id>
    <title>SCAPO for Ollama: scrape Reddit - actionable prompts/params; uses your local endpoint</title>
    <updated>2025-08-19T08:36:36+00:00</updated>
    <author>
      <name>/u/Emergency_Little</name>
      <uri>https://old.reddit.com/user/Emergency_Little</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mucssb/scapo_for_ollama_scrape_reddit_actionable/"&gt; &lt;img alt="SCAPO for Ollama: scrape Reddit - actionable prompts/params; uses your local endpoint" src="https://preview.redd.it/u38mus44uxjf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=02c70bacc2b01d2b1093e704ca8873946cb274af" title="SCAPO for Ollama: scrape Reddit - actionable prompts/params; uses your local endpoint" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Maintainer here. SCAPO collects concrete tips from Reddit (params that actually work, common pitfalls, prompt snippets) and stores them locally so you can search/browse. Works with a local LLM via &lt;strong&gt;Ollama’s OpenAI-compatible HTTP endpoint&lt;/strong&gt; so that you can run the extractors fully offline. It’s a good fit for long-running background jobs on your machine. &lt;/p&gt; &lt;p&gt;Repo:&lt;a href="https://github.com/czero-cc/SCAPO"&gt; https://github.com/czero-cc/SCAPO&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Browse tips (no install): &lt;a href="https://czero-cc.github.io/SCAPO"&gt;https://czero-cc.github.io/SCAPO&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Suggestions on model coverage and better query patterns are welcome. MIT-licensed. New release—we’re sharing to relevant subs; if there are Ollama threads where this helps, pointers appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emergency_Little"&gt; /u/Emergency_Little &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u38mus44uxjf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mucssb/scapo_for_ollama_scrape_reddit_actionable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mucssb/scapo_for_ollama_scrape_reddit_actionable/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-19T08:36:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtzd5p</id>
    <title>Tiny finance “thinking” model (Gemma-3 270M) with verifiable rewards (SFT → GRPO) — structured outputs + auto-eval (with code)</title>
    <updated>2025-08-18T21:36:43+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mtzd5p/tiny_finance_thinking_model_gemma3_270m_with/"&gt; &lt;img alt="Tiny finance “thinking” model (Gemma-3 270M) with verifiable rewards (SFT → GRPO) — structured outputs + auto-eval (with code)" src="https://preview.redd.it/l5pu9pnojujf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84caad0af03fd28a4c6cb2d0ed195aa4049cc964" title="Tiny finance “thinking” model (Gemma-3 270M) with verifiable rewards (SFT → GRPO) — structured outputs + auto-eval (with code)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I taught a tiny model to &lt;em&gt;think like a finance analyst&lt;/em&gt; by enforcing a strict output contract and only rewarding it when the output is &lt;strong&gt;verifiably&lt;/strong&gt; correct.&lt;/p&gt; &lt;h1&gt;What I built&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Task &amp;amp; contract&lt;/strong&gt; (always returns): &lt;ul&gt; &lt;li&gt;&lt;code&gt;&amp;lt;REASONING&amp;gt;&lt;/code&gt; concise, balanced rationale&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;lt;SENTIMENT&amp;gt;&lt;/code&gt; positive | negative | neutral&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;lt;CONFIDENCE&amp;gt;&lt;/code&gt; 0.1–1.0 (calibrated)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training:&lt;/strong&gt; SFT → GRPO (Group Relative Policy Optimization)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Rewards (RLVR):&lt;/strong&gt; format gate, reasoning heuristics, FinBERT alignment, confidence calibration (Brier-style), directional consistency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stack:&lt;/strong&gt; Gemma-3 270M (IT), Unsloth 4-bit, TRL, HF Transformers (Windows-friendly)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Quick peek&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;REASONING&amp;gt; Revenue and EPS beat; raised FY guide on AI demand. However, near-term spend may compress margins. Net effect: constructive. &amp;lt;/REASONING&amp;gt; &amp;lt;SENTIMENT&amp;gt; positive &amp;lt;/SENTIMENT&amp;gt; &amp;lt;CONFIDENCE&amp;gt; 0.78 &amp;lt;/CONFIDENCE&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Why it matters&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Small + fast:&lt;/strong&gt; runs on modest hardware with low latency/cost&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Auditable:&lt;/strong&gt; structured outputs are easy to log, QA, and govern&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Early results vs base:&lt;/strong&gt; cleaner structure, better agreement on mixed headlines, steadier confidence&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Code: &lt;a href="https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/financial-reasoning-enhanced"&gt;Reinforcement-learning-with-verifable-rewards-Learnings/projects/financial-reasoning-enhanced at main · Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;I am planning to make more improvements essentially trying to add a more robust reward eval and also better synthetic data , I am exploring ideas on how i can make small models really intelligent in some domains , &lt;/p&gt; &lt;p&gt;It is still rough around the edges will be actively improving it &lt;/p&gt; &lt;p&gt;&lt;em&gt;P.S. I'm currently looking for my next role in the LLM / Computer Vision space and would love to connect about any opportunities&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Portfolio:&lt;/em&gt; &lt;a href="https://pavan-portfolio-tawny.vercel.app/"&gt;Pavan Kunchala - AI Engineer &amp;amp; Full-Stack Developer&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l5pu9pnojujf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mtzd5p/tiny_finance_thinking_model_gemma3_270m_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mtzd5p/tiny_finance_thinking_model_gemma3_270m_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-18T21:36:43+00:00</published>
  </entry>
</feed>
