<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-04-09T23:48:52+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ju6izn</id>
    <title>Experience with mistral-small3.1:24b-instruct-2503-q4_K_M</title>
    <updated>2025-04-08T05:52:02+00:00</updated>
    <author>
      <name>/u/Impossible_Art9151</name>
      <uri>https://old.reddit.com/user/Impossible_Art9151</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running in my usecase models in the 32b up to 90b class.&lt;br /&gt; Mostly qwen, llama, deepseek, aya..&lt;br /&gt; The brandnew mistral can compete here. I tested it over a day.&lt;br /&gt; The size/quality ratio is excellent.&lt;br /&gt; And it is - of course - extremly fast.&lt;br /&gt; Thanx for the release!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impossible_Art9151"&gt; /u/Impossible_Art9151 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju6izn/experience_with_mistralsmall3124binstruct2503q4_k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ju6izn/experience_with_mistralsmall3124binstruct2503q4_k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ju6izn/experience_with_mistralsmall3124binstruct2503q4_k/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T05:52:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jum9ks</id>
    <title>üïØÔ∏è Candle Test Arena: A Tool for Evaluating LLM Reasoning (Now on Hugging Face!)</title>
    <updated>2025-04-08T19:29:14+00:00</updated>
    <author>
      <name>/u/hansklepitko</name>
      <uri>https://old.reddit.com/user/hansklepitko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jum9ks/candle_test_arena_a_tool_for_evaluating_llm/"&gt; &lt;img alt="üïØÔ∏è Candle Test Arena: A Tool for Evaluating LLM Reasoning (Now on Hugging Face!)" src="https://preview.redd.it/sjqbt4t47kte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db4b3183b1075f487f888a2dc704fb844b1876ad" title="üïØÔ∏è Candle Test Arena: A Tool for Evaluating LLM Reasoning (Now on Hugging Face!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hansklepitko"&gt; /u/hansklepitko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sjqbt4t47kte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jum9ks/candle_test_arena_a_tool_for_evaluating_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jum9ks/candle_test_arena_a_tool_for_evaluating_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T19:29:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1junnz0</id>
    <title>Try Observer AI Agents Instantly (Before the Full Ollama Setup!) - Easier Demo Experience!</title>
    <updated>2025-04-08T20:27:17+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Ollama community!&lt;/p&gt; &lt;p&gt;I'm the solo dev behind Observer AI, the open-source project for building local AI agents that can see your screen and react, powered by LLMs with Ollama.&lt;/p&gt; &lt;p&gt;People have told me that setting up local inference has been a bit of a hurdle just to try Observer. So, I spent the last week focused on making it way easier to get a feel for Observer AI &lt;strong&gt;before&lt;/strong&gt; you commit to the full local install.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's New:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I've completely rebuilt the free Ob-Server demo service at &lt;a href="https://app.observer-ai.com"&gt;https://app.observer-ai.com&lt;/a&gt; !&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Instant Try-Out:&lt;/strong&gt; Experience the core agent creation flow without any local setup needed. (Uses cloud models for the demo only, but shows you the ropes!)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;More Models:&lt;/strong&gt; Added 11 different models (including multimodal) you can test with directly in the demo.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smoother UI:&lt;/strong&gt; Refined the interface based on initial feedback.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why This Matters for Ollama Users:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This lets you instantly play around with creating agents that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Observe screen content.&lt;/li&gt; &lt;li&gt;Process info using LLMs (see how different models respond).&lt;/li&gt; &lt;li&gt;Get a feel for the potential before hooking it up to your own powerful Observer-Ollama instance locally for full screen observation and privacy.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;See What's Possible (Examples from Local Setup):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Even simple agents running locally are surprisingly useful! Things like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Activity Tracking Agent:&lt;/strong&gt; Keeps a simple log of what you're working on.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;German Flashcard Agent:&lt;/strong&gt; Spots relevant vocabulary you use in your day to day life and does German-English flashcards to learn them.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The demo helps you visualize building these before setting up ObserverOllama locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Looking for Feedback &amp;amp; Ideas:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Give the revamped demo a quick spin at &lt;a href="https://app.observer-ai.com"&gt;https://app.observer-ai.com&lt;/a&gt; !&lt;/li&gt; &lt;li&gt;How's the UX for creating a simple agent in the demo? Is it intuitive?&lt;/li&gt; &lt;li&gt;What other &lt;strong&gt;simple but useful&lt;/strong&gt; agents (like the examples above) could you imagine building once connected to your local Ollama? Need ideas!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Join the Community:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We also just started a Discord server to share agent ideas, get help, and chat about local AI: &lt;a href="https://discord.gg/k4ruE6WG"&gt;https://discord.gg/k4ruE6WG&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Observer AI remains 100% FOSS and is designed to run fully locally with Ollama (any v1/chat/completions service comming soon!) for maximum privacy and control. Check out the code at &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Thanks for checking it out and for all the great feedback so far! Let me know what you think of the easier demo experience!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1junnz0/try_observer_ai_agents_instantly_before_the_full/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1junnz0/try_observer_ai_agents_instantly_before_the_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1junnz0/try_observer_ai_agents_instantly_before_the_full/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T20:27:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jukoca</id>
    <title>Local LLM MCP, what is your preferred model?</title>
    <updated>2025-04-08T18:23:49+00:00</updated>
    <author>
      <name>/u/Representative-Park6</name>
      <uri>https://old.reddit.com/user/Representative-Park6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are working on some internal tooling at work that would bennefit greatly from moving away from individual standard function calling to a MCP server approach, so I have been toying around with MCP servers over the past few weeks. &lt;/p&gt; &lt;p&gt;From my testing setup where I have a rtx3080 I do find &lt;strong&gt;llama3.2&lt;/strong&gt; waaaay too weak, and &lt;strong&gt;qwq&lt;/strong&gt; a bit too slow. Enabeling function calling on &lt;strong&gt;Gemma3(12b)&lt;/strong&gt; is surprisingly fast and quite strong for most tasks. (Tho requires a bit of schafolding and context loss for doing function lookups. But its clearly the best i have found sofar.)&lt;/p&gt; &lt;p&gt;So im pretty happy with &lt;strong&gt;Gemma3&lt;/strong&gt; for my needs, but would love to have an option to turn up the dial a bit as a fallback mechanism if it fails. &lt;/p&gt; &lt;p&gt;So my question is, are there anything between &lt;strong&gt;Gemma3&lt;/strong&gt; and &lt;strong&gt;qwq&lt;/strong&gt; that are worth exploring?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Representative-Park6"&gt; /u/Representative-Park6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jukoca/local_llm_mcp_what_is_your_preferred_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jukoca/local_llm_mcp_what_is_your_preferred_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jukoca/local_llm_mcp_what_is_your_preferred_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T18:23:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jurfbj</id>
    <title>I‚Äôm only trying to help here Hermes3 70 billion parameter model!!</title>
    <updated>2025-04-08T23:12:35+00:00</updated>
    <author>
      <name>/u/AIForOver50Plus</name>
      <uri>https://old.reddit.com/user/AIForOver50Plus</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jurfbj/im_only_trying_to_help_here_hermes3_70_billion/"&gt; &lt;img alt="I‚Äôm only trying to help here Hermes3 70 billion parameter model!!" src="https://preview.redd.it/lfjbmfq61pte1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d1b7bc63f24191d6249a9a52d5f8d8f1baa0eca" title="I‚Äôm only trying to help here Hermes3 70 billion parameter model!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIForOver50Plus"&gt; /u/AIForOver50Plus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lfjbmfq61pte1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jurfbj/im_only_trying_to_help_here_hermes3_70_billion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jurfbj/im_only_trying_to_help_here_hermes3_70_billion/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T23:12:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1juz8pp</id>
    <title>I uploaded Q6 / Q5 quants of Mistral-Small-3.1-24B to ollama</title>
    <updated>2025-04-09T06:28:22+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1juz7o6/i_uploaded_q6_q5_quants_of_mistralsmall3124b_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1juz8pp/i_uploaded_q6_q5_quants_of_mistralsmall3124b_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1juz8pp/i_uploaded_q6_q5_quants_of_mistralsmall3124b_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-09T06:28:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1juqw0i</id>
    <title>Anyone gotten Nemotron 49B Running in Ollama?</title>
    <updated>2025-04-08T22:47:24+00:00</updated>
    <author>
      <name>/u/mj3815</name>
      <uri>https://old.reddit.com/user/mj3815</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have tried both MHKetbi/nvidia_Llama-3.3-Nemotron-Super-49B-v1:q5_K_L and MHKetbi/nvidia_Llama-3.3-Nemotron-Super-49B-v1:q4_K_M on my 2x 3090 system, but Ollama gives me an out of memory error. &lt;/p&gt; &lt;p&gt;I have no trouble running 70B Llama 3.3 q4_k_m which is much larger. &lt;/p&gt; &lt;p&gt;Has anyone successfully run Nemotron 49B and have some advice? TIA&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mj3815"&gt; /u/mj3815 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1juqw0i/anyone_gotten_nemotron_49b_running_in_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1juqw0i/anyone_gotten_nemotron_49b_running_in_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1juqw0i/anyone_gotten_nemotron_49b_running_in_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T22:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv05o3</id>
    <title>Use cases for AI agents</title>
    <updated>2025-04-09T07:36:30+00:00</updated>
    <author>
      <name>/u/Neoyon</name>
      <uri>https://old.reddit.com/user/Neoyon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been thinking about the use case of LLMs, specifically agents and tooling using Semantic Kernel and Ollama. If we can call functions using LLMs, what are some implications or applications we can integrate it with? I have an idea like creating data visualizations while prompting the LLM and accessing an SQL database to return the output with a visualization. But aside from that, what else can we use the agentic workflow for? can you guys guide me, fairly new to this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neoyon"&gt; /u/Neoyon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jv05o3/use_cases_for_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jv05o3/use_cases_for_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jv05o3/use_cases_for_ai_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-09T07:36:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jux2h1</id>
    <title>Morphik now Supports any LLM or Embedding model!</title>
    <updated>2025-04-09T04:06:31+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;My brother and I have been working on &lt;a href="https://github.com/morphik-org/morphik-core"&gt;Morphik&lt;/a&gt; - an open source, end-to-end, research-driven RAG system. We recently migrated our LLM provider to support LiteLLM, and we now support all models that LiteLLM does! &lt;/p&gt; &lt;p&gt;This includes: embedding models, completion models, our &lt;a href="https://docs.morphik.ai/concepts/knowledge-graphs"&gt;GraphRAG systems&lt;/a&gt;, and even our &lt;a href="https://docs.morphik.ai/concepts/rules-processing"&gt;metadata extraction layer&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;Use gemini for knowledge graphs, Openai for embeddings, Claude for completions, and Ollama for extractions. Or any other permutation. All with single-line changes in our configuration file.&lt;/p&gt; &lt;p&gt;Lmk what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jux2h1/morphik_now_supports_any_llm_or_embedding_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jux2h1/morphik_now_supports_any_llm_or_embedding_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jux2h1/morphik_now_supports_any_llm_or_embedding_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-09T04:06:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1juvi2w</id>
    <title>Need 10 early adopters</title>
    <updated>2025-04-09T02:39:12+00:00</updated>
    <author>
      <name>/u/Emotional-Evening-62</name>
      <uri>https://old.reddit.com/user/Emotional-Evening-62</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone ‚Äì I‚Äôm building something called Oblix (&lt;a href="https://oblix.ai/"&gt;https://oblix.ai/&lt;/a&gt;), a new tool for orchestrating AI between edge and cloud. On the edge, it integrates directly with Ollama, and for the cloud, it supports both OpenAI and ClaudeAI. The goal is to help developers create smart, low-latency, privacy-conscious workflows without giving up the power of cloud APIs when needed‚Äîall through a CLI-first experience.&lt;/p&gt; &lt;p&gt;It‚Äôs still early days, and I‚Äôm looking for a few CLI-native, ninja-level developers to try it out, break it, and share honest feedback. If that sounds interesting, drop a or DM me‚Äîwould love to get your thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emotional-Evening-62"&gt; /u/Emotional-Evening-62 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1juvi2w/need_10_early_adopters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1juvi2w/need_10_early_adopters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1juvi2w/need_10_early_adopters/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-09T02:39:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jurg1c</id>
    <title>How to create a chatbot that reads from a large .txt file</title>
    <updated>2025-04-08T23:13:32+00:00</updated>
    <author>
      <name>/u/joshc279</name>
      <uri>https://old.reddit.com/user/joshc279</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! &lt;/p&gt; &lt;p&gt;For my group's capstone project, our task is to develop an offline chatbot that our Universities' Security Office student workers will use to learn more about their entry level role at the office. The outcome we ideally want is the bot to take our txt file (which contains the office's procedural documentation and is about 700k characters) and use that to answer prompted questions. We tried using LM Studio and used AI to help us create Python scripts to link LM studio with the txt document, but we were not able to get it to work. We just want an offline chatbot just like you would create on ChatGPT Plus, but offline. What is the easiest way to do this without using a bunch of scripts/programs/packages, etc. None of us have python experience so when we inevitably run into errors in the code and ChatGPT doesn't know what's going on. Any pointers? Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joshc279"&gt; /u/joshc279 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jurg1c/how_to_create_a_chatbot_that_reads_from_a_large/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jurg1c/how_to_create_a_chatbot_that_reads_from_a_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jurg1c/how_to_create_a_chatbot_that_reads_from_a_large/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T23:13:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv20h2</id>
    <title>ollama error if i have not enough system RAM</title>
    <updated>2025-04-09T09:55:59+00:00</updated>
    <author>
      <name>/u/evofromk0</name>
      <uri>https://old.reddit.com/user/evofromk0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, i have 32GB gpu, testing ollama with gemma 3 27B q8 and getting errors&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Error: model requires more system memory (1.4 GiB) than is available (190.9 MiB)&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Had 1GB of system RAM. ... expanded to 4GB and got this:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Error: Post &amp;quot;&lt;a href="http://127.0.0.1:11434/api/generate%22:"&gt;http://127.0.0.1:11434/api/generate&amp;quot;:&lt;/a&gt; EOF&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Expanded to 5+ GB of system RAM - started fine.&lt;/p&gt; &lt;p&gt;Question - why does it needs my system ram RAM when i see model is loaded to gpu VRAM ( 27 GB )&lt;/p&gt; &lt;p&gt;Have not changed context size , nothing ... or its due to gemma 3 is automatically takes context size to its set preferences of &lt;strong&gt;27B parameter model&lt;/strong&gt; (128k context window) ?&lt;/p&gt; &lt;p&gt;P.s. running inside terminal. not web gui.&lt;/p&gt; &lt;p&gt;Thank You.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/evofromk0"&gt; /u/evofromk0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jv20h2/ollama_error_if_i_have_not_enough_system_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jv20h2/ollama_error_if_i_have_not_enough_system_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jv20h2/ollama_error_if_i_have_not_enough_system_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-09T09:55:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv3smc</id>
    <title>Custom Modelfile with LOTS of template</title>
    <updated>2025-04-09T11:47:05+00:00</updated>
    <author>
      <name>/u/gotninjaskills</name>
      <uri>https://old.reddit.com/user/gotninjaskills</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For a small project, is it ok to put a lot of input-output pairs in the template for my custom Modelfile? I know there's a more correct way of customizing or fine tuning models but is this technically OK to do? Will it slow down the processing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gotninjaskills"&gt; /u/gotninjaskills &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jv3smc/custom_modelfile_with_lots_of_template/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jv3smc/custom_modelfile_with_lots_of_template/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jv3smc/custom_modelfile_with_lots_of_template/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-09T11:47:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jumfkf</id>
    <title>Best small models for survival situations?</title>
    <updated>2025-04-08T19:36:21+00:00</updated>
    <author>
      <name>/u/Mr-Barack-Obama</name>
      <uri>https://old.reddit.com/user/Mr-Barack-Obama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are the current smartest models that take up less than 4GB as a guff file?&lt;/p&gt; &lt;p&gt;I'm going camping and won't have internet connection. I can run models under 4GB on my iphone.&lt;/p&gt; &lt;p&gt;It's so hard to keep track of what models are the smartest because I can't find good updated benchmarks for small open-source models.&lt;/p&gt; &lt;p&gt;I'd like the model to be able to help with any questions I might possibly want to ask during a camping trip. It would be cool if the model could help in a survival situation or just answer random questions.&lt;/p&gt; &lt;p&gt;(I have power banks and solar panels lol.)&lt;/p&gt; &lt;p&gt;I'm thinking maybe gemma 3 4B, but i'd like to have multiple models to cross check answers.&lt;/p&gt; &lt;p&gt;I think I could maybe get a quant of a 9B model small enough to work.&lt;/p&gt; &lt;p&gt;Let me know if you find some other models that would be good!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr-Barack-Obama"&gt; /u/Mr-Barack-Obama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jumfkf/best_small_models_for_survival_situations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jumfkf/best_small_models_for_survival_situations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jumfkf/best_small_models_for_survival_situations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T19:36:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jup1tl</id>
    <title>1. Record work activity 2. Replace by AI: OSS lib to stream user desktop activity to LLM</title>
    <updated>2025-04-08T21:25:37+00:00</updated>
    <author>
      <name>/u/louis3195</name>
      <uri>https://old.reddit.com/user/louis3195</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jup1tl/1_record_work_activity_2_replace_by_ai_oss_lib_to/"&gt; &lt;img alt="1. Record work activity 2. Replace by AI: OSS lib to stream user desktop activity to LLM" src="https://external-preview.redd.it/ZXU0dDJpMmpob3RlMWm5Zr8O-VuVgt8_SGpytB7oRxT3LsPZDsOEzTMcg4ik.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81cf45088a7f837c5bf94603eddc70ab85e6c6bf" title="1. Record work activity 2. Replace by AI: OSS lib to stream user desktop activity to LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OSS code:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mediar-ai/ui-events"&gt;https://github.com/mediar-ai/ui-events&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/louis3195"&gt; /u/louis3195 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vdspde2jhote1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jup1tl/1_record_work_activity_2_replace_by_ai_oss_lib_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jup1tl/1_record_work_activity_2_replace_by_ai_oss_lib_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T21:25:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv7qww</id>
    <title>Looking for a syncing TTS model with cloning functionality</title>
    <updated>2025-04-09T14:54:49+00:00</updated>
    <author>
      <name>/u/Dependent-Sport-1128</name>
      <uri>https://old.reddit.com/user/Dependent-Sport-1128</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Simply, I am searching for a TTS cloning model that can replace specific words in an audio file with other words while maintaining the syncing and timing of other words.&lt;/p&gt; &lt;p&gt;For example:&lt;br /&gt; Input: &lt;em&gt;&amp;quot;The forest was&lt;/em&gt; &lt;strong&gt;&lt;em&gt;alive&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;with the sound of chirping birds and rustling leaves.&amp;quot;&lt;/em&gt;&lt;br /&gt; Output: &lt;em&gt;&amp;quot;The forest was&lt;/em&gt; &lt;strong&gt;&lt;em&gt;calm&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;with the sound of chirping birds and rustling leaves.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;As you can see in the previous example, the &amp;quot;alive&amp;quot; word was replaced with the &amp;quot;calm&amp;quot; word.&lt;/p&gt; &lt;p&gt;My goal is for the modified audio should match the original in duration, pacing, and sync, ensuring that unchanged words retain their exact start and end times.&lt;/p&gt; &lt;p&gt;Most TTS and voice cloning tools regenerate full speech, but I need one that precisely aligns with the original. Any recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dependent-Sport-1128"&gt; /u/Dependent-Sport-1128 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jv7qww/looking_for_a_syncing_tts_model_with_cloning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jv7qww/looking_for_a_syncing_tts_model_with_cloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jv7qww/looking_for_a_syncing_tts_model_with_cloning/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-09T14:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1juehkv</id>
    <title>Built my own AI Self ‚Äî runs locally, connects via API, and remembers stuff like I do</title>
    <updated>2025-04-08T14:10:12+00:00</updated>
    <author>
      <name>/u/TopRavenfruit</name>
      <uri>https://old.reddit.com/user/TopRavenfruit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1juehkv/built_my_own_ai_self_runs_locally_connects_via/"&gt; &lt;img alt="Built my own AI Self ‚Äî runs locally, connects via API, and remembers stuff like I do" src="https://preview.redd.it/fhlx5mz8cmte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6b2f235b07c6ee5e20cb496715dcf2f3dd65129f" title="Built my own AI Self ‚Äî runs locally, connects via API, and remembers stuff like I do" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;br /&gt; I'm one of the contributors to &lt;a href="https://github.com/Mindverse/Second-Me"&gt;Second Me&lt;/a&gt;, an open-source, fully local AI project designed for personal memory, reasoning, and identity modeling. Think of it as a customizable ‚ÄúAI self‚Äù ‚Äî trained on your data, aligned with your values, and fully under your control (not OpenAI‚Äôs).We hit &lt;strong&gt;6,000+ stars in 7 days&lt;/strong&gt;, which is wild ‚Äî but what‚Äôs even cooler is what‚Äôs been happening &lt;strong&gt;after&lt;/strong&gt; launch:&lt;/p&gt; &lt;h1&gt;üîß What It Does (tl;dr):&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Personal AI, locally trained and run. 100% privacy with local execution options.&lt;/li&gt; &lt;li&gt;Hierarchical Memory Modeling (HMM) for authentic personalization.&lt;/li&gt; &lt;li&gt;Me-alignment structure tailored to individual values.&lt;/li&gt; &lt;li&gt;Second Me Protocol (SMP) for decentralized AI interactio&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;New in this release:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full Docker support for macOS (Apple Silicon), Windows, and Linux&lt;/li&gt; &lt;li&gt;OpenAI-Compatible API Interface&lt;/li&gt; &lt;li&gt;MLX training support (Beta)&lt;/li&gt; &lt;li&gt;Significant performance enhancements&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üíª Community Contributions&lt;/h1&gt; &lt;p&gt;In just 2 weeks post-launch:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;60+ PRs, 70+ issues&lt;/li&gt; &lt;li&gt;Contributors from Tokyo to Dubai: students, academics, enterprise devs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Some great GitHub PRs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;WeChat bot integration ‚Äî &lt;a href="https://github.com/mindverse/Second-Me/pull/81"&gt;#81&lt;/a&gt; by &lt;a href="https://github.com/Zero-coder"&gt;u/Zero-coder&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Japanese README localization ‚Äî &lt;a href="https://github.com/mindverse/Second-Me/pull/115"&gt;#115&lt;/a&gt; by &lt;a href="https://github.com/eltociear"&gt;@eltociear&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Improved file resource management ‚Äî &lt;a href="https://github.com/mindverse/Second-Me/pull/74"&gt;#74&lt;/a&gt; by &lt;a href="https://github.com/mahdirahimi1999"&gt;@mahdirahimi1999&lt;/a&gt;&lt;/li&gt; &lt;li&gt;File name validation for added security ‚Äî &lt;a href="https://github.com/mindverse/Second-Me/pull/62"&gt;#62&lt;/a&gt; by &lt;a href="https://github.com/umutcrs"&gt;@umutcrs&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks to their and others' feedback, features like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-platform deployment&lt;/li&gt; &lt;li&gt;Note-based continuous training&lt;br /&gt;&lt;/li&gt; &lt;li&gt;‚Ä¶have been added to the roadmap.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also, shoutout to &lt;a href="https://twitter.com/GOROman"&gt;@GOROman&lt;/a&gt; for his &lt;a href="https://qiita.com/Null-Sensei"&gt;full guide&lt;/a&gt; to deploying Second Me - ‚Äî he trained Second Me on 75GB of personal X data since 2007 and inspired new use cases, like @Yuzunose‚Äôs &lt;a href="https://note.com/yuzunose/n/n0ec8b300c10c?sub_rt=share_sb"&gt;VRChat integration idea&lt;/a&gt;.We‚Äôre grateful ‚Äî and excited ‚Äî to see where the community takes it next.&lt;/p&gt; &lt;p&gt;üîó GitHub: &lt;a href="https://github.com/Mindverse/Second-Me"&gt;https://github.com/Mindverse/Second-Me&lt;/a&gt;&lt;br /&gt; üìÑ Paper: &lt;a href="https://arxiv.org/abs/2503.08102"&gt;https://arxiv.org/abs/2503.08102&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üí° The goal is building AI that extends your capabilities while remaining under your control, not corporate systems. If you value digital freedom, we'd appreciate your contributions and feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TopRavenfruit"&gt; /u/TopRavenfruit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fhlx5mz8cmte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1juehkv/built_my_own_ai_self_runs_locally_connects_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1juehkv/built_my_own_ai_self_runs_locally_connects_via/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-08T14:10:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvdo0j</id>
    <title>Simple Ollama Agent Ideas</title>
    <updated>2025-04-09T18:55:37+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! &lt;/p&gt; &lt;p&gt;I've been making little micro-agents that work with small ollama models. Some ideas that i've come across are the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Activity Tracking:&lt;/strong&gt; Just keeps a basic log of apps/docs you're working on.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Day Summary Writer:&lt;/strong&gt; Reads the activity log at EOD and gives you a quick summary.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Focus Assistant:&lt;/strong&gt; Gently nudges you if you seem to be browsing distracting sites.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vocabulary Agent:&lt;/strong&gt; If learning a language, spots words on screen and builds a list with definitions/translations for review.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flashcard Agent:&lt;/strong&gt; Turns those vocabulary words into simple flashcard pairs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Command Tracker:&lt;/strong&gt; Tracks the commands you run in any terminal.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And i have some other ideas for a bit bigger models like: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Process tracker:&lt;/strong&gt; watches for a certain process you do and creates a report with steps to do this process.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code reviewer:&lt;/strong&gt; Sees code on screen and suggests relevant edits or syntax corrections.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code documenter:&lt;/strong&gt; Makes relevant documentation of the code it sees on screen.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The thing is, i've made the simple agents above work but i'm trying to think about more simple ideas that can work with small models (&amp;lt;20B), that are not as ambitious as the last three examples (i've tried to make them work but they do require bigger models and maybe advanced MCP). Can you guys think of any ideas? Thanks :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jvdo0j/simple_ollama_agent_ideas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jvdo0j/simple_ollama_agent_ideas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jvdo0j/simple_ollama_agent_ideas/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-09T18:55:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1juwn9r</id>
    <title>RAG integrated into Chat tool</title>
    <updated>2025-04-09T03:42:11+00:00</updated>
    <author>
      <name>/u/mspamnamem</name>
      <uri>https://old.reddit.com/user/mspamnamem</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working on integrating RAG into my chat tool called pychat. I‚Äôve been very happy with the results and I wanted to share. I think integrating RAG in this way has really been helpful for some of my very specific domain work for my real job.&lt;/p&gt; &lt;p&gt;If you‚Äôre interested, test/download from the rag2 branch on my GitHub repository. The RAG stuff will work with ollama and the other third party services.&lt;/p&gt; &lt;p&gt;It currently only supports PDF and text files. I want to add support for MS word documents next. &lt;/p&gt; &lt;p&gt;Have fun!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Magnetron85/PyChat"&gt;https://github.com/Magnetron85/PyChat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mspamnamem"&gt; /u/mspamnamem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1juwn9r/rag_integrated_into_chat_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1juwn9r/rag_integrated_into_chat_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1juwn9r/rag_integrated_into_chat_tool/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-09T03:42:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvfp3q</id>
    <title>2x mi50 16gb HBM2 - good MB / CPU?</title>
    <updated>2025-04-09T20:19:18+00:00</updated>
    <author>
      <name>/u/atomicpapa210</name>
      <uri>https://old.reddit.com/user/atomicpapa210</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I purchased 2 of the above-mentioned Mi50 cards. What would be a good MB / CPU combo to run these 2 cards? How much RAM? If you were building a budget-friendly system to run LLMs around these 2 cards, how would you do it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atomicpapa210"&gt; /u/atomicpapa210 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jvfp3q/2x_mi50_16gb_hbm2_good_mb_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jvfp3q/2x_mi50_16gb_hbm2_good_mb_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jvfp3q/2x_mi50_16gb_hbm2_good_mb_cpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-09T20:19:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv7w1b</id>
    <title>DeepSeek default session, can't delete it, can't empty it. I just want to start over.</title>
    <updated>2025-04-09T15:00:44+00:00</updated>
    <author>
      <name>/u/D3V10517Y</name>
      <uri>https://old.reddit.com/user/D3V10517Y</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running a local copy of DeepSeek using Ollama. In the Webui, there is a default session. It remembers everything we talked about in that session. When I ask it a new question it answers in context of the whole conversation up to that point. Lesson Learned, make a new session for each unrelated session. But HOW do I purge the contents of the default? I can't delete it, can't rename it, can't create a new default. I don't want to manually delete files and break something. I'd like to go back to a clean slate without going as far as reinstalling. Any ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/D3V10517Y"&gt; /u/D3V10517Y &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jv7w1b/deepseek_default_session_cant_delete_it_cant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jv7w1b/deepseek_default_session_cant_delete_it_cant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jv7w1b/deepseek_default_session_cant_delete_it_cant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-09T15:00:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvjgsf</id>
    <title>We've Been Conned: The truth about Big LLM</title>
    <updated>2025-04-09T23:04:44+00:00</updated>
    <author>
      <name>/u/liquidcoffeee</name>
      <uri>https://old.reddit.com/user/liquidcoffeee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jvjgsf/weve_been_conned_the_truth_about_big_llm/"&gt; &lt;img alt="We've Been Conned: The truth about Big LLM" src="https://external-preview.redd.it/9qt_Whn8e32_fvdVPdI8EUfi4K3pO-uGo0UprFNkoh0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6b917c0261f8e08936dccc1bad7eea4c2ee7eb4a" title="We've Been Conned: The truth about Big LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liquidcoffeee"&gt; /u/liquidcoffeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.dolthub.com/blog/2025-04-09-weve-been-conned-the-truth-about-llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jvjgsf/weve_been_conned_the_truth_about_big_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jvjgsf/weve_been_conned_the_truth_about_big_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-09T23:04:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvbapm</id>
    <title>Hi. I'm new to programming. Can someone tell me which model here is the most powerful model here for deepcoder?</title>
    <updated>2025-04-09T17:20:10+00:00</updated>
    <author>
      <name>/u/Love_of_Mango</name>
      <uri>https://old.reddit.com/user/Love_of_Mango</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jvbapm/hi_im_new_to_programming_can_someone_tell_me/"&gt; &lt;img alt="Hi. I'm new to programming. Can someone tell me which model here is the most powerful model here for deepcoder?" src="https://preview.redd.it/irzdhpvteute1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ec6cefd630daa8469a49da5200e7b223a0d9871d" title="Hi. I'm new to programming. Can someone tell me which model here is the most powerful model here for deepcoder?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are multiple models. The &amp;quot;latest&amp;quot; is 9gb. The 14b is 9gb. But there are others that are 30gb. Can someone let me know which one I need to use that is the latest and the most powerful model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Love_of_Mango"&gt; /u/Love_of_Mango &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/irzdhpvteute1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jvbapm/hi_im_new_to_programming_can_someone_tell_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jvbapm/hi_im_new_to_programming_can_someone_tell_me/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-09T17:20:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvfb7r</id>
    <title>Can i run Ollama on a macbook air m2 (16gb ram)?</title>
    <updated>2025-04-09T20:03:19+00:00</updated>
    <author>
      <name>/u/Raxious</name>
      <uri>https://old.reddit.com/user/Raxious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi all, so I've been looking around into maybe trying to get a local llm running on my macbook air M2 with 16gb of ram. I tried looking around but couldn't find any clear proper answer as to whether it's doable or if it's something not recommended at all. Right now, I typically just head into either Copilot or ChatGPT just for brainstorming ideas, help me with lesson materials or create coding exercises for myself. (C# and basic web development)&lt;/p&gt; &lt;p&gt;Creating images would be a fun little extra, but something that is absolutely not a requirement, especially with my hardware.&lt;/p&gt; &lt;p&gt;Would my macbook be able to run any llm comfortably and if so, what would be a good recommendation. Please keep in mind that I can't run Deepseek cause it's my device from work and they're a bit iffy about Deepseek xD&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Raxious"&gt; /u/Raxious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jvfb7r/can_i_run_ollama_on_a_macbook_air_m2_16gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jvfb7r/can_i_run_ollama_on_a_macbook_air_m2_16gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jvfb7r/can_i_run_ollama_on_a_macbook_air_m2_16gb_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-09T20:03:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv4q8o</id>
    <title>Framework 16 RISCV 128GB RAM 100 TOPS</title>
    <updated>2025-04-09T12:37:05+00:00</updated>
    <author>
      <name>/u/grigio</name>
      <uri>https://old.reddit.com/user/grigio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jv4q8o/framework_16_riscv_128gb_ram_100_tops/"&gt; &lt;img alt="Framework 16 RISCV 128GB RAM 100 TOPS" src="https://preview.redd.it/biofggij0tte1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62bc0a505bc04035c00ba13e852fd99f7a8ee4c2" title="Framework 16 RISCV 128GB RAM 100 TOPS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you think? Will it be faster than Nvidia digits or Mac Studio?&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://m.youtube.com/watch?v=-sxdvDbvJFM"&gt;https://m.youtube.com/watch?v=-sxdvDbvJFM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grigio"&gt; /u/grigio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/biofggij0tte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jv4q8o/framework_16_riscv_128gb_ram_100_tops/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jv4q8o/framework_16_riscv_128gb_ram_100_tops/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-09T12:37:05+00:00</published>
  </entry>
</feed>
