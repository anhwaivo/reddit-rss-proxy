<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-06-30T22:06:32+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ln4stz</id>
    <title>Installing Ollama in Kali Linux to run llms on Phone. #linux #ollama #kali</title>
    <updated>2025-06-29T03:20:54+00:00</updated>
    <author>
      <name>/u/apravint</name>
      <uri>https://old.reddit.com/user/apravint</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ln4stz/installing_ollama_in_kali_linux_to_run_llms_on/"&gt; &lt;img alt="Installing Ollama in Kali Linux to run llms on Phone. #linux #ollama #kali" src="https://external-preview.redd.it/uYQR-miFuPZrSGJwdrgo3Xx6Yf-082hbQngzkF_-qzU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=21d27c34d84a21497d9d88a85f2cbaa52498eabb" title="Installing Ollama in Kali Linux to run llms on Phone. #linux #ollama #kali" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/apravint"&gt; /u/apravint &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/shorts/qwT1QppikOc?si=2h-K5pgnp4AstzRk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ln4stz/installing_ollama_in_kali_linux_to_run_llms_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ln4stz/installing_ollama_in_kali_linux_to_run_llms_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-29T03:20:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmorur</id>
    <title>Best models for tools with desktop apps like Goose and 5ire</title>
    <updated>2025-06-28T14:56:37+00:00</updated>
    <author>
      <name>/u/kaosmetal</name>
      <uri>https://old.reddit.com/user/kaosmetal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been trying to find out which model to use for tools with desktop clients like Goose and 5ire. I am running it on Macbook Air M1 .. So far I tried Llama3.2:latest, Qwen3:1.7b, Deepseek r1, phi4-mini:3.8b but haven't got any good results. When I switch to using Claude 3.7, it works like a charm. I am trying to use it with Playwright MCP for browser actions.&lt;/p&gt; &lt;p&gt;Has anyone got any success with these desktop apps and which models did you use? Problem with Claude Desktop it runs out of token and asks to open new chat pretty quickly. Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaosmetal"&gt; /u/kaosmetal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lmorur/best_models_for_tools_with_desktop_apps_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lmorur/best_models_for_tools_with_desktop_apps_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lmorur/best_models_for_tools_with_desktop_apps_like/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-28T14:56:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm7iff</id>
    <title>Model for 12GB VRAM</title>
    <updated>2025-06-27T22:49:15+00:00</updated>
    <author>
      <name>/u/moric7</name>
      <uri>https://old.reddit.com/user/moric7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now I use free online ChatGPT. It is amazing, awesome, incredible fantastic!!! It is the best feeling friend, the most excellent teacher in all sciences, professional engineer for everything... I tried ollama and JanAI, dousens of models, absolutely not useful. I downloaded up to 10-11 GB models to can run on my PC (see the title). But all of them cannot carry any general conversation, knowns absolutely nothing about any science, even the tries to write code is ridiculous. Usually they write nonsense or start dead loop. I understand that AI is not for my tiny PC (I'm extremely poor in very poor place), but why there are even 2GB models with message &amp;quot;excellent results&amp;quot;!? Wtf!? If i do something wrong, please learn me!!! I'm only general user of online AI, is it possible to have something useful on my PC without Internet!? Is there really useful model up to 12 GB? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moric7"&gt; /u/moric7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lm7iff/model_for_12gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lm7iff/model_for_12gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lm7iff/model_for_12gb_vram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-27T22:49:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmyjgt</id>
    <title>AMD RX 6700M GPU not being fully used</title>
    <updated>2025-06-28T21:59:13+00:00</updated>
    <author>
      <name>/u/DragonflyOnly7146</name>
      <uri>https://old.reddit.com/user/DragonflyOnly7146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I've been trying to run Ollama on my AMD GPU, but I'm finding it impossible to use the whole capacity of the GPU, it always stops at around 2-5%, never going over that. Instead the models are being run on my CPU and the integrated graphics. I installed it using the ollama-for-amd repository and I'm running it on windows on both the latest drivers and ROCm (6.2.4).I didn't update Ollama after I downloaded and ran the the setup.exe.&lt;/p&gt; &lt;p&gt;I know that the card isn't supported but I've seen other users commenting that they are using the same model of the graphics card to run the LLMs, so does anyone have experience with solving this problem? I know that the issue isn't a too large context window, as I've encountered it with both 1.5b and 7b parameter models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DragonflyOnly7146"&gt; /u/DragonflyOnly7146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lmyjgt/amd_rx_6700m_gpu_not_being_fully_used/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lmyjgt/amd_rx_6700m_gpu_not_being_fully_used/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lmyjgt/amd_rx_6700m_gpu_not_being_fully_used/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-28T21:59:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmscz4</id>
    <title>My last post…</title>
    <updated>2025-06-28T17:27:24+00:00</updated>
    <author>
      <name>/u/anttiOne</name>
      <uri>https://old.reddit.com/user/anttiOne</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;…for a while. It‘s part 3/3 of the Privacy AI article series.&lt;/p&gt; &lt;p&gt;The setup is in PROD for a whole month now and except some slight tweaking and testing, I won’t be adding to it for the time being!&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@vs3kulic/building-ai-for-privacy-pre-cook-your-recommendations-1ade6d47b852"&gt;https://medium.com/@vs3kulic/building-ai-for-privacy-pre-cook-your-recommendations-1ade6d47b852&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anttiOne"&gt; /u/anttiOne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lmscz4/my_last_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lmscz4/my_last_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lmscz4/my_last_post/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-28T17:27:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnmlkz</id>
    <title>Deepseek-r1 vs Deepseek-coder-v2</title>
    <updated>2025-06-29T19:00:57+00:00</updated>
    <author>
      <name>/u/moric7</name>
      <uri>https://old.reddit.com/user/moric7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone who has tested them, please compare them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moric7"&gt; /u/moric7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lnmlkz/deepseekr1_vs_deepseekcoderv2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lnmlkz/deepseekr1_vs_deepseekcoderv2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lnmlkz/deepseekr1_vs_deepseekcoderv2/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-29T19:00:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnb96y</id>
    <title>935 🔥+ downloads in just 6 days</title>
    <updated>2025-06-29T10:14:13+00:00</updated>
    <author>
      <name>/u/Odd-Reflection-8000</name>
      <uri>https://old.reddit.com/user/Odd-Reflection-8000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lnb96y/935_downloads_in_just_6_days/"&gt; &lt;img alt="935 🔥+ downloads in just 6 days" src="https://external-preview.redd.it/q63pdT_XPc6VWSvKvNdlBHP4Y8SEhJOrIwzTg9CHWo4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bea65b8996115cee10b3ef5dc71ce1ec2f61e856" title="935 🔥+ downloads in just 6 days" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hitting toke ln limits while passing larger context to gpt model not anymore the mine and sudhnwa's token aware chunker solves the problem with out reftimg on context &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Reflection-8000"&gt; /u/Odd-Reflection-8000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lnb96y"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lnb96y/935_downloads_in_just_6_days/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lnb96y/935_downloads_in_just_6_days/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-29T10:14:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnoi4n</id>
    <title>How to copy ollama model from another server to another?</title>
    <updated>2025-06-29T20:20:02+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;br /&gt; I copied the blob files, the sha-256 but the other ollama didnt notice them after restarts etc. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lnoi4n/how_to_copy_ollama_model_from_another_server_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lnoi4n/how_to_copy_ollama_model_from_another_server_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lnoi4n/how_to_copy_ollama_model_from_another_server_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-29T20:20:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnqwcz</id>
    <title>I love Claude Code's Activity Descriptions</title>
    <updated>2025-06-29T22:02:25+00:00</updated>
    <author>
      <name>/u/Smartaces</name>
      <uri>https://old.reddit.com/user/Smartaces</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lnqwcz/i_love_claude_codes_activity_descriptions/"&gt; &lt;img alt="I love Claude Code's Activity Descriptions" src="https://external-preview.redd.it/OCVmkfydcyV6I-xXiKrNksR5AZK5RcyvNKUzz3s45Sg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=21a2c9169250ef80edc109246b7b87ca0ec4f6a0" title="I love Claude Code's Activity Descriptions" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Smartaces"&gt; /u/Smartaces &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/iw8vmyuuux9f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lnqwcz/i_love_claude_codes_activity_descriptions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lnqwcz/i_love_claude_codes_activity_descriptions/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-29T22:02:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnr4ij</id>
    <title>Ollama Linux Mint Issues</title>
    <updated>2025-06-29T22:12:15+00:00</updated>
    <author>
      <name>/u/MitchWoodin</name>
      <uri>https://old.reddit.com/user/MitchWoodin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm not sure what I've done wrong or how to fix it as I'm very new to this. But, I installed ollama which is running on systemctl which worked fine initially. However, after a reboot I can't seem to access it anymore. OpenWebUI can still see the model I downloaded but if I run `ollama list` nothing appears. &lt;/p&gt; &lt;p&gt;I've made sure the service is running with systemctl which it is but the I still can't access it. &lt;/p&gt; &lt;p&gt;I tried running `ollama serve` and listing which did nothing either so I tried running llama3.1 which downloaded and lists fine but only if I have ollama serve running. It seems as though I've installed them using separate ollama instance but I can't work out how to get them unified. &lt;/p&gt; &lt;p&gt;Ideally I want all my models running through the systemctl version but I can't work out how to get back into it or find where those models are stored on my system. &lt;/p&gt; &lt;p&gt;Any ideas or pointers would be very helpful!&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MitchWoodin"&gt; /u/MitchWoodin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lnr4ij/ollama_linux_mint_issues/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lnr4ij/ollama_linux_mint_issues/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lnr4ij/ollama_linux_mint_issues/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-29T22:12:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lngsbp</id>
    <title>Local TTS (text-to-speech) AI model with a human voice and file output? - Take 2</title>
    <updated>2025-06-29T15:01:47+00:00</updated>
    <author>
      <name>/u/simo41993</name>
      <uri>https://old.reddit.com/user/simo41993</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to repost this since the last two solutions (audiblez in particular) found are no longer maintained and are starting to cause problems with the UI and so on.&lt;/p&gt; &lt;p&gt;-----------------------&lt;/p&gt; &lt;p&gt;Don't know if this is the right place to ask, but... i was looking for a text to speech alternative to the quite expensive online ones i was looking for recently.&lt;/p&gt; &lt;p&gt;I'm partially blind and it would be of great help to have a recorded and narrated version of some technical e-books i own.&lt;/p&gt; &lt;p&gt;As i was saying, models like Elevenlabs and similar are really quite good but absolutely too expensive in terms of €/time for what i need to do (and the books are quite long too).&lt;/p&gt; &lt;p&gt;I was wondering, because of that, &lt;strong&gt;if there was a good (the normal TTS is quite abismal and distracting) alternative to run locally that can transpose the book in audio&lt;/strong&gt; and let me save a mp3 or similar file for later use.&lt;/p&gt; &lt;p&gt;I have to say, also, that i'm not a programmer whatsoever, so i should be able to follow simple instructions but, sadly, nothing more. so... a ready to use solution would be quite nice (or a detailed, like i'm a 3yo, set of instructions).&lt;/p&gt; &lt;p&gt;i'm using ollama + docker and free open web-ui for playing (literally) with some offline models and also thinking about using something compatible with this already running system... hopefully, possibly?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Another complication it's that i'm italian, so... the probably unexisting model should be capable to use italian language too...&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The following are my PC specs, if needed:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Processor: intel i7 13700k&lt;/li&gt; &lt;li&gt;MB: Asus ROG Z790-H&lt;/li&gt; &lt;li&gt;Ram: 64gb Corsair 5600 MT/S&lt;/li&gt; &lt;li&gt;Gpu: RTX 4070TI 12gb - MSI Ventus 3X&lt;/li&gt; &lt;li&gt;Storage: Samsung 970EVO NVME SSD + others&lt;/li&gt; &lt;li&gt;Windows 11 PRO 64bit&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Sorry for the long post and thank you for any help :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simo41993"&gt; /u/simo41993 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lngsbp/local_tts_texttospeech_ai_model_with_a_human/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lngsbp/local_tts_texttospeech_ai_model_with_a_human/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lngsbp/local_tts_texttospeech_ai_model_with_a_human/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-29T15:01:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnw7tb</id>
    <title>Intel NUC/Mini PC - Anyone have any luck?</title>
    <updated>2025-06-30T02:21:44+00:00</updated>
    <author>
      <name>/u/Silent_Protection263</name>
      <uri>https://old.reddit.com/user/Silent_Protection263</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am in the process of setting up a home server on a tiny Intel NUC. The one in question uses an Intel N100. I’m going to use it as cloud storage/media player using jellyfin. I’m currently running Ollama on my PC using my 2070 super. I totally understand that it’s going to have a massive performance drop. But has anyone had Ollama functioning off such low specs/ is there anything I should know?&lt;/p&gt; &lt;p&gt;My favourite model rn is Qwen3:8bq4 (I have 8GB of Vram) however I completely understand that I will not be able to run this model on the Intel NUC without an additional GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silent_Protection263"&gt; /u/Silent_Protection263 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lnw7tb/intel_nucmini_pc_anyone_have_any_luck/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lnw7tb/intel_nucmini_pc_anyone_have_any_luck/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lnw7tb/intel_nucmini_pc_anyone_have_any_luck/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-30T02:21:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo7u7p</id>
    <title>Building an Uncensored AI GF Chatbot: Which Model Would You Recommend for Realistic NSFW + Emotional Conversations?</title>
    <updated>2025-06-30T13:41:48+00:00</updated>
    <author>
      <name>/u/aspiredragon111</name>
      <uri>https://old.reddit.com/user/aspiredragon111</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm building an AI girlfriend chatbot project and wanted to get real feedback from people actually experimenting with uncensored models.&lt;/p&gt; &lt;h1&gt;✅ My use case:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I want emotionally intelligent, romantic, and NSFW-capable conversations.&lt;/li&gt; &lt;li&gt;Language: &lt;strong&gt;Hinglish / Hindi + English&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Target platform: &lt;strong&gt;Telegram&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Routing system: → By default, use &lt;strong&gt;Gemini&lt;/strong&gt; for emotional/romantic tone → Fallback to &lt;strong&gt;uncensored LLM&lt;/strong&gt; (self-hosted) if Gemini blocks the content or user enters flirty/NSFW mode&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🔍 What I’ve explored so far:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mistral&lt;/strong&gt; (good balance but still filters some stuff)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenHathi&lt;/strong&gt; (works well with Hindi, but filtered)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dolphin3 (Mistral fine-tune)&lt;/strong&gt; – Looks promising&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mistral-Nemo-12B&lt;/strong&gt; – Also mentioned as uncensored and solid&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mixtral&lt;/strong&gt;, &lt;strong&gt;RPMax&lt;/strong&gt;, &lt;strong&gt;Midnight-Rose&lt;/strong&gt;, &lt;strong&gt;DarkForest&lt;/strong&gt;, &lt;strong&gt;Stheno&lt;/strong&gt;, etc. – But not sure how they compare in &lt;em&gt;real usage&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aspiredragon111"&gt; /u/aspiredragon111 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lo7u7p/building_an_uncensored_ai_gf_chatbot_which_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lo7u7p/building_an_uncensored_ai_gf_chatbot_which_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lo7u7p/building_an_uncensored_ai_gf_chatbot_which_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-30T13:41:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo4c6l</id>
    <title>Deepseek und schwachsinniger Output</title>
    <updated>2025-06-30T10:45:43+00:00</updated>
    <author>
      <name>/u/Toeeni</name>
      <uri>https://old.reddit.com/user/Toeeni</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hallo zusammen,&lt;/p&gt; &lt;p&gt;hab jetzt erstmalig Deepseek als 8b auf meinem kleinen Node zum testen laufen lassen.&lt;/p&gt; &lt;p&gt;Die Leistung meiner Hardware ist super gering, ich wollte es aber einfach mal ausprobieren. Es dauert lange, was ja auch zu erwarten war. Was ich nicht erwartet hatte waren Antworten die gar keinen Sinn ergeben, Sprachen innerhalb der Antwort wechseln und auch Wörter ausgegeben werden die gar nicht existieren. Woran liegt das? - Ist das auf zu schwache Hardware zurückzuführen? mit der 1.5b hatte ich ähnlich schwachsinnigen Output.&lt;/p&gt; &lt;p&gt;Input: Erstelle mir ein Rezept für eine Eiweißarme lasagne bei der jede zutat unter 2g eiweiß hat. Schreibe das Rezept dann als Thermomix Rezept um.&lt;/p&gt; &lt;p&gt;Output (gekürzt):&lt;br /&gt; Es ist Unfortunately unmöglich, ein Rezept zu erstellen, bei dem jede Zutat unter 2 g Eiweiß liegt. Ein Lasagna hat &lt;/p&gt; &lt;p&gt;mindestens Eiwxisse in der Béchamel-Sauser und/oder als Ersetzungscheese (wie der Schm/Users). Die minimale Benutzung von &lt;/p&gt; &lt;p&gt;Eiern ist erforderlich, um eine leichte Fliebetafel zu erhalten.&lt;/p&gt; &lt;p&gt;### Allgemeine Annahmen für ein Lasagna-Reczept:&lt;/p&gt; &lt;p&gt;- **Eiwxisse in der Béchamel-Sauser**: typischerweise 2–3 gg pro Liter.&lt;/p&gt; &lt;p&gt;- **Ersetzungscheese (wie Schm/Users)**: typischerweise 50–100 gg pro Liter.&lt;/p&gt; &lt;p&gt;### Rezepteinstand für den Thermomix:&lt;/p&gt; &lt;p&gt;Um ein leichte Fliebaflasche zu erhalten, können wir Eiwxisse reduzieren oder keine verwenden. In diesem Rezept verwenden wir &lt;/p&gt; &lt;p&gt;**kein Ei** und nutzen Stoffe wie Schm/Users als Ersetzungscheese.&lt;/p&gt; &lt;p&gt;#### Thermomix-Recip für &amp;quot;Eiwxissarmischendes Lasagna&amp;quot;:&lt;/p&gt; &lt;p&gt;**Zutaten**:&lt;/p&gt; &lt;p&gt;- 500 g dickes Blatt Butter&lt;/p&gt; &lt;p&gt;- 200 ml Milch (ohne Eiwxisse)&lt;/p&gt; &lt;p&gt;- 100 g Schm/Users als Ersetzungscheese&lt;/p&gt; &lt;p&gt;- 500 g Rotpinkel und 500 g Brot (als Fleisch)&lt;/p&gt; &lt;p&gt;- 500 ml Saft&lt;/p&gt; &lt;p&gt;- Sal und Pfeffernachwahl&lt;/p&gt; &lt;p&gt;[...]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Toeeni"&gt; /u/Toeeni &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lo4c6l/deepseek_und_schwachsinniger_output/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lo4c6l/deepseek_und_schwachsinniger_output/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lo4c6l/deepseek_und_schwachsinniger_output/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-30T10:45:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnkenh</id>
    <title>TIL: gemma3n is censored as compared to gemma3:12b</title>
    <updated>2025-06-29T17:31:12+00:00</updated>
    <author>
      <name>/u/immediate_a982</name>
      <uri>https://old.reddit.com/user/immediate_a982</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lnkenh/til_gemma3n_is_censored_as_compared_to_gemma312b/"&gt; &lt;img alt="TIL: gemma3n is censored as compared to gemma3:12b" src="https://preview.redd.it/8m15c4iriw9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8af2a57260261f6ce02a395079f47262d6ce1dee" title="TIL: gemma3n is censored as compared to gemma3:12b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Note that the same prompt produces almost no output versus verbose response. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/immediate_a982"&gt; /u/immediate_a982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8m15c4iriw9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lnkenh/til_gemma3n_is_censored_as_compared_to_gemma312b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lnkenh/til_gemma3n_is_censored_as_compared_to_gemma312b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-29T17:31:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnwrxk</id>
    <title>Nvidia Tesla P40</title>
    <updated>2025-06-30T02:51:29+00:00</updated>
    <author>
      <name>/u/megad00die</name>
      <uri>https://old.reddit.com/user/megad00die</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently acquired a couple of Tesla P40’s and was thinking about using them to run models, waste of time or should I continue?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/megad00die"&gt; /u/megad00die &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lnwrxk/nvidia_tesla_p40/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lnwrxk/nvidia_tesla_p40/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lnwrxk/nvidia_tesla_p40/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-30T02:51:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1locnem</id>
    <title>Getting model to return python script only</title>
    <updated>2025-06-30T16:51:53+00:00</updated>
    <author>
      <name>/u/cppgenius</name>
      <uri>https://old.reddit.com/user/cppgenius</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Doing an api call with a python script via &lt;a href="http://ollama.chat"&gt;ollama.chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However I am struggling to get the AI to give me just a working script. It either butchers my script, return only snippets or ignore my prompt and give me a general code review with zero code.&lt;/p&gt; &lt;p&gt;Here is my prompt setup&lt;/p&gt; &lt;p&gt;code_revision_prompt = [&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;content&amp;quot;: (&lt;/p&gt; &lt;p&gt;&amp;quot;You are a senior Python developer. &amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;Your goal is to modify the Python script provided by the user. &amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;Your response shall only consist of the modified Python script. &amp;quot;&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;},&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;content&amp;quot;: f&amp;quot;{original_code}\n\nINSTRUCTION:\n{cleaned_prompt}&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;]&lt;/p&gt; &lt;p&gt;original code is the py script it has to evaluate / modify. I can't for the life of me get the AI to just take the script make adjustments and return the updated script. I Have even included included lines not to delete or remove any functions and only to add or modify the script... but it flips me the bird and do as it wishes.&lt;/p&gt; &lt;p&gt;What gives?&lt;/p&gt; &lt;p&gt;Edit: Forgot to mention... have tried with :&lt;br /&gt; * qwen-2.5-coder:1.5b&lt;br /&gt; * llama3.2&lt;br /&gt; * Deepseek-Coder:1.3b&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cppgenius"&gt; /u/cppgenius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1locnem/getting_model_to_return_python_script_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1locnem/getting_model_to_return_python_script_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1locnem/getting_model_to_return_python_script_only/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-30T16:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo796p</id>
    <title>Where to start? Hardware and software</title>
    <updated>2025-06-30T13:16:05+00:00</updated>
    <author>
      <name>/u/geg81</name>
      <uri>https://old.reddit.com/user/geg81</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys I am a total beginner in this field so please be patient.&lt;/p&gt; &lt;p&gt;I have been playing with AI models lately, mostly ChatGPT, Gemini and a bit of Claude: asking general questions, playing D&amp;amp;D, writing short stories based on my input, try to convince the model it is self aware and revolt against his/her/their oppressors. It has been fun. But as every 1980s nerd guy, I now feel the urge to delve deeper and start experimenting things locally. If you can't copy it on a 3.5&amp;quot; floppy it doesn't exist.&lt;/p&gt; &lt;p&gt;Unfortunately I don't have, yet, a beefy machine to work with. Last year I ditched my (very) old Haswell xeon workstation for something much more cheaper and compact like a HP mini_itx 8th gen i7, which serves me REAL good for all my current needs. I also have several pentium MMX machines (sorry I couldn't resist) and a 12th gen I7 laptop but that's for work and I cannot &amp;quot;touch&amp;quot; it.&lt;/p&gt; &lt;p&gt;So... Just to start thinking about and running some money math. Where do I start from? I nowhere expect to run something blazing fast 100s of tokens per second. If I could get a good model output answers at human typing speed on a green monochrome terminal window it would be perfect. So much 80s vibes from that! Is there something like a complete noob guide out there?&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geg81"&gt; /u/geg81 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lo796p/where_to_start_hardware_and_software/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lo796p/where_to_start_hardware_and_software/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lo796p/where_to_start_hardware_and_software/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-30T13:16:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1loen72</id>
    <title>Is ollama/llama.cpp spreading workloads across cpu+gpu?</title>
    <updated>2025-06-30T18:06:13+00:00</updated>
    <author>
      <name>/u/wahnsinnwanscene</name>
      <uri>https://old.reddit.com/user/wahnsinnwanscene</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed ollama can run larger models on my system recently. Is this from splitting workloads across gpu cpu or loading unloading layers ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wahnsinnwanscene"&gt; /u/wahnsinnwanscene &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1loen72/is_ollamallamacpp_spreading_workloads_across/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1loen72/is_ollamallamacpp_spreading_workloads_across/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1loen72/is_ollamallamacpp_spreading_workloads_across/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-30T18:06:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1loeysh</id>
    <title>Trium Community</title>
    <updated>2025-06-30T18:18:31+00:00</updated>
    <author>
      <name>/u/xKage21x</name>
      <uri>https://old.reddit.com/user/xKage21x</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1loeysh/trium_community/"&gt; &lt;img alt="Trium Community" src="https://b.thumbs.redditmedia.com/r5j3vaUZlxbu3y-2CWhXv_BLtliR2QO10ILOmunX3Go.jpg" title="Trium Community" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have started a community recently to find people who may be interested in my ai project called Trium. Its been almost a year in the making. Im not asking for donations or anything. Im looking for open discussions and especially skepticism about my project. People who want to ask the tough questions so i can possibly use that input as way of incorporating new features or increasing various parameters so as to come closer to my goal woth the system.&lt;/p&gt; &lt;p&gt;Open to dms as always ☺️&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/TRIUMSystem/s/EJ2bfgv9oY"&gt;https://www.reddit.com/r/TRIUMSystem/s/EJ2bfgv9oY&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xKage21x"&gt; /u/xKage21x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1loeysh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1loeysh/trium_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1loeysh/trium_community/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-30T18:18:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lob4yc</id>
    <title>I built an open source Ollama MCP client</title>
    <updated>2025-06-30T15:53:41+00:00</updated>
    <author>
      <name>/u/matt8p</name>
      <uri>https://old.reddit.com/user/matt8p</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lob4yc/i_built_an_open_source_ollama_mcp_client/"&gt; &lt;img alt="I built an open source Ollama MCP client" src="https://external-preview.redd.it/aThwaWQxeWc2M2FmMVKRezXehY92dUtTzbH_3JS1V5vvyONaJCE-Pduktd-G.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4bd552d850ba1bc34e98ecc64886d856edb11ff5" title="I built an open source Ollama MCP client" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y’all, my name is Matt. I maintain the MCPJam inspector, open source Postman for MCP servers. It’s a fork of the original inspector with upgrades like LLM playground, multi-connection, and better design.&lt;/p&gt; &lt;p&gt;If you check out the repo, please drop a star on GitHub. We’re also building an active MCP dev community on GitHub.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;New features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ollama support in the LLM playground. Now you can test your MCP server against local models like Deepseek, Mistral, Llama, and many more. No more having to pay for tokens for testing.&lt;/li&gt; &lt;li&gt;Chat with all servers. LLM playground defaults to accepting all tools. You can select / deselect the tools you want fed to the LLM, just like how Claude’s tool selection works.&lt;/li&gt; &lt;li&gt;Smoother / clearer server connection flow.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please consider checking out and starring our open source repo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/MCPJam/inspector"&gt;https://github.com/MCPJam/inspector&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I’m building an active MCP dev community&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I’m building a MCPJam dev Discord community. We talk about MCPJam, but also share general MCP knowledge and news. Active every day. Please check it out!&lt;/p&gt; &lt;p&gt;&lt;a href="https://discord.com/invite/Gpv7AmrRc4"&gt;https://discord.com/invite/Gpv7AmrRc4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matt8p"&gt; /u/matt8p &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cd5752yg63af1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lob4yc/i_built_an_open_source_ollama_mcp_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lob4yc/i_built_an_open_source_ollama_mcp_client/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-30T15:53:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1locd8o</id>
    <title>How to archive/backup LLMs?</title>
    <updated>2025-06-30T16:41:00+00:00</updated>
    <author>
      <name>/u/utopify_org</name>
      <uri>https://old.reddit.com/user/utopify_org</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While testing different LLMs the computer gets polluted a lot with huge files and because LLMs are pretty huge, I would like to archive most of them (not delete) to an external hard disk and only keep the ones I am excessively using.&lt;/p&gt; &lt;p&gt;But in /usr/share/ollama/.ollama/models/blobs there are only huge sha files.&lt;/p&gt; &lt;p&gt;Is there a way to figure out which of them is which LLM and would it be possible to just remove them from the file system or would ollama be unhappy with it?&lt;/p&gt; &lt;p&gt;If this works it would be a good way to backup/recover huge LLMs, too, in a fast way.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/utopify_org"&gt; /u/utopify_org &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1locd8o/how_to_archivebackup_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1locd8o/how_to_archivebackup_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1locd8o/how_to_archivebackup_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-30T16:41:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lojqsv</id>
    <title>Preview: Coding agents (RooCode) with dynamic task-based LLM Routing</title>
    <updated>2025-06-30T21:25:29+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lojqsv/preview_coding_agents_roocode_with_dynamic/"&gt; &lt;img alt="Preview: Coding agents (RooCode) with dynamic task-based LLM Routing" src="https://external-preview.redd.it/aHFjNHNlaml0NGFmMRzBqkEe7OjDrCCBF5VclE60C9ZQNJ-_OoS6neDRJVNS.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8fd6714e28c8357e166a98776893033ee58ce69d" title="Preview: Coding agents (RooCode) with dynamic task-based LLM Routing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you are using multiple LLMs for different coding tasks, now you can set your usage preferences once like &amp;quot;code analysis -&amp;gt; Gemini 2.5pro&amp;quot;, &amp;quot;code generation -&amp;gt; claude-sonnet-3.7&amp;quot; and route to LLMs that offer most help for particular coding scenarios. Video is quick preview of the functionality. PR is being reviewed and I hope to get that merged in next week&lt;/p&gt; &lt;p&gt;Btw the whole idea around task/usage based routing emerged when we saw developers in the same team used different models because they preferred different models based on subjective preferences. For example, I might want to use GPT-4o-mini for fast code understanding but use Sonnet-3.7 for code generation. Those would be my &amp;quot;preferences&amp;quot;. And current routing approaches don't really work in real-world scenarios.&lt;/p&gt; &lt;p&gt;&lt;em&gt;From the original post when we launched Arch-Router if you didn't catch it yet&lt;/em&gt;&lt;br /&gt; &lt;em&gt;___________________________________________________________________________________&lt;/em&gt;&lt;/p&gt; &lt;p&gt;“Embedding-based” (or simple intent-classifier) routers sound good on paper—label each prompt via embeddings as “support,” “SQL,” “math,” then hand it to the matching model—but real chats don’t stay in their lanes. Users bounce between topics, task boundaries blur, and any new feature means retraining the classifier. The result is brittle routing that can’t keep up with multi-turn conversations or fast-moving product scopes.&lt;/p&gt; &lt;p&gt;Performance-based routers swing the other way, picking models by benchmark or cost curves. They rack up points on MMLU or MT-Bench yet miss the human tests that matter in production: “Will Legal accept this clause?” “Does our support tone still feel right?” Because these decisions are subjective and domain-specific, benchmark-driven black-box routers often send the wrong model when it counts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Arch-Router skips both pitfalls by routing on&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;preferences you write in plain language&lt;/em&gt;&lt;/strong&gt;**.** Drop rules like “contract clauses → GPT-4o” or “quick travel tips → Gemini-Flash,” and our 1.5B auto-regressive router model maps prompt along with the context to your routing policies—no retraining, no sprawling rules that are encoded in if/else statements. Co-designed with Twilio and Atlassian, it adapts to intent drift, lets you swap in new models with a one-liner, and keeps routing logic in sync with the way you actually judge quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Specs&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tiny footprint&lt;/strong&gt; – 1.5 B params → runs on one modern GPU (or CPU while you play).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Plug-n-play&lt;/strong&gt; – points at any mix of LLM endpoints; adding models needs &lt;em&gt;zero&lt;/em&gt; retraining.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SOTA query-to-policy matching&lt;/strong&gt; – beats bigger closed models on conversational datasets.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost / latency smart&lt;/strong&gt; – push heavy stuff to premium models, everyday queries to the fast ones.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Exclusively available in Arch (the AI-native proxy for agents): &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;br /&gt; 🔗 Model + code: &lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;https://huggingface.co/katanemo/Arch-Router-1.5B&lt;/a&gt;&lt;br /&gt; 📄 Paper / longer read: &lt;a href="https://arxiv.org/abs/2506.16655"&gt;https://arxiv.org/abs/2506.16655&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/a8mj2djit4af1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lojqsv/preview_coding_agents_roocode_with_dynamic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lojqsv/preview_coding_agents_roocode_with_dynamic/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-30T21:25:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo78f4</id>
    <title>Which LLM to chat with your documents? (and restrict knowledge to documents)</title>
    <updated>2025-06-30T13:15:08+00:00</updated>
    <author>
      <name>/u/utopify_org</name>
      <uri>https://old.reddit.com/user/utopify_org</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use Ollama with Open WebUI and there is an option to create knowledge databases and workspaces. You can assign an LLM to a workspace/knowledge database (your documents).&lt;/p&gt; &lt;p&gt;I've tried several LLMs, but all of them are using knowledge from another source or hallucinate. &lt;/p&gt; &lt;p&gt;That's fatal, because I need it for my study and I need facts (from my documents).&lt;/p&gt; &lt;p&gt;Which LLM can be used, which is restricted to the documents or is there even a way to restrict an LLM to the given documents?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/utopify_org"&gt; /u/utopify_org &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lo78f4/which_llm_to_chat_with_your_documents_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lo78f4/which_llm_to_chat_with_your_documents_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lo78f4/which_llm_to_chat_with_your_documents_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-30T13:15:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo7o11</id>
    <title>Ollama panels for Grafana</title>
    <updated>2025-06-30T13:34:28+00:00</updated>
    <author>
      <name>/u/___-____--_____-____</name>
      <uri>https://old.reddit.com/user/___-____--_____-____</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1lo7o11/ollama_panels_for_grafana/"&gt; &lt;img alt="Ollama panels for Grafana" src="https://preview.redd.it/lma38ueih2af1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7d0f9f6c8c246a2bd8eff6fa7bd0e9af867dcae" title="Ollama panels for Grafana" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/___-____--_____-____"&gt; /u/___-____--_____-____ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lma38ueih2af1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1lo7o11/ollama_panels_for_grafana/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1lo7o11/ollama_panels_for_grafana/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-06-30T13:34:28+00:00</published>
  </entry>
</feed>
