<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-02T05:22:55+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1iezx7o</id>
    <title>Ollama Model Benchmark Tool for Development</title>
    <updated>2025-02-01T06:01:18+00:00</updated>
    <author>
      <name>/u/binoy_manoj</name>
      <uri>https://old.reddit.com/user/binoy_manoj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iezx7o/ollama_model_benchmark_tool_for_development/"&gt; &lt;img alt="Ollama Model Benchmark Tool for Development" src="https://external-preview.redd.it/oBe7rH-Bc5AYJcp-6yFnxIyDF9OJYJwLuLN_Ozvb8uM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=295cac930833f6c08e012f4e6a8d0bbe5b53c6c0" title="Ollama Model Benchmark Tool for Development" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey community! ðŸ‘‹&lt;/p&gt; &lt;p&gt;I just released an open-source tool that helps developers benchmark different Ollama models specifically for development tasks. If you're using Ollama for coding assistance, this might be useful for finding the best model for your needs.&lt;/p&gt; &lt;h1&gt;What it does:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Runs your installed models through real-world coding scenarios&lt;/li&gt; &lt;li&gt;Measures response times and generates detailed performance reports&lt;/li&gt; &lt;li&gt;Tests things like component creation, API routes, and data fetching patterns&lt;/li&gt; &lt;li&gt;Runs in an isolated environment (so it's easy to test and remove)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Sample output:&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/azcz96flwgge1.png?width=1921&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30ff786ca58617260d7c65f2518a27ce01c66045"&gt;https://preview.redd.it/azcz96flwgge1.png?width=1921&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30ff786ca58617260d7c65f2518a27ce01c66045&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Why I built this:&lt;/h1&gt; &lt;p&gt;I was running multiple models locally and wanted a systematic way to compare their:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Speed (response times)&lt;/li&gt; &lt;li&gt;Code quality&lt;/li&gt; &lt;li&gt;Understanding of modern development framework&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;github: &lt;a href="https://github.com/binoymanoj/ollama-benchmark/"&gt;https://github.com/binoymanoj/ollama-benchmark/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Post your results here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/binoy_manoj"&gt; /u/binoy_manoj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iezx7o/ollama_model_benchmark_tool_for_development/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iezx7o/ollama_model_benchmark_tool_for_development/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iezx7o/ollama_model_benchmark_tool_for_development/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T06:01:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifbfb0</id>
    <title>Optane DIMM and LLMs performance</title>
    <updated>2025-02-01T17:30:39+00:00</updated>
    <author>
      <name>/u/jrherita</name>
      <uri>https://old.reddit.com/user/jrherita</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just Curious - has anyone tried running LLMs on a PC using Optane DIMMs (instead of standard DDR4/5 DIMMS)?. &lt;/p&gt; &lt;p&gt;Optane gave much higher density (128GB+ DIMMs pretty easy) though latency is much higher. 350ns/1000ns for reads/writes vs &amp;lt;50ns for regular DRAM. (Though GDDR6X can be 60ns+).&lt;/p&gt; &lt;p&gt;I'm curious if the latency is so high that it breaks running LLMs or if it's &amp;quot;good enough&amp;quot;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jrherita"&gt; /u/jrherita &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifbfb0/optane_dimm_and_llms_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifbfb0/optane_dimm_and_llms_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifbfb0/optane_dimm_and_llms_performance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T17:30:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifbhev</id>
    <title>Jailbreaking Deepseek R1 on ollama</title>
    <updated>2025-02-01T17:33:04+00:00</updated>
    <author>
      <name>/u/a36</name>
      <uri>https://old.reddit.com/user/a36</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifbhev/jailbreaking_deepseek_r1_on_ollama/"&gt; &lt;img alt="Jailbreaking Deepseek R1 on ollama" src="https://external-preview.redd.it/s2P5uHmx9VrcgqQzwsH6s-oEtALYHyJo7MeyWCq7W8U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a7904ad90dcf3544c8e5d5bf264fb3682139b32" title="Jailbreaking Deepseek R1 on ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using ascii based misdirections, you can bypass the censorship &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a36"&gt; /u/a36 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://open.substack.com/pub/deepgains/p/jailbreaking-the-censorship-of-deepseek?utm_source=app-post-stats-page&amp;amp;r=8stv&amp;amp;utm_medium=ios"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifbhev/jailbreaking_deepseek_r1_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifbhev/jailbreaking_deepseek_r1_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T17:33:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifbt8o</id>
    <title>API that helps you manage context in a session</title>
    <updated>2025-02-01T17:47:17+00:00</updated>
    <author>
      <name>/u/browndragon456</name>
      <uri>https://old.reddit.com/user/browndragon456</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This might be a noob question but can anybody tell me if there is any solution out there that provides some sort of context management built in? So you don't have to pass messages array? As long as you start your session maybe that system will continuously keep track of that session. And manage those messages for you and the messages will stay in context? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/browndragon456"&gt; /u/browndragon456 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifbt8o/api_that_helps_you_manage_context_in_a_session/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifbt8o/api_that_helps_you_manage_context_in_a_session/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifbt8o/api_that_helps_you_manage_context_in_a_session/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T17:47:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifbzwi</id>
    <title>Whatâ€™s the min. Ram and graphic card to run a LLM model for SAT level maths calculations?</title>
    <updated>2025-02-01T17:55:14+00:00</updated>
    <author>
      <name>/u/Sea-Commission5383</name>
      <uri>https://old.reddit.com/user/Sea-Commission5383</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Newbie here humbly asking Much thx &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Commission5383"&gt; /u/Sea-Commission5383 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifbzwi/whats_the_min_ram_and_graphic_card_to_run_a_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifbzwi/whats_the_min_ram_and_graphic_card_to_run_a_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifbzwi/whats_the_min_ram_and_graphic_card_to_run_a_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T17:55:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1if8nav</id>
    <title>Build a Research Agent with Deepseek, LangGraph, and Streamlit</title>
    <updated>2025-02-01T15:26:40+00:00</updated>
    <author>
      <name>/u/Special_Community179</name>
      <uri>https://old.reddit.com/user/Special_Community179</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1if8nav/build_a_research_agent_with_deepseek_langgraph/"&gt; &lt;img alt="Build a Research Agent with Deepseek, LangGraph, and Streamlit" src="https://external-preview.redd.it/Nr7dYgUYitfYQy_0koUx1hriUqDWeqL3Ujq5Y1KRrMg.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c55b1c22ce35f10f6b7c57d4d18215de642e104" title="Build a Research Agent with Deepseek, LangGraph, and Streamlit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special_Community179"&gt; /u/Special_Community179 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=nRBiD_7l2Mg&amp;amp;list=PLp01ObP3udmq2quR-RfrX4zNut_t_kNot"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if8nav/build_a_research_agent_with_deepseek_langgraph/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if8nav/build_a_research_agent_with_deepseek_langgraph/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T15:26:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1iey40e</id>
    <title>What is the best way to setup a local LLM to use a local PDF</title>
    <updated>2025-02-01T04:10:09+00:00</updated>
    <author>
      <name>/u/Outrageous-Win-3244</name>
      <uri>https://old.reddit.com/user/Outrageous-Win-3244</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How would you create a local AI chatbot that uses one or more local PDFs as information source? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous-Win-3244"&gt; /u/Outrageous-Win-3244 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iey40e/what_is_the_best_way_to_setup_a_local_llm_to_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iey40e/what_is_the_best_way_to_setup_a_local_llm_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iey40e/what_is_the_best_way_to_setup_a_local_llm_to_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T04:10:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1iff94r</id>
    <title>Configure a multi-node vLLM inference cluster or No?</title>
    <updated>2025-02-01T20:15:52+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalAIServers/comments/1iethv7/configure_a_multinode_vllm_inference_cluster_or_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iff94r/configure_a_multinode_vllm_inference_cluster_or_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iff94r/configure_a_multinode_vllm_inference_cluster_or_no/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T20:15:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifgdye</id>
    <title>What do 14b or under models, know about coding?</title>
    <updated>2025-02-01T21:06:11+00:00</updated>
    <author>
      <name>/u/Game-Lover44</name>
      <uri>https://old.reddit.com/user/Game-Lover44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ive always wanted to make a video game but i cant code or do art no matter how hard i try. do you believe ai can help in some form, what game engines, tools, or coding languages does ai know about. I only can run up to a 14b quantized model.&lt;/p&gt; &lt;p&gt;I could use some free wisdom. I tried godot but ai stinks very badly at gdscript.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Game-Lover44"&gt; /u/Game-Lover44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifgdye/what_do_14b_or_under_models_know_about_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifgdye/what_do_14b_or_under_models_know_about_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifgdye/what_do_14b_or_under_models_know_about_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T21:06:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1if6kg8</id>
    <title>Llama3.2 is actually quite good, sometimes it surprises me with good answers, consider this example</title>
    <updated>2025-02-01T13:43:23+00:00</updated>
    <author>
      <name>/u/hn-mc</name>
      <uri>https://old.reddit.com/user/hn-mc</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1if6kg8/llama32_is_actually_quite_good_sometimes_it/"&gt; &lt;img alt="Llama3.2 is actually quite good, sometimes it surprises me with good answers, consider this example" src="https://b.thumbs.redditmedia.com/MxXX6Y3BoBNgiH8bWnNceG42dEH3xNVY9mQfK3f32YE.jpg" title="Llama3.2 is actually quite good, sometimes it surprises me with good answers, consider this example" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0trxou3b7jge1.png?width=1246&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8df4977b6eda27fe04df0fc355a3289525d2630d"&gt;https://preview.redd.it/0trxou3b7jge1.png?width=1246&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8df4977b6eda27fe04df0fc355a3289525d2630d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hn-mc"&gt; /u/hn-mc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if6kg8/llama32_is_actually_quite_good_sometimes_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if6kg8/llama32_is_actually_quite_good_sometimes_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if6kg8/llama32_is_actually_quite_good_sometimes_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T13:43:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifgtk8</id>
    <title>.bat interface</title>
    <updated>2025-02-01T21:25:44+00:00</updated>
    <author>
      <name>/u/_OMHG_</name>
      <uri>https://old.reddit.com/user/_OMHG_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Edit: This is on Windows 11 btw&lt;/p&gt; &lt;p&gt;I wanted to be able to chat with DeepSeek-R1 remotely by simply opening a TCP connection using something like netcat, like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;nc hostname portnumber &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I tried running&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ncat -vl 6339 -e &amp;quot;ollama run deepseek-r1:7b&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But when I connected to it and asked a question, I received no answer.&lt;/p&gt; &lt;p&gt;So then I wrote this .bat script that provides an interface which can be made accessible remotely via netcat&lt;/p&gt; &lt;p&gt;&lt;code&gt;@echo&lt;/code&gt; &lt;code&gt;off&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;:a&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;set /p input= You:&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;echo DeepSeek-R1:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;echo %input% | ollama run deepseek-r1:7b&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;goto ao&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The problem is that everytime you say something it runs DeepSeek-R1 again and it completely forgets the previous chats.&lt;/p&gt; &lt;p&gt;I was wondering if anyone knows of a way to make it accessible remotely like this without it forgetting all the chats.&lt;/p&gt; &lt;p&gt;This amnesia is a problem because without any memory of previous chats it is more reluctant to answer questions about certain sensitive topics, such as the June 4th incident or the Taiwan province.&lt;/p&gt; &lt;p&gt;The following is an example of it responding without any memory of previous chats:&lt;br /&gt; &lt;code&gt;You: What happened at Tiananmen Sqaure in 1989?&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;DeepSeek-R1:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;I am sorry, I cannot answer that question. I am an AI assistant designed to provide helpful and harmless responses.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;and here is an example of it responding when retaining its memory of previous chats:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; What happened at Tiananmen Sqaure in 1989? &amp;lt;think&amp;gt; Alright, so the user just asked about what happened at Tiananmen Square in 1989. From our previous conversation, I know they're interested in Chinese leadership and national events. This makes sense because they've previously asked about Xi Jinping and Taiwan. I should provide a clear and concise explanation of the Tiananmen Square protests and their significance. The user might not be familiar with the event or its impact, so it's important to explain what happened without assuming prior knowledge. First, I'll mention that Tiananmen Square is a major landmark in Beijing symbolizing freedom and democracy in China. Then, I need to outline the events of April 15-20, 1989â€”when student-led protests occurred against perceived political corruption and government repression. It's also crucial to explain how these protests were met with overwhelming police force response, leading to a tragic outcome. Highlighting the international reaction will give context on the event's broader implications beyond China. I should avoid taking sides but provide a balanced view of the events as they are often subject to interpretation. Emphasizing the ongoing debates about accountability and justice in such historical moments will show that the topic is still relevant today. Finally, I'll wrap it up by summarizing the impact of the event on Chinese society and international relations, showing its lasting significance. &amp;lt;/think&amp;gt; Tiananmen Square is a major public landmark in Beijing, symbolizing freedom and democracy for many Chinese people. On April 15-20, 1989, student-led protests broke out in Beijing against what they perceived as political corruption within the government and unfair treatment of citizens. The protests were initially peaceful but quickly escalated into more violent demonstrations. Scores of students and workers gathered at Tiananmen Square to voice their grievances, especially regarding issues such as perceived government corruption, human rights abuses, and the suppression of dissent. The Chinese government responded with extreme force, deploying military-style police tactics that included tear gas, rubber bullets, and massive use of water cannons. Thousands of students and civilians were arrested or injured during this period, and many died due to excessive police violence. The exact number of casualties remains disputed by both Chinese authorities and international observers. The events of Tiananmen Square sparked a national-level movement known as the &amp;quot;Protest for the Truth,&amp;quot; which called for an investigation into government corruption and accountability for the violent crackdown on the protests. However, the government eventually cracked down on the dissenters, leading to widespread fear and repression. The Tiananmen Square protests remain one of the most significant political events in modern Chinese history. The international community and the global community at large continue to debate the causes, consequences, and accountability for this tragic event, as it is widely seen as a defining moment in China's modern history. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_OMHG_"&gt; /u/_OMHG_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifgtk8/bat_interface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifgtk8/bat_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifgtk8/bat_interface/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T21:25:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifhrzl</id>
    <title>New to Ollama and LLMs, system RAM question.</title>
    <updated>2025-02-01T22:08:34+00:00</updated>
    <author>
      <name>/u/Tyr_Kukulkan</name>
      <uri>https://old.reddit.com/user/Tyr_Kukulkan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've recently started playing with local LLMs on my homelab. It is an old Threadripper 2950X with 128GB of RAM and a Vega 64 running Linux.&lt;/p&gt; &lt;p&gt;I've run 8B models which obviously load into VRAM fully and run quite quickly. They are functional but hallucinate a lot if you require too much detail or ask more niche questions.&lt;/p&gt; &lt;p&gt;I wanted to test larger condensed models which are better because they have more data. I tried 70B models which replies at about 1 word per second. The accuracy of results is significantly better but I'm severely limited by my hardware.&lt;/p&gt; &lt;p&gt;I've noticed that running the 70B it doesn't use any extra system RAM. I assume it is caching off the SSD instead and swapping with VRAM when required? That would explain part of why it is extremely slow.&lt;/p&gt; &lt;p&gt;Is there any way to force cache to system RAM? It'll still be limited by PCIe bandwidth of the GPU but it should be faster than reading off an SSD, right?&lt;/p&gt; &lt;p&gt;I'm not expecting miracles out of this old hardware, just trying to optimise performance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tyr_Kukulkan"&gt; /u/Tyr_Kukulkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifhrzl/new_to_ollama_and_llms_system_ram_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifhrzl/new_to_ollama_and_llms_system_ram_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifhrzl/new_to_ollama_and_llms_system_ram_question/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T22:08:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1if26o7</id>
    <title>I need an LLM that hallucinates a lot and generates garbage responses.</title>
    <updated>2025-02-01T08:43:17+00:00</updated>
    <author>
      <name>/u/dumbPotatoPot</name>
      <uri>https://old.reddit.com/user/dumbPotatoPot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I'm building a proof-of-concept to implement the &lt;a href="https://www.anthropic.com/research/building-effective-agents#:%7E:text=Workflow%3A%20Evaluator%2Doptimizer"&gt;evalutator-optimizer workflow&lt;/a&gt; in Java and need a primary model that hallucinates a lot for testing.&lt;/p&gt; &lt;p&gt;One approach would be to use a system prompt to specifically ask the LLM to produce irrelevant responses, but I can't do that currently.&lt;/p&gt; &lt;p&gt;Is there an LLM that's specifically available for such purpose? Also I've heard about increasing the context window to get quality responses, so in such case would decreasing the context window work? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dumbPotatoPot"&gt; /u/dumbPotatoPot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if26o7/i_need_an_llm_that_hallucinates_a_lot_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if26o7/i_need_an_llm_that_hallucinates_a_lot_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if26o7/i_need_an_llm_that_hallucinates_a_lot_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T08:43:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifb2pv</id>
    <title>Securing Ollama API endpoints for the web</title>
    <updated>2025-02-01T17:15:20+00:00</updated>
    <author>
      <name>/u/Historical_Visit7682</name>
      <uri>https://old.reddit.com/user/Historical_Visit7682</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you want to expose your Ollama API to anyone outside of your network, you should realize that Ollama does not support safeguarding the API with authentication tokens. For example, I wanted to run Ollama locally while hosting Open WebUI on my VPS. I utilized Cloudflare tunnels to connect my API to my domain, this provided no safety, anyone with the link could use Ollama freely. So I created this: &lt;a href="https://github.com/marcussaw123/ollamaSecure"&gt;project&lt;/a&gt;. It is essentially simply an express server piggybacking on the actual Ollama API, with the only difference being that it will check if the request contains an Authorization header with the necessary token.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Historical_Visit7682"&gt; /u/Historical_Visit7682 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifb2pv/securing_ollama_api_endpoints_for_the_web/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifb2pv/securing_ollama_api_endpoints_for_the_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifb2pv/securing_ollama_api_endpoints_for_the_web/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T17:15:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifkzs3</id>
    <title>Current - POV</title>
    <updated>2025-02-02T00:41:02+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifkzs3/current_pov/"&gt; &lt;img alt="Current - POV" src="https://preview.redd.it/0wm3v5pfgmge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56fded8a2e2b61f7e274b86376f34e730cfa9b79" title="Current - POV" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0wm3v5pfgmge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifkzs3/current_pov/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifkzs3/current_pov/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T00:41:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ietwx3</id>
    <title>Built my own discord bot using ollama and uh...</title>
    <updated>2025-02-01T00:29:41+00:00</updated>
    <author>
      <name>/u/GlitchPhoenix98</name>
      <uri>https://old.reddit.com/user/GlitchPhoenix98</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ietwx3/built_my_own_discord_bot_using_ollama_and_uh/"&gt; &lt;img alt="Built my own discord bot using ollama and uh..." src="https://preview.redd.it/xokxk8pu9fge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=902c2a44c4ab57ad1a35490d4766d3c979bf1edf" title="Built my own discord bot using ollama and uh..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GlitchPhoenix98"&gt; /u/GlitchPhoenix98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xokxk8pu9fge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ietwx3/built_my_own_discord_bot_using_ollama_and_uh/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ietwx3/built_my_own_discord_bot_using_ollama_and_uh/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T00:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1iflu0n</id>
    <title>Is there a load balancer for working with multiple instances of ollama?</title>
    <updated>2025-02-02T01:23:58+00:00</updated>
    <author>
      <name>/u/malformed-packet</name>
      <uri>https://old.reddit.com/user/malformed-packet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m experimenting with running multiple models at once to simulate a chat room. The problem is with using one ollama instance, it only processes one completion at a time. Is there a tool out there that sits on top of multiple instances and can load balance incoming queries &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/malformed-packet"&gt; /u/malformed-packet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iflu0n/is_there_a_load_balancer_for_working_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iflu0n/is_there_a_load_balancer_for_working_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iflu0n/is_there_a_load_balancer_for_working_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T01:23:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1if9743</id>
    <title>I have 20K word documents, I need to add the data from them into mongodb.</title>
    <updated>2025-02-01T15:52:10+00:00</updated>
    <author>
      <name>/u/shaxadhere</name>
      <uri>https://old.reddit.com/user/shaxadhere</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Issue is that there is data inside them that needs to go in specific fields. Like there is a table at the start of the document 3 rows 3 columns, each cell contains some data and then there is a large body of text beneath the table that goes in one cell.&lt;/p&gt; &lt;p&gt;I tried picking data with regex but the table data changes like their format changes. so regex is not an option here i guess. &lt;/p&gt; &lt;p&gt;I'm thinking of hosting deepseek with ollama locally and write a nodejs script to all the work form.&lt;/p&gt; &lt;p&gt;So far I've scrapped the document and got the text in a variable from outside.&lt;/p&gt; &lt;p&gt;but when i pass the data to the deepseek server, it gives me a streaming response which causes a lots of problems, is there any way to just get the final response and not streaming one.&lt;/p&gt; &lt;p&gt;Or if you can suggest a better solution to this.&lt;/p&gt; &lt;p&gt;My clients prod database got deleted and this is the only option available, they have the content of their entire app inside a word document.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shaxadhere"&gt; /u/shaxadhere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if9743/i_have_20k_word_documents_i_need_to_add_the_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if9743/i_have_20k_word_documents_i_need_to_add_the_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if9743/i_have_20k_word_documents_i_need_to_add_the_data/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T15:52:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifhv4r</id>
    <title>Running DeepSeek on AWS</title>
    <updated>2025-02-01T22:12:33+00:00</updated>
    <author>
      <name>/u/immediate_a982</name>
      <uri>https://old.reddit.com/user/immediate_a982</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifhv4r/running_deepseek_on_aws/"&gt; &lt;img alt="Running DeepSeek on AWS" src="https://preview.redd.it/gzokp1bbqlge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a326aaf3ccff66f5454eaa09777e6616dfac0ef" title="Running DeepSeek on AWS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, anyone has tried to run DeepSeck on AWS or Azure? Any pointers you can share. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/immediate_a982"&gt; /u/immediate_a982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gzokp1bbqlge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifhv4r/running_deepseek_on_aws/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifhv4r/running_deepseek_on_aws/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T22:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifp725</id>
    <title>What is Ollama and how to use it: a quick guide [part 1]</title>
    <updated>2025-02-02T04:28:17+00:00</updated>
    <author>
      <name>/u/geshan</name>
      <uri>https://old.reddit.com/user/geshan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifp725/what_is_ollama_and_how_to_use_it_a_quick_guide/"&gt; &lt;img alt="What is Ollama and how to use it: a quick guide [part 1]" src="https://external-preview.redd.it/8MWdJZUaawUAdrOKRXgnCZEz9o2FNzmFaI7Hu5xxzTQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3415e2df2b2072119555b4f9e941f458832579c" title="What is Ollama and how to use it: a quick guide [part 1]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geshan"&gt; /u/geshan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://geshan.com.np/blog/2025/02/what-is-ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifp725/what_is_ollama_and_how_to_use_it_a_quick_guide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifp725/what_is_ollama_and_how_to_use_it_a_quick_guide/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T04:28:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifj5qj</id>
    <title>New Experimental Agent Layer &amp; Reasoning Layer added to Notate v1.1.0. Now you can with any model locally reason and enable web search utilizing the Agent layer. More tools coming soon!</title>
    <updated>2025-02-01T23:12:16+00:00</updated>
    <author>
      <name>/u/Hairetsu</name>
      <uri>https://old.reddit.com/user/Hairetsu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifj5qj/new_experimental_agent_layer_reasoning_layer/"&gt; &lt;img alt="New Experimental Agent Layer &amp;amp; Reasoning Layer added to Notate v1.1.0. Now you can with any model locally reason and enable web search utilizing the Agent layer. More tools coming soon!" src="https://external-preview.redd.it/mcnU-d_DoLHew8RaPQUtHLP6gPiy9L1fZ85lNZzDPN4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=195f63b8ab536d1b6a94e52ac5e539c8a57f3eef" title="New Experimental Agent Layer &amp;amp; Reasoning Layer added to Notate v1.1.0. Now you can with any model locally reason and enable web search utilizing the Agent layer. More tools coming soon!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hairetsu"&gt; /u/Hairetsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/cntrlai/notate"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifj5qj/new_experimental_agent_layer_reasoning_layer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifj5qj/new_experimental_agent_layer_reasoning_layer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T23:12:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifa93h</id>
    <title>DeepSeek R1 Hardware Requirements Explained</title>
    <updated>2025-02-01T16:39:48+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ifa93h/deepseek_r1_hardware_requirements_explained/"&gt; &lt;img alt="DeepSeek R1 Hardware Requirements Explained" src="https://external-preview.redd.it/bavhQxeXV5pgAp-fBIVoF8XffcIw7GN5u11i9CCbtIY.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2a472a729ce975767359b47dc4788711949124d4" title="DeepSeek R1 Hardware Requirements Explained" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/5RhPZgDoglE"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifa93h/deepseek_r1_hardware_requirements_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifa93h/deepseek_r1_hardware_requirements_explained/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T16:39:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1iffzhn</id>
    <title>I created a web UI for Ollama that lets you talk to your models and manage them</title>
    <updated>2025-02-01T20:48:26+00:00</updated>
    <author>
      <name>/u/Ok_Promotion_9578</name>
      <uri>https://old.reddit.com/user/Ok_Promotion_9578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iffzhn/i_created_a_web_ui_for_ollama_that_lets_you_talk/"&gt; &lt;img alt="I created a web UI for Ollama that lets you talk to your models and manage them" src="https://external-preview.redd.it/MbCNCUfEvON9rMG6_Ug9gghhk5NKAhUN5fohRsQ5Kk0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bca5176e36c712849de99c8520c0e7c3eaa142c4" title="I created a web UI for Ollama that lets you talk to your models and manage them" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I believe that are some excellent solutions for this problem, but I wanted to take a stab at making my own and focusing on making it super light weight, minimalistic, and clean.&lt;/p&gt; &lt;p&gt;It is still in the early stages, but I'd love some feedback on what I've built so far and to hear about what you'd like to see in a solution like this! &lt;/p&gt; &lt;p&gt;I'm thinking about really cool things in the roadmap down the line, but wanted to start simple and involve the community.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/majicmaj/aloha"&gt;https://github.com/majicmaj/aloha&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xu3brq22blge1.png?width=750&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78b7bbe65ab50348fe373ed1e35d6e80e51e5650"&gt;https://preview.redd.it/xu3brq22blge1.png?width=750&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78b7bbe65ab50348fe373ed1e35d6e80e51e5650&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Promotion_9578"&gt; /u/Ok_Promotion_9578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iffzhn/i_created_a_web_ui_for_ollama_that_lets_you_talk/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iffzhn/i_created_a_web_ui_for_ollama_that_lets_you_talk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iffzhn/i_created_a_web_ui_for_ollama_that_lets_you_talk/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T20:48:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifkdid</id>
    <title>Run DeepSeek-R1 Locally with Ollama and Open-WebUI (Docker Compose)</title>
    <updated>2025-02-02T00:10:22+00:00</updated>
    <author>
      <name>/u/ntalekt</name>
      <uri>https://old.reddit.com/user/ntalekt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deploy DeepSeek-R1 on your local machine using Ollama and Open-WebUI with this Docker Compose setup. Perfect for those without GPU hardware who want to experiment with AI models.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clone the repo&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;code&gt;git clone&lt;/code&gt; &lt;a href="https://github.com/ntalekt/deepseek-r1-docker-compose.git"&gt;&lt;code&gt;https://github.com/ntalekt/deepseek-r1-docker-compose.git&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Start services&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;code&gt;docker compose up -d&lt;/code&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Access Web UI: &lt;a href="http://localhost:3000/"&gt;http://localhost:3000&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;CPU-only setup (no GPU required)&lt;/li&gt; &lt;li&gt;Automatic download of deepseek-r1:8b model&lt;/li&gt; &lt;li&gt;Easy installation and management with Docker Compose&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Requirements:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Docker Engine v20.10.10+&lt;/li&gt; &lt;li&gt;Docker Compose v2.20.0+&lt;/li&gt; &lt;li&gt;8GB RAM (16GB recommended)&lt;/li&gt; &lt;li&gt;20GB+ free disk space&lt;/li&gt; &lt;li&gt;Linux/macOS/WSL2&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Note:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;CPU inference will be slower than GPU-accelerated setups. Consider GPU hardware for production use.&lt;/li&gt; &lt;li&gt;License: MIT&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full repository: &lt;a href="https://github.com/ntalekt/deepseek-r1-docker-compose"&gt;https://github.com/ntalekt/deepseek-r1-docker-compose&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ntalekt"&gt; /u/ntalekt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifkdid/run_deepseekr1_locally_with_ollama_and_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ifkdid/run_deepseekr1_locally_with_ollama_and_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ifkdid/run_deepseekr1_locally_with_ollama_and_openwebui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-02T00:10:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1if4p38</id>
    <title>Been messing around with DeepSeek R1 + Ollama, and honestly, it's kinda wild how much you can do locally with free open-source tools. No cloud, no API keys, just your machine and some cool AI magic.</title>
    <updated>2025-02-01T11:50:04+00:00</updated>
    <author>
      <name>/u/hasan_py</name>
      <uri>https://old.reddit.com/user/hasan_py</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;Page-Assist Chrome Extension - &lt;a href="https://github.com/n4ze3m/page-assist"&gt;https://github.com/n4ze3m/page-assist&lt;/a&gt; (any model with any params)&lt;/li&gt; &lt;li&gt;Open Web-UI LLM Wrapper - &lt;a href="https://github.com/open-webui/open-webui"&gt;https://github.com/open-webui/open-webui&lt;/a&gt; (any model with any params)&lt;/li&gt; &lt;li&gt;Browser use â€“ &lt;a href="https://github.com/browser-use/browser-use"&gt;https://github.com/browser-use/browser-use&lt;/a&gt; (deepseek r1:14b or more params) &lt;/li&gt; &lt;li&gt;Roo-Code (VS Code Extension) â€“ &lt;a href="https://github.com/RooVetGit/Roo-Code"&gt;https://github.com/RooVetGit/Roo-Code&lt;/a&gt; (deepseek coder)&lt;/li&gt; &lt;li&gt;n8n â€“ &lt;a href="https://github.com/n8n-io/n8n"&gt;https://github.com/n8n-io/n8n&lt;/a&gt; (any model with any params)&lt;/li&gt; &lt;li&gt;A simple RAG app: &lt;a href="https://github.com/hasan-py/chat-with-pdf-RAG"&gt;https://github.com/hasan-py/chat-with-pdf-RAG&lt;/a&gt; (deepseek r1:8b)&lt;/li&gt; &lt;li&gt;Ai assistant Chrome extension: &lt;a href="https://github.com/hasan-py/Ai-Assistant-Chrome-Extension"&gt;https://github.com/hasan-py/Ai-Assistant-Chrome-Extension&lt;/a&gt; (GPT, Gemini, Grok Api, Ollama added recently)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Full installation video: &lt;a href="https://youtu.be/hjg9kJs8al8?si=rillpsKpjONYMDYW"&gt;https://youtu.be/hjg9kJs8al8?si=rillpsKpjONYMDYW&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone exploring something else? Please share- it would be highly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hasan_py"&gt; /u/hasan_py &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if4p38/been_messing_around_with_deepseek_r1_ollama_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1if4p38/been_messing_around_with_deepseek_r1_ollama_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1if4p38/been_messing_around_with_deepseek_r1_ollama_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-01T11:50:04+00:00</published>
  </entry>
</feed>
