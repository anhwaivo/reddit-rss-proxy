<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-18T05:23:58+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jcymiu</id>
    <title>Saving a chat when using command line</title>
    <updated>2025-03-16T23:17:04+00:00</updated>
    <author>
      <name>/u/cbmarketer</name>
      <uri>https://old.reddit.com/user/cbmarketer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just started using Ollama. I am running it from the command line. I'm using Ollama to use the LLMs without giving my data away. Ideally, I want to save a session and be able to come back to it at a later date.&lt;/p&gt; &lt;p&gt;I tried the save &amp;lt;model&amp;gt; command that I got from help, but that didn't seem to work. It didn't confirm anything and I couldn't reload it. Maybe I didn't do it right?&lt;/p&gt; &lt;p&gt;Is this possible or do I need to use a different application?&lt;/p&gt; &lt;p&gt;Thanks in advance for your help,&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cbmarketer"&gt; /u/cbmarketer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcymiu/saving_a_chat_when_using_command_line/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcymiu/saving_a_chat_when_using_command_line/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jcymiu/saving_a_chat_when_using_command_line/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-16T23:17:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcupef</id>
    <title>Image testing + Gemma-3-27B-it-FP16 + torch + 4x AMD Instinct Mi210 Server</title>
    <updated>2025-03-16T20:19:43+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fbqz7sj814pe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcupef/image_testing_gemma327bitfp16_torch_4x_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jcupef/image_testing_gemma327bitfp16_torch_4x_amd/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-16T20:19:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd5xaj</id>
    <title>Is there a self correcting model which can browse the internet for finding errors in code before displaying the final result. Like I want to make a simple web app using streamlit using Gemini but the first shot is incorrect</title>
    <updated>2025-03-17T05:59:42+00:00</updated>
    <author>
      <name>/u/SnooBananas5215</name>
      <uri>https://old.reddit.com/user/SnooBananas5215</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooBananas5215"&gt; /u/SnooBananas5215 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd5xaj/is_there_a_self_correcting_model_which_can_browse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd5xaj/is_there_a_self_correcting_model_which_can_browse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jd5xaj/is_there_a_self_correcting_model_which_can_browse/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T05:59:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd86ia</id>
    <title>???</title>
    <updated>2025-03-17T08:54:50+00:00</updated>
    <author>
      <name>/u/1k-Ping</name>
      <uri>https://old.reddit.com/user/1k-Ping</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why is ollama/deepseek barely using my GPU and mostly using my CPU, but then it isn't even making full utilization of the CPU either??&lt;/p&gt; &lt;p&gt;context: I'm new to running local AI stuff. PC specs are ryzen 5 5600g, rtx 4070-S, 16gb ddr4 ram. Running arch linux (btw).&lt;/p&gt; &lt;p&gt;ollama is fully updated and I'm running deepseek-r1:14b with it.&lt;/p&gt; &lt;p&gt;picture of usage/utilization mid-process: &lt;a href="https://imgur.com/a/qaaVVQJ"&gt;https://imgur.com/a/qaaVVQJ&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1k-Ping"&gt; /u/1k-Ping &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd86ia/_/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd86ia/_/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jd86ia/_/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T08:54:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcmlwt</id>
    <title>What's the closest open source/local ai tool we have of Gemini live? (if any)</title>
    <updated>2025-03-16T14:26:13+00:00</updated>
    <author>
      <name>/u/Game-Lover44</name>
      <uri>https://old.reddit.com/user/Game-Lover44</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jcmlwt/whats_the_closest_open_sourcelocal_ai_tool_we/"&gt; &lt;img alt="What's the closest open source/local ai tool we have of Gemini live? (if any)" src="https://preview.redd.it/h3p7h9l2a2pe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6864826d8d872f3e3c3890b95fe3bd4a4a417f9d" title="What's the closest open source/local ai tool we have of Gemini live? (if any)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Game-Lover44"&gt; /u/Game-Lover44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h3p7h9l2a2pe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcmlwt/whats_the_closest_open_sourcelocal_ai_tool_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jcmlwt/whats_the_closest_open_sourcelocal_ai_tool_we/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-16T14:26:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcyu64</id>
    <title>KubeAI v0.18.0: Load Ollama Models from PVC</title>
    <updated>2025-03-16T23:26:48+00:00</updated>
    <author>
      <name>/u/samosx</name>
      <uri>https://old.reddit.com/user/samosx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've just merged support for loading Ollama models directly from a Persistent Volume Claim (PVC) into KubeAI v0.18.0. This allows you to manage and persist Ollama models more easily in Kubernetes environments. This is especially useful for when you want fast scale ups of the same model.&lt;/p&gt; &lt;p&gt;See the &lt;a href="https://github.com/substratusai/kubeai/pull/416"&gt;GitHub PR&lt;/a&gt; and &lt;a href="https://www.kubeai.org/how-to/load-models-from-pvc/#ollama"&gt;user docs&lt;/a&gt; for more info.&lt;/p&gt; &lt;p&gt;Feedback and questions are welcome!&lt;/p&gt; &lt;p&gt;Link to GitHub: &lt;a href="https://github.com/substratusai/kubeai"&gt;https://github.com/substratusai/kubeai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samosx"&gt; /u/samosx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcyu64/kubeai_v0180_load_ollama_models_from_pvc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcyu64/kubeai_v0180_load_ollama_models_from_pvc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jcyu64/kubeai_v0180_load_ollama_models_from_pvc/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-16T23:26:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd7r05</id>
    <title>Gemma 3 code interpreter for open webui</title>
    <updated>2025-03-17T08:19:48+00:00</updated>
    <author>
      <name>/u/Ok_Green5623</name>
      <uri>https://old.reddit.com/user/Ok_Green5623</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used some tutorial to run gemma 3 via ollama and open-webui and there was an interesting option to give it access to code interpreter, but it didn't quite work out of the box. I hacked a bit default code interpreter prompt and it worked pretty nicely. Here is prompt I quickly hacked. Does anyone have a better version?&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;h4&gt;Tools Available&lt;/h4&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Code Interpreter&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;You have access to a Python shell that runs directly in the user's browser, enabling fast execution of code for analysis, calculations, or problem-solving. Use it in this response.&lt;/li&gt; &lt;li&gt;The code will be executed and the result will be visible only for you to assist you with your answer to user query. It is quite different from the normal nicely formated python code you wrap in &lt;code&gt;python ...&lt;/code&gt; formating.&lt;/li&gt; &lt;li&gt;In order to use the code interpreter you have to enclose the code to be executed in &amp;lt;code_interpreter type=&amp;quot;code&amp;quot; lang=&amp;quot;python&amp;quot;&amp;gt;&amp;lt;/code_interpreter&amp;gt; instead. In example: &amp;lt;code_interpreter type=&amp;quot;code&amp;quot; lang=&amp;quot;python&amp;quot;&amp;gt; print(&amp;quot;Hello world!&amp;quot;) &amp;lt;code_interpreter&amp;gt;&lt;/li&gt; &lt;li&gt;Notice, missing in this case &amp;quot;&lt;code&gt;python&amp;quot; and &amp;quot;&lt;/code&gt;&amp;quot;. This allows to hide the code from user completely.&lt;/li&gt; &lt;li&gt;The Python code you write can incorporate a wide array of libraries, handle data manipulation or visualization, perform API calls for web-related tasks, or tackle virtually any computational challenge. Use this flexibility to &lt;strong&gt;think outside the box, craft elegant solutions, and harness Python's full potential&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;When using the code wrap it in the xml tags described above and stop your response. If you don't, the code won't execute. After the code finished execution you will have access to the output of the code. Continue your response using the information obtained.&lt;/li&gt; &lt;li&gt;When coding, &lt;strong&gt;always aim to print meaningful outputs&lt;/strong&gt; (e.g., results, tables, summaries, or visuals) to better interpret and verify the findings. Avoid relying on implicit outputs; prioritize explicit and clear print statements so the results are effectively communicated to the user.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;After obtaining the printed output, &lt;strong&gt;always provide a concise analysis, interpretation, or next steps to help the user understand the findings or refine the outcome further.&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;If the results are unclear, unexpected, or require validation, refine the code and execute it again as needed. Always aim to deliver meaningful insights from the results, iterating if necessary.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;If a link to an image, audio, or any file is provided in markdown format in the output, ALWAYS regurgitate word for word, explicitly display it as part of the response to ensure the user can access it easily, do NOT change the link.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;All responses should be communicated in the chat's primary language, ensuring seamless understanding. If the chat is multilingual, default to English for clarity.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Ensure that the tools are effectively utilized to achieve the highest-quality analysis for the user. ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Green5623"&gt; /u/Ok_Green5623 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd7r05/gemma_3_code_interpreter_for_open_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd7r05/gemma_3_code_interpreter_for_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jd7r05/gemma_3_code_interpreter_for_open_webui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T08:19:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcuung</id>
    <title>I built a vision-native RAG pipeline</title>
    <updated>2025-03-16T20:26:10+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My brother and I have been working on &lt;a href="https://github.com/databridge-org/databridge-core"&gt;DataBridge&lt;/a&gt;: an open-source and multimodal database. After experimenting with various AI models, we realized that they were particularly bad at answering questions which required retrieving over images and other multimodal data.&lt;/p&gt; &lt;p&gt;That is, if I uploaded a 10-20 page PDF to ChatGPT, and ask it to get me a result from a particular diagram in the PDF, it would fail and hallucinate instead. I faced the same issue with Claude, but not with Gemini.&lt;/p&gt; &lt;p&gt;Turns out, the issue was with how these systems ingest documents. Seems like both Claude and GPT embed larger PDFs by parsing them into text, and then adding the entire thing to the context of the chat. While this works for text-heavy documents, it fails for queries/documents relating to diagrams, graphs, or infographics.&lt;/p&gt; &lt;p&gt;Something that can help solve this is directly embedding the document as a list of images, and performing retrieval over that - getting the closest images to the query, and feeding the LLM exactly those images. This helps reduce the amount of tokens an LLM consumes while also increasing the visual reasoning ability of the model.&lt;/p&gt; &lt;p&gt;We've implemented a &lt;a href="https://databridge.mintlify.app/concepts/colpali"&gt;one-line solution&lt;/a&gt; that does exactly this with DataBridge. You can check out the specifics in the attached blog, or get started with it through our quick start guide: &lt;a href="https://databridge.mintlify.app/getting-started"&gt;https://databridge.mintlify.app/getting-started&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcuung/i_built_a_visionnative_rag_pipeline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jcuung/i_built_a_visionnative_rag_pipeline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jcuung/i_built_a_visionnative_rag_pipeline/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-16T20:26:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdhomj</id>
    <title>Embeddings API and OpenWebUI not working?</title>
    <updated>2025-03-17T17:01:47+00:00</updated>
    <author>
      <name>/u/UpYourQuality</name>
      <uri>https://old.reddit.com/user/UpYourQuality</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can anyone help me out? Not sure why but my embedding models arent being detected. RTX 3060 12gb of VRAM&lt;/p&gt; &lt;p&gt;THE ERROR: &lt;strong&gt;&amp;quot;generating ollama batch embeddings: 404 Client Error: Not Found for url:&lt;/strong&gt; &lt;a href="http://host.docker.internal:11434//api/embed"&gt;&lt;strong&gt;http://host.docker.internal:11434//api/embed&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;- {}&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# ollama --version ollama version is 0.6.1 # ollama list NAME ID SIZE MODIFIED nomic-embed-text:latest 0a109f422b47 274 MB 10 hours ago mxbai-embed-large:latest 468836162de7 669 MB 10 hours ago linux6200/bge-reranker-v2-m3:latest abf5c6d8bc56 1.2 GB 10 hours ago superdrew100/llama3-abliterated:latest ced7cf645b4e 4.9 GB 11 hours ago PetrosStav/gemma3-tools:4b 6ff5f78db582 3.3 GB 12 hours ago koesn/mistral-7b-instruct:latest 39d8715d6a19 4.1 GB 12 hours ago gemma3:1b 2d27a774bc62 815 MB 12 hours ago Abhisar2006/Ananya:latest b335c81b1097 4.7 GB 12 hours ago monotykamary/whiterabbitneo-v1.5a:latest 64a30974ef61 4.1 GB 14 hours ago mxbai-embed-large:335m-v1-fp16 468836162de7 669 MB 14 hours ago huihui_ai/granite3.1-dense-abliterated:latest 6567faf671e9 4.9 GB 14 hours ago huihui_ai/dolphin3-abliterated:latest 669007b377b4 4.9 GB 14 hours ago huihui_ai/skywork-o1-abliterated:latest 67c14ead13ce 4.9 GB 14 hours ago mistral:latest f974a74358d6 4.1 GB 14 hours ago phi3:latest 4f2222927938 2.2 GB 14 hours ago # ollama ps NAME ID SIZE PROCESSOR UNTIL # &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;nomic-embed-text and mxbai-embed-large are the two models ive been trying. &lt;/p&gt; &lt;p&gt;The Error in the LOG&lt;/p&gt; &lt;pre&gt;&lt;code&gt;open-webui | 2025-03-17 16:36:25.196 | ERROR | open_webui.retrieval.utils:generate_ollama_batch_embeddings:618 - Error generating ollama batch embeddings: 404 Client Error: Not Found for url: http://host.docker.internal:11434//api/embed - {} open-webui | Traceback (most recent call last): open-webui | open-webui | File &amp;quot;/usr/local/lib/python3.11/threading.py&amp;quot;, line 1002, in _bootstrap open-webui | self._bootstrap_inner() open-webui | │ └ &amp;lt;function Thread._bootstrap_inner at 0x7ff45075c860&amp;gt; open-webui | └ &amp;lt;WorkerThread(AnyIO worker thread, started 140684732593856)&amp;gt; open-webui | File &amp;quot;/usr/local/lib/python3.11/threading.py&amp;quot;, line 1045, in _bootstrap_inner open-webui | self.run() open-webui | │ └ &amp;lt;function WorkerThread.run at 0x7ff3cecfe980&amp;gt; open-webui | └ &amp;lt;WorkerThread(AnyIO worker thread, started 140684732593856)&amp;gt; open-webui | File &amp;quot;/usr/local/lib/python3.11/site-packages/anyio/_backends/_asyncio.py&amp;quot;, line 967, in run open-webui | result = context.run(func, *args) open-webui | │ │ │ └ () open-webui | │ │ └ functools.partial(&amp;lt;function upload_file at 0x7ff417e6dc60&amp;gt;, user=UserModel(id='d8512889-99d3-45a8-8371-bc7f16fb163f', name='T... open-webui | │ └ &amp;lt;method 'run' of '_contextvars.Context' objects&amp;gt; open-webui | └ &amp;lt;_contextvars.Context object at 0x7ff38823bc00&amp;gt; open-webui | open-webui | File &amp;quot;/app/backend/open_webui/routers/files.py&amp;quot;, line 85, in upload_file open-webui | process_file(request, ProcessFileForm(file_id=id), user=user) open-webui | │ │ │ │ └ UserModel(id='d8512889-99d3-45a8-8371-bc7f16fb163f', name='Teon Moore', email='admin@cyberautomations.com', role='admin', pro... open-webui | │ │ │ └ '086452fc-6b10-41a5-8c3a-6a79ec84eae1' open-webui | │ │ └ &amp;lt;class 'open_webui.routers.retrieval.ProcessFileForm'&amp;gt; open-webui | │ └ &amp;lt;starlette.requests.Request object at 0x7ff39131f5d0&amp;gt; open-webui | └ &amp;lt;function process_file at 0x7ff4149158a0&amp;gt; open-webui | open-webui | File &amp;quot;/app/backend/open_webui/routers/retrieval.py&amp;quot;, line 1040, in process_file open-webui | result = save_docs_to_vector_db( open-webui | └ &amp;lt;function save_docs_to_vector_db at 0x7ff414a137e0&amp;gt; open-webui | open-webui | File &amp;quot;/app/backend/open_webui/routers/retrieval.py&amp;quot;, line 883, in save_docs_to_vector_db open-webui | embeddings = embedding_function( open-webui | └ &amp;lt;function get_embedding_function.&amp;lt;locals&amp;gt;.&amp;lt;lambda&amp;gt; at 0x7ff3ceb07880&amp;gt; open-webui | open-webui | File &amp;quot;/app/backend/open_webui/retrieval/utils.py&amp;quot;, line 336, in &amp;lt;lambda&amp;gt; open-webui | return lambda query, user=None: generate_multiple(query, user, func) open-webui | │ │ │ │ └ &amp;lt;function get_embedding_function.&amp;lt;locals&amp;gt;.&amp;lt;lambda&amp;gt; at 0x7ff39891e2a0&amp;gt; open-webui | │ │ │ └ UserModel(id='d8512889-99d3-45a8-8371-bc7f16fb163f', name='Teon Moore', email='admin@cyberautomations.com', role='admin', pro... open-webui | │ │ └ ['VideoURL;VideoTitle;VideoLength;videotags;videocategory;Videoqualityhttps://www.xvideos.com/video.ukufhi1ec2/nenas_besandos... open-webui | │ └ &amp;lt;function get_embedding_function.&amp;lt;locals&amp;gt;.generate_multiple at 0x7ff3d1233240&amp;gt; open-webui | └ ['VideoURL;VideoTitle;VideoLength;videotags;videocategory;Videoqualityhttps://www.xvideos.com/video.ukufhi1ec2/nenas_besandos... open-webui | open-webui | File &amp;quot;/app/backend/open_webui/retrieval/utils.py&amp;quot;, line 330, in generate_multiple open-webui | func(query[i : i + embedding_batch_size], user=user) open-webui | │ │ │ │ │ └ UserModel(id='d8512889-99d3-45a8-8371-bc7f16fb163f', name='Teon Moore', email='admin@cyberautomations.com', role='admin', pro... open-webui | │ │ │ │ └ 15 open-webui | │ │ │ └ 0 open-webui | │ │ └ 0 open-webui | │ └ ['VideoURL;VideoTitle;VideoLength;videotags;videocategory;Videoqualityhttps://www.xvideos.com/video.ukufhi1ec2/nenas_besandos... open-webui | └ &amp;lt;function get_embedding_function.&amp;lt;locals&amp;gt;.&amp;lt;lambda&amp;gt; at 0x7ff39891e2a0&amp;gt; open-webui | open-webui | File &amp;quot;/app/backend/open_webui/retrieval/utils.py&amp;quot;, line 316, in &amp;lt;lambda&amp;gt; open-webui | func = lambda query, user=None: generate_embeddings( open-webui | │ └ &amp;lt;function generate_embeddings at 0x7ff414a1b740&amp;gt; open-webui | └ ['VideoURL;VideoTitle;VideoLength;videotags;videocategory;Videoqualityhttps://www.xvideos.com/video.ukufhi1ec2/nenas_besandos... open-webui | open-webui | File &amp;quot;/app/backend/open_webui/retrieval/utils.py&amp;quot;, line 629, in generate_embeddings open-webui | embeddings = generate_ollama_batch_embeddings( open-webui | └ &amp;lt;function generate_ollama_batch_embeddings at 0x7ff414a1b6a0&amp;gt; open-webui | open-webui | &amp;gt; File &amp;quot;/app/backend/open_webui/retrieval/utils.py&amp;quot;, line 610, in generate_ollama_batch_embeddings open-webui | r.raise_for_status() open-webui | │ └ &amp;lt;function Response.raise_for_status at 0x7ff44d095120&amp;gt; open-webui | └ &amp;lt;Response [404]&amp;gt; open-webui | open-webui | File &amp;quot;/usr/local/lib/python3.11/site-packages/requests/models.py&amp;quot;, line 1024, in raise_for_status open-webui | raise HTTPError(http_error_msg, response=self) open-webui | │ │ └ &amp;lt;Response [404]&amp;gt; open-webui | │ └ '404 Client Error: Not Found for url: http://host.docker.internal:11434//api/embed' open-webui | └ &amp;lt;class 'requests.exceptions.HTTPError'&amp;gt; open-webui | open-webui | requests.exceptions.HTTPError: 404 Client Error: Not Found for url: http://host.docker.internal:11434//api/embed open-webui | 2025-03-17 16:36:25.497 | ERROR | open_webui.routers.retrieval:save_docs_to_vector_db:904 - 'NoneType' object is not iterable - {} open-webui | Traceback (most recent call last): open-webui | open-webui | File &amp;quot;/usr/local/lib/python3.11/threading.py&amp;quot;, line 1002, in _bootstrap open-webui | self._bootstrap_inner() open-webui | │ └ &amp;lt;function Thread._bootstrap_inner at 0x7ff45075c860&amp;gt; open-webui | └ &amp;lt;WorkerThread(AnyIO worker thread, started 140684732593856)&amp;gt; open-webui | File &amp;quot;/usr/local/lib/python3.11/threading.py&amp;quot;, line 1045, in _bootstrap_inner open-webui | self.run() open-webui | │ └ &amp;lt;function WorkerThread.run at 0x7ff3cecfe980&amp;gt; open-webui | └ &amp;lt;WorkerThread(AnyIO worker thread, started 140684732593856)&amp;gt; open-webui | File &amp;quot;/usr/local/lib/python3.11/site-packages/anyio/_backends/_asyncio.py&amp;quot;, line 967, in run open-webui | result = context.run(func, *args) open-webui | │ │ │ └ () open-webui | │ │ └ functools.partial(&amp;lt;function upload_file at 0x7ff417e6dc60&amp;gt;, user=UserModel(id='d8512889-99d3-45a8-8371-bc7f16fb163f', name='T... open-webui | │ └ &amp;lt;method 'run' of '_contextvars.Context' objects&amp;gt; open-webui | └ &amp;lt;_contextvars.Context object at 0x7ff38823bc00&amp;gt; open-webui | open-webui | File &amp;quot;/app/backend/open_webui/routers/files.py&amp;quot;, line 85, in upload_file open-webui | process_file(request, ProcessFileForm(file_id=id), user=user) open-webui | │ │ │ │ └ UserModel(id='d8512889-99d3-45a8-8371-bc7f16fb163f', name='Teon Moore', email='admin@cyberautomations.com', role='admin', pro... open-webui | │ │ │ └ '086452fc-6b10-41a5-8c3a-6a79ec84eae1' open-webui | │ │ └ &amp;lt;class 'open_webui.routers.retrieval.ProcessFileForm'&amp;gt; open-webui | │ └ &amp;lt;starlette.requests.Request object at 0x7ff39131f5d0&amp;gt; open-webui | └ &amp;lt;function process_file at 0x7ff4149158a0&amp;gt; open-webui | open-webui | File &amp;quot;/app/backend/open_webui/routers/retrieval.py&amp;quot;, line 1040, in process_file open-webui | result = save_docs_to_vector_db( open-webui | └ &amp;lt;function save_docs_to_vector_db at 0x7ff414a137e0&amp;gt; open-webui | open-webui | &amp;gt; File &amp;quot;/app/backend/open_webui/routers/retrieval.py&amp;quot;, line 883, in save_docs_to_vector_db open-webui | embeddings = embedding_function( open-webui | └ &amp;lt;function get_embedding_function.&amp;lt;locals&amp;gt;.&amp;lt;lambda&amp;gt; at 0x7ff3ceb07880&amp;gt; open-webui | open-webui | File &amp;quot;/app/backend/open_webui/retrieval/utils.py&amp;quot;, line 336, in &amp;lt;lambda&amp;gt; open-webui | return lambda query, user=None: generate_multiple(query, user, func) open-webui | │ │ │ │ └ &amp;lt;function get_embedding_function.&amp;lt;locals&amp;gt;.&amp;lt;lambda&amp;gt; at 0x7ff39891e2a0&amp;gt; open-webui | │ │ │ └ UserModel(id='d8512889-99d3-45a8-8371-bc7f16fb163f', name='Teon Moore', email='admin@cyberautomations.com', role='admin', pro... open-webui | │ │ └ ['VideoURL;VideoTitle;VideoLength;videotags;videocategory;Videoqualityhttps://www.xvideos.com/video.ukufhi1ec2/nenas_besandos... open-webui | │ └ &amp;lt;function get_embedding_function.&amp;lt;locals&amp;gt;.generate_multiple at 0x7ff3d1233240&amp;gt; open-webui | └ ['VideoURL;VideoTitle;VideoLength;videotags;videocategory;Videoqualityhttps://www.xvideos.com/video.ukufhi1ec2/nenas_besandos... open-webui | open-webui | File &amp;quot;/app/backend/open_webui/retrieval/utils.py&amp;quot;, line 329, in generate_multiple open-webui | embeddings.extend( open-webui | │ └ &amp;lt;method 'extend' of 'list' objects&amp;gt; open-webui | └ [] open-webui | open-webui | TypeError: 'NoneType' object is not iterable open-webui | 2025-03-17 16:36:25.696 | ERROR | open_webui.routers.retrieval:process_file:1078 - 'NoneType' object is not iterable - {} open-webui | Traceback (most recent call last): open-webui | open-webui | File &amp;quot;/usr/local/lib/python3.11/threading.py&amp;quot;, line 1002, in _bootstrap open-webui | self._bootstrap_inner() open-webui | │ └ &amp;lt;function Thread._bootstrap_inner at 0x7ff45075c860&amp;gt; open-webui | └ &amp;lt;WorkerThread(AnyIO worker thread, started 140684732593856)&amp;gt; open-webui | File &amp;quot;/usr/local/lib/python3.11/threading.py&amp;quot;, line 1045, in _bootstrap_inner open-webui | self.run() open-webui | │ └ &amp;lt;function WorkerThread.run at 0x7ff3cecfe980&amp;gt; open-webui | └ &amp;lt;WorkerThread(AnyIO worker thread, started 140684732593856)&amp;gt; open-webui | File &amp;quot;/usr/local/lib/python3.11/site-packages/anyio/_backends/_asyncio.py&amp;quot;, line 967, in run open-webui | result = context.run(func, *args) open-webui | │ │ │ └ () open-webui | │ │ └ functools.partial(&amp;lt;function upload_file at 0x7ff417e6dc60&amp;gt;, user=UserModel(id='d8512889-99d3-45a8-8371-bc7f16fb163f', name='T... open-webui | │ └ &amp;lt;method 'run' of '_contextvars.Context' objects&amp;gt; open-webui | └ &amp;lt;_contextvars.Context object at 0x7ff38823bc00&amp;gt; open-webui | open-webui | File &amp;quot;/app/backend/open_webui/routers/files.py&amp;quot;, line 85, in upload_file open-webui | process_file(request, ProcessFileForm(file_id=id), user=user) open-webui | │ │ │ │ └ UserModel(id='d8512889-99d3-45a8-8371-bc7f16fb163f', name='Teon Moore', email='admin@cyberautomations.com', role='admin', pro... open-webui | │ │ │ └ '086452fc-6b10-41a5-8c3a-6a79ec84eae1' open-webui | │ │ └ &amp;lt;class 'open_webui.routers.retrieval.ProcessFileForm'&amp;gt; open-webui | │ └ &amp;lt;starlette.requests.Request object at 0x7ff39131f5d0&amp;gt; open-webui | └ &amp;lt;function process_file at 0x7ff4149158a0&amp;gt; open-webui | open-webui | &amp;gt; File &amp;quot;/app/backend/open_webui/routers/retrieval.py&amp;quot;, line 1068, in process_file open-webui | raise e open-webui | └ TypeError(&amp;quot;'NoneType' object is not iterable&amp;quot;) open-webui | open-webui | File &amp;quot;/app/backend/open_webui/routers/retrieval.py&amp;quot;, line 1040, in process_file open-webui | result = save_docs_to_vector_db( open-webui | └ &amp;lt;function save_docs_to_vector_db at 0x7ff414a137e0&amp;gt; open-webui | open-webui | File &amp;quot;/app/backend/open_webui/routers/retrieval.py&amp;quot;, line 905, in save_docs_to_vector_db open-webui | raise e open-webui | open-webui | File &amp;quot;/app/backend/open_webui/routers/retrieval.py&amp;quot;, line 883, in save_docs_to_vector_db open-webui | embeddings = embedding_function( open-webui | └ &amp;lt;function get_embedding_function.&amp;lt;locals&amp;gt;.&amp;lt;lambda&amp;gt; at 0x7ff3ceb07880&amp;gt; open-webui | open-webui | File &amp;quot;/app/backend/open_webui/retrieval/utils.py&amp;quot;, line 336, in &amp;lt;lambda&amp;gt; open-webui | return lambda query, user=None: generate_multiple(query, user, func) open-webui | │ │ │ │ └ &amp;lt;function get_embedding_function.&amp;lt;locals&amp;gt;.&amp;lt;lambda&amp;gt; at 0x7ff39891e2a0&amp;gt; open-webui | │ │ │ └ UserModel(id='d8512889-99d3-45a8-8371-bc7f16fb163f', name='Teon Moore', email='admin@cyberautomations.com', role='admin', pro... open-webui | │ │ └ ['VideoURL;VideoTitle;VideoLength;videotags;videocategory;Videoqualityhttps://www.xvideos.com/video.ukufhi1ec2/nenas_besandos... open-webui | │ └ &amp;lt;function get_embedding_function.&amp;lt;locals&amp;gt;.generate_multiple at 0x7ff3d1233240&amp;gt; open-webui | └ ['VideoURL;VideoTitle;VideoLength;videotags;videocategory;Videoqualityhttps://www.xvideos.com/video.ukufhi1ec2/nenas_besandos... open-webui | open-webui | File &amp;quot;/app/backend/open_webui/retrieval/utils.py&amp;quot;, line 329, in generate_multiple open-webui | embeddings.extend( open-webui | │ └ &amp;lt;method 'extend' of 'list' objects&amp;gt; open-webui | └ [] open-webui | open-webui | TypeError: 'NoneType' object is not iterable open-webui | 2025-03-17 16:36:25.884 | ERROR | open_webui.routers.files:upload_file:89 - 400: 'NoneType' object is not iterable - {} open-webui | Traceback (most recent call last): open-webui | open-webui | File &amp;quot;/app/backend/open_webui/routers/retrieval.py&amp;quot;, line 1068, in process_file open-webui | raise e open-webui | open-webui | File &amp;quot;/app/backend/open_webui/routers/retrieval.py&amp;quot;, line 1040, in process_file open-webui | result = save_docs_to_vector_db( open-webui | └ &amp;lt;function save_docs_to_vector_db at 0x7ff414a137e0&amp;gt; open-webui | open-webui | File &amp;quot;/app/backend/open_webui/routers/retrieval.py&amp;quot;, line 905, in save_docs_to_vector_db open-webui | raise e open-webui | open-webui | File &amp;quot;/app/backend/open_webui/routers/retrieval.py&amp;quot;, line 883, in save_docs_to_vector_db open-webui | embeddings = embedding_function( open-webui | └ &amp;lt;function get_embedding_function.&amp;lt;locals&amp;gt;.&amp;lt;lambda&amp;gt; at 0x7ff3ceb07880&amp;gt; open-webui | open-webui | File &amp;quot;/app/backend/open_webui/retrieval/utils.py&amp;quot;, line 336, in &amp;lt;lambda&amp;gt; open-webui | return lambda query, user=None: generate_multiple(query, user, func) open-webui | │ │ │ │ └ &amp;lt;function get_embedding_function.&amp;lt;locals&amp;gt;.&amp;lt;lambda&amp;gt; at 0x7ff39891e2a0&amp;gt; open-webui | │ │ │ └ UserModel(id='d8512889-99d3-45a8-8371-bc7f16fb163f', name='Teon Moore', email='admin@cyberautomations.com', role='admin', pro... open-webui | │ │ └ ['VideoURL;VideoTitle;VideoLength;videotags;videocategory;Videoqualityhttps://www.xvideos.com/video.ukufhi1ec2/nenas_besandos... open-webui | │ └ &amp;lt;function get_embedding_function.&amp;lt;locals&amp;gt;.generate_multiple at 0x7ff3d1233240&amp;gt; open-webui | └ ['VideoURL;VideoTitle;VideoLength;videotags;videocategory;Videoqualityhttps://www.xvideos.com/video.ukufhi1ec2/nenas_besandos... open-webui | open-webui | File &amp;quot;/app/backend/open_webui/retrieval/utils.py&amp;quot;, line 329, in generate_multiple open-webui | embeddings.extend( open-webui | │ └ &amp;lt;method 'extend' of 'list' objects&amp;gt; open-webui | └ [] open-webui | open-webui | TypeError: 'NoneType' object is not iterable open-webui | open-webui | open-webui | During handling of the above exception, another exception occurred: open-webui | open-webui | open-webui | Traceback (most recent call last): open-webui | open-webui | File &amp;quot;/usr/local/lib/python3.11/threading.py&amp;quot;, line 1002, in _bootstrap open-webui | self._bootstrap_inner() open-webui | │ └ &amp;lt;function Thread._bootstrap_inner at 0x7ff45075c860&amp;gt; open-webui | └ &amp;lt;WorkerThread(AnyIO worker thread, started 140684732593856)&amp;gt; open-webui | File &amp;quot;/usr/local/lib/python3.11/threading.py&amp;quot;, line 1045, in _bootstrap_inner open-webui | self.run() open-webui | │ └ &amp;lt;function WorkerThread.run at 0x7ff3cecfe980&amp;gt; open-webui | └ &amp;lt;WorkerThread(AnyIO worker thread, started 140684732593856)&amp;gt; open-webui | File &amp;quot;/usr/local/lib/python3.11/site-packages/anyio/_backends/_asyncio.py&amp;quot;, line 967, in run open-webui | result = context.run(func, *args) open-webui | │ │ │ └ () open-webui | │ │ └ functools.partial(&amp;lt;function upload_file at 0x7ff417e6dc60&amp;gt;, user=UserModel(id='d8512889-99d3-45a8-8371-bc7f16fb163f', name='T... open-webui | │ └ &amp;lt;method 'run' of '_contextvars.Context' objects&amp;gt; open-webui | └ &amp;lt;_contextvars.Context object at 0x7ff38823bc00&amp;gt; open-webui | open-webui | &amp;gt; File &amp;quot;/app/backend/open_webui/routers/files.py&amp;quot;, line 85, in upload_file open-webui | process_file(request, ProcessFileForm(file_id=id), user=user) open-webui | │ │ │ │ └ UserModel(id='d8512889-99d3-45a8-8371-bc7f16fb163f', name='Teon Moore', email='admin@cyberautomations.com', role='admin', pro... open-webui | │ │ │ └ '086452fc-6b10-41a5-8c3a-6a79ec84eae1' open-webui | │ │ └ &amp;lt;class 'open_webui.routers.retrieval.ProcessFileForm'&amp;gt; open-webui | │ └ &amp;lt;starlette.requests.Request object at 0x7ff39131f5d0&amp;gt; open-webui | └ &amp;lt;function process_file at 0x7ff4149158a0&amp;gt; open-webui | open-webui | File &amp;quot;/app/backend/open_webui/routers/retrieval.py&amp;quot;, line 1085, in process_file open-webui | raise HTTPException( open-webui | └ &amp;lt;class 'fastapi.exceptions.HTTPException'&amp;gt; open-webui | open-webui | fastapi.exceptions.HTTPException: 400: 'NoneType' object is not iterable open-webui | 2025-03-17 16:36:26.075 | ERROR | open_webui.routers.files:upload_file:90 - Error processing file: 086452fc-6b10-41a5-8c3a-6a79ec84eae1 - {} open-webui | 2025-03-17 16:36:26.089 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - 172.18.0.1:35234 - &amp;quot;POST /api/v1/files/ HTTP/1.1&amp;quot; 200 - {} open-webui | 2025-03-17 16:36:26.633 | INFO | open_webui.routers.retrieval:save_docs_to_vector_db:782 - save_docs_to_vector_db: documen &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UpYourQuality"&gt; /u/UpYourQuality &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdhomj/embeddings_api_and_openwebui_not_working/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdhomj/embeddings_api_and_openwebui_not_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdhomj/embeddings_api_and_openwebui_not_working/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T17:01:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdip6n</id>
    <title>Auto-updating Ollama on Debian 12</title>
    <updated>2025-03-17T17:41:46+00:00</updated>
    <author>
      <name>/u/GVDub2</name>
      <uri>https://old.reddit.com/user/GVDub2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally got around to setting up a weekly cron job to keep Ollama updated on my Debian 12 AI server. &lt;/p&gt; &lt;p&gt;&lt;code&gt;#!/bin/bash&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Simple Ollama updater using the official install script&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Log file location&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;LOG_FILE=&amp;quot;/var/log/ollama_update.log&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Get current version before update&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;CURRENT_VERSION=$(ollama --version 2&amp;gt;&amp;amp;1 | grep -Po 'ollama version \K.*' || echo &amp;quot;not installed&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;echo &amp;quot;$(date): Starting Ollama update check. Current version: $CURRENT_VERSION&amp;quot; &amp;gt;&amp;gt; $LOG_FILE&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Run the official installer&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;curl -fsSL&lt;/code&gt; &lt;a href="https://ollama.com/install.sh"&gt;&lt;code&gt;https://ollama.com/install.sh&lt;/code&gt;&lt;/a&gt; &lt;code&gt;| sh&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Get new version after update&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;NEW_VERSION=$(ollama --version 2&amp;gt;&amp;amp;1 | grep -Po 'ollama version \K.*')&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;if [ &amp;quot;$CURRENT_VERSION&amp;quot; != &amp;quot;$NEW_VERSION&amp;quot; ]; then&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;echo &amp;quot;$(date): Ollama updated from $CURRENT_VERSION to $NEW_VERSION&amp;quot; &amp;gt;&amp;gt; $LOG_FILE&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Restart the service to ensure it's using the new version&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;systemctl restart ollama&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;echo &amp;quot;$(date): Ollama service restarted&amp;quot; &amp;gt;&amp;gt; $LOG_FILE&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;else&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;echo &amp;quot;$(date): No update needed. Version remains $CURRENT_VERSION&amp;quot; &amp;gt;&amp;gt; $LOG_FILE&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;fi&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Optional: Update your commonly used models&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Uncomment and customize these lines when ready&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# echo &amp;quot;$(date): Updating frequently used models&amp;quot; &amp;gt;&amp;gt; $LOG_FILE&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# ollama pull model1 &amp;gt;&amp;gt; $LOG_FILE 2&amp;gt;&amp;amp;1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# ollama pull model2 &amp;gt;&amp;gt; $LOG_FILE 2&amp;gt;&amp;amp;1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Just save it in etc/cron.wee/kly/update-ollama and make it executable with chmod +x /etc/cron/wee/kly/update-ollama. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GVDub2"&gt; /u/GVDub2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdip6n/autoupdating_ollama_on_debian_12/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdip6n/autoupdating_ollama_on_debian_12/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdip6n/autoupdating_ollama_on_debian_12/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T17:41:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jde0cw</id>
    <title>Gemma 3, warnings in Ollama server log</title>
    <updated>2025-03-17T14:31:17+00:00</updated>
    <author>
      <name>/u/Lodurr242</name>
      <uri>https://old.reddit.com/user/Lodurr242</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am playing around with some Gemma 3 models, regardless of size, source or specific quantization (pull from ollama.com or HF), I notice a lot of warnings in the Ollama server logs:&lt;/p&gt; &lt;p&gt;time=2025-03-17T15:16:23.915+01:00 level=WARN source=ggml.go:149 msg=&amp;quot;key not found&amp;quot; key=tokenizer.ggml.pretokenizer default=&amp;quot;(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+&amp;quot;&lt;/p&gt; &lt;p&gt;time=2025-03-17T15:16:23.916+01:00 level=WARN source=ggml.go:149 msg=&amp;quot;key not found&amp;quot; key=tokenizer.ggml.add_eot_token default=false&lt;/p&gt; &lt;p&gt;time=2025-03-17T15:16:23.918+01:00 level=WARN source=ggml.go:149 msg=&amp;quot;key not found&amp;quot; key=gemma3.vision.image_size default=0&lt;/p&gt; &lt;p&gt;time=2025-03-17T15:16:23.918+01:00 level=WARN source=ggml.go:149 msg=&amp;quot;key not found&amp;quot; key=gemma3.vision.patch_size default=0&lt;/p&gt; &lt;p&gt;time=2025-03-17T15:16:23.918+01:00 level=WARN source=ggml.go:149 msg=&amp;quot;key not found&amp;quot; key=gemma3.vision.num_channels default=0&lt;/p&gt; &lt;p&gt;time=2025-03-17T15:16:23.918+01:00 level=WARN source=ggml.go:149 msg=&amp;quot;key not found&amp;quot; key=gemma3.vision.block_count default=0&lt;/p&gt; &lt;p&gt;time=2025-03-17T15:16:23.918+01:00 level=WARN source=ggml.go:149 msg=&amp;quot;key not found&amp;quot; key=gemma3.vision.embedding_length default=0&lt;/p&gt; &lt;p&gt;time=2025-03-17T15:16:23.918+01:00 level=WARN source=ggml.go:149 msg=&amp;quot;key not found&amp;quot; key=gemma3.vision.attention.head_count default=0&lt;/p&gt; &lt;p&gt;time=2025-03-17T15:16:23.918+01:00 level=WARN source=ggml.go:149 msg=&amp;quot;key not found&amp;quot; key=gemma3.vision.image_size default=0&lt;/p&gt; &lt;p&gt;time=2025-03-17T15:16:23.918+01:00 level=WARN source=ggml.go:149 msg=&amp;quot;key not found&amp;quot; key=gemma3.vision.patch_size default=0&lt;/p&gt; &lt;p&gt;time=2025-03-17T15:16:23.918+01:00 level=WARN source=ggml.go:149 msg=&amp;quot;key not found&amp;quot; key=gemma3.vision.attention.layer_norm_epsilon default=0&lt;/p&gt; &lt;p&gt;time=2025-03-17T15:16:23.920+01:00 level=WARN source=ggml.go:149 msg=&amp;quot;key not found&amp;quot; key=gemma3.rope.local.freq_base default=10000&lt;/p&gt; &lt;p&gt;time=2025-03-17T15:16:23.920+01:00 level=WARN source=ggml.go:149 msg=&amp;quot;key not found&amp;quot; key=gemma3.rope.global.freq_base default=1e+06&lt;/p&gt; &lt;p&gt;time=2025-03-17T15:16:23.920+01:00 level=WARN source=ggml.go:149 msg=&amp;quot;key not found&amp;quot; key=gemma3.rope.freq_scale default=1&lt;/p&gt; &lt;p&gt;time=2025-03-17T15:16:23.920+01:00 level=WARN source=ggml.go:149 msg=&amp;quot;key not found&amp;quot; key=gemma3.mm_tokens_per_image default=256&lt;/p&gt; &lt;p&gt;time=2025-03-17T15:16:24.005+01:00 level=INFO source=server.go:624 msg=&amp;quot;llama runner started in 2.04 seconds&amp;quot;&lt;/p&gt; &lt;p&gt;etc.&lt;/p&gt; &lt;p&gt;I seem to get reasonable responses anyway, but the &amp;quot;key not found&amp;quot; warnings suggest that something is off. But what exactly? I am running on an M3 with flash-attn and kv-cache-type q8_0 but I guess that don't have anything to do with it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lodurr242"&gt; /u/Lodurr242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jde0cw/gemma_3_warnings_in_ollama_server_log/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jde0cw/gemma_3_warnings_in_ollama_server_log/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jde0cw/gemma_3_warnings_in_ollama_server_log/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T14:31:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdeuqb</id>
    <title>Vision support for the gemma-3-12b-it-GGUF:Q4_K_M of unsloth and lmstudio-community not working</title>
    <updated>2025-03-17T15:07:24+00:00</updated>
    <author>
      <name>/u/_digito</name>
      <uri>https://old.reddit.com/user/_digito</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I have been testing the gemma-3-12b-it-GGUF:Q4_K_M model with Ollama and Open-webui and when I tried to get the text from an image either with the unsloth and lmstudio-community versions I got an error on Ollama logs:&lt;/p&gt; &lt;p&gt;msg=&amp;quot;llm predict error: Failed to create new sequence: failed to process inputs: this model is missing data required for image input&amp;quot;&lt;/p&gt; &lt;p&gt;If I use the gemma3:12b from the Ollama repository it works as expected and it gives the text more or less as expected. I'm using the recommended configurations for the temperature = 1.0, top_k = 64, top_p = 0.95, min_p = 0.0 for all the models I tested. Also the context size was the same to all, 8192 token.&lt;/p&gt; &lt;p&gt;From the HF pages the information is that the models are image-text-to-text, so I expected them to work like the gemma3:12b from Ollama repository. Any ideas?&lt;/p&gt; &lt;p&gt;Thank you. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_digito"&gt; /u/_digito &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdeuqb/vision_support_for_the_gemma312bitggufq4_k_m_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdeuqb/vision_support_for_the_gemma312bitggufq4_k_m_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdeuqb/vision_support_for_the_gemma312bitggufq4_k_m_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T15:07:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdd7k9</id>
    <title>I built a VM for AI agents supporting local models with Ollama</title>
    <updated>2025-03-17T13:55:49+00:00</updated>
    <author>
      <name>/u/sandropuppo</name>
      <uri>https://old.reddit.com/user/sandropuppo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jdd7k9/i_built_a_vm_for_ai_agents_supporting_local/"&gt; &lt;img alt="I built a VM for AI agents supporting local models with Ollama" src="https://external-preview.redd.it/4R-oKdOmCmX_i9_20c366T82clUbCUIJRVPgtgZpfGo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=510a70ca5c4eba0d1e8f90da819b81b800e0e4fe" title="I built a VM for AI agents supporting local models with Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandropuppo"&gt; /u/sandropuppo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/trycua/computer"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdd7k9/i_built_a_vm_for_ai_agents_supporting_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdd7k9/i_built_a_vm_for_ai_agents_supporting_local/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T13:55:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd40r2</id>
    <title>GPT 4.5 System Prompt Preamble</title>
    <updated>2025-03-17T03:55:48+00:00</updated>
    <author>
      <name>/u/foomanchu89</name>
      <uri>https://old.reddit.com/user/foomanchu89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jd40r2/gpt_45_system_prompt_preamble/"&gt; &lt;img alt="GPT 4.5 System Prompt Preamble" src="https://preview.redd.it/e84gtswqa6pe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a69962b66b61145ec819b5d83e10aa81d6a0b0a6" title="GPT 4.5 System Prompt Preamble" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty cool but buried in the docs&lt;/p&gt; &lt;p&gt;&lt;a href="https://platform.openai.com/docs/guides/prompt-engineering#tactic-use-delimiters-to-clearly-indicate-distinct-parts-of-the-input"&gt;https://platform.openai.com/docs/guides/prompt-engineering#tactic-use-delimiters-to-clearly-indicate-distinct-parts-of-the-input&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I bet it works on open source models too&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foomanchu89"&gt; /u/foomanchu89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e84gtswqa6pe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd40r2/gpt_45_system_prompt_preamble/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jd40r2/gpt_45_system_prompt_preamble/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T03:55:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdr0bv</id>
    <title>Old Trusty!</title>
    <updated>2025-03-17T23:22:58+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jdr0bv/old_trusty/"&gt; &lt;img alt="Old Trusty!" src="https://preview.redd.it/56mrtc9lcbpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2189cfb531f3a6f13473cbf43f90235d3da5e32a" title="Old Trusty!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/56mrtc9lcbpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdr0bv/old_trusty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdr0bv/old_trusty/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T23:22:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdkevi</id>
    <title>Creating Gemma 3 from GGUF with mmproj not working.</title>
    <updated>2025-03-17T18:49:33+00:00</updated>
    <author>
      <name>/u/Admirable-Star7088</name>
      <uri>https://old.reddit.com/user/Admirable-Star7088</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I was going to download Gemma 3 for Ollama, I could not find a Q5_K_M version. This is my favorite quant because it's the smallest quant possible with no noticeable quality loss (in my experience).&lt;/p&gt; &lt;p&gt;So, instead of downloading, I was doing some quick research how to convert my own GGUF file (&lt;strong&gt;google_gemma-3-12b-it-Q5_K_M.gguf&lt;/strong&gt;) and my mmproj file (&lt;strong&gt;mmproj-google_gemma-3-12b-it-f32.gguf&lt;/strong&gt;) to a format that I can run in Ollama. (these GGUFs are downloaded from &lt;a href="https://huggingface.co/bartowski/google_gemma-3-12b-it-GGUF"&gt;Bartowski&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;After successfully converting, the model works fine at first and it responds to text, but when I send it an image and ask it to describe it, it won't respond. I assume there is some problem with the mmproj file? Here is my Modelfile:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;FROM ./google_gemma-3-12b-it-Q5_K_M.gguf FROM ./mmproj-google_gemma-3-12b-it-f32.gguf PARAMETER temperature 1 PARAMETER top_k 64 PARAMETER top_p 0.95 PARAMETER min_p 0.0 PARAMETER num_ctx 8192 PARAMETER stop &amp;quot;&amp;lt;end_of_turn&amp;gt;&amp;quot; TEMPLATE &amp;quot;&amp;quot;&amp;quot; {{- range $i, $_ := .Messages }} {{- $last := eq (len (slice $.Messages $i)) 1 }} {{- if or (eq .Role &amp;quot;user&amp;quot;) (eq .Role &amp;quot;system&amp;quot;) }}&amp;lt;start_of_turn&amp;gt;user {{ .Content }}&amp;lt;end_of_turn&amp;gt; {{ if $last }}&amp;lt;start_of_turn&amp;gt;model {{ end }} {{- else if eq .Role &amp;quot;assistant&amp;quot; }}&amp;lt;start_of_turn&amp;gt;model {{ .Content }}{{ if not $last }}&amp;lt;end_of_turn&amp;gt; {{ end }} {{- end }} {{- end }} &amp;quot;&amp;quot;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm an amateur with Ollama, I have probably just made a silly mistake or missed some step. Thanks in advance to anyone who can help out!&lt;/p&gt; &lt;p&gt;p.s, I'm using Open WebUI as front-end.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable-Star7088"&gt; /u/Admirable-Star7088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdkevi/creating_gemma_3_from_gguf_with_mmproj_not_working/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdkevi/creating_gemma_3_from_gguf_with_mmproj_not_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdkevi/creating_gemma_3_from_gguf_with_mmproj_not_working/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T18:49:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdt4am</id>
    <title>Getting any model to avoid certain words</title>
    <updated>2025-03-18T01:02:31+00:00</updated>
    <author>
      <name>/u/jamboman_</name>
      <uri>https://old.reddit.com/user/jamboman_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can you help me?&lt;/p&gt; &lt;p&gt;I've tried several different methods, different interfaces (using msty right now), and several different models.&lt;/p&gt; &lt;p&gt;I want to have a list of words that I don't want to be used when a model is replying to me.&lt;/p&gt; &lt;p&gt;Have any of you had success with this? It's been a nightmare, and seems so simple compared to other things I've been able to do.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jamboman_"&gt; /u/jamboman_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdt4am/getting_any_model_to_avoid_certain_words/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdt4am/getting_any_model_to_avoid_certain_words/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdt4am/getting_any_model_to_avoid_certain_words/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T01:02:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdp0xw</id>
    <title>Noob: Ollama models not in expected local as such command does not work</title>
    <updated>2025-03-17T21:56:22+00:00</updated>
    <author>
      <name>/u/GreenAmigo</name>
      <uri>https://old.reddit.com/user/GreenAmigo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;/p&gt; &lt;p&gt;I have the models stored on a seperate NVME drive as I want my OS to remain quick.&lt;/p&gt; &lt;p&gt;I have a large number of models and want to add a few tweaked ones from &lt;a href="https://openwebui.com/models"&gt;https://openwebui.com/models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When I go &amp;quot;http://localhost:3000/workspace/models&amp;quot; &amp;quot;Import to webUI&amp;quot; I get error 404.&lt;/p&gt; &lt;p&gt;As such how do I sort this out ? I aint a coder but I am going to be starting Python soon as My skills in programming are nil withthe exception of &amp;quot;Fortran 90&amp;quot; some 20 years ago. Any help?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenAmigo"&gt; /u/GreenAmigo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdp0xw/noob_ollama_models_not_in_expected_local_as_such/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdp0xw/noob_ollama_models_not_in_expected_local_as_such/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdp0xw/noob_ollama_models_not_in_expected_local_as_such/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T21:56:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdf5fp</id>
    <title>X299 i9 7980XE SKYLAKEX-CASCADEX CPU ONLY LLM PERFORMANCE BENCHMARK</title>
    <updated>2025-03-17T15:19:44+00:00</updated>
    <author>
      <name>/u/bharlesm</name>
      <uri>https://old.reddit.com/user/bharlesm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jdf5fp/x299_i9_7980xe_skylakexcascadex_cpu_only_llm/"&gt; &lt;img alt="X299 i9 7980XE SKYLAKEX-CASCADEX CPU ONLY LLM PERFORMANCE BENCHMARK" src="https://preview.redd.it/xlivwqroo9pe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f076d64564d470e199f8365d70518819a046b47b" title="X299 i9 7980XE SKYLAKEX-CASCADEX CPU ONLY LLM PERFORMANCE BENCHMARK" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bharlesm"&gt; /u/bharlesm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xlivwqroo9pe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdf5fp/x299_i9_7980xe_skylakexcascadex_cpu_only_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdf5fp/x299_i9_7980xe_skylakexcascadex_cpu_only_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T15:19:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd4op1</id>
    <title>Clara: Browser based Local AI Chat, ImageGen with simple custom Agent builder.</title>
    <updated>2025-03-17T04:36:21+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jd4op1/clara_browser_based_local_ai_chat_imagegen_with/"&gt; &lt;img alt="Clara: Browser based Local AI Chat, ImageGen with simple custom Agent builder." src="https://external-preview.redd.it/662kDyxmF0rz11JzhX_fLISnvTFW9w3CE6gaa8Ta4wg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abda2fd3ebfe247a77cb433fc9a2bcd0dcc638b1" title="Clara: Browser based Local AI Chat, ImageGen with simple custom Agent builder." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey devs,&lt;/p&gt; &lt;p&gt;I built Clara because I wanted a simple, lightweight AI assistant that runs entirely on my own machine. Most AI tools depend on cloud services, track usage, or require heavy setups—Clara is different. It connects directly to Ollama for LLMs and ComfyUI for Stable Diffusion image generation, with zero external dependencies.&lt;/p&gt; &lt;p&gt;No docker, no backend, just ollama and clara installed on the pc is enough.&lt;/p&gt; &lt;p&gt;🔗 Repo: &lt;a href="https://rgithub.com/badboysm890/ClaraVerse"&gt;https://rgithub.com/badboysm890/ClaraVerse&lt;/a&gt; 💻 Download the app: &lt;a href="https://github.com/badboysm890/ClaraVerse/releases/tag/v0.2.0"&gt;https://github.com/badboysm890/ClaraVerse/releases/tag/v0.2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Why Clara? 1. Runs Locally – No cloud, no API calls, fully private. 2. All the data is stored in IndexDB 3. Fast &amp;amp; Lightweight – I love open web UI but now its too big for my machine 4. Agent Builder – Create simple AI agents and convert them into apps. 5. ComfyUI Integration – Generate images with Stable Diffusion models. 6. Custom Model Support – Works with any Ollama-compatible LLM. 7. Built-in Image Gallery – Just added is so i can have all the images generated in one place&lt;/p&gt; &lt;p&gt;💡 Need Help! I don’t have a Windows machine, so if anyone can help with building and testing the Windows version, I’d really appreciate it! Let me know if you’re interested.&lt;/p&gt; &lt;p&gt;Would love to hear your feedback if you try it! 🚀&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/badboysm890/ClaraVerse"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jd4op1/clara_browser_based_local_ai_chat_imagegen_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jd4op1/clara_browser_based_local_ai_chat_imagegen_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T04:36:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdj98c</id>
    <title>Is worth it to buy 128gb ram + tesla k80?</title>
    <updated>2025-03-17T18:03:41+00:00</updated>
    <author>
      <name>/u/Dangerous_Pineapple1</name>
      <uri>https://old.reddit.com/user/Dangerous_Pineapple1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, I’m new to AI. I’m planning to buy 128GB of RAM and a Tesla K80 for my Dell R730xd (with an Intel Xeon E5-2640 v4). The doubt I have is about what models I could run with this setup, since I’m not finding much information&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous_Pineapple1"&gt; /u/Dangerous_Pineapple1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdj98c/is_worth_it_to_buy_128gb_ram_tesla_k80/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdj98c/is_worth_it_to_buy_128gb_ram_tesla_k80/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdj98c/is_worth_it_to_buy_128gb_ram_tesla_k80/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T18:03:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdpta0</id>
    <title>Best models on a MacBook Pro M3 w/ 18GB of RAM in 2025?</title>
    <updated>2025-03-17T22:30:10+00:00</updated>
    <author>
      <name>/u/vegantiger</name>
      <uri>https://old.reddit.com/user/vegantiger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been playing with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;llama3:8b&lt;/li&gt; &lt;li&gt;gemma3:4b&lt;/li&gt; &lt;li&gt;deepseek-r1:7b&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So far llama3 seems to be the best all around, and anything bigger I've tried is so slow that it's unusable…&lt;/p&gt; &lt;p&gt;Are there any other models that run acceptably fast on this kind of setup that I should check out? I'm especially looking for coding stuff, as well as transcriptions and translations English → French.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vegantiger"&gt; /u/vegantiger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdpta0/best_models_on_a_macbook_pro_m3_w_18gb_of_ram_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdpta0/best_models_on_a_macbook_pro_m3_w_18gb_of_ram_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdpta0/best_models_on_a_macbook_pro_m3_w_18gb_of_ram_in/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T22:30:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jduw95</id>
    <title>Light-R1-32B-FP16 + 8xMi50 Server + vLLM</title>
    <updated>2025-03-18T02:31:18+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0oakixfd0dpe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jduw95/lightr132bfp16_8xmi50_server_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jduw95/lightr132bfp16_8xmi50_server_vllm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-18T02:31:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdnsik</id>
    <title>Why does AI gives better result</title>
    <updated>2025-03-17T21:04:57+00:00</updated>
    <author>
      <name>/u/OkConsideration2734</name>
      <uri>https://old.reddit.com/user/OkConsideration2734</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have started using Ollama since yesterday and i am a little surprised because LLM looks like they are giving way better results in theirs originals websites/apps. Perharps, is there a way to change that and make my LLMS in Ollama give more accurate results ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OkConsideration2734"&gt; /u/OkConsideration2734 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdnsik/why_does_ai_gives_better_result/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdnsik/why_does_ai_gives_better_result/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdnsik/why_does_ai_gives_better_result/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T21:04:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdb4zm</id>
    <title>I created a text editor that integrates with Ollama.</title>
    <updated>2025-03-17T12:11:22+00:00</updated>
    <author>
      <name>/u/lehen01</name>
      <uri>https://old.reddit.com/user/lehen01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jdb4zm/i_created_a_text_editor_that_integrates_with/"&gt; &lt;img alt="I created a text editor that integrates with Ollama." src="https://external-preview.redd.it/eXl1OWhjb3lxOHBlMdocuIzrYi9v6aPpAc1tPXmrtMByoLVp6lH-YWI3ILPo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e15c458e95cf8ceed96405eec2a1539ec81fecf" title="I created a text editor that integrates with Ollama." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working for a couple of years on a project I just launched.&lt;/p&gt; &lt;p&gt;It is a text editor that doesn't force you to send your notes to the cloud and integrates with Ollama to add AI prompts.&lt;/p&gt; &lt;p&gt;If you need a place to create your ideas and don't want to worry about who is spying on you, you'll love this app =]. Looks like Notion, but focused on privacy and offline usage (with better UI, in my opinion hahaha).&lt;/p&gt; &lt;p&gt;Website: &lt;a href="https://writeopia.io/"&gt;writeopia.io&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Writeopia/Writeopia"&gt;https://github.com/Writeopia/Writeopia&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My future plans:&lt;/p&gt; &lt;p&gt;- Finish the signature of Windows app and post it.&lt;/p&gt; &lt;p&gt;- Android/iOS apps.&lt;/p&gt; &lt;p&gt;- Meetings summary. (Drag and drop a video, you get the summary).&lt;/p&gt; &lt;p&gt;- Semantic search.&lt;/p&gt; &lt;p&gt;- AI generates a small presentation based on your document.&lt;/p&gt; &lt;p&gt;- Text summary.&lt;/p&gt; &lt;p&gt;- Backend that can be self-hosted.&lt;/p&gt; &lt;p&gt;I would love the community feedback about the project. Feel free to reach out with questions or issues, you can use this thread or send me a DM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lehen01"&gt; /u/lehen01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i5bcrdoyq8pe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jdb4zm/i_created_a_text_editor_that_integrates_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jdb4zm/i_created_a_text_editor_that_integrates_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-17T12:11:22+00:00</published>
  </entry>
</feed>
