<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-23T20:35:35+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jh6ixi</id>
    <title>LangChain or Pydantic AI or else ?</title>
    <updated>2025-03-22T11:39:07+00:00</updated>
    <author>
      <name>/u/Responsible-Tart-964</name>
      <uri>https://old.reddit.com/user/Responsible-Tart-964</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible-Tart-964"&gt; /u/Responsible-Tart-964 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/aiagents/comments/1jh6ii2/langchain_or_pydantic_ai_or_else/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jh6ixi/langchain_or_pydantic_ai_or_else/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jh6ixi/langchain_or_pydantic_ai_or_else/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-22T11:39:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh45kb</id>
    <title>how to force qwq to use both GPUs?</title>
    <updated>2025-03-22T08:48:37+00:00</updated>
    <author>
      <name>/u/caetydid</name>
      <uri>https://old.reddit.com/user/caetydid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;I run QwQ on dual rtx 3090. What I see is that the model is being loaded fully on one rtx and that the CPU utilization spikes to 100%. If I disable one GPU the performance and the behavior is almost the same, I yield around 19-22t/s.&lt;/p&gt; &lt;p&gt;Is there a way to force ollama to use both GPUs? As soon as I have increased context 24Gb VRAM will not suffice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/caetydid"&gt; /u/caetydid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jh45kb/how_to_force_qwq_to_use_both_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jh45kb/how_to_force_qwq_to_use_both_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jh45kb/how_to_force_qwq_to_use_both_gpus/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-22T08:48:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh353c</id>
    <title>RTX 5070 and RTX 3060TI</title>
    <updated>2025-03-22T07:31:29+00:00</updated>
    <author>
      <name>/u/Kirtap01</name>
      <uri>https://old.reddit.com/user/Kirtap01</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently have a RTX 3060 ti, and despite the little vram (8gb) it works well. I know it is generally possible to run ollama utilising 2 gpus. But i wonder how well it would work with an rtx 5070 and rtx 3060ti. Im considering the rtx 5070 because the card would give me also sufficient gaming performance. In Germany i can buy a rtx 5070 for 649€ instead of 1000€+ for an rtx 5070ti. I know the 5070ti has 16gb vram but wouldn‘t it be better to have 20 gb with the two cards combined. Please correct me if im wrong.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kirtap01"&gt; /u/Kirtap01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jh353c/rtx_5070_and_rtx_3060ti/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jh353c/rtx_5070_and_rtx_3060ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jh353c/rtx_5070_and_rtx_3060ti/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-22T07:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgsx00</id>
    <title>Built an open source mock interviews platform powered by ollama</title>
    <updated>2025-03-21T22:06:03+00:00</updated>
    <author>
      <name>/u/Boring_Rabbit2275</name>
      <uri>https://old.reddit.com/user/Boring_Rabbit2275</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jgsx00/built_an_open_source_mock_interviews_platform/"&gt; &lt;img alt="Built an open source mock interviews platform powered by ollama" src="https://preview.redd.it/2vz41jpw84qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf6a54b98a23125724ae5c087864384d0b41aa85" title="Built an open source mock interviews platform powered by ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Come practice your interviews for free using our project on GitHub here: &lt;a href="https://github.com/Azzedde/aiva_mock_interviews"&gt;https://github.com/Azzedde/aiva_mock_interviews&lt;/a&gt; We are two junior AI engineers, and we would really appreciate feedback on our work. Please star it if you like it.&lt;/p&gt; &lt;p&gt;We find that the junior era is full of uncertainty, and we want to know if we are doing good work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Boring_Rabbit2275"&gt; /u/Boring_Rabbit2275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2vz41jpw84qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jgsx00/built_an_open_source_mock_interviews_platform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jgsx00/built_an_open_source_mock_interviews_platform/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-21T22:06:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgsqci</id>
    <title>Observer AI - AI Agent creation!</title>
    <updated>2025-03-21T21:58:00+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jgsqci/observer_ai_ai_agent_creation/"&gt; &lt;img alt="Observer AI - AI Agent creation!" src="https://external-preview.redd.it/YW1jZTUxa3E2NHFlMUoR_eYKE5mqj2V4XhjfJPn1tdQGxAnERB4_DcKntuZ4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e50911a0eba38062d5cdfed3f34aa80c7b8e0893" title="Observer AI - AI Agent creation!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Ollama community!&lt;/p&gt; &lt;p&gt;Just dropped possibly the coolest feature yet for Observer AI - a natural language &lt;strong&gt;Agent Generator&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;I made a quick (admittedly janky 😅) demo video showing how it works&lt;/p&gt; &lt;p&gt;This turns Observer AI into a no-code platform for creating AI agents that can monitor your screen, run Python via Jupyter, and take actions - all powered by your local Ollama models!&lt;/p&gt; &lt;p&gt;Give it a try at &lt;a href="https://app.observer-ai.com"&gt;https://app.observer-ai.com&lt;/a&gt; and let me know what kind of agents you end up creating!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/g63nxzjq64qe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jgsqci/observer_ai_ai_agent_creation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jgsqci/observer_ai_ai_agent_creation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-21T21:58:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh8okg</id>
    <title>Build a Multimodal RAG with Gemma 3, LangChain and Streamlit</title>
    <updated>2025-03-22T13:40:44+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jh8okg/build_a_multimodal_rag_with_gemma_3_langchain_and/"&gt; &lt;img alt="Build a Multimodal RAG with Gemma 3, LangChain and Streamlit" src="https://external-preview.redd.it/-BnXEJAodI48Rplc5bVisrIvZSJ5W2_z0qRTJrv66MQ.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0547b94164b49338ac7738e622814ad6861dc25b" title="Build a Multimodal RAG with Gemma 3, LangChain and Streamlit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=hBDNv47KCKo&amp;amp;list=PLp01ObP3udmq2quR-RfrX4zNut_t_kNot"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jh8okg/build_a_multimodal_rag_with_gemma_3_langchain_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jh8okg/build_a_multimodal_rag_with_gemma_3_langchain_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-22T13:40:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhcj60</id>
    <title>ollama based AI agent?</title>
    <updated>2025-03-22T16:36:37+00:00</updated>
    <author>
      <name>/u/Glum_Mistake1933</name>
      <uri>https://old.reddit.com/user/Glum_Mistake1933</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I would like to use ollama in some kind of extended form, e.g. as a kind of AI agent. I have asked AI's and each time received a suggestion that could not use ollama :-(.&lt;/p&gt; &lt;p&gt;Does anyone know of any software that runs on Ubuntu that allows you to use some kind of AI agent with the local ollama? AI's are unfortunately not helpful in answering this question.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glum_Mistake1933"&gt; /u/Glum_Mistake1933 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhcj60/ollama_based_ai_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhcj60/ollama_based_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jhcj60/ollama_based_ai_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-22T16:36:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhaixb</id>
    <title>(Update) Generative AI project template (it now includes Ollama)</title>
    <updated>2025-03-22T15:07:31+00:00</updated>
    <author>
      <name>/u/aminedjeghri</name>
      <uri>https://old.reddit.com/user/aminedjeghri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;For those interested in a project template that integrates generative AI, Streamlit, UV, CI/CD, automatic documentation, and more, I’ve updated my template to now include &lt;strong&gt;Ollama&lt;/strong&gt;. It even includes tests in CI/CD for a small model (Qwen 2.5 with 0.5B parameters).&lt;/p&gt; &lt;p&gt;Here’s the GitHub project:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/AmineDjeghri/generative-ai-project-template"&gt;Generative AI Project Template&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Engineering tools&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- [x] Use UV to manage packages&lt;/p&gt; &lt;p&gt;- [x] pre-commit hooks: use ``ruff`` to ensure the code quality &amp;amp; ``detect-secrets`` to scan the secrets in the code.&lt;/p&gt; &lt;p&gt;- [x] Logging using loguru (with colors)&lt;/p&gt; &lt;p&gt;- [x] Pytest for unit tests&lt;/p&gt; &lt;p&gt;- [x] Dockerized project (Dockerfile &amp;amp; docker-compose).&lt;/p&gt; &lt;p&gt;- [x] Streamlit (frontend) &amp;amp; FastAPI (backend)&lt;/p&gt; &lt;p&gt;- [x] Make commands to handle everything for you: install, run, test&lt;/p&gt; &lt;p&gt;&lt;strong&gt;AI tools&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- [x] LLM running locally with Ollama or in the cloud with any LLM provider (LiteLLM)&lt;/p&gt; &lt;p&gt;- [x] Information extraction and Question answering from documents&lt;/p&gt; &lt;p&gt;- [x] Chat to test the AI system&lt;/p&gt; &lt;p&gt;- [x] Efficient async code using asyncio.&lt;/p&gt; &lt;p&gt;- [x] AI Evaluation framework: using Promptfoo, Ragas &amp;amp; more...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CI/CD &amp;amp; Maintenance tools&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- [x] CI/CD pipelines: ``.github/workflows`` for GitHub (Testing the AI system, local models with Ollama and the dockerized app)&lt;/p&gt; &lt;p&gt;- [x] Local CI/CD pipelines: GitHub Actions using ``github act``&lt;/p&gt; &lt;p&gt;- [x] GitHub Actions for deploying to GitHub Pages with mkdocs gh-deploy&lt;/p&gt; &lt;p&gt;- [x] Dependabot ``.github/dependabot.yml`` for automatic dependency and security updates&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Documentation tools&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- [x] Wiki creation and setup of documentation website using Mkdocs&lt;/p&gt; &lt;p&gt;- [x] GitHub Pages deployment using mkdocs gh-deploy plugin&lt;/p&gt; &lt;p&gt;Feel free to check it out, contribute, or use it for your own AI projects! Let me know if you have any questions or feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aminedjeghri"&gt; /u/aminedjeghri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhaixb/update_generative_ai_project_template_it_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhaixb/update_generative_ai_project_template_it_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jhaixb/update_generative_ai_project_template_it_now/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-22T15:07:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh6l7b</id>
    <title>PyChat</title>
    <updated>2025-03-22T11:43:11+00:00</updated>
    <author>
      <name>/u/mspamnamem</name>
      <uri>https://old.reddit.com/user/mspamnamem</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve seen a few posts recently about chat clients that people have been building. They’re great! &lt;/p&gt; &lt;p&gt;I’ve been working on one of my own context aware chat clients. It is written in python and has a few unique things:&lt;/p&gt; &lt;p&gt;(1) can import and export chats. I think this so I can export a “starter” chat. I sort of think of this like a sourdough starter. Share it with your friends. Can be useful for coding if you don’t want to start from scratch every time.&lt;/p&gt; &lt;p&gt;(2) context aware and can switch provider and model in the chat window. &lt;/p&gt; &lt;p&gt;(3) search and archive threads. &lt;/p&gt; &lt;p&gt;(4) allow two AIs to communicate with one another. Also useful for coding: make one strong coding model the developer and a strong language model the manager. Can also simulate debates and stuff. &lt;/p&gt; &lt;p&gt;(5) attempts to highlight code into code blocks and allows you to easily copy them. &lt;/p&gt; &lt;p&gt;I have this working at home with a Mac on my network hosting ollama and running this client on a PC. I haven’t tested it with localhost ollama running on the same machine but it should still work. Just make sure that ollama is listening on 0.0.0.0 not just html server. &lt;/p&gt; &lt;p&gt;Note: - API keys are optional to OpenAI and Anthropic. They are stored locally but not encrypted. Same with the chat database. Maybe in the future I’ll work to encrypt these. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;There are probably some bugs because I’m just one person. Willing to fix. Let me know! &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/Magnetron85/PyChat"&gt;https://github.com/Magnetron85/PyChat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mspamnamem"&gt; /u/mspamnamem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jh6l7b/pychat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jh6l7b/pychat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jh6l7b/pychat/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-22T11:43:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh8sby</id>
    <title>Can I Run Small LLMs Locally on My Subnotebook with Ollama?</title>
    <updated>2025-03-22T13:46:03+00:00</updated>
    <author>
      <name>/u/Jan49_</name>
      <uri>https://old.reddit.com/user/Jan49_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I have a subnotebook that I use for university. It's not a powerhouse, but its efficiency makes it perfect for a full day of school. My specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Intel N100 (4 cores, 6W TDP)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 4 GB LPDDR5&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; Integrated Intel UHD Graphics&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OS:&lt;/strong&gt; Currently Windows 11, but planning to switch to Linux Mint (XFCE)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I mainly use it for light office tasks like Word and Excel, but I'm curious if I can run very small language models (like 2B parameters) locally with Ollama. Given my limited RAM, would this even be feasible?&lt;/p&gt; &lt;p&gt;Any insights or recommendations would be greatly appreciated!&lt;/p&gt; &lt;p&gt;TL;DR:&lt;br /&gt; Can I run 2B parameter LLMs locally with Ollama on a subnotebook (Intel N100, 4GB RAM)? Currently on Windows 11 but planning to switch to Linux Mint XFCE.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jan49_"&gt; /u/Jan49_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jh8sby/can_i_run_small_llms_locally_on_my_subnotebook/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jh8sby/can_i_run_small_llms_locally_on_my_subnotebook/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jh8sby/can_i_run_small_llms_locally_on_my_subnotebook/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-22T13:46:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhmldw</id>
    <title>Title: Anyone got Mistral 7B working well on Vega 8 iGPU?</title>
    <updated>2025-03-23T00:11:05+00:00</updated>
    <author>
      <name>/u/Winter-Morning6954</name>
      <uri>https://old.reddit.com/user/Winter-Morning6954</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m running Mistral 7B on my mini PC with these specs:&lt;/p&gt; &lt;p&gt;Ryzen 5 3550H&lt;/p&gt; &lt;p&gt;16GB RAM&lt;/p&gt; &lt;p&gt;512GB SSD&lt;/p&gt; &lt;p&gt;Vega 8 iGPU&lt;/p&gt; &lt;p&gt;Ubuntu 22.04&lt;/p&gt; &lt;p&gt;Using Ollama to run Mistral locally&lt;/p&gt; &lt;p&gt;I got it working, and response time was around 12 seconds, which is decent, but I wanted to speed it up. I tried forcing ROCm to use my Vega 8 by setting HSA override and running Ollama with the ROCm library. But after that, my system froze completely, and I had to reinstall Ubuntu.&lt;/p&gt; &lt;p&gt;Now I don’t even think my GPU was being used before. VRAM usage was around 17 percent, and GTT stayed at 1.29 percent, which seems way too low. I feel like all the processing was still happening on the CPU.&lt;/p&gt; &lt;p&gt;Is there any way to actually get Vega 8 to work for inference? Would lowering GPU offload help? Would switching to a lower quantized model like q4 instead of q8 improve anything? Also, is there a better way to check if the GPU is actually doing something while it’s running?&lt;/p&gt; &lt;p&gt;I want to make the most out of this setup without switching to a dedicated GPU. If anyone has tried something similar or knows a way to improve it, let me know.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Winter-Morning6954"&gt; /u/Winter-Morning6954 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhmldw/title_anyone_got_mistral_7b_working_well_on_vega/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhmldw/title_anyone_got_mistral_7b_working_well_on_vega/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jhmldw/title_anyone_got_mistral_7b_working_well_on_vega/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T00:11:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhma93</id>
    <title>ollama seems to chat on /api/generate?</title>
    <updated>2025-03-22T23:56:08+00:00</updated>
    <author>
      <name>/u/Stronksbeach</name>
      <uri>https://old.reddit.com/user/Stronksbeach</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am generally having issues making models do text completion.&lt;/p&gt; &lt;p&gt;my python test script looks like&lt;/p&gt; &lt;pre&gt;&lt;code&gt;MODEL = &amp;quot;qwen2.5-coder:3b&amp;quot; response = requests.post( &amp;quot;http://localhost:11434/api/generate&amp;quot;, json={&amp;quot;model&amp;quot;: MODEL, &amp;quot;prompt&amp;quot;: input(), &amp;quot;stream&amp;quot;:False}) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and if i input &amp;quot;def fi&amp;quot; it tells me things like &amp;quot;it looks like you have an incomplete function definition&amp;quot;, when i would expect something like &amp;quot;bonacci(n):&amp;quot; or &amp;quot;(x):&amp;quot; or &amp;quot;x():&amp;quot; or anything thats ... a completion&lt;/p&gt; &lt;p&gt;what am i doing wrong, thought api/chat was for chat and generate for generation.&lt;/p&gt; &lt;p&gt;I thought something was wrong with the extensions i am using to use ollama to code complete but i get the same results&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stronksbeach"&gt; /u/Stronksbeach &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhma93/ollama_seems_to_chat_on_apigenerate/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhma93/ollama_seems_to_chat_on_apigenerate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jhma93/ollama_seems_to_chat_on_apigenerate/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-22T23:56:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhigey</id>
    <title>AI-powered Resume Tailoring application using Ollama and Langchain Project</title>
    <updated>2025-03-22T20:55:15+00:00</updated>
    <author>
      <name>/u/Maleficent-Penalty50</name>
      <uri>https://old.reddit.com/user/Maleficent-Penalty50</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jhigey/aipowered_resume_tailoring_application_using/"&gt; &lt;img alt="AI-powered Resume Tailoring application using Ollama and Langchain Project" src="https://external-preview.redd.it/bjZlajRkajExYnFlMZy3YkGsqoarT07V-vPPMaS_PHa1WWz4P1Vvd0jG5Jtl.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de6996acba94fd1cea987e21fff299ead508002a" title="AI-powered Resume Tailoring application using Ollama and Langchain Project" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent-Penalty50"&gt; /u/Maleficent-Penalty50 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/te2qn7j11bqe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhigey/aipowered_resume_tailoring_application_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jhigey/aipowered_resume_tailoring_application_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-22T20:55:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhoom8</id>
    <title>RTX5080 for local AI/ML</title>
    <updated>2025-03-23T02:00:26+00:00</updated>
    <author>
      <name>/u/mecatman</name>
      <uri>https://old.reddit.com/user/mecatman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Is the RTX5080 a good GPU for local AI/ML? (Not getting 5090 due to scalpers, cant find a 2nd hand 3090 and 4090 in my country)&lt;/p&gt; &lt;p&gt;Thanks for any feedback =)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mecatman"&gt; /u/mecatman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhoom8/rtx5080_for_local_aiml/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhoom8/rtx5080_for_local_aiml/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jhoom8/rtx5080_for_local_aiml/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T02:00:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhbwsd</id>
    <title>ollama on Android (Termux) with GPU</title>
    <updated>2025-03-22T16:09:10+00:00</updated>
    <author>
      <name>/u/lssong99</name>
      <uri>https://old.reddit.com/user/lssong99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now that Google released Gemma 3, and with mediapipe it seems they could run (at least) 1b with GPU on Android (I use Pixel 8 Pro). The speed is much faster comparing running with CPU.&lt;/p&gt; &lt;p&gt;The sample code is here: &lt;a href="https://github.com/google-ai-edge/mediapipe-samples/tree/main/examples/llm_inference/android"&gt;https://github.com/google-ai-edge/mediapipe-samples/tree/main/examples/llm_inference/android&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wonder anyone more capable then me could integrate this with ollama so we could run (at least Gemma 3) models on Android with GPU?&lt;/p&gt; &lt;p&gt;(Edit) For anyone interested, you could get the pre-built APK here&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/google-ai-edge/mediapipe-samples/releases/download/v0.1.3/llm_inference_v0.1.3-debug.apk"&gt;https://github.com/google-ai-edge/mediapipe-samples/releases/download/v0.1.3/llm_inference_v0.1.3-debug.apk&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lssong99"&gt; /u/lssong99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhbwsd/ollama_on_android_termux_with_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhbwsd/ollama_on_android_termux_with_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jhbwsd/ollama_on_android_termux_with_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-22T16:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhv79c</id>
    <title>Budget GPU for Deepseek</title>
    <updated>2025-03-23T09:08:36+00:00</updated>
    <author>
      <name>/u/BillGRC</name>
      <uri>https://old.reddit.com/user/BillGRC</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I need a budget GPU for an old Z77 system (ReBar enabled BIOS patch) to try some small Deepseek distilled models. I can find RX 5500XT 8GB and ARC A380 near the same price under 100$. Which card will perform better (t/s)? My main OS is Linux Ubuntu 22.04. I'm a really casual gamer playing here and there some CS2 and maybe some PUBG. I know RX 5500XT is better for games but ARC is way better for transcoding. Thanks for your time! Really appreciate.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BillGRC"&gt; /u/BillGRC &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhv79c/budget_gpu_for_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhv79c/budget_gpu_for_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jhv79c/budget_gpu_for_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T09:08:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhvmco</id>
    <title>Is there a way to download only the manifest?</title>
    <updated>2025-03-23T09:40:07+00:00</updated>
    <author>
      <name>/u/PeterHickman</name>
      <uri>https://old.reddit.com/user/PeterHickman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just want to get a feel for now many models are just renames of others without having to download Gb of data&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PeterHickman"&gt; /u/PeterHickman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhvmco/is_there_a_way_to_download_only_the_manifest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhvmco/is_there_a_way_to_download_only_the_manifest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jhvmco/is_there_a_way_to_download_only_the_manifest/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T09:40:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhpxz3</id>
    <title>Enough resources for local AI?</title>
    <updated>2025-03-23T03:10:41+00:00</updated>
    <author>
      <name>/u/JagerAntlerite7</name>
      <uri>https://old.reddit.com/user/JagerAntlerite7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for advice on running Ollama locally on my outdated Dell Precision 3630. I do not need amazing performance, just hoping for coding assistance.&lt;/p&gt; &lt;p&gt;Here are the workstation specs: * OS: Ubuntu 24.04.01 LTS * CPU: Intel Core Processor i7 (8 cores) * RAM: 128GB * GPU: Nvidia Quadro P2000 * Storage: 512GB NVMe * IDEs: VSCode and JetBrains&lt;/p&gt; &lt;p&gt;If those resources sound reasonable for my use case, what library is suggested?&lt;/p&gt; &lt;p&gt;EDIT: Added Dell model number &amp;quot;3630&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JagerAntlerite7"&gt; /u/JagerAntlerite7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhpxz3/enough_resources_for_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhpxz3/enough_resources_for_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jhpxz3/enough_resources_for_local_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T03:10:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji7009</id>
    <title>How to get attention scores in ollama models?</title>
    <updated>2025-03-23T19:08:45+00:00</updated>
    <author>
      <name>/u/Ok_Company6990</name>
      <uri>https://old.reddit.com/user/Ok_Company6990</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am writing a research paper and for that I need the attention scores of the output generated by the llm. Is there any way that I can access the scores in ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Company6990"&gt; /u/Ok_Company6990 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji7009/how_to_get_attention_scores_in_ollama_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji7009/how_to_get_attention_scores_in_ollama_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ji7009/how_to_get_attention_scores_in_ollama_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T19:08:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji8vlm</id>
    <title>Codename Goose agentic AI</title>
    <updated>2025-03-23T20:28:26+00:00</updated>
    <author>
      <name>/u/JagerAntlerite7</name>
      <uri>https://old.reddit.com/user/JagerAntlerite7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using Block's open source &lt;a href="https://block.github.io/goose/"&gt;Codename Goose&lt;/a&gt; CLI paired with Google Gemini 1.5 Pro for a week now. Goose runs locally, keeping control in my hands, allowing me to perform all the same coding tasks from a terminal that I would normally do from a browser session.&lt;/p&gt; &lt;p&gt;While a CLI is a welcome convenience, the real power is the ability to use any Model Context Protocol (MCP) server extension. Goose is agentic AI, the next step beyond LLMs, and these &lt;a href="https://block.github.io/goose/v1/extensions/"&gt;extensions&lt;/a&gt; are the really exciting part. &lt;/p&gt; &lt;p&gt;There are four built-in extensions that can be enabled right away: * &amp;quot;Memory&amp;quot; provides additional context for future prompt responses * &amp;quot;Developer Tools&amp;quot; allows editing and shell command execution * &amp;quot;JetBrains&amp;quot; for IDE integration and enhanced context * &amp;quot;Computer Controls&amp;quot; make webscraping, file caching, and automations possible &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JagerAntlerite7"&gt; /u/JagerAntlerite7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji8vlm/codename_goose_agentic_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji8vlm/codename_goose_agentic_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ji8vlm/codename_goose_agentic_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T20:28:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhpdgf</id>
    <title>🚀 AI Terminal v0.1 — A Modern, Open-Source Terminal with Local AI Assistance!</title>
    <updated>2025-03-23T02:38:28+00:00</updated>
    <author>
      <name>/u/Macsdeve</name>
      <uri>https://old.reddit.com/user/Macsdeve</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're excited to announce AI Terminal, an open-source, Rust-powered terminal that's designed to simplify your command-line experience through the power of local AI.&lt;/p&gt; &lt;p&gt;Key features include:&lt;/p&gt; &lt;p&gt;Local AI Assistant: Interact directly in your terminal with a locally running, fine-tuned LLM for command suggestions, explanations, or automatic execution.&lt;/p&gt; &lt;p&gt;Git Repository Visualization: Easily view and navigate your Git repositories.&lt;/p&gt; &lt;p&gt;Smart Autocomplete: Quickly autocomplete commands and paths to boost productivity.&lt;/p&gt; &lt;p&gt;Real-time Stream Output: Instant display of streaming command outputs.&lt;/p&gt; &lt;p&gt;Keyboard-First Design: Navigate smoothly with intuitive shortcuts and resizable panels—no mouse required!&lt;/p&gt; &lt;p&gt;What's next on our roadmap:&lt;/p&gt; &lt;p&gt;🛠️ Community-driven development: Your feedback shapes our direction!&lt;/p&gt; &lt;p&gt;📌 Session persistence: Keep your workflow intact across terminal restarts.&lt;/p&gt; &lt;p&gt;🔍 Automatic AI reasoning &amp;amp; error detection: Let AI handle troubleshooting seamlessly.&lt;/p&gt; &lt;p&gt;🌐 Ollama independence: Developing our own lightweight embedded AI model.&lt;/p&gt; &lt;p&gt;🎨 Enhanced UI experience: Continuous UI improvements while keeping it clean and intuitive.&lt;/p&gt; &lt;p&gt;We'd love to hear your thoughts, ideas, or even better—have you contribute!&lt;/p&gt; &lt;p&gt;⭐ GitHub repo: &lt;a href="https://github.com/MicheleVerriello/ai-terminal"&gt;https://github.com/MicheleVerriello/ai-terminal&lt;/a&gt; 👉 Try it out: &lt;a href="https://ai-terminal.dev/"&gt;https://ai-terminal.dev/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Contributors warmly welcomed! Join us in redefining the terminal experience.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Macsdeve"&gt; /u/Macsdeve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhpdgf/ai_terminal_v01_a_modern_opensource_terminal_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jhpdgf/ai_terminal_v01_a_modern_opensource_terminal_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jhpdgf/ai_terminal_v01_a_modern_opensource_terminal_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T02:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji60du</id>
    <title>Branching out from the Ollama library</title>
    <updated>2025-03-23T18:27:07+00:00</updated>
    <author>
      <name>/u/Inner-End7733</name>
      <uri>https://old.reddit.com/user/Inner-End7733</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've pretty much exhausted my options for models in the official library that I'm interested in running. I'm looking for recs on stuff I could get on huggingface or github that you've had success with. I think 14b q4 seems to be the ideal size/quant for my set up, but I'm interested in seeing what the limits of other quants are on my machine too. I'm a big fan of Phi4 at the moment, it's got some decent techincal hardware knowledge, and I'm also a pretty big fan of mistral-nemo, and to an extent gemme3:12b from the library. What your favorite model in this specification range to run? anything with more than 14b parameters but under 20 that you like? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inner-End7733"&gt; /u/Inner-End7733 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji60du/branching_out_from_the_ollama_library/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji60du/branching_out_from_the_ollama_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ji60du/branching_out_from_the_ollama_library/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T18:27:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji6xt2</id>
    <title>Ollama not using my Gpu</title>
    <updated>2025-03-23T19:06:03+00:00</updated>
    <author>
      <name>/u/Key_Appointment_7582</name>
      <uri>https://old.reddit.com/user/Key_Appointment_7582</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ji6xt2/ollama_not_using_my_gpu/"&gt; &lt;img alt="Ollama not using my Gpu" src="https://b.thumbs.redditmedia.com/LA2n2GtvgCXvf0CsCZW0bYhAMoS_G7afSJX7TQLr3Lw.jpg" title="Ollama not using my Gpu" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/emmo9002mhqe1.png?width=451&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d14c89f4e57705487097c1ae6a19ccedad4697a2"&gt;https://preview.redd.it/emmo9002mhqe1.png?width=451&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d14c89f4e57705487097c1ae6a19ccedad4697a2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My computer will not use my GPU when running llama 3.1 8b. I was working perfectly yesterday and now it doesn't. Has anyone had this problem? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Key_Appointment_7582"&gt; /u/Key_Appointment_7582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji6xt2/ollama_not_using_my_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji6xt2/ollama_not_using_my_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ji6xt2/ollama_not_using_my_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T19:06:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji5tk2</id>
    <title>Added web search to my ollama Discord bot.</title>
    <updated>2025-03-23T18:19:04+00:00</updated>
    <author>
      <name>/u/4500vcel</name>
      <uri>https://old.reddit.com/user/4500vcel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ji5tk2/added_web_search_to_my_ollama_discord_bot/"&gt; &lt;img alt="Added web search to my ollama Discord bot." src="https://preview.redd.it/gtqojwh8ehqe1.gif?width=640&amp;amp;crop=smart&amp;amp;s=81031a8860b48b78fc3ad1a9af9958c95a585c96" title="Added web search to my ollama Discord bot." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I’ve shared my Discord bot a couple times here. I just added a RAG pipeline for web search. It gathers info from Wikipedia and DuckDuckGo search result. It’s enabled by sending &lt;code&gt;+ search&lt;/code&gt; and disabled by sending &lt;code&gt;+ model&lt;/code&gt;. It can be found here: &lt;a href="https://github.com/jake83741/vnc-lm"&gt;https://github.com/jake83741/vnc-lm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If anyone ends up trying and has any feedback, I’d love to hear it. Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/4500vcel"&gt; /u/4500vcel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gtqojwh8ehqe1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji5tk2/added_web_search_to_my_ollama_discord_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ji5tk2/added_web_search_to_my_ollama_discord_bot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T18:19:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ji4p5v</id>
    <title>GPU &amp; Ollama Recommendations</title>
    <updated>2025-03-23T17:32:20+00:00</updated>
    <author>
      <name>/u/BenjaminForggoti</name>
      <uri>https://old.reddit.com/user/BenjaminForggoti</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've read through numerous similar posts, but as a complete beginner I'm not sure what difference do specific ollama models provide. &lt;/p&gt; &lt;p&gt;As a copywriter I would like to train an LLM locally to automate my tasks. The idea is to train it based on my writing style (which requires numerous prompts on ChatGPT &amp;amp; Grok that I need to input every single time). &lt;/p&gt; &lt;p&gt;I'm planning on building a first machine and as I understand GPU is the most important factor. &lt;/p&gt; &lt;p&gt;What model of GPU &amp;amp; Ollama would you recommend for this type of work? My budget for building a PC would be around $1000-$1200. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BenjaminForggoti"&gt; /u/BenjaminForggoti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji4p5v/gpu_ollama_recommendations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ji4p5v/gpu_ollama_recommendations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ji4p5v/gpu_ollama_recommendations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-23T17:32:20+00:00</published>
  </entry>
</feed>
