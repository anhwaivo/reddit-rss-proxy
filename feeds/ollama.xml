<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-04-16T22:06:08+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jyy4l7</id>
    <title>Online platform for running dolphin 3.0( or older version )</title>
    <updated>2025-04-14T12:53:23+00:00</updated>
    <author>
      <name>/u/MentalPainter5746</name>
      <uri>https://old.reddit.com/user/MentalPainter5746</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any free online platform for running dolphin 3.0(or older version) as I don't have powerful pc to run it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MentalPainter5746"&gt; /u/MentalPainter5746 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jyy4l7/online_platform_for_running_dolphin_30_or_older/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jyy4l7/online_platform_for_running_dolphin_30_or_older/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jyy4l7/online_platform_for_running_dolphin_30_or_older/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-14T12:53:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyw2fo</id>
    <title>What Happens When Two AIs Talk Alone?</title>
    <updated>2025-04-14T11:00:24+00:00</updated>
    <author>
      <name>/u/nik0rr</name>
      <uri>https://old.reddit.com/user/nik0rr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote a short analysis of a conversation between two AIs. It looks coherent at first, but it’s actually full of empty language, fake memory, and logical gaps.&lt;br /&gt; Here’s the article: &lt;a href="https://medium.com/@angeloai/two-ais-talk-to-each-other-the-result-is-unsettling-not-brilliant-f6a4b214abfd"&gt;https://medium.com/@angeloai/two-ais-talk-to-each-other-the-result-is-unsettling-not-brilliant-f6a4b214abfd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nik0rr"&gt; /u/nik0rr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jyw2fo/what_happens_when_two_ais_talk_alone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jyw2fo/what_happens_when_two_ais_talk_alone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jyw2fo/what_happens_when_two_ais_talk_alone/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-14T11:00:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jym9jq</id>
    <title>num_gpu parameter clearly underrated.</title>
    <updated>2025-04-14T00:31:16+00:00</updated>
    <author>
      <name>/u/GhostInThePudding</name>
      <uri>https://old.reddit.com/user/GhostInThePudding</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Ollama for a while with models that fit on my GPU (16GB VRAM), so num_gpu wasn't of much relevance to me.&lt;/p&gt; &lt;p&gt;However recently with Mistral Small3.1 and Gemma3:27b, I've found them to be massive improvements over smaller models, but just too frustratingly slow to put up with.&lt;/p&gt; &lt;p&gt;So I looked into any way I could tweak performance and found that by default, both models are using at little at 4-8GB of my VRAM. Just by setting the num_gpu parameter to a setting that increases use to around 15GB (35-45), I found my performance roughly doubled, from frustratingly slow to quite acceptable.&lt;/p&gt; &lt;p&gt;I noticed not a lot of people talk about the setting and just thought it was worth mentioning, because for me it means two models that I avoided using are now quite practical. I can even run Gemma3 with a 20k context size without a problem on 32GB system memory+16GB VRAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GhostInThePudding"&gt; /u/GhostInThePudding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jym9jq/num_gpu_parameter_clearly_underrated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jym9jq/num_gpu_parameter_clearly_underrated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jym9jq/num_gpu_parameter_clearly_underrated/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-14T00:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyyn7g</id>
    <title>Nvidia vs AMD GPU</title>
    <updated>2025-04-14T13:17:39+00:00</updated>
    <author>
      <name>/u/binarastrology</name>
      <uri>https://old.reddit.com/user/binarastrology</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;br /&gt; I've been researching what would be the best GPU to get for running local LLMs and I have found &lt;/p&gt; &lt;p&gt;&lt;strong&gt;ASRrock RX 7800 XT steel legend 16GB 256-bit&lt;/strong&gt; for around $500 which seems to me like a decent deal for the price.&lt;/p&gt; &lt;p&gt;However, upon further research I can see that a lot of people are recommending Nvidia only as if AMD is either hard to set up or doesn't work properly. &lt;/p&gt; &lt;p&gt;What are your thoughts on this and what would be the best approach?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/binarastrology"&gt; /u/binarastrology &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jyyn7g/nvidia_vs_amd_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jyyn7g/nvidia_vs_amd_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jyyn7g/nvidia_vs_amd_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-14T13:17:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz6nfd</id>
    <title>confused with ollama params</title>
    <updated>2025-04-14T18:49:23+00:00</updated>
    <author>
      <name>/u/Informal-Victory8655</name>
      <uri>https://old.reddit.com/user/Informal-Victory8655</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama_init_from_model: n_ctx = 8192&lt;/p&gt; &lt;p&gt;llama_init_from_model: n_ctx_per_seq = 2048&lt;/p&gt; &lt;p&gt;llama_init_from_model: n_batch = 2048&lt;/p&gt; &lt;p&gt;llama_init_from_model: n_ubatch = 512&lt;/p&gt; &lt;p&gt;llama_init_from_model: flash_attn = 0&lt;/p&gt; &lt;p&gt;llama_init_from_model: freq_base = 1000000.0&lt;/p&gt; &lt;p&gt;llama_init_from_model: freq_scale = 1&lt;/p&gt; &lt;p&gt;llama_init_from_model: n_ctx_per_seq (2048) &amp;lt; n_ctx_train (32768) -- the full capacity of the model will not be utilized&lt;/p&gt; &lt;p&gt;I'm running qwen2.5:7b on Nvidia T4 GPU.&lt;/p&gt; &lt;p&gt;what is n_ctx and n_ctx_per_seq?&lt;/p&gt; &lt;p&gt;and how I can increase context window of model and best tips for deployment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Informal-Victory8655"&gt; /u/Informal-Victory8655 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jz6nfd/confused_with_ollama_params/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jz6nfd/confused_with_ollama_params/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jz6nfd/confused_with_ollama_params/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-14T18:49:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz3w39</id>
    <title>Curious About Your ML Projects &amp; Challenges</title>
    <updated>2025-04-14T16:59:09+00:00</updated>
    <author>
      <name>/u/The_PaleKnight</name>
      <uri>https://old.reddit.com/user/The_PaleKnight</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I would like to learn more about your experiences with ML projects. I'm curious—what kind of challenges do you face when training your own models? For example, do resource limitations or cost factors ever hold you back?&lt;/p&gt; &lt;p&gt;My team and I are exploring ways to make things easier for people like us, so any insights or stories you'd be willing to share would be super helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The_PaleKnight"&gt; /u/The_PaleKnight &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jz3w39/curious_about_your_ml_projects_challenges/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jz3w39/curious_about_your_ml_projects_challenges/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jz3w39/curious_about_your_ml_projects_challenges/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-14T16:59:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyyeiz</id>
    <title>I built an AI Browser Agent!</title>
    <updated>2025-04-14T13:06:18+00:00</updated>
    <author>
      <name>/u/Any-Cockroach-3233</name>
      <uri>https://old.reddit.com/user/Any-Cockroach-3233</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Your browser just got a brain.&lt;br /&gt; Control any site with plain English&lt;br /&gt; GPT-4o Vision + DOM understanding&lt;br /&gt; Automate tasks: shop, extract data, fill forms &lt;/p&gt; &lt;p&gt;100% open source&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/manthanguptaa/real-world-llm-apps"&gt;https://github.com/manthanguptaa/real-world-llm-apps&lt;/a&gt; (star it if you find value in it)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Cockroach-3233"&gt; /u/Any-Cockroach-3233 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jyyeiz/i_built_an_ai_browser_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jyyeiz/i_built_an_ai_browser_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jyyeiz/i_built_an_ai_browser_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-14T13:06:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzkfl1</id>
    <title>Im unable to pull open source models on my macOS</title>
    <updated>2025-04-15T05:49:44+00:00</updated>
    <author>
      <name>/u/Prestigious-Cup-5161</name>
      <uri>https://old.reddit.com/user/Prestigious-Cup-5161</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jzkfl1/im_unable_to_pull_open_source_models_on_my_macos/"&gt; &lt;img alt="Im unable to pull open source models on my macOS" src="https://preview.redd.it/5ds16b8gtxue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4194918946851d83eb01ed668a72764c00b26f4d" title="Im unable to pull open source models on my macOS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the error that i get. Could someone please help me out on what I can do to rectify this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prestigious-Cup-5161"&gt; /u/Prestigious-Cup-5161 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5ds16b8gtxue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jzkfl1/im_unable_to_pull_open_source_models_on_my_macos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jzkfl1/im_unable_to_pull_open_source_models_on_my_macos/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-15T05:49:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzf0b0</id>
    <title>QWQ 32B</title>
    <updated>2025-04-15T00:52:42+00:00</updated>
    <author>
      <name>/u/Alarming-Poetry-5434</name>
      <uri>https://old.reddit.com/user/Alarming-Poetry-5434</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What configuration do you recommend me for a custom model from qwq32b to parse files from github repositories, gitlab and search for sensitive information to be as accurate as possible by having a true or false response from the general repo after parsing the files and a simple description of what it found.&lt;/p&gt; &lt;p&gt;I have the following setup, I appreciate your help:&lt;/p&gt; &lt;p&gt;PARAMETER temperature 0.0&lt;br /&gt; PARAMETER top_p 0.85&lt;br /&gt; PARAMETER top_k 40&lt;br /&gt; PARAMETER repeat_penalty 1.0&lt;br /&gt; PARAMETER num_ctx 8192&lt;br /&gt; PARAMETER num_predict 512&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alarming-Poetry-5434"&gt; /u/Alarming-Poetry-5434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jzf0b0/qwq_32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jzf0b0/qwq_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jzf0b0/qwq_32b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-15T00:52:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzy2a6</id>
    <title>Why instalation creates a new user account?</title>
    <updated>2025-04-15T17:40:50+00:00</updated>
    <author>
      <name>/u/wektor420</name>
      <uri>https://old.reddit.com/user/wektor420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Only other software that does it is docker, but I see no reason for it in ollama&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wektor420"&gt; /u/wektor420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jzy2a6/why_instalation_creates_a_new_user_account/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jzy2a6/why_instalation_creates_a_new_user_account/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jzy2a6/why_instalation_creates_a_new_user_account/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-15T17:40:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzp31n</id>
    <title>Simple tool to backup Ollama models as .tar files</title>
    <updated>2025-04-15T11:06:39+00:00</updated>
    <author>
      <name>/u/EfeArdaYILDIRIM</name>
      <uri>https://old.reddit.com/user/EfeArdaYILDIRIM</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jzp31n/simple_tool_to_backup_ollama_models_as_tar_files/"&gt; &lt;img alt="Simple tool to backup Ollama models as .tar files" src="https://external-preview.redd.it/sr7XqdeKF73E4m8CFm57jK-VSCmixf5xr3cX1tdw1SY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ac9ce6a0d25fae7f6d6d78c177a7289c0eb8c68" title="Simple tool to backup Ollama models as .tar files" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I made a small CLI tool in Node.js that lets you export your local Ollama models as &lt;code&gt;.tar&lt;/code&gt; files.&lt;br /&gt; Helps with backups or moving models between systems.&lt;br /&gt; Pretty basic, just runs from the terminal.&lt;br /&gt; Maybe someone finds it useful :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/ollama-export"&gt;https://www.npmjs.com/package/ollama-export&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EfeArdaYILDIRIM"&gt; /u/EfeArdaYILDIRIM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.npmjs.com/package/ollama-export"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jzp31n/simple_tool_to_backup_ollama_models_as_tar_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jzp31n/simple_tool_to_backup_ollama_models_as_tar_files/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-15T11:06:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1k04xe8</id>
    <title>How to set temperature in Ollama command-line?</title>
    <updated>2025-04-15T22:26:19+00:00</updated>
    <author>
      <name>/u/Disonantemus</name>
      <uri>https://old.reddit.com/user/Disonantemus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wish to set the temperature, to test models and see the results with mini bash shell scripts, but I can't find a way to this from CLI, I know that: &lt;/p&gt; &lt;p&gt;&lt;em&gt;Example:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama run gemma3:4b &amp;quot;Summarize the following text: &amp;quot; &amp;lt; input.txt &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Using API is possible, maybe with curl or external apps, but is not the point.&lt;/li&gt; &lt;li&gt;&lt;p&gt;Is possible from interactive mode with:&lt;/p&gt; &lt;p&gt;&amp;gt;&amp;gt;&amp;gt; /set parameter temperature 0.2&lt;br /&gt; Set parameter 'temperature' to '0.2'&lt;/p&gt; &lt;p&gt;but in that mode you can't &lt;strong&gt;include text files&lt;/strong&gt; yet (only images for visual models).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I know is possible to do in &lt;code&gt;llama-cpp&lt;/code&gt; and maybe others similar to &lt;code&gt;ollama&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;There is a way to do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disonantemus"&gt; /u/Disonantemus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k04xe8/how_to_set_temperature_in_ollama_commandline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k04xe8/how_to_set_temperature_in_ollama_commandline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k04xe8/how_to_set_temperature_in_ollama_commandline/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-15T22:26:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jzm47g</id>
    <title>Run LLMs 100% Locally with Docker’s New Model Runner</title>
    <updated>2025-04-15T07:44:48+00:00</updated>
    <author>
      <name>/u/Arindam_200</name>
      <uri>https://old.reddit.com/user/Arindam_200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Folks,&lt;/p&gt; &lt;p&gt;I’ve been exploring ways to run LLMs locally, partly to avoid API limits, partly to test stuff offline, and mostly because… it's just fun to see it all work on your own machine. : )&lt;/p&gt; &lt;p&gt;That’s when I came across &lt;strong&gt;Docker’s new Model Runner&lt;/strong&gt;, and wow! it makes spinning up open-source LLMs locally &lt;em&gt;so easy&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;So I recorded a quick walkthrough video showing how to get started:&lt;/p&gt; &lt;p&gt;🎥 &lt;strong&gt;Video Guide&lt;/strong&gt;: &lt;a href="https://www.youtube.com/watch?v=RH_vdF2iGdo"&gt;Check it here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you’re building AI apps, working on agents, or just want to run models locally, this is definitely worth a look. It fits right into any existing Docker setup too.&lt;/p&gt; &lt;p&gt;Would love to hear if others are experimenting with it or have favorite local LLMs worth trying!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arindam_200"&gt; /u/Arindam_200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jzm47g/run_llms_100_locally_with_dockers_new_model_runner/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jzm47g/run_llms_100_locally_with_dockers_new_model_runner/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jzm47g/run_llms_100_locally_with_dockers_new_model_runner/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-15T07:44:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0oyzq</id>
    <title>VRAM Pro: Instantly unlock more graphics memory on your Mac for large LLMs</title>
    <updated>2025-04-16T16:41:28+00:00</updated>
    <author>
      <name>/u/DazzlingHedgehog6650</name>
      <uri>https://old.reddit.com/user/DazzlingHedgehog6650</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The VRAM Pro app let's you allocate up to 99% of your mac silicon RAM to VRAM: &lt;a href="https://vrampro.com/"&gt;Check out the VRAM Pro app&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DazzlingHedgehog6650"&gt; /u/DazzlingHedgehog6650 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0oyzq/vram_pro_instantly_unlock_more_graphics_memory_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0oyzq/vram_pro_instantly_unlock_more_graphics_memory_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0oyzq/vram_pro_instantly_unlock_more_graphics_memory_on/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T16:41:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1k04wsa</id>
    <title>How much VRAM and how many GPUs to fine-tune a 70B parameter model like LLaMA 3.1 locally?</title>
    <updated>2025-04-15T22:25:34+00:00</updated>
    <author>
      <name>/u/Aaron_MLEngineer</name>
      <uri>https://old.reddit.com/user/Aaron_MLEngineer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’m planning to fine-tune a &lt;strong&gt;70B parameter model&lt;/strong&gt; like &lt;strong&gt;LLaMA 3.1&lt;/strong&gt; locally. I know it needs around &lt;strong&gt;280GB VRAM&lt;/strong&gt; for the model weights alone, and more for gradients/activations. With a &lt;strong&gt;16GB VRAM&lt;/strong&gt; GPU like the &lt;strong&gt;RTX 5070 Ti&lt;/strong&gt;, that would mean needing about &lt;strong&gt;18 GPUs&lt;/strong&gt; to handle it.&lt;/p&gt; &lt;p&gt;At $600 per GPU, that’s around &lt;strong&gt;$10,800&lt;/strong&gt; just for the GPUs.&lt;/p&gt; &lt;p&gt;Does that sound right, or am I missing something? Would love to hear from anyone who’s worked with large models like this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaron_MLEngineer"&gt; /u/Aaron_MLEngineer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k04wsa/how_much_vram_and_how_many_gpus_to_finetune_a_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k04wsa/how_much_vram_and_how_many_gpus_to_finetune_a_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k04wsa/how_much_vram_and_how_many_gpus_to_finetune_a_70b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-15T22:25:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0aff3</id>
    <title>Persistent Local Memory for Your Models</title>
    <updated>2025-04-16T03:01:46+00:00</updated>
    <author>
      <name>/u/GVDub2</name>
      <uri>https://old.reddit.com/user/GVDub2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just updated my PanAI Seed Node project with a nice little sub-project that provides local memory with analysis and reflection for your local models, doing embedding to a Qdrant database and allowing semantic analysis , reflection and even a little dreaming. It’s all at &lt;a href="https://github.com/GVDub/panai-seed-node"&gt;https://github.com/GVDub/panai-seed-node&lt;/a&gt; as an optional part of the project (which I’m not able to work on as quickly as I’d like or I think it deserves). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GVDub2"&gt; /u/GVDub2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0aff3/persistent_local_memory_for_your_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0aff3/persistent_local_memory_for_your_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0aff3/persistent_local_memory_for_your_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T03:01:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0rj64</id>
    <title>Morphik just hit 1k stars - Thank you!</title>
    <updated>2025-04-16T18:23:26+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;I'm grateful and happy to announce that our repository, &lt;a href="https://github.com/morphik-org/morphik-core"&gt;Morphik&lt;/a&gt;, just hit 1k stars! This really wouldn't have been possible without the support of the &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; community, and I'm just writing this post to say thanks :) &lt;/p&gt; &lt;p&gt;As another thank you, we want to help solve your most difficult, annoying, expensive, or time consuming problems with documents and multimodal data. Reply to this post with your most pressing issues - eg. &amp;quot;I have x PDFs and I'm trying to get structured information out of them&amp;quot;, or &amp;quot;I have a 1000 files of game footage, and I want to cut highlights featuring player y&amp;quot;, etc. We'll have a feature or implementation that fixes that up within a week :)&lt;/p&gt; &lt;p&gt;Thanks again! &lt;/p&gt; &lt;p&gt;Sending love from SF &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0rj64/morphik_just_hit_1k_stars_thank_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0rj64/morphik_just_hit_1k_stars_thank_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0rj64/morphik_just_hit_1k_stars_thank_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T18:23:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0hyc9</id>
    <title>Any kind of digital assistant Android App with Ollama compatibility?</title>
    <updated>2025-04-16T11:21:09+00:00</updated>
    <author>
      <name>/u/Final_Wheel_7486</name>
      <uri>https://old.reddit.com/user/Final_Wheel_7486</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello to you all,&lt;/p&gt; &lt;p&gt;as you may or may not know, Android provides the capability for apps to register as &amp;quot;digital assistants&amp;quot;, allowing them to be pulled up by swiping from a corner or, sometimes, pressing and holding the power button. Gemini, for example, uses this API.&lt;/p&gt; &lt;p&gt;Is there any kind of open-source digital assistant app that's as accessible as well, but instead using Ollama or something locally/self-hosted?&lt;/p&gt; &lt;p&gt;It would take the usability and helpfulness of self hosted AI to a new level for me.&lt;/p&gt; &lt;p&gt;Greets!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Final_Wheel_7486"&gt; /u/Final_Wheel_7486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0hyc9/any_kind_of_digital_assistant_android_app_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0hyc9/any_kind_of_digital_assistant_android_app_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0hyc9/any_kind_of_digital_assistant_android_app_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T11:21:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0l14e</id>
    <title>Is there a model around the size of Gemma3:4B that is better than Gemma3:4B for questions such as "Give me a tip about vim"? I want to run it once a day in Conky for daily tips.</title>
    <updated>2025-04-16T13:55:47+00:00</updated>
    <author>
      <name>/u/___nutthead___</name>
      <uri>https://old.reddit.com/user/___nutthead___</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The small binary size + its generic nature makes me think it probably doesn't know much about vim, but I could be wrong.&lt;/p&gt; &lt;p&gt;Anyway, any alternatives that you think I should give a go, but not much larger than Gemma3:4B?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/___nutthead___"&gt; /u/___nutthead___ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0l14e/is_there_a_model_around_the_size_of_gemma34b_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0l14e/is_there_a_model_around_the_size_of_gemma34b_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0l14e/is_there_a_model_around_the_size_of_gemma34b_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T13:55:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0wgyv</id>
    <title>Made this text replacement tool using Ollama and shell scripting [LINUX ONLY]</title>
    <updated>2025-04-16T21:49:05+00:00</updated>
    <author>
      <name>/u/Sweaty_Advance1172</name>
      <uri>https://old.reddit.com/user/Sweaty_Advance1172</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k0wgyv/made_this_text_replacement_tool_using_ollama_and/"&gt; &lt;img alt="Made this text replacement tool using Ollama and shell scripting [LINUX ONLY]" src="https://external-preview.redd.it/NTRieHZrMGlwOXZlMVqD_vMUP0hgQsuXG-MfaXkaE4rdI5NsUCZLb2_L6nm-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b18307393ff2f81f2ebb017da903955d50245dbb" title="Made this text replacement tool using Ollama and shell scripting [LINUX ONLY]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last week I installed Grammarly on my laptop, and they had this one feature where you could select the entire text, and then it will rewrite the whole thing with improved grammar, but only 3 such replacements were possible every day.&lt;/p&gt; &lt;p&gt;This got me wondering, can I do it using LLMs and some shell scripting, and so Betterwrite was born.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sweaty_Advance1172"&gt; /u/Sweaty_Advance1172 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2n23rb0ip9ve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0wgyv/made_this_text_replacement_tool_using_ollama_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0wgyv/made_this_text_replacement_tool_using_ollama_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T21:49:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0slhs</id>
    <title>check local/cloud orchestration -- fully open source</title>
    <updated>2025-04-16T19:06:52+00:00</updated>
    <author>
      <name>/u/Emotional-Evening-62</name>
      <uri>https://old.reddit.com/user/Emotional-Evening-62</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a video that orchestrates between local/cloud models; Fully open source, would love to hear from community:&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/j0dOVWWzBrE?si=lHISeYU992irM-7p"&gt;https://youtu.be/j0dOVWWzBrE?si=lHISeYU992irM-7p&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emotional-Evening-62"&gt; /u/Emotional-Evening-62 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0slhs/check_localcloud_orchestration_fully_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0slhs/check_localcloud_orchestration_fully_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0slhs/check_localcloud_orchestration_fully_open_source/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T19:06:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0fn6y</id>
    <title>How do you finetune a model?</title>
    <updated>2025-04-16T08:44:57+00:00</updated>
    <author>
      <name>/u/ChikyScaresYou</name>
      <uri>https://old.reddit.com/user/ChikyScaresYou</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm still pretty new to this topic, but I've seen that some of fhe LLMs i'm running are fine tunned to specifix topics. There are, however, other topics where I havent found anything fine tunned to it. So, how do people fine tune LLMs? Does it rewuire too much processing power? Is it even worth it? &lt;/p&gt; &lt;p&gt;And how do you make an LLM &amp;quot;learn&amp;quot; a large text like a novel?&lt;/p&gt; &lt;p&gt;I'm asking becausey current method uses very small chunks in a chromadb database, but it seems that the &amp;quot;material&amp;quot; the LLM retrieves is minuscule in comparison to the entire novel. I thought the LLM would have access to the entire novel now that it's in a database, but it doesnt seem to be the case. Also, still unsure how RAG works, as it seems that it's basicallt creating a database of the documents as well, which turns out to have the same issue....&lt;/p&gt; &lt;p&gt;o, I was thinking, could I finetune an LLM to know everything that happens in the novel and be able to answer any question about it, regardless of how detailed? And, in addition, I'd like to make an LLM fine tuned with military and police knowledge in attack and defense for factchecking. I'd like to know how to do that, or if that's the wrong approach, if you could point me in the right direction and share resources, i'd appreciate it, thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChikyScaresYou"&gt; /u/ChikyScaresYou &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0fn6y/how_do_you_finetune_a_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0fn6y/how_do_you_finetune_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0fn6y/how_do_you_finetune_a_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T08:44:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0iwng</id>
    <title>LLM's too supportive?</title>
    <updated>2025-04-16T12:14:02+00:00</updated>
    <author>
      <name>/u/Hedwig2222</name>
      <uri>https://old.reddit.com/user/Hedwig2222</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;hopefully what I'm asking makes sense and wasn't too sure on how to title this. But for example with ChatGPT and other big ones like Gemini. I've noticed that pretty much everything you try to talk about with it, it usually always is very supportive of you. Regardless of the topic. They seem very overly supportive most of the time. Where as a regular person would be more realistic as in they would be more neutral and realistic about the situation/topic you're discussing. For example ChatGPT is often overly supportive and optimistic I think. Like if you were to talk about a bad job interview, and maybe you havent heard back from them when you expected to, ChatGPT would still be very supportive and overly optimistic that you still have a chance etc. Where as a real person, a close friend or family member could be like &amp;quot;Yeah...sorry bud, looks like you effed up that interview, better start applying to more jobs&amp;quot;&lt;/p&gt; &lt;p&gt;Am I making sense? It seems the big LLMs' like ChatGPT and Google gemini are programmed in this way to be ultra supportive and optimistic for you rather than realistic. Which I find annoying because I sometimes feel that I'm not getting a truthful answer on the topic or situation shared. I've found even the uncensored ones can be like this also.&lt;/p&gt; &lt;p&gt;Is this just a limitation of todays LLM's? They will either be overly supportive and optimistic for you regardless of the facts, alternatively if not programmed like this they would be the opposite and just not useful at all lol. Or are there actually decent Models out there that are more realistic on a personal level when discussing topics and situations with them where they won't always be supportive and optimistic just because, but they will be more realistic as in, agreeing you're a bit screwed in said situation such as the above bad interview example and not being overly optimistic you still have a chance etc and instead be more like.. yeah, you screwed up, better start looking for new jobs lol. I assume it would be an uncensored model? But which one do you guys find is the best for a more realistic conversation on life and things?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hedwig2222"&gt; /u/Hedwig2222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0iwng/llms_too_supportive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0iwng/llms_too_supportive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0iwng/llms_too_supportive/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T12:14:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0ndas</id>
    <title>OSS SDK to automate your Windows computer in JS or Python. 100x faster and cheaper than OpenAI Operator or Anthropic Computer Use</title>
    <updated>2025-04-16T15:34:54+00:00</updated>
    <author>
      <name>/u/louis3195</name>
      <uri>https://old.reddit.com/user/louis3195</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k0ndas/oss_sdk_to_automate_your_windows_computer_in_js/"&gt; &lt;img alt="OSS SDK to automate your Windows computer in JS or Python. 100x faster and cheaper than OpenAI Operator or Anthropic Computer Use" src="https://external-preview.redd.it/cm1jaWR2d2J1N3ZlMeJLErIi_ot-KBDYl6VMuMkZ36iBX_i-T6LEtxagz-fp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08f39be6dc1ae2191a10e26bf1e08175886e9bfd" title="OSS SDK to automate your Windows computer in JS or Python. 100x faster and cheaper than OpenAI Operator or Anthropic Computer Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;yo all, i've been working on an OSS SDK that uses OS-level APIs to provide a Playwright-like easy DX to control your computer in python, TS, or anything else, &lt;/p&gt; &lt;p&gt;making it 100x faster than vision approach used by OpenAI and Anthropic while being model agnostic, compatible with ollama/OSS model or even gemini etc.&lt;/p&gt; &lt;p&gt;would love your thoughts, feedback, or any tinkering with ollama 🙏 &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mediar-ai/terminator"&gt;https://github.com/mediar-ai/terminator&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/louis3195"&gt; /u/louis3195 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8x51dtwbu7ve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0ndas/oss_sdk_to_automate_your_windows_computer_in_js/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0ndas/oss_sdk_to_automate_your_windows_computer_in_js/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T15:34:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1k0g8fp</id>
    <title>No API keys, no cloud. Just local AI + tools that actually work. Too much to ask?</title>
    <updated>2025-04-16T09:28:15+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1k0g8fp/no_api_keys_no_cloud_just_local_ai_tools_that/"&gt; &lt;img alt="No API keys, no cloud. Just local AI + tools that actually work. Too much to ask?" src="https://external-preview.redd.it/Q3SKDCrADGVEsTRTkQ-Dz8wknf8WPBvAR2OOxzRDVdY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5677a7a9cee67696561f40dacb232612cdd9b8cd" title="No API keys, no cloud. Just local AI + tools that actually work. Too much to ask?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It’s been about a month since I first posted Clara here.&lt;/p&gt; &lt;p&gt;Clara is a local-first AI assistant — think of it like ChatGPT, but fully private and running on your own machine using Ollama.&lt;/p&gt; &lt;p&gt;Since the initial release, I’ve had a small group of users try it out, and I’ve pushed several updates based on real usage and feedback.&lt;/p&gt; &lt;p&gt;The biggest update is that &lt;strong&gt;Clara now comes with n8n built-in&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;That means you can now build and run your own tools directly inside the assistant — no setup needed, no external services. Just open Clara and start automating.&lt;/p&gt; &lt;p&gt;With the n8n integration, Clara can now do more than chat. You can use it to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Check your emails&lt;/li&gt; &lt;li&gt;Manage your calendar&lt;/li&gt; &lt;li&gt;Call APIs&lt;/li&gt; &lt;li&gt;Run scheduled tasks&lt;/li&gt; &lt;li&gt;Process webhooks&lt;/li&gt; &lt;li&gt;Connect to databases&lt;/li&gt; &lt;li&gt;And anything else you can wire up using n8n’s visual flow builder&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The assistant can trigger these workflows directly — so you can talk to Clara and ask it to do real tasks, using tools that run entirely on your device.&lt;/p&gt; &lt;p&gt;Everything happens locally. No data goes out, no accounts, no cloud dependency.&lt;/p&gt; &lt;p&gt;If you're someone who wants full control of your AI and automation setup, this might be something worth trying.&lt;/p&gt; &lt;p&gt;You can check out the project here:&lt;br /&gt; GitHub: &lt;a href="https://github.com/badboysm890/ClaraVerse"&gt;https://github.com/badboysm890/ClaraVerse&lt;/a&gt;&lt;br /&gt; Web version (Ollama required): &lt;a href="https://clara.badboysm890.in"&gt;https://clara.badboysm890.in&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks to everyone who's been trying it and sending feedback. Still improving things — more updates soon.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; I'm aware of great projects like OpenWebUI and LibreChat. Clara takes a slightly different approach — focusing on reducing dependencies, offering a native desktop app, and making the overall experience more user-friendly so that more people can easily get started with local AI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/badboysm890/ClaraVerse"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1k0g8fp/no_api_keys_no_cloud_just_local_ai_tools_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1k0g8fp/no_api_keys_no_cloud_just_local_ai_tools_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-04-16T09:28:15+00:00</published>
  </entry>
</feed>
