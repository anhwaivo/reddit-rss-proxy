<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-23T16:05:57+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ivk0vm</id>
    <title>Ollama UI on iPhone</title>
    <updated>2025-02-22T14:33:49+00:00</updated>
    <author>
      <name>/u/SplittyDev</name>
      <uri>https://old.reddit.com/user/SplittyDev</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SplittyDev"&gt; /u/SplittyDev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/shorts/bzEXF1SS70Y?si=hSWRNEm2vZ8BZm_x"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivk0vm/ollama_ui_on_iphone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivk0vm/ollama_ui_on_iphone/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T14:33:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1iv2r5s</id>
    <title>Uncensored model for novel writing</title>
    <updated>2025-02-21T21:50:06+00:00</updated>
    <author>
      <name>/u/SnooDogs7610</name>
      <uri>https://old.reddit.com/user/SnooDogs7610</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am writing a book and when I get a bit stuck, have been using AI to help get me started. It is a murder mystery and I have been running into roadblocks due to the content, is there an uncensored model that has a large enough token memory and the creative ability? I have tried quite a few different models on ollama, but there are so many so I'm sure I've missed one. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooDogs7610"&gt; /u/SnooDogs7610 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iv2r5s/uncensored_model_for_novel_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iv2r5s/uncensored_model_for_novel_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iv2r5s/uncensored_model_for_novel_writing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-21T21:50:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivpetq</id>
    <title>Help Needed: Creating a Multi-Agent System with Ollama for Different API Endpoints</title>
    <updated>2025-02-22T18:27:10+00:00</updated>
    <author>
      <name>/u/FlimsyAd9995</name>
      <uri>https://old.reddit.com/user/FlimsyAd9995</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;I'm working on a project where I want to create a multi-agent system using Ollama's LLM. The goal is to use three different API endpoints to retrieve information based on user queries:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Order Details: Retrieves order-related information.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;ServiceNow Incident Details: Retrieves incident details from ServiceNow.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Wikipedia: Retrieves general-information from Wikipedia&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here's the use case:&lt;/p&gt; &lt;p&gt;If a user asks a question about an order, the system should use the order API endpoint, process the response through the trained LLM model, and display it in the chat.&lt;/p&gt; &lt;p&gt;If the user asks about incident details in ServiceNow, it should retrieve the data from the ServiceNow API, process it through the LLM, and show the response.&lt;/p&gt; &lt;p&gt;• For general queries, it should fetch data from Wikipedia, process it, and display the response.&lt;/p&gt; &lt;p&gt;The problem I'm facing is that the system always responds to order-related queries but fails to answer incident queries and general queries. It seems to be stuck on the order API endpoint.&lt;/p&gt; &lt;p&gt;Has anyone faced a similar issue or can provide guidance on how to properly route the queries to the correct API endpoint and process them through the LLM? Any help or suggestions would be greatly appreciated!&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FlimsyAd9995"&gt; /u/FlimsyAd9995 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivpetq/help_needed_creating_a_multiagent_system_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivpetq/help_needed_creating_a_multiagent_system_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivpetq/help_needed_creating_a_multiagent_system_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T18:27:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivxwwp</id>
    <title>Let say you got a built with 120gb of vram, what would you try first or do with it</title>
    <updated>2025-02-23T00:53:38+00:00</updated>
    <author>
      <name>/u/voidwater1</name>
      <uri>https://old.reddit.com/user/voidwater1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/voidwater1"&gt; /u/voidwater1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivxwwp/let_say_you_got_a_built_with_120gb_of_vram_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivxwwp/let_say_you_got_a_built_with_120gb_of_vram_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivxwwp/let_say_you_got_a_built_with_120gb_of_vram_what/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T00:53:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivmzj3</id>
    <title>ollama vs HF API</title>
    <updated>2025-02-22T16:46:35+00:00</updated>
    <author>
      <name>/u/mans-987</name>
      <uri>https://old.reddit.com/user/mans-987</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any comparison between Ollama and HF API for vision LLMs? &lt;/p&gt; &lt;p&gt;In my experience, I noted that when I am asking questions about an image using HF API, the model (in this case &amp;quot;&lt;a href="https://ollama.com/library/moondream"&gt;moondream&lt;/a&gt;&amp;quot; answers better and more accurately than when I am using Ollama. In the comparison, I used the same image and the same prompt but left the other parameters as default (for example, system prompt, temperature...)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mans-987"&gt; /u/mans-987 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivmzj3/ollama_vs_hf_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivmzj3/ollama_vs_hf_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivmzj3/ollama_vs_hf_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T16:46:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivue0z</id>
    <title>DeepSeek R1 Local Setup - Ollama</title>
    <updated>2025-02-22T22:05:09+00:00</updated>
    <author>
      <name>/u/Prize_Appearance_67</name>
      <uri>https://old.reddit.com/user/Prize_Appearance_67</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ivue0z/deepseek_r1_local_setup_ollama/"&gt; &lt;img alt="DeepSeek R1 Local Setup - Ollama" src="https://external-preview.redd.it/E37PN7tEWZv7HqSj3gKtTeckqNH8ndwYLiEDay9WAO8.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a37293656dccd074237e8fc34eeb4371786d5d0f" title="DeepSeek R1 Local Setup - Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prize_Appearance_67"&gt; /u/Prize_Appearance_67 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/DgO4v8aJGQI?si=CcyJdNOjiNsTkMqU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivue0z/deepseek_r1_local_setup_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivue0z/deepseek_r1_local_setup_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T22:05:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivztqn</id>
    <title>Response speed seems very slow.</title>
    <updated>2025-02-23T02:33:07+00:00</updated>
    <author>
      <name>/u/Efficient_Try8674</name>
      <uri>https://old.reddit.com/user/Efficient_Try8674</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ivztqn/response_speed_seems_very_slow/"&gt; &lt;img alt="Response speed seems very slow." src="https://b.thumbs.redditmedia.com/BYZEqLfzJFHvSXiwbKJ_kQMwbBhRA04JTRZGpS1t_As.jpg" title="Response speed seems very slow." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently getting this response speed. I'm running llama3.3 on 1x 3090. Is this what I should be expecting?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dc8fj6kvvske1.png?width=544&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1e8048808bbcbc28d570cd21746af1717d4173f"&gt;https://preview.redd.it/dc8fj6kvvske1.png?width=544&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1e8048808bbcbc28d570cd21746af1717d4173f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Efficient_Try8674"&gt; /u/Efficient_Try8674 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivztqn/response_speed_seems_very_slow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivztqn/response_speed_seems_very_slow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivztqn/response_speed_seems_very_slow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T02:33:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivvrpw</id>
    <title>Ollama structured output</title>
    <updated>2025-02-22T23:09:01+00:00</updated>
    <author>
      <name>/u/mans-987</name>
      <uri>https://old.reddit.com/user/mans-987</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How can I change the sample code here: &lt;a href="https://github.com/ollama/ollama-python/blob/main/examples/structured-outputs-image.py"&gt;https://github.com/ollama/ollama-python/blob/main/examples/structured-outputs-image.py&lt;/a&gt;&lt;/p&gt; &lt;p&gt;to return the bounding box of the objects that it can see in the image?&lt;/p&gt; &lt;p&gt;I tried to change the code and add bonding_box to the object as follows:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Define the schema for image objects class Object(BaseModel): name: str confidence: float attributes: str bounding_box: tuple[int, int, int, int] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;but the models (in my case 'llama3.2-vision:90b' always return (0,0,1,1) for all objects.&lt;/p&gt; &lt;p&gt;How can I change the system and/or user prompt to ask the model to fill these values too?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mans-987"&gt; /u/mans-987 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivvrpw/ollama_structured_output/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivvrpw/ollama_structured_output/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivvrpw/ollama_structured_output/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T23:09:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1iv7hp9</id>
    <title>Ollama web Search Part 2</title>
    <updated>2025-02-22T01:45:07+00:00</updated>
    <author>
      <name>/u/Pure-Caramel1216</name>
      <uri>https://old.reddit.com/user/Pure-Caramel1216</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As promised, here is the GitHub repository for Ollama Web Search.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/nik549/ollama-search"&gt;GitHub Link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In my previous post, I mentioned plans to launch this project as a tool. If you’re interested in the tool and want to stay updated, please subscribe with &lt;a href="https://subscribepage.io/Ollamaweb"&gt;email&lt;/a&gt; for the latest news and developments.&lt;/p&gt; &lt;p&gt;Looking ahead, two additional versions are in the works:&lt;/p&gt; &lt;p&gt;One version will be faster but slightly less accurate.&lt;/p&gt; &lt;p&gt;The other will be slower yet more precise.&lt;/p&gt; &lt;p&gt;To help the project reach a wider audience, please consider upvoting if you’d like to see further developments.&lt;/p&gt; &lt;p&gt;P.S. Email subscribers will receive all updates first, and I’ll reserve subreddit posts for the most important announcements.&lt;/p&gt; &lt;p&gt;P.S.S. I’d love your suggestions for a name for the tool.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pure-Caramel1216"&gt; /u/Pure-Caramel1216 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iv7hp9/ollama_web_search_part_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iv7hp9/ollama_web_search_part_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iv7hp9/ollama_web_search_part_2/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T01:45:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivqz1x</id>
    <title>Buying a prebuilt desktop, 8GB VRAM, ~$500 budget?</title>
    <updated>2025-02-22T19:33:27+00:00</updated>
    <author>
      <name>/u/No-Abalone1029</name>
      <uri>https://old.reddit.com/user/No-Abalone1029</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Noticed there's a good amount of discussion on building custom setups, I suppose I'd be interested in that, but firstly was curious about purchasing a gaming desktop and just dedicating that to be my 24/7 LLM server at home.&lt;/p&gt; &lt;p&gt;8GB Vram is optimal because it'd let me tinker with a small but good enough LLM. I just don't know the best way to go about this as I'm new to home server development (and GPUs for that matter).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Abalone1029"&gt; /u/No-Abalone1029 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivqz1x/buying_a_prebuilt_desktop_8gb_vram_500_budget/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivqz1x/buying_a_prebuilt_desktop_8gb_vram_500_budget/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivqz1x/buying_a_prebuilt_desktop_8gb_vram_500_budget/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T19:33:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivdshw</id>
    <title>How much of a difference does a GPU Make?</title>
    <updated>2025-02-22T07:57:58+00:00</updated>
    <author>
      <name>/u/Rerouter_</name>
      <uri>https://old.reddit.com/user/Rerouter_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've a 3960X Threadripper with 256GB of RAM which is handling the larger models reasonably well on CPU only ~5 tokens / second for the 2.51bit 671B. &lt;/p&gt; &lt;p&gt;I'm curious if adding say 3x 3060's (Going for pretty cheap nearby) into the machine would make much of a difference seeing as their RAM would not be adding much to the picture, mainly just the ability to process the model faster.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rerouter_"&gt; /u/Rerouter_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivdshw/how_much_of_a_difference_does_a_gpu_make/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivdshw/how_much_of_a_difference_does_a_gpu_make/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivdshw/how_much_of_a_difference_does_a_gpu_make/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T07:57:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivsbp0</id>
    <title>8x AMD Instinct Mi60 Server + Llama-3.3-70B-Instruct + vLLM + Tensor Parallelism -&gt; 25.6t/s</title>
    <updated>2025-02-22T20:32:44+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/87hq6azi3rke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivsbp0/8x_amd_instinct_mi60_server_llama3370binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivsbp0/8x_amd_instinct_mi60_server_llama3370binstruct/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T20:32:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1iuxn1v</id>
    <title>Perplexity’s R1 1776 is now available in Ollama's library.</title>
    <updated>2025-02-21T18:17:40+00:00</updated>
    <author>
      <name>/u/Frisky_777</name>
      <uri>https://old.reddit.com/user/Frisky_777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iuxn1v/perplexitys_r1_1776_is_now_available_in_ollamas/"&gt; &lt;img alt="Perplexity’s R1 1776 is now available in Ollama's library." src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="Perplexity’s R1 1776 is now available in Ollama's library." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Frisky_777"&gt; /u/Frisky_777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/library/r1-1776"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iuxn1v/perplexitys_r1_1776_is_now_available_in_ollamas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iuxn1v/perplexitys_r1_1776_is_now_available_in_ollamas/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-21T18:17:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivsiza</id>
    <title>Any AI model for detecting accent?</title>
    <updated>2025-02-22T20:41:37+00:00</updated>
    <author>
      <name>/u/texasdude11</name>
      <uri>https://old.reddit.com/user/texasdude11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a non native English Speaker. I'm trying to build an app that can score &amp;quot;mispronounced&amp;quot; words per accent (let's say american accent from MN state). &lt;/p&gt; &lt;p&gt;Is there any model like that, that I can use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/texasdude11"&gt; /u/texasdude11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivsiza/any_ai_model_for_detecting_accent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivsiza/any_ai_model_for_detecting_accent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivsiza/any_ai_model_for_detecting_accent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T20:41:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivrfjt</id>
    <title>8x AMD Instinct Mi50 Server + Llama-3.3-70B-Instruct + vLLM + Tensor Parallelism -&gt; 25t/s</title>
    <updated>2025-02-22T19:53:29+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ort8fxcawqke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivrfjt/8x_amd_instinct_mi50_server_llama3370binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivrfjt/8x_amd_instinct_mi50_server_llama3370binstruct/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T19:53:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivd3ip</id>
    <title>Ollama frontend using ChatterUI</title>
    <updated>2025-02-22T07:08:47+00:00</updated>
    <author>
      <name>/u/----Val----</name>
      <uri>https://old.reddit.com/user/----Val----</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ivd3ip/ollama_frontend_using_chatterui/"&gt; &lt;img alt="Ollama frontend using ChatterUI" src="https://external-preview.redd.it/OWU1M2MwNzc0bmtlMdFQRLeEpBBtEHSSn-qbmJ4l5ADqGikYUAGhEl3DS_ow.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aaea36dd077ddc1ee7db47b15f1d2574fca9e4ad" title="Ollama frontend using ChatterUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! I've been working on my app, ChatterUI for a while now, and I just wanted to show off its use as a frontend for various LLM services, including a few open source projects like Ollama!&lt;/p&gt; &lt;p&gt;You can get the app here (android only): &lt;a href="https://github.com/Vali-98/ChatterUI/releases/latest"&gt;https://github.com/Vali-98/ChatterUI/releases/latest&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/----Val----"&gt; /u/----Val---- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/oy6s7lf74nke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivd3ip/ollama_frontend_using_chatterui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivd3ip/ollama_frontend_using_chatterui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T07:08:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivl5fo</id>
    <title>Is it worth running 7b dpsk r1 or should I buy more ram?</title>
    <updated>2025-02-22T15:26:57+00:00</updated>
    <author>
      <name>/u/dTechAnimeGamingGuy</name>
      <uri>https://old.reddit.com/user/dTechAnimeGamingGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My pc specs Amd ryzen 5600g Gpu rx6600 8gb vram Ram 16gb &lt;/p&gt; &lt;p&gt;I usually work with code and reasoning for copywriting or learning. I’m a no code developer / designer and using mainly for generating scripts.&lt;/p&gt; &lt;p&gt;Been using ChatGPT free version till now but thinking to upgrading but I’m not sure if I should buy plus subscription/ get OpenAI/deepseek api or just upgrade my pc for local llm.&lt;/p&gt; &lt;p&gt;My current setup can run bartkowski’s dpsk r1 Q_6 7b/8b somewhat well. &lt;/p&gt; &lt;p&gt;P.S. I know my gpu isn’t officially supported. Found a GitHub repo that bypasses that so it’s ok. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dTechAnimeGamingGuy"&gt; /u/dTechAnimeGamingGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivl5fo/is_it_worth_running_7b_dpsk_r1_or_should_i_buy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivl5fo/is_it_worth_running_7b_dpsk_r1_or_should_i_buy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivl5fo/is_it_worth_running_7b_dpsk_r1_or_should_i_buy/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T15:26:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivmel8</id>
    <title>I designed “Prompt Targets” - a higher level abstraction than function-calling. Route to downstream agents, clarify questions and trigger common agentic scenarios</title>
    <updated>2025-02-22T16:21:50+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ivmel8/i_designed_prompt_targets_a_higher_level/"&gt; &lt;img alt="I designed “Prompt Targets” - a higher level abstraction than function-calling. Route to downstream agents, clarify questions and trigger common agentic scenarios" src="https://preview.redd.it/lyhvhusuupke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a73aee9d1cfdc50799b4ac3c8ed979288c5a940" title="I designed “Prompt Targets” - a higher level abstraction than function-calling. Route to downstream agents, clarify questions and trigger common agentic scenarios" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Function calling is now a core primitive now in building agentic applications - but there is still alot of engineering muck and duck tape required to build an accurate conversational experience. Meaning - sometimes you need to forward a prompt to the right down stream agent to handle the query, or ask for clarifying questions before you can trigger/ complete an agentic task.&lt;/p&gt; &lt;p&gt;I’ve designed a higher level abstraction called &amp;quot;prompt targets&amp;quot; inspired and modeled after how load balancers direct traffic to backend servers. The idea is to process prompts, extract critical information from them and effectively route to a downstream agent or task to handle the user prompt. The devex doesn’t deviate too much from function calling semantics - but the functionality operates at a higher level of abstraction to simplify building agentic systems&lt;/p&gt; &lt;p&gt;So how do you get started? Check out the OSS project: &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt; for more &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lyhvhusuupke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivmel8/i_designed_prompt_targets_a_higher_level/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivmel8/i_designed_prompt_targets_a_higher_level/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T16:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivx91h</id>
    <title>Can someone help me figure out how to do this?</title>
    <updated>2025-02-23T00:20:47+00:00</updated>
    <author>
      <name>/u/MrWinterCreates</name>
      <uri>https://old.reddit.com/user/MrWinterCreates</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm wanting to set something up with ollama that all it does is I can add a pdf, then ask it questions and get accurate answers. While being ran from my own pc. How do I do this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrWinterCreates"&gt; /u/MrWinterCreates &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivx91h/can_someone_help_me_figure_out_how_to_do_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivx91h/can_someone_help_me_figure_out_how_to_do_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivx91h/can_someone_help_me_figure_out_how_to_do_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T00:20:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw63re</id>
    <title>Is it possible to deploy Ollama on AWS and allow access to specific IPS only?</title>
    <updated>2025-02-23T09:06:24+00:00</updated>
    <author>
      <name>/u/asji4</name>
      <uri>https://old.reddit.com/user/asji4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a very simple app that just setups Ollama on flask. Works fine locally and on a public EC2 DNS, but I can't seem to figure out how to get it to run with AWS cloudfront. Here's what I have done so far:&lt;/p&gt; &lt;p&gt;Application Configuration: - Flask application running on localhost:8080. - Ollama service running on localhost:11434.&lt;/p&gt; &lt;p&gt;Deployment Environment: - Both services are hosted on a single EC2 instance. - AWS CloudFront is used as a content delivery network.&lt;/p&gt; &lt;p&gt;What works - the application works perfectly locally and when deployed on a public ec2 DNS on HTTP - I have a security group setup so that only flask is accessible via public, and Ollama has no access except for being called by flask internally via port number &lt;/p&gt; &lt;p&gt;Issue Encountered: - Post-deployment on cloudfront the Flask application is unable to communicate with the Ollama service because of my security group restrictions to block 0.0.0.0 but allow inbound traffic within the security group - CloudFront operates over standard HTTP (port 80) and HTTPS (port 443) ports and doesn't support forwarding traffic to custom ports.&lt;/p&gt; &lt;p&gt;Constraints: - I need Ollama endpoint only accessible via a private IP for security reasons - The Ollama endpoint should only be called by the flask app - I cannot make modifications to client-side endpoints.&lt;/p&gt; &lt;p&gt;What I have tried so far: - tried nginx reverse proxies: didn't work - setup Ollama on a separate EC2 server but now it's accessible to the public which I don't want&lt;/p&gt; &lt;p&gt;Any help or advice would be appreciated as I have used chatgpt but it's starting to hallucinate wrong answers &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asji4"&gt; /u/asji4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iw63re/is_it_possible_to_deploy_ollama_on_aws_and_allow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iw63re/is_it_possible_to_deploy_ollama_on_aws_and_allow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iw63re/is_it_possible_to_deploy_ollama_on_aws_and_allow/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T09:06:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivsl80</id>
    <title>Wired on 240v - Test time!</title>
    <updated>2025-02-22T20:44:22+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ivsl80/wired_on_240v_test_time/"&gt; &lt;img alt="Wired on 240v - Test time!" src="https://preview.redd.it/c8wvltbciqke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a50bd31faa4ee94d41203a9b4a07dcfbdf1e6fd0" title="Wired on 240v - Test time!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c8wvltbciqke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivsl80/wired_on_240v_test_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivsl80/wired_on_240v_test_time/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T20:44:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivxody</id>
    <title>What should I build with this?</title>
    <updated>2025-02-23T00:41:43+00:00</updated>
    <author>
      <name>/u/_astronerd</name>
      <uri>https://old.reddit.com/user/_astronerd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ivxody/what_should_i_build_with_this/"&gt; &lt;img alt="What should I build with this?" src="https://preview.redd.it/161j8f92cske1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0bbfffaf8eaceb0b0dec8e8a1473f5b046a9169" title="What should I build with this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I prefer to run everything locally and have built multiple AI agents, but I struggle with the next step—how to share or sell them effectively. While I enjoy developing and experimenting with different ideas, I often find it difficult to determine when a project is &amp;quot;good enough&amp;quot; to be put in front of users. I tend to keep refining and iterating, unsure of when to stop.&lt;/p&gt; &lt;p&gt;Another challenge I face is originality. Whenever I come up with what I believe is a novel idea, I often discover that someone else has already built something similar. This makes me question whether my work is truly innovative or valuable enough to stand out.&lt;/p&gt; &lt;p&gt;One of my strengths is having access to powerful tools and the ability to rigorously test and push AI models—something that many others may not have. However, despite these advantages, I feel stuck. I don't know how to move forward, how to bring my work to an audience, or how to turn my projects into something meaningful and shareable.&lt;/p&gt; &lt;p&gt;Any guidance on how to break through this stagnation would be greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_astronerd"&gt; /u/_astronerd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/161j8f92cske1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivxody/what_should_i_build_with_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivxody/what_should_i_build_with_this/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T00:41:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivpm8m</id>
    <title>I Make a Customized RAG Chatbot to Talk to CSV File Using Ollama DeepSeek and Streamlit Full Tutorial Part 2</title>
    <updated>2025-02-22T18:35:42+00:00</updated>
    <author>
      <name>/u/Spirited-Wind6803</name>
      <uri>https://old.reddit.com/user/Spirited-Wind6803</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ivpm8m/i_make_a_customized_rag_chatbot_to_talk_to_csv/"&gt; &lt;img alt="I Make a Customized RAG Chatbot to Talk to CSV File Using Ollama DeepSeek and Streamlit Full Tutorial Part 2" src="https://preview.redd.it/3zxqk8oqiqke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4bd9122479810ab06552684685b83cd3d6fe122" title="I Make a Customized RAG Chatbot to Talk to CSV File Using Ollama DeepSeek and Streamlit Full Tutorial Part 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spirited-Wind6803"&gt; /u/Spirited-Wind6803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3zxqk8oqiqke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ivpm8m/i_make_a_customized_rag_chatbot_to_talk_to_csv/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ivpm8m/i_make_a_customized_rag_chatbot_to_talk_to_csv/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-22T18:35:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw51wo</id>
    <title>Self hosted LLM cpu non-gpu AVX512 importance ?</title>
    <updated>2025-02-23T07:51:38+00:00</updated>
    <author>
      <name>/u/centminmod</name>
      <uri>https://old.reddit.com/user/centminmod</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fairly new to self hosted LLM side. I use LM Studio on my 14&amp;quot; MacBook Pro M4 Pro with 48GB and 1TB drive and save LLM models to JEYI 2464 Pro fan edition USB4 NVMe external enclosure with 2TB Kingston KC3000.&lt;/p&gt; &lt;p&gt;However just started self hosted journey on my existing dedicated web servers developing my or-cli.py python client script that supports Openrouter.ai API + local Ollama &lt;a href="https://github.com/centminmod/or-cli"&gt;https://github.com/centminmod/or-cli&lt;/a&gt; and plan on adding vLLM support.&lt;/p&gt; &lt;p&gt;But the dedicated servers are fairly old and ram limited and lack AVX512 support. AMD Ryzen 5950X and Intel Xeon E-2276G with 64GB and 32GB memory respectively.&lt;/p&gt; &lt;p&gt;Short of GPU hosted servers, how much difference in performance would cpu only based usage for Ollama and vLLM and the like would there be if server supported AVX512 instructions for x86_64 based servers? Anyone got any past performance benchmark/results?&lt;/p&gt; &lt;p&gt;Even for GPU hosted, any noticeable difference pairing with/without cpu support for AVX512?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/centminmod"&gt; /u/centminmod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iw51wo/self_hosted_llm_cpu_nongpu_avx512_importance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iw51wo/self_hosted_llm_cpu_nongpu_avx512_importance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iw51wo/self_hosted_llm_cpu_nongpu_avx512_importance/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T07:51:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwc049</id>
    <title>Moe for LLMs</title>
    <updated>2025-02-23T15:01:10+00:00</updated>
    <author>
      <name>/u/wahnsinnwanscene</name>
      <uri>https://old.reddit.com/user/wahnsinnwanscene</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What does it mean to have a mixture of experts in llama.cpp? Does it mean parts of weights are loaded when the mixture router decides on the expert, or is the entire model loaded and is partitioned programmatically ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wahnsinnwanscene"&gt; /u/wahnsinnwanscene &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwc049/moe_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iwc049/moe_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iwc049/moe_for_llms/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-23T15:01:10+00:00</published>
  </entry>
</feed>
