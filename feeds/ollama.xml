<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-01-28T12:26:18+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ibyqix</id>
    <title>數學</title>
    <updated>2025-01-28T10:37:45+00:00</updated>
    <author>
      <name>/u/Character_Sea_587</name>
      <uri>https://old.reddit.com/user/Character_Sea_587</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;3.6和11.3 哪個大&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Character_Sea_587"&gt; /u/Character_Sea_587 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibyqix/數學/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibyqix/數學/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibyqix/數學/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T10:37:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibmmoq</id>
    <title>GFX 1030: AMD 6800XT with Koboldcpp finally letting use my amd gpu on Windows</title>
    <updated>2025-01-27T22:50:51+00:00</updated>
    <author>
      <name>/u/Wreid23</name>
      <uri>https://old.reddit.com/user/Wreid23</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Small Win or old news I dunno but here goes.&lt;/p&gt; &lt;p&gt;6800XT Unsupported I know I know... and can finally run Ollama on Windows with Koboldcpp it has a precompiled gfx1030 config that actually works (long time coming or maybe im late) as no matter what combo I tried with amd ollama it would not work initially.&lt;/p&gt; &lt;p&gt;Install Roc M:&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@afilipe362/using-ollama-on-older-amd-gpus-with-rocm-on-windows-a0b08de2cf02"&gt;https://medium.com/@afilipe362/using-ollama-on-older-amd-gpus-with-rocm-on-windows-a0b08de2cf02&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.amd.com/en/developer/resources/rocm-hub/hip-sdk.html"&gt;https://www.amd.com/en/developer/resources/rocm-hub/hip-sdk.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Optional Step: Move to a Secondary Drive&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="https://github.com/ollama/ollama/issues/2551"&gt;https://github.com/ollama/ollama/issues/2551&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;if you guys get confused, let me explain step by step:&lt;/p&gt; &lt;p&gt;1-first of all uninstall ollama (if you already installed) unless you want to keep it on your Main C Drive&lt;/p&gt; &lt;p&gt;2-then follow this:&lt;/p&gt; &lt;p&gt;Open Windows Settings.&lt;/p&gt; &lt;p&gt;Go to System.&lt;/p&gt; &lt;p&gt;Select About&lt;/p&gt; &lt;p&gt;Select Advanced System Settings.&lt;/p&gt; &lt;p&gt;Go to the Advanced tab.&lt;/p&gt; &lt;p&gt;Select Environment Variables....&lt;/p&gt; &lt;p&gt;Click on New...&lt;/p&gt; &lt;p&gt;And create a variable called OLLAMA_MODELS pointing to where you want to store the models(set path for store models)&lt;/p&gt; &lt;p&gt;3-after adjusting that then open the path that you downloaded OllamaSetup.exe (setup file)&lt;/p&gt; &lt;p&gt;4-then on file explorer write cmd to open cmd on that path&lt;/p&gt; &lt;p&gt;5-on cmd write--&amp;gt; OllamaSetup.exe /DIR=D:\MYDIRECTORY&lt;/p&gt; &lt;p&gt;6-after installation everything should works well and both ollama and models will save same drive you defined for&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Install Ollama for AMD&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://github.com/likelovewant/ollama-for-amd"&gt;https://github.com/likelovewant/ollama-for-amd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Do The Above but then: After Step 2 Just replace all the rocblas folders with &lt;a href="https://github.com/YellowRoseCx/koboldcpp-rocm/releases/tag/v1.82.1.yr0-ROCm"&gt;https://github.com/YellowRoseCx/koboldcpp-rocm/releases/tag/v1.82.1.yr0-ROCm&lt;/a&gt; files in&lt;/p&gt; &lt;p&gt;Download koboldcpp_rocm_files.zip&lt;/p&gt; &lt;p&gt;In The Following Locations:&lt;/p&gt; &lt;p&gt;Ollama Appdata (if its on c)&lt;/p&gt; &lt;p&gt;C:\Users\username\AppData\Local\Programs\Ollama\lib\ollama&lt;/p&gt; &lt;p&gt;If its on another disk navigate to ollama\lib\ollama &amp;gt; Overwrite &amp;amp; Replace the Rocblas Folder with rocblas from koboldcpp_rocm_files.zip&lt;/p&gt; &lt;p&gt;Go to Your Installed ROCM Folders:&lt;/p&gt; &lt;p&gt;C:\Program Files\AMD\ROCm\5.7\bin (depending on your gpu you might be using this one or 6.1)&lt;/p&gt; &lt;p&gt;C:\Program Files\AMD\ROCm\6.1\bin&lt;/p&gt; &lt;p&gt;If you cant find the above folders above download wiztree: &lt;a href="https://diskanalyzer.com/"&gt;https://diskanalyzer.com/&lt;/a&gt; and search for the file name or some of the path you will get there&lt;/p&gt; &lt;p&gt;Whichever one applies to you^&lt;/p&gt; &lt;p&gt;Download and Open KoboldCPP: &lt;a href="https://github.com/YellowRoseCx/koboldcpp-rocm/releases/download/v1.82.1.yr0-ROCm/koboldcpp_rocm.exe"&gt;https://github.com/YellowRoseCx/koboldcpp-rocm/releases/download/v1.82.1.yr0-ROCm/koboldcpp_rocm.exe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let it open attach a model from your Current Ollama folder: it will be the giant files in models folder check the dates and file sizes when you open it it will show you which model you are using in the command prompt.&lt;/p&gt; &lt;p&gt;Attach the Model You want to Use and test that works in the lil ui that launches.&lt;/p&gt; &lt;p&gt;put up 2 windows side to side task manager and the ui window&lt;/p&gt; &lt;p&gt;Open Task Manager and ask a semi complicated question like: what is the largest city per capita&lt;/p&gt; &lt;p&gt;It should spike GPU to 80%&lt;/p&gt; &lt;p&gt;You can test against running regular ollama as well&lt;/p&gt; &lt;p&gt;Finally.... Read the Kobold docs to see how much you can speed up the models based on your vram but the auto config in kobold does an ok job&lt;/p&gt; &lt;p&gt;You can also copy shortcut to your windows startup folder and launch it automatically.&lt;/p&gt; &lt;p&gt;For MSTY, OPENWEBUI ETC &amp;gt; Add a Local or Remote Open AI Compatible Model&lt;/p&gt; &lt;p&gt;Use Local Open AI Compatible Model: &lt;a href="http://localhost:5001/v1"&gt;http://localhost:5001/v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Change the Model to Kobold or whatever you name it and you will again see it using your GPU&lt;/p&gt; &lt;p&gt;API KEY: 0&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/open-webui/open-webui/discussions/2700"&gt;https://github.com/open-webui/open-webui/discussions/2700&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wreid23"&gt; /u/Wreid23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibmmoq/gfx_1030_amd_6800xt_with_koboldcpp_finally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibmmoq/gfx_1030_amd_6800xt_with_koboldcpp_finally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibmmoq/gfx_1030_amd_6800xt_with_koboldcpp_finally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T22:50:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibufiy</id>
    <title>How to make build an Enterprise Knowledge Integration Platform?</title>
    <updated>2025-01-28T05:20:53+00:00</updated>
    <author>
      <name>/u/D675vroom</name>
      <uri>https://old.reddit.com/user/D675vroom</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;goal: self-hosted AI-powered knowledge base that intelligently aggregates, indexes, and retrieves information across confluence, JIRA, slack channels, docs and maybe code repos.&lt;/p&gt; &lt;p&gt;want &lt;/p&gt; &lt;pre&gt;&lt;code&gt;Semantic search across fragmented enterprise knowledge Context-aware information retrieval no hallucinations &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;challenges I see: &lt;/p&gt; &lt;ul&gt; &lt;li&gt; we have many different releases of the software now, 10+yr old product, with technical details that aren't necessarily easy to understand b/w versions. So if I ask a question, and maybe the only info we have relevant to that is from 5 years ago, it shouldn't confidently give an answer as if its relevant today. Is this possible?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If I decide to integrate the code repo, maybe it's only for the latest release so we're not lost in the versions.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;how can this be done? Is it as simple as hosting OpenWebUI/LLMStudio and creating pipeines to feed the data?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If we integrate the code repository, maybe it could be done only for the latest release, that way we're not lost in&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/D675vroom"&gt; /u/D675vroom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibufiy/how_to_make_build_an_enterprise_knowledge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibufiy/how_to_make_build_an_enterprise_knowledge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibufiy/how_to_make_build_an_enterprise_knowledge/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T05:20:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1iburn5</id>
    <title>parameters incorrect ??</title>
    <updated>2025-01-28T05:42:11+00:00</updated>
    <author>
      <name>/u/mizdavilly</name>
      <uri>https://old.reddit.com/user/mizdavilly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hello&lt;/p&gt; &lt;p&gt;im new to this, ive jist downloaded the program pulled 2 models one is 9b and the other is 2b&lt;br /&gt; when im running the command run, i get the &amp;quot;parameters incorrect&amp;quot; response&lt;/p&gt; &lt;p&gt;my pc is old, its a 4th g3n i7 with a gt730 2gb and 16gb or ram&lt;br /&gt; i figured if the raspberry are running it my cpu should be able to&lt;br /&gt; cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mizdavilly"&gt; /u/mizdavilly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iburn5/parameters_incorrect/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iburn5/parameters_incorrect/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iburn5/parameters_incorrect/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T05:42:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibv1rj</id>
    <title>I created a CLI tool to easily create cloud GPU instances for Ollama</title>
    <updated>2025-01-28T06:00:34+00:00</updated>
    <author>
      <name>/u/alexandrevilain</name>
      <uri>https://old.reddit.com/user/alexandrevilain</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just released ollama-machine, an open-source tool inspired by docker-machine. It’s designed to make it easier and safer to manage GPU-powered cloud instances configured for running Ollama.&lt;/p&gt; &lt;p&gt;Right now, it supports a few cloud providers like OVHcloud and OpenStack (the one I'm using), but more are planned (AWS, Scaleway, etc.). It’s still early days, and there’s a lot left to improve, but I wanted to share it now to get feedback and see if the community vibes with it.&lt;/p&gt; &lt;p&gt;If you’re curious, you can check it out here: &lt;a href="https://github.com/alexandrevilain/ollama-machine"&gt;https://github.com/alexandrevilain/ollama-machine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please try it out and let me know what you think! And of course, contributions are more than welcome! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alexandrevilain"&gt; /u/alexandrevilain &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibv1rj/i_created_a_cli_tool_to_easily_create_cloud_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibv1rj/i_created_a_cli_tool_to_easily_create_cloud_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibv1rj/i_created_a_cli_tool_to_easily_create_cloud_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T06:00:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibwa3j</id>
    <title>what the hell is happening exactly?</title>
    <updated>2025-01-28T07:26:13+00:00</updated>
    <author>
      <name>/u/AhmedElakkad0</name>
      <uri>https://old.reddit.com/user/AhmedElakkad0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ibwa3j/what_the_hell_is_happening_exactly/"&gt; &lt;img alt="what the hell is happening exactly?" src="https://b.thumbs.redditmedia.com/eODF-ICFfPsQijgAJ0W7_5bi-b4ATGv1hfJ0SqcFnog.jpg" title="what the hell is happening exactly?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/xfwrl8ndsofe1.png?width=2772&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1957fef796170d605e486939c9f8d7300e7cb8b2"&gt;https://preview.redd.it/xfwrl8ndsofe1.png?width=2772&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1957fef796170d605e486939c9f8d7300e7cb8b2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I ran ollama run deepseek-r1:1.5b and this is what's happening when I ask it anything, what is this? 😂&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AhmedElakkad0"&gt; /u/AhmedElakkad0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibwa3j/what_the_hell_is_happening_exactly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibwa3j/what_the_hell_is_happening_exactly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibwa3j/what_the_hell_is_happening_exactly/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T07:26:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibtp06</id>
    <title>How to know LLM model can run in my computer?</title>
    <updated>2025-01-28T04:37:53+00:00</updated>
    <author>
      <name>/u/iNdramal</name>
      <uri>https://old.reddit.com/user/iNdramal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are many LLM models available in Ollama. Also, available in various sizes. How to know which LLM models can run on my computer? does it give an issues when running the model?&lt;/p&gt; &lt;p&gt;My computer specifications:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPU: Nvidia GetForce RTX 1050&lt;/li&gt; &lt;li&gt;Ram: 12GB&lt;/li&gt; &lt;li&gt;Processor: Intel(R) Core(TM) i7-8750H CPU @ 2.20GHz 2.21 GHz&lt;/li&gt; &lt;li&gt;OS: Ubuntu&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iNdramal"&gt; /u/iNdramal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibtp06/how_to_know_llm_model_can_run_in_my_computer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibtp06/how_to_know_llm_model_can_run_in_my_computer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibtp06/how_to_know_llm_model_can_run_in_my_computer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T04:37:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibx81l</id>
    <title>GPU working ok from Open WebUI, but not from Python?</title>
    <updated>2025-01-28T08:40:07+00:00</updated>
    <author>
      <name>/u/old-mike</name>
      <uri>https://old.reddit.com/user/old-mike</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. When using Ollama from Open WebUI, I can see how the GPU is used in NVTOP. Calling Ollama from a Python script does not use GPU. What am I doing wrong?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama.Client(host='http://192.168.100.61:11434') response = client.generate(model_name, prompt, options={&amp;quot;num_ctx&amp;quot;: 32000,&amp;quot;stream&amp;quot;: &amp;quot;false&amp;quot;} &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/old-mike"&gt; /u/old-mike &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibx81l/gpu_working_ok_from_open_webui_but_not_from_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibx81l/gpu_working_ok_from_open_webui_but_not_from_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibx81l/gpu_working_ok_from_open_webui_but_not_from_python/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T08:40:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibu574</id>
    <title>Model for content moderation?</title>
    <updated>2025-01-28T05:04:04+00:00</updated>
    <author>
      <name>/u/HackTheDev</name>
      <uri>https://old.reddit.com/user/HackTheDev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used openi's api to do automated content moderation which got expensive. today i heard about deepseek but it out right refuses requests if they &amp;quot;get too spicy&amp;quot; making it useless.&lt;/p&gt; &lt;p&gt;I then tried &amp;quot;closex/neuraldaredevil-8b-abliterated:latest&amp;quot; which worked better but i think its a bit dumb, or i am maybe. A simplified prompt like &amp;quot;detect insults against people or groups but allow non directed profanity&amp;quot; will always be flagged where openai handles this just fine&lt;/p&gt; &lt;p&gt;currently im downloading &amp;quot;jean-luc/big-tiger-gemma:27b-v1c-Q3_K_M&amp;quot; and hope this one will be better and be able to the job.&lt;/p&gt; &lt;p&gt;im using ollama on windows with a ryzen 9 5900x and a nvidia rtx 3080&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HackTheDev"&gt; /u/HackTheDev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibu574/model_for_content_moderation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibu574/model_for_content_moderation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibu574/model_for_content_moderation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T05:04:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibmf8q</id>
    <title>[v1.0.8] Notate - New Reasoning Layer Added (Agent actions will be added) | UI/UX Improvements &amp; Deepseek Integration - Have suggestions? love to hear em!</title>
    <updated>2025-01-27T22:41:52+00:00</updated>
    <author>
      <name>/u/Hairetsu</name>
      <uri>https://old.reddit.com/user/Hairetsu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ibmf8q/v108_notate_new_reasoning_layer_added_agent/"&gt; &lt;img alt="[v1.0.8] Notate - New Reasoning Layer Added (Agent actions will be added) | UI/UX Improvements &amp;amp; Deepseek Integration - Have suggestions? love to hear em!" src="https://external-preview.redd.it/mLmb6PM3DcPIkGqR7oQ2EX6QH1e3GfDbIdNH1M_vKQw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=769808d36520d2e157bfc35efdd17fa7406c869f" title="[v1.0.8] Notate - New Reasoning Layer Added (Agent actions will be added) | UI/UX Improvements &amp;amp; Deepseek Integration - Have suggestions? love to hear em!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hairetsu"&gt; /u/Hairetsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/CNTRLAI/Notate"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibmf8q/v108_notate_new_reasoning_layer_added_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibmf8q/v108_notate_new_reasoning_layer_added_agent/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T22:41:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibmlty</id>
    <title>Can someone suggest me a model to use for a personal project</title>
    <updated>2025-01-27T22:49:49+00:00</updated>
    <author>
      <name>/u/AVBGaming</name>
      <uri>https://old.reddit.com/user/AVBGaming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am interested in implementing AI/LLM into my note taking software (Obsidian). What i’m envisioning is a tool i can use which i can enter text prompts (like asking where a certain note is, or a general question that could be answered with something in my notes) and get a response. Any pointers on where i could start? i’m not sure how many parameters would suffice for this task, and im not sure if i need to look exclusively at models i can run locally vs on the cloud.&lt;/p&gt; &lt;p&gt;I have a 4070 8GB VRAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AVBGaming"&gt; /u/AVBGaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibmlty/can_someone_suggest_me_a_model_to_use_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibmlty/can_someone_suggest_me_a_model_to_use_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibmlty/can_someone_suggest_me_a_model_to_use_for_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T22:49:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibhxvm</id>
    <title>Guide to Installing and Locally Running Ollama LLM models in Comfy (ELI5 Level)</title>
    <updated>2025-01-27T19:38:29+00:00</updated>
    <author>
      <name>/u/GreyScope</name>
      <uri>https://old.reddit.com/user/GreyScope</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ibhxvm/guide_to_installing_and_locally_running_ollama/"&gt; &lt;img alt="Guide to Installing and Locally Running Ollama LLM models in Comfy (ELI5 Level)" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="Guide to Installing and Locally Running Ollama LLM models in Comfy (ELI5 Level)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Firstly, due diligence still applies to checking out any security issues to all models and software.&lt;/p&gt; &lt;p&gt;Secondly, this is written in the (kiss) style of all my guides : simple steps, it is not a technical paper, nor is it written for people who have greater technical knowledge, they are written as best I can in ELI5 style .&lt;/p&gt; &lt;h1&gt;Pre-requisites&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;A (quick) internet connection (if downloading large models&lt;/li&gt; &lt;li&gt;A working install of ComfyUI&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Usage Case:&lt;/h1&gt; &lt;p&gt;1. For Stable Diffusion purposes it’s for writing or expanding prompts, ie to make descriptions or make them more detailed / refined for a purpose (eg like a video) if used on an existing bare bones prompt .&lt;/p&gt; &lt;p&gt;2. If the LLM is used to describe an existing image, it can help replicate the style or substance of it.&lt;/p&gt; &lt;p&gt;3. Use it as a Chat bot or as a LLM front end for whatever you want (eg coding)&lt;/p&gt; &lt;h1&gt;Basic Steps to carry out (Part 1):&lt;/h1&gt; &lt;p&gt;1. Download Ollama itself&lt;/p&gt; &lt;p&gt;2. Turn off Ollama’s Autostart entry (&amp;amp; start when needed) or leave it&lt;/p&gt; &lt;p&gt;3. Set the Ollama ENV in Windows – to set where it saves the models that it uses&lt;/p&gt; &lt;p&gt;4. Run Ollama in a CMD window and download a model&lt;/p&gt; &lt;p&gt;5. Run Ollama with the model you just downloaded&lt;/p&gt; &lt;h1&gt;Basic Steps to carry out (Part 2):&lt;/h1&gt; &lt;p&gt;1. For use within Comfy download/install nodes for its use&lt;/p&gt; &lt;p&gt;2. Setup nodes within your own flow or download a flow with them in&lt;/p&gt; &lt;p&gt;3. Setup the settings within the LLM node to use Ollama &lt;/p&gt; &lt;h1&gt;Basic Explanation of Terms&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;An &lt;strong&gt;LLM (Large Language Model)&lt;/strong&gt; is an AI system trained on vast amounts of text data to understand, generate, and manipulate human-like language for various tasks - like coding, describing images, writing text etc&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt; is a tool that allows users to easily download, run, and manage open-source large language models (LLMs) locally on their own hardware.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;---------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Part 1 - Ollama&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;DownLoad Ollama&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Download Ollama and install from - &lt;a href="https://ollama.com/"&gt;https://ollama.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You will see nothing after it installs but if you go down the bottom right of the taskbar in the Notification section, you'll see it is active (running a background server).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/omup1pj0nkfe1.png?width=453&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c5484788c42d83c84cb53f38a5143afc62d9155"&gt;https://preview.redd.it/omup1pj0nkfe1.png?width=453&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c5484788c42d83c84cb53f38a5143afc62d9155&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Ollama and Autostart&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Be aware that Ollama autoruns on your PC’s startup, if you don’t want that then turn off its Autostart on (Ctrl -Alt-Del to start the Task Manager and then click on Startup Apps and lastly just right clock on its entry on the list and select ‘Disabled’)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Set Ollama's ENV settings&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Now setup where you want Ollama to save its models (eg your hard drive with your SD installs on or the one with the most space)&lt;/p&gt; &lt;p&gt;Type ‘ENV’ into search box on your taskbar&lt;/p&gt; &lt;p&gt;Select &amp;quot;Edit the System Environment Variables&amp;quot; (part of Windows Control Panel) , see below&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y03ubm8dnkfe1.png?width=393&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=179daa73ce2009eb1ea2343dd225a07c7a772455"&gt;https://preview.redd.it/y03ubm8dnkfe1.png?width=393&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=179daa73ce2009eb1ea2343dd225a07c7a772455&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On the newly opened ‘System Properties‘ window, click on &amp;quot;Environment Variables&amp;quot; (bottom right on pic below)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f2tv16minkfe1.png?width=283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=919004d372bd169d80d4a0378be070962a79f78d"&gt;https://preview.redd.it/f2tv16minkfe1.png?width=283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=919004d372bd169d80d4a0378be070962a79f78d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;System Variables are split into two sections of User and System - click on New under &amp;quot;User Variables&amp;quot; (top section on pic below)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zhbudfjnnkfe1.png?width=414&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef70cb35da716b05a365d8eab65ca7406eef7fdb"&gt;https://preview.redd.it/zhbudfjnnkfe1.png?width=414&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ef70cb35da716b05a365d8eab65ca7406eef7fdb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On the new input window, input the following -&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Variable name:&lt;/strong&gt; OLLAMA_MODELS&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Variable value:&lt;/strong&gt; (input directory path you wish to save models to. Make your folder structure as you wish ( eg H:\Ollama\Models).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4xitvv0ynkfe1.png?width=535&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e07784593316bd1147b7b02547eece860a49fb0c"&gt;https://preview.redd.it/4xitvv0ynkfe1.png?width=535&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e07784593316bd1147b7b02547eece860a49fb0c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NB&lt;/strong&gt; Don’t change the ‘Variable name’ or Ollama will not save to the directory you wish.&lt;/p&gt; &lt;p&gt;Click OK on each screen until the Environment Variables windows and then the System Properties windows close down (the variables are not saved until they're all closed)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open a CMD window and type 'Ollama' it will return its commands that you can use (see pic below)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nazdqgraokfe1.png?width=353&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=badd182f68014c2e38d69cfa06e8eeecbec96763"&gt;https://preview.redd.it/nazdqgraokfe1.png?width=353&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=badd182f68014c2e38d69cfa06e8eeecbec96763&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here’s a list of popular Large Language Models (LLMs) available on &lt;strong&gt;Ollama&lt;/strong&gt;, categorized by their simplified use cases. These models can be downloaded and run locally using Ollama or any others that are available (due diligence required) :&lt;/p&gt; &lt;h1&gt;A. Chat Models&lt;/h1&gt; &lt;p&gt;These models are optimized for conversational AI and interactive chat applications.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Llama 2 (7B, 13B, 70B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: General-purpose chat, conversational AI, and answering questions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run llama2&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mistral (7B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Lightweight and efficient chat model for conversational tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run mistral&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;B. Text Generation Models&lt;/h1&gt; &lt;p&gt;These models excel at generating coherent and creative text for various purposes.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OpenLLaMA (7B, 13B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Open-source alternative for text generation and summarization.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run openllama&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;C. Coding Models&lt;/h1&gt; &lt;p&gt;These models are specialized for code generation, debugging, and programming assistance.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CodeLlama (7B, 13B, 34B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Code generation, debugging, and programming assistance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run codellama&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;C. Image Description Models&lt;/h1&gt; &lt;p&gt;These models are designed to generate text descriptions of images (multimodal capabilities).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LLaVA (7B, 13B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Image captioning, visual question answering, and multimodal tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run llava&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;D. Multimodal Models&lt;/h1&gt; &lt;p&gt;These models combine text and image understanding for advanced tasks.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fuyu (8B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Multimodal tasks, including image understanding and text generation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run fuyu&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;E. Specialized Models&lt;/h1&gt; &lt;p&gt;These models are fine-tuned for specific tasks or domains.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;WizardCoder (15B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Specialized in coding tasks and programming assistance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run wizardcoder&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Alpaca (7B)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Use Case&lt;/strong&gt;: Instruction-following tasks and fine-tuned conversational AI.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Command&lt;/strong&gt;: &lt;code&gt;ollama run alpaca&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Model Strengths&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;As you can see above, an LLM is focused to a particular strength, it's for the best to expect a Coding biased LLM to provide a good description of an image.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Size&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Go into the Ollama website and pick a variant (noted by the number and followed by a B in brackets after each model) to fit into your graphics cards VRAM.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Downloading a model - When you have decided which model you want, say the Gemma 2 model in its smallest 2b variant at 1.6G (pic below). The arrow shows the command to put into the CMD window to download and run it (it autodownloads and then runs)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gvb8dq6rukfe1.png?width=504&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16ad8f0c116f45cbdff67179599af06ffedefa6a"&gt;https://preview.redd.it/gvb8dq6rukfe1.png?width=504&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16ad8f0c116f45cbdff67179599af06ffedefa6a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9cutop61wkfe1.png?width=429&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b406060c153a068c78537275b70193f71cf6976"&gt;Models downloads and then runs - I asked it what an LLM is. Typing 'ollama list' tells you the models you have.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;-------------------------------------------------------.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Part 2 - Comfy&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I prefer a working workflow to have everything in a state where you can work on and adjust it to your needs / interests.&lt;/p&gt; &lt;p&gt;This is a great example from a user here &lt;a href="/u/EnragedAntelope"&gt;u/EnragedAntelope&lt;/a&gt; posted on Civitai - its for a workflow that uses LLMs in picture description for Cosmos I2V.&lt;/p&gt; &lt;p&gt;&lt;a href="https://civitai.com/models/1145020/cosmos-automated-image-to-video-i2v-enragedantelope"&gt;Cosmos AUTOMATED Image to Video (I2V) - EnragedAntelope - v1.2 | Other Workflows | Civitai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The initial LLM (Florence2) auto-downloads and installs itself , it then carries out the initial Image description (bottom right text box)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jh8rvq4oxkfe1.png?width=1117&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35f30f6ea6bd6a318c6ec75414400f30006175aa"&gt;https://preview.redd.it/jh8rvq4oxkfe1.png?width=1117&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35f30f6ea6bd6a318c6ec75414400f30006175aa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The text in the initial description is then passed to the second LLM module (within the Plush nodes) , this is initially set to use bigger internet based LLMs.&lt;/p&gt; &lt;p&gt;From everything carried out above, this can be changed to use your local Ollama install. Ensure the server is running (Llama in the notification area) - note the settings in the Advanced Prompt Enhancer node in the pic below.&lt;/p&gt; &lt;p&gt;That node is from the &lt;a href="https://github.com/glibsonoran/Plush-for-ComfyUI"&gt;https://github.com/glibsonoran/Plush-for-ComfyUI&lt;/a&gt; , let manager sort it all out for you.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qzl0bt0fykfe1.png?width=1190&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b2a9a26ba3e39ee647171b937735d969812ca37"&gt;Advanced Prompt Generator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You select the Ollama model from your downloads with a simple click on the box (see pic below) .&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5zva3rqnykfe1.png?width=468&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=93e02f32d2ba61cb00b0c470b8a06fc16e4dc4a5"&gt;Ollama Model selection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the context of this workflow, the added second LLM is given the purpose of rewriting the prompt for a video to increase the quality.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ibhxvm/video/44vn5inmzkfe1/player"&gt;https://reddit.com/link/1ibhxvm/video/44vn5inmzkfe1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ibhxvm/video/3conlucvzkfe1/player"&gt;https://reddit.com/link/1ibhxvm/video/3conlucvzkfe1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreyScope"&gt; /u/GreyScope &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibhxvm/guide_to_installing_and_locally_running_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibhxvm/guide_to_installing_and_locally_running_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibhxvm/guide_to_installing_and_locally_running_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T19:38:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iby4np</id>
    <title>Anyone got GEITje-7B?</title>
    <updated>2025-01-28T09:51:56+00:00</updated>
    <author>
      <name>/u/ScrapEngineer_</name>
      <uri>https://old.reddit.com/user/ScrapEngineer_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Dutch copyright troll Brein has taken offline GEITje-7B.&lt;/p&gt; &lt;p&gt;I had previously downloaded this model, but unfortunately lost it.&lt;/p&gt; &lt;p&gt;So does anyone had downloaded GEITje-7B before it was taken down and is willing to share it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ScrapEngineer_"&gt; /u/ScrapEngineer_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iby4np/anyone_got_geitje7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iby4np/anyone_got_geitje7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iby4np/anyone_got_geitje7b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T09:51:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibybpt</id>
    <title>GPU question</title>
    <updated>2025-01-28T10:06:42+00:00</updated>
    <author>
      <name>/u/Primary_Arm_1175</name>
      <uri>https://old.reddit.com/user/Primary_Arm_1175</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ok here is the question. suppose I bought 2 6900 or 6800 AMD GPUs. will I be able to run something like 32B deep seek r1?&lt;br /&gt; will it be slow? will it be reasonably slow?&lt;br /&gt; also what if I bought a really good CPU with 128 GB of RAM? has anyone here has any idea what each setup can run? can anyone share their setup and what it's capable of? I have a 3050 8 GB with 32 GB RAM and a 12400f and it can run the 1.5B easily it DOES run the 8B, slowly but it sometimes crashes. can people here share what they have and how well it runs? thank you &amp;lt;3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Primary_Arm_1175"&gt; /u/Primary_Arm_1175 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibybpt/gpu_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibybpt/gpu_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibybpt/gpu_question/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T10:06:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibvoyt</id>
    <title>Looking For GPU Help</title>
    <updated>2025-01-28T06:44:11+00:00</updated>
    <author>
      <name>/u/thedayzed</name>
      <uri>https://old.reddit.com/user/thedayzed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys I am looking for some ideas on how to move on GPUs. currently I have a 4070ti but want to move to a larger Vram card. so was looking at the 7900xtx or a 3090. my board is only dual spaced so if I were to do a 2 card setup both cards would need to be only two slot but still as powerful as my 4070ti as I game also. any recommendations would be helpful as I have not decided on what way to go. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thedayzed"&gt; /u/thedayzed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibvoyt/looking_for_gpu_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibvoyt/looking_for_gpu_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibvoyt/looking_for_gpu_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T06:44:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibyn8k</id>
    <title>Is anyone running LLM on a Radeon Instinct Mi50?</title>
    <updated>2025-01-28T10:30:54+00:00</updated>
    <author>
      <name>/u/East-Engineering-653</name>
      <uri>https://old.reddit.com/user/East-Engineering-653</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently bought a used Radeon Instinct Mi50 and I'm going to try running LLM with it.&lt;/p&gt; &lt;p&gt;Since the VRAM is 16GB, the model size that can be run seems quite large, but I'm curious about the running speed. Also, I know that it's been a while since this card was released, so I'm curious if the latest ROCm supports this card.&lt;/p&gt; &lt;p&gt;Lastly, the seller said that he changed the graphics card BIOS to Radeon 7. Will this affect software for LLM such as ROCm?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/East-Engineering-653"&gt; /u/East-Engineering-653 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibyn8k/is_anyone_running_llm_on_a_radeon_instinct_mi50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibyn8k/is_anyone_running_llm_on_a_radeon_instinct_mi50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibyn8k/is_anyone_running_llm_on_a_radeon_instinct_mi50/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T10:30:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibadzz</id>
    <title>LLM powered scraping with ollama</title>
    <updated>2025-01-27T14:31:29+00:00</updated>
    <author>
      <name>/u/Financial-Article-12</name>
      <uri>https://old.reddit.com/user/Financial-Article-12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been developing Parsera, a simple yet powerful Python library that leverages LLMs for web scraping. Many users requested Ollama support, and now that I’ve added it, I wanted to share it with the Ollama community.&lt;/p&gt; &lt;p&gt;If you are looking for a way to extract data from websites (especially when dealing with multiple websites), Parsera lets you do it with just a few lines of code, without the need to develop custom scrapers.&lt;/p&gt; &lt;p&gt;Give it a try and let me know what you think. Feedback is always welcome!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/raznem/parsera"&gt;github.com/raznem/parsera&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial-Article-12"&gt; /u/Financial-Article-12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibadzz/llm_powered_scraping_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibadzz/llm_powered_scraping_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibadzz/llm_powered_scraping_with_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T14:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibuk3j</id>
    <title>Has anyone used the "distilled" versions of deepseek?</title>
    <updated>2025-01-28T05:28:43+00:00</updated>
    <author>
      <name>/u/daHsu</name>
      <uri>https://old.reddit.com/user/daHsu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for advice on model selection. I was looking for a DeepSeek model to download, and it turns out that not only are there 5 different sizes of the model, but they also went and fine-tuned the model on a bunch of different versions of qwen/llama as well!&lt;/p&gt; &lt;p&gt;I couldn't find any recommendations/info on these, other than their explanation&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1. We slightly change their configs and tokenizers. Please use our setting to run these models.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Does anyone have experience with them, or have any tips on when to use them? I'm not sure what a fine-tune here would really mean--like a DeepSeek that &amp;quot;talks like&amp;quot; qwen?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/daHsu"&gt; /u/daHsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibuk3j/has_anyone_used_the_distilled_versions_of_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibuk3j/has_anyone_used_the_distilled_versions_of_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibuk3j/has_anyone_used_the_distilled_versions_of_deepseek/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T05:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibzoat</id>
    <title>Why Ollama uses 90% GPU with both Deepseek r1 1.5b and 7b on MacbookPro M1Pro?</title>
    <updated>2025-01-28T11:41:24+00:00</updated>
    <author>
      <name>/u/sP0re90</name>
      <uri>https://old.reddit.com/user/sP0re90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; It's the first time I use Ollama. I'm running it with Open Web UI on Macbook Pro M1 Pro.&lt;br /&gt; I'm wondering why Ollama process in Activity Monitor shows 90% GPU usage with both 1.5b and 7b Deepseek models while waiting for an answer?&lt;br /&gt; If I type in terminal Ollama ps I see the models and both shows PROCESSOR 100% GPU. &lt;/p&gt; &lt;p&gt;They both works, not super fast answers but they do their job.&lt;br /&gt; But I'm trying to understand how Ollama/models resources consumption works.&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sP0re90"&gt; /u/sP0re90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibzoat/why_ollama_uses_90_gpu_with_both_deepseek_r1_15b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibzoat/why_ollama_uses_90_gpu_with_both_deepseek_r1_15b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibzoat/why_ollama_uses_90_gpu_with_both_deepseek_r1_15b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T11:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1iby6rq</id>
    <title>Running deepseek-r1:1.5b on Apple Silicon GPU</title>
    <updated>2025-01-28T09:56:26+00:00</updated>
    <author>
      <name>/u/qwpajrty</name>
      <uri>https://old.reddit.com/user/qwpajrty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running DeepSeek R1 (1.5B) on a MacBook M1 Pro with 16GB RAM. While monitoring performance, I noticed that GPU usage remained at 0% throughout the Ollama process, while CPU usage spiked to 500%.&lt;/p&gt; &lt;p&gt;How can I configure this model to utilize the Apple Silicon GPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/qwpajrty"&gt; /u/qwpajrty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iby6rq/running_deepseekr115b_on_apple_silicon_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iby6rq/running_deepseekr115b_on_apple_silicon_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iby6rq/running_deepseekr115b_on_apple_silicon_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T09:56:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibz1i7</id>
    <title>Can anyone help regarding this error?</title>
    <updated>2025-01-28T10:59:44+00:00</updated>
    <author>
      <name>/u/SnooGiraffes4275</name>
      <uri>https://old.reddit.com/user/SnooGiraffes4275</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ibz1i7/can_anyone_help_regarding_this_error/"&gt; &lt;img alt="Can anyone help regarding this error?" src="https://preview.redd.it/oae86xlmupfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25023ba8cbe81cec5094cfe83595e47900fd6856" title="Can anyone help regarding this error?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My net speed is 300 mbps. I’m pretty sure something else is wrong but I can’t figure out what&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooGiraffes4275"&gt; /u/SnooGiraffes4275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oae86xlmupfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibz1i7/can_anyone_help_regarding_this_error/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibz1i7/can_anyone_help_regarding_this_error/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T10:59:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1iblmdi</id>
    <title>Is Deepseek R-1 overthinking everything?</title>
    <updated>2025-01-27T22:08:10+00:00</updated>
    <author>
      <name>/u/nPrevail</name>
      <uri>https://old.reddit.com/user/nPrevail</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I like the critical thinking that Deep Seek R-1 (reason) has, but I also think that it's been overthinking a lot, and giving me too many alternatives and options, and it keeps running ideas beyond reason (ha ha ha).&lt;/p&gt; &lt;p&gt;At worst, it'll give me an incorrect answer, and will talk to itself, saying things like &amp;quot;Oh, that not the right answer. Let me try it again&amp;quot; type of conversation with itself, but it will keep going until it's satisfied with its own answer.&lt;/p&gt; &lt;p&gt;Has anyone else experienced this?&lt;/p&gt; &lt;p&gt;I almost want to shift back to using llama3 as my main LLM. It was pretty straightforward, despite not giving me any critical responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nPrevail"&gt; /u/nPrevail &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iblmdi/is_deepseek_r1_overthinking_everything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iblmdi/is_deepseek_r1_overthinking_everything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iblmdi/is_deepseek_r1_overthinking_everything/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T22:08:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibngl3</id>
    <title>Qwen2.5-VL just released</title>
    <updated>2025-01-27T23:26:33+00:00</updated>
    <author>
      <name>/u/numinouslymusing</name>
      <uri>https://old.reddit.com/user/numinouslymusing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5"&gt;https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/numinouslymusing"&gt; /u/numinouslymusing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibngl3/qwen25vl_just_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibngl3/qwen25vl_just_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibngl3/qwen25vl_just_released/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-27T23:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibulvz</id>
    <title>These random accounts have been showing up ever since I started using ollama. Should I be worried?</title>
    <updated>2025-01-28T05:31:49+00:00</updated>
    <author>
      <name>/u/Liquidmesh</name>
      <uri>https://old.reddit.com/user/Liquidmesh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ibulvz/these_random_accounts_have_been_showing_up_ever/"&gt; &lt;img alt="These random accounts have been showing up ever since I started using ollama. Should I be worried?" src="https://preview.redd.it/lqatzv6y7ofe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=21464609696b2ac648efbca375cb46f4d41f5c57" title="These random accounts have been showing up ever since I started using ollama. Should I be worried?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Liquidmesh"&gt; /u/Liquidmesh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lqatzv6y7ofe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibulvz/these_random_accounts_have_been_showing_up_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibulvz/these_random_accounts_have_been_showing_up_ever/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T05:31:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibwdvx</id>
    <title>Ollama enjoying the Chinese New Year! Open Source FTW 🚀</title>
    <updated>2025-01-28T07:34:16+00:00</updated>
    <author>
      <name>/u/eternviking</name>
      <uri>https://old.reddit.com/user/eternviking</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ibwdvx/ollama_enjoying_the_chinese_new_year_open_source/"&gt; &lt;img alt="Ollama enjoying the Chinese New Year! Open Source FTW 🚀" src="https://preview.redd.it/nv34uyjqtofe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9be62292f2e57c4882e28f231b0a82d1126bdabd" title="Ollama enjoying the Chinese New Year! Open Source FTW 🚀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eternviking"&gt; /u/eternviking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nv34uyjqtofe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ibwdvx/ollama_enjoying_the_chinese_new_year_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ibwdvx/ollama_enjoying_the_chinese_new_year_open_source/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-01-28T07:34:16+00:00</published>
  </entry>
</feed>
