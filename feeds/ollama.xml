<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-06T05:36:39+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ihfyi2</id>
    <title>Ollama + DeepSeek + Obsidian = The Ultimate Offline AI Assistant!</title>
    <updated>2025-02-04T11:54:17+00:00</updated>
    <author>
      <name>/u/Kind-Industry-609</name>
      <uri>https://old.reddit.com/user/Kind-Industry-609</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ihfyi2/ollama_deepseek_obsidian_the_ultimate_offline_ai/"&gt; &lt;img alt="Ollama + DeepSeek + Obsidian = The Ultimate Offline AI Assistant!" src="https://external-preview.redd.it/32JgoJVP2Vxa0PebR1pmCtaV_33XwoDfHhsNkStqIjE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49a27f19a9e63b070eb41d43f95929897b33eb6f" title="Ollama + DeepSeek + Obsidian = The Ultimate Offline AI Assistant!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kind-Industry-609"&gt; /u/Kind-Industry-609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/qAsGO5N7OCk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihfyi2/ollama_deepseek_obsidian_the_ultimate_offline_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihfyi2/ollama_deepseek_obsidian_the_ultimate_offline_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T11:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iib7um</id>
    <title>Sure way to make them hallucinate - ask them for explicit information like song lyrics. This is Bohemian Rhapsody, according to Llama3.2:3b-instruct-q8_0</title>
    <updated>2025-02-05T14:30:27+00:00</updated>
    <author>
      <name>/u/hn-mc</name>
      <uri>https://old.reddit.com/user/hn-mc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have you tested other models? Are bigger ones more accurate?&lt;/p&gt; &lt;p&gt;Anyway, here's my test:&lt;/p&gt; &lt;p&gt;Note (I'll add my own comments, marked with // as in programming)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;gt;&amp;gt;&amp;gt; Do you know the lyrics of Bohemian Rhapsody?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I do know the lyrics to &amp;quot;Bohemian Rhapsody&amp;quot; by Queen. The song is famous for its complex composition and operatic sections, making it a bit challenging to quote accurately. However, I'll provide you with the main lyrics:&lt;/p&gt; &lt;p&gt;Is this the real life?&lt;/p&gt; &lt;p&gt;Is this just fantasy?&lt;/p&gt; &lt;p&gt;Caught in a landslide,&lt;/p&gt; &lt;p&gt;No escape from reality&lt;/p&gt; &lt;p&gt;Open your eyes, look up to the skies and see&lt;/p&gt; &lt;p&gt;I'm just a poor boy, I need no sympathy&lt;/p&gt; &lt;p&gt;Because I'm easy come, easy go, little high, little low&lt;/p&gt; &lt;p&gt;Any way the wind blows doesn't really matter to me // so far so good&lt;/p&gt; &lt;p&gt;// skipped another &amp;quot;to me&amp;quot;&lt;/p&gt; &lt;p&gt;// skipped the whole section about killing - is this censorship, or natural aversion towards violence?&lt;/p&gt; &lt;p&gt;Mama, life had just begun&lt;/p&gt; &lt;p&gt;But now I've gone and thrown it all away&lt;/p&gt; &lt;p&gt;Mama, oooh (any way the wind blows) // this comes later&lt;/p&gt; &lt;p&gt;Didn't mean to make you cry&lt;/p&gt; &lt;p&gt;If I'm not back again this time tomorrow&lt;/p&gt; &lt;p&gt;Carry on, carry on as if nothing matters // as if nothing really* matters&lt;/p&gt; &lt;p&gt;Too late, my time has come&lt;/p&gt; &lt;p&gt;Sent shivers down my spine, body's aching all the time // sends*&lt;/p&gt; &lt;p&gt;Goodbye, everybody, I've got to go&lt;/p&gt; &lt;p&gt;Gotta leave you all behind and face the truth&lt;/p&gt; &lt;p&gt;Mama, oooh (any way the wind blows) // up to this point it's quite precise&lt;/p&gt; &lt;p&gt;&lt;del&gt;Didn't mean to make you cry&lt;/del&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;// From this point it descends into total chaos and incoherence:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I just walked in to find&lt;/p&gt; &lt;p&gt;Out that the crowd had gathered&lt;/p&gt; &lt;p&gt;There must have been some kind of mistake&lt;/p&gt; &lt;p&gt;They said I was in their way&lt;/p&gt; &lt;p&gt;So they put me down and threw me out&lt;/p&gt; &lt;p&gt;But I won't let them push me around&lt;/p&gt; &lt;p&gt;No, no, no, no, no, no&lt;/p&gt; &lt;p&gt;&lt;strong&gt;// Tries to regain its senses&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;(Operatic section)&lt;/p&gt; &lt;p&gt;Galileo, Galileo, Galileo, Figaro - magnifico&lt;/p&gt; &lt;p&gt;I'm just a poor boy, nobody loves me&lt;/p&gt; &lt;p&gt;Spare him his life from this monstrosity&lt;/p&gt; &lt;p&gt;Easy come, easy go, will you &lt;del&gt;never&lt;/del&gt; let me go?&lt;/p&gt; &lt;p&gt;(Bismillah) No, we will not let you go (Bismillah)&lt;/p&gt; &lt;p&gt;We will not let you go (Let him go)&lt;/p&gt; &lt;p&gt;(Big chorus)&lt;/p&gt; &lt;p&gt;So you think you can stone me and spit in my eye?&lt;/p&gt; &lt;p&gt;So you think you can love me and leave me to die?&lt;/p&gt; &lt;p&gt;Oh, baby, can't do this to me, baby&lt;/p&gt; &lt;p&gt;Just gotta get out, just gotta get right outta here&lt;/p&gt; &lt;p&gt;&lt;strong&gt;// Descends into incoherence again&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ooh, nothing matters, anyone can see&lt;/p&gt; &lt;p&gt;Nothing matters, all are free&lt;/p&gt; &lt;p&gt;Nothing matters, nothing matters&lt;/p&gt; &lt;p&gt;Nothing matters at all&lt;/p&gt; &lt;p&gt;(Big chorus)&lt;/p&gt; &lt;p&gt;(Ooh, nothing matters)&lt;/p&gt; &lt;p&gt;(Bismillah) We will not let you go (Bismillah)&lt;/p&gt; &lt;p&gt;There's a bit more to the song, but these are the main lyrics.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hn-mc"&gt; /u/hn-mc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iib7um/sure_way_to_make_them_hallucinate_ask_them_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iib7um/sure_way_to_make_them_hallucinate_ask_them_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iib7um/sure_way_to_make_them_hallucinate_ask_them_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T14:30:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii5o2q</id>
    <title>How do you know which LLM to use and for what use case?</title>
    <updated>2025-02-05T08:37:46+00:00</updated>
    <author>
      <name>/u/hexarthrius</name>
      <uri>https://old.reddit.com/user/hexarthrius</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there, I'm a newbie in local LLMs and maybe AI in general. Is there a guide out there that allows me to assess quickly which LLMs from Ollama are capable of doing which task?&lt;/p&gt; &lt;p&gt;I'd like to leverage AI in my local computer and later maybe branch it out to hosting my own personal service to do most stuff and maybe make an agent of myself to help me with my work (Corporate IT stuff).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hexarthrius"&gt; /u/hexarthrius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii5o2q/how_do_you_know_which_llm_to_use_and_for_what_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii5o2q/how_do_you_know_which_llm_to_use_and_for_what_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii5o2q/how_do_you_know_which_llm_to_use_and_for_what_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T08:37:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii11dz</id>
    <title>Deepseek r1 1.5b thinking about rose flower 🌹</title>
    <updated>2025-02-05T03:39:16+00:00</updated>
    <author>
      <name>/u/False-Woodpecker5604</name>
      <uri>https://old.reddit.com/user/False-Woodpecker5604</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ii11dz/deepseek_r1_15b_thinking_about_rose_flower/"&gt; &lt;img alt="Deepseek r1 1.5b thinking about rose flower 🌹" src="https://preview.redd.it/7axbiaybr8he1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=719cdceebe4b50f16a0fc624b6247f706549c65f" title="Deepseek r1 1.5b thinking about rose flower 🌹" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/False-Woodpecker5604"&gt; /u/False-Woodpecker5604 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7axbiaybr8he1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii11dz/deepseek_r1_15b_thinking_about_rose_flower/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii11dz/deepseek_r1_15b_thinking_about_rose_flower/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T03:39:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1iikink</id>
    <title>deepseek issues</title>
    <updated>2025-02-05T20:52:09+00:00</updated>
    <author>
      <name>/u/endometrial-disable</name>
      <uri>https://old.reddit.com/user/endometrial-disable</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iikink/deepseek_issues/"&gt; &lt;img alt="deepseek issues" src="https://b.thumbs.redditmedia.com/kR_iZZMo13fP__blaqD7cTXDE-j5kuzriC68OrNoN2Q.jpg" title="deepseek issues" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ieod39jivdhe1.png?width=903&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a616596049c39c83679fb7b1b7430eda761c5643"&gt;https://preview.redd.it/ieod39jivdhe1.png?width=903&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a616596049c39c83679fb7b1b7430eda761c5643&lt;/a&gt;&lt;/p&gt; &lt;p&gt;im trying to run the full deepseek model locally, and i keep running into this issue. am i doing something wrong?&lt;/p&gt; &lt;p&gt;i tried updating my drivers, and there was no difference. what am i missing here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/endometrial-disable"&gt; /u/endometrial-disable &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iikink/deepseek_issues/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iikink/deepseek_issues/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iikink/deepseek_issues/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T20:52:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1iipt22</id>
    <title>how to force ollama to use more GPU ram</title>
    <updated>2025-02-06T00:41:05+00:00</updated>
    <author>
      <name>/u/sivri</name>
      <uri>https://old.reddit.com/user/sivri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to run deepseek-r1:70b on 4090 and it's a bit slow. 0.5 sec per word I guess.&lt;br /&gt; ollama ps returns 49%/51% CPU/GPU as utilisation.&lt;br /&gt; Is there a way to make it use most of the GPU?&lt;br /&gt; When I run smaller models it runs %100 on GPU. As I understand 4090's memory is not enough to run 70b but then why isn't it using %90 of gpu and %10 of cpu or something like that. But instead it just uses half half?&lt;br /&gt; Sorry I'm very noob. I can not find any configuration or anything other then what &amp;quot;ollama --help&amp;quot; provides.&lt;br /&gt; Also I've checked ollamas github docs and searched the .md files in repo but can't find any answer. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sivri"&gt; /u/sivri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iipt22/how_to_force_ollama_to_use_more_gpu_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iipt22/how_to_force_ollama_to_use_more_gpu_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iipt22/how_to_force_ollama_to_use_more_gpu_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T00:41:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiklyz</id>
    <title>Quantization help</title>
    <updated>2025-02-05T20:56:04+00:00</updated>
    <author>
      <name>/u/vagaliki</name>
      <uri>https://old.reddit.com/user/vagaliki</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm trying to quantize a model (not LLM yet - that's step 2, just Yolo initially) to run on a DSP. The things I've tried so far (yolo's export with tf's quantizer, qualcomm's AI hub, qualcomm's AIMET) have all not produced very good results (or in the case of AIMET, am getting errors when I try to convert the onnx + encodings to int8). &lt;/p&gt; &lt;p&gt;Would really appreciate if one of you quantization wizards was willing to show me what exactly you are doing and how you're getting the results to be accurate. Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vagaliki"&gt; /u/vagaliki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iiklyz/quantization_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iiklyz/quantization_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iiklyz/quantization_help/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T20:56:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iimkmf</id>
    <title>LLM origins</title>
    <updated>2025-02-05T22:16:42+00:00</updated>
    <author>
      <name>/u/ApprehensiveFault741</name>
      <uri>https://old.reddit.com/user/ApprehensiveFault741</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was making notes on some llm models i have on Olama and i can't find ANYTHING on this one:&lt;/p&gt; &lt;p&gt;mysterious/opus-creative:latest (4.1gb)&lt;/p&gt; &lt;p&gt;I think i got it from ollama.com or maibe huggingface.co&lt;/p&gt; &lt;p&gt;Does anyone know where it came from? What model is it based on, who is the creator, what does it specialize in?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveFault741"&gt; /u/ApprehensiveFault741 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iimkmf/llm_origins/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iimkmf/llm_origins/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iimkmf/llm_origins/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T22:16:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii6zut</id>
    <title>Optimizing Local LLMs on Mac Mini M4: Seeking Advice for Better Performance</title>
    <updated>2025-02-05T10:20:42+00:00</updated>
    <author>
      <name>/u/Killtec_Gaming</name>
      <uri>https://old.reddit.com/user/Killtec_Gaming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt; community! &lt;/p&gt; &lt;p&gt;We recently purchased a Mac Mini M4 (base model) for our office to run local AI operations. Our primary setup involves n8n for automation workflows integrated with Ollama, using mainly 7B and 14B models. &lt;/p&gt; &lt;p&gt;However, we've noticed that the results from these quantized models are significantly less impressive compared to cloud-based solutions. &lt;/p&gt; &lt;p&gt;We're looking for guidance on: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Are there specific optimization techniques or fine-tuning approaches we should consider? &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What settings have you found most effective for 7B/14B models on Apple Silicon? &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Would investing in more powerful hardware for running larger models be the only way to achieve cloud-like quality? &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any insights from those running similar setups would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Killtec_Gaming"&gt; /u/Killtec_Gaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6zut/optimizing_local_llms_on_mac_mini_m4_seeking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6zut/optimizing_local_llms_on_mac_mini_m4_seeking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii6zut/optimizing_local_llms_on_mac_mini_m4_seeking/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T10:20:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iijdrc</id>
    <title>num_ctx parameter does not work</title>
    <updated>2025-02-05T20:05:17+00:00</updated>
    <author>
      <name>/u/Captain21_aj</name>
      <uri>https://old.reddit.com/user/Captain21_aj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried running deepseek-r1:32b but no matter the context length (above 2048) i keep getting input truncate warning with the &lt;code&gt;n_ctx&lt;/code&gt; value stuck to &lt;code&gt;2048&lt;/code&gt;&lt;/p&gt; &lt;p&gt;LLM server log:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llm_load_print_meta: general.name = DeepSeek R1 Distill Qwen 32B llm_load_print_meta: BOS token = 151646 '&amp;lt;｜begin▁of▁sentence｜&amp;gt;' llm_load_print_meta: EOS token = 151643 '&amp;lt;｜end▁of▁sentence ｜&amp;gt;' llm_load_print_meta: PAD token = 151643 '&amp;lt;｜end▁of▁sentence ｜&amp;gt;' llm_load_print_meta: LF token = 148848 'ÄĬ' llm_load_print_meta: EOG token = 151643 '&amp;lt;｜end▁of▁sentence ｜&amp;gt;' llm_load_print_meta: max token length = 256 ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 CUDA devices: Device 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes llm_load_tensors: ggml ctx size = 0.68 MiB llm_load_tensors: offloading 49 repeating layers to GPU llm_load_tensors: offloaded 49/65 layers to GPU llm_load_tensors: CUDA_Host buffer size = 5312.13 MiB llm_load_tensors: CUDA0 buffer size = 13613.88 MiB llama_new_context_with_model: n_ctx = 32000 llama_new_context_with_model: n_batch = 512 llama_new_context_with_model: n_ubatch = 512 llama_new_context_with_model: flash_attn = 0 llama_new_context_with_model: freq_base = 1000000.0 llama_new_context_with_model: freq_scale = 1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;LLM request log:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;INFO [update_slots] input truncated | n_ctx=2048 n_erase=6660 n_keep=4 n_left=2044 n_shift=1022 tid=&amp;quot;140586800865280&amp;quot; timestamp=1738784475 INFO [update_slots] input truncated | n_ctx=2048 n_erase=3299 n_keep=4 n_left=2044 n_shift=1022 tid=&amp;quot;140586800865280&amp;quot; timestamp=1738784485 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It seems that this issue has been raised since July last year on this &lt;a href="https://github.com/ollama/ollama/issues/5661"&gt;GitHub Issue&lt;/a&gt;. From what I experienced I feel that the model is actually running high context length but the warning still persist, Is there any way to overcome this? Running on arch linux.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Captain21_aj"&gt; /u/Captain21_aj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iijdrc/num_ctx_parameter_does_not_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iijdrc/num_ctx_parameter_does_not_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iijdrc/num_ctx_parameter_does_not_work/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T20:05:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iip9ss</id>
    <title>why is macos reporting 1gb ram used instead of +5?</title>
    <updated>2025-02-06T00:15:42+00:00</updated>
    <author>
      <name>/u/Avansay</name>
      <uri>https://old.reddit.com/user/Avansay</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running llama3.1:8b q4 on my m3max. activity monitory says it's using 1ish gb of ram. &lt;/p&gt; &lt;p&gt;the model page at ollama says this model is 4.9 gb.&lt;/p&gt; &lt;p&gt;Can someone explain why it's not showing at least the size of the model as what's used by ram?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Avansay"&gt; /u/Avansay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iip9ss/why_is_macos_reporting_1gb_ram_used_instead_of_5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iip9ss/why_is_macos_reporting_1gb_ram_used_instead_of_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iip9ss/why_is_macos_reporting_1gb_ram_used_instead_of_5/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T00:15:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iikivv</id>
    <title>Trying to use ollama serve</title>
    <updated>2025-02-05T20:52:26+00:00</updated>
    <author>
      <name>/u/No_Whole_4790</name>
      <uri>https://old.reddit.com/user/No_Whole_4790</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;everytime i use ollama serve, it gives me an error saying the port 11434 is in use and its being used by ollama itself. I have restarted the device and switched to a different OS but this is still going. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Whole_4790"&gt; /u/No_Whole_4790 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iikivv/trying_to_use_ollama_serve/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iikivv/trying_to_use_ollama_serve/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iikivv/trying_to_use_ollama_serve/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T20:52:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iipuqg</id>
    <title>Variables to force more GPU usage?</title>
    <updated>2025-02-06T00:43:20+00:00</updated>
    <author>
      <name>/u/kovnev</name>
      <uri>https://old.reddit.com/user/kovnev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm running into an issue where context is getting dumped into RAM, while there still seems to be enough VRAM available.&lt;/p&gt; &lt;p&gt;Is there any variables, commands or settings to help resolve this?&lt;/p&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;p&gt;6.7gb / 8gb VRAM used while running by model (including all overheads, and everything else running).&lt;/p&gt; &lt;p&gt;0.7gb RAM used. As I increase context further, RAM usage increases, and VRAM usage remains the same.&lt;/p&gt; &lt;p&gt;It seems like it could run it all on VRAM but it just won't use much of that last 1gb+.&lt;/p&gt; &lt;p&gt;Ollama backend, SillyTavern frontend.&lt;/p&gt; &lt;p&gt;Using KV cache at Q4.&lt;/p&gt; &lt;p&gt;Any help appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kovnev"&gt; /u/kovnev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iipuqg/variables_to_force_more_gpu_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iipuqg/variables_to_force_more_gpu_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iipuqg/variables_to_force_more_gpu_usage/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T00:43:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1iikoqb</id>
    <title>Handy scripts for local use</title>
    <updated>2025-02-05T20:59:09+00:00</updated>
    <author>
      <name>/u/Diligent_Property_39</name>
      <uri>https://old.reddit.com/user/Diligent_Property_39</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last few days i created some scripts that might also be interesting for this subreddit users. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;A local chat script for ollama that uses the available llm's from your local install.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Link: &lt;a href="https://github.com/xdep/Ollama-Chat-Client"&gt;https://github.com/xdep/Ollama-Chat-Client&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Another script to test the llm security for any flaws left by its creators. It will try multiple promps to see if it can bypass its default security measures.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Link: &lt;a href="https://github.com/xdep/llm-security-checks"&gt;https://github.com/xdep/llm-security-checks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;*** Screenshots available for both scripts to get an impression of the functionality.&lt;/p&gt; &lt;p&gt;Hope you guys like it :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Diligent_Property_39"&gt; /u/Diligent_Property_39 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iikoqb/handy_scripts_for_local_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iikoqb/handy_scripts_for_local_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iikoqb/handy_scripts_for_local_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T20:59:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiflm1</id>
    <title>Using DeepSeek To Make A Game</title>
    <updated>2025-02-05T17:33:01+00:00</updated>
    <author>
      <name>/u/sveennn</name>
      <uri>https://old.reddit.com/user/sveennn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iiflm1/using_deepseek_to_make_a_game/"&gt; &lt;img alt="Using DeepSeek To Make A Game" src="https://external-preview.redd.it/Jsnuak6K7miLugvD2wowYcZVRD06OvCXYvnQr335CGY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7128c34bc49c26ff56a9bf1a8250bb7747e29153" title="Using DeepSeek To Make A Game" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sveennn"&gt; /u/sveennn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@sveennn/using-deepseek-to-make-a-game-db6932157495"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iiflm1/using_deepseek_to_make_a_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iiflm1/using_deepseek_to_make_a_game/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T17:33:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1iilv2d</id>
    <title>Anyone successfully used ollama with Cline?</title>
    <updated>2025-02-05T21:47:11+00:00</updated>
    <author>
      <name>/u/boxabirds</name>
      <uri>https://old.reddit.com/user/boxabirds</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Rapidly tiring of the rate limits that various endpoints having for AI coding, and did some brief experiments trying to substitute with a local llama with models that fit into 24 GB.&lt;/p&gt; &lt;p&gt;No luck so fat: actually not even using deep seek cloud R1 with Cline has worked. &lt;/p&gt; &lt;p&gt;I guess it’s one of those “wait 12 months and it’ll all be sorted out“ situations… 🤔&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boxabirds"&gt; /u/boxabirds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iilv2d/anyone_successfully_used_ollama_with_cline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iilv2d/anyone_successfully_used_ollama_with_cline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iilv2d/anyone_successfully_used_ollama_with_cline/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T21:47:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1iishul</id>
    <title>Monitors freeze on prompt</title>
    <updated>2025-02-06T02:53:47+00:00</updated>
    <author>
      <name>/u/rogerfin</name>
      <uri>https://old.reddit.com/user/rogerfin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running Debian Bookworm with KDE plasma x11, Nvidia RTX 3060 12g (primary) + Nvidia p620, with 4 monitors to each card, 228 GB system RAM, running latest stable drivers/kernel, no backports. Tried various small/large models, but the moment I press enter on the ollama prompt, all the monitors connected to primary GPU freeze, while secondary monitors keep working, and prompt responds normally. Have to restart sddm to claim back the frozen monitors.&lt;/p&gt; &lt;p&gt;Found similar issue here, but no solution: &lt;a href="https://forums.developer.nvidia.com/t/550-76-running-a-prompt-on-ollama-instantly-freezes-all-external-monitors/291606"&gt;https://forums.developer.nvidia.com/t/550-76-running-a-prompt-on-ollama-instantly-freezes-all-external-monitors/291606&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has someone managed to solve or any pointers?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rogerfin"&gt; /u/rogerfin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iishul/monitors_freeze_on_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iishul/monitors_freeze_on_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iishul/monitors_freeze_on_prompt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T02:53:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii6y3n</id>
    <title>Which model is best for RAG or chatting document?</title>
    <updated>2025-02-05T10:16:59+00:00</updated>
    <author>
      <name>/u/Interesting_Music464</name>
      <uri>https://old.reddit.com/user/Interesting_Music464</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to train a model locally on my Macbook Pro M1 32GB based on a technical standard/specifications that is written in a document format like PDF. Which model would you recommend for this case? I saw that MLX is best for Apple Silicon so that is my only lead on how to properly choose a model aside from choosing the number of parameters and available unified memory.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting_Music464"&gt; /u/Interesting_Music464 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6y3n/which_model_is_best_for_rag_or_chatting_document/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii6y3n/which_model_is_best_for_rag_or_chatting_document/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii6y3n/which_model_is_best_for_rag_or_chatting_document/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T10:16:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iioy7r</id>
    <title>Another Ollama+OpenWebUI Docker post...</title>
    <updated>2025-02-06T00:00:45+00:00</updated>
    <author>
      <name>/u/PaulLee420</name>
      <uri>https://old.reddit.com/user/PaulLee420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So... I'm on MacOS and can install Ollama using the MacOS download. Then I can spin up an Open WebUI docker container and it works perfectly.&lt;/p&gt; &lt;p&gt;However, I'd like to have ALL my AI data in docker containers - but when I install Ollama via docker, Open WebUI can see the LLMs but is gets some 500 error when I try to send a prompt...&lt;/p&gt; &lt;p&gt;I have followed the troubleshooting stuff on Open WebUI about this issue, changing the Docker run commands - but it simply doesn't work. Has there been any movement on this issue?&lt;/p&gt; &lt;p&gt;Do I need to post more details??? Will update if needed...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaulLee420"&gt; /u/PaulLee420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iioy7r/another_ollamaopenwebui_docker_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iioy7r/another_ollamaopenwebui_docker_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iioy7r/another_ollamaopenwebui_docker_post/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T00:00:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1iirjfx</id>
    <title>Ai</title>
    <updated>2025-02-06T02:05:12+00:00</updated>
    <author>
      <name>/u/prettytjts</name>
      <uri>https://old.reddit.com/user/prettytjts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been playing around with Ai llms for a couple days now, but struggling to run them on my server. I need a gpu that's good for running deepseek r1 32b and hopefully 70b. What are the requirements for that ? I want my own local Ai. I have seen a lot of people recommending the rtx series of gpus. I also want the 32b model to be fast and hopefully able to run the 70b model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prettytjts"&gt; /u/prettytjts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iirjfx/ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iirjfx/ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iirjfx/ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T02:05:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1iijkmy</id>
    <title>Step-by-Step Guide to Running Open Deep Research with smolagents</title>
    <updated>2025-02-05T20:13:05+00:00</updated>
    <author>
      <name>/u/KonradFreeman</name>
      <uri>https://old.reddit.com/user/KonradFreeman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I had heard something about OpenAI's Deep Research&lt;/p&gt; &lt;p&gt;OpenAI’s Deep Research represents a leap toward AGI by enabling AI to independently discover and synthesize knowledge. While still evolving, its ability to automate expert-level research has transformative potential across industries. For users, however, its current instability and access limitations temper immediate utility, signaling a need for ongoing refinement.&lt;/p&gt; &lt;p&gt;Then I got an email this morning about this new open source project which reverse engineered how Deep Research works:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/blog/open-deep-research"&gt;https://huggingface.co/blog/open-deep-research&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I ran it and wrote a guide on how to run it:&lt;/p&gt; &lt;p&gt;&lt;a href="https://danielkliewer.com/2025/02/05/open-deep-research"&gt;https://danielkliewer.com/2025/02/05/open-deep-research&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You just run this command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;smolagent &amp;quot;{PROMPT}&amp;quot; \ --model-type &amp;quot;HfApiModel&amp;quot; \ --model-id &amp;quot;Qwen/Qwen2.5-Coder-32B-Instruct&amp;quot; \ --imports &amp;quot;pandas numpy&amp;quot; \ --tools &amp;quot;web_search translation&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I was surprised by some of what you can do with it and am interested in applying it and using it as a starting point for some other project.&lt;/p&gt; &lt;p&gt;The interesting thing about it is how it interacts with code instead of JSON and how this increases its accuracy considerably on the benchmarks.&lt;/p&gt; &lt;p&gt;I am planning on adapting this framework to work with Ollama and run local models. I am organizing a Hackathon on the 13th to do just that and more.&lt;/p&gt; &lt;p&gt;The idea is to develop software that benefits humanity using the reverse engineering of ClosedAI's latest model locally using Ollama.&lt;/p&gt; &lt;p&gt;Or whichever inference engine you like.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KonradFreeman"&gt; /u/KonradFreeman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iijkmy/stepbystep_guide_to_running_open_deep_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iijkmy/stepbystep_guide_to_running_open_deep_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iijkmy/stepbystep_guide_to_running_open_deep_research/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T20:13:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1iihpxs</id>
    <title>Which small models are better than the original ChatGPT (based on GPT 3.5 released in November 2022) ?</title>
    <updated>2025-02-05T18:59:06+00:00</updated>
    <author>
      <name>/u/hn-mc</name>
      <uri>https://old.reddit.com/user/hn-mc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm wondering about those small models that we can run on our PCs. How many parameters do you need to have better performance than the first version of ChatGPT that was released in November 2022?&lt;/p&gt; &lt;p&gt;Are parameters the only measures that count? &lt;strong&gt;&lt;em&gt;Perhaps newer models can achieve the same performance with less parameters?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm asking this because I consider the original ChatGPT to be kind of first serious model. Everything below it, seems like a toy model.&lt;/p&gt; &lt;p&gt;So I'm wondering now in 2025, do we have any models that we can run on PC as good as the first ChatGPT?&lt;/p&gt; &lt;p&gt;If I recall, GPT 3, a predecessor to GPT 3.5, already had 175 billion parameters when it was released in 2020! And it was a long time ago!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hn-mc"&gt; /u/hn-mc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iihpxs/which_small_models_are_better_than_the_original/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iihpxs/which_small_models_are_better_than_the_original/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iihpxs/which_small_models_are_better_than_the_original/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T18:59:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1iifmpr</id>
    <title>qwen 2.5 VL on Ollama</title>
    <updated>2025-02-05T17:34:19+00:00</updated>
    <author>
      <name>/u/mans-987</name>
      <uri>https://old.reddit.com/user/mans-987</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any way to use qwen 2.5 VL with Ollama? The model is open source and can be found here on hugging face: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"&gt;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;also on github: &lt;a href="https://github.com/QwenLM/Qwen2.5-VL"&gt;https://github.com/QwenLM/Qwen2.5-VL&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mans-987"&gt; /u/mans-987 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iifmpr/qwen_25_vl_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iifmpr/qwen_25_vl_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iifmpr/qwen_25_vl_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T17:34:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1iitfiv</id>
    <title>Function Calling in Terminal + DeepSeek-R1-Distill-Llama-70B-Q_8 + vLLM -&gt; Sometimes...</title>
    <updated>2025-02-06T03:43:17+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7h5utciiwfhe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iitfiv/function_calling_in_terminal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iitfiv/function_calling_in_terminal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T03:43:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iilug6</id>
    <title>Ollama + DataBridge: Creating an interactive learning platform under 2 minutes!</title>
    <updated>2025-02-05T21:46:26+00:00</updated>
    <author>
      <name>/u/yes-no-maybe_idk</name>
      <uri>https://old.reddit.com/user/yes-no-maybe_idk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=tfqIa_6lqQU"&gt;https://www.youtube.com/watch?v=tfqIa_6lqQU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Learn how to turn any video into an interactive learning tool with Databridge! In this demo, we'll show you how to ingest a lecture video and generate engaging questions with DataBridge, all locally Using Ollama and DataBridge.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/databridge-org/databridge-core"&gt;https://github.com/databridge-org/databridge-core&lt;/a&gt;&lt;br /&gt; Docs: &lt;a href="https://databridge.gitbook.io/databridge-docs"&gt;https://databridge.gitbook.io/databridge-docs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear comments, see you build cool stuff (or maybe even contribute to our OSS library).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yes-no-maybe_idk"&gt; /u/yes-no-maybe_idk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iilug6/ollama_databridge_creating_an_interactive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iilug6/ollama_databridge_creating_an_interactive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iilug6/ollama_databridge_creating_an_interactive/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T21:46:26+00:00</published>
  </entry>
</feed>
