<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-05-18T18:25:17+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1kn2zd6</id>
    <title>Seeking Guidance: Integrating RealtimeTTS with dia-1.6B or OrpheusTTS for Arabic Conversational AI</title>
    <updated>2025-05-15T08:18:38+00:00</updated>
    <author>
      <name>/u/No-Reindeer-9968</name>
      <uri>https://old.reddit.com/user/No-Reindeer-9968</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a way to use RealtimeTTS with &lt;a href="https://github.com/nari-labs/dia"&gt;Nari/dia-1.6B&lt;/a&gt; or &lt;a href="https://github.com/canopyai/Orpheus-TTS"&gt;Canopy-AI/OrpheusTTS &lt;/a&gt;&lt;/p&gt; &lt;p&gt;I want want to finetune one of these models for arabic and build realtime conversational model.&lt;br /&gt; What I am looking to do is use:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/fixie-ai/ultravox-v0_5-llama-3_3-70b"&gt;UltraVox-v0.5&lt;/a&gt;, which can take in audio as input&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/snakers4/silero-vad"&gt;Silero-VAD&lt;/a&gt; for turn detection&lt;/li&gt; &lt;li&gt;Either dia-1.6B or orpheus fine-tuned for arabic for tts &lt;ul&gt; &lt;li&gt;but I want to know how can I utilize &lt;a href="https://github.com/KoljaB/RealtimeTTS"&gt;RealTimeTTS&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My ultimate goal is to have an alternative to OpenAI's RealtimeClient&lt;br /&gt; Ultimately I want to be able to connect to this speech-to-speech system using WebRTC (I am still looking for the best way to handle this)&lt;/p&gt; &lt;p&gt;I would like to get your thoughts on this, and mainly on how to use utilize RealTimeTTS with these TTS models, and on handling WebRTC connection&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Reindeer-9968"&gt; /u/No-Reindeer-9968 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kn2zd6/seeking_guidance_integrating_realtimetts_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kn2zd6/seeking_guidance_integrating_realtimetts_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kn2zd6/seeking_guidance_integrating_realtimetts_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-15T08:18:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmw283</id>
    <title>Now we know where Alex Jones lives.</title>
    <updated>2025-05-15T01:23:32+00:00</updated>
    <author>
      <name>/u/2Bit_Dev</name>
      <uri>https://old.reddit.com/user/2Bit_Dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kmw283/now_we_know_where_alex_jones_lives/"&gt; &lt;img alt="Now we know where Alex Jones lives." src="https://preview.redd.it/i5ox6vu8lu0f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9891408d2816c8b1ff50c107bf115a033b8e226" title="Now we know where Alex Jones lives." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/2Bit_Dev"&gt; /u/2Bit_Dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i5ox6vu8lu0f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmw283/now_we_know_where_alex_jones_lives/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kmw283/now_we_know_where_alex_jones_lives/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-15T01:23:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1knhp14</id>
    <title>Another step closer to AGI. Self Improve LLM and it's open source.</title>
    <updated>2025-05-15T19:56:05+00:00</updated>
    <author>
      <name>/u/Gazuroth</name>
      <uri>https://old.reddit.com/user/Gazuroth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1knhp14/another_step_closer_to_agi_self_improve_llm_and/"&gt; &lt;img alt="Another step closer to AGI. Self Improve LLM and it's open source." src="https://external-preview.redd.it/LRVBsdPuZB7DzsvErOIhojc7KtBvAcyPIK-4vHZgXoU.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5c227fbf10e944825ea3a963865e2ee18f85fa8" title="Another step closer to AGI. Self Improve LLM and it's open source." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gazuroth"&gt; /u/Gazuroth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/X37tgx0ngQE?si=8yZHfEaYXQWJKfOC"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knhp14/another_step_closer_to_agi_self_improve_llm_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1knhp14/another_step_closer_to_agi_self_improve_llm_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-15T19:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1knbsus</id>
    <title>HanaVerse - Chat with AI through an interactive anime character! üå∏</title>
    <updated>2025-05-15T15:57:51+00:00</updated>
    <author>
      <name>/u/OrganicTelevision652</name>
      <uri>https://old.reddit.com/user/OrganicTelevision652</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1knbsus/hanaverse_chat_with_ai_through_an_interactive/"&gt; &lt;img alt="HanaVerse - Chat with AI through an interactive anime character! üå∏" src="https://external-preview.redd.it/VMExyAyOE_4W1BYj5ZE65UYho8s1S8iYWLFddyI6R88.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a56e1472206dfd96091c85f5438f8e274f3dd61" title="HanaVerse - Chat with AI through an interactive anime character! üå∏" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on something I think you'll love - HanaVerse, an interactive web UI for Ollama that brings your AI conversations to life through a charming 2D anime character named Hana!&lt;/p&gt; &lt;p&gt;What is &lt;strong&gt;HanaVerse&lt;/strong&gt;? ü§î&lt;/p&gt; &lt;p&gt;HanaVerse transforms how you interact with Ollama's language models by adding a visual, animated companion to your conversations. Instead of just text on a screen, you chat with Hana - a responsive anime character who reacts to your interactions in real-time!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features that make HanaVerse special&lt;/strong&gt;: ‚ú®&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Talks Back:&lt;/strong&gt; Answers with voice&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Streaming Responses:&lt;/strong&gt; See answers form in real-time as they're generated&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full Markdown Support:&lt;/strong&gt; Beautiful formatting with syntax highlighting&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LaTeX Math Rendering:&lt;/strong&gt; Perfect for equations and scientific content&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Customizable:&lt;/strong&gt; Choose any Ollama model and configure system prompts&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Responsive Design:&lt;/strong&gt; Works on both desktop(preferred) and mobile&lt;/p&gt; &lt;p&gt;Why I built this üõ†Ô∏è&lt;/p&gt; &lt;p&gt;I wanted to make AI interactions more engaging and personal while leveraging the power of self-hosted Ollama models. The result is an interface that makes AI conversations feel more natural and enjoyable.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1knbsus/video/z09umqaaxy0f1/player"&gt;hanaverse demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you're looking for a more engaging way to interact with your Ollama models, give HanaVerse a try and let me know what you think!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Ashish-Patnaik/HanaVerse"&gt;https://github.com/Ashish-Patnaik/HanaVerse&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Skeleton Demo = &lt;a href="https://hanaverse.vercel.app/"&gt;https://hanaverse.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love your feedback and contributions - stars ‚≠ê are always appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OrganicTelevision652"&gt; /u/OrganicTelevision652 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knbsus/hanaverse_chat_with_ai_through_an_interactive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knbsus/hanaverse_chat_with_ai_through_an_interactive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1knbsus/hanaverse_chat_with_ai_through_an_interactive/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-15T15:57:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmys0r</id>
    <title>Open Source Alternative to NotebookLM</title>
    <updated>2025-05-15T03:46:22+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kmys0r/open_source_alternative_to_notebooklm/"&gt; &lt;img alt="Open Source Alternative to NotebookLM" src="https://external-preview.redd.it/VevUXxFMDNJ5FeBLYffcQpExFjNnxm4O_4LDinSqjLg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0be8dbe7226b07582e11fdba905929cdcc99a533" title="Open Source Alternative to NotebookLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with &lt;strong&gt;SurfSense&lt;/strong&gt;, it aims to be the open-source alternative to &lt;strong&gt;NotebookLM&lt;/strong&gt;, &lt;strong&gt;Perplexity&lt;/strong&gt;, or &lt;strong&gt;Glean&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent but connected to your personal external sources search engines (Tavily, LinkUp), Slack, Linear, Notion, YouTube, GitHub, and more coming soon.&lt;/p&gt; &lt;p&gt;I'll keep this short‚Äîhere are a few highlights of SurfSense:&lt;/p&gt; &lt;p&gt;üìä Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports &lt;strong&gt;150+ LLM's&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Supports local &lt;strong&gt;Ollama LLM's&lt;/strong&gt; or &lt;strong&gt;vLLM&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Supports &lt;strong&gt;6000+ Embedding Models&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt; &lt;li&gt;Uses &lt;strong&gt;Hierarchical Indices&lt;/strong&gt; (2-tiered RAG setup)&lt;/li&gt; &lt;li&gt;Combines &lt;strong&gt;Semantic + Full-Text Search&lt;/strong&gt; with &lt;strong&gt;Reciprocal Rank Fusion&lt;/strong&gt; (Hybrid Search)&lt;/li&gt; &lt;li&gt;Offers a &lt;strong&gt;RAG-as-a-Service API Backend&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Supports 34+ File extensions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üéôÔ∏è Podcasts&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)&lt;/li&gt; &lt;li&gt;Convert your chat conversations into engaging audio content&lt;/li&gt; &lt;li&gt;Support for multiple TTS providers (OpenAI, Azure, Google Vertex AI)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚ÑπÔ∏è &lt;strong&gt;External Sources&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Search engines (Tavily, LinkUp)&lt;/li&gt; &lt;li&gt;Slack&lt;/li&gt; &lt;li&gt;Linear&lt;/li&gt; &lt;li&gt;Notion&lt;/li&gt; &lt;li&gt;YouTube videos&lt;/li&gt; &lt;li&gt;GitHub&lt;/li&gt; &lt;li&gt;...and more on the way&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üîñ &lt;strong&gt;Cross-Browser Exten&lt;/strong&gt;sion&lt;br /&gt; The SurfSense extension lets you save any dynamic webpage you like. Its main use case is capturing pages that are protected behind authentication.&lt;/p&gt; &lt;p&gt;Check out SurfSense on GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MODSetter/SurfSense"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kmys0r/open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kmys0r/open_source_alternative_to_notebooklm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-15T03:46:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1knhx7j</id>
    <title>When is ollama going to support re-ranking models?</title>
    <updated>2025-05-15T20:05:33+00:00</updated>
    <author>
      <name>/u/Agreeable_Cat602</name>
      <uri>https://old.reddit.com/user/Agreeable_Cat602</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;like through Open WebUI ...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agreeable_Cat602"&gt; /u/Agreeable_Cat602 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knhx7j/when_is_ollama_going_to_support_reranking_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knhx7j/when_is_ollama_going_to_support_reranking_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1knhx7j/when_is_ollama_going_to_support_reranking_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-15T20:05:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1knpgno</id>
    <title>Best model to use in ollama for faster chat &amp; best Structured output result</title>
    <updated>2025-05-16T01:56:25+00:00</updated>
    <author>
      <name>/u/gilzonme</name>
      <uri>https://old.reddit.com/user/gilzonme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am building a chatbot based data extraction platform. Which model should i use to achieve faster chat &amp;amp; best Structured output result&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gilzonme"&gt; /u/gilzonme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knpgno/best_model_to_use_in_ollama_for_faster_chat_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knpgno/best_model_to_use_in_ollama_for_faster_chat_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1knpgno/best_model_to_use_in_ollama_for_faster_chat_best/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-16T01:56:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn9yil</id>
    <title>Project NOVA: Giving Ollama Control of 25+ Self-Hosted Services</title>
    <updated>2025-05-15T14:43:11+00:00</updated>
    <author>
      <name>/u/kingduj</name>
      <uri>https://old.reddit.com/user/kingduj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a system that uses Ollama models to control all my self-hosted applications through function calling. Wanted to share with the community!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ollama (with qwen3, llama3.1, or mistral) provides the reasoning layer&lt;/li&gt; &lt;li&gt;A router agent analyzes requests and delegates to specialized experts&lt;/li&gt; &lt;li&gt;25+ domain-specific agents connect to various applications via MCP servers&lt;/li&gt; &lt;li&gt;n8n handles workflow orchestration and connects everything together&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What it can control:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Knowledge bases (TriliumNext, BookStack, Outline)&lt;/li&gt; &lt;li&gt;Media tools (Reaper DAW, OBS Studio, YouTube transcription)&lt;/li&gt; &lt;li&gt;Development (Gitea, CLI server)&lt;/li&gt; &lt;li&gt;Home automation (Home Assistant)&lt;/li&gt; &lt;li&gt;And many more...&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've found this setup works really well with Ollama's speed and local privacy (the above mentioned models work well a 8GB VRAM GPU -- I'm using a 2070). All processing stays on my LAN, and the specialized agent approach means each domain gets expert handling rather than trying to force one model to know everything.&lt;/p&gt; &lt;p&gt;The repo includes all system prompts, Docker configurations, n8n workflows, and detailed documentation to get it running with your own Ollama instance.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/dujonwalker/project-nova"&gt;GitHub: dujonwalker/project-nova&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone else built similar integrations with Ollama? Would love to compare notes!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kingduj"&gt; /u/kingduj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kn9yil/project_nova_giving_ollama_control_of_25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kn9yil/project_nova_giving_ollama_control_of_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kn9yil/project_nova_giving_ollama_control_of_25/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-15T14:43:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1knvf2s</id>
    <title>Qwen 2.5 VL 72B: 4-bit quant almost as big as 8-bit (doesn't fit in 48GB VRAM)</title>
    <updated>2025-05-16T08:00:49+00:00</updated>
    <author>
      <name>/u/Netcob</name>
      <uri>https://old.reddit.com/user/Netcob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1knvf2s/qwen_25_vl_72b_4bit_quant_almost_as_big_as_8bit/"&gt; &lt;img alt="Qwen 2.5 VL 72B: 4-bit quant almost as big as 8-bit (doesn't fit in 48GB VRAM)" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="Qwen 2.5 VL 72B: 4-bit quant almost as big as 8-bit (doesn't fit in 48GB VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;8_0: 79GB&lt;/p&gt; &lt;p&gt;Q4_K_M: 71GB&lt;/p&gt; &lt;p&gt;In other words, this won't fit in 48GB VRAM unlike other 72B 4-bit quants. Not sure what this means - maybe only a small part of the model can be quantized?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Netcob"&gt; /u/Netcob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/library/qwen2.5vl/tags"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knvf2s/qwen_25_vl_72b_4bit_quant_almost_as_big_as_8bit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1knvf2s/qwen_25_vl_72b_4bit_quant_almost_as_big_as_8bit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-16T08:00:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1knr5jf</id>
    <title>Started building a fun weekend project using Ollama &amp; Postgres</title>
    <updated>2025-05-16T03:27:47+00:00</updated>
    <author>
      <name>/u/Few-Needleworker3764</name>
      <uri>https://old.reddit.com/user/Few-Needleworker3764</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1knr5jf/started_building_a_fun_weekend_project_using/"&gt; &lt;img alt="Started building a fun weekend project using Ollama &amp;amp; Postgres" src="https://external-preview.redd.it/j9TkJF57vnTN2_XYEbMx8Zjn-VHgh9cntuV7uDWdLFk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31fb31fac1ef33102fee2df90d00ebfb3c8238ee" title="Started building a fun weekend project using Ollama &amp;amp; Postgres" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fun weekend 'Vibe Coding' project building SQL query generation from Natural Language&lt;/p&gt; &lt;ul&gt; &lt;li&gt; Ollama to serve Qwen3:4b&lt;/li&gt; &lt;li&gt;Netflix demo db&lt;/li&gt; &lt;li&gt;Postgres DB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Current progress&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Used a detailed prompt to feed in Schema &amp;amp; sample SQL queries.&lt;/li&gt; &lt;li&gt;Set context about datatypes it should consider when generating queries&lt;/li&gt; &lt;li&gt;Append the query to the base prompt&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d8dkkk0cc21f1.png?width=2557&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=991910bb2bc8250f9f5ee6910a8c6bc165794de3"&gt;https://preview.redd.it/d8dkkk0cc21f1.png?width=2557&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=991910bb2bc8250f9f5ee6910a8c6bc165794de3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Next Steps&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Adding a UI&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/ai-in-plain-english/essential-ollama-commands-you-should-know-e8b29e436391"&gt;https://medium.com/ai-in-plain-english/essential-ollama-commands-you-should-know-e8b29e436391&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few-Needleworker3764"&gt; /u/Few-Needleworker3764 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knr5jf/started_building_a_fun_weekend_project_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knr5jf/started_building_a_fun_weekend_project_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1knr5jf/started_building_a_fun_weekend_project_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-16T03:27:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko76bp</id>
    <title>Ollama Not Using GPU (AMD RX 9070XT)</title>
    <updated>2025-05-16T17:49:43+00:00</updated>
    <author>
      <name>/u/Libroru</name>
      <uri>https://old.reddit.com/user/Libroru</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just downloaded ollama to try out the llama3:4b performance on my new GPU.&lt;/p&gt; &lt;p&gt;I am having issues with ollama not targetting the GPU at all and just going ham on the CPU.&lt;/p&gt; &lt;p&gt;Running on Windows 11 with the newest ollama binary directly installed on windows.&lt;br /&gt; Also using the docker version of open-webui.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Libroru"&gt; /u/Libroru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ko76bp/ollama_not_using_gpu_amd_rx_9070xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ko76bp/ollama_not_using_gpu_amd_rx_9070xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ko76bp/ollama_not_using_gpu_amd_rx_9070xt/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-16T17:49:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1knu8bm</id>
    <title>Is anyone using ollama for production purposes?</title>
    <updated>2025-05-16T06:36:10+00:00</updated>
    <author>
      <name>/u/gilzonme</name>
      <uri>https://old.reddit.com/user/gilzonme</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gilzonme"&gt; /u/gilzonme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knu8bm/is_anyone_using_ollama_for_production_purposes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1knu8bm/is_anyone_using_ollama_for_production_purposes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1knu8bm/is_anyone_using_ollama_for_production_purposes/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-16T06:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko1jph</id>
    <title>What model repositories work with ollama pull?</title>
    <updated>2025-05-16T13:58:57+00:00</updated>
    <author>
      <name>/u/synthphreak</name>
      <uri>https://old.reddit.com/user/synthphreak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;By default, &lt;code&gt;ollama pull&lt;/code&gt; seems set up to work with models in the &lt;a href="https://ollama.com/library"&gt;Ollama models library&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;However, digging a bit, I learned that you can pull Ollama-compatible models off the HuggingFace model hub by appending &lt;code&gt;hf.co/&lt;/code&gt; to the model ID. However, it seems most models in the hub are not compatible with &lt;code&gt;ollama&lt;/code&gt; and will throw an error.&lt;/p&gt; &lt;p&gt;This raises two questions for me:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Is there a convenient, robust way to filter the HF models hub down to &lt;code&gt;ollama&lt;/code&gt;-compatible models only? You can filter in the browser with &lt;code&gt;other=ollama&lt;/code&gt;, but about half of the resulting models fail with&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;code&gt; Error: pull model manifest: 400: {&amp;quot;error&amp;quot;:&amp;quot;Repository is not GGUF or is not compatible with llama.cpp&amp;quot;} &lt;/code&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;What other model hubs exist which work with &lt;code&gt;ollama pull&lt;/code&gt;? For example, I've read that &lt;a href="https://modelscope.cn/models"&gt;https://modelscope.cn/models&lt;/a&gt; allegedly works, but all the models I've tried with have failed to download. For example:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;code&gt;shell ‚ùØ ollama pull LKShizuku/ollama3_7B_cat-gguf pulling manifest Error: pull model manifest: file does not exist ‚ùØ ollama pull modelscope.com/LKShizuku/ollama3_7B_cat-gguf pulling manifest Error: unexpected status code 301 ‚ùØ ollama pull modelscope.co/LKShizuku/ollama3_7B_cat-gguf pulling manifest Error: pull model manifest: invalid character '&amp;lt;' looking for beginning of value &lt;/code&gt;&lt;/p&gt; &lt;p&gt;(using &lt;a href="https://modelscope.cn/models/LKShizuku/ollama3_7B_cat-gguf"&gt;this model&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/synthphreak"&gt; /u/synthphreak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ko1jph/what_model_repositories_work_with_ollama_pull/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ko1jph/what_model_repositories_work_with_ollama_pull/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ko1jph/what_model_repositories_work_with_ollama_pull/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-16T13:58:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kopl1c</id>
    <title>auto-openwebui: I made a bash script to automate running Open WebUI on Linux systems with Ollama and Cloudflare via Docker on AMD &amp; NVIDIA GPUs</title>
    <updated>2025-05-17T10:12:07+00:00</updated>
    <author>
      <name>/u/turjid</name>
      <uri>https://old.reddit.com/user/turjid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kopl1c/autoopenwebui_i_made_a_bash_script_to_automate/"&gt; &lt;img alt="auto-openwebui: I made a bash script to automate running Open WebUI on Linux systems with Ollama and Cloudflare via Docker on AMD &amp;amp; NVIDIA GPUs" src="https://external-preview.redd.it/Rt6bl8Q8H0VZS2zeB9QUJzgHlKOy2lYMmVu3-MAST-U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25b6517914cb11bfdf6641f16dc14948f44ac5f6" title="auto-openwebui: I made a bash script to automate running Open WebUI on Linux systems with Ollama and Cloudflare via Docker on AMD &amp;amp; NVIDIA GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/turjid"&gt; /u/turjid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ivansrbulov/auto-openwebui"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kopl1c/autoopenwebui_i_made_a_bash_script_to_automate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kopl1c/autoopenwebui_i_made_a_bash_script_to_automate/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-17T10:12:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kogn7f</id>
    <title>Qwen2.5-VL on Ollama</title>
    <updated>2025-05-17T00:53:04+00:00</updated>
    <author>
      <name>/u/remyxai</name>
      <uri>https://old.reddit.com/user/remyxai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kogn7f/qwen25vl_on_ollama/"&gt; &lt;img alt="Qwen2.5-VL on Ollama" src="https://preview.redd.it/5kfnn5dko81f1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=375f579bd54ef51e3a36eb3df4e352b01b15c084" title="Qwen2.5-VL on Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's never been easier to bring SOTA spatial reasoning to the real world scenes around you, thanks ollama!&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama run hf.co/remyxai/SpaceThinker-Qwen2.5VL-3B:latest&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Read more on SpaceThinker here: &lt;a href="https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B#ollama"&gt;https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B#ollama&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remyxai"&gt; /u/remyxai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5kfnn5dko81f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kogn7f/qwen25vl_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kogn7f/qwen25vl_on_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-17T00:53:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kobsv7</id>
    <title>web, simple and free...Ollama UI</title>
    <updated>2025-05-16T21:03:33+00:00</updated>
    <author>
      <name>/u/andreadev3d</name>
      <uri>https://old.reddit.com/user/andreadev3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kobsv7/web_simple_and_freeollama_ui/"&gt; &lt;img alt="web, simple and free...Ollama UI" src="https://external-preview.redd.it/RVWcNAoIZdNGmSpOZ1tcbyWVWi05e_qCe8GGprGSklQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb70255de5bf15194a456cdf4bc17a4e7767622f" title="web, simple and free...Ollama UI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After my last &lt;a href="https://www.reddit.com/r/ollama/comments/1klp6v8/basic_dark_mode_ui_for_ollama"&gt;post &lt;/a&gt;I choose to improve a bit the chat layout and functionality and following some feedback I added CSV and XLSX support and multi-language support.&lt;/p&gt; &lt;p&gt;of course on Github : &lt;a href="https://github.com/AndreaDev3D/OllamaChat"&gt;https://github.com/AndreaDev3D/OllamaChat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As usual any feedback is appreciated.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7n7eo56ik71f1.png?width=1811&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35c99e78a849db76a76a3f56df219cb43852d6dd"&gt;https://preview.redd.it/7n7eo56ik71f1.png?width=1811&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35c99e78a849db76a76a3f56df219cb43852d6dd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andreadev3d"&gt; /u/andreadev3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kobsv7/web_simple_and_freeollama_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kobsv7/web_simple_and_freeollama_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kobsv7/web_simple_and_freeollama_ui/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-16T21:03:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kox9ew</id>
    <title>Model Recommendations</title>
    <updated>2025-05-17T16:38:51+00:00</updated>
    <author>
      <name>/u/TheMicrosoftMan</name>
      <uri>https://old.reddit.com/user/TheMicrosoftMan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have two main devices that I can use to run local AI models on. The first of those devices is my Surface Pro 11 with a Snapdragon X Elite chip. The other one is an old surface book 2 with an Nvidia 1060 GPU. Which one is better for running AI models with Ollama on? Does the Nvidia 1000-series support Cuda? What are the best models for each device? Is there a way to have the computer remain idle until a request is sent to it so it is not constantly sucking power?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheMicrosoftMan"&gt; /u/TheMicrosoftMan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kox9ew/model_recommendations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kox9ew/model_recommendations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kox9ew/model_recommendations/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-17T16:38:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kork29</id>
    <title>MULTI MODAL VIDEO RAG PROJECT</title>
    <updated>2025-05-17T12:14:39+00:00</updated>
    <author>
      <name>/u/Pez_99</name>
      <uri>https://old.reddit.com/user/Pez_99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to build a multimodal RAG application specifically for videos. The core idea is to leverage the visual content of videos, essentially the individual frames, which are just images, to extract and utilize the information they contain. These frames can present various forms of data such as: ‚Ä¢ On screen text ‚Ä¢ Diagrams and charts ‚Ä¢ Images of objects or scenes&lt;/p&gt; &lt;p&gt;My understanding is that everything in a video can essentially be broken down into two primary formats: text and images. ‚Ä¢ Audio can be converted into text using speech to text models. ‚Ä¢ Frames are images that may contain embedded text or visual context.&lt;/p&gt; &lt;p&gt;So, the system should primarily focus on these two modalities: text and images.&lt;/p&gt; &lt;p&gt;Here‚Äôs what I envision building: 1. Extract and store all textual information present in each frame. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;If a frame lacks text, the system should still be able to understand the visual context. Maybe using a Vision Language Model (VLM).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Maintain contextual continuity across neighboring frames, since the meaning of one frame may heavily rely on the preceding or succeeding frames.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Apply the same principle to audio: segment transcripts based on sentence boundaries and associate them with the relevant sequence of frames (this seems less challenging, as it‚Äôs mostly about syncing text with visuals).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Generate image captions for frames to add an extra layer of context and understanding. (Using CLIP or something)&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;To be honest, I‚Äôm still figuring out the details and would appreciate guidance on how to approach this effectively.&lt;/p&gt; &lt;p&gt;What I want from this Video RAG application:&lt;/p&gt; &lt;p&gt;I want the system to be able to answer user queries about a video, even if the video contains ambiguous or sparse information. For example:&lt;/p&gt; &lt;p&gt;‚Ä¢ Provide a summary of the quarterly sales chart. ‚Ä¢ What were the main points discussed by the trainer in this video ‚Ä¢ List all the policies mentioned throughout the video.&lt;/p&gt; &lt;p&gt;Note: I‚Äôm not trying to build the kind of advanced video RAG that understands a video purely from visual context alone, such as a silent video of someone tying a tie, where the system infers the steps without any textual or audio cues. That‚Äôs beyond the current scope.&lt;/p&gt; &lt;p&gt;The three main scenarios I want to address: 1. Videos with both transcription and audio 2. Videos with visuals and audio, but no pre existing transcription (We can use models like Whisper to transcribe the audio) 3. Videos with no transcription or audio (These could have background music or be completely silent, requiring visual only understanding)&lt;/p&gt; &lt;p&gt;Please help me refine this idea further or guide me on the right tools, architectures, and strategies to implement such a system effectively. Any other approach or anything that I missing. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pez_99"&gt; /u/Pez_99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kork29/multi_modal_video_rag_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kork29/multi_modal_video_rag_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kork29/multi_modal_video_rag_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-17T12:14:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1kor7be</id>
    <title>macOS Application for Ollama - macLlama</title>
    <updated>2025-05-17T11:55:38+00:00</updated>
    <author>
      <name>/u/gogimandoo</name>
      <uri>https://old.reddit.com/user/gogimandoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kor7be/macos_application_for_ollama_macllama/"&gt; &lt;img alt="macOS Application for Ollama - macLlama" src="https://preview.redd.it/77q61hsxzb1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b28e30f101d196edd3641446170d2d020bfdcda" title="macOS Application for Ollama - macLlama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;macLlama is a native macOS application providing a graphical user interface for the Ollama command-line tool. This application facilitates model management and interaction with local language models.&lt;/p&gt; &lt;p&gt;Features include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A dedicated interface for interacting with language models.&lt;/li&gt; &lt;li&gt;Open-source development and availability.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The application is developed using SwiftUI.&lt;/p&gt; &lt;p&gt;Release information are available at: &lt;a href="https://github.com/hellotunamayo/macLlama/releases"&gt;https://github.com/hellotunamayo/macLlama/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repository: &lt;a href="https://github.com/hellotunamayo/macLlama"&gt;https://github.com/hellotunamayo/macLlama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The application is in early development, and feedback is greatly appreciated to guide future enhancements. Please submit suggestions and bug reports via the GitHub repository.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gogimandoo"&gt; /u/gogimandoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/77q61hsxzb1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kor7be/macos_application_for_ollama_macllama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kor7be/macos_application_for_ollama_macllama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-17T11:55:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kov3n4</id>
    <title>Photoshop using Local Computer Use agents.</title>
    <updated>2025-05-17T15:04:21+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kov3n4/photoshop_using_local_computer_use_agents/"&gt; &lt;img alt="Photoshop using Local Computer Use agents." src="https://external-preview.redd.it/YnhuZThpNm14YzFmMRXG3T7XldLhbQ1-zZgDOchlvuRH1Qq3ebSvcq1i84vf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70e12542867145b258ac69c65700c043640b8337" title="Photoshop using Local Computer Use agents." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Photoshop using c/ua.&lt;/p&gt; &lt;p&gt;No code. Just a user prompt, picking models and a Docker, and the right agent loop.&lt;/p&gt; &lt;p&gt;A glimpse at the more managed experience c/ua is building to lower the barrier for casual vibe-coders.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/up7g8ydmxc1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kov3n4/photoshop_using_local_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kov3n4/photoshop_using_local_computer_use_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-17T15:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1kp2akd</id>
    <title>Offline real-time voice conversations with custom chatbots</title>
    <updated>2025-05-17T20:19:22+00:00</updated>
    <author>
      <name>/u/w00fl35</name>
      <uri>https://old.reddit.com/user/w00fl35</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kp2akd/offline_realtime_voice_conversations_with_custom/"&gt; &lt;img alt="Offline real-time voice conversations with custom chatbots" src="https://external-preview.redd.it/bW5pZHczd3RoZTFmMatKx9EjWmqye1H1Sfl0oBQ-Ipj7s6y-PHQT0KQMotHc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88790253488a75fc0d85a1a296293c7ac9f639cd" title="Offline real-time voice conversations with custom chatbots" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w00fl35"&gt; /u/w00fl35 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/q36s4h6ohe1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kp2akd/offline_realtime_voice_conversations_with_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kp2akd/offline_realtime_voice_conversations_with_custom/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-17T20:19:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kp8t1v</id>
    <title>I built an AI-powered Food &amp; Nutrition Tracker that analyzes meals from photos! Planning to open-source it</title>
    <updated>2025-05-18T01:42:57+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kp8t1v/i_built_an_aipowered_food_nutrition_tracker_that/"&gt; &lt;img alt="I built an AI-powered Food &amp;amp; Nutrition Tracker that analyzes meals from photos! Planning to open-source it" src="https://external-preview.redd.it/a3hkZzZ2MzczZzFmMevpjUkJAH29ctL9GGNTRuXbe-uU1nbp5uR8WvjIiEr4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61e785d23d7de38d4e10e32eae081497ca1c2912" title="I built an AI-powered Food &amp;amp; Nutrition Tracker that analyzes meals from photos! Planning to open-source it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;/p&gt; &lt;p&gt;Been working on this Diet &amp;amp; Nutrition tracking app and wanted to share a quick demo of its current state. The core idea is to make food logging as painless as possible.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features so far:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI Meal Analysis:&lt;/strong&gt; You can upload an image of your food, and the AI tries to identify it and provide nutritional estimates (calories, protein, carbs, fat).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Manual Logging &amp;amp; Edits:&lt;/strong&gt; Of course, you can add/edit entries manually.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Daily Nutrition Overview:&lt;/strong&gt; Tracks calories against goals, macro distribution.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Water Intake:&lt;/strong&gt; Simple water tracking.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Weekly Stats &amp;amp; Streaks:&lt;/strong&gt; To keep motivation up.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm really excited about the AI integration. It's still a work in progress, but the goal is to streamline the most tedious part of tracking.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code Status:&lt;/strong&gt; I'm planning to clean up the codebase and open-source it on GitHub in the near future! For now, if you're interested in other AI/LLM related projects and learning resources I've put together, you can check out my &amp;quot;LLM-Learn-PK&amp;quot; repo:&lt;br /&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2FPavankunchala%2FLLM-Learn-PK"&gt;https://github.com/Pavankunchala/LLM-Learn-PK&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; On a related note, I'm actively looking for new opportunities in Computer Vision and LLM engineering. If your team is hiring or you know of any openings, I'd be grateful if you'd reach out!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Email:&lt;/strong&gt; [&lt;a href="mailto:pavankunchalaofficial@gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;](mailto:&lt;a href="mailto:pavankunchalaofficial@gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My other projects on GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Resume:&lt;/strong&gt; &lt;a href="https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view"&gt;https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/tcdgsv373g1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kp8t1v/i_built_an_aipowered_food_nutrition_tracker_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kp8t1v/i_built_an_aipowered_food_nutrition_tracker_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-18T01:42:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpi3vz</id>
    <title>Sentiment Analysis - hit and miss when it comes to results</title>
    <updated>2025-05-18T11:47:58+00:00</updated>
    <author>
      <name>/u/Banana5kin</name>
      <uri>https://old.reddit.com/user/Banana5kin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else using (or trying to use) Ollama to perform Sentiment Analysis?&lt;/p&gt; &lt;p&gt;I thought I'd give it a test drive, but results are inconsistent, failure to run through the dataset, incorrect analysis and 100% correct analysis all within a 1/2 dozen runs. To eliminate any potential issues with the text for analysis I ran it through a n8n code node to remove an punctuation, uppercase to lower &amp;amp; remove any white space. I have used Gemma3:1b which hits all 3 inconsistencies (more often failing) and ALIENTELLIGENCE/sentimentanalyzer which produces 100% results when it runs without error.&lt;/p&gt; &lt;p&gt;For clarity ollama is being called by the n8n sentiment analysis node using the standard system prompt as supplied by the node.&lt;/p&gt; &lt;p&gt;*edit - openai and anthropic both work flawlessly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Banana5kin"&gt; /u/Banana5kin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpi3vz/sentiment_analysis_hit_and_miss_when_it_comes_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpi3vz/sentiment_analysis_hit_and_miss_when_it_comes_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kpi3vz/sentiment_analysis_hit_and_miss_when_it_comes_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-18T11:47:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpp1jd</id>
    <title>OLLAMA_NEW_ENGINE</title>
    <updated>2025-05-18T17:14:04+00:00</updated>
    <author>
      <name>/u/planetf1a</name>
      <uri>https://old.reddit.com/user/planetf1a</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This seems initially targetted at running new visual models.&lt;/p&gt; &lt;p&gt;Is there feature parity for other model types vs llamacp? For example running models like granite3.3, qwen3 ~8b on a mac m1 ? Any info on relative performance?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/planetf1a"&gt; /u/planetf1a &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpp1jd/ollama_new_engine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpp1jd/ollama_new_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kpp1jd/ollama_new_engine/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-18T17:14:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpo4aw</id>
    <title>Contribution to ollama-python: decorators, helper functions and simplified creation tool</title>
    <updated>2025-05-18T16:34:31+00:00</updated>
    <author>
      <name>/u/chavomodder</name>
      <uri>https://old.reddit.com/user/chavomodder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1kpo4aw/contribution_to_ollamapython_decorators_helper/"&gt; &lt;img alt="Contribution to ollama-python: decorators, helper functions and simplified creation tool" src="https://external-preview.redd.it/0WHEreexf2DJMw78A-6XfudwOUYNJRPPM2H2EZ2R2b8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6bbde65c5b5324258d0882eae80d38a008f28e7e" title="Contribution to ollama-python: decorators, helper functions and simplified creation tool" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! (This post was written in Portuguese)&lt;/p&gt; &lt;p&gt;I made a commit to ollama-python with the aim of making it easier to create and use custom tools. You can now use simple decorators to register functions:&lt;/p&gt; &lt;p&gt;@ollama_tool ‚Äì for synchronous functions&lt;/p&gt; &lt;p&gt;@ollama_async_tool ‚Äì for asynchronous functions&lt;/p&gt; &lt;p&gt;I also added auxiliary functions to make organizing and using the tools easier:&lt;/p&gt; &lt;p&gt;get_tools() ‚Äì returns all registered tools&lt;/p&gt; &lt;p&gt;get_tools_name() ‚Äì dictionary with the name of the tools and their respective functions&lt;/p&gt; &lt;p&gt;get_name_async_tools() ‚Äì list of asynchronous tool names&lt;/p&gt; &lt;p&gt;Additionally, I created a new function called create_function_tool, which allows you to create tools in a similar way to manual, but without worrying about the JSON structure. Just pass the Python parameters like: (tool_name, description, parameter_list, required_parameters)&lt;/p&gt; &lt;p&gt;Now, to work with the tools, the flow is very simple:&lt;/p&gt; &lt;h1&gt;Returns the functions that are with the decorators&lt;/h1&gt; &lt;p&gt;tools = get_tools()&lt;/p&gt; &lt;h1&gt;dictionary with all functions using decorators (as already used)&lt;/h1&gt; &lt;p&gt;available_functions = get_tools_name() &lt;/p&gt; &lt;h1&gt;returns the names of asynchronous functions&lt;/h1&gt; &lt;p&gt;async_available_functions = get_name_async_tools() &lt;/p&gt; &lt;p&gt;And in the code, you can use an if to check if the function is asynchronous (based on the list of async_available_functions) and use await or asyncio.run() as necessary.&lt;/p&gt; &lt;p&gt;These changes help reduce the boilerplate and make development with the library more practical.&lt;/p&gt; &lt;p&gt;Anyone who wants to take a look or suggest something, follow:&lt;/p&gt; &lt;p&gt;Commit link: [ &lt;a href="https://github.com/ollama/ollama-python/pull/516"&gt;https://github.com/ollama/ollama-python/pull/516&lt;/a&gt; ]&lt;/p&gt; &lt;p&gt;My repository link:&lt;/p&gt; &lt;p&gt;[ &lt;a href="https://github.com/caua1503/ollama-python/tree/main"&gt;https://github.com/caua1503/ollama-python/tree/main&lt;/a&gt; ]&lt;/p&gt; &lt;p&gt;Observation:&lt;/p&gt; &lt;p&gt;I was already using this in my real project and decided to share it. &lt;/p&gt; &lt;p&gt;I'm an experienced Python dev, but this is my first time working with decorators and I decided to do this in the simplest way possible, I hope to help the community, I know defining global lists, maybe it's not the best way to do this but I haven't found another way&lt;/p&gt; &lt;p&gt;In addition to langchain being complicated and changing everything with each update, I couldn't use it with ollama models, so I went to the Ollama Python library&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chavomodder"&gt; /u/chavomodder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ollama/ollama-python/pull/516"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1kpo4aw/contribution_to_ollamapython_decorators_helper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1kpo4aw/contribution_to_ollamapython_decorators_helper/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-05-18T16:34:31+00:00</published>
  </entry>
</feed>
