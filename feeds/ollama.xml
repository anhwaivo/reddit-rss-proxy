<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-05T09:24:06+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ih4bjv</id>
    <title>Local vs. Cloud? A Simple Diagram to Help You Choose an LLM</title>
    <updated>2025-02-04T00:08:25+00:00</updated>
    <author>
      <name>/u/Fun-Assignment4054</name>
      <uri>https://old.reddit.com/user/Fun-Assignment4054</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ih4bjv/local_vs_cloud_a_simple_diagram_to_help_you/"&gt; &lt;img alt="Local vs. Cloud? A Simple Diagram to Help You Choose an LLM" src="https://b.thumbs.redditmedia.com/SdQHBpXNjLeoDqok6HP4ruKNgH2Pa31TiMYMgGCnKts.jpg" title="Local vs. Cloud? A Simple Diagram to Help You Choose an LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/91w79sw7k0he1.png?width=2862&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d4f9b28206c2d51d503c9a0a6a340c8f3a181962"&gt;Diagram&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLM/comments/1ih46wf/local_vs_cloud_a_simple_diagram_to_help_you/"&gt;Originally posted&lt;/a&gt; in on &lt;a href="/r/LocalLLM"&gt;r/LocalLLM&lt;/a&gt;.&lt;br /&gt; ---- &lt;/p&gt; &lt;p&gt;Hi, I’m new to local LLMs and have been learning through Reddit and YouTube. I made a diagram to show when to use on‑device models vs. cloud‑based models. While building it, I added a branch labeled “on‑device AI determines,” thinking about an ideal setup. &lt;strong&gt;Is it possible to create a programmatic way to handle that?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In this diagram, I assume two things: (1) the user knows how to set up local models, and (2) they have already paid for (a) cloud‑based model(s). I hope this visual helps others out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Assignment4054"&gt; /u/Fun-Assignment4054 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih4bjv/local_vs_cloud_a_simple_diagram_to_help_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih4bjv/local_vs_cloud_a_simple_diagram_to_help_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ih4bjv/local_vs_cloud_a_simple_diagram_to_help_you/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T00:08:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ighr82</id>
    <title>Customizable GUI for ollama (less than 1MB)</title>
    <updated>2025-02-03T05:17:25+00:00</updated>
    <author>
      <name>/u/A8LR</name>
      <uri>https://old.reddit.com/user/A8LR</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ighr82/customizable_gui_for_ollama_less_than_1mb/"&gt; &lt;img alt="Customizable GUI for ollama (less than 1MB)" src="https://preview.redd.it/vyq6efv0zuge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fdbf99e396bc2b32ada4b4be7b12700a7f25056" title="Customizable GUI for ollama (less than 1MB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A barebones chat interface for Ollama in 4 files; HTML, CSS, JS and Python.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/qusaismael/localllm"&gt;https://github.com/qusaismael/localllm&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Why post: seeing people struggle with over-engineered examples. MIT licensed = modify freely. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A8LR"&gt; /u/A8LR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vyq6efv0zuge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ighr82/customizable_gui_for_ollama_less_than_1mb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ighr82/customizable_gui_for_ollama_less_than_1mb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T05:17:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihq1pr</id>
    <title>Is human consciousness an illusion reproducible by AI?</title>
    <updated>2025-02-04T19:26:22+00:00</updated>
    <author>
      <name>/u/MoreIndependent5967</name>
      <uri>https://old.reddit.com/user/MoreIndependent5967</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If human consciousness is just an emergent product of the complex interactions between our neurons, then it is logical to think that one day, AI could develop a form of consciousness. Consciousness could be seen as a functional illusion, created by memory, reflection, and constant adjustments between neural signals, initially developed for survival. If AI reaches a point where it can adjust its own weights, learn from its experiences and set its own goals, it could naturally pass a critical threshold.&lt;/p&gt; &lt;p&gt;This does not necessarily mean that she will feel emotions or qualia (subjective feeling), but she could have a functional consciousness: an ability to represent herself, to adapt to her environment, and to anticipate consequences. of his actions. It would be a form of pragmatic consciousness, useful for one's own autonomous development.&lt;/p&gt; &lt;p&gt;But when would we consider that an AI is truly conscious? Is it when she simulates complex human behaviors or when she begins to set her own priorities without supervision? The line between simulation and reality could become blurred much faster than we think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoreIndependent5967"&gt; /u/MoreIndependent5967 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihq1pr/is_human_consciousness_an_illusion_reproducible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihq1pr/is_human_consciousness_an_illusion_reproducible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihq1pr/is_human_consciousness_an_illusion_reproducible/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T19:26:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihahfr</id>
    <title>How to save the llm state after binding it with tools?</title>
    <updated>2025-02-04T05:23:30+00:00</updated>
    <author>
      <name>/u/Lower-Substance3655</name>
      <uri>https://old.reddit.com/user/Lower-Substance3655</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’m working with an LLM (specifically using Ollama), and I’ve successfully customized it by binding some tools to the model using the .bindtools() function. Now I want to save the state of this model along with the tool bindings so that I can run it later using the command:&lt;/p&gt; &lt;p&gt;ollama run modelname&lt;/p&gt; &lt;p&gt;The idea is to avoid re-binding the tools every time I need to use the model and just save the whole setup once, but I haven’t been able to figure out how to persist the LLM with the tools attached.&lt;/p&gt; &lt;p&gt;Does anyone know how to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Save the model and the tools as one entity in Ollama (or another environment)?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Run it later with the command ollama run modelname without needing to reconfigure everything?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any help or pointers in the right direction would be appreciated!&lt;/p&gt; &lt;p&gt;Thanks in advance!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lower-Substance3655"&gt; /u/Lower-Substance3655 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihahfr/how_to_save_the_llm_state_after_binding_it_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihahfr/how_to_save_the_llm_state_after_binding_it_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihahfr/how_to_save_the_llm_state_after_binding_it_with/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T05:23:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihfsji</id>
    <title>Error parsing Excel file with PyMuPDFPro</title>
    <updated>2025-02-04T11:43:33+00:00</updated>
    <author>
      <name>/u/Single_Teacher_5926</name>
      <uri>https://old.reddit.com/user/Single_Teacher_5926</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone. I have been using Pymupdf4llm for Japanese resume pdfs extraction to push it to llm along with prompt. In order to get structured resume schema&lt;/p&gt; &lt;p&gt;CODE - import pymupdf4llm def extract_text_and_tables(pdf_path): md_text = pymupdf4llm.to_markdown(pdf_path) return md_text&lt;/p&gt; &lt;p&gt;But now i want to process excel files using PyMuPDFPro. &lt;/p&gt; &lt;p&gt;CODE - import pymupdf.pro pymupdf.pro.unlock(api_key) #i am putting my key here&lt;/p&gt; &lt;p&gt;doc = pymupdf.open(&amp;quot;/data/JP resume format 011.xlsx&amp;quot;)&lt;/p&gt; &lt;p&gt;for page in doc: text = page.get_text(&amp;quot;text&amp;quot;) print(text)&lt;/p&gt; &lt;p&gt;I am getting the error mentioned below - &lt;/p&gt; &lt;p&gt;File &amp;quot;/Bluparrot/test1/knowledge/test1.py&amp;quot;, line 1, in &amp;lt;module&amp;gt; import pymupdf.pro File &amp;quot;/Bluparrot/test1/myenv/lib/python3.12/site-packages/pymupdf/pro.py&amp;quot;, line 10, in &amp;lt;module&amp;gt; from . import _pro ImportError: libmupdf.so.25.1: cannot open shared object file: No such file or directory&lt;/p&gt; &lt;p&gt;What am i doing wrong ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Single_Teacher_5926"&gt; /u/Single_Teacher_5926 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihfsji/error_parsing_excel_file_with_pymupdfpro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihfsji/error_parsing_excel_file_with_pymupdfpro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihfsji/error_parsing_excel_file_with_pymupdfpro/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T11:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ih9w5x</id>
    <title>Threadripper CPU Testing</title>
    <updated>2025-02-04T04:49:36+00:00</updated>
    <author>
      <name>/u/BuffMcBigHuge</name>
      <uri>https://old.reddit.com/user/BuffMcBigHuge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ih9w5x/threadripper_cpu_testing/"&gt; &lt;img alt="Threadripper CPU Testing" src="https://b.thumbs.redditmedia.com/9l5wy5MbnsTyEe3u6Sfvh4UBwqXoF42_7TFF9jZuFro.jpg" title="Threadripper CPU Testing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ojb33b52y1he1.png?width=888&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b60337a4431b2394c7882236fae787bb7e806d31"&gt;https://preview.redd.it/ojb33b52y1he1.png?width=888&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b60337a4431b2394c7882236fae787bb7e806d31&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been testing with a Threadripper 3960x and 256gb of RAM. The issue I'm experiencing is that when the inference is completed, half of my CPU cores go in overdrive doing nothing. I feel like it's a bug with Ollama. I will test further.&lt;/p&gt; &lt;p&gt;Here are some results:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9pwodtxny1he1.png?width=778&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=363f6fede2a1cbbd9dfd5c78efda3e32a925be0d"&gt;https://preview.redd.it/9pwodtxny1he1.png?width=778&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=363f6fede2a1cbbd9dfd5c78efda3e32a925be0d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Getting around 1.25 tokens per second with 2.22bit 671b, RAM at 3200mhz. My system is unstable at 3600mhz 256gb.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BuffMcBigHuge"&gt; /u/BuffMcBigHuge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih9w5x/threadripper_cpu_testing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ih9w5x/threadripper_cpu_testing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ih9w5x/threadripper_cpu_testing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T04:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1igrtd7</id>
    <title>LLM Powered Map</title>
    <updated>2025-02-03T15:36:26+00:00</updated>
    <author>
      <name>/u/ranoutofusernames__</name>
      <uri>https://old.reddit.com/user/ranoutofusernames__</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1igrtd7/llm_powered_map/"&gt; &lt;img alt="LLM Powered Map" src="https://preview.redd.it/imcksipg1yge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ffd22c99c31c7a7c90376fee3c2d82e64d1c2451" title="LLM Powered Map" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open source, LLM powered discovery/exploration map I made a month ago. Runs locally or using cloud models. With a big enough model, it’s pretty much like having an offline, global map. Cheers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/space0blaster/godview"&gt;Repo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ranoutofusernames__"&gt; /u/ranoutofusernames__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/imcksipg1yge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1igrtd7/llm_powered_map/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1igrtd7/llm_powered_map/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T15:36:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ignq9z</id>
    <title>Is there a way to "train" an open-source LLM to do one type of task really well?</title>
    <updated>2025-02-03T12:15:45+00:00</updated>
    <author>
      <name>/u/ArtPerToken</name>
      <uri>https://old.reddit.com/user/ArtPerToken</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, forgive me if its a silly question, but is there a way to train or modify an existing LLM (i guess an open source one) to do one type of tasks really well?&lt;/p&gt; &lt;p&gt;For example if I have 50 poems I wrote in my own unique style, how can I &amp;quot;feed&amp;quot; it to the LLM and then ask it to generate a new poem about a new subject in the same style?&lt;/p&gt; &lt;p&gt;Would appreciate any thoughts on the best way to go about this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArtPerToken"&gt; /u/ArtPerToken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ignq9z/is_there_a_way_to_train_an_opensource_llm_to_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ignq9z/is_there_a_way_to_train_an_opensource_llm_to_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ignq9z/is_there_a_way_to_train_an_opensource_llm_to_do/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-03T12:15:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihll6s</id>
    <title>Enhanced Privacy with Ollama and others</title>
    <updated>2025-02-04T16:26:08+00:00</updated>
    <author>
      <name>/u/Key_Opening_3243</name>
      <uri>https://old.reddit.com/user/Key_Opening_3243</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’m excited to announce my Open Source tool focused on privacy during inference with AI models locally via Ollama or generic obfuscation for any case.&lt;/p&gt; &lt;p&gt;&lt;a href="https://maltese.johan.chat/"&gt;https://maltese.johan.chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I invite you all to contribute to this idea, which, although quite simple, can be highly effective in certain cases.&lt;br /&gt; Feel free to reach out to discuss the idea and how to evolve it.&lt;/p&gt; &lt;p&gt;Best regards, Johan.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Key_Opening_3243"&gt; /u/Key_Opening_3243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihll6s/enhanced_privacy_with_ollama_and_others/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihll6s/enhanced_privacy_with_ollama_and_others/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihll6s/enhanced_privacy_with_ollama_and_others/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T16:26:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihf1ou</id>
    <title>Ollama Flashcard creation</title>
    <updated>2025-02-04T10:52:15+00:00</updated>
    <author>
      <name>/u/Key_King_1216</name>
      <uri>https://old.reddit.com/user/Key_King_1216</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not a programmer. I need help creating flashcards in a csv file format and exporting it to anki using a reliable language model. Can anyone explain to me how I would do this. After downloading ollama and downloading the model whether it be deepseek-r1, or llama3, what do I do after?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Key_King_1216"&gt; /u/Key_King_1216 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihf1ou/ollama_flashcard_creation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihf1ou/ollama_flashcard_creation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihf1ou/ollama_flashcard_creation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T10:52:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihu2by</id>
    <title>Titan XP vs 2 Tesla M40 12GB Cards</title>
    <updated>2025-02-04T22:10:32+00:00</updated>
    <author>
      <name>/u/MajorJakePennington</name>
      <uri>https://old.reddit.com/user/MajorJakePennington</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking at building a box for running local DeepSeek models, but am having difficulty finding performance metrics for the Titan XP and the Tesla M40. For 1/2 the price of a Titan XP I can buy 2 Tesla M40 cards and have 24GB of VRAM, but is the performance there for 8b+ models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MajorJakePennington"&gt; /u/MajorJakePennington &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihu2by/titan_xp_vs_2_tesla_m40_12gb_cards/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihu2by/titan_xp_vs_2_tesla_m40_12gb_cards/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihu2by/titan_xp_vs_2_tesla_m40_12gb_cards/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T22:10:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihjfy9</id>
    <title>Slow performance on K8S</title>
    <updated>2025-02-04T14:54:29+00:00</updated>
    <author>
      <name>/u/geeky217</name>
      <uri>https://old.reddit.com/user/geeky217</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m trying to run ollama with deepseek-r1 7b on CPU only inside K8S. I’m using the official helm chart inside RedHat Openshift 4.12.2 with cephRBD storage. This is all on a vm (it’s a single node openshift dev box) with 24 cores of 6148 gold Xeon cpu and 96GB. The ollama deployment has 16 cores and 16GB set as reserved. Now the issue is that it runs like a dog compared to ollama on a basic Ubuntu vm on the same esx host which has half the resources (8&amp;amp;8). The only difference is one is containerised the other just a vm. I’m at a loss why there is such a performance difference. Both ollama instances run off nvme local storage so have plenty of bandwidth and low latency. Anyone got any insights here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geeky217"&gt; /u/geeky217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihjfy9/slow_performance_on_k8s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihjfy9/slow_performance_on_k8s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihjfy9/slow_performance_on_k8s/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T14:54:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihzvrd</id>
    <title>Deepseek benchmarks</title>
    <updated>2025-02-05T02:39:54+00:00</updated>
    <author>
      <name>/u/darkgamer_nw</name>
      <uri>https://old.reddit.com/user/darkgamer_nw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Where can I find benchmarks comparing the performance of deepseek-r1 8b with llama with a similar number of parameters?&lt;/p&gt; &lt;p&gt;I cannot find a page with benchmarks on recent models&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darkgamer_nw"&gt; /u/darkgamer_nw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihzvrd/deepseek_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihzvrd/deepseek_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihzvrd/deepseek_benchmarks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T02:39:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihqub9</id>
    <title>I want to get into Local LLMs for coding, home assistant, and maybe a little conversation. Low token/s is fine to see if I even like it. Which hardware that I have listed could do it? Or do I need a GPU solely for this, even to start off?</title>
    <updated>2025-02-04T19:59:03+00:00</updated>
    <author>
      <name>/u/bigrjsuto</name>
      <uri>https://old.reddit.com/user/bigrjsuto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have the following hardware:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Personal PC (LInux Mint 21) &lt;ul&gt; &lt;li&gt;Ryzen 5800X, 64GB DDR4, 3060 12GB, 1TB NVMe + 2TB NVMe&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Server PC (Proxmox) &lt;ul&gt; &lt;li&gt;Intel 12500T, 128GB DDR4, A4000 16GB (passthrough to Windows 11 VM for Solidworks), 128GB NVMe (boot) + 1TB NVMe (VMs/LXCs), 2x 18TB HDDs&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Gaming/HTPC (Windows) &lt;ul&gt; &lt;li&gt;Intel 10600K, 32GB DDR4, RX 590 8GB, 128GB + 1TB SSD&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;MiniPCs &lt;ul&gt; &lt;li&gt;20x Datto ALTO 3 V2 &lt;ul&gt; &lt;li&gt;Celeron 3865U, 2-16GB DDR4 (Can configure as needed)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;2x Datto S3X2 Dual-NIC &lt;ul&gt; &lt;li&gt;Intel i3-7100U, 2-16GB DDR4&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;4x Optiplex 3040 &lt;ul&gt; &lt;li&gt;Intel i3-6100T, 2-8GB DDR3&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Intel NUC &lt;ul&gt; &lt;li&gt;Intel i7-7567U, 2-16GB DDR4&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Other Hardware &lt;ul&gt; &lt;li&gt;RX 560 4GB LP&lt;/li&gt; &lt;li&gt;Various extra HDDs 160GB - 8TB&lt;/li&gt; &lt;li&gt;Various extra NVMe 32GB - 500GB&lt;/li&gt; &lt;li&gt;A few extra network switches (if needed for clustering)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wanted to add the RX 560 to the server, but after some research, everything I've seen says that 4GB is too little VRAM for even slow output. That's the case, right?&lt;/p&gt; &lt;p&gt;How about a Coral TPU? Or multiple? Each of those Datto ALTO MiniPCs have a A+E keyed m.2 slot, where I could place them and cluster them together.&lt;/p&gt; &lt;p&gt;Could I just run it on my PC? Would the 3060 be good enough to get some output?&lt;/p&gt; &lt;p&gt;I know there's the A4000, but I need it for CAD work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigrjsuto"&gt; /u/bigrjsuto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihqub9/i_want_to_get_into_local_llms_for_coding_home/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihqub9/i_want_to_get_into_local_llms_for_coding_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihqub9/i_want_to_get_into_local_llms_for_coding_home/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T19:59:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihu6s2</id>
    <title>Need help with training</title>
    <updated>2025-02-04T22:15:47+00:00</updated>
    <author>
      <name>/u/Kind_Ad_2866</name>
      <uri>https://old.reddit.com/user/Kind_Ad_2866</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just entered the AI race not too long ago and I have some concepts to wrap my head around &lt;/p&gt; &lt;p&gt;1- what’s the difference between training the model via chat and training the model via unsloth&lt;/p&gt; &lt;p&gt;2- what’s the difference between datasets fed to unsloth and standard RAG where you upload bunch of files then you can ask the model about them? I’m asking because I have pdf text files (books, novels, etc) and I want to chat with the models about it or ask the AI to give me a decision based on data in these files. &lt;/p&gt; &lt;p&gt;3- if unsloth is the way to go, how would I go about creating a dataset for a novel? I have seen datasets where they mention the characters, but I don’t understand how the model would piece the story just buy using the character description!&lt;/p&gt; &lt;p&gt;here is an example on the dataset I mean: &lt;a href="https://huggingface.co/datasets/xywang1/OpenCharacter?row=1"&gt;https://huggingface.co/datasets/xywang1/OpenCharacter?row=1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kind_Ad_2866"&gt; /u/Kind_Ad_2866 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihu6s2/need_help_with_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihu6s2/need_help_with_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihu6s2/need_help_with_training/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T22:15:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii3nvf</id>
    <title>Deepseek Ollama download issue</title>
    <updated>2025-02-05T06:13:17+00:00</updated>
    <author>
      <name>/u/RecordSimilar2356</name>
      <uri>https://old.reddit.com/user/RecordSimilar2356</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am downloading deepseek-r1:7b using Ollama in my windows 11, but the downloading keep rolling back even if the internet speed is fine. The downloading reaches 45% let's say, then after sometime it back to 30% or even less. The downloading never completes! Any help or guide?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RecordSimilar2356"&gt; /u/RecordSimilar2356 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii3nvf/deepseek_ollama_download_issue/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii3nvf/deepseek_ollama_download_issue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii3nvf/deepseek_ollama_download_issue/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T06:13:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii4wb3</id>
    <title>As a noob, which language model could you suggest for me?</title>
    <updated>2025-02-05T07:39:53+00:00</updated>
    <author>
      <name>/u/Live-Pause-6543</name>
      <uri>https://old.reddit.com/user/Live-Pause-6543</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a macbook pro with m1 pro chip. And every model writes the requirements based on vram. Therefore, I have no Idea which model I can run without any issues. &lt;/p&gt; &lt;p&gt;For now, I’ve only tried the distilled version of deepseek r1 with 7b. It worked really fast but the results were awful, although my expectations were low bc I know that these models performs worse than the website version of deepseek. &lt;/p&gt; &lt;p&gt;I will mostly use this for coding, but the languages I use changes often. (For school, I use Java, at a student internship Tailwind and vue and for personal coding, I try to explore game development, so c# c++ either. &lt;/p&gt; &lt;p&gt;Basically for general coding and sometimes for writing essays for school and so on. &lt;/p&gt; &lt;p&gt;For writing essays etc. I did not use any local model, I know the distilled version of r1 performs well just in coding and math. So, for other models, I am open to use. &lt;/p&gt; &lt;p&gt;With that being said, can you suggest me any model to discover the local llms? &lt;/p&gt; &lt;p&gt;Thx for your answers :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Live-Pause-6543"&gt; /u/Live-Pause-6543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii4wb3/as_a_noob_which_language_model_could_you_suggest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii4wb3/as_a_noob_which_language_model_could_you_suggest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii4wb3/as_a_noob_which_language_model_could_you_suggest/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T07:39:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii4zg6</id>
    <title>500: Ollama: 500, message='Internal Server Error', url='http://host.docker.internal:11434/api/chat '</title>
    <updated>2025-02-05T07:46:19+00:00</updated>
    <author>
      <name>/u/PCOwner12</name>
      <uri>https://old.reddit.com/user/PCOwner12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm relatively new to the local models and AI. I just ran my 1st docker with Ollama Win 11 and it was working perfectly, but now I consitently get this error. Does anyone know how to resolve it?&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PCOwner12"&gt; /u/PCOwner12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii4zg6/500_ollama_500_messageinternal_server_error/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii4zg6/500_ollama_500_messageinternal_server_error/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii4zg6/500_ollama_500_messageinternal_server_error/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T07:46:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii5ky5</id>
    <title>Facing error while installing deepseek 1.5b</title>
    <updated>2025-02-05T08:31:00+00:00</updated>
    <author>
      <name>/u/The_Arcane19</name>
      <uri>https://old.reddit.com/user/The_Arcane19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ii5ky5/facing_error_while_installing_deepseek_15b/"&gt; &lt;img alt="Facing error while installing deepseek 1.5b" src="https://preview.redd.it/l5hpg8997ahe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=301ecaa1fa019c3b33920f10364ee9848f6749b7" title="Facing error while installing deepseek 1.5b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The_Arcane19"&gt; /u/The_Arcane19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l5hpg8997ahe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii5ky5/facing_error_while_installing_deepseek_15b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii5ky5/facing_error_while_installing_deepseek_15b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T08:31:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihnhpl</id>
    <title>Whats the best open source model for video generation?</title>
    <updated>2025-02-04T17:43:10+00:00</updated>
    <author>
      <name>/u/gl2101</name>
      <uri>https://old.reddit.com/user/gl2101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im currently running a 3060 setup but planning to upgrade to a more powerful GPU. &lt;/p&gt; &lt;p&gt;My main goal is to build ai videos but I don’t know where to start. &lt;/p&gt; &lt;p&gt;Any recommendations are greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gl2101"&gt; /u/gl2101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihnhpl/whats_the_best_open_source_model_for_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihnhpl/whats_the_best_open_source_model_for_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihnhpl/whats_the_best_open_source_model_for_video/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T17:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1iht2tf</id>
    <title>How To Setup DeepSeek-R1 LLM AI ChatBot Using Ollama On An Ubuntu Linux GPU Cloud Server (VPS)</title>
    <updated>2025-02-04T21:29:42+00:00</updated>
    <author>
      <name>/u/websplaining</name>
      <uri>https://old.reddit.com/user/websplaining</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1iht2tf/how_to_setup_deepseekr1_llm_ai_chatbot_using/"&gt; &lt;img alt="How To Setup DeepSeek-R1 LLM AI ChatBot Using Ollama On An Ubuntu Linux GPU Cloud Server (VPS)" src="https://external-preview.redd.it/uNOHV2Maw2LzcoSfFMG6sju1JNogiBy71eR2QYEqjOQ.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbe5a0f4cb1ca54f001e5f84b38b48e405eb752e" title="How To Setup DeepSeek-R1 LLM AI ChatBot Using Ollama On An Ubuntu Linux GPU Cloud Server (VPS)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/websplaining"&gt; /u/websplaining &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/S_JEkuE9EyU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iht2tf/how_to_setup_deepseekr1_llm_ai_chatbot_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iht2tf/how_to_setup_deepseekr1_llm_ai_chatbot_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T21:29:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihibp9</id>
    <title>Is Wikipedia RAG possible entirely locally with a gaming machine?</title>
    <updated>2025-02-04T14:02:46+00:00</updated>
    <author>
      <name>/u/trichofobia</name>
      <uri>https://old.reddit.com/user/trichofobia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y'all, I'm super super new, so I'm sorry if this is a stupid question, but I just heard what RAG is, I'd like to improve a local model (I'm only really familiar with deepseek, but I understand that ollama is great with RAG) with RAG.&lt;/p&gt; &lt;p&gt;I'd like to download Wikipedia locally, and use that for RAG. I've got a passable gaming laptop I don't use which has 32gb RAM, an RTX 3070 and an i7, along with an SSD.&lt;/p&gt; &lt;p&gt;I know I can download Wikipedia without images and it's something like 12-17gb. Would a local LLM be capable of searching through it automatically and choosing the best 2-3 articles based on my question? Or am I opening a can of worms?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/trichofobia"&gt; /u/trichofobia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihibp9/is_wikipedia_rag_possible_entirely_locally_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihibp9/is_wikipedia_rag_possible_entirely_locally_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihibp9/is_wikipedia_rag_possible_entirely_locally_with_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T14:02:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii5o2q</id>
    <title>How do you know which LLM to use and for what use case?</title>
    <updated>2025-02-05T08:37:46+00:00</updated>
    <author>
      <name>/u/hexarthrius</name>
      <uri>https://old.reddit.com/user/hexarthrius</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there, I'm a newbie in local LLMs and maybe AI in general. Is there a guide out there that allows me to assess quickly which LLMs from Ollama are capable of doing which task?&lt;/p&gt; &lt;p&gt;I'd like to leverage AI in my local computer and later maybe branch it out to hosting my own personal service to do most stuff and maybe make an agent of myself to help me with my work (Corporate IT stuff).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hexarthrius"&gt; /u/hexarthrius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii5o2q/how_do_you_know_which_llm_to_use_and_for_what_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii5o2q/how_do_you_know_which_llm_to_use_and_for_what_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii5o2q/how_do_you_know_which_llm_to_use_and_for_what_use/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T08:37:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihfyi2</id>
    <title>Ollama + DeepSeek + Obsidian = The Ultimate Offline AI Assistant!</title>
    <updated>2025-02-04T11:54:17+00:00</updated>
    <author>
      <name>/u/Kind-Industry-609</name>
      <uri>https://old.reddit.com/user/Kind-Industry-609</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ihfyi2/ollama_deepseek_obsidian_the_ultimate_offline_ai/"&gt; &lt;img alt="Ollama + DeepSeek + Obsidian = The Ultimate Offline AI Assistant!" src="https://external-preview.redd.it/32JgoJVP2Vxa0PebR1pmCtaV_33XwoDfHhsNkStqIjE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49a27f19a9e63b070eb41d43f95929897b33eb6f" title="Ollama + DeepSeek + Obsidian = The Ultimate Offline AI Assistant!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kind-Industry-609"&gt; /u/Kind-Industry-609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/qAsGO5N7OCk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ihfyi2/ollama_deepseek_obsidian_the_ultimate_offline_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ihfyi2/ollama_deepseek_obsidian_the_ultimate_offline_ai/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-04T11:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii11dz</id>
    <title>Deepseek r1 1.5b thinking about rose flower 🌹</title>
    <updated>2025-02-05T03:39:16+00:00</updated>
    <author>
      <name>/u/False-Woodpecker5604</name>
      <uri>https://old.reddit.com/user/False-Woodpecker5604</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ii11dz/deepseek_r1_15b_thinking_about_rose_flower/"&gt; &lt;img alt="Deepseek r1 1.5b thinking about rose flower 🌹" src="https://preview.redd.it/7axbiaybr8he1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=719cdceebe4b50f16a0fc624b6247f706549c65f" title="Deepseek r1 1.5b thinking about rose flower 🌹" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/False-Woodpecker5604"&gt; /u/False-Woodpecker5604 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7axbiaybr8he1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ii11dz/deepseek_r1_15b_thinking_about_rose_flower/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ii11dz/deepseek_r1_15b_thinking_about_rose_flower/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-05T03:39:16+00:00</published>
  </entry>
</feed>
