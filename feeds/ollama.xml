<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-11T12:12:21+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mm0n7h</id>
    <title>Using a local LLM for proofreading on macOS?</title>
    <updated>2025-08-09T21:21:17+00:00</updated>
    <author>
      <name>/u/whooshingsounds</name>
      <uri>https://old.reddit.com/user/whooshingsounds</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to use a LLM to rephrase paragraphs of text, editing them for brevity, clarity, or to aim for a certain tone (formal, casual, businesslike…). I would also prefer not to upload these text to a server. Which local LLMs would be best suited for this? And would any Mac laptop be up to the task?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/whooshingsounds"&gt; /u/whooshingsounds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm0n7h/using_a_local_llm_for_proofreading_on_macos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm0n7h/using_a_local_llm_for_proofreading_on_macos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm0n7h/using_a_local_llm_for_proofreading_on_macos/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T21:21:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlwtkf</id>
    <title>Size of Gpt OSS</title>
    <updated>2025-08-09T18:40:05+00:00</updated>
    <author>
      <name>/u/Waakaari</name>
      <uri>https://old.reddit.com/user/Waakaari</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the size of gpt-oss-20b and gpt-oss-120b when I download from ollama? Is it the same from hugging face? &lt;/p&gt; &lt;p&gt;Is there any difference betwecn running gpt-oss models using ollama and using comfyui or setting it up any other way? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Waakaari"&gt; /u/Waakaari &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlwtkf/size_of_gpt_oss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlwtkf/size_of_gpt_oss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlwtkf/size_of_gpt_oss/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T18:40:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlz15z</id>
    <title>Are you good grok?</title>
    <updated>2025-08-09T20:12:41+00:00</updated>
    <author>
      <name>/u/Gandalfusmaximale</name>
      <uri>https://old.reddit.com/user/Gandalfusmaximale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mlz15z/are_you_good_grok/"&gt; &lt;img alt="Are you good grok?" src="https://preview.redd.it/lt6953j7x1if1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc8b58ce17e3bc02fb5fb414b4cf50dc3eb00ba4" title="Are you good grok?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else had this before ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gandalfusmaximale"&gt; /u/Gandalfusmaximale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lt6953j7x1if1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlz15z/are_you_good_grok/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlz15z/are_you_good_grok/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T20:12:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlxqtz</id>
    <title>Get a second GPU or go for a Mac Mini? (Qwen3-Coder30b)</title>
    <updated>2025-08-09T19:18:13+00:00</updated>
    <author>
      <name>/u/Manaberryio</name>
      <uri>https://old.reddit.com/user/Manaberryio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;I have a server machine I want to use for AI. I have an RX6800 (16GB VRAM) inside, along with a 13900KF and 128GB of ram. I've tried Qwen3-coder 30b 3ab but I cannot host it fully on my GPU with a bigger context than 16K. It's really slow and somehow unable to process debug request from Roo Code.&lt;/p&gt; &lt;p&gt;Will a second RX 6800 (around $300 used) would be helpful to do so? Or should I sell my stuff and get a Mac Mini M4 with at least 24GB of memory?&lt;/p&gt; &lt;p&gt;Thanks for helping&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Manaberryio"&gt; /u/Manaberryio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlxqtz/get_a_second_gpu_or_go_for_a_mac_mini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlxqtz/get_a_second_gpu_or_go_for_a_mac_mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlxqtz/get_a_second_gpu_or_go_for_a_mac_mini/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T19:18:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm7mp8</id>
    <title>Has anyone successfully run Ollama on a Jetson Orin Nano?</title>
    <updated>2025-08-10T02:56:49+00:00</updated>
    <author>
      <name>/u/matsyui_</name>
      <uri>https://old.reddit.com/user/matsyui_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m trying to turn my Jetson Orin Nano Super Developer Kit into a self-hosted AI API server running Ollama.&lt;/p&gt; &lt;p&gt;Has anyone here tried this, patched it, or gotten decent performance running Ollama on a Jetson device?&lt;/p&gt; &lt;p&gt;I’d love to hear about your setup, steps, or any pitfalls you ran into.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matsyui_"&gt; /u/matsyui_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm7mp8/has_anyone_successfully_run_ollama_on_a_jetson/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm7mp8/has_anyone_successfully_run_ollama_on_a_jetson/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm7mp8/has_anyone_successfully_run_ollama_on_a_jetson/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T02:56:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmf6xl</id>
    <title>Reasoning LLMs Explorer</title>
    <updated>2025-08-10T10:32:50+00:00</updated>
    <author>
      <name>/u/Boring_Rabbit2275</name>
      <uri>https://old.reddit.com/user/Boring_Rabbit2275</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a web page where a lot of information is compiled about Reasoning in LLMs (A tree of surveys, an atlas of definitions and a map of techniques in reasoning)&lt;/p&gt; &lt;p&gt;&lt;a href="https://azzedde.github.io/reasoning-explorer/"&gt;https://azzedde.github.io/reasoning-explorer/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Your insights ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Boring_Rabbit2275"&gt; /u/Boring_Rabbit2275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmf6xl/reasoning_llms_explorer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmf6xl/reasoning_llms_explorer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmf6xl/reasoning_llms_explorer/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T10:32:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm9uds</id>
    <title>Need help with benchmarking for RAG + LLM running locally.</title>
    <updated>2025-08-10T04:56:32+00:00</updated>
    <author>
      <name>/u/irodov4030</name>
      <uri>https://old.reddit.com/user/irodov4030</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to benchmark RAG setup for multiple file formats like - doc, xls, csv, ppt, png etc.&lt;/p&gt; &lt;p&gt;Are there any benchmarks with which I can test accuracy/quality of answers across multiple file formats?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/irodov4030"&gt; /u/irodov4030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm9uds/need_help_with_benchmarking_for_rag_llm_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm9uds/need_help_with_benchmarking_for_rag_llm_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm9uds/need_help_with_benchmarking_for_rag_llm_running/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T04:56:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlxx5h</id>
    <title>LLMs are Stochastic Parrots - Interactive Visualization</title>
    <updated>2025-08-09T19:25:34+00:00</updated>
    <author>
      <name>/u/kushalgoenka</name>
      <uri>https://old.reddit.com/user/kushalgoenka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mlxx5h/llms_are_stochastic_parrots_interactive/"&gt; &lt;img alt="LLMs are Stochastic Parrots - Interactive Visualization" src="https://external-preview.redd.it/NvAI6Yum9O40l3qZlOeyOssVIs2oLgJwnoMTWT8Xzzg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64d63adc6a8fdef03b738bcffef859cc8986b0e8" title="LLMs are Stochastic Parrots - Interactive Visualization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kushalgoenka"&gt; /u/kushalgoenka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/6dn1kUwTFcc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mlxx5h/llms_are_stochastic_parrots_interactive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mlxx5h/llms_are_stochastic_parrots_interactive/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-09T19:25:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmmwcg</id>
    <title>Rookie question. Avoiding FOMO…</title>
    <updated>2025-08-10T16:25:44+00:00</updated>
    <author>
      <name>/u/Famous-Recognition62</name>
      <uri>https://old.reddit.com/user/Famous-Recognition62</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Famous-Recognition62"&gt; /u/Famous-Recognition62 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLM/comments/1mmmtlf/rookie_question_avoiding_fomo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmmwcg/rookie_question_avoiding_fomo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmmwcg/rookie_question_avoiding_fomo/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T16:25:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmhxiv</id>
    <title>Qwen 4B Instruct works beautifully ❤️</title>
    <updated>2025-08-10T12:58:52+00:00</updated>
    <author>
      <name>/u/krishnajeya</name>
      <uri>https://old.reddit.com/user/krishnajeya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mmhxiv/qwen_4b_instruct_works_beautifully/"&gt; &lt;img alt="Qwen 4B Instruct works beautifully ❤️" src="https://preview.redd.it/izyjc6vps6if1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c27f3d6f828cbb7921438248a37810117753209" title="Qwen 4B Instruct works beautifully ❤️" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/krishnajeya"&gt; /u/krishnajeya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/izyjc6vps6if1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmhxiv/qwen_4b_instruct_works_beautifully/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmhxiv/qwen_4b_instruct_works_beautifully/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T12:58:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm9zhv</id>
    <title>What should I do with my home ollama lab?</title>
    <updated>2025-08-10T05:04:27+00:00</updated>
    <author>
      <name>/u/onemorequickchange</name>
      <uri>https://old.reddit.com/user/onemorequickchange</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Setup an x99 with dual xeon 2699 v4s and dual 3090s, 256GB RAM, linux distro. Curious what people use this setup for? I just subscribed to Claude @ $100/month and have had chatGPT for teams $30/pers/month. Do people just use local to save money or are there legit use cases? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onemorequickchange"&gt; /u/onemorequickchange &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm9zhv/what_should_i_do_with_my_home_ollama_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm9zhv/what_should_i_do_with_my_home_ollama_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm9zhv/what_should_i_do_with_my_home_ollama_lab/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T05:04:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmnptg</id>
    <title>How to run multiple versions of same model?</title>
    <updated>2025-08-10T16:57:44+00:00</updated>
    <author>
      <name>/u/petr_bena</name>
      <uri>https://old.reddit.com/user/petr_bena</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Right now ollama always works only with latest version of a model, say Mistral:7b&lt;/p&gt; &lt;p&gt;These models get periodic updates. What if I wanted to retain version from 2024 and 2025 and be able to switch between them? Does ollama supports something like version tagging and maintaining multiple versions of same model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/petr_bena"&gt; /u/petr_bena &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmnptg/how_to_run_multiple_versions_of_same_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmnptg/how_to_run_multiple_versions_of_same_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmnptg/how_to_run_multiple_versions_of_same_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T16:57:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmfhfa</id>
    <title>Fastest and best model for my really low spec hardware</title>
    <updated>2025-08-10T10:50:01+00:00</updated>
    <author>
      <name>/u/PurpleUser0000</name>
      <uri>https://old.reddit.com/user/PurpleUser0000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an I5-4th gen, ddr3 8GB ram 1600hz , no GPU ( IGPU ) &lt;/p&gt; &lt;p&gt;What's the best model I can go with here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PurpleUser0000"&gt; /u/PurpleUser0000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmfhfa/fastest_and_best_model_for_my_really_low_spec/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmfhfa/fastest_and_best_model_for_my_really_low_spec/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmfhfa/fastest_and_best_model_for_my_really_low_spec/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T10:50:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmtawi</id>
    <title>Noob here. Please Help me find the perfect model.</title>
    <updated>2025-08-10T20:32:41+00:00</updated>
    <author>
      <name>/u/Many-Kaleidoscope-72</name>
      <uri>https://old.reddit.com/user/Many-Kaleidoscope-72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone. I’m new to LLMs and ollama. But I’m interested and hyped. I am a programmer (IT technician) and I started dealing with local LLMs and vision based ais like stable diffusion. I invested into a Rog Zephyrus G16 2025 model with 64GB RAM, 5070ti (12GB), Core Ultra 9 285H model. As far as I know, this hardware lets me do light or medium AI work locally. I plan to learn AI development later on University. I will just start my first semester. That’s why I choose these specs. I have a 4070ti, 32GB RAM, i7 13700K desktop pc too. Obviously that’s faster but lacks immense VRAM for large models. I could remote onto the desktop from a weaker laptop, but I’d rather be able to do light / medium work locally, than to always remote onto the home PC.&lt;/p&gt; &lt;p&gt;Question is, what’s the absolute best AI model for text + vision, that can speak English and Hungarian (my native language) well. I know that there are runtime parameters for both ollama and the models, so maybe a bigger model is runable with the right tweaks? Currently gemma3 12B runs fast, knows Hungarian well. 27B runs very poorly. Even when changing model settings (I might set it up incorrectly).&lt;/p&gt; &lt;p&gt;So what are your recommendations, guys? Let me know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Many-Kaleidoscope-72"&gt; /u/Many-Kaleidoscope-72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmtawi/noob_here_please_help_me_find_the_perfect_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmtawi/noob_here_please_help_me_find_the_perfect_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmtawi/noob_here_please_help_me_find_the_perfect_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T20:32:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmr2n1</id>
    <title>is we can run gpt oss in 16gb vram ? why mine is offload to cpu im running in docker in ubuntu 22.04</title>
    <updated>2025-08-10T19:06:17+00:00</updated>
    <author>
      <name>/u/actuallytech</name>
      <uri>https://old.reddit.com/user/actuallytech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mmr2n1/is_we_can_run_gpt_oss_in_16gb_vram_why_mine_is/"&gt; &lt;img alt="is we can run gpt oss in 16gb vram ? why mine is offload to cpu im running in docker in ubuntu 22.04" src="https://a.thumbs.redditmedia.com/T8e2323zafFhBWk7siLTxyY4iplSS1jgitzzPLUcEX8.jpg" title="is we can run gpt oss in 16gb vram ? why mine is offload to cpu im running in docker in ubuntu 22.04" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;gpu used is rtx 5060ti 16gb vram&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/actuallytech"&gt; /u/actuallytech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mmr2n1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmr2n1/is_we_can_run_gpt_oss_in_16gb_vram_why_mine_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmr2n1/is_we_can_run_gpt_oss_in_16gb_vram_why_mine_is/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T19:06:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmj279</id>
    <title>The first time I heard a sound coming from my MacBook was while using GPT-OSS 20B</title>
    <updated>2025-08-10T13:49:09+00:00</updated>
    <author>
      <name>/u/anakedsuperman</name>
      <uri>https://old.reddit.com/user/anakedsuperman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mmj279/the_first_time_i_heard_a_sound_coming_from_my/"&gt; &lt;img alt="The first time I heard a sound coming from my MacBook was while using GPT-OSS 20B" src="https://external-preview.redd.it/ZThhMDNvMmE1N2lmMTSXhaJcSNaMLiLEA491TTFq3lE-Ha0mGK07Lje4LN3h.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7372d0bd82781bd6388dff2cabe3eca39528ceca" title="The first time I heard a sound coming from my MacBook was while using GPT-OSS 20B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running GPT-OSS 20B on my MacBook M4 Max with 36GB RAM. I don't hear anything from other models, though, even with Devtra 140B.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anakedsuperman"&gt; /u/anakedsuperman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vyk9un2a57if1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmj279/the_first_time_i_heard_a_sound_coming_from_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmj279/the_first_time_i_heard_a_sound_coming_from_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T13:49:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmrqgh</id>
    <title>How do I get vision models working in Ollama/LM Studio?</title>
    <updated>2025-08-10T19:31:37+00:00</updated>
    <author>
      <name>/u/avdsrj</name>
      <uri>https://old.reddit.com/user/avdsrj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I've been messing around with Ollama and LM Studio to run LLMs locally, and I'm hitting a wall with vision models.&lt;/p&gt; &lt;p&gt;So here's the deal - I know vision models need these &amp;quot;mmproj&amp;quot; files to actually see pictures, and everything works fine when I grab models straight from Ollama or LM Studio's repos. But the moment I try to use GGUF models from somewhere else (like Hugging Face), I'm completely lost on how to get the mmproj stuff working.&lt;/p&gt; &lt;p&gt;I've been googling this for way too long and honestly can't find a clear answer anywhere. It feels like there's some obvious step I'm missing.&lt;/p&gt; &lt;p&gt;Has anyone figured out how to manually add mmproj files to models? Like, is there a specific way to structure the Modelfile or some command I'm not seeing?&lt;/p&gt; &lt;p&gt;Would really appreciate if someone could point me in the right direction - this is driving me crazy!&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/avdsrj"&gt; /u/avdsrj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmrqgh/how_do_i_get_vision_models_working_in_ollamalm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmrqgh/how_do_i_get_vision_models_working_in_ollamalm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmrqgh/how_do_i_get_vision_models_working_in_ollamalm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T19:31:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm4ibk</id>
    <title>I ran OpenAI’s GPT-OSS 20B locally on a 16GB Mac with Ollama — setup, gotchas, and mini demo</title>
    <updated>2025-08-10T00:19:29+00:00</updated>
    <author>
      <name>/u/Spirited-Wind6803</name>
      <uri>https://old.reddit.com/user/Spirited-Wind6803</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mm4ibk/i_ran_openais_gptoss_20b_locally_on_a_16gb_mac/"&gt; &lt;img alt="I ran OpenAI’s GPT-OSS 20B locally on a 16GB Mac with Ollama — setup, gotchas, and mini demo" src="https://external-preview.redd.it/MHQ5M3V2aXQ0M2lmMf_ZwYHO2m1fMNCQy9M-9mV9J_Z510ikdbK6GDGwXk75.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=678ce1f3ab9b2ba28aeffdad56e26d3d77e4258f" title="I ran OpenAI’s GPT-OSS 20B locally on a 16GB Mac with Ollama — setup, gotchas, and mini demo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all — I’ve been testing &lt;strong&gt;GPT-OSS 20B&lt;/strong&gt; locally using &lt;strong&gt;Ollama&lt;/strong&gt; and wanted to share a clean setup path, what worked, what didn’t, and a tiny QA demo. &lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Yes, 20B runs on a 16GB Mac&lt;/strong&gt; with Ollama. Do I have the patience? No, it took toooo long&lt;/li&gt; &lt;li&gt;Should you use 16GB to perform any other tasks such as coding, agent, RAG? No, not worth it - upgrade to 32GB maybe..maybe will give you more. I tried on A100 GPU and still did not meet my expectation &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spirited-Wind6803"&gt; /u/Spirited-Wind6803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qlifjvit43if1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mm4ibk/i_ran_openais_gptoss_20b_locally_on_a_16gb_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mm4ibk/i_ran_openais_gptoss_20b_locally_on_a_16gb_mac/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T00:19:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn74d7</id>
    <title>Help for creating llm</title>
    <updated>2025-08-11T08:06:54+00:00</updated>
    <author>
      <name>/u/matin1099</name>
      <uri>https://old.reddit.com/user/matin1099</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matin1099"&gt; /u/matin1099 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LLMDevs/comments/1mn736g/help_for_creating_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn74d7/help_for_creating_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mn74d7/help_for_creating_llm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T08:06:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmwz6c</id>
    <title>People with MacBook Pro with 36gb of memory, which models you are running for coding?</title>
    <updated>2025-08-10T23:06:39+00:00</updated>
    <author>
      <name>/u/Sea-Emu2600</name>
      <uri>https://old.reddit.com/user/Sea-Emu2600</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have m3 max 36gb of memory but kinda new to ollama so not sure which model use for coding. Also you are using what as a front-end? Vscode Copilot, cline?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Emu2600"&gt; /u/Sea-Emu2600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmwz6c/people_with_macbook_pro_with_36gb_of_memory_which/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmwz6c/people_with_macbook_pro_with_36gb_of_memory_which/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmwz6c/people_with_macbook_pro_with_36gb_of_memory_which/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T23:06:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn8jot</id>
    <title>Coding model for 4080S + 32Gb RAM</title>
    <updated>2025-08-11T09:41:01+00:00</updated>
    <author>
      <name>/u/tresslessone</name>
      <uri>https://old.reddit.com/user/tresslessone</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Title says most of it - I’m looking for some coding model recommendations for my 4080S (16Gb VRAM) + 32Gb RAM (7800X3D) gaming PC.&lt;/p&gt; &lt;p&gt;I’m currently running QWEN3-coder 30b (Q4_K_XL) and whilst it runs, it’s pretty slow (especially once the context fills up) and I’d like something a bit snappier. &lt;/p&gt; &lt;p&gt;Is there a 14b version of QWEN3-coder out there perhaps that I can’t seem to find?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tresslessone"&gt; /u/tresslessone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn8jot/coding_model_for_4080s_32gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn8jot/coding_model_for_4080s_32gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mn8jot/coding_model_for_4080s_32gb_ram/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T09:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn5uvk</id>
    <title>devstral:24b</title>
    <updated>2025-08-11T06:44:42+00:00</updated>
    <author>
      <name>/u/barrulus</name>
      <uri>https://old.reddit.com/user/barrulus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone tried to use devstral:24b for any coding work?&lt;/p&gt; &lt;p&gt;I am interested to know what kind of work I could pass to this model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/barrulus"&gt; /u/barrulus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn5uvk/devstral24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn5uvk/devstral24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mn5uvk/devstral24b/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T06:44:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmu24w</id>
    <title>What is the Best coding LLM for my system?</title>
    <updated>2025-08-10T21:02:54+00:00</updated>
    <author>
      <name>/u/tarsonis125</name>
      <uri>https://old.reddit.com/user/tarsonis125</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;rtx 3090 24vram + 96gig ram&lt;/p&gt; &lt;p&gt;What is the best local LLM to use on my system?&lt;br /&gt; Do some models do better then other at some tasks?&lt;br /&gt; I am trying out a bunch of them, but its hard for me to properly rate them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarsonis125"&gt; /u/tarsonis125 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmu24w/what_is_the_best_coding_llm_for_my_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mmu24w/what_is_the_best_coding_llm_for_my_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mmu24w/what_is_the_best_coding_llm_for_my_system/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-10T21:02:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn1v6v</id>
    <title>Integrated Mistral AI into a vehicle</title>
    <updated>2025-08-11T02:58:38+00:00</updated>
    <author>
      <name>/u/SoftDuckling1</name>
      <uri>https://old.reddit.com/user/SoftDuckling1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently working on building an AI to install into my vehicle as an assistant, NOT a driver. The main purpose is to have an assistant that can tell me if something is wrong with the vehicle, provide casual chat on long drives, provide directions (if i add offline GPS/maps), and make driving overall more enjoyable, all while keeping my safety a top priority. I'm using Mistral AI (local) and I have a RAG memory script somewhat working, still working out some kinks. It's using PIPER as a TTS, and im currently working on STT. I eventually want to try to integrate a dual facing dash cam with facial recognition and record drives with loop recording, like a normal dash cam. It will have access to my vehicles speakers, mic, radio display, and possibly OBD-II. That last part still makes me worry, which is why I'm looking for any advice. I already gave it explicit directives and prompts telling it to never alter anything, only read data, but I still don't fully trust it. I'll be adding failsafes before i install it. I plan to install this AI onto a Jetson Orin Nano Super DEV. (at least that's what I plan on right now, might change later). I'll give it a 1-2TB extreme SD to store &amp;quot;.json&amp;quot; memories that it will be able to use as an index library, retrieving only relative information. Then it will be installed into the glove box. I will renovate the glove box with a vibration-proof housings, heat sink, proper cooling, ceramic heaters for colder months, air filters, and anything else I can think of.&lt;/p&gt; &lt;p&gt;So any thoughts? Fully offline AI as a vehicle companion, an okay idea or a ticking bomb? I'll gladly accept any advice about this project. And again, it WILL NOT be controlling any part of the vehicle, only reading data while providing conversation and info.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SoftDuckling1"&gt; /u/SoftDuckling1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn1v6v/integrated_mistral_ai_into_a_vehicle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn1v6v/integrated_mistral_ai_into_a_vehicle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mn1v6v/integrated_mistral_ai_into_a_vehicle/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T02:58:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn653l</id>
    <title>PSA: Secure Your Ollama / LLM Ports ( Even on Home LAN )</title>
    <updated>2025-08-11T07:02:47+00:00</updated>
    <author>
      <name>/u/Immediate_Fun4182</name>
      <uri>https://old.reddit.com/user/Immediate_Fun4182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I do not know where to begin. Ok here we go.&lt;/p&gt; &lt;p&gt;Recently someone on &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; &lt;a href="https://www.reddit.com/r/ollama/comments/1k6m1b3/someone_found_my_open_ai_server_and_used_it_to/"&gt;posted&lt;/a&gt; that their Ollama API had been exposed to the public internet for over a month, and strangers used it to process massive amounts of personal data.&lt;/p&gt; &lt;p&gt;I am posting this because Ollama is a beginner friendly platform and not many people do realize that Ollama and similar LLM servers like vLLM bind to &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; by default. Meaning &lt;strong&gt;any network interface&lt;/strong&gt; can accept connections (lan wifi and even an if your router is forwarding). No authentication or rate limiting exists on default. If your router has UPnP or manual port forwarding enabled, the API can be fully exposed to the internet without you realizing it.&lt;/p&gt; &lt;p&gt;Best practices I can recommend for beginners is this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Bind only to Tailscale or localhost and/or bind to tailscale0 only so it is only reachable inside your Tailnet.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Close risky ports by default:&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Use usw or firewalld to block everything except what you explicitly allow:&lt;/p&gt; &lt;p&gt;```&lt;br /&gt; sudo ufw default deny incoming&lt;/p&gt; &lt;p&gt;sudo ufw default allow outgoing&lt;/p&gt; &lt;p&gt;sudo ufw allow from &lt;a href="http://100.64.0.0/10"&gt;100.64.0.0/10&lt;/a&gt; # Tailscale subnet&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;and lastly&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Watch your exposed services on ports:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;sudo sof -i -P -n | grep LISTEN&lt;br /&gt; ```&lt;/p&gt; &lt;p&gt;F.e. I was exposing my 11434 port to public IP which is a terrible mistake. make sure you only open through vpn/tailscale set up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Immediate_Fun4182"&gt; /u/Immediate_Fun4182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn653l/psa_secure_your_ollama_llm_ports_even_on_home_lan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mn653l/psa_secure_your_ollama_llm_ports_even_on_home_lan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mn653l/psa_secure_your_ollama_llm_ports_even_on_home_lan/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-11T07:02:47+00:00</published>
  </entry>
</feed>
