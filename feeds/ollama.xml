<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-03-31T03:53:54+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1jll087</id>
    <title>Great event tonight with Ollama and vLLM</title>
    <updated>2025-03-28T02:39:46+00:00</updated>
    <author>
      <name>/u/Rude-Bad-6579</name>
      <uri>https://old.reddit.com/user/Rude-Bad-6579</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jll087/great_event_tonight_with_ollama_and_vllm/"&gt; &lt;img alt="Great event tonight with Ollama and vLLM" src="https://preview.redd.it/6yvrist4fcre1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=90f67ffb2ffcb6b8fada40af2bd2ba6a22bfc94b" title="Great event tonight with Ollama and vLLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Packed house, lots of great attendees. Loved Gemma demo running off 1 Mac laptop live. Super impressive &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rude-Bad-6579"&gt; /u/Rude-Bad-6579 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6yvrist4fcre1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jll087/great_event_tonight_with_ollama_and_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jll087/great_event_tonight_with_ollama_and_vllm/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T02:39:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm35s2</id>
    <title>Computer vision for reading</title>
    <updated>2025-03-28T19:19:07+00:00</updated>
    <author>
      <name>/u/gttcoelho</name>
      <uri>https://old.reddit.com/user/gttcoelho</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, guys! I am using the Google vision API for transcribing text from images, but it is too expensive... do you know some cheaper alternative for this? I have tried llava but it is petty bad for text transcribing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gttcoelho"&gt; /u/gttcoelho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm35s2/computer_vision_for_reading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm35s2/computer_vision_for_reading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jm35s2/computer_vision_for_reading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T19:19:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm5spn</id>
    <title>Ollama blobs</title>
    <updated>2025-03-28T21:11:53+00:00</updated>
    <author>
      <name>/u/techmago</name>
      <uri>https://old.reddit.com/user/techmago</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a ton of blobs...&lt;br /&gt; How do i figure out which model is the owner of each blob?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techmago"&gt; /u/techmago &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm5spn/ollama_blobs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm5spn/ollama_blobs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jm5spn/ollama_blobs/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T21:11:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmuasb</id>
    <title>What is the best model i can run?</title>
    <updated>2025-03-29T19:36:37+00:00</updated>
    <author>
      <name>/u/xdvst8x</name>
      <uri>https://old.reddit.com/user/xdvst8x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the best model i can run on my machine? It is a ThreadRipper with 128GB RAM, 8TB SSD, 3x 3090 Nvidia cards with 24GB.&lt;/p&gt; &lt;p&gt;i have tried a lot of models, but I can seem to find anything that works as well as claude or GPT.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xdvst8x"&gt; /u/xdvst8x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmuasb/what_is_the_best_model_i_can_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmuasb/what_is_the_best_model_i_can_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmuasb/what_is_the_best_model_i_can_run/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T19:36:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmrr83</id>
    <title>Ollama connect to Microsoft o365 account mail, calendar, contact oneDrive SharePoint</title>
    <updated>2025-03-29T17:42:02+00:00</updated>
    <author>
      <name>/u/Awkward-Desk-8340</name>
      <uri>https://old.reddit.com/user/Awkward-Desk-8340</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How connect ollama to my Microsoft webmail to talk with im ?&lt;/p&gt; &lt;p&gt;I m looking how to connect ollama to my webmail Microsoft account&lt;/p&gt; &lt;p&gt;Calendar Mail One drive&lt;/p&gt; &lt;p&gt;To make it my agent and works with him&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Awkward-Desk-8340"&gt; /u/Awkward-Desk-8340 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmrr83/ollama_connect_to_microsoft_o365_account_mail/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmrr83/ollama_connect_to_microsoft_o365_account_mail/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmrr83/ollama_connect_to_microsoft_o365_account_mail/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T17:42:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm1a4g</id>
    <title>Mastering Text Chunking with Ollama: A Comprehensive Guide to Advanced Processing</title>
    <updated>2025-03-28T18:00:04+00:00</updated>
    <author>
      <name>/u/KonradFreeman</name>
      <uri>https://old.reddit.com/user/KonradFreeman</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KonradFreeman"&gt; /u/KonradFreeman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://danielkliewer.com/blog/2025-03-28-Ollama-Chunking"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jm1a4g/mastering_text_chunking_with_ollama_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jm1a4g/mastering_text_chunking_with_ollama_a/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-28T18:00:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmnhbj</id>
    <title>Build a Voice RAG with Deepseek, LangChain and Streamlit</title>
    <updated>2025-03-29T14:28:46+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jmnhbj/build_a_voice_rag_with_deepseek_langchain_and/"&gt; &lt;img alt="Build a Voice RAG with Deepseek, LangChain and Streamlit" src="https://external-preview.redd.it/kTJZuoiqL6QSf3wgIhIvK5EAijtopYLgpBSTlRcJH9I.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cbdb9aeb80a0fda3ec7fff61aa3213901aac5065" title="Build a Voice RAG with Deepseek, LangChain and Streamlit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=HT4a6A_wXdA&amp;amp;list=PLp01ObP3udmq2quR-RfrX4zNut_t_kNot&amp;amp;index=14"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmnhbj/build_a_voice_rag_with_deepseek_langchain_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmnhbj/build_a_voice_rag_with_deepseek_langchain_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T14:28:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmt8y3</id>
    <title>Ollama python library "chat" method question</title>
    <updated>2025-03-29T18:48:22+00:00</updated>
    <author>
      <name>/u/Haghiri75</name>
      <uri>https://old.reddit.com/user/Haghiri75</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a python code which uses the chat method. I just need to know does this chat method come with any sort of logging? You know something like when you are generating with SD/FLUX on terminal and there is a progress bar. &lt;/p&gt; &lt;p&gt;I saw source codes but couldn't find anything showing the progress. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Haghiri75"&gt; /u/Haghiri75 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmt8y3/ollama_python_library_chat_method_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmt8y3/ollama_python_library_chat_method_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmt8y3/ollama_python_library_chat_method_question/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T18:48:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmnb8b</id>
    <title>Testability of LLMs: the elusive hunt for deterministic output with ollama (or any vendor actually)</title>
    <updated>2025-03-29T14:20:32+00:00</updated>
    <author>
      <name>/u/boxabirds</name>
      <uri>https://old.reddit.com/user/boxabirds</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a bit obsessed about testability and LLMs. I worked with pytorch in the past and found at least with diffusion models, &lt;strong&gt;passing a seed would give deterministic output&lt;/strong&gt; (on the same hardware / software config). This was very powerful because it meant I could test variations and factor out common parameters. &lt;/p&gt; &lt;p&gt;And in the open weight world I saw the seed parameter, I saw it exposed as a parameter with ollama and I saw it exposed in GPT-4+ API (though OpenAI has since augmented it with system fingerprint). &lt;/p&gt; &lt;p&gt;This brought joy to my heart, as an engineer who hates fuzziness. &amp;quot;The capital of France is Paris&amp;quot; is NOT THE SAME AS &amp;quot;The capital of France is Paris!&amp;quot;. &lt;/p&gt; &lt;p&gt;HOWEVER I've only found two specific configurations of language models anywhere that seems to produce deterministic results, and that is aws Bedrock nova lite and nano, when temperature = 0 they are &amp;quot;reasonably deterministic&amp;quot; which of course is an oxymoron. But better than others. &lt;/p&gt; &lt;p&gt;I also tried Gemini and OpenAI and had no luck. &lt;/p&gt; &lt;p&gt;Am I missing something here? Or are we really seeing what is effectively a global denial from vendors that deterministic output is basicaly a pipe dream. &lt;/p&gt; &lt;p&gt;Please if someone can correct me to provide example code that guarantees (for some reasonable definition of guarantee) deterministic output so I don't have to introduce another whole language model evaluation evaluation piece. &lt;/p&gt; &lt;p&gt;thanks in advance&lt;/p&gt; &lt;p&gt;🙏&lt;/p&gt; &lt;p&gt;Here's a super basic script that tries to find any deterministic models you have installed with ollama&lt;/p&gt; &lt;p&gt;&lt;a href="https://gist.github.com/boxabirds/6257440850d2a874dd467f891879c776"&gt;https://gist.github.com/boxabirds/6257440850d2a874dd467f891879c776&lt;/a&gt;&lt;/p&gt; &lt;p&gt;needs jq installed. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boxabirds"&gt; /u/boxabirds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmnb8b/testability_of_llms_the_elusive_hunt_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmnb8b/testability_of_llms_the_elusive_hunt_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmnb8b/testability_of_llms_the_elusive_hunt_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T14:20:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmxl2z</id>
    <title>Haproxy infront of multiple ollama servers</title>
    <updated>2025-03-29T22:08:32+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Does anyone have haproxy balancing load to multiple Ollama servers?&lt;br /&gt; Not able to get my app to see/use the models. &lt;/p&gt; &lt;p&gt;Seems that for example&lt;br /&gt; curl ollamaserver_IP:11434 returns &amp;quot;ollama is running&amp;quot;&lt;br /&gt; From haproxy and from application server, so at least that request goes to haproxy and then to ollama and back to appserver.&lt;/p&gt; &lt;p&gt;When I take the haproxy away from between application server and the AI server all works. But when I put the haproxy, for some reason the traffic wont flow from application server -&amp;gt; haproxy to AI server. At least my application says were unable to Failed to get models from Ollama: cURL error 7: Failed to connect to ai.server05.net port 11434 after 1 ms: Couldn't connect to server.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmxl2z/haproxy_infront_of_multiple_ollama_servers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmxl2z/haproxy_infront_of_multiple_ollama_servers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmxl2z/haproxy_infront_of_multiple_ollama_servers/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T22:08:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmypm0</id>
    <title>ollama docker api</title>
    <updated>2025-03-29T23:03:02+00:00</updated>
    <author>
      <name>/u/wbiggs205</name>
      <uri>https://old.reddit.com/user/wbiggs205</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a server off site running in docker desktop. On windows 11 pro . But It is open to everyone I would like to know how to local it down so I'm the only one that can access it ? I do have tailscale installed then I block the port for ollama in windows firewall but now I can not access it thought tailscale &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wbiggs205"&gt; /u/wbiggs205 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmypm0/ollama_docker_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmypm0/ollama_docker_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmypm0/ollama_docker_api/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T23:03:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmgz50</id>
    <title>MCP servers using Ollama</title>
    <updated>2025-03-29T07:23:01+00:00</updated>
    <author>
      <name>/u/mehul_gupta1997</name>
      <uri>https://old.reddit.com/user/mehul_gupta1997</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jmgz50/mcp_servers_using_ollama/"&gt; &lt;img alt="MCP servers using Ollama" src="https://external-preview.redd.it/BIpLRntRPIc0zEIJuZa-Z96CiFN4WBZP78bNcyh3oVU.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53645309938d8d9462dc3e9eefaa8383ea65ceb6" title="MCP servers using Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehul_gupta1997"&gt; /u/mehul_gupta1997 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=z0DScLrix48"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmgz50/mcp_servers_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmgz50/mcp_servers_using_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T07:23:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmvmhg</id>
    <title>Adding GPU to old desktop to run Ollama</title>
    <updated>2025-03-29T20:37:05+00:00</updated>
    <author>
      <name>/u/zair</name>
      <uri>https://old.reddit.com/user/zair</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a Lenovo V55t desktop with the following specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen 5 3400G Processor&lt;/li&gt; &lt;li&gt;24GB DDR4-2666Mhz RAM&lt;/li&gt; &lt;li&gt;256GB SSD M.2 PCIe NVMe Opal&lt;/li&gt; &lt;li&gt;Radeon Vega 11 Graphics &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If I added a suitable GPU, could this run a reasonably large model? Considering this is a relatively slow PC that may not be able to fully leverage the latest GPUs, can you suggest what GPU I could get?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zair"&gt; /u/zair &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmvmhg/adding_gpu_to_old_desktop_to_run_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmvmhg/adding_gpu_to_old_desktop_to_run_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmvmhg/adding_gpu_to_old_desktop_to_run_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T20:37:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnijzt</id>
    <title>Seeking advise about Surface laptop 4</title>
    <updated>2025-03-30T18:09:35+00:00</updated>
    <author>
      <name>/u/elshazlik89</name>
      <uri>https://old.reddit.com/user/elshazlik89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jnijzt/seeking_advise_about_surface_laptop_4/"&gt; &lt;img alt="Seeking advise about Surface laptop 4" src="https://preview.redd.it/xw52a50z8vre1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78d290c4ab4a5dfd76b021d91fe4a4e1e57fae85" title="Seeking advise about Surface laptop 4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Everybody,&lt;/p&gt; &lt;p&gt;I know most would actually hate on me for trying because of my laptop, but i always wanted to have a personal AI assistant that i can use for lightweight stuff such as helping with my MBA studies, looking up information (treating it like an encyclopedia), perhaps small help with very very amateur coding, or anything a general AI assistant would do.&lt;/p&gt; &lt;p&gt;My current laptop is surface laptop 4 with ryzen 7 and only 8GB ram, tried to download models that are 4B or less because the bigger ones almost killed my laptop :D but i still getting a very sluggish experience.&lt;/p&gt; &lt;p&gt;Tried WSL then ubuntu ollama and docker + webui all through WSL environment/power shell but did not work&lt;br /&gt; Tried ollama from their website, docker app + webui and still no improvement in performance.&lt;br /&gt; Also tried LLMStudio with slightly better performance but not what i was looking for and after couple of chats everything falls behind.&lt;/p&gt; &lt;p&gt;I adjusted the virtual memory and paging file to the maximum i can do with no luck of any improvements. &lt;/p&gt; &lt;p&gt;I know my ram is limited, and while it is not upgradable, unfortunately I'm stuck with this laptop for a while.&lt;br /&gt; Financially unable to and honestly beside this, the laptop does day to day tasks without an issue so i aint complaining.&lt;/p&gt; &lt;p&gt;Seeking advice if there is any other way to have alternative for online like experience or should i stick with openai or deepseek's online options.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elshazlik89"&gt; /u/elshazlik89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xw52a50z8vre1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnijzt/seeking_advise_about_surface_laptop_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jnijzt/seeking_advise_about_surface_laptop_4/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-30T18:09:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnfb6h</id>
    <title>Need help stopping runaway GPU due to inferencing with Ollama and Open WebUI</title>
    <updated>2025-03-30T15:47:01+00:00</updated>
    <author>
      <name>/u/Haunting_Bat_4240</name>
      <uri>https://old.reddit.com/user/Haunting_Bat_4240</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Haunting_Bat_4240"&gt; /u/Haunting_Bat_4240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/unRAID/comments/1jnf9y6/need_help_stopping_runaway_gpu_due_to_inferencing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnfb6h/need_help_stopping_runaway_gpu_due_to_inferencing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jnfb6h/need_help_stopping_runaway_gpu_due_to_inferencing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-30T15:47:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmrw67</id>
    <title>ollama inference 25% faster on Linux than windows</title>
    <updated>2025-03-29T17:48:02+00:00</updated>
    <author>
      <name>/u/AdhesivenessLatter57</name>
      <uri>https://old.reddit.com/user/AdhesivenessLatter57</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;running latest version of ollama 0.6.2 on both systems, updated windows 11 and latest build of kali Linux with kernel 3.11. python 3.12.9, pytorch 2.6, cuda 12.6 on both pc.&lt;/p&gt; &lt;p&gt;I have tested major under 8b models(llama3.2, gemma2, gemma3, qwen2.5 and mistral) available in ollama that inference is 25% faster on Linux pc than windows pc.&lt;/p&gt; &lt;p&gt;nividia quadro rtx 4000 8gb vram, 32gb ram, intel i7&lt;/p&gt; &lt;p&gt;is this a known fact? any benchmarking data or article on this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdhesivenessLatter57"&gt; /u/AdhesivenessLatter57 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmrw67/ollama_inference_25_faster_on_linux_than_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jmrw67/ollama_inference_25_faster_on_linux_than_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jmrw67/ollama_inference_25_faster_on_linux_than_windows/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-29T17:48:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnbr1c</id>
    <title>Looking for a ChatGPT-like Mac app that supports multiple AI models and MCP protocol</title>
    <updated>2025-03-30T12:52:23+00:00</updated>
    <author>
      <name>/u/MorpheusML</name>
      <uri>https://old.reddit.com/user/MorpheusML</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;I’ve been using the official ChatGPT app for Mac for quite some time now, and honestly, it’s fantastic. The Swift app is responsive, intuitive, and has many features that make it much nicer than the browser version. However, there’s one major limitation: It only works with OpenAI’s models. I’m looking for a similar desktop experience but with the ability to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Connect to Claude models (especially Sonnet 3.7)&lt;/li&gt; &lt;li&gt;Use local models via Ollama&lt;/li&gt; &lt;li&gt;Connect to MCP servers&lt;/li&gt; &lt;li&gt;Switch between different AI providers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’ve tried a few open-source alternatives (for example, &lt;a href="https://github.com/Renset/macai"&gt;https://github.com/Renset/macai&lt;/a&gt;), but none have matched the polish and user experience of the official ChatGPT app. I know browser-based solutions like OpenWebUI, but I prefer a native Mac application.&lt;/p&gt; &lt;p&gt;Do you know of a well-designed Mac app that fits these requirements?&lt;/p&gt; &lt;p&gt;Any recommendations would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MorpheusML"&gt; /u/MorpheusML &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnbr1c/looking_for_a_chatgptlike_mac_app_that_supports/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnbr1c/looking_for_a_chatgptlike_mac_app_that_supports/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jnbr1c/looking_for_a_chatgptlike_mac_app_that_supports/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-30T12:52:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jndb9h</id>
    <title>RAG and permissions broken?</title>
    <updated>2025-03-30T14:14:17+00:00</updated>
    <author>
      <name>/u/OrganizationHot731</name>
      <uri>https://old.reddit.com/user/OrganizationHot731</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone&lt;/p&gt; &lt;p&gt;Maybe my expectations on how things work are off... So please correct me if I am wrong&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I have 10 collections of knowledge loaded &lt;/li&gt; &lt;li&gt;I have a model that is to use the collection of knowledge (set in the settings of the model) &lt;/li&gt; &lt;li&gt;I have users loaded that have part of a group 4 that ground is restricted to only access 1-2 knowledge collections&lt;/li&gt; &lt;li&gt;I have the instructions for the model set to only answer questions from the data in the knowledge collections that is accessible by the user.&lt;br /&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Based on that when the user talks with the model it should ONLY reference the knowledge the users/group is assigned. Not all that is available to the model. &lt;/p&gt; &lt;p&gt;Instead the model is pulling data from all collections and not just the 2 that the user should be limited to in the group. &lt;/p&gt; &lt;p&gt;While I type # and only the collections assigned are correct, it's like the backend is ignoring that the user is restricted to that when the model has all knowledge collections.... &lt;/p&gt; &lt;p&gt;What am I missing? Or is something broken? &lt;/p&gt; &lt;p&gt;My end goal is to have 1 model that has access to all the collections but when a user asks it only uses data and references the collection the user has access to. &lt;/p&gt; &lt;p&gt;Example: - User is restricted to collection 3&amp;amp;5 - Model has 1-10 access in its settings - User asks a question that should only be available in collection 6 - Model will pull data from 6 and answer to user, when it shouldn't say it doesn't have access to that data. -User asks a question that's should be available in collection 5 - Model should answer fully without any restriction&lt;/p&gt; &lt;p&gt;Anyone have any idea what I'm missing or what I'm doing wrong. Or is something broken?? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OrganizationHot731"&gt; /u/OrganizationHot731 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jndb9h/rag_and_permissions_broken/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jndb9h/rag_and_permissions_broken/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jndb9h/rag_and_permissions_broken/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-30T14:14:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnjh1h</id>
    <title>Ollama on laptop with 2 GPU</title>
    <updated>2025-03-30T18:49:07+00:00</updated>
    <author>
      <name>/u/Successful_Power2125</name>
      <uri>https://old.reddit.com/user/Successful_Power2125</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, good day..is it possible for Olama to use the 2 GPUs in my computer since one is an AMD 780M and a dedicated Nvidia 4070? Thanks for your answers &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Successful_Power2125"&gt; /u/Successful_Power2125 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnjh1h/ollama_on_laptop_with_2_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnjh1h/ollama_on_laptop_with_2_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jnjh1h/ollama_on_laptop_with_2_gpu/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-30T18:49:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnbr3f</id>
    <title>Looking for a ChatGPT-like Mac app that supports multiple AI models and MCP protocol</title>
    <updated>2025-03-30T12:52:27+00:00</updated>
    <author>
      <name>/u/MorpheusML</name>
      <uri>https://old.reddit.com/user/MorpheusML</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;I’ve been using the official ChatGPT app for Mac for quite some time now, and honestly, it’s fantastic. The Swift app is responsive, intuitive, and has many features that make it much nicer than the browser version. However, there’s one major limitation: It only works with OpenAI’s models. I’m looking for a similar desktop experience but with the ability to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Connect to Claude models (especially Sonnet 3.7)&lt;/li&gt; &lt;li&gt;Use local models via Ollama&lt;/li&gt; &lt;li&gt;Connect to MCP servers&lt;/li&gt; &lt;li&gt;Switch between different AI providers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’ve tried a few open-source alternatives (for example, &lt;a href="https://github.com/Renset/macai"&gt;https://github.com/Renset/macai&lt;/a&gt;), but none have matched the polish and user experience of the official ChatGPT app. I know browser-based solutions like OpenWebUI, but I prefer a native Mac application.&lt;/p&gt; &lt;p&gt;Do you know of a well-designed Mac app that fits these requirements?&lt;/p&gt; &lt;p&gt;Any recommendations would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MorpheusML"&gt; /u/MorpheusML &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnbr3f/looking_for_a_chatgptlike_mac_app_that_supports/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnbr3f/looking_for_a_chatgptlike_mac_app_that_supports/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jnbr3f/looking_for_a_chatgptlike_mac_app_that_supports/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-30T12:52:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnjvwh</id>
    <title>Most of the models I have tried got it right. But baby llama triped over itself.</title>
    <updated>2025-03-30T19:06:49+00:00</updated>
    <author>
      <name>/u/tahaan</name>
      <uri>https://old.reddit.com/user/tahaan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1jnjvwh/most_of_the_models_i_have_tried_got_it_right_but/"&gt; &lt;img alt="Most of the models I have tried got it right. But baby llama triped over itself." src="https://preview.redd.it/gzui4kpxkvre1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24bfa3ba69eed11474b9e7da24d6ab5d4ed56cc2" title="Most of the models I have tried got it right. But baby llama triped over itself." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tahaan"&gt; /u/tahaan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gzui4kpxkvre1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnjvwh/most_of_the_models_i_have_tried_got_it_right_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jnjvwh/most_of_the_models_i_have_tried_got_it_right_but/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-30T19:06:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnnz1l</id>
    <title>Struggling with a simple summary bot</title>
    <updated>2025-03-30T22:05:42+00:00</updated>
    <author>
      <name>/u/why_not_my_email</name>
      <uri>https://old.reddit.com/user/why_not_my_email</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm still very new to Ollama. I'm trying to create a setup that returns a one-sentence summary of a document, as a stepping stone towards identifying and providing key quotations relevant to a project. &lt;/p&gt; &lt;p&gt;I've spent the last couple of hours playing around with different prompts, &lt;code&gt;system&lt;/code&gt; arguments, source documents, and models (primarily llama3.2, gemma3:12b, and a couple different sizes of deepseek-r1). In every case, the model gives a long, articulated summary (along with commentary about how the document is thoughtful or complex or whatever). &lt;/p&gt; &lt;p&gt;I'm using the &lt;code&gt;ollamar&lt;/code&gt; package, since I'm more comfortable with R than bash scripts. FWIW here's the current version: ``` library(ollamar) library(stringr) library(glue) library(pdftools) library(tictoc)&lt;/p&gt; &lt;p&gt;source = '/path/to/doc' |&amp;gt; readLines() |&amp;gt; str_c(collapse = '\n')&lt;/p&gt; &lt;p&gt;system = &amp;quot;You are an academic research assistant. The user will give you the text of a source document. Your job is to provide a one-sentence summary of the overall conclusion of the source. Do not include any other analysis or commentary.&amp;quot;&lt;/p&gt; &lt;p&gt;prompt = glue(&amp;quot;{source}&amp;quot;)&lt;/p&gt; &lt;p&gt;str_length(prompt) / 4&lt;/p&gt; &lt;p&gt;tic() resp = generate('llama3.2', system = system, prompt = prompt, output = 'resp', stream = TRUE, temperature = 0)&lt;/p&gt; &lt;h1&gt;resp = chat('gemma3:12b',&lt;/h1&gt; &lt;h1&gt;messages = list(&lt;/h1&gt; &lt;h1&gt;list(role = 'system', content = system),&lt;/h1&gt; &lt;h1&gt;list(role = 'user', content = prompt)),&lt;/h1&gt; &lt;h1&gt;output = 'text', stream = TRUE)&lt;/h1&gt; &lt;p&gt;toc() ```&lt;/p&gt; &lt;p&gt;Help? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/why_not_my_email"&gt; /u/why_not_my_email &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnnz1l/struggling_with_a_simple_summary_bot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnnz1l/struggling_with_a_simple_summary_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jnnz1l/struggling_with_a_simple_summary_bot/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-30T22:05:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jno67z</id>
    <title>API and Local file access</title>
    <updated>2025-03-30T22:14:49+00:00</updated>
    <author>
      <name>/u/LeeAnt74</name>
      <uri>https://old.reddit.com/user/LeeAnt74</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm very new to using Ollama but finally got to the point today where I was able to install the Web UI. However, two things are still causing me headaches.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;How do you use the API to send requests? I've been trying localhost:8080/api/chat and the same on 11414 without success.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Every time I attempt to get Ollama to examine files it tells me that I have to explicitly give authorisation. This makes sense but how do I do this?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Sorry, I'm sure these are going to appear to be problems with obvious answers but I've got nowhere and just ended up frustrated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeeAnt74"&gt; /u/LeeAnt74 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jno67z/api_and_local_file_access/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jno67z/api_and_local_file_access/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jno67z/api_and_local_file_access/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-30T22:14:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jngors</id>
    <title>Agent - A Local Computer-Use Operator for macOS</title>
    <updated>2025-03-30T16:48:19+00:00</updated>
    <author>
      <name>/u/sandropuppo</name>
      <uri>https://old.reddit.com/user/sandropuppo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've just open-sourced Agent, our framework for running computer-use workflows across multiple apps in isolated macOS/Linux sandboxes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Grab the code at&lt;/strong&gt; &lt;a href="https://github.com/trycua/cua"&gt;&lt;strong&gt;https://github.com/trycua/cua&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After launching Computer a few weeks ago, we realized many of you wanted to run complex workflows that span multiple applications. Agent builds on Computer to make this possible. It works with local Ollama models (if you're privacy-minded) or cloud providers like OpenAI, Anthropic, and others.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why we built this:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We kept hitting the same problems when building multi-app AI agents - they'd break in unpredictable ways, work inconsistently across environments, or just fail with complex workflows. So we built Agent to solve these headaches:&lt;/p&gt; &lt;p&gt;•⁠ ⁠It handles complex workflows across multiple apps without falling apart&lt;/p&gt; &lt;p&gt;•⁠ ⁠You can use your preferred model (local or cloud) - we're not locking you into one provider&lt;/p&gt; &lt;p&gt;•⁠ ⁠You can swap between different agent loop implementations depending on what you're building&lt;/p&gt; &lt;p&gt;•⁠ ⁠You get clean, structured responses that work well with other tools&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The code is pretty straightforward:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;async with Computer() as macos_computer:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;agent = ComputerAgent(&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;computer=macos_computer,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;loop=AgentLoop.OPENAI,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;model=LLM(provider=LLMProvider.OPENAI)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;tasks = [&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Look for a repository named trycua/cua on GitHub.&amp;quot;,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Check the open issues, open the most recent one and read it.&amp;quot;,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Clone the repository if it doesn't exist yet.&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;for i, task in enumerate(tasks):&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;print(f&amp;quot;\nTask {i+1}/{len(tasks)}: {task}&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;async for result in agent.run(task):&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;print(result)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;print(f&amp;quot;\nFinished task {i+1}!&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some cool things you can do with it:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;•⁠ ⁠Mix and match agent loops - OpenAI for some tasks, Claude for others, or try our experimental OmniParser&lt;/p&gt; &lt;p&gt;•⁠ ⁠Run it with various models - works great with OpenAI's computer_use_preview, but also with Claude and others&lt;/p&gt; &lt;p&gt;•⁠ ⁠Get detailed logs of what your agent is thinking/doing (super helpful for debugging)&lt;/p&gt; &lt;p&gt;•⁠ ⁠All the sandboxing from Computer means your main system stays protected&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Getting started is easy:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install &amp;quot;cua-agent[all]&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;# Or if you only need specific providers:&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install &amp;quot;cua-agent[openai]&amp;quot; # Just OpenAI&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install &amp;quot;cua-agent[anthropic]&amp;quot; # Just Anthropic&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install &amp;quot;cua-agent[omni]&amp;quot; # Our experimental OmniParser&lt;/code&gt;&lt;/p&gt; &lt;p&gt;We've been dogfooding this internally for weeks now, and it's been a game-changer for automating our workflows. &lt;/p&gt; &lt;p&gt;Would love to hear your thoughts ! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandropuppo"&gt; /u/sandropuppo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jngors/agent_a_local_computeruse_operator_for_macos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jngors/agent_a_local_computeruse_operator_for_macos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jngors/agent_a_local_computeruse_operator_for_macos/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-30T16:48:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnrmd9</id>
    <title>I built an open-source NotebookLM alternative using Morphik</title>
    <updated>2025-03-31T01:07:54+00:00</updated>
    <author>
      <name>/u/Advanced_Army4706</name>
      <uri>https://old.reddit.com/user/Advanced_Army4706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like using NoteBook LM, especially when I have a bunch of research papers I'm trying to extract insights from.&lt;/p&gt; &lt;p&gt;For example, if I'm implementing a new feature (like re-ranking) into Morphik, I like to create a notebook with some papers about it, and then compare those models with each other on different benchmarks.&lt;/p&gt; &lt;p&gt;I thought it would be cool to create a free, completely open-source version of it, so that I could use some private docs (like my journal!) and see if a NoteBook LM like system can help with that. I've found it to be insanely helpful, so I added a version of it onto the Morphik UI Component!&lt;/p&gt; &lt;p&gt;Try it out:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clone the repo at: &lt;a href="https://github.com/morphik-org/morphik-core"&gt;https://github.com/morphik-org/morphik-core&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Launch the UI component following instructions here: &lt;a href="https://docs.morphik.ai/using-morphik/morphik-ui"&gt;https://docs.morphik.ai/using-morphik/morphik-ui&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love to hear the &lt;a href="/r/ollama"&gt;r/ollama&lt;/a&gt; community's thoughts and feature requests!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Advanced_Army4706"&gt; /u/Advanced_Army4706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnrmd9/i_built_an_opensource_notebooklm_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1jnrmd9/i_built_an_opensource_notebooklm_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1jnrmd9/i_built_an_opensource_notebooklm_alternative/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-03-31T01:07:54+00:00</published>
  </entry>
</feed>
