<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-08-08T09:12:40+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1mjtgbi</id>
    <title>Crush AI Coding Agent Setup with Ollama - for GPT-OSS model</title>
    <updated>2025-08-07T07:19:08+00:00</updated>
    <author>
      <name>/u/NoobMLDude</name>
      <uri>https://old.reddit.com/user/NoobMLDude</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mjtgbi/crush_ai_coding_agent_setup_with_ollama_for/"&gt; &lt;img alt="Crush AI Coding Agent Setup with Ollama - for GPT-OSS model" src="https://external-preview.redd.it/dVF9vgZbDZkrkGBjDZh1ca-6h-GTLwZaN8Ys-xeskAc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55709d3e7002abc6951207193982d7c17b9105c4" title="Crush AI Coding Agent Setup with Ollama - for GPT-OSS model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoobMLDude"&gt; /u/NoobMLDude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/mdpPUoxCo44"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjtgbi/crush_ai_coding_agent_setup_with_ollama_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjtgbi/crush_ai_coding_agent_setup_with_ollama_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T07:19:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1miyj5e</id>
    <title>Ollama uses internet by default?</title>
    <updated>2025-08-06T07:50:22+00:00</updated>
    <author>
      <name>/u/icecoffee888</name>
      <uri>https://old.reddit.com/user/icecoffee888</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;so after using LM Studio for a while I finally decided to try Ollama now that I'm doing more local LLM coding (for privacy reasons), I was shocked when I saw docs URLs in ollama's thinking log.&lt;/p&gt; &lt;p&gt;Then I figured I have to go to settings an turn airplane mode on to be offline, shouldn't that be the default? is this new? what else is ollama sending to the internet, I have deleted it now, just pretty shocked.&lt;/p&gt; &lt;p&gt;EDIT: Thanks for the answers, it looks like this is new and part of their efforts to sell ollama Turbo, I hope they fail, I wont be trying Ollama again.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/icecoffee888"&gt; /u/icecoffee888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miyj5e/ollama_uses_internet_by_default/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1miyj5e/ollama_uses_internet_by_default/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1miyj5e/ollama_uses_internet_by_default/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T07:50:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mij9gu</id>
    <title>Open AI GPT-OSS:20b is bullshit</title>
    <updated>2025-08-05T19:45:48+00:00</updated>
    <author>
      <name>/u/Embarrassed-Way-1350</name>
      <uri>https://old.reddit.com/user/Embarrassed-Way-1350</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have just tried GPT-OSS:20b on my machine. This is the stupidest COT MOE model I have ever interacted with. Open AI chose to shit on the open-source community by releasing this abomination of a model.&lt;/p&gt; &lt;p&gt;Cannot perform basic arithmetic reasoning tasks, Thinks too much, and thinking traits remind me of deepseek-distill:70b, Would have been a great model 3 generations ago. As of today there are a ton of better models out there GLM is a far better alternative. Do not even try this model, Pure shit spray dried into fine powder.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed-Way-1350"&gt; /u/Embarrassed-Way-1350 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mij9gu/open_ai_gptoss20b_is_bullshit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mij9gu/open_ai_gptoss20b_is_bullshit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mij9gu/open_ai_gptoss20b_is_bullshit/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-05T19:45:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjvjyi</id>
    <title>SSL verification and URL change in ZScalar - model not downloading( wrong json error)</title>
    <updated>2025-08-07T09:33:56+00:00</updated>
    <author>
      <name>/u/Anxious-Resort1043</name>
      <uri>https://old.reddit.com/user/Anxious-Resort1043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The issue is I am unable to download the model using ollama pull and give me &amp;lt;‚Äú error at the end of json. &lt;/p&gt; &lt;p&gt;I tried the same with gpt4all and was able to fix that in their source code where it downloads the json from a url. Since my firm uses ZScalar , it starts a thing called isolated environment which probably gives the bad json error. &lt;/p&gt; &lt;p&gt;Anyone been able to workaround ZScalar ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxious-Resort1043"&gt; /u/Anxious-Resort1043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjvjyi/ssl_verification_and_url_change_in_zscalar_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjvjyi/ssl_verification_and_url_change_in_zscalar_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjvjyi/ssl_verification_and_url_change_in_zscalar_model/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T09:33:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjb2vv</id>
    <title>I built an interactive and customizable open-source meeting assistant (runs locally)</title>
    <updated>2025-08-06T17:32:29+00:00</updated>
    <author>
      <name>/u/Square-Test-515</name>
      <uri>https://old.reddit.com/user/Square-Test-515</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mjb2vv/i_built_an_interactive_and_customizable/"&gt; &lt;img alt="I built an interactive and customizable open-source meeting assistant (runs locally)" src="https://external-preview.redd.it/NHRtcTBsemJwZmhmMf7so8CSE-8PmjQuJPM-OgOW72CEju6_3gCE3GVMC0Pl.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c578bfb2644350d52ce8c0b015b669f1b01084b1" title="I built an interactive and customizable open-source meeting assistant (runs locally)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;two friends and I built an open-source meeting assistant. We‚Äôre now at the stage where we have an MVP on GitHub that developers can try out (with just 2 terminal commands), and we‚Äôd love your feedback on what to improve. üëâ &lt;a href="https://github.com/joinly-ai/joinly"&gt;https://github.com/joinly-ai/joinly&lt;/a&gt; &lt;/p&gt; &lt;p&gt;There are (at least) two very nice things about the assistant: First, it is interactive, so it speaks with you and can solve tasks in real time. Second, it is customizable. Customizable, meaning that you can add your favorite MCP servers so you canaccess their functionality during meetings. In addition, you can also easily change the agent‚Äôs system prompt. The meeting assistant also comes with real-time transcription.&lt;/p&gt; &lt;p&gt;A bit more on the technical side: We built a joinly MCP server that enables AI agents to interact in meetings, providing them tools like speak_text, write_chat_message, and leave_meeting and as a resource, the meeting transcript. We connected a sample joinly agent as the MCP client. But you can also connect your own agent to our joinly MCP server to make it meeting-ready.&lt;/p&gt; &lt;p&gt;You can run everything locally using Whisper (STT), Kokoro (TTS), and OLLaMA (LLM). But it is all provider-agnostic, meaning you can also use external APIs like Deepgram for STT, ElevenLabs for TTS, and OpenAI as LLM. &lt;/p&gt; &lt;p&gt;We‚Äôre currently using the slogan: ‚ÄúAgentic Meeting Assistant beyond note-taking.‚Äù But we‚Äôre wondering: Do you have better ideas for a slogan? And what do you think about the concept?&lt;/p&gt; &lt;p&gt;Btw, we‚Äôre reaching for the stars right now, so if you like it, consider giving us a star on GitHub :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Square-Test-515"&gt; /u/Square-Test-515 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y3p4plzbpfhf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjb2vv/i_built_an_interactive_and_customizable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjb2vv/i_built_an_interactive_and_customizable/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T17:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj5h97</id>
    <title>Is this good enough to run Ollama models on my laptop?</title>
    <updated>2025-08-06T14:01:01+00:00</updated>
    <author>
      <name>/u/summitsc</name>
      <uri>https://old.reddit.com/user/summitsc</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mj5h97/is_this_good_enough_to_run_ollama_models_on_my/"&gt; &lt;img alt="Is this good enough to run Ollama models on my laptop?" src="https://preview.redd.it/aqlc28i0oehf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57aba9789460eeabdc7f177180238768d4258ef3" title="Is this good enough to run Ollama models on my laptop?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/summitsc"&gt; /u/summitsc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aqlc28i0oehf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mj5h97/is_this_good_enough_to_run_ollama_models_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mj5h97/is_this_good_enough_to_run_ollama_models_on_my/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T14:01:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjsure</id>
    <title>RAG Problem Map 2.0 ¬∑ debug local RAG stacks that run on Ollama (MIT, offline ready)</title>
    <updated>2025-08-07T06:41:32+00:00</updated>
    <author>
      <name>/u/wfgy_engine</name>
      <uri>https://old.reddit.com/user/wfgy_engine</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;### TL;DR &lt;/p&gt; &lt;p&gt;Running Llama 3 in Ollama is the easy part.&lt;br /&gt; Keeping retrieval sane is the hard part.&lt;br /&gt; Problem Map 2.0 puts the full RAG chain on one screen. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Docs ‚Üí parse ‚Üí chunk ‚Üí embed ‚Üí index ‚Üí retrieve ‚Üí prompt ‚Üí reason.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Each hop lights up with a diagnostic signal so you see *where* it breaks and *why* (ŒîS, E-resonance, coherence drift‚Ä¶). &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Everything is MIT licensed, works offline, no API keys.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;### Why Ollama users will care &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Works with local FAISS or Qdrant, no cloud callbacks&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Flags silent failures that only show up with small VRAM models&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Gives instant feedback when embeddings drift after a model swap&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Shows why multi-GB PDFs crash the context window and how to chunk them right&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If your current flow sometimes spits perfect answers and sometimes gibberish, this map tells you the exact hop that snapped.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;### What is new in 2.0 &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single-page flowchart with live semantic probes&lt;br /&gt;&lt;/li&gt; &lt;li&gt;ŒîS heat map plus Œª vectors for meaning shift in real time&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Three ready scripts: basic chat, hybrid search, ugly-JSON stress test&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Replayable edge cases: vector junk, wrong skips, context collapse&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All runnable on a laptop that can already load Ollama.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;### Inside the repo &lt;/p&gt; &lt;ul&gt; &lt;li&gt;RAG failure atlas with 16 reproduce-and-fix scripts&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Trace viewer backed by Tesseract so you see embeddings instead of guessing&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Adapters for FAISS, Chroma, Qdrant already wired for Ollama&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Weird-JSON corpus to blow up naive chunkers&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Metrics: ŒîS, Œª, E_r, œÉ_ctx, all pre-wired&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;### Links&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full Problem Map ‚Üí &lt;a href="https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md"&gt;https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Tesseract.js author starred the repo ‚Üí &lt;a href="https://github.com/bijection?tab=stars"&gt;https://github.com/bijection?tab=stars&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;MIT license. No telemetry.&lt;/strong&gt;&lt;br /&gt; Made possible with a star from the author of Tesseract.js.&lt;br /&gt; Break it, send a PR, it becomes 2.1.&lt;/p&gt; &lt;p&gt;Enjoy hacking, ship faster ^^&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wfgy_engine"&gt; /u/wfgy_engine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjsure/rag_problem_map_20_debug_local_rag_stacks_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjsure/rag_problem_map_20_debug_local_rag_stacks_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjsure/rag_problem_map_20_debug_local_rag_stacks_that/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T06:41:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk3sgi</id>
    <title>My 5070 Ti crashing again and again</title>
    <updated>2025-08-07T15:53:03+00:00</updated>
    <author>
      <name>/u/Rough_Philosopher877</name>
      <uri>https://old.reddit.com/user/Rough_Philosopher877</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rough_Philosopher877"&gt; /u/Rough_Philosopher877 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/nvidia/comments/1mk3rrj/my_5070_ti_crashing_again_and_again/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mk3sgi/my_5070_ti_crashing_again_and_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mk3sgi/my_5070_ti_crashing_again_and_again/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T15:53:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk5tou</id>
    <title>How to make the font smaller and change color of new GUI?</title>
    <updated>2025-08-07T17:09:09+00:00</updated>
    <author>
      <name>/u/TriodeTopologist</name>
      <uri>https://old.reddit.com/user/TriodeTopologist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The ollama GUI that released is just awful to look at, with huge font, huge empty space, and white. Has anyone found settings to make the font and spacing both smaller, and change background color?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TriodeTopologist"&gt; /u/TriodeTopologist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mk5tou/how_to_make_the_font_smaller_and_change_color_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mk5tou/how_to_make_the_font_smaller_and_change_color_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mk5tou/how_to_make_the_font_smaller_and_change_color_of/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T17:09:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjsi21</id>
    <title>Evolution Tree of Encoder / Decoder / Encoder-Decoder Models.</title>
    <updated>2025-08-07T06:19:32+00:00</updated>
    <author>
      <name>/u/theMonarch776</name>
      <uri>https://old.reddit.com/user/theMonarch776</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mjsi21/evolution_tree_of_encoder_decoder_encoderdecoder/"&gt; &lt;img alt="Evolution Tree of Encoder / Decoder / Encoder-Decoder Models." src="https://preview.redd.it/m087zu1gajhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f63bd45ac59bf4cce9cbdd69df3bb8da15faf3c6" title="Evolution Tree of Encoder / Decoder / Encoder-Decoder Models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theMonarch776"&gt; /u/theMonarch776 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m087zu1gajhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjsi21/evolution_tree_of_encoder_decoder_encoderdecoder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjsi21/evolution_tree_of_encoder_decoder_encoderdecoder/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T06:19:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjijvj</id>
    <title>Model providing correct date. How?</title>
    <updated>2025-08-06T22:18:24+00:00</updated>
    <author>
      <name>/u/rob_0</name>
      <uri>https://old.reddit.com/user/rob_0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mjijvj/model_providing_correct_date_how/"&gt; &lt;img alt="Model providing correct date. How?" src="https://preview.redd.it/qgm95fpi4hhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58652231a2353b4c6ada948a586c08a18baa0391" title="Model providing correct date. How?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sure there's a good reason, but as you can see I've not included any facts and functions in my system prompt. How is it able to output a correct date?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rob_0"&gt; /u/rob_0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qgm95fpi4hhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjijvj/model_providing_correct_date_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjijvj/model_providing_correct_date_how/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-06T22:18:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mka8qm</id>
    <title>Ollama error loading GLM-4.5 Air GGUF. It says it ‚Äúsharted‚Äù ?ü§∑‚Äç‚ôÇÔ∏è</title>
    <updated>2025-08-07T19:55:54+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried to load the GGUF from the hugging face repo using the Ollama instructions on HF but it says something like the GGUF was sharded or sharted itself while loading and Ollama didn‚Äôt like that. The buzz around this model is pretty good, but i guess it‚Äôs not working with Ollama yet? Does anyone know when Ollama is going to support GLM-4.5? Isn‚Äôt this model supported by llama.cpp now? ü§∑‚Äç‚ôÇÔ∏è. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mka8qm/ollama_error_loading_glm45_air_gguf_it_says_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mka8qm/ollama_error_loading_glm45_air_gguf_it_says_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mka8qm/ollama_error_loading_glm45_air_gguf_it_says_it/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T19:55:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkby9u</id>
    <title>Ollama uses only 16GB of VRAM even 20GB available?</title>
    <updated>2025-08-07T21:01:26+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;br /&gt; In Ubuntu why this:&lt;br /&gt; gemma_production:latest 5fda5e3d6007 21 GB 14%/86% CPU/GPU 2048 36 minutes from now&lt;br /&gt; the GPU is nvidia ada 4000 sff and it has 20GB of vram. Why it uses only 16GB of it and puts 5GB to slow RAM? gemma3 27b, ollama version is 0.10.1&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[Service] Environment=&amp;quot;OLLAMA_HOST=0.0.0.0:11434&amp;quot; Environment=&amp;quot;OLLAMA_CONTEXT_LENGTH=2048&amp;quot; Environment=&amp;quot;OLLAMA_NUM_PARALLEL=1&amp;quot; Environment=&amp;quot;OLLAMA_FLASH_ATTENTION=1&amp;quot; Environment=&amp;quot;GGML_CUDA_ENABLE_UNIFIED_MEMORY=1&amp;quot; Environment=&amp;quot;OLLAMA_MAX_LOADED_MODELS=1&amp;quot; Environment=&amp;quot;OLLAMA_KEEP_ALIVE=2200&amp;quot; User=ubuntu Group=ubuntu &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkby9u/ollama_uses_only_16gb_of_vram_even_20gb_available/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkby9u/ollama_uses_only_16gb_of_vram_even_20gb_available/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mkby9u/ollama_uses_only_16gb_of_vram_even_20gb_available/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T21:01:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk79e3</id>
    <title>Looking for recommendation</title>
    <updated>2025-08-07T18:02:31+00:00</updated>
    <author>
      <name>/u/fliberdygibits</name>
      <uri>https://old.reddit.com/user/fliberdygibits</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Asking google gets answers across the spectrum so I thought I'd come right to the people. I'm looking to build a small dedicated LLM system on a 4 core 32gb mini-pc that will be always on. I need something conversational that will eventually be part of a home assistant setup. If it could help with the occasional very light coding or networking task that would be bonus but not necessarily. Most importantly I want it to be fast. I've tried a dozen different ollama models in the 1gb to 2gb range which is approx where I want to stay I think. Much over 2 starts to run kind of slowly in casual conversation.&lt;/p&gt; &lt;p&gt;What's a good current recommended go-to?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fliberdygibits"&gt; /u/fliberdygibits &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mk79e3/looking_for_recommendation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mk79e3/looking_for_recommendation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mk79e3/looking_for_recommendation/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T18:02:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjtd84</id>
    <title>gpt-oss thoughts</title>
    <updated>2025-08-07T07:13:46+00:00</updated>
    <author>
      <name>/u/cride20</name>
      <uri>https://old.reddit.com/user/cride20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share my experience with their open weight model..&lt;/p&gt; &lt;p&gt;I had some CMake issues (Not really familiar with it) and most LLMs like Gemini 2.5Pro, GPT free model with thinking couldn't really help me at all. They started hallucinating after 2-3error message.&lt;br /&gt; gpt-oss:20b just nailed the error in 1 prompt and fixed the whole building process..&lt;/p&gt; &lt;p&gt;So far it looks really promising while it's being really small (just 20b)&lt;/p&gt; &lt;p&gt;What are your thoughts so far?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cride20"&gt; /u/cride20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjtd84/gptoss_thoughts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjtd84/gptoss_thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjtd84/gptoss_thoughts/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T07:13:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkgzrc</id>
    <title>I am new to running things locally. Is this normal?</title>
    <updated>2025-08-08T00:35:30+00:00</updated>
    <author>
      <name>/u/RESPECTATOR_DE_FEMEI</name>
      <uri>https://old.reddit.com/user/RESPECTATOR_DE_FEMEI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mkgzrc/i_am_new_to_running_things_locally_is_this_normal/"&gt; &lt;img alt="I am new to running things locally. Is this normal?" src="https://preview.redd.it/u72aul46yohf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b26698405ac103b120fedd8630d294a320eb149e" title="I am new to running things locally. Is this normal?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RESPECTATOR_DE_FEMEI"&gt; /u/RESPECTATOR_DE_FEMEI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u72aul46yohf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkgzrc/i_am_new_to_running_things_locally_is_this_normal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mkgzrc/i_am_new_to_running_things_locally_is_this_normal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T00:35:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjxd87</id>
    <title>gpt-oss-20b running on i5 Iris Xe graphics with 16 gb ram.</title>
    <updated>2025-08-07T11:20:16+00:00</updated>
    <author>
      <name>/u/Western_Art_3308</name>
      <uri>https://old.reddit.com/user/Western_Art_3308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mjxd87/gptoss20b_running_on_i5_iris_xe_graphics_with_16/"&gt; &lt;img alt="gpt-oss-20b running on i5 Iris Xe graphics with 16 gb ram." src="https://preview.redd.it/zg28ifvd0lhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8921388569ea35dfe3d1ca4612738bd5072d2883" title="gpt-oss-20b running on i5 Iris Xe graphics with 16 gb ram." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Western_Art_3308"&gt; /u/Western_Art_3308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zg28ifvd0lhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjxd87/gptoss20b_running_on_i5_iris_xe_graphics_with_16/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjxd87/gptoss20b_running_on_i5_iris_xe_graphics_with_16/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T11:20:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkc2e3</id>
    <title>Looking for suggestions on a coding assistant model available in Ollama</title>
    <updated>2025-08-07T21:05:46+00:00</updated>
    <author>
      <name>/u/0x426C797A</name>
      <uri>https://old.reddit.com/user/0x426C797A</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm new to the AI game and I need a coding assistant to model. I was always told claude and Gemini are great but I cannot find them within ollama so I'm looking for suggestions &lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/0x426C797A"&gt; /u/0x426C797A &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkc2e3/looking_for_suggestions_on_a_coding_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkc2e3/looking_for_suggestions_on_a_coding_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mkc2e3/looking_for_suggestions_on_a_coding_assistant/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T21:05:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkd67y</id>
    <title>Actually good benchmarks</title>
    <updated>2025-08-07T21:49:29+00:00</updated>
    <author>
      <name>/u/Mr-Barack-Obama</name>
      <uri>https://old.reddit.com/user/Mr-Barack-Obama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spend a lot of time making private benchmarks for my real world use cases. It's extremely important to create your own unique benchmark for the specific tasks you will be using ai for, but we all know it's helpful to look at other benchmarks too. I think we've all found many benchmarks to not mean much in the real world, but I've found 2 benchmarks that when combined correlate accurately to real world intelligence and capability.&lt;/p&gt; &lt;p&gt;First lets start with livebench.ai . Besides livebench.ai 's coding benchmark, which I always turn off when looking at the total average scores, their total average score is often very accurate to real world use cases. All of their benchmarks combined into one average score tell a great story for how capable the model is. However, the only way that Livebench lacks is that it seems to only test at very short context lengths.&lt;/p&gt; &lt;p&gt;This is where another benchmark comes in, &lt;a href="https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87"&gt;https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87&lt;/a&gt; From a website about fiction writing and while it's not a super serious website, it is the best benchmark for real world long context. No one comes close. For example, I noticed Sonnet 4 performing much better than Opus 4 on context windows over 4,000 words. ONLY the Fiction Live benchmark reliably shows real world long context performance like this.&lt;/p&gt; &lt;p&gt;To estimate real world intelligence, I've found it very accurate to combine the results of both:&lt;/p&gt; &lt;p&gt;- &amp;quot;Fiction Live&amp;quot;: &lt;a href="https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87"&gt;https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- &amp;quot;Livebench&amp;quot;: &lt;a href="https://livebench.ai"&gt;https://livebench.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For models that many people run locally, not enough are represented on Livebench or Fiction Live. For example, GPT OSS 20b has not been tested on these benchmarks and it will likely be one of the most widely used open source models ever.&lt;/p&gt; &lt;p&gt;Livebench seems to have a responsive github. We should make posts politely asking for more models to be tested.&lt;/p&gt; &lt;p&gt;Livebench github: &lt;a href="https://github.com/LiveBench/LiveBench/issues"&gt;https://github.com/LiveBench/LiveBench/issues&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also on X, &lt;a href="/u/bindureddy"&gt;u/bindureddy&lt;/a&gt; runs the benchmark and is even more responsive to comments. I think we should make an effort to express that we want more models tested. It's totally worth trying!&lt;/p&gt; &lt;p&gt;FYI I wrote this by hand because I'm so passionate about benchmarks, no ai lol.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr-Barack-Obama"&gt; /u/Mr-Barack-Obama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkd67y/actually_good_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkd67y/actually_good_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mkd67y/actually_good_benchmarks/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T21:49:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjo9ki</id>
    <title>Best models under 16GB</title>
    <updated>2025-08-07T02:31:33+00:00</updated>
    <author>
      <name>/u/Mr-Barack-Obama</name>
      <uri>https://old.reddit.com/user/Mr-Barack-Obama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a macbook m4 pro with 16gb ram so I've made a list of the best models that should be able to run on it. I will be using llama.cpp without GUI for max efficiency but even still some of these quants might be too large to have enough space for reasoning tokens and some context, idk I'm a noob.&lt;/p&gt; &lt;p&gt;Here are the best models and quants for under 16gb based on my research, but I'm a noob and I haven't tested these yet:&lt;/p&gt; &lt;p&gt;Best Reasoning: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Qwen3-32B (IQ3_XXS 12.8 GB)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Qwen3-30B-A3B-Thinking-2507 (IQ3_XS 12.7GB)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Qwen 14B (Q6_K_L 12.50GB)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;gpt-oss-20b (12GB)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Phi-4-reasoning-plus (Q6_K_L 12.3 GB)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Best non reasoning:&lt;br /&gt; 1. gemma-3-27b (IQ4_XS 14.77GB)&lt;br /&gt; 2. Mistral-Small-3.2-24B-Instruct-2506 (Q4_K_L 14.83GB)&lt;br /&gt; 3. gemma-3-12b (Q8_0 12.5 GB) &lt;/p&gt; &lt;p&gt;My use cases:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Accurately summarizing meeting transcripts.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Creating an anonymized/censored version of a a document by removing confidential info while keeping everything else the same.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Asking survival questions for scenarios without internet like camping. I think medgemma-27b-text would be cool for this scenario.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I prefer maximum accuracy and intelligence over speed. How's my list and quants for my use cases? Am I missing any model or have something wrong? Any advice for getting the best performance with llama.cpp on a macbook m4pro 16gb?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr-Barack-Obama"&gt; /u/Mr-Barack-Obama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjo9ki/best_models_under_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mjo9ki/best_models_under_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mjo9ki/best_models_under_16gb/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T02:31:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkm54t</id>
    <title>Gpt-oss not working with Cline - Ollama, vLLM, Llamaccp</title>
    <updated>2025-08-08T04:50:02+00:00</updated>
    <author>
      <name>/u/Holiday_Purpose_3166</name>
      <uri>https://old.reddit.com/user/Holiday_Purpose_3166</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi peeps,&lt;/p&gt; &lt;p&gt;Leave this post for posterity. &lt;/p&gt; &lt;p&gt;Seems like the new Openai open-source models don't work very well with Cline, if you're running them locally. &lt;/p&gt; &lt;p&gt;My inference via Ollama works perfectly fine on CLI and Open WebUI, but Cline is practically broken due to harmony style prompt from the model. &lt;/p&gt; &lt;p&gt;Some users complain the same with vLLM and Llamaccp over at Github.&lt;/p&gt; &lt;p&gt;Other notes from Ollama. Changing KV Cache or Flash Attention doesn't seem to make any effect with the model's new quant MXFP4.&lt;/p&gt; &lt;p&gt;Other popular quants (e.g Q1...) don't seem the load either, questioning why these exist in the first place from popular teams. &lt;/p&gt; &lt;p&gt;The MXFP4 is the only one that loads, and despite being a small model, the memory footprint is large. I was constrained 55k context window with RTX 5090. &lt;/p&gt; &lt;p&gt;I'd like to use the model with Cline as it seems to fit my workflow, as my own test prompts via Open Router, beats Qwen3 30B series out of the water.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Purpose_3166"&gt; /u/Holiday_Purpose_3166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkm54t/gptoss_not_working_with_cline_ollama_vllm_llamaccp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkm54t/gptoss_not_working_with_cline_ollama_vllm_llamaccp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mkm54t/gptoss_not_working_with_cline_ollama_vllm_llamaccp/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T04:50:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk1xbe</id>
    <title>Probably dumb question: why doesn't Ollama forWindows work in airplane mode?</title>
    <updated>2025-08-07T14:42:05+00:00</updated>
    <author>
      <name>/u/ImAProAtSomeStuff</name>
      <uri>https://old.reddit.com/user/ImAProAtSomeStuff</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ImAProAtSomeStuff"&gt; /u/ImAProAtSomeStuff &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="/r/LocalLLaMA/comments/1mk1jwk/probably_dumb_question_why_doesnt_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mk1xbe/probably_dumb_question_why_doesnt_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mk1xbe/probably_dumb_question_why_doesnt_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T14:42:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkg49d</id>
    <title>Just released v1 of my open-source CLI app for coding locally: Nanocoder</title>
    <updated>2025-08-07T23:55:07+00:00</updated>
    <author>
      <name>/u/willlamerton</name>
      <uri>https://old.reddit.com/user/willlamerton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mkg49d/just_released_v1_of_my_opensource_cli_app_for/"&gt; &lt;img alt="Just released v1 of my open-source CLI app for coding locally: Nanocoder" src="https://external-preview.redd.it/Drrvz-4kHMvi4lMmu6VO9fxf9_IMIOSmZHpoZGn5meI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb9f517488bd723213b3f631a62e468793339cc9" title="Just released v1 of my open-source CLI app for coding locally: Nanocoder" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I just released version 1.0 of Nanocoder, a CLI tool I‚Äôve been building to make it easier to code agentically with large language models locally with Ollama and OpenRouter in your terminal to offer a similar experience to Claude Code and Gemini CLI. For me terminal experiences feel cleaner and more flexible so that‚Äôs where I was going with this.&lt;/p&gt; &lt;p&gt;Right now it‚Äôs very much MVP stage - works well enough to be useful, but rough around the edges. I want to polish it, add more features (better context handling, more tools, improved UX), and make it truly awesome for coding work.&lt;/p&gt; &lt;p&gt;I‚Äôm a big believer in AI being open and for the people, not locked behind subscriptions or proprietary APIs. That‚Äôs why it‚Äôs open source, I‚Äôm hoping to build it as a community.&lt;/p&gt; &lt;p&gt;If you think this is cool, I‚Äôd be grateful for GitHub stars and contributors to help shape where it goes next. Feedback, feature ideas, bug reports - all welcome!&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="https://github.com/Mote-Software/nanocoder"&gt;https://github.com/Mote-Software/nanocoder&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/willlamerton"&gt; /u/willlamerton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Mote-Software/nanocoder"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mkg49d/just_released_v1_of_my_opensource_cli_app_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mkg49d/just_released_v1_of_my_opensource_cli_app_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-07T23:55:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mklsm5</id>
    <title>GPT-5 style router, but for local models</title>
    <updated>2025-08-08T04:31:18+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mklsm5/gpt5_style_router_but_for_local_models/"&gt; &lt;img alt="GPT-5 style router, but for local models" src="https://preview.redd.it/k9oxbvcd3qhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d15425761a134887e8b496ee976b02a5a1b4acfd" title="GPT-5 style router, but for local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPT-5 launched today, which is essentially different model underneath the covers abstracted away by a real-time router. In June, we published our &lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;preference-aligned routing model&lt;/a&gt; and &lt;a href="https://github.com/katanemo/archgw"&gt;framework&lt;/a&gt; for developers so that they can build a best-of-breed agentic experience with choice of models they care about. &lt;/p&gt; &lt;p&gt;Sharing the research and framework again, as it might be helpful to developers looking for similar tools.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k9oxbvcd3qhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mklsm5/gpt5_style_router_but_for_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mklsm5/gpt5_style_router_but_for_local_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T04:31:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mklwrl</id>
    <title>GPT 5 for Computer Use agents.</title>
    <updated>2025-08-08T04:37:34+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1mklwrl/gpt_5_for_computer_use_agents/"&gt; &lt;img alt="GPT 5 for Computer Use agents." src="https://external-preview.redd.it/amZianVtMGc1cWhmMffa9LUhs6wvp7jU6XPjtPFZB1S0k_8zNod6eLcZn2nM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2533cb96234ebc6fab32f27912ae124582ad0b00" title="GPT 5 for Computer Use agents." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Same tasks, same grounding model we just swapped GPT 4o with GPT 5 as the thinking model. &lt;/p&gt; &lt;p&gt;Left = 4o, right = 5. &lt;/p&gt; &lt;p&gt;Watch GPT 5 pull away.&lt;/p&gt; &lt;p&gt;Try it yourself here : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs : &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agents"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wxbv78dg5qhf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1mklwrl/gpt_5_for_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1mklwrl/gpt_5_for_computer_use_agents/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-08-08T04:37:34+00:00</published>
  </entry>
</feed>
