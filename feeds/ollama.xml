<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/ollama/.rss</id>
  <title>ollama</title>
  <updated>2025-02-07T14:49:03+00:00</updated>
  <link href="https://old.reddit.com/r/ollama/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Atom feed for r/ollama</subtitle>
  <entry>
    <id>t3_1ije4bw</id>
    <title>Rtx 3050 + i5-11th isn't enough?!</title>
    <updated>2025-02-06T21:33:23+00:00</updated>
    <author>
      <name>/u/Brief_Ticket4771</name>
      <uri>https://old.reddit.com/user/Brief_Ticket4771</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have dell g15 5511 with RTX 3050 4gb vram and i5 11th 16 RAM which local ai model from ollama is good for coding and what about performance? and will it need additional RAM ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brief_Ticket4771"&gt; /u/Brief_Ticket4771 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ije4bw/rtx_3050_i511th_isnt_enough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ije4bw/rtx_3050_i511th_isnt_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ije4bw/rtx_3050_i511th_isnt_enough/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T21:33:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijfdun</id>
    <title>NPU Support?</title>
    <updated>2025-02-06T22:26:15+00:00</updated>
    <author>
      <name>/u/TheMicrosoftMan</name>
      <uri>https://old.reddit.com/user/TheMicrosoftMan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a Surface Pro 11 with a Snapdragon X Elite chip, which has a NPU with 40 TOPS which would be a lot more power efficient and could be faster than the built in CPU and GPU. A couple months ago, Ollama didn't support NPUs, has that changed? Is there any way to add support in any method?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheMicrosoftMan"&gt; /u/TheMicrosoftMan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijfdun/npu_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijfdun/npu_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijfdun/npu_support/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T22:26:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij23pm</id>
    <title>Can a malicious model execute code ?</title>
    <updated>2025-02-06T13:07:27+00:00</updated>
    <author>
      <name>/u/niilzon</name>
      <uri>https://old.reddit.com/user/niilzon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Could a hypothetical malicious model (such as Deepseek or any other) execute code on a machine running Ollama (without exploiting a vulnerability in Ollama) ?&lt;/p&gt; &lt;p&gt;It's not clear to me if models used by Ollama are &amp;quot;just weights&amp;quot; or if they also embed custom dependencies, executables etc&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/niilzon"&gt; /u/niilzon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij23pm/can_a_malicious_model_execute_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij23pm/can_a_malicious_model_execute_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij23pm/can_a_malicious_model_execute_code/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T13:07:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij7dmw</id>
    <title>Please help me out with this issue in downloading ollama deepseek model</title>
    <updated>2025-02-06T17:00:47+00:00</updated>
    <author>
      <name>/u/oye_ap</name>
      <uri>https://old.reddit.com/user/oye_ap</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ij7dmw/please_help_me_out_with_this_issue_in_downloading/"&gt; &lt;img alt="Please help me out with this issue in downloading ollama deepseek model" src="https://b.thumbs.redditmedia.com/QQEzKHUA0gBIZdUfvktJsyHlCUrLE0zeG37WUPx9imo.jpg" title="Please help me out with this issue in downloading ollama deepseek model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's my first time trying to download ollama and deepseek, I'm not much into tech &amp;amp; while I try to run deepseek, this is the issue that I'm facing&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r8c1h6r66khe1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d9f7fdce5488ae0a9a3554d584c0855a516ee8a5"&gt;https://preview.redd.it/r8c1h6r66khe1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d9f7fdce5488ae0a9a3554d584c0855a516ee8a5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oye_ap"&gt; /u/oye_ap &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij7dmw/please_help_me_out_with_this_issue_in_downloading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij7dmw/please_help_me_out_with_this_issue_in_downloading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij7dmw/please_help_me_out_with_this_issue_in_downloading/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T17:00:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijbn6m</id>
    <title>Best EU alternative for lambda labs to run ollama</title>
    <updated>2025-02-06T19:54:00+00:00</updated>
    <author>
      <name>/u/SelectSpread</name>
      <uri>https://old.reddit.com/user/SelectSpread</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;we deployed ollama on lambda labs servers. Our CEO is not amused, as lambda labs is a U.S. company (cloud act, etc. not perfect when it comes to GDPR)&lt;/p&gt; &lt;p&gt;What's the best EU alternative to run on demand workloads on at least Nvidia (G)H200 GPU&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SelectSpread"&gt; /u/SelectSpread &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijbn6m/best_eu_alternative_for_lambda_labs_to_run_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijbn6m/best_eu_alternative_for_lambda_labs_to_run_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijbn6m/best_eu_alternative_for_lambda_labs_to_run_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T19:54:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijog7g</id>
    <title>Can my laptop runs Ollama locally?</title>
    <updated>2025-02-07T06:10:05+00:00</updated>
    <author>
      <name>/u/Jealous-Wafer-8239</name>
      <uri>https://old.reddit.com/user/Jealous-Wafer-8239</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I using my laptop to work in my office, bought it 3 years ago.&lt;br /&gt; Model: MSI Bravo 15 B5DD&lt;br /&gt; CPU: AMD Ryzen 7 5800H&lt;br /&gt; GPU: AMD RX 5500M 4G&lt;br /&gt; RAM: 32G&lt;/p&gt; &lt;p&gt;I want run model such as DeepSeek or Qwen&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jealous-Wafer-8239"&gt; /u/Jealous-Wafer-8239 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijog7g/can_my_laptop_runs_ollama_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijog7g/can_my_laptop_runs_ollama_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijog7g/can_my_laptop_runs_ollama_locally/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T06:10:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij942k</id>
    <title>Could i run deepseek 16B Rtx 3070 with GTX 1080Ti?</title>
    <updated>2025-02-06T18:10:49+00:00</updated>
    <author>
      <name>/u/Efficient_Band181</name>
      <uri>https://old.reddit.com/user/Efficient_Band181</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 2 of these cards laying around as I havent done much gaming lately, the GTX is older but i thought might as well because of the 11GB of Vram.&lt;/p&gt; &lt;p&gt;Could it run at least the 16B model in parallel? Or should I just use the lesser 8GB 3070 I have alone, just want to make sure before buying the motherboard.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Efficient_Band181"&gt; /u/Efficient_Band181 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij942k/could_i_run_deepseek_16b_rtx_3070_with_gtx_1080ti/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij942k/could_i_run_deepseek_16b_rtx_3070_with_gtx_1080ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij942k/could_i_run_deepseek_16b_rtx_3070_with_gtx_1080ti/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T18:10:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij71ow</id>
    <title>[Help] Why I can't pull this model from offical ollama model page?</title>
    <updated>2025-02-06T16:47:22+00:00</updated>
    <author>
      <name>/u/josephwang123</name>
      <uri>https://old.reddit.com/user/josephwang123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to pull this model: &lt;a href="https://ollama.com/daiseur/darkidol-llama-3.1-8b_instruct-1.2"&gt;https://ollama.com/daiseur/darkidol-llama-3.1-8b_instruct-1.2&lt;/a&gt;&lt;br /&gt; Both &lt;code&gt;ollama pull&lt;/code&gt; and &lt;code&gt;ollama serve&lt;/code&gt; is failed.&lt;br /&gt; Can anybody help?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/josephwang123"&gt; /u/josephwang123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij71ow/help_why_i_cant_pull_this_model_from_offical/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij71ow/help_why_i_cant_pull_this_model_from_offical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij71ow/help_why_i_cant_pull_this_model_from_offical/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T16:47:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij345i</id>
    <title>Can anyone help me? i really don't understand what i'm doing wrong.</title>
    <updated>2025-02-06T13:57:13+00:00</updated>
    <author>
      <name>/u/RevolutionaryBus4545</name>
      <uri>https://old.reddit.com/user/RevolutionaryBus4545</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ij345i/can_anyone_help_me_i_really_dont_understand_what/"&gt; &lt;img alt="Can anyone help me? i really don't understand what i'm doing wrong." src="https://preview.redd.it/gurn3vbgyihe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81c1c17805ce018b0fc08afcf7535c8018e411f7" title="Can anyone help me? i really don't understand what i'm doing wrong." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RevolutionaryBus4545"&gt; /u/RevolutionaryBus4545 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gurn3vbgyihe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij345i/can_anyone_help_me_i_really_dont_understand_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij345i/can_anyone_help_me_i_really_dont_understand_what/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T13:57:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijor4r</id>
    <title>Any ideas to wade thru dependency hell using multiple tools?</title>
    <updated>2025-02-07T06:29:55+00:00</updated>
    <author>
      <name>/u/shrimp_allergy_maybe</name>
      <uri>https://old.reddit.com/user/shrimp_allergy_maybe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hiiii, what do you all do if you want to use multiple packages/libraries but the dependencies conflict with one another?? I would love any ideas or just commiseration &lt;/p&gt; &lt;p&gt;Ex: I read a recent paper on BioChatter and wanted to try it out but its dependencies conflict with Open WebUI. One example is that they require completely diff versions of some lang-chain stuff 😭&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/biocypher/biochatter"&gt;https://github.com/biocypher/biochatter&lt;/a&gt; &lt;a href="https://github.com/open-webui/open-webui"&gt;https://github.com/open-webui/open-webui&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Is there a combo of conda, pip, pipx, poetry, venv, etc I can try? Also having diff versions of python available to use in the same environment? I'm on Windows btw &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shrimp_allergy_maybe"&gt; /u/shrimp_allergy_maybe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijor4r/any_ideas_to_wade_thru_dependency_hell_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijor4r/any_ideas_to_wade_thru_dependency_hell_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijor4r/any_ideas_to_wade_thru_dependency_hell_using/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T06:29:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijq45s</id>
    <title>Host ollama on cloud</title>
    <updated>2025-02-07T08:09:03+00:00</updated>
    <author>
      <name>/u/Physical_Horror_1549</name>
      <uri>https://old.reddit.com/user/Physical_Horror_1549</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I want to host uncensored quantized model on cloud. I don’t want an always on server. I can bear long latency for first token. Be billed for only what I use (time/token). It should be able to expose OAI compatible API (probably through fastapi) so I can use them in whatever chat client.&lt;/p&gt; &lt;p&gt;So far I didn’t find a good solution. Anyone suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Physical_Horror_1549"&gt; /u/Physical_Horror_1549 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijq45s/host_ollama_on_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijq45s/host_ollama_on_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijq45s/host_ollama_on_cloud/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T08:09:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijqoco</id>
    <title>One Quadro RTX 5000, or Two Quadro RTX 4000, for deepseek-coder-v2?</title>
    <updated>2025-02-07T08:51:57+00:00</updated>
    <author>
      <name>/u/alsutton</name>
      <uri>https://old.reddit.com/user/alsutton</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking to run &lt;a href="https://ollama.com/library/deepseek-coder-v2"&gt;deepseek-coder-v2&lt;/a&gt; locally (yes, I'm late to the game on that one), and it looks like the 16b param model won't fit into a single RTX 4000. My local eBay pricing is such that a single Turing based RTX 5000 is roughly 1.8 times the price of a Turing based RTX 4000, so I'm wondering if I should just spend a little extra, get 2xRTX 4000s, use them in my motherboards two 16 lane PCIe 3.0 slots, and bask in the joy of a lot more CUDA cores and the same RAM.&lt;/p&gt; &lt;p&gt;Is that reasonable, or is ollama limited to a single card for memory and/or processing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alsutton"&gt; /u/alsutton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijqoco/one_quadro_rtx_5000_or_two_quadro_rtx_4000_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijqoco/one_quadro_rtx_5000_or_two_quadro_rtx_4000_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijqoco/one_quadro_rtx_5000_or_two_quadro_rtx_4000_for/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T08:51:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijnqb9</id>
    <title>VRAM usage is still high after running /bye, how to release that capacity?</title>
    <updated>2025-02-07T05:25:28+00:00</updated>
    <author>
      <name>/u/SpecialistPear755</name>
      <uri>https://old.reddit.com/user/SpecialistPear755</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was running llama 3.2 3b on a laptop, u ubuntu.&lt;/p&gt; &lt;p&gt;After I run /bye and closed the termina, the Vram usage is still high like the model is still running. (Around 4or5 g)&lt;/p&gt; &lt;p&gt;Is there a way to release that burden?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpecialistPear755"&gt; /u/SpecialistPear755 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijnqb9/vram_usage_is_still_high_after_running_bye_how_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijnqb9/vram_usage_is_still_high_after_running_bye_how_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijnqb9/vram_usage_is_still_high_after_running_bye_how_to/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T05:25:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijmfao</id>
    <title>Can I run 70b with my RTX5090 (32GB GDDR7 VRAM) and 64GB DDR5-6000?</title>
    <updated>2025-02-07T04:10:02+00:00</updated>
    <author>
      <name>/u/AzysLla</name>
      <uri>https://old.reddit.com/user/AzysLla</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;CPU is 9800X3D.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AzysLla"&gt; /u/AzysLla &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijmfao/can_i_run_70b_with_my_rtx5090_32gb_gddr7_vram_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijmfao/can_i_run_70b_with_my_rtx5090_32gb_gddr7_vram_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijmfao/can_i_run_70b_with_my_rtx5090_32gb_gddr7_vram_and/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T04:10:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijh27y</id>
    <title>📝🧵 Introducing Text Loom: A Node-Based Text Processing Playground!</title>
    <updated>2025-02-06T23:39:49+00:00</updated>
    <author>
      <name>/u/kleer001</name>
      <uri>https://old.reddit.com/user/kleer001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;&lt;a href="https://github.com/kleer001/Text_Loom"&gt;TEXT LOOM!&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/kleer001/Text_Loom"&gt;https://github.com/kleer001/Text_Loom&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey text wranglers! 👋 Ever wanted to slice, dice, and weave text like a digital textile artist? &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/kleer001/Text_Loom/blob/main/images/leaderloop_trim_4.gif?raw=true"&gt;https://github.com/kleer001/Text_Loom/blob/main/images/leaderloop_trim_4.gif?raw=true&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Text Loom is your new best friend! It's a &lt;strong&gt;node-based workspace&lt;/strong&gt; where you can build awesome text processing pipelines by connecting simple, powerful nodes. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Want to split a script into scenes? Done. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Need to process a batch of files through an LLM? Easy peasy. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;How about automatically formatting numbered lists or merging multiple documents? We've got you covered! &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each node is like a tiny text-processing specialist: the &lt;a href="https://github.com/kleer001/Text_Loom/wiki/Section-Node"&gt;Section Node&lt;/a&gt; slices text based on patterns, the &lt;a href="https://github.com/kleer001/Text_Loom/wiki/Query-Node"&gt;Query Node&lt;/a&gt; talks to AI models, and the &lt;a href="https://github.com/kleer001/Text_Loom/wiki/Looper-Node"&gt;Looper Node&lt;/a&gt; handles all your iteration needs. &lt;/p&gt; &lt;p&gt;Mix and match to create your perfect text processing flow! Check out our &lt;a href="https://github.com/kleer001/Text_Loom/wiki"&gt;wiki&lt;/a&gt; to see what's possible. 🚀&lt;/p&gt; &lt;h2&gt;Why Terminal? Because Hackers Know Best! 💻&lt;/h2&gt; &lt;p&gt;Remember those awesome 1900's movies where hackers typed furiously on glowing green screens, making magic happen with just their keyboards? &lt;/p&gt; &lt;p&gt;&lt;em&gt;Turns out they were onto something!&lt;/em&gt; &lt;/p&gt; &lt;p&gt;While Text Loom's got a cool node-based interface, it's running on good old-fashioned terminal power. Just like Matthew Broderick in &lt;em&gt;WarGames&lt;/em&gt; or the crew in &lt;em&gt;Hackers&lt;/em&gt;, we're keeping it real with that sweet, sweet command line efficiency. No fancy GUI bloat, no mouse-hunting required – just you, your keyboard, and pure text-processing power. Want to feel like you're hacking the Gibson while actually getting real work done? We've got you covered! 🕹️&lt;/p&gt; &lt;p&gt;&lt;em&gt;Because text should flow, not fight you.&lt;/em&gt; ✨&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kleer001"&gt; /u/kleer001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijh27y/introducing_text_loom_a_nodebased_text_processing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijh27y/introducing_text_loom_a_nodebased_text_processing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijh27y/introducing_text_loom_a_nodebased_text_processing/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T23:39:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijtt8v</id>
    <title>Actually Benefiting from Structured Output Support with Ollama and LangChainJS</title>
    <updated>2025-02-07T12:25:00+00:00</updated>
    <author>
      <name>/u/Inevitable-Judge2642</name>
      <uri>https://old.reddit.com/user/Inevitable-Judge2642</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ijtt8v/actually_benefiting_from_structured_output/"&gt; &lt;img alt="Actually Benefiting from Structured Output Support with Ollama and LangChainJS" src="https://external-preview.redd.it/dtRSrdeN5WqSxGmu3zNM8x-79n94KvvGaZcUGyRp3w8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0317bb9986fdd1b1a3a33c47f4aa6b82315f80a6" title="Actually Benefiting from Structured Output Support with Ollama and LangChainJS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Judge2642"&gt; /u/Inevitable-Judge2642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://k33g.hashnode.dev/actually-benefiting-from-structured-output-support-with-ollama-and-langchainjs"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijtt8v/actually_benefiting_from_structured_output/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijtt8v/actually_benefiting_from_structured_output/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T12:25:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1iju74a</id>
    <title>Sharing Ollama models between users in macOS?</title>
    <updated>2025-02-07T12:47:37+00:00</updated>
    <author>
      <name>/u/joyfulsparrow</name>
      <uri>https://old.reddit.com/user/joyfulsparrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a way to shared Ollama models between users? They're pretty big, so I don't want to fill up the hard disk with duplicates. Can I put them in '/Users/Shared'?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joyfulsparrow"&gt; /u/joyfulsparrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iju74a/sharing_ollama_models_between_users_in_macos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1iju74a/sharing_ollama_models_between_users_in_macos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1iju74a/sharing_ollama_models_between_users_in_macos/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T12:47:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijuah4</id>
    <title>How to Handle Missing Parameters and Chained Tool Calls in LangChain with Ollama Llama 3.2:8B?</title>
    <updated>2025-02-07T12:52:56+00:00</updated>
    <author>
      <name>/u/MindIndividual4397</name>
      <uri>https://old.reddit.com/user/MindIndividual4397</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’ve built a simple call tool setup using Ollama Llama 3.2:8B and LangChain, but I’m facing some issues when calling tools that depend on each other.&lt;/p&gt; &lt;p&gt;Problem 1: Handling Missing Parameters&lt;/p&gt; &lt;p&gt;I have a tool user_status(user_id: int), which requires an integer user ID. However, when I say something like:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Check user status for test&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;LangChain doesn’t detect an integer in the prompt and instead assigns a random user ID like 1 or 1234.&lt;/p&gt; &lt;p&gt;How can I make it force the user to provide a user ID explicitly, instead of assuming a random one? Ideally, it should either ask for the missing parameter or refuse execution.&lt;/p&gt; &lt;p&gt;Problem 2: Automatically Resolving Dependencies&lt;/p&gt; &lt;p&gt;I also have another tool:&lt;/p&gt; &lt;p&gt;get_user_id(username: str) -&amp;gt; int&lt;/p&gt; &lt;p&gt;I want the system to automatically call get_user_id(&amp;quot;test&amp;quot;) first and use the returned value as input for user_status(user_id).&lt;/p&gt; &lt;p&gt;Do I need to implement a custom agent executor for this? If so, how can I handle similar cases when multiple tools depend on each other?&lt;/p&gt; &lt;p&gt;Would love to hear your approaches! Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MindIndividual4397"&gt; /u/MindIndividual4397 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijuah4/how_to_handle_missing_parameters_and_chained_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijuah4/how_to_handle_missing_parameters_and_chained_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijuah4/how_to_handle_missing_parameters_and_chained_tool/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T12:52:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij7nuo</id>
    <title>UNCENSORED AI MODELS</title>
    <updated>2025-02-06T17:12:07+00:00</updated>
    <author>
      <name>/u/yng_kydd</name>
      <uri>https://old.reddit.com/user/yng_kydd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some months ago i tried for the first time wizard vicuna and i was ok with it being a lil slow and not that optimized, i wasn't even complaining cause at least i had an AI uncensored, something i could ask for everything.&lt;/p&gt; &lt;p&gt;This week i've seen a post talking about other new models that are pretty much better like tiger gemma, dolphin and others&lt;/p&gt; &lt;p&gt;i've been searching about this for quite a lot and i'd want to ask y'all what is the best uncensored AI model right now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yng_kydd"&gt; /u/yng_kydd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij7nuo/uncensored_ai_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij7nuo/uncensored_ai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij7nuo/uncensored_ai_models/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T17:12:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijspg3</id>
    <title>Is it normal for ollama to use CPU when OLLAMA_KEEP_ALIVE=-1</title>
    <updated>2025-02-07T11:15:47+00:00</updated>
    <author>
      <name>/u/gmetothemoongodspeed</name>
      <uri>https://old.reddit.com/user/gmetothemoongodspeed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m using the Windows client and when setting OLLAMA_KEEP_ALIVE=-1 my CPU usage doesn’t stop at the end of the query. Is this normal? I would say it uses CPU for approximately 5 minutes after the query ends. Then the CPU drops to minimal as expected.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gmetothemoongodspeed"&gt; /u/gmetothemoongodspeed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijspg3/is_it_normal_for_ollama_to_use_cpu_when_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijspg3/is_it_normal_for_ollama_to_use_cpu_when_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijspg3/is_it_normal_for_ollama_to_use_cpu_when_ollama/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T11:15:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijs34u</id>
    <title>LLMs as Embeddings?</title>
    <updated>2025-02-07T10:33:09+00:00</updated>
    <author>
      <name>/u/Better-Designer-8904</name>
      <uri>https://old.reddit.com/user/Better-Designer-8904</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/ollama/comments/1ijs34u/llms_as_embeddings/"&gt; &lt;img alt="LLMs as Embeddings?" src="https://b.thumbs.redditmedia.com/ytaJOU70XQ_OdGIs6ff3q5cDHpWWu6pRAi8DYLo9CwY.jpg" title="LLMs as Embeddings?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/pu81il0r2phe1.png?width=832&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=539f8ec6328553c328a6bced0c730e87952fb5ba"&gt;https://preview.redd.it/pu81il0r2phe1.png?width=832&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=539f8ec6328553c328a6bced0c730e87952fb5ba&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been using LangChain to run LLMs as embeddings through Ollama, and it actually works pretty well. But I’m kinda wondering… how does it actually work? And does it even make sense to use an LLM for embeddings instead of a dedicated model?&lt;/p&gt; &lt;p&gt;If anyone understands the details, I’d love an explanation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Better-Designer-8904"&gt; /u/Better-Designer-8904 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijs34u/llms_as_embeddings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijs34u/llms_as_embeddings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijs34u/llms_as_embeddings/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T10:33:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij2pw7</id>
    <title>🎉 Being Thankful for Everyone Who Made This Project a Super Hit! 🚀</title>
    <updated>2025-02-06T13:37:59+00:00</updated>
    <author>
      <name>/u/akhilpanja</name>
      <uri>https://old.reddit.com/user/akhilpanja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are thrilled to announce that our project, DeepSeek-RAG-Chatbot, has officially hit 100 stars on GitHub repo: &lt;a href="https://github.com/SaiAkhil066/DeepSeek-RAG-Chatbot.git"&gt;https://github.com/SaiAkhil066/DeepSeek-RAG-Chatbot.git&lt;/a&gt; 🌟✨ &lt;/p&gt; &lt;p&gt;This journey has been incredible, and we couldn’t have achieved this milestone without the support of our amazing community. Your contributions, feedback, and enthusiasm have helped shape this project into what it is today!&lt;/p&gt; &lt;p&gt;🔍 Performance Boost The graph above showcases the significant improvements in Graph Context Relevancy and Graph Context Recall after integrating GraphRAG and further advancements. Our system is now more accurate, contextually aware, and efficient in retrieving relevant information.&lt;/p&gt; &lt;p&gt;We are committed to making this project even better and look forward to the next milestones! 🚀&lt;/p&gt; &lt;p&gt;Thank you all once again for being part of this journey. Let’s keep building together! 💡🔥 ￼&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akhilpanja"&gt; /u/akhilpanja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij2pw7/being_thankful_for_everyone_who_made_this_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ij2pw7/being_thankful_for_everyone_who_made_this_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ij2pw7/being_thankful_for_everyone_who_made_this_project/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-06T13:37:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijwk1c</id>
    <title>How can you utilize DeepSeek R1 for personal productivity?</title>
    <updated>2025-02-07T14:44:19+00:00</updated>
    <author>
      <name>/u/Ok-Ebb-1486</name>
      <uri>https://old.reddit.com/user/Ok-Ebb-1486</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;I always wanted to collect statistics about my productivity on the computer. This idea is not new; there are plenty of apps designed to solve this issue. However, all of them have one significant caveat: you must send highly sensitive and personal information about ALL your activity to “BIG BROTHER” and trust that your data won’t end up in the hands of personal data reselling firms. That’s why I decided to create one myself and make it 100% open-source for complete transparency and trustworthiness — and you can use it too!&lt;/h1&gt; &lt;p&gt;Understanding your productivity focus over a long period of time is essential because it provides valuable insights into how you allocate your time, identify patterns in your workflow, and discover areas for improvement. Long-term productivity tracking can help you pinpoint activities that consistently contribute to your goals and those that drain your time and energy without meaningful results.&lt;/p&gt; &lt;p&gt;For example, tracking your productivity trends can reveal whether you’re more effective during certain times of the day or in specific environments. It can also help you assess the long-term impact of adjustments, like changing your schedule, adopting new tools, or tackling procrastination. This data-driven approach not only empowers you to optimize your daily routines but also helps you set realistic, achievable goals based on evidence rather than assumptions. In essence, understanding your productivity focus over time is a critical step toward creating a sustainable, efficient work-life balance — something &lt;strong&gt;Personal-Productivity-Assistant&lt;/strong&gt; is designed to support.&lt;/p&gt; &lt;p&gt;Here are the main features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Privacy &amp;amp; Security: No information about your activity is sent over the internet, ensuring complete privacy.&lt;/li&gt; &lt;li&gt;Raw Time Log: The application stores a raw log of your activity in an open format within a designated folder, offering full transparency and user control.&lt;/li&gt; &lt;li&gt;AI Analysis: An AI model analyzes your long-term activity to uncover hidden patterns and provide actionable insights to enhance productivity.&lt;/li&gt; &lt;li&gt;Classification Customization: Users can manually adjust AI classifications to better reflect their personal productivity goals.&lt;/li&gt; &lt;li&gt;AI Customization: Right now the application is using &lt;code&gt;deepseek-r1:14b&lt;/code&gt;. In the future, users will be able to choose from a variety of AI models to suit their specific needs.&lt;/li&gt; &lt;li&gt;Browsers Domain Tracking: The application also tracks the time spent on individual websites within browsers (Chrome, Safari, Edge), offering a comprehensive view of online activity.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But before I continue explaining how to play with it, let me say a few words about the main killer feature here: &lt;strong&gt;DeepSeek R1&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepSeek R1&lt;/strong&gt; is not good for everything, there are reasonable concerns, but it’s perfect for our productivity tasks!&lt;/p&gt; &lt;p&gt;Using this model we can classify applications or websites without sending any data to the cloud and thus keep your data secure.&lt;/p&gt; &lt;p&gt;I strongly believe that &lt;strong&gt;Personal-Productivity-Assistant&lt;/strong&gt; may lead to increased competition and drive innovation across the sector of similar productivity-tracking services (the combined user base of all time-tracking applications reaches tens of millions). Its open-source nature and free availability make it an excellent alternative.&lt;/p&gt; &lt;p&gt;The model itself will be delivered to your computer via another project called &lt;strong&gt;Ollama.&lt;/strong&gt; This is done for convenience and better resources allocation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Now how to install and run?&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Install &lt;a href="https://ollama.com/"&gt;&lt;strong&gt;Ollama&lt;/strong&gt;&lt;/a&gt;: &lt;a href="https://ollama.com/download/OllamaSetup.exe"&gt;Windows&lt;/a&gt; | &lt;a href="https://ollama.com/download/Ollama-darwin.zip"&gt;MacOS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Install &lt;strong&gt;Personal-Productivity-Assistant&lt;/strong&gt;: &lt;a href="https://github.com/smelnyk/Personal-Productivity-Assistant/raw/refs/heads/main/downloads/Personal-Productivity-Assistant.exe"&gt;Windows&lt;/a&gt; | &lt;a href="https://github.com/smelnyk/Personal-Productivity-Assistant/raw/refs/heads/main/downloads/Personal-Productivity-Assistant.zip"&gt;MacOS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;First start can take some, because of &lt;code&gt;deepseek-r1:14b&lt;/code&gt; (14 billion params, chain of thoughts)&lt;/li&gt; &lt;li&gt;Once installed, a black circle will appear in the system tray:&lt;/li&gt; &lt;li&gt;Now do your regular work and wait some time to collect good amount of statistics. Application will store amount of second you spend in each application or website.&lt;/li&gt; &lt;li&gt;Finally generate the report.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Note: Generating the report requires a minimum of 9GB of RAM, and the process may take a few minutes. If memory usage is a concern, it’s possible to switch to a smaller model for more efficient resource management&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I’d love to hear your feedback! Whether it’s feature requests, bug reports, or your success stories, join the community on GitHub to contribute and help make the tool even better. Together, we can shape the future of productivity tools. Check it out &lt;a href="https://github.com/smelnyk/Personal-Productivity-Assistant"&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;About Me&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I’m Serhii Melnyk, with over 16 years of experience in designing and implementing high-reliability, scalable, and high-quality projects. My technical expertise is complemented by strong team-leading and communication skills, which have helped me successfully lead teams for over 5 years.&lt;/p&gt; &lt;p&gt;Throughout my career, I’ve focused on creating workflows for machine learning and data science API services in cloud infrastructure, as well as designing monolithic and Kubernetes (K8S) containerized microservices architectures. I’ve also worked extensively with high-load SaaS solutions, REST/GRPC API implementations, and CI/CD pipeline design.&lt;/p&gt; &lt;p&gt;I’m passionate about product delivery, and my background includes mentoring team members, conducting thorough code and design reviews, and managing people. Additionally, I’ve worked with AWS Cloud services, as well as GCP and Azure integrations.&lt;/p&gt; &lt;p&gt;Currently, I’m all in on the Large Language Models (LLM) field, diving deep into the latest innovations and applications in this rapidly evolving space.&lt;/p&gt; &lt;p&gt;Poke me on &lt;a href="https://www.linkedin.com/in/smelnyk/"&gt;LinkedIn&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Ebb-1486"&gt; /u/Ok-Ebb-1486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijwk1c/how_can_you_utilize_deepseek_r1_for_personal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijwk1c/how_can_you_utilize_deepseek_r1_for_personal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijwk1c/how_can_you_utilize_deepseek_r1_for_personal/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T14:44:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijm8m0</id>
    <title>Dora - Local Drive Semantic Search</title>
    <updated>2025-02-07T03:59:52+00:00</updated>
    <author>
      <name>/u/ranoutofusernames__</name>
      <uri>https://old.reddit.com/user/ranoutofusernames__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Sharing Dora, an alternative to the Mac Explorer app that I wrote today so you can retrieve files using natural language. It runs a local crawler at the target directory to index file names and paths recursively, embeds them and then lets you retrieve them using a chat window (semantic search). You can then open the files directly from the results as well.&lt;/p&gt; &lt;p&gt;It runs completely local and no data is sent out.&lt;/p&gt; &lt;p&gt;Adding file content embedding for plaintext, PDFs and images on the next update for even better results. The goal is to do deep-research with local files eventually.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/space0blaster/dora"&gt;https://github.com/space0blaster/dora&lt;/a&gt;&lt;/p&gt; &lt;p&gt;License: MIT&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ranoutofusernames__"&gt; /u/ranoutofusernames__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijm8m0/dora_local_drive_semantic_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijm8m0/dora_local_drive_semantic_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijm8m0/dora_local_drive_semantic_search/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T03:59:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijrwas</id>
    <title>Best LLM for Coding</title>
    <updated>2025-02-07T10:20:01+00:00</updated>
    <author>
      <name>/u/anshul2k</name>
      <uri>https://old.reddit.com/user/anshul2k</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for LLM for coding i got 32GB ram and 4080&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anshul2k"&gt; /u/anshul2k &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijrwas/best_llm_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/ollama/comments/1ijrwas/best_llm_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/ollama/comments/1ijrwas/best_llm_for_coding/"/>
    <category term="ollama" label="r/ollama"/>
    <published>2025-02-07T10:20:01+00:00</published>
  </entry>
</feed>
