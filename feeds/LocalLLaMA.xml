<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-05T02:51:24+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mh3s7q</id>
    <title>new Hunyuan Instruct 7B/4B/1.8B/0.5B models</title>
    <updated>2025-08-04T04:16:20+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tescent has released new models (llama.cpp support is already merged!)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-7B-Instruct"&gt;https://huggingface.co/tencent/Hunyuan-7B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-4B-Instruct"&gt;https://huggingface.co/tencent/Hunyuan-4B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-1.8B-Instruct"&gt;https://huggingface.co/tencent/Hunyuan-1.8B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-0.5B-Instruct"&gt;https://huggingface.co/tencent/Hunyuan-0.5B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Model Introduction&lt;/h1&gt; &lt;p&gt;Hunyuan is Tencent's open-source efficient large language model series, designed for versatile deployment across diverse computational environments. From edge devices to high-concurrency production systems, these models deliver optimal performance with advanced quantization support and ultra-long context capabilities.&lt;/p&gt; &lt;p&gt;We have released a series of Hunyuan dense models, comprising both pre-trained and instruction-tuned variants, with parameter scales of 0.5B, 1.8B, 4B, and 7B. These models adopt training strategies similar to the Hunyuan-A13B, thereby inheriting its robust performance characteristics. This comprehensive model family enables flexible deployment optimization - from resource-constrained edge computing with smaller variants to high-throughput production environments with larger models, all while maintaining strong capabilities across diverse scenarios.&lt;/p&gt; &lt;h1&gt;Key Features and Advantages&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hybrid Reasoning Support&lt;/strong&gt;: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ultra-Long Context Understanding&lt;/strong&gt;: Natively supports a 256K context window, maintaining stable performance on long-text tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced Agent Capabilities&lt;/strong&gt;: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3, τ-Bench and C3-Bench.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient Inference&lt;/strong&gt;: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;UPDATE&lt;/p&gt; &lt;p&gt;pretrain models&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-7B-Pretrain"&gt;https://huggingface.co/tencent/Hunyuan-7B-Pretrain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-4B-Pretrain"&gt;https://huggingface.co/tencent/Hunyuan-4B-Pretrain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-1.8B-Pretrain"&gt;https://huggingface.co/tencent/Hunyuan-1.8B-Pretrain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-0.5B-Pretrain"&gt;https://huggingface.co/tencent/Hunyuan-0.5B-Pretrain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUFs&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF"&gt;https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF"&gt;https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF"&gt;https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/gabriellarson/Hunyuan-0.5B-Instruct-GGUF"&gt;https://huggingface.co/gabriellarson/Hunyuan-0.5B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh3s7q/new_hunyuan_instruct_7b4b18b05b_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh3s7q/new_hunyuan_instruct_7b4b18b05b_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mh3s7q/new_hunyuan_instruct_7b4b18b05b_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T04:16:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhp2e5</id>
    <title>What's your 'primary' model and why? Do you run a secondary model?</title>
    <updated>2025-08-04T20:39:21+00:00</updated>
    <author>
      <name>/u/ayylmaonade</name>
      <uri>https://old.reddit.com/user/ayylmaonade</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With all the new models coming out recently, I've been more and more curious about this. It seems like a few months ago we were all running Gemma 3, now everybody seems to be running Qwen 3, but with recent model releases, which is your go-to daily-driver and why, and if you have secondary model(s), what do you use them for?&lt;/p&gt; &lt;p&gt;I've got a 7900 XTX 24GB, so all of my models are &amp;lt;32B. But here are mine;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Mistral Small 3.2: A &amp;quot;better&amp;quot; version of Gemma 3, in a way. I really liked Gemma 3, but it hallucinated far too much on basic facts. Mistral doesn't on the other hand, it hallucinates far less ime. I'm mainly using it for general knowledge and image analysis and consistently does a better job at both than Gemma for me. Feels a bit cold or sterile compared to Gemma 3 though.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Qwen 3 30B-A3B-Thinking-2507: The &amp;quot;Gemini 2.5&amp;quot; at home model. I've compared it pretty extensively to 2.5 Flash Reasoning, and 2.5 Pro, and it's able to consistently beat Flash and more often than not come close to or match 2.5 Pro. I'm mainly using this model for complex queries, problem solving, and writing. It's a damn good writing model imo, but that's not a &lt;em&gt;major&lt;/em&gt; use-case for me.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Qwen 3-Coder 30B-A3B-Instruct-2507: This model acts a lot like a mix of Gemini, Claude, and an openAI model to me in my eyes. It's a really, really capable coder. I'm a software engineer and it's a nice companion in that regard. A lot of people say it's like most like Claude, and from what I've seen from Claude outputs, I tend to agree. although I've never used Claude, admittedly.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So there we have it, those are the models I use and the use-case for each. I do occasionally use OpenRouter to serve GLM 4.5-Air and Kimi K2, but that's mostly just out of curiosity. So what's everybody else here running?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayylmaonade"&gt; /u/ayylmaonade &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T20:39:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhhy47</id>
    <title>A free goldmine of tutorials for the components you need to create production-level agents Extensive open source resource with tutorials for creating robust AI agents</title>
    <updated>2025-08-04T16:19:40+00:00</updated>
    <author>
      <name>/u/Nir777</name>
      <uri>https://old.reddit.com/user/Nir777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I’ve worked really hard and launched a FREE resource with 30+ detailed tutorials for building comprehensive production-level AI agents, as part of my Gen AI educational initiative.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The tutorials cover all the key components you need to create agents that are ready for real-world deployment. I plan to keep adding more tutorials over time and will make sure the content stays up to date.&lt;/p&gt; &lt;p&gt;The response so far has been incredible! (the repo got nearly 10,000 stars in one month from launch - all organic) This is part of my broader effort to create high-quality open source educational material. I already have over 130 code tutorials on GitHub with over 50,000 stars.&lt;/p&gt; &lt;p&gt;I hope you find it useful. The tutorials are available here: &lt;a href="https://github.com/NirDiamant/agents-towards-production"&gt;https://github.com/NirDiamant/agents-towards-production&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(most of the tutorials can be run locally, but some of them don't, so please enjoy those who are and don't hate me for those how aren't :D )&lt;/p&gt; &lt;p&gt;The content is organized into these categories:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Orchestration&lt;/li&gt; &lt;li&gt;Tool integration&lt;/li&gt; &lt;li&gt;Observability&lt;/li&gt; &lt;li&gt;Deployment&lt;/li&gt; &lt;li&gt;Memory&lt;/li&gt; &lt;li&gt;UI &amp;amp; Frontend&lt;/li&gt; &lt;li&gt;Agent Frameworks&lt;/li&gt; &lt;li&gt;Model Customization&lt;/li&gt; &lt;li&gt;Multi-agent Coordination&lt;/li&gt; &lt;li&gt;Security&lt;/li&gt; &lt;li&gt;Evaluation&lt;/li&gt; &lt;li&gt;Tracing &amp;amp; Debugging&lt;/li&gt; &lt;li&gt;Web Scraping&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nir777"&gt; /u/Nir777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhy47/a_free_goldmine_of_tutorials_for_the_components/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhy47/a_free_goldmine_of_tutorials_for_the_components/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhy47/a_free_goldmine_of_tutorials_for_the_components/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T16:19:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhix7d</id>
    <title>GLM ranks #2 for chat according to lmarena</title>
    <updated>2025-08-04T16:55:51+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Style control removed.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Rank (UB)&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;th align="left"&gt;95% CI (±)&lt;/th&gt; &lt;th align="left"&gt;Votes&lt;/th&gt; &lt;th align="left"&gt;Company&lt;/th&gt; &lt;th align="left"&gt;License&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;gemini-2.5-pro&lt;/td&gt; &lt;td align="left"&gt;1470&lt;/td&gt; &lt;td align="left"&gt;±5&lt;/td&gt; &lt;td align="left"&gt;26,019&lt;/td&gt; &lt;td align="left"&gt;Google&lt;/td&gt; &lt;td align="left"&gt;Closed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;grok-4-0709&lt;/td&gt; &lt;td align="left"&gt;1435&lt;/td&gt; &lt;td align="left"&gt;±6&lt;/td&gt; &lt;td align="left"&gt;13,058&lt;/td&gt; &lt;td align="left"&gt;xAI&lt;/td&gt; &lt;td align="left"&gt;Closed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;glm-4.5&lt;/td&gt; &lt;td align="left"&gt;1435&lt;/td&gt; &lt;td align="left"&gt;±9&lt;/td&gt; &lt;td align="left"&gt;4,112&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;MIT&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;chatgpt-4o-latest-20250326&lt;/td&gt; &lt;td align="left"&gt;1430&lt;/td&gt; &lt;td align="left"&gt;±5&lt;/td&gt; &lt;td align="left"&gt;30,777&lt;/td&gt; &lt;td align="left"&gt;Closed AI&lt;/td&gt; &lt;td align="left"&gt;Closed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;o3-2025-04-16&lt;/td&gt; &lt;td align="left"&gt;1429&lt;/td&gt; &lt;td align="left"&gt;±5&lt;/td&gt; &lt;td align="left"&gt;32,033&lt;/td&gt; &lt;td align="left"&gt;Closed AI&lt;/td&gt; &lt;td align="left"&gt;Closed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;deepseek-r1-0528&lt;/td&gt; &lt;td align="left"&gt;1427&lt;/td&gt; &lt;td align="left"&gt;±6&lt;/td&gt; &lt;td align="left"&gt;18,284&lt;/td&gt; &lt;td align="left"&gt;DeepSeek&lt;/td&gt; &lt;td align="left"&gt;MIT&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;qwen3-235b-a22b-instruct-2507&lt;/td&gt; &lt;td align="left"&gt;1427&lt;/td&gt; &lt;td align="left"&gt;±9&lt;/td&gt; &lt;td align="left"&gt;4,154&lt;/td&gt; &lt;td align="left"&gt;Alibaba&lt;/td&gt; &lt;td align="left"&gt;Apache 2.0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://x.com/lmarena_ai/status/1952402506497020330"&gt;https://x.com/lmarena_ai/status/1952402506497020330&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmarena.ai/leaderboard/text"&gt;https://lmarena.ai/leaderboard/text&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhix7d/glm_ranks_2_for_chat_according_to_lmarena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhix7d/glm_ranks_2_for_chat_according_to_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhix7d/glm_ranks_2_for_chat_according_to_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T16:55:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhb5el</id>
    <title>GLM-4.5 llama.cpp PR is nearing completion</title>
    <updated>2025-08-04T11:44:02+00:00</updated>
    <author>
      <name>/u/DistanceSolar1449</name>
      <uri>https://old.reddit.com/user/DistanceSolar1449</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Current status:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14939#issuecomment-3150197036"&gt;https://github.com/ggml-org/llama.cpp/pull/14939#issuecomment-3150197036&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Everyone get ready to fire up your GPUs...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DistanceSolar1449"&gt; /u/DistanceSolar1449 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhb5el/glm45_llamacpp_pr_is_nearing_completion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhb5el/glm45_llamacpp_pr_is_nearing_completion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhb5el/glm45_llamacpp_pr_is_nearing_completion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T11:44:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhh6se</id>
    <title>Qwen/Qwen-Image · Hugging Face</title>
    <updated>2025-08-04T15:51:47+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhh6se/qwenqwenimage_hugging_face/"&gt; &lt;img alt="Qwen/Qwen-Image · Hugging Face" src="https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a65aaca919e956009709dd069f70c0c907403912" title="Qwen/Qwen-Image · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-Image"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhh6se/qwenqwenimage_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhh6se/qwenqwenimage_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T15:51:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhbvig</id>
    <title>What kind of Qwen 2508 do you want tonight? ;)</title>
    <updated>2025-08-04T12:19:39+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhbvig/what_kind_of_qwen_2508_do_you_want_tonight/"&gt; &lt;img alt="What kind of Qwen 2508 do you want tonight? ;)" src="https://preview.redd.it/3f5by1b8wzgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f4a0d87b3973237c269d2cef31fbc18fbe655b1" title="What kind of Qwen 2508 do you want tonight? ;)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3f5by1b8wzgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhbvig/what_kind_of_qwen_2508_do_you_want_tonight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhbvig/what_kind_of_qwen_2508_do_you_want_tonight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T12:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhpm02</id>
    <title>Quick Qwen Image Gen with 4090+3060</title>
    <updated>2025-08-04T20:59:58+00:00</updated>
    <author>
      <name>/u/fp4guru</name>
      <uri>https://old.reddit.com/user/fp4guru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhpm02/quick_qwen_image_gen_with_40903060/"&gt; &lt;img alt="Quick Qwen Image Gen with 4090+3060" src="https://b.thumbs.redditmedia.com/rc6cs4fuadas1yBd9N2cMHV6iAM0qvhoekwcLUHsAfU.jpg" title="Quick Qwen Image Gen with 4090+3060" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just tested the new &lt;strong&gt;Qwen-Image&lt;/strong&gt; model from Alibaba using 🤗 Diffusers with bfloat16 + dual-GPU memory config (4090 + 3060). Prompted it to generate a &lt;strong&gt;cyberpunk night market scene&lt;/strong&gt;—complete with neon signs, rainy pavement, futuristic street food vendors, and a monorail in the background.&lt;/p&gt; &lt;p&gt;Ran at &lt;code&gt;1472x832&lt;/code&gt;, 32 steps, &lt;code&gt;true_cfg_scale=3.0&lt;/code&gt;. No LoRA, no refiner—just straight from the base checkpoint.&lt;/p&gt; &lt;p&gt;Full prompt and code below. Let me know what you think of the result or if you’ve got prompt ideas to push it further.&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;from diffusers import DiffusionPipeline&lt;/p&gt; &lt;p&gt;import torch, gc&lt;/p&gt; &lt;p&gt;pipe = DiffusionPipeline.from_pretrained(&lt;/p&gt; &lt;p&gt;&amp;quot;Qwen/Qwen-Image&amp;quot;,&lt;/p&gt; &lt;p&gt;torch_dtype=torch.bfloat16,&lt;/p&gt; &lt;p&gt;device_map=&amp;quot;balanced&amp;quot;,&lt;/p&gt; &lt;p&gt;max_memory={0: &amp;quot;23GiB&amp;quot;, 1: &amp;quot;11GiB&amp;quot;},&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;pipe.enable_attention_slicing()&lt;/p&gt; &lt;p&gt;pipe.enable_vae_tiling()&lt;/p&gt; &lt;p&gt;prompt = (&lt;/p&gt; &lt;p&gt;&amp;quot;A bustling cyberpunk night market street scene. Neon signs in Chinese hang above steaming food stalls. &amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;A robotic vendor is grilling skewers while a crowd of futuristic characters—some wearing glowing visors, &amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;some holding umbrellas under a light drizzle—gathers around. Bright reflections on the wet pavement. &amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;In the distance, a monorail passes by above the alley. Ultra HD, 4K, cinematic composition.&amp;quot;&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;negative_prompt = (&lt;/p&gt; &lt;p&gt;&amp;quot;low quality, blurry, distorted, bad anatomy, text artifacts, poor lighting&amp;quot;&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;img = pipe(&lt;/p&gt; &lt;p&gt;prompt=prompt,&lt;/p&gt; &lt;p&gt;negative_prompt=negative_prompt,&lt;/p&gt; &lt;p&gt;width=1472, height=832,&lt;/p&gt; &lt;p&gt;num_inference_steps=32,&lt;/p&gt; &lt;p&gt;true_cfg_scale=3.0,&lt;/p&gt; &lt;p&gt;generator=torch.Generator(&amp;quot;cuda&amp;quot;).manual_seed(8899)&lt;/p&gt; &lt;p&gt;).images[0]&lt;/p&gt; &lt;p&gt;img.save(&amp;quot;qwen_cyberpunk_market.png&amp;quot;)&lt;/p&gt; &lt;p&gt;del pipe; gc.collect(); torch.cuda.empty_cache()&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0djfy60ms3hf1.png?width=1472&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=77be5a785c6eb51c997d0faa3226371547b7fcdc"&gt;https://preview.redd.it/0djfy60ms3hf1.png?width=1472&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=77be5a785c6eb51c997d0faa3226371547b7fcdc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;thanks to &lt;a href="https://www.reddit.com/user/motorcycle_frenzy889/"&gt;motorcycle_frenzy889&lt;/a&gt; , 60 steps can craft correct text.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fp4guru"&gt; /u/fp4guru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhpm02/quick_qwen_image_gen_with_40903060/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhpm02/quick_qwen_image_gen_with_40903060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhpm02/quick_qwen_image_gen_with_40903060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T20:59:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhimmj</id>
    <title>Qwen Image Japanese and Chinese text generation test</title>
    <updated>2025-08-04T16:44:57+00:00</updated>
    <author>
      <name>/u/shokuninstudio</name>
      <uri>https://old.reddit.com/user/shokuninstudio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhimmj/qwen_image_japanese_and_chinese_text_generation/"&gt; &lt;img alt="Qwen Image Japanese and Chinese text generation test" src="https://b.thumbs.redditmedia.com/qgQuHunQQo7mnCA7jscRiLJC9qwUEnTBUGvNSAP0Kgs.jpg" title="Qwen Image Japanese and Chinese text generation test" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The results are a mix of real and made up characters. The signs are meaningless gibberish. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shokuninstudio"&gt; /u/shokuninstudio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mhimmj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhimmj/qwen_image_japanese_and_chinese_text_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhimmj/qwen_image_japanese_and_chinese_text_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T16:44:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhlo6g</id>
    <title>Google introduces a new Benchmark: Game Arena and they're streaming your favorite open weight models playing chess against close source models.</title>
    <updated>2025-08-04T18:33:46+00:00</updated>
    <author>
      <name>/u/mtmttuan</name>
      <uri>https://old.reddit.com/user/mtmttuan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhlo6g/google_introduces_a_new_benchmark_game_arena_and/"&gt; &lt;img alt="Google introduces a new Benchmark: Game Arena and they're streaming your favorite open weight models playing chess against close source models." src="https://external-preview.redd.it/UFZqcZq9i0MOfYzexLrIUEZbhTVVNRWezkExEe_y0E4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc604dc195cda31f472811e38b2354a3cb7b4e27" title="Google introduces a new Benchmark: Game Arena and they're streaming your favorite open weight models playing chess against close source models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is the original blog post: &lt;a href="https://blog.google/technology/ai/kaggle-game-arena/"&gt;https://blog.google/technology/ai/kaggle-game-arena/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;About the benchmark, I personally prefer game as a head-to-head benchmark to LMArena. At least if they do benchmaxxing, we might have models that's more intelligent comparing to the more glazing effect of LMArena. &lt;/p&gt; &lt;p&gt;About the exhibition stream, it's funny to see they let Deepseek R1 play against o4-mini and Grok 4 play against gemini flash. Kimi-K2 vs O3 would be fun though. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/83xmndz6q1hf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=376f9bc5e41eb6f87a3f4c9cfd3e4be8aadf8814"&gt;https://preview.redd.it/83xmndz6q1hf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=376f9bc5e41eb6f87a3f4c9cfd3e4be8aadf8814&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mtmttuan"&gt; /u/mtmttuan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhlo6g/google_introduces_a_new_benchmark_game_arena_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhlo6g/google_introduces_a_new_benchmark_game_arena_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhlo6g/google_introduces_a_new_benchmark_game_arena_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T18:33:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhdnye</id>
    <title>New Qwen model has vision</title>
    <updated>2025-08-04T13:37:05+00:00</updated>
    <author>
      <name>/u/Relative_Rope4234</name>
      <uri>https://old.reddit.com/user/Relative_Rope4234</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhdnye/new_qwen_model_has_vision/"&gt; &lt;img alt="New Qwen model has vision" src="https://preview.redd.it/vypcvak2a0hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb260c7a9be4af5c73e35111d5c25d23acd339bd" title="New Qwen model has vision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Relative_Rope4234"&gt; /u/Relative_Rope4234 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vypcvak2a0hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhdnye/new_qwen_model_has_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhdnye/new_qwen_model_has_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T13:37:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhrx3m</id>
    <title>How to use your Local Models to watch your screen. Open Source and Completely Free!!</title>
    <updated>2025-08-04T22:30:05+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhrx3m/how_to_use_your_local_models_to_watch_your_screen/"&gt; &lt;img alt="How to use your Local Models to watch your screen. Open Source and Completely Free!!" src="https://external-preview.redd.it/czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba0ed7244b693bf15ef5f04c6f2d7cddc4f49c67" title="How to use your Local Models to watch your screen. Open Source and Completely Free!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; I built this &lt;strong&gt;open source&lt;/strong&gt; and &lt;strong&gt;local&lt;/strong&gt; app that lets your local models &lt;strong&gt;watch your screen&lt;/strong&gt; and do stuff! It is now &lt;strong&gt;suuuper easy to install&lt;/strong&gt; and use, to make local AI &lt;strong&gt;accessible to&lt;/strong&gt; &lt;strong&gt;everybody&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;Hey r/LocalLLaMA! I'm back with some Observer updates c: first of all &lt;strong&gt;Thank You&lt;/strong&gt; so much for all of your support and feedback, i've been working hard to take this project to this current state. I added the app installation which is a significant QOL improvement for ease of use for first time users!! The docker-compose option is still supported and viable for people wanting a more specific and custom install.&lt;/p&gt; &lt;p&gt;The new app tools are a &lt;strong&gt;game-changer&lt;/strong&gt;!! You can now have direct system-level pop ups or notifications that come up right &lt;strong&gt;up to your face&lt;/strong&gt; hahaha. And sorry to everyone who tried out SMS and WhatsApp and were frustrated because you weren't getting notifications, Meta started blocking my account thinking i was just spamming messages to you guys.&lt;/p&gt; &lt;p&gt;But the pushover and discord notifications work perfectly well!&lt;/p&gt; &lt;p&gt;If you have any feedback please reach out through the discord, i'm really open to suggestions.&lt;/p&gt; &lt;p&gt;This is the projects &lt;a href="https://github.com/Roy3838/Observer"&gt;Github&lt;/a&gt; (completely open source)&lt;br /&gt; And the discord: &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you have any questions i'll be hanging out here for a while!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/g3pod2zlw2hf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhrx3m/how_to_use_your_local_models_to_watch_your_screen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhrx3m/how_to_use_your_local_models_to_watch_your_screen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T22:30:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhhhpi</id>
    <title>Qwen-Image — a 20B MMDiT model</title>
    <updated>2025-08-04T16:02:49+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 Meet Qwen-Image — a 20B MMDiT model for next-gen text-to-image generation. Especially strong at creating stunning graphic posters with native text. Now open-source.&lt;/p&gt; &lt;p&gt;🔍 Key Highlights:&lt;/p&gt; &lt;p&gt;🔹 SOTA text rendering — rivals GPT-4o in English, best-in-class for Chinese&lt;/p&gt; &lt;p&gt;🔹 In-pixel text generation — no overlays, fully integrated&lt;/p&gt; &lt;p&gt;🔹 Bilingual support, diverse fonts, complex layouts&lt;/p&gt; &lt;p&gt;🎨 Also excels at general image generation — from photorealistic to anime, impressionist to minimalist. A true creative powerhouse.&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://qwenlm.github.io/blog/qwen-image/%5BBlog%5D(https://qwenlm.github.io/blog/qwen-image/)"&gt;https://qwenlm.github.io/blog/qwen-image/[Blog](https://qwenlm.github.io/blog/qwen-image/)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="http://huggingface.co/Qwen/Qwen-Image"&gt;huggingface.co/Qwen/Qwen-Image&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhhpi/qwenimage_a_20b_mmdit_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhhpi/qwenimage_a_20b_mmdit_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhhpi/qwenimage_a_20b_mmdit_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T16:02:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhg8rt</id>
    <title>Get ready for GLM-4-5 local gguf woot woot</title>
    <updated>2025-08-04T15:16:51+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This model is insane! I have been testing the ongoing llama.cpp PR and this morning has been amazing! GLM can spit out LOOOOOOOOOOOOOOOOOONG tokens! The original was a beast, and the new one is even better. I gave it 2500 lines of python code, told it to refactor it, it do so without dropping anything! Then I told it to translate it to ruby and it did so completely. The model is very coherent across long contexts, the quality so far is great. The model is fast! Full loaded on 3090's, It starts out at 45tk/sec and this is with llama.cpp.&lt;/p&gt; &lt;p&gt;I have only driven it for about an hour and this is the smaller model air, not the big one! I'm very convinced that this will replace deepseek-r1/chimera/v3/ernie-300b/kimi-k2 for me.&lt;/p&gt; &lt;p&gt;Is this better than sonnet/opus/gemini/openai? For me yup! I don't use closed models, so I really can't tell, but this so far is looking like the best damn model locally. I have only thrown code generation at it, so I can't tell how it would perform in creative writing, role play, other sorts of generation etc. I haven't played at all with tool calling, instruction following, etc, but based on how well it's responding, I think it's going to be great. The only short coming I see is the 128k context window.&lt;/p&gt; &lt;p&gt;It's fast too, 50k+ token, 16.44 tk/sec&lt;/p&gt; &lt;p&gt;slot release: id 0 | task 42155 | stop processing: n_past = 51785, truncated = 0&lt;/p&gt; &lt;p&gt;slot print_timing: id 0 | task 42155 |&lt;/p&gt; &lt;p&gt;prompt eval time = 421.72 ms / 35 tokens ( 12.05 ms per token, 82.99 tokens per second)&lt;/p&gt; &lt;p&gt;eval time = 983525.01 ms / 16169 tokens ( 60.83 ms per token, 16.44 tokens per second)&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; q4 quants down to 67.85gb&lt;br /&gt; I decide to run q4, offload only shared experts to 1 3090 GPU and the rest to system ram (ddr4 2400mhz quad channel on dual x99 platform). The entire shared experts for 47 layers takes about 4gb of vram, that means you can put all of the shared expert on your 8gb GPU. I decide to not load any other tensor but just these and see how it performs. It start out at 10tk/sec. I'm going to run q3_k_l on a 3060 and P40 and put up the results later.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhg8rt/get_ready_for_glm45_local_gguf_woot_woot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhg8rt/get_ready_for_glm45_local_gguf_woot_woot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhg8rt/get_ready_for_glm45_local_gguf_woot_woot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T15:16:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhctvk</id>
    <title>Huawei released weights of Pangu Ultra,a 718B model.</title>
    <updated>2025-08-04T13:02:04+00:00</updated>
    <author>
      <name>/u/Overflow_al</name>
      <uri>https://old.reddit.com/user/Overflow_al</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhctvk/huawei_released_weights_of_pangu_ultraa_718b_model/"&gt; &lt;img alt="Huawei released weights of Pangu Ultra,a 718B model." src="https://external-preview.redd.it/b4vAhRXu0QISFGzKNI7MMEeFrdQG1UWQqC8GhQPUCNU.png?width=70&amp;amp;height=70&amp;amp;crop=70:70,smart&amp;amp;auto=webp&amp;amp;s=42d2a16045706201098a626b24ce7402818223f6" title="Huawei released weights of Pangu Ultra,a 718B model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Overflow_al"&gt; /u/Overflow_al &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ai.gitcode.com/ascend-tribe/openpangu-ultra-moe-718b-model/blob/main/README_EN.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhctvk/huawei_released_weights_of_pangu_ultraa_718b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhctvk/huawei_released_weights_of_pangu_ultraa_718b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T13:02:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhl5yo</id>
    <title>Gemini 3 is coming?..</title>
    <updated>2025-08-04T18:15:40+00:00</updated>
    <author>
      <name>/u/SlerpE</name>
      <uri>https://old.reddit.com/user/SlerpE</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhl5yo/gemini_3_is_coming/"&gt; &lt;img alt="Gemini 3 is coming?.." src="https://preview.redd.it/59joqndkn1hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e89c768daaac9653e4f2ad00c6a0ee5f6412107" title="Gemini 3 is coming?.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/OfficialLoganK/status/1952430214375493808"&gt;https://x.com/OfficialLoganK/status/1952430214375493808&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlerpE"&gt; /u/SlerpE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/59joqndkn1hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhl5yo/gemini_3_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhl5yo/gemini_3_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T18:15:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhf0kl</id>
    <title>Qwen image 20B is coming!</title>
    <updated>2025-08-04T14:30:09+00:00</updated>
    <author>
      <name>/u/sunshinecheung</name>
      <uri>https://old.reddit.com/user/sunshinecheung</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhf0kl/qwen_image_20b_is_coming/"&gt; &lt;img alt="Qwen image 20B is coming!" src="https://external-preview.redd.it/qRUOYZYoHmNR9iE-xb-2D9P2t108utUKO0BsEEFsXs0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b2d120bc77d25aceff54c01b358c0fa229b74ef" title="Qwen image 20B is coming!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/3n3tfhqbj0hf1.png?width=529&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5234584d7973049a12fc3c428b50a1d35e48858f"&gt;https://preview.redd.it/3n3tfhqbj0hf1.png?width=529&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5234584d7973049a12fc3c428b50a1d35e48858f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uxg8kr5ej0hf1.png?width=1664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f06cc61180cfced07aa66367d23e552e605c0f75"&gt;https://preview.redd.it/uxg8kr5ej0hf1.png?width=1664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f06cc61180cfced07aa66367d23e552e605c0f75&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen image is ready to drop:&lt;a href="https://github.com/huggingface/diffusers/pull/12055"&gt;https://github.com/huggingface/diffusers/pull/12055&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunshinecheung"&gt; /u/sunshinecheung &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhf0kl/qwen_image_20b_is_coming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhf0kl/qwen_image_20b_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhf0kl/qwen_image_20b_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T14:30:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mht910</id>
    <title>GLM 4.5 GGUFs are coming</title>
    <updated>2025-08-04T23:26:08+00:00</updated>
    <author>
      <name>/u/Pro-editor-1105</name>
      <uri>https://old.reddit.com/user/Pro-editor-1105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mht910/glm_45_ggufs_are_coming/"&gt; &lt;img alt="GLM 4.5 GGUFs are coming" src="https://external-preview.redd.it/mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cbaa78bb76999536a7337e9b0c9e2f578691b200" title="GLM 4.5 GGUFs are coming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;FINALLY&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pro-editor-1105"&gt; /u/Pro-editor-1105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mradermacher/GLM-4.5-Air-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mht910/glm_45_ggufs_are_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mht910/glm_45_ggufs_are_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T23:26:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhbpmo</id>
    <title>New Qwen Models Today!!!</title>
    <updated>2025-08-04T12:12:00+00:00</updated>
    <author>
      <name>/u/R46H4V</name>
      <uri>https://old.reddit.com/user/R46H4V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhbpmo/new_qwen_models_today/"&gt; &lt;img alt="New Qwen Models Today!!!" src="https://preview.redd.it/qemmgysvuzgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e45a424e82bdde4384e3d7ba1be6631b2a25639" title="New Qwen Models Today!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R46H4V"&gt; /u/R46H4V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qemmgysvuzgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhbpmo/new_qwen_models_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhbpmo/new_qwen_models_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T12:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhlkyx</id>
    <title>support for GLM 4.5 family of models has been merged into llama.cpp</title>
    <updated>2025-08-04T18:30:31+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhlkyx/support_for_glm_45_family_of_models_has_been/"&gt; &lt;img alt="support for GLM 4.5 family of models has been merged into llama.cpp" src="https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e04ffa9cbd0a435f87d74eaf876a5853c1e06023" title="support for GLM 4.5 family of models has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14939"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhlkyx/support_for_glm_45_family_of_models_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhlkyx/support_for_glm_45_family_of_models_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T18:30:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhe1rl</id>
    <title>r/LocalLLaMA right now</title>
    <updated>2025-08-04T13:52:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhe1rl/rlocalllama_right_now/"&gt; &lt;img alt="r/LocalLLaMA right now" src="https://preview.redd.it/f0xr7mshc0hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0d701cbdf33b1ec6ea47dd4b4874202dbea647e" title="r/LocalLLaMA right now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f0xr7mshc0hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhe1rl/rlocalllama_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhe1rl/rlocalllama_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T13:52:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhhctd</id>
    <title>🚀 Meet Qwen-Image</title>
    <updated>2025-08-04T15:58:11+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhctd/meet_qwenimage/"&gt; &lt;img alt="🚀 Meet Qwen-Image" src="https://preview.redd.it/7a463it8z0hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2a9bc46837ad4e1dac08bb6879d131c650ac6476" title="🚀 Meet Qwen-Image" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 Meet Qwen-Image — a 20B MMDiT model for next-gen text-to-image generation. Especially strong at creating stunning graphic posters with native text. Now open-source.&lt;/p&gt; &lt;p&gt;🔍 Key Highlights:&lt;/p&gt; &lt;p&gt;🔹 SOTA text rendering — rivals GPT-4o in English, best-in-class for Chinese&lt;/p&gt; &lt;p&gt;🔹 In-pixel text generation — no overlays, fully integrated&lt;/p&gt; &lt;p&gt;🔹 Bilingual support, diverse fonts, complex layouts&lt;/p&gt; &lt;p&gt;🎨 Also excels at general image generation — from photorealistic to anime, impressionist to minimalist. A true creative powerhouse.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7a463it8z0hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhctd/meet_qwenimage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhctd/meet_qwenimage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T15:58:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhiqqn</id>
    <title>Qwen-Image is out</title>
    <updated>2025-08-04T16:49:14+00:00</updated>
    <author>
      <name>/u/BoJackHorseMan53</name>
      <uri>https://old.reddit.com/user/BoJackHorseMan53</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhiqqn/qwenimage_is_out/"&gt; &lt;img alt="Qwen-Image is out" src="https://external-preview.redd.it/ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2392cbc2ee2f68a83914b65971bd3a688fc24739" title="Qwen-Image is out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1952398250121756992"&gt;https://x.com/Alibaba_Qwen/status/1952398250121756992&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's better than Flux Kontext, gpt-image level&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoJackHorseMan53"&gt; /u/BoJackHorseMan53 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4077mfg081hf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhiqqn/qwenimage_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhiqqn/qwenimage_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T16:49:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhgu6t</id>
    <title>Sam Altman watching Qwen drop model after model</title>
    <updated>2025-08-04T15:38:39+00:00</updated>
    <author>
      <name>/u/TheRealSerdra</name>
      <uri>https://old.reddit.com/user/TheRealSerdra</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhgu6t/sam_altman_watching_qwen_drop_model_after_model/"&gt; &lt;img alt="Sam Altman watching Qwen drop model after model" src="https://preview.redd.it/g7t8cmgrv0hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=460e01fe2091b6bc2347e41a918d258022a40353" title="Sam Altman watching Qwen drop model after model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealSerdra"&gt; /u/TheRealSerdra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g7t8cmgrv0hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhgu6t/sam_altman_watching_qwen_drop_model_after_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhgu6t/sam_altman_watching_qwen_drop_model_after_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T15:38:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhhdig</id>
    <title>QWEN-IMAGE is released!</title>
    <updated>2025-08-04T15:58:55+00:00</updated>
    <author>
      <name>/u/TheIncredibleHem</name>
      <uri>https://old.reddit.com/user/TheIncredibleHem</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhdig/qwenimage_is_released/"&gt; &lt;img alt="QWEN-IMAGE is released!" src="https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a65aaca919e956009709dd069f70c0c907403912" title="QWEN-IMAGE is released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;and it's better than Flux Kontext Pro (according to their benchmarks). That's insane. Really looking forward to it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheIncredibleHem"&gt; /u/TheIncredibleHem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-Image"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhdig/qwenimage_is_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhdig/qwenimage_is_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T15:58:55+00:00</published>
  </entry>
</feed>
