<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-13T15:06:04+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mp02yy</id>
    <title>Gemma3n e4b or Qwen 3 4b thinking? what's the best one?</title>
    <updated>2025-08-13T09:49:35+00:00</updated>
    <author>
      <name>/u/pumukidelfuturo</name>
      <uri>https://old.reddit.com/user/pumukidelfuturo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very straightforward question. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pumukidelfuturo"&gt; /u/pumukidelfuturo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp02yy/gemma3n_e4b_or_qwen_3_4b_thinking_whats_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp02yy/gemma3n_e4b_or_qwen_3_4b_thinking_whats_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp02yy/gemma3n_e4b_or_qwen_3_4b_thinking_whats_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T09:49:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1moq2wh</id>
    <title>LM Studio 0.3.23</title>
    <updated>2025-08-13T00:36:34+00:00</updated>
    <author>
      <name>/u/sleepingsysadmin</name>
      <uri>https://old.reddit.com/user/sleepingsysadmin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moq2wh/lm_studio_0323/"&gt; &lt;img alt="LM Studio 0.3.23" src="https://external-preview.redd.it/zP98hWqmZu7rI92YGtSTK2E6AnhmDYmDAkiErXA8_Qk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e08118516c863a423abc655194d378da62501492" title="LM Studio 0.3.23" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Opencode testing right now is working without any tool failures. Huge win. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sleepingsysadmin"&gt; /u/sleepingsysadmin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://lmstudio.ai/blog/lmstudio-v0.3.23"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moq2wh/lm_studio_0323/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1moq2wh/lm_studio_0323/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T00:36:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mogxpr</id>
    <title>OpenAI GPT-OSS-120b is an excellent model</title>
    <updated>2025-08-12T18:35:17+00:00</updated>
    <author>
      <name>/u/xxPoLyGLoTxx</name>
      <uri>https://old.reddit.com/user/xxPoLyGLoTxx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm kind of blown away right now. I downloaded this model not expecting much, as I am an avid fan of the qwen3 family (particularly, the new qwen3-235b-2507 variants). But this OpenAI model is really, really good. &lt;/p&gt; &lt;p&gt;For coding, it has nailed just about every request I've sent its way, and that includes things qwen3-235b was struggling to do. It gets the job done in very few prompts, and because of its smaller size, it's incredibly fast (on my m4 max I get around ~70 tokens / sec with 64k context). Often, it solves everything I want on the first prompt, and then I need one more prompt for a minor tweak. That's been my experience. &lt;/p&gt; &lt;p&gt;For context, I've mainly been using it for web-based programming tasks (e.g., JavaScript, PHP, HTML, CSS). I have not tried many other languages...yet. I also routinely set reasoning mode to &amp;quot;High&amp;quot; as accuracy is important to me.&lt;/p&gt; &lt;p&gt;I'm curious: How are you guys finding this model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xxPoLyGLoTxx"&gt; /u/xxPoLyGLoTxx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mogxpr/openai_gptoss120b_is_an_excellent_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mogxpr/openai_gptoss120b_is_an_excellent_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mogxpr/openai_gptoss120b_is_an_excellent_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T18:35:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1moakv3</id>
    <title>We tested Qwen3-Coder, GPT-5 and other 30+ models on new SWE-Bench like tasks from July 2025</title>
    <updated>2025-08-12T14:41:09+00:00</updated>
    <author>
      <name>/u/Fabulous_Pollution10</name>
      <uri>https://old.reddit.com/user/Fabulous_Pollution10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moakv3/we_tested_qwen3coder_gpt5_and_other_30_models_on/"&gt; &lt;img alt="We tested Qwen3-Coder, GPT-5 and other 30+ models on new SWE-Bench like tasks from July 2025" src="https://preview.redd.it/lcee3fueolif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bc5d986a916d0445a79f4b3d5044d02c9aacef2" title="We tested Qwen3-Coder, GPT-5 and other 30+ models on new SWE-Bench like tasks from July 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I’m Ibragim from Nebius.&lt;/p&gt; &lt;p&gt;We ran a benchmark on &lt;strong&gt;34 fresh GitHub PR tasks&lt;/strong&gt; from July 2025 using the &lt;a href="https://swe-rebench.com/leaderboard"&gt;SWE-rebench leaderboard&lt;/a&gt;. These are real, recent problems — no training-set contamination — and include both proprietary and open-source models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick takeaways:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPT-5-Medium&lt;/strong&gt; leads overall (29.4% resolved rate, 38.2% pass@5).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Coder&lt;/strong&gt; is the &lt;strong&gt;best open-source performer&lt;/strong&gt;, matching GPT-5-High in pass@5 (32.4%) despite a lower resolved rate.&lt;/li&gt; &lt;li&gt;Claude Sonnet 4.0 lags behind in pass@5 at 23.5%.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All tasks come from the continuously updated, decontaminated &lt;a href="https://huggingface.co/datasets/nebius/SWE-rebench-leaderboard"&gt;SWE-rebench-leaderboard&lt;/a&gt; dataset for real-world SWE tasks.&lt;/p&gt; &lt;p&gt;We’re already adding gpt-oss-120b and GLM-4.5 next — which OSS model should we include after that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Pollution10"&gt; /u/Fabulous_Pollution10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lcee3fueolif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moakv3/we_tested_qwen3coder_gpt5_and_other_30_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1moakv3/we_tested_qwen3coder_gpt5_and_other_30_models_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T14:41:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp3oyt</id>
    <title>Triton 3.4 for MI50</title>
    <updated>2025-08-13T12:56:22+00:00</updated>
    <author>
      <name>/u/jetaudio</name>
      <uri>https://old.reddit.com/user/jetaudio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've built triton 3.4 whl for ubuntu 24.04 + pytorch 2.8.0 + rocm 6.3 + MI50 (chinese version, flashed with 16gb radeon pro vii firmware from techpowerup). I can install it on my system, everything run just fine. You can download it here: &lt;a href="https://huggingface.co/datasets/jetaudio/triton_gfx906"&gt;https://huggingface.co/datasets/jetaudio/triton_gfx906&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P/s: only tested on my system, so feedbacks are welcomed&lt;/p&gt; &lt;p&gt;P/s2: I'm trying to make FA2 work on these cards too&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jetaudio"&gt; /u/jetaudio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp3oyt/triton_34_for_mi50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp3oyt/triton_34_for_mi50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp3oyt/triton_34_for_mi50/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T12:56:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mowxb3</id>
    <title>So I tried to run gpt-oss:20b using llama-cli in my MacBook...</title>
    <updated>2025-08-13T06:27:37+00:00</updated>
    <author>
      <name>/u/qscwdv351</name>
      <uri>https://old.reddit.com/user/qscwdv351</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mowxb3/so_i_tried_to_run_gptoss20b_using_llamacli_in_my/"&gt; &lt;img alt="So I tried to run gpt-oss:20b using llama-cli in my MacBook..." src="https://external-preview.redd.it/OW54ZWN4cjRjcWlmMXoKX18xRFkOvTxBhp0YKGJ7rKqpCw2PYiSs9tD7VgN2.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bfe0458849d93d3cbeb31b77ea64134fe83b453e" title="So I tried to run gpt-oss:20b using llama-cli in my MacBook..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;...and this happened. How can I fix this?&lt;/p&gt; &lt;p&gt;I'm using M3 pro 18gb MacBook. I used command from llama.cpp repo(&lt;code&gt;llama-cli -hf modelname&lt;/code&gt;). I expected the model to run since it ran without errors when using Ollama.&lt;/p&gt; &lt;p&gt;The graphic glitch happened after the line &lt;code&gt;load_tensors: loading model tensors, this can take a while... (nmap = true)&lt;/code&gt;. After that, the machine became unresponsive(it responded to pointer movement etc but only pointer movement was visible) and I had to force shutdown to make it usable again.&lt;/p&gt; &lt;p&gt;Why did this happen, and how can I avoid this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/qscwdv351"&gt; /u/qscwdv351 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hgs8wvr4cqif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mowxb3/so_i_tried_to_run_gptoss20b_using_llamacli_in_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mowxb3/so_i_tried_to_run_gptoss20b_using_llamacli_in_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T06:27:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1moefc2</id>
    <title>GPT-5 Style Router, but for any LLM including local.</title>
    <updated>2025-08-12T17:04:22+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moefc2/gpt5_style_router_but_for_any_llm_including_local/"&gt; &lt;img alt="GPT-5 Style Router, but for any LLM including local." src="https://preview.redd.it/vvlzu888emif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d3880b86b3003400a078b5895ab79ba837d29781" title="GPT-5 Style Router, but for any LLM including local." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPT-5 launched a few days ago, which essentially wraps different models underneath via a real-time router. In June, we published our &lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;preference-aligned routing model&lt;/a&gt; and &lt;a href="https://github.com/katanemo/archgw"&gt;framework&lt;/a&gt; for developers so that they can build a unified experience with choice of models they care about using a real-time router.&lt;/p&gt; &lt;p&gt;Sharing the research and framework again, as it might be helpful to developers looking for similar solutions and tools.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vvlzu888emif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moefc2/gpt5_style_router_but_for_any_llm_including_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1moefc2/gpt5_style_router_but_for_any_llm_including_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T17:04:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp6it6</id>
    <title>now it can turn your PDFs and docs into clean fine tuning datasets</title>
    <updated>2025-08-13T14:48:26+00:00</updated>
    <author>
      <name>/u/Interesting-Area6418</name>
      <uri>https://old.reddit.com/user/Interesting-Area6418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6it6/now_it_can_turn_your_pdfs_and_docs_into_clean/"&gt; &lt;img alt="now it can turn your PDFs and docs into clean fine tuning datasets" src="https://external-preview.redd.it/3sG_aaHa7N5A_uKldFg_ckXPZRKSagJ4eq_vlsxxQ-g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b05b86b865816d2d239bd9c679d5afbf3fd0461" title="now it can turn your PDFs and docs into clean fine tuning datasets" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/l4z271b5usif1.png?width=1812&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e4d98143bf7d60e382b53787e3ce6eb6272f8c8"&gt;The flow on how it generates datasets using local resources&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1mp6it6/video/hhwtavqwusif1/player"&gt;Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;repo is here &lt;a href="https://github.com/Datalore-ai/datalore-localgen-cli"&gt;https://github.com/Datalore-ai/datalore-localgen-cli&lt;/a&gt;&lt;/p&gt; &lt;p&gt;a while back I posted here about a terminal tool I made during my internship that could generate fine tuning datasets from real world data using deep research.&lt;br /&gt; after that post, I got quite a few dms and some really thoughtful feedback. thank you to everyone who reached out.&lt;/p&gt; &lt;p&gt;also, it got around 15 stars on GitHub which might be small but it was my first project so I am really happy about it. thanks to everyone who checked it out.&lt;/p&gt; &lt;p&gt;one of the most common requests was if it could work on local resources instead of only going online.&lt;br /&gt; so over the weekend I built a separate version that does exactly that.&lt;/p&gt; &lt;p&gt;you point it to a local file like a pdf, docx, jpg or txt and describe the dataset you want. it extracts the text, finds relevant parts with semantic search, applies your instructions through a generated schema, and outputs the dataset.&lt;/p&gt; &lt;p&gt;I am planning to integrate this into the main tool soon so it can handle both online and offline sources in one workflow.&lt;/p&gt; &lt;p&gt;if you want to see some example datasets it generated, feel free to dm me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Area6418"&gt; /u/Interesting-Area6418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6it6/now_it_can_turn_your_pdfs_and_docs_into_clean/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6it6/now_it_can_turn_your_pdfs_and_docs_into_clean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6it6/now_it_can_turn_your_pdfs_and_docs_into_clean/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T14:48:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1moqhvf</id>
    <title>Apple users: Unsloth's quants could be coming to MLX - if we show interest</title>
    <updated>2025-08-13T00:55:49+00:00</updated>
    <author>
      <name>/u/Bus9917</name>
      <uri>https://old.reddit.com/user/Bus9917</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As title.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/unsloth/comments/1mlsoar/upcoming_mlx_support_news/"&gt;yoracale &amp;quot;Working on it we have Macs now!&amp;quot; No_Conversation9561 &amp;quot;will there be UD MLX quants?&amp;quot; yoracale &amp;quot;Oh maybe if demand is more!&amp;quot;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you're interested in MLX UD quants - please show your interest.&lt;/p&gt; &lt;p&gt;(edit) yoracale &amp;quot;Ok thanks for the encouragement we'll see what we can do :)&amp;quot;&lt;/p&gt; &lt;p&gt;Thank you &lt;a href="/u/yorcale"&gt;u/yorcale&lt;/a&gt; and everyone who shows interest and support to Unsloth!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bus9917"&gt; /u/Bus9917 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moqhvf/apple_users_unsloths_quants_could_be_coming_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moqhvf/apple_users_unsloths_quants_could_be_coming_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1moqhvf/apple_users_unsloths_quants_could_be_coming_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T00:55:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mokxdv</id>
    <title>Why is everyone suddenly loving gpt-oss today?</title>
    <updated>2025-08-12T21:03:11+00:00</updated>
    <author>
      <name>/u/Pro-editor-1105</name>
      <uri>https://old.reddit.com/user/Pro-editor-1105</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone was hating on it and one fine day we got this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pro-editor-1105"&gt; /u/Pro-editor-1105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mokxdv/why_is_everyone_suddenly_loving_gptoss_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mokxdv/why_is_everyone_suddenly_loving_gptoss_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mokxdv/why_is_everyone_suddenly_loving_gptoss_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T21:03:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp4vxe</id>
    <title>Plant UML Generator LLM finetune</title>
    <updated>2025-08-13T13:44:53+00:00</updated>
    <author>
      <name>/u/lolzinventor</name>
      <uri>https://old.reddit.com/user/lolzinventor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4vxe/plant_uml_generator_llm_finetune/"&gt; &lt;img alt="Plant UML Generator LLM finetune" src="https://external-preview.redd.it/A2vjjq6KSpHPh1Yu7sh0j0dqmFW8S4lQdPeKAxnYi1A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ffbbaa449ca9e4ad6cfa5fac6ce6ffc23c3275b" title="Plant UML Generator LLM finetune" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Introducing pumlGenV2-1: The AI That Visualizes Complex Ideas as PlantUML Diagrams&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;What It Does&lt;/h1&gt; &lt;p&gt;Give it a complex question—whether about &lt;strong&gt;architecture&lt;/strong&gt;, &lt;strong&gt;philosophical debates&lt;/strong&gt;, or &lt;strong&gt;historical events&lt;/strong&gt;—and it generates a structured PlantUML diagram with (all input text is treated as a question):&lt;br /&gt; ✔ &lt;strong&gt;Logical relationships&lt;/strong&gt; between concepts&lt;br /&gt; ✔ &lt;strong&gt;Hierarchical grouping&lt;/strong&gt; (packages, components)&lt;br /&gt; ✔ &lt;strong&gt;Annotations &amp;amp; notes&lt;/strong&gt; for clarity&lt;br /&gt; ✔ &lt;strong&gt;Color-coding &amp;amp; legends&lt;/strong&gt; for readability&lt;/p&gt; &lt;h1&gt;Examples&lt;/h1&gt; &lt;h1&gt;1️⃣ Philosophy Made Visual&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; &lt;em&gt;&amp;quot;Can AI achieve true understanding, or is it just pattern recognition?&amp;quot;&lt;/em&gt;&lt;br /&gt; &lt;strong&gt;Output:&lt;/strong&gt;&lt;br /&gt; Philosophy Diagram&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Maps consciousness, semantics, and computational limits&lt;/li&gt; &lt;li&gt;Contrasts human vs. machine cognition&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2️⃣ Technical Architecture in Seconds&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; &lt;em&gt;&amp;quot;Diagram a microservices e-commerce system with Kubernetes, Kafka, and Istio.&amp;quot;&lt;/em&gt;&lt;br /&gt; &lt;strong&gt;Output:&lt;/strong&gt;&lt;br /&gt; Microservices Diagram&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Shows frontend/backend/services&lt;/li&gt; &lt;li&gt;Visualizes async flows (RabbitMQ, Kafka)&lt;/li&gt; &lt;li&gt;Includes security (mTLS, Istio)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-domain&lt;/strong&gt;: Works for tech, philosophy, history, law&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Precise syntax&lt;/strong&gt;: Generates valid PlantUML code&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context-aware&lt;/strong&gt;: Adds relevant notes and structure&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Limitations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Best for analytical questions (not narratives/stories)&lt;/li&gt; &lt;li&gt;Output may need minor tweaks for custom styling&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lolzinventor"&gt; /u/lolzinventor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/chrisrutherford/pumlGenV2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4vxe/plant_uml_generator_llm_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4vxe/plant_uml_generator_llm_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T13:44:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mom4qm</id>
    <title>The SERVE-AI-VAL Box - I built a portable local AI-in-a-box that runs off solar &amp; hand crank power for under $300</title>
    <updated>2025-08-12T21:49:20+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mom4qm/the_serveaival_box_i_built_a_portable_local/"&gt; &lt;img alt="The SERVE-AI-VAL Box - I built a portable local AI-in-a-box that runs off solar &amp;amp; hand crank power for under $300" src="https://external-preview.redd.it/NHFweWRmMW1ybmlmMcFLkpQep1-CmSQZ5gYPoLq4j-dB85f-NSL82e-hnm-C.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e54c5ae34be216108c952bff2df7249f4f229d91" title="The SERVE-AI-VAL Box - I built a portable local AI-in-a-box that runs off solar &amp;amp; hand crank power for under $300" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL:DR I made an offline, off-grid, self-powered, locally-hosted AI server using Google AI Edge Gallery, with Gemma3:4b running on an XREAL Beam Pro. It’s powered by a $50 MQOUNY solar / hand crank / USB power bank. I used heavy duty 3M Velcro-like picture hanging strips to hold it all together. I’m storing it all in a Faraday Cage Bag in case of EMPs (hope those never happen). I created a GitHub repo with the full parts list and DIY instructions here: &lt;a href="https://github.com/porespellar/SERVE-AI-VAL-Box"&gt;https://github.com/porespellar/SERVE-AI-VAL-Box&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ok, ok, so “built” is maybe too strong a word for this. It was really more just combining some hardware and software products together. &lt;/p&gt; &lt;p&gt;I’m not a “doomsday prepper” but I recognize the need for having access to a Local LLM in emergency off-grid situations where you have no power and no network connectivity, Maybe you need access to medical, or survival knowledge, or whatever, and perhaps a local LLM could provide relevant information. So that’s why I took on this project. That, and I just like tinkering around with fun tech stuff like this. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;My goal was to build a portable AI-in-a-box that:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is capable of running at least one LLM or multiple LLMs at an acceptable generation speed (preferably 2+ tk/ps)&lt;/li&gt; &lt;li&gt;Requires absolutely no connectivity (after initial provisioning of course) &lt;/li&gt; &lt;li&gt;Is handheld, extremely portable, and ruggedized if possible &lt;/li&gt; &lt;li&gt;Accepts multiple power sources (Solar, hand-crank, AC/DC, etc.) and provides multiple power output types &lt;/li&gt; &lt;li&gt;Has a camera, microphone, speaker, and touch screen for input &lt;/li&gt; &lt;li&gt;Doesn’t require any separate cords or power adapters that aren’t already attached / included in the box itself&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Those were the basic requirements I made before I began my research. Originally, I wanted to do the whole thing using a Raspberry Pi device with an AI accelerator, but the more I thought about it, I realized that an android-mini tablet or a budget unlocked android phone would probably be the best and easiest option. It’s really the perfect form factor and can readily run LLMs, so why reinvent the wheel when I could just get a cheap mini android tablet (XREAL Beam Pro - see my repo for full hardware details). &lt;/p&gt; &lt;p&gt;The second part of the solution was I wanted multiple power sources with a small form factor that closely matched the tablet / phone form factor. After a pretty exhaustive search, I found a Lithium battery power bank that had some really unique features. It had a solar panel, and a hand crank for charging, it included 3 built-in cords for power output, 2 USB types for power input, it even had a bonus flashlight, and was ruggedized and waterproof.&lt;/p&gt; &lt;p&gt;I’ve created a GitHub repository where I’ve posted the full part needed list, pictures, instructions for assembly, how to set up all the software needed, etc. &lt;/p&gt; &lt;p&gt;Here’s my GitHub: &lt;a href="https://github.com/porespellar/SERVE-AI-VAL-Box"&gt;https://github.com/porespellar/SERVE-AI-VAL-Box&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I know it’s not super complex or fancy, but I had fun building it and thought it was worth sharing in case anyone else was considering something similar. &lt;/p&gt; &lt;p&gt;If you have any questions about it. Please feel free to ask. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/40yzby3mrnif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mom4qm/the_serveaival_box_i_built_a_portable_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mom4qm/the_serveaival_box_i_built_a_portable_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T21:49:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1moxqht</id>
    <title>Free, open source, no data collected app (done as a hobby - no commercial purpose) running Qwen3-4B-4bit beats Mistral, Deepseek, Qwen web search functionalities and matches ChatGPT on most queries.</title>
    <updated>2025-08-13T07:17:50+00:00</updated>
    <author>
      <name>/u/Valuable-Run2129</name>
      <uri>https://old.reddit.com/user/Valuable-Run2129</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moxqht/free_open_source_no_data_collected_app_done_as_a/"&gt; &lt;img alt="Free, open source, no data collected app (done as a hobby - no commercial purpose) running Qwen3-4B-4bit beats Mistral, Deepseek, Qwen web search functionalities and matches ChatGPT on most queries." src="https://external-preview.redd.it/NXgwbnczbzZqcWlmMdVEZN68k09jQTVjAWzZ-7bMpjYBGMRNAxORrrhQtEef.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8096b9fe842e1af72c53de5b4efa4c24d0e4ead" title="Free, open source, no data collected app (done as a hobby - no commercial purpose) running Qwen3-4B-4bit beats Mistral, Deepseek, Qwen web search functionalities and matches ChatGPT on most queries." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys!&lt;br /&gt; The new updates to the LLM pigeon companion apps are out and have a much improved web search functionality.&lt;br /&gt; LLM Pigeon and LLM Pigeon Server are two companion apps. One for Mac and one for iOS. They are both free and open source. They collect no data (it's just a cool tool I wanted for myself).&lt;br /&gt; To put it in familiar terms, the iOS app is like ChatGPT, while the MacOS app is its personal LLM provider.&lt;br /&gt; The apps use iCloud to send back and forward your conversations (so it's not 100% local, but if you are like me and use iCloud for all your files anyways, it's a great solution - the most important thing to me is that my conversations aren't in any AI company hands).&lt;br /&gt; The app automatically hooks up to your LMStudio or Ollama, or it allows you to download directly a handful of models without needing anything else.&lt;/p&gt; &lt;p&gt;The new updates have a much improved web search functionality. I'm attaching a video of an example running on my base Mac Mini (expect 2x/3x speed bump with the Pro chip). LLM Pigeon on the left, Mistral in the middle and GPT5 on the right.&lt;br /&gt; It's not a deep research, which is something I'm working on right now, but it beats easily all the regular web search functionalities of mid AI apps like Mistral, Deepseek, Qwen... it doesn't beat GPT5, but it provides comparable answers on many queries. Which is more than I asked for before starting this project.&lt;br /&gt; Give the apps a try!&lt;/p&gt; &lt;p&gt;This is the iOS app:&lt;br /&gt; &lt;a href="https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB"&gt;https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is the MacOS app:&lt;br /&gt; &lt;a href="https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12"&gt;https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12&lt;/a&gt;&lt;/p&gt; &lt;p&gt;here they are on github:&lt;br /&gt; &lt;a href="https://github.com/permaevidence/LLM-Pigeon-Server"&gt;https://github.com/permaevidence/LLM-Pigeon-Server&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/permaevidence/LLM-Pigeon"&gt;https://github.com/permaevidence/LLM-Pigeon&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Valuable-Run2129"&gt; /u/Valuable-Run2129 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y6zpo2o6jqif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moxqht/free_open_source_no_data_collected_app_done_as_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1moxqht/free_open_source_no_data_collected_app_done_as_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T07:17:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mozddg</id>
    <title>Is there a wiki that is updated once a month containing recommended models per use case?</title>
    <updated>2025-08-13T09:04:40+00:00</updated>
    <author>
      <name>/u/Yugen42</name>
      <uri>https://old.reddit.com/user/Yugen42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As someone who doesn't constantly follow developments, is there a good resource for determining good models for different use cases? I understand benchmarks are suboptimal, but even something like a vote based resource or something that's manually curated would be great. Things are still moving fast, and it's hard to tell which models are actually good, and downloading and manually testing 20+GB files is quite inefficient. As is posting here and asking every time, I feel like we could identify a few common categories and a few common hardware configurations and curate a good list.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Yugen42"&gt; /u/Yugen42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mozddg/is_there_a_wiki_that_is_updated_once_a_month/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mozddg/is_there_a_wiki_that_is_updated_once_a_month/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mozddg/is_there_a_wiki_that_is_updated_once_a_month/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T09:04:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp1ras</id>
    <title>[Beta] Local TTS Studio with Kokoro, Kitten TTS, and Piper built in, completely in JavaScript (930+ voices to choose from)</title>
    <updated>2025-08-13T11:24:08+00:00</updated>
    <author>
      <name>/u/CommunityTough1</name>
      <uri>https://old.reddit.com/user/CommunityTough1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! Last week, &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mi45h1/kitten_tts_web_demo/"&gt;I posted&lt;/a&gt; a Kitten TTS web demo that it seemed like a lot of people liked, so I decided to take it a step further and add Piper and Kokoro to the project! The project lets you load Kitten TTS, Piper Voices, or Kokoro completely in the browser, 100% local. It also has a quick preview feature in the voice selection dropdowns.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://clowerweb.github.io/tts-studio/"&gt;&lt;strong&gt;Online Demo&lt;/strong&gt;&lt;/a&gt; (GitHub Pages)&lt;/h1&gt; &lt;p&gt;Repo (Apache 2.0): &lt;a href="https://github.com/clowerweb/tts-studio"&gt;https://github.com/clowerweb/tts-studio&lt;/a&gt; &lt;/p&gt; &lt;p&gt;The &lt;a href="https://clowerweb.github.io/kitten-tts-web-demo/"&gt;Kitten TTS standalone&lt;/a&gt; was also updated to include a bunch of your feedback including bug fixes and requested features! There's also a &lt;a href="https://clowerweb.github.io/piper-tts-web-demo/"&gt;Piper standalone&lt;/a&gt; available.&lt;/p&gt; &lt;p&gt;Lemme know what you think and if you've got any feedback or suggestions!&lt;/p&gt; &lt;p&gt;If this project helps you save a few GPU hours, please consider &lt;a href="https://github.com/sponsors/clowerweb"&gt;grabbing me a coffee!&lt;/a&gt; ☕&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CommunityTough1"&gt; /u/CommunityTough1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1ras/beta_local_tts_studio_with_kokoro_kitten_tts_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1ras/beta_local_tts_studio_with_kokoro_kitten_tts_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1ras/beta_local_tts_studio_with_kokoro_kitten_tts_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T11:24:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mokyp0</id>
    <title>Fuck Groq, Amazon, Azure, Nebius, fucking scammers</title>
    <updated>2025-08-12T21:04:34+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mokyp0/fuck_groq_amazon_azure_nebius_fucking_scammers/"&gt; &lt;img alt="Fuck Groq, Amazon, Azure, Nebius, fucking scammers" src="https://preview.redd.it/76rkrod6lnif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=46f02163e60e8403a123e529ea53f224ae744ef3" title="Fuck Groq, Amazon, Azure, Nebius, fucking scammers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/76rkrod6lnif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mokyp0/fuck_groq_amazon_azure_nebius_fucking_scammers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mokyp0/fuck_groq_amazon_azure_nebius_fucking_scammers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T21:04:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mov3d9</id>
    <title>I tried the Jan-v1 model released today and here are the results</title>
    <updated>2025-08-13T04:41:17+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mov3d9/i_tried_the_janv1_model_released_today_and_here/"&gt; &lt;img alt="I tried the Jan-v1 model released today and here are the results" src="https://b.thumbs.redditmedia.com/Vlwsqp7XrYSffFPbRsbOOK6OJG64K1FIWlwv_HqoDjw.jpg" title="I tried the Jan-v1 model released today and here are the results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Search tool was brave. Tried 3 searches and its broken - the chat screenshots are attached and summarized below&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Whats the GDP of the US?:&lt;/strong&gt; Gave me a growth rate number, not the GDP figure itself.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Whats the popilation of the world?:&lt;/strong&gt; Got stuck in loop searching for the same thing and then thinking. I waited for several minutes, gave up and stopped it.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Whats the size of the Jan AI team and where are they based?:&lt;/strong&gt; Same thing.. This time I let it go on for over 5 minutes and was just in a loop. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mov3d9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mov3d9/i_tried_the_janv1_model_released_today_and_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mov3d9/i_tried_the_janv1_model_released_today_and_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T04:41:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mon8it</id>
    <title>Woah. Letta vs Mem0. (For AI memory nerds)</title>
    <updated>2025-08-12T22:34:04+00:00</updated>
    <author>
      <name>/u/LoveMind_AI</name>
      <uri>https://old.reddit.com/user/LoveMind_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mon8it/woah_letta_vs_mem0_for_ai_memory_nerds/"&gt; &lt;img alt="Woah. Letta vs Mem0. (For AI memory nerds)" src="https://preview.redd.it/8sl96y461oif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e2350ded0c596ea924dac589bbe58ff31eb68579" title="Woah. Letta vs Mem0. (For AI memory nerds)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m an absolute AI memory nerd, and have probably read every proposal made about memory, and demoed virtually all of the professional solutions out there. But I’m absolutely stunned to see Letta basically call out Mem0 like a WWE feud. To be clear: I do not have any kind of affiliation with any memory company (beyond my own, which is not a memory company per se), but Letta (which began as MemGPT) are in many ways the OGs in this space. So, in this tiny corner of AI nerd land, this is a fairly wild smack down to watch. Just posting this in case any other memory heads are paying attention. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LoveMind_AI"&gt; /u/LoveMind_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8sl96y461oif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mon8it/woah_letta_vs_mem0_for_ai_memory_nerds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mon8it/woah_letta_vs_mem0_for_ai_memory_nerds/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T22:34:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mowlg2</id>
    <title>Multi-Token Prediction(MTP) in llama.cpp</title>
    <updated>2025-08-13T06:07:57+00:00</updated>
    <author>
      <name>/u/UpperParamedicDude</name>
      <uri>https://old.reddit.com/user/UpperParamedicDude</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15225"&gt;https://github.com/ggml-org/llama.cpp/pull/15225&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The dev says they're pretty new to ML outside of python so patience is required. It's only a draft for now but i felt like i need to share it with you folks, maybe some of you have the required knowledge and skills to help them&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UpperParamedicDude"&gt; /u/UpperParamedicDude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mowlg2/multitoken_predictionmtp_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mowlg2/multitoken_predictionmtp_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mowlg2/multitoken_predictionmtp_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T06:07:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp4gwl</id>
    <title>Beelink GTR9 Pro Mini PC Launched: 140W AMD Ryzen AI MAX+ 395 APU, 128 GB LPDDR5x 8000 MT/s Memory, 2 TB Crucial SSD, Dual 10GbE LAN For $1985</title>
    <updated>2025-08-13T13:28:20+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4gwl/beelink_gtr9_pro_mini_pc_launched_140w_amd_ryzen/"&gt; &lt;img alt="Beelink GTR9 Pro Mini PC Launched: 140W AMD Ryzen AI MAX+ 395 APU, 128 GB LPDDR5x 8000 MT/s Memory, 2 TB Crucial SSD, Dual 10GbE LAN For $1985" src="https://external-preview.redd.it/Bvw60PvhPgoef0Ng9Djae_QLUotq8vncLfnhqt8cL74.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=218782160b09032a3d5202434bc6cfb1ccd15a36" title="Beelink GTR9 Pro Mini PC Launched: 140W AMD Ryzen AI MAX+ 395 APU, 128 GB LPDDR5x 8000 MT/s Memory, 2 TB Crucial SSD, Dual 10GbE LAN For $1985" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/beelink-gtr9-pro-mini-pc-launched-140w-amd-ryzen-ai-max-395-128-gb-dual-10gbe-1985-usd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4gwl/beelink_gtr9_pro_mini_pc_launched_140w_amd_ryzen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4gwl/beelink_gtr9_pro_mini_pc_launched_140w_amd_ryzen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T13:28:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mox183</id>
    <title>[UPDATE] DocStrange - Structured data extraction from images/pdfs/docs</title>
    <updated>2025-08-13T06:34:17+00:00</updated>
    <author>
      <name>/u/LostAmbassador6872</name>
      <uri>https://old.reddit.com/user/LostAmbassador6872</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mox183/update_docstrange_structured_data_extraction_from/"&gt; &lt;img alt="[UPDATE] DocStrange - Structured data extraction from images/pdfs/docs" src="https://preview.redd.it/nclxmfireqif1.gif?width=640&amp;amp;crop=smart&amp;amp;s=3b16e237e84f24c17c50122327bd2265e9514a10" title="[UPDATE] DocStrange - Structured data extraction from images/pdfs/docs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I previously shared the open‑source library DocStrange. Now I have hosted it as a free to use web app to upload pdfs/images/docs to get clean structured data in Markdown/CSV/JSON/Specific-fields and other formats. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Live Demo:&lt;/strong&gt; &lt;a href="https://docstrange.nanonets.com"&gt;&lt;strong&gt;https://docstrange.nanonets.com&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Would love to hear feedbacks! &lt;/p&gt; &lt;p&gt;Original Post - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mepr38/docstrange_open_source_document_data_extractor/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mepr38/docstrange_open_source_document_data_extractor/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostAmbassador6872"&gt; /u/LostAmbassador6872 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nclxmfireqif1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mox183/update_docstrange_structured_data_extraction_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mox183/update_docstrange_structured_data_extraction_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T06:34:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp2wq3</id>
    <title>There is a new text-to-image model named nano-banana</title>
    <updated>2025-08-13T12:20:32+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp2wq3/there_is_a_new_texttoimage_model_named_nanobanana/"&gt; &lt;img alt="There is a new text-to-image model named nano-banana" src="https://preview.redd.it/jmw88evj4sif1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53c768eb9781ffe9119d98e2a2e9f3c88c8adab5" title="There is a new text-to-image model named nano-banana" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jmw88evj4sif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp2wq3/there_is_a_new_texttoimage_model_named_nanobanana/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp2wq3/there_is_a_new_texttoimage_model_named_nanobanana/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T12:20:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1moz341</id>
    <title>gpt-oss-120B most intelligent model that fits on an H100 in native precision</title>
    <updated>2025-08-13T08:46:18+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moz341/gptoss120b_most_intelligent_model_that_fits_on_an/"&gt; &lt;img alt="gpt-oss-120B most intelligent model that fits on an H100 in native precision" src="https://preview.redd.it/4okvse7e2rif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=943876a00ac037e2110c919f54e46c6e6d4303b4" title="gpt-oss-120B most intelligent model that fits on an H100 in native precision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Interesting analysis thread: &lt;a href="https://x.com/artificialanlys/status/1952887733803991070"&gt;https://x.com/artificialanlys/status/1952887733803991070&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4okvse7e2rif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moz341/gptoss120b_most_intelligent_model_that_fits_on_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1moz341/gptoss120b_most_intelligent_model_that_fits_on_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T08:46:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp5bjc</id>
    <title>God I love Qwen and llamacpp so much!</title>
    <updated>2025-08-13T14:01:37+00:00</updated>
    <author>
      <name>/u/Limp_Classroom_2645</name>
      <uri>https://old.reddit.com/user/Limp_Classroom_2645</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"&gt; &lt;img alt="God I love Qwen and llamacpp so much!" src="https://external-preview.redd.it/YWE3eDdxZG5tc2lmMRvVg1psIEfKedgCcU_ySdSE0fdUxqG9M3HUjgrx1S5i.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8afab7c45ab87f6ac2ce8db445bb27de25840096" title="God I love Qwen and llamacpp so much!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Local batch inference with qwen3 30B Instruct on a single RTX3090, 4 requests in parallel &lt;/p&gt; &lt;p&gt;Gonna use it to mass process some data to generate insights about our platform usage&lt;/p&gt; &lt;p&gt;I feel like I'm hitting my limits here and gonna need a multi GPU setup soon 😄&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Limp_Classroom_2645"&gt; /u/Limp_Classroom_2645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ur3oxzhnmsif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T14:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp1j7e</id>
    <title>Peak safety theater: gpt-oss-120b refuses to discuss implementing web search in llama.cpp</title>
    <updated>2025-08-13T11:12:30+00:00</updated>
    <author>
      <name>/u/csixtay</name>
      <uri>https://old.reddit.com/user/csixtay</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1j7e/peak_safety_theater_gptoss120b_refuses_to_discuss/"&gt; &lt;img alt="Peak safety theater: gpt-oss-120b refuses to discuss implementing web search in llama.cpp" src="https://preview.redd.it/j7hi9xgjrrif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=328e2e9fc9cd738d0907c1394e77c1ec12b827b3" title="Peak safety theater: gpt-oss-120b refuses to discuss implementing web search in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/csixtay"&gt; /u/csixtay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j7hi9xgjrrif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1j7e/peak_safety_theater_gptoss120b_refuses_to_discuss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1j7e/peak_safety_theater_gptoss120b_refuses_to_discuss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T11:12:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
</feed>
