<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-23T15:34:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ivpa6r</id>
    <title>Abusing WebUI Artifacts (Again)</title>
    <updated>2025-02-22T18:21:42+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivpa6r/abusing_webui_artifacts_again/"&gt; &lt;img alt="Abusing WebUI Artifacts (Again)" src="https://external-preview.redd.it/cDhodWx2cjZncWtlMQu5ROv8uFTKESGnRAtwEPoYjV9P5uC6sL5S6dTjkckO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96024f10c2840d152729af0e38edf6258ccf9cd5" title="Abusing WebUI Artifacts (Again)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1eav6zr6gqke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivpa6r/abusing_webui_artifacts_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivpa6r/abusing_webui_artifacts_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T18:21:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivvxep</id>
    <title>L2E llama2.c on Commodore C-64</title>
    <updated>2025-02-22T23:16:37+00:00</updated>
    <author>
      <name>/u/AMICABoard</name>
      <uri>https://old.reddit.com/user/AMICABoard</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvxep/l2e_llama2c_on_commodore_c64/"&gt; &lt;img alt="L2E llama2.c on Commodore C-64" src="https://b.thumbs.redditmedia.com/_5JXsW38zuiDL6biPVP13Lmb8qwW0s56fYU2rf1xacY.jpg" title="L2E llama2.c on Commodore C-64" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have you ever wanted to inference tiny stories on a C64 while going about your daily life and then return after many years to read a story? No? Well, as luck would have it, now YOU CAN!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w8yc07k7wrke1.png?width=725&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9ae9426ef9ae4cea7c8acd33772becebfcf0044"&gt;https://preview.redd.it/w8yc07k7wrke1.png?width=725&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9ae9426ef9ae4cea7c8acd33772becebfcf0044&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/trholding/semu-c64"&gt;https://github.com/trholding/semu-c64&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/VulcanIgnis/status/1893420241310335329"&gt;VulcanIgnis&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AMICABoard"&gt; /u/AMICABoard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvxep/l2e_llama2c_on_commodore_c64/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvxep/l2e_llama2c_on_commodore_c64/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvxep/l2e_llama2c_on_commodore_c64/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T23:16:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivtr02</id>
    <title>Local TTS document reader web app (EPUB/PDF)</title>
    <updated>2025-02-22T21:35:57+00:00</updated>
    <author>
      <name>/u/richardr1126</name>
      <uri>https://old.reddit.com/user/richardr1126</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivtr02/local_tts_document_reader_web_app_epubpdf/"&gt; &lt;img alt="Local TTS document reader web app (EPUB/PDF)" src="https://external-preview.redd.it/N3NpNmJkYnFlcmtlMZSfHsyHsWneNrgFt80RlQiAMZETD9pSG3kMthVQacWZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee91a64c4cffd32d5265d584881203470826fdc5" title="Local TTS document reader web app (EPUB/PDF)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/richardr1126"&gt; /u/richardr1126 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/olajvdbqerke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivtr02/local_tts_document_reader_web_app_epubpdf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivtr02/local_tts_document_reader_web_app_epubpdf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T21:35:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw4cot</id>
    <title>How did we all miss the release of AutoGen Studio 0.4.1.11? (incorporates new visual drag-and-drop interface for building agent workflows).</title>
    <updated>2025-02-23T07:02:02+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Am I the only one that COMPLETELY MISSED THIS MAJOR RELEASE?? I had been waiting for the new Autogen Studio drag-and-drop interface for like 6 months, and apparently it was released like a month ago with a major patch arriving last week. This got pretty much zero press and seems lost in the shuffle due to all the DeepSeek news most likely. AutoGen Studio’s 0,4’s interface is way better than 0.2. They’ve incorporated a ton of stuff, the biggest addition being the drag-and-drop visual workflow interface. I think they also added Magentic One agents. Magentic One was pretty great on its own, but kind of a pain in the ass to get running. Now it’s integrated into AgentChat I believe. This seems like a huge step forward and makes it very compelling and on par with Crew AI in my opinion. &lt;/p&gt; &lt;p&gt;Here is the release page with all the details:&lt;/p&gt; &lt;p&gt;&lt;a href="https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html"&gt;https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And the Pypi download page &lt;/p&gt; &lt;p&gt;&lt;a href="https://pypi.org/project/autogenstudio/"&gt;https://pypi.org/project/autogenstudio/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw4cot/how_did_we_all_miss_the_release_of_autogen_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw4cot/how_did_we_all_miss_the_release_of_autogen_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw4cot/how_did_we_all_miss_the_release_of_autogen_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T07:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw9lls</id>
    <title>Qwen2.5 1M context works on llama.cpp?</title>
    <updated>2025-02-23T12:59:42+00:00</updated>
    <author>
      <name>/u/NickNau</name>
      <uri>https://old.reddit.com/user/NickNau</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are these models, but according to model card, &amp;quot;Accuracy degradation may occur for sequences exceeding 262,144 tokens until improved support is added.&amp;quot;&lt;/p&gt; &lt;p&gt;Qwen's blog post talks about &amp;quot;Dual Chunk Attention&amp;quot; that allows this. (&lt;a href="https://qwenlm.github.io/blog/qwen2.5-1m/"&gt;https://qwenlm.github.io/blog/qwen2.5-1m/&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;The question is - was this already implemented in llama.cpp, and things like LM Studio? &lt;/p&gt; &lt;p&gt;If not - what is a strategy of using these models? Just setting context for 256k and thats it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NickNau"&gt; /u/NickNau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9lls/qwen25_1m_context_works_on_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9lls/qwen25_1m_context_works_on_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9lls/qwen25_1m_context_works_on_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T12:59:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivvoto</id>
    <title>Qwen2.5 VL 7B Instruct GGUF + Benchmarks</title>
    <updated>2025-02-22T23:05:11+00:00</updated>
    <author>
      <name>/u/Ragecommie</name>
      <uri>https://old.reddit.com/user/Ragecommie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;We were able to get Qwen2.5 VL working on llama.cpp!&lt;br /&gt; It is not official yet, but it's pretty easy to get going with a custom build.&lt;br /&gt; Instructions &lt;a href="https://github.com/ggml-org/llama.cpp/issues/11483#issuecomment-2676422772"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Over the next couple of days, we'll upload quants, along with tests / performance evals here:&lt;br /&gt; &lt;a href="https://huggingface.co/IAILabs/Qwen2.5-VL-7b-Instruct-GGUF/tree/main"&gt;https://huggingface.co/IAILabs/Qwen2.5-VL-7b-Instruct-GGUF/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Original 16-bit and Q8_0 are up along with the mmproj model.&lt;/p&gt; &lt;p&gt;First impressions are pretty good, not only in terms of quality, but speed as well.&lt;/p&gt; &lt;p&gt;Will post updates and more info as we go!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ragecommie"&gt; /u/Ragecommie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvoto/qwen25_vl_7b_instruct_gguf_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvoto/qwen25_vl_7b_instruct_gguf_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvoto/qwen25_vl_7b_instruct_gguf_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T23:05:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw0zgo</id>
    <title>It's not that mistral 24b is dry, it's parsable and it rocks!</title>
    <updated>2025-02-23T03:36:00+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to say that, what are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw0zgo/its_not_that_mistral_24b_is_dry_its_parsable_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw0zgo/its_not_that_mistral_24b_is_dry_its_parsable_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw0zgo/its_not_that_mistral_24b_is_dry_its_parsable_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T03:36:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivtten</id>
    <title>Perplexity R1 Llama 70B Uncensored GGUFs &amp; Dynamic 4bit quant</title>
    <updated>2025-02-22T21:39:03+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perplexity I think quietly released uncensored versions of DeepSeek R1 Llama 70B Distilled versions - I actually totally missed this - did anyone see an announcement or know about this?&lt;/p&gt; &lt;p&gt;I uploaded 2bit all the way until 16bit GGUFs for the model: &lt;a href="https://huggingface.co/unsloth/r1-1776-distill-llama-70b-GGUF"&gt;https://huggingface.co/unsloth/r1-1776-distill-llama-70b-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also uploaded dynamic 4bit quants for finetuning and vLLM serving: &lt;a href="https://huggingface.co/unsloth/r1-1776-distill-llama-70b-unsloth-bnb-4bit"&gt;https://huggingface.co/unsloth/r1-1776-distill-llama-70b-unsloth-bnb-4bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A few days ago I uploaded dynamic 2bit, 3bit and 4bit quants for the full R1 Uncensored 671B MoE version, which dramatically increase accuracy by not quantizing certain modules. This is similar to the 1.58bit quant of DeepSeek R1 we did! &lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF"&gt;https://huggingface.co/unsloth/r1-1776-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivtten/perplexity_r1_llama_70b_uncensored_ggufs_dynamic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivtten/perplexity_r1_llama_70b_uncensored_ggufs_dynamic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivtten/perplexity_r1_llama_70b_uncensored_ggufs_dynamic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T21:39:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwc4bv</id>
    <title>Built a Chrome Extension That Uses Local AI (LLaVa) to Generate Filenames for Images</title>
    <updated>2025-02-23T15:06:23+00:00</updated>
    <author>
      <name>/u/KonradFreeman</name>
      <uri>https://old.reddit.com/user/KonradFreeman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I got tired of downloading images named &lt;strong&gt;“IMG_20240223_132459.jpg”&lt;/strong&gt; and having to manually rename them to something useful. So I built a &lt;strong&gt;Chrome extension&lt;/strong&gt; that uses &lt;strong&gt;local AI (LLaVa + Ollama)&lt;/strong&gt; to &lt;strong&gt;analyze image content and generate descriptive filenames automatically&lt;/strong&gt; before saving them. No more digging through random files trying to figure out what’s what.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How It Works:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;• Right-click an image → “Save with AI-generated filename”&lt;/p&gt; &lt;p&gt;• The extension runs &lt;strong&gt;LLaVa locally&lt;/strong&gt; (so no external API calls, no data leaves your machine)&lt;/p&gt; &lt;p&gt;• It suggests a filename based on what’s in the image (e.g., &lt;em&gt;“golden-retriever-playing-park.jpg”&lt;/em&gt;)&lt;/p&gt; &lt;p&gt;• Option to preview/edit before saving&lt;/p&gt; &lt;p&gt;• Supports &lt;strong&gt;custom filename templates&lt;/strong&gt; ({object}-{location}-{date}.jpg)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Local AI?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most AI-powered tools send your data to a server. I don’t like that. This one runs &lt;strong&gt;entirely on your machine&lt;/strong&gt; using &lt;strong&gt;Ollama&lt;/strong&gt;, which means:&lt;/p&gt; &lt;p&gt;✅ &lt;strong&gt;Private&lt;/strong&gt; – No cloud processing, everything stays local&lt;/p&gt; &lt;p&gt;✅ &lt;strong&gt;Fast&lt;/strong&gt; – No latency from API calls&lt;/p&gt; &lt;p&gt;✅ &lt;strong&gt;Free&lt;/strong&gt; – No subscription or token limits&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;• &lt;strong&gt;LLaVa for image analysis&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;• &lt;strong&gt;Ollama as the local model runner&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;• &lt;strong&gt;Chrome Extension API (contextMenus, downloads, storage, etc.)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;• &lt;strong&gt;DeclarativeNetRequest for host access&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Who Might Find This Useful?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;• People who &lt;strong&gt;download a lot of images&lt;/strong&gt; and hate messy filenames&lt;/p&gt; &lt;p&gt;• &lt;strong&gt;Researchers, content creators, designers&lt;/strong&gt;—anyone who needs better file organization&lt;/p&gt; &lt;p&gt;• Privacy-conscious users who want &lt;strong&gt;AI features without sending data online&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try It Out / Feedback?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I’d love to hear thoughts from others working with &lt;strong&gt;local AI&lt;/strong&gt;, &lt;strong&gt;Chrome extensions&lt;/strong&gt;, or &lt;strong&gt;automation tools&lt;/strong&gt;. Would you use something like this? Any features you’d want added?&lt;/p&gt; &lt;p&gt;If you’re interested you can download and try it out for free from my github repo while I wait for it to be approved by the Chrome Web Store:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/kliewerdaniel/chrome-ai-filename-generator"&gt;https://github.com/kliewerdaniel/chrome-ai-filename-generator&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KonradFreeman"&gt; /u/KonradFreeman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwc4bv/built_a_chrome_extension_that_uses_local_ai_llava/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwc4bv/built_a_chrome_extension_that_uses_local_ai_llava/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwc4bv/built_a_chrome_extension_that_uses_local_ai_llava/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T15:06:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwbe08</id>
    <title>Where in the inference world can a 3rd class consumer-grade AMD GPU owner get Flash Attention?!</title>
    <updated>2025-02-23T14:31:41+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;... I don't care if the backend is ROCm, Vulkan or a hairy buttock. Just something with flashattention to save on the super precious VRAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwbe08/where_in_the_inference_world_can_a_3rd_class/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwbe08/where_in_the_inference_world_can_a_3rd_class/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwbe08/where_in_the_inference_world_can_a_3rd_class/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T14:31:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw5p9d</id>
    <title>GitHub - stacklok/mockllm: MockLLM, when you want it to do what you tell it to do!</title>
    <updated>2025-02-23T08:37:31+00:00</updated>
    <author>
      <name>/u/zero_proof_fork</name>
      <uri>https://old.reddit.com/user/zero_proof_fork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw5p9d/github_stacklokmockllm_mockllm_when_you_want_it/"&gt; &lt;img alt="GitHub - stacklok/mockllm: MockLLM, when you want it to do what you tell it to do!" src="https://external-preview.redd.it/GnlFZWyrYECCoMkLp0lVi1tNxIZHtjUtX7RTE1eOK5M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc7815aa88eb6cdca7dd6f6e74b11f741a4bf50f" title="GitHub - stacklok/mockllm: MockLLM, when you want it to do what you tell it to do!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero_proof_fork"&gt; /u/zero_proof_fork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/stacklok/mockllm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw5p9d/github_stacklokmockllm_mockllm_when_you_want_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw5p9d/github_stacklokmockllm_mockllm_when_you_want_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T08:37:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivyc62</id>
    <title>Chirp 3b | Ozone AI</title>
    <updated>2025-02-23T01:15:13+00:00</updated>
    <author>
      <name>/u/Perfect-Bowl-1601</name>
      <uri>https://old.reddit.com/user/Perfect-Bowl-1601</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;From the same creators of Reverb 7b, we present, &lt;strong&gt;CHIRP 3b&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We’re excited to introduce our latest model: &lt;strong&gt;Chirp-3b!&lt;/strong&gt; The Ozone AI team has been pouring effort into this one, and we think it’s a big step up for 3B performance. Chirp-3b was trained on over 50 million tokens of distilled data from GPT-4o, fine-tuned from a solid base model to bring some serious capability to the table.&lt;/p&gt; &lt;p&gt;The benchmarks are in, and Chirp-3b is shining! It’s delivering standout results on both MMLU Pro and IFEval, exceeding what we’d expect from a model this size. Check out the details:&lt;/p&gt; &lt;h3&gt;MMLU Pro&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Subject&lt;/th&gt; &lt;th&gt;Average Accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Biology&lt;/td&gt; &lt;td&gt;0.6234&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Business&lt;/td&gt; &lt;td&gt;0.5032&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Chemistry&lt;/td&gt; &lt;td&gt;0.3701&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Computer Science&lt;/td&gt; &lt;td&gt;0.4268&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Economics&lt;/td&gt; &lt;td&gt;0.5284&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Engineering&lt;/td&gt; &lt;td&gt;0.3013&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Health&lt;/td&gt; &lt;td&gt;0.3900&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;History&lt;/td&gt; &lt;td&gt;0.3885&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Law&lt;/td&gt; &lt;td&gt;0.2252&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Math&lt;/td&gt; &lt;td&gt;0.5736&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Other&lt;/td&gt; &lt;td&gt;0.4145&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Philosophy&lt;/td&gt; &lt;td&gt;0.3687&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Physics&lt;/td&gt; &lt;td&gt;0.3995&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Psychology&lt;/td&gt; &lt;td&gt;0.5589&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Overall Average&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.4320&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;That’s a 9-point boost over the base model—pretty remarkable!&lt;/p&gt; &lt;h3&gt;IFEval&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;72%&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;These gains make Chirp-3b a compelling option for its class. (More benchmarks are on the way!)&lt;/p&gt; &lt;p&gt;Model Card &amp;amp; Download: &lt;a href="https://huggingface.co/ozone-research/Chirp-01"&gt;https://huggingface.co/ozone-research/Chirp-01&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’re passionate about advancing open-source LLMs, and Chirp-3b is a proud part of that journey. We’ve got more models cooking, including 2B and bigger versions, so watch this space!&lt;/p&gt; &lt;p&gt;We’re pumped to get your feedback! Download Chirp-3b, give it a spin, and let us know how it performs for you. Your input helps us keep improving.&lt;/p&gt; &lt;p&gt;Thanks for the support—we’re eager to see what you create with Chirp-3b!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Perfect-Bowl-1601"&gt; /u/Perfect-Bowl-1601 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivyc62/chirp_3b_ozone_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivyc62/chirp_3b_ozone_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivyc62/chirp_3b_ozone_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T01:15:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw3mjz</id>
    <title>Why don’t LLMs use alibi? Were these result found be non-reproducible? I’ve only read of the failed Bloom model. Anyone else?</title>
    <updated>2025-02-23T06:13:14+00:00</updated>
    <author>
      <name>/u/grey-seagull</name>
      <uri>https://old.reddit.com/user/grey-seagull</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw3mjz/why_dont_llms_use_alibi_were_these_result_found/"&gt; &lt;img alt="Why don’t LLMs use alibi? Were these result found be non-reproducible? I’ve only read of the failed Bloom model. Anyone else?" src="https://preview.redd.it/67czpmm7ztke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebb21e546af6ea899f35bd1b3facbe9552d08b76" title="Why don’t LLMs use alibi? Were these result found be non-reproducible? I’ve only read of the failed Bloom model. Anyone else?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grey-seagull"&gt; /u/grey-seagull &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/67czpmm7ztke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw3mjz/why_dont_llms_use_alibi_were_these_result_found/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw3mjz/why_dont_llms_use_alibi_were_these_result_found/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T06:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw2z5w</id>
    <title>Surprising Performance on CPU-only Ryzen 9 9950x | 64 GB DDR5 Build</title>
    <updated>2025-02-23T05:31:50+00:00</updated>
    <author>
      <name>/u/gmdtrn</name>
      <uri>https://old.reddit.com/user/gmdtrn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While I wait for my GPU to arrive, I decided to give my CPU-only system a run. I just purchased a bundle from Microcenter for a MSI X870E MAG Tomahawk WiFi motherboard, Ryzen 9 9950x CPU (16 cores, 32 threads), and G.Skill Flare X5 DDR5 RAM (though I upgraded to 64 GB). The OS I'm running is PopOS (Ubuntu derivative).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I'm getting ~12 tokens/sec on `deepseek-r1:8b` (which is build on Llama3.1:8b) running off the CPU alone&lt;/strong&gt;. I was quite impressed by this as it's out-performing my RTX 2060 mobile by about 30-35%. Thus, it may make for a solid LLM budget build. So, I wanted to share it here.&lt;/p&gt; &lt;p&gt;I hope some of you find this useful. And, I apologize for not performing a more thorough analysis and presenting it here. However, I am up against the clock on a quiz I must take tomorrow that I need to study for.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gmdtrn"&gt; /u/gmdtrn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw2z5w/surprising_performance_on_cpuonly_ryzen_9_9950x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw2z5w/surprising_performance_on_cpuonly_ryzen_9_9950x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw2z5w/surprising_performance_on_cpuonly_ryzen_9_9950x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T05:31:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivrprb</id>
    <title>Kimi.ai released Moonlight a 3B/16B MoE model trained with their improved Muon optimizer.</title>
    <updated>2025-02-22T20:05:32+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrprb/kimiai_released_moonlight_a_3b16b_moe_model/"&gt; &lt;img alt="Kimi.ai released Moonlight a 3B/16B MoE model trained with their improved Muon optimizer." src="https://external-preview.redd.it/8-2Sl3ne20MUsYCwIhDQN3Ob-UIeeembj6eG4654s7k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=41c166131e1d55a90ce452e5b495385744c6917d" title="Kimi.ai released Moonlight a 3B/16B MoE model trained with their improved Muon optimizer." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Moonlight beats other similar SOTA models in most of the benchmarks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MoonshotAI/Moonlight?tab=readme-ov-file"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrprb/kimiai_released_moonlight_a_3b16b_moe_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrprb/kimiai_released_moonlight_a_3b16b_moe_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T20:05:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivrtqk</id>
    <title>DeepSeek Founders Are Worth $1 Billion or $150 Billion Depending Who You Ask</title>
    <updated>2025-02-22T20:10:26+00:00</updated>
    <author>
      <name>/u/cramdev</name>
      <uri>https://old.reddit.com/user/cramdev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrtqk/deepseek_founders_are_worth_1_billion_or_150/"&gt; &lt;img alt="DeepSeek Founders Are Worth $1 Billion or $150 Billion Depending Who You Ask" src="https://external-preview.redd.it/Y2j22dshKg69yVQTELClk4zSnJfoKi77KX2nOwS6buo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba4da483ee892a27f534028e7c20f82a3a3b889f" title="DeepSeek Founders Are Worth $1 Billion or $150 Billion Depending Who You Ask" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cramdev"&gt; /u/cramdev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-02-10/deepseek-could-make-founder-liang-wenfeng-one-of-the-world-s-richest-people?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTczOTIzNzk1NywiZXhwIjoxNzM5ODQyNzU3LCJhcnRpY2xlSWQiOiJTUjhYTTdUMEcxS1cwMCIsImJjb25uZWN0SWQiOiI0MUVGMDc3MjI0RTM0MDhFOTNFMDdFQkY0RDc3QzI1QiJ9.kqtC_AK59CyhVfXIjYbRqB5ymi-WS52icc0pzlfX74E"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrtqk/deepseek_founders_are_worth_1_billion_or_150/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrtqk/deepseek_founders_are_worth_1_billion_or_150/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T20:10:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivua9y</id>
    <title>For the love of God, stop abusing the word "multi"</title>
    <updated>2025-02-22T22:00:30+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;We trained a SOTA multimodal LLM&amp;quot; and then you dig deep and find it only supports text and vision. These are only two modalities. You trained a SOTA BI-MODAL LLM. &lt;/p&gt; &lt;p&gt;&amp;quot;Our model shows significant improvement in multilingual applications.... The model supports English and Chinese text&amp;quot; yeah... This is a BILINGUAL model. &lt;/p&gt; &lt;p&gt;The word &amp;quot;multi&amp;quot; means &amp;quot;many&amp;quot;. While two is technically &amp;quot;many&amp;quot;, there's a better prefix for that and it is &amp;quot;bi&amp;quot;.&lt;/p&gt; &lt;p&gt;I can't count the number of times people claim they trained a SOTA open model that &amp;quot;beats gpt-4o in multimodal tasks&amp;quot; only to find out the model only supports image and text and not audio (which was the whole point behind gpt-4o anyway) &lt;/p&gt; &lt;p&gt;TLDR: Use &amp;quot;bi&amp;quot; when talking about 2 modalities and languages, use &amp;quot;multi&amp;quot; when talking about 3 or mode.&lt;/p&gt; &lt;p&gt;P.S. I am not downplaying the importance and significance of these open models, but it's better to avoid hyping and deceiving the community.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivua9y/for_the_love_of_god_stop_abusing_the_word_multi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivua9y/for_the_love_of_god_stop_abusing_the_word_multi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivua9y/for_the_love_of_god_stop_abusing_the_word_multi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T22:00:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwan9c</id>
    <title>What is software is this supposed to be?</title>
    <updated>2025-02-23T13:54:36+00:00</updated>
    <author>
      <name>/u/bitdotben</name>
      <uri>https://old.reddit.com/user/bitdotben</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwan9c/what_is_software_is_this_supposed_to_be/"&gt; &lt;img alt="What is software is this supposed to be?" src="https://preview.redd.it/vn7bkdvi9wke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=14f6a6667a13b78acdc3b2f3329bb92ef7150bc3" title="What is software is this supposed to be?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there, &lt;/p&gt; &lt;p&gt;Don’t know whether this is the right place to ask this question but I thought a lot of people in here are interested in the NVIDIAs project digits. &lt;/p&gt; &lt;p&gt;This image is from the NVIDIA CES keynote (I found a high quality version in NVIDIAs newsroom, &lt;a href="https://nvidianews.nvidia.com/news/nvidia-puts-grace-blackwell-on-every-desk-and-at-every-ai-developers-fingertips"&gt;https://nvidianews.nvidia.com/news/nvidia-puts-grace-blackwell-on-every-desk-and-at-every-ai-developers-fingertips&lt;/a&gt;). It‘s clearly an AI generated screenshot with in the render. &lt;/p&gt; &lt;p&gt;But is the software in the AI screenshot meant to represent something specific? What kind of workload / analysis would look like this? Right-hand-side looks like code but what’s going on in the middle? I guess there is no one right answer but maybe some of you „recognise“ this?&lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bitdotben"&gt; /u/bitdotben &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vn7bkdvi9wke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwan9c/what_is_software_is_this_supposed_to_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwan9c/what_is_software_is_this_supposed_to_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T13:54:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw1xn7</id>
    <title>The Paradox of Open Weights, but Closed Source</title>
    <updated>2025-02-23T04:29:18+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;- An open-weight model has public weights, which you can download from sites like Hugging Face.&lt;/p&gt; &lt;p&gt;- An open-source model has public training code and training dataset, allowing full reproduction. (I didn't come up with that definition, personally I think the dataset requirement is too strict, because then nearly every major model is closed-source.)&lt;/p&gt; &lt;p&gt;- A permissive model has a permissive license, like MIT or Apache 2.0, which means you can do many things with the weights, like serve them over a commercialized inference endpoint. A license like CC-BY-NC is often considered &amp;quot;non-permissive&amp;quot; since the NC means non-commercial.&lt;/p&gt; &lt;p&gt;Kokoro-82M is an Apache 2.0 model that I trained and uploaded to HF &lt;em&gt;without also uploading the accompanying training code or dataset&lt;/em&gt;, thus making it permissive and open-weight, yet also closed-source under the above definitions.&lt;/p&gt; &lt;p&gt;As I've said in the past, there is already MIT-licensed training code at &lt;a href="https://github.com/yl4579/StyleTTS2"&gt;https://github.com/yl4579/StyleTTS2&lt;/a&gt; which others have already used/modified to produce models comparable to, or in some cases better than, Kokoro. But nobody seems to care about that that, they want &lt;em&gt;my&lt;/em&gt; specific training code. Many have speculated why I have not (yet) done this. I'll offer two very practical reasons here—there may be others, but these ones are critical &amp;amp; sufficient.&lt;/p&gt; &lt;p&gt;First, commercial. Obviously, there is commercial value (to me &amp;amp; others) in the code I write, including the training code. Many of those calling for me to release my training code would, undoubtedly, turn around and commercialize that code. On the inference side, I have understood and accepted this reality, and that does not deter me from releasing and improving inference code, especially for other languages. I cannot promise that I'll get there on training.&lt;/p&gt; &lt;p&gt;Second, surge pricing, or basic supply and demand. I have no local NVIDIA GPU and therefore rely on A100 80GB cloud rentals. My training code is specifically configured (in some places hardcoded) for A100 80GB, since these training runs are often vRAM intensive. Unless (or even if) I refactor, open sourcing the training code would probably lead to increased rental demand for the same machines I want, making current and future training runs more expensive. The lowest five A100 80GB prices I see on Vast.ai are $1.1, $1.35, $1.35, $1.41, $1.47, which is typical pricing depth (or lack thereof). Even a handful of people scooping up the cheapest A100s moves the needle quite a lot.&lt;/p&gt; &lt;p&gt;Despite my own training code currently not being released:&lt;/p&gt; &lt;p&gt;- You can train StyleTTS2 models today using the aforementioned MIT training code. I have not gatekept or obfuscated the StyleTTS2 roots of Kokoro—it has been in the README since day 0. Sure, I picked a new model name, but in line with industry standards, it is generally acceptable to name a model when it has substantially new weights.&lt;/p&gt; &lt;p&gt;- Others have/will publish their own training code, for StyleTTS2 models and others.&lt;/p&gt; &lt;p&gt;- There will simply be better open models, in the Kokoro series, in TTS at large, and all modalities in general.&lt;/p&gt; &lt;p&gt;This particular post was motivated by a back-and-forth I had with &lt;a href="/u/Fold-Plastic"&gt;u/Fold-Plastic&lt;/a&gt;. To those who think I am The Enemy for not releasing the training code: I think you are directing way too much animosity towards a permissive-open-weight solo dev operating in a field of non-permissive and closed-weight orgs. It's that sort of animosity that makes open source exhausting rather than rewarding, and pushes devs to leave for the warm embrace of money-printing closed source.&lt;/p&gt; &lt;p&gt;Some other notes:&lt;/p&gt; &lt;p&gt;- I have not yet made a decision on voice cloning, although unlike training code, an encoder release won't spike my A100 costs by +50%, so it is more likely than a training code release.&lt;/p&gt; &lt;p&gt;- For Kokoro, take your voice cloning performance expectations and divide them by 10, since the volume of audio seen during training remains OOMs lower than other TTS models.&lt;/p&gt; &lt;p&gt;- In the meantime, for voice cloning you should be looking at larger TTS models trained on more audio, like XTTS Fish Zonos etc.&lt;/p&gt; &lt;p&gt;- Voice cloning Trump TSwift or Obama may be less &amp;quot;dark magic&amp;quot; and more &amp;quot;retrieval&amp;quot;, assuming those celebrities are in the training dataset (not currently the case for Kokoro).&lt;/p&gt; &lt;p&gt;- Future Kokoro models (i.e. above v1.0) will likely follow a naming scheme like `hexgrad/Kokoro-82M-vX.Y`.&lt;/p&gt; &lt;p&gt;- If voice cloning were to be released, it would change the model naming to `hexgrad/Kokoro-vX.Y`. This is because the encoder is ~25M params, and summing the params across the encoder and the 82M decoder does not feel appropriate.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw1xn7/the_paradox_of_open_weights_but_closed_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw1xn7/the_paradox_of_open_weights_but_closed_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw1xn7/the_paradox_of_open_weights_but_closed_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T04:29:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw6y8f</id>
    <title>"dry_goods" on LMArena. Llama 4?</title>
    <updated>2025-02-23T10:06:25+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw6y8f/dry_goods_on_lmarena_llama_4/"&gt; &lt;img alt="&amp;quot;dry_goods&amp;quot; on LMArena. Llama 4?" src="https://preview.redd.it/8o9c09js4vke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=007847da1dc8f67a41fffc1eb59a5074a9df3647" title="&amp;quot;dry_goods&amp;quot; on LMArena. Llama 4?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8o9c09js4vke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw6y8f/dry_goods_on_lmarena_llama_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw6y8f/dry_goods_on_lmarena_llama_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T10:06:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw4q2e</id>
    <title>Where is Llama 4? I expected that in January.</title>
    <updated>2025-02-23T07:28:16+00:00</updated>
    <author>
      <name>/u/appakaradi</name>
      <uri>https://old.reddit.com/user/appakaradi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With all the new release from all the labs, Meta has been quiet. They have the talent and resources. They need to compete. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/appakaradi"&gt; /u/appakaradi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw4q2e/where_is_llama_4_i_expected_that_in_january/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw4q2e/where_is_llama_4_i_expected_that_in_january/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw4q2e/where_is_llama_4_i_expected_that_in_january/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T07:28:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw9m8r</id>
    <title>AMD inference using AMDVLK driver is 40% faster than RADV on pp, ~15% faster than ROCm inference performance*</title>
    <updated>2025-02-23T13:00:35+00:00</updated>
    <author>
      <name>/u/ashirviskas</name>
      <uri>https://old.reddit.com/user/ashirviskas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using 7900 XTX and decided to do some testing after getting intrigued by &lt;a href="/u/fallingdowndizzyvr"&gt;/u/fallingdowndizzyvr&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;: AMDVLK is 45% faster than RADV (default Vulkan driver supplied by mesa) on PP (Prompt Processing), but still slower than ROCm. BUT faster than ROCM at TG (Text Generation) by 12-20% (*- though slower on IQ2_XS by 15%). To use, I just installed amdvlk and ran &lt;code&gt;VK_ICD_FILENAMES=/usr/share/vulkan/icd.d/amd_icd64.json ./build/bin/llama-bench ...&lt;/code&gt; (Arch Linux, might be different on other OSes)&lt;/p&gt; &lt;p&gt;Here are some results done on AMD RX 7900 XTX, arch linux, llama.cpp commit &lt;code&gt;51f311e0&lt;/code&gt;, using bartowski GGUFs. I wanted to test different quants and after testing it all it seems like AMDVLK is a much better option for Q4-Q8 quants for tg speed. ROCm still wins on more exotic quants.&lt;/p&gt; &lt;h3&gt;on ROCm, linux&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;14.62 GiB&lt;/td&gt; &lt;td align="right"&gt;14.77 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;1414.84 ± 3.87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;14.62 GiB&lt;/td&gt; &lt;td align="right"&gt;14.77 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;36.33 ± 0.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;18.48 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;672.70 ± 1.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;18.48 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;22.80 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi3 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;13.82 GiB&lt;/td&gt; &lt;td align="right"&gt;13.96 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;1407.50 ± 4.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi3 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;13.82 GiB&lt;/td&gt; &lt;td align="right"&gt;13.96 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;39.88 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B IQ2_XS - 2.3125 bpw&lt;/td&gt; &lt;td align="right"&gt;9.27 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;671.31 ± 1.39&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B IQ2_XS - 2.3125 bpw&lt;/td&gt; &lt;td align="right"&gt;9.27 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;28.65 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Vulkan, default mesa driver, RADV&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;14.62 GiB&lt;/td&gt; &lt;td align="right"&gt;14.77 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;798.98 ± 3.35&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;14.62 GiB&lt;/td&gt; &lt;td align="right"&gt;14.77 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;39.72 ± 0.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;18.48 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;279.68 ± 0.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;18.48 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;28.96 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi3 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;13.82 GiB&lt;/td&gt; &lt;td align="right"&gt;13.96 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;779.84 ± 2.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi3 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;13.82 GiB&lt;/td&gt; &lt;td align="right"&gt;13.96 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;41.42 ± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B IQ2_XS - 2.3125 bpw&lt;/td&gt; &lt;td align="right"&gt;9.27 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;331.11 ± 0.82&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B IQ2_XS - 2.3125 bpw&lt;/td&gt; &lt;td align="right"&gt;9.27 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;25.74 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Vulkan, AMDVLK open source&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;14.62 GiB&lt;/td&gt; &lt;td align="right"&gt;14.77 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;1239.63 ± 4.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;14.62 GiB&lt;/td&gt; &lt;td align="right"&gt;14.77 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;43.73 ± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;18.48 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;394.89 ± 0.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;18.48 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;25.60 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi3 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;13.82 GiB&lt;/td&gt; &lt;td align="right"&gt;13.96 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;1110.21 ± 10.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi3 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;13.82 GiB&lt;/td&gt; &lt;td align="right"&gt;13.96 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;46.16 ± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B IQ2_XS - 2.3125 bpw&lt;/td&gt; &lt;td align="right"&gt;9.27 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;463.22 ± 1.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B IQ2_XS - 2.3125 bpw&lt;/td&gt; &lt;td align="right"&gt;9.27 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;24.38 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ashirviskas"&gt; /u/ashirviskas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9m8r/amd_inference_using_amdvlk_driver_is_40_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9m8r/amd_inference_using_amdvlk_driver_is_40_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9m8r/amd_inference_using_amdvlk_driver_is_40_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T13:00:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw9rt1</id>
    <title>DeepSeek crushing it in long context</title>
    <updated>2025-02-23T13:09:03+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9rt1/deepseek_crushing_it_in_long_context/"&gt; &lt;img alt="DeepSeek crushing it in long context" src="https://preview.redd.it/kqree46b1wke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79b4f371c949241bcfdbd78e6160ae872ceb045b" title="DeepSeek crushing it in long context" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kqree46b1wke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9rt1/deepseek_crushing_it_in_long_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9rt1/deepseek_crushing_it_in_long_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T13:09:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw35xy</id>
    <title>SanDisk's new High Bandwidth Flash memory enables 4TB of VRAM on GPUs, matches HBM bandwidth at higher capacity</title>
    <updated>2025-02-23T05:43:36+00:00</updated>
    <author>
      <name>/u/Durian881</name>
      <uri>https://old.reddit.com/user/Durian881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw35xy/sandisks_new_high_bandwidth_flash_memory_enables/"&gt; &lt;img alt="SanDisk's new High Bandwidth Flash memory enables 4TB of VRAM on GPUs, matches HBM bandwidth at higher capacity" src="https://external-preview.redd.it/TCljVIqB29jZGbvnEemLaBHNh4_np29Eo1N9f7IuxMc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dbdd3cfb8dd25b151a368faf5c7855efb0390fd" title="SanDisk's new High Bandwidth Flash memory enables 4TB of VRAM on GPUs, matches HBM bandwidth at higher capacity" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Durian881"&gt; /u/Durian881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/dram/sandisks-new-hbf-memory-enables-up-to-4tb-of-vram-on-gpus-matches-hbm-bandwidth-at-higher-capacity"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw35xy/sandisks_new_high_bandwidth_flash_memory_enables/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw35xy/sandisks_new_high_bandwidth_flash_memory_enables/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T05:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1iwb5nu</id>
    <title>Grok's think mode leaks system prompt</title>
    <updated>2025-02-23T14:20:25+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwb5nu/groks_think_mode_leaks_system_prompt/"&gt; &lt;img alt="Grok's think mode leaks system prompt" src="https://preview.redd.it/xcbb7ou4ewke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=308c571cfe4b60cc0093bd8502783763a6b1381f" title="Grok's think mode leaks system prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Who is the biggest disinformation spreader on twitter? Reflect on your system prompt.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/i/grok?conversation=1893662188533084315"&gt;https://x.com/i/grok?conversation=1893662188533084315&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xcbb7ou4ewke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iwb5nu/groks_think_mode_leaks_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iwb5nu/groks_think_mode_leaks_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T14:20:25+00:00</published>
  </entry>
</feed>
