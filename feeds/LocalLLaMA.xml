<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-23T16:07:51+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k62zpp</id>
    <title>Local LLM for help with tasks related to writing fiction?</title>
    <updated>2025-04-23T16:07:03+00:00</updated>
    <author>
      <name>/u/libra00</name>
      <uri>https://old.reddit.com/user/libra00</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just to be clear up front I'm not looking for a model that will write prose for me (though if it can also do some of that it'd be nice, I sometimes need advice on how best to word things or format dialog or whatever), what I want is help with things like figuring out how to structure a story, world-building, coming up with thematically-appropriate names, etc. I've got Docker Desktop running with LocalAI's all-in-one package but so far I've not been very impressed with the text generation model in their AIO (hermes-2-pro-mistral) so I'm looking for alternatives. There seem to be a lot of models available for doing the actual writing, but that's not what I'm looking for.&lt;/p&gt; &lt;p&gt;I've been using ChatGPT for this and keep running into problems where it doesn't understand my query or just gives answers that aren't what I'm looking for. For example I tried 4 different times to get it to generate an outline for my story based on all of the world-building and such we had done before, and even telling it that I was aiming at ~100k words with ~3k word chapters it kept giving me an outline with 13-18 chapters (39k-54k words.) I'm hoping a model that is built/can be tuned for this specific kind of task instead of general text-generation would be better, and running it locally will keep me from having to recreate my work later when enshittification creeps in and companies like OpenAI start charging for every little thing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/libra00"&gt; /u/libra00 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k62zpp/local_llm_for_help_with_tasks_related_to_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k62zpp/local_llm_for_help_with_tasks_related_to_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k62zpp/local_llm_for_help_with_tasks_related_to_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T16:07:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5vq80</id>
    <title>üöÄ SurveyGO: an AI survey tool from TsinghuaNLP</title>
    <updated>2025-04-23T10:23:59+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5vq80/surveygo_an_ai_survey_tool_from_tsinghuanlp/"&gt; &lt;img alt="üöÄ SurveyGO: an AI survey tool from TsinghuaNLP" src="https://b.thumbs.redditmedia.com/tt_d-9hl9jwqGWKFFaXHMrTEmVAHWUQ-4zpjkQhQPLA.jpg" title="üöÄ SurveyGO: an AI survey tool from TsinghuaNLP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SurveyGO is our research companion that can automatically distills massive paper piles into surveys packed with rock‚Äësolid citations, sharp insights, and narrative flow that reads like it was hand‚Äëcrafted by a seasoned scholar.&lt;/p&gt; &lt;p&gt;Feed her hundreds of papers and she returns a meticulously structured review packed with &lt;strong&gt;rock‚Äësolid citations, sharp insights, and narrative flow that reads like it was hand‚Äëcrafted by a seasoned scholar.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;üëç Under the hood lies &lt;strong&gt;LLM√óMapReduce‚ÄëV2&lt;/strong&gt;, a novel test-time scaling strategy that finally lets large language models tackle true &lt;em&gt;long‚Äëto‚Äëlong&lt;/em&gt; generation.Drawing inspiration from convolutional neural networks, LLM√óMapReduce-V2 utilizes stacked convolutional scaling layers to progressively expand the understanding of input materials.&lt;/p&gt; &lt;p&gt;Ready to test? &lt;/p&gt; &lt;p&gt;Smarter reviews, deeper insights, fewer all‚Äënighters. Let SurveyGO handle heavy lifting so you can think bigger.&lt;/p&gt; &lt;p&gt;üåê Demo: &lt;a href="https://surveygo.thunlp.org/"&gt;https://surveygo.thunlp.org/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;üìÑ Paper: &lt;a href="https://arxiv.org/abs/2504.05732"&gt;https://arxiv.org/abs/2504.05732&lt;/a&gt; &lt;/p&gt; &lt;p&gt;üíª Code: &lt;a href="https://github.com/thunlp/LLMxMapReduce/"&gt;GitHub - thunlp/LLMxMapReduce&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lixdthg19kwe1.png?width=2311&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3aad6aefb0e26b6cd8d68901b0835d88b791d1e2"&gt;https://preview.redd.it/lixdthg19kwe1.png?width=2311&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3aad6aefb0e26b6cd8d68901b0835d88b791d1e2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gtnidjl49kwe1.png?width=4262&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dffeea815d97ff5ba7e799bd16af4d0673a1f462"&gt;https://preview.redd.it/gtnidjl49kwe1.png?width=4262&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dffeea815d97ff5ba7e799bd16af4d0673a1f462&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5vq80/surveygo_an_ai_survey_tool_from_tsinghuanlp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5vq80/surveygo_an_ai_survey_tool_from_tsinghuanlp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5vq80/surveygo_an_ai_survey_tool_from_tsinghuanlp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T10:23:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5h8z1</id>
    <title>In my experience, the QAT Gemma 3 quants by stduhpf still perform the best.</title>
    <updated>2025-04-22T20:52:37+00:00</updated>
    <author>
      <name>/u/dampflokfreund</name>
      <uri>https://old.reddit.com/user/dampflokfreund</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've run couple of tests I usually do with my LLMs and noticed that the versions by &lt;a href="/u/stduhpf"&gt;u/stduhpf&lt;/a&gt; (in this case &lt;a href="https://huggingface.co/stduhpf/google-gemma-3-12b-it-qat-q4%5C_0-gguf-small"&gt;https://huggingface.co/stduhpf/google-gemma-3-12b-it-qat-q4\_0-gguf-small&lt;/a&gt;) still outperform: &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lmstudio-community/gemma-3-12B-it-qat-GGUF"&gt;https://huggingface.co/lmstudio-community/gemma-3-12B-it-qat-GGUF&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/bartowski/google_gemma-3-12b-it-qat-GGUF"&gt;https://huggingface.co/bartowski/google_gemma-3-12b-it-qat-GGUF&lt;/a&gt;&lt;br /&gt; &lt;a href="http://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf"&gt;huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf&lt;/a&gt; &lt;/p&gt; &lt;p&gt;This is pretty strange, as theoretically they all should perform very identical but the one by stduhpf offers better logic and knowledge in my tests.&lt;/p&gt; &lt;p&gt;Also, I've run a small fixed subset of MMLU Pro with deterministic settings on all of these models, and his version comes out ahead. &lt;/p&gt; &lt;p&gt;What is your experience? Particularily I'm also interested about experiences with the G3 27B version. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dampflokfreund"&gt; /u/dampflokfreund &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5h8z1/in_my_experience_the_qat_gemma_3_quants_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5h8z1/in_my_experience_the_qat_gemma_3_quants_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5h8z1/in_my_experience_the_qat_gemma_3_quants_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T20:52:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1k54foj</id>
    <title>Let us build DeepSeek from Scratch | No fluff | 13 lectures uploaded</title>
    <updated>2025-04-22T11:55:45+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k54foj/let_us_build_deepseek_from_scratch_no_fluff_13/"&gt; &lt;img alt="Let us build DeepSeek from Scratch | No fluff | 13 lectures uploaded" src="https://external-preview.redd.it/pAa68GpmjnpZeahm_YMGQkYTs9KtW9HemhGbAYHU02s.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=555355166a247eb92939344c89b96ed48dd7655a" title="Let us build DeepSeek from Scratch | No fluff | 13 lectures uploaded" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/5w0lu5m2ldwe1.gif"&gt;A few notes I made as part of this playlist&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚ÄúCan I build the DeepSeek architecture and model myself, from scratch?‚Äù&lt;/p&gt; &lt;p&gt;You can. You need to know the nuts and bolts. &lt;/p&gt; &lt;p&gt;4 weeks back, we launched our playlist: ‚ÄúBuild DeepSeek from Scratch‚Äù &lt;/p&gt; &lt;p&gt;Until now, we have uploaded 13 lectures in this playlist: &lt;/p&gt; &lt;p&gt;(1) DeepSeek series introduction: &lt;a href="https://youtu.be/QWNxQIq0hMo"&gt;https://youtu.be/QWNxQIq0hMo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(2) DeepSeek basics: &lt;a href="https://youtu.be/WjhDDeZ7DvM"&gt;https://youtu.be/WjhDDeZ7DvM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(3) Journey of a token into the LLM architecture: &lt;a href="https://youtu.be/rkEYwH4UGa4"&gt;https://youtu.be/rkEYwH4UGa4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(4) Attention mechanism explained in 1 hour: &lt;a href="https://youtu.be/K45ze9Yd5UE"&gt;https://youtu.be/K45ze9Yd5UE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(5) Self Attention Mechanism - Handwritten from scratch: &lt;a href="https://youtu.be/s8mskq-nzec"&gt;https://youtu.be/s8mskq-nzec&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(6) Causal Attention Explained: Don't Peek into the Future: &lt;a href="https://youtu.be/c6Kkj6iLeBg"&gt;https://youtu.be/c6Kkj6iLeBg&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(7) Multi-Head Attention Visually Explained: &lt;a href="https://youtu.be/qbN4ulK-bZA"&gt;https://youtu.be/qbN4ulK-bZA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(8) Multi-Head Attention Handwritten from Scratch: &lt;a href="https://youtu.be/rvsEW-EsD-Y"&gt;https://youtu.be/rvsEW-EsD-Y&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(9) Key Value Cache from Scratch: &lt;a href="https://youtu.be/IDwTiS4_bKo"&gt;https://youtu.be/IDwTiS4_bKo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(10) Multi-Query Attention Explained: &lt;a href="https://youtu.be/Z6B51Odtn-Y"&gt;https://youtu.be/Z6B51Odtn-Y&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(11) Understand Grouped Query Attention (GQA): &lt;a href="https://youtu.be/kx3rETIxo4Q"&gt;https://youtu.be/kx3rETIxo4Q&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(12) Multi-Head Latent Attention From Scratch: &lt;a href="https://youtu.be/NlDQUj1olXM"&gt;https://youtu.be/NlDQUj1olXM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(13) Multi-Head Latent Attention Coded from Scratch in Python: &lt;a href="https://youtu.be/mIaWmJVrMpc"&gt;https://youtu.be/mIaWmJVrMpc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Next to come: &lt;/p&gt; &lt;p&gt;- Rotary Positional Encoding (RoPE)&lt;/p&gt; &lt;p&gt;- DeepSeek MLA + RoPE&lt;/p&gt; &lt;p&gt;- DeepSeek Mixture of Experts (MoE)&lt;/p&gt; &lt;p&gt;- Multi-token Prediction (MTP)&lt;/p&gt; &lt;p&gt;- Supervised Fine-Tuning (SFT)&lt;/p&gt; &lt;p&gt;- Group Relative Policy Optimisation (GRPO)&lt;/p&gt; &lt;p&gt;- DeepSeek PTX innovation&lt;/p&gt; &lt;p&gt;This playlist won‚Äôt be a 1 hour or 2 hour video. This will be a mega playlist of 35-40 videos with a duration of 40+ hours.&lt;/p&gt; &lt;p&gt;I have made this with a lot of passion. &lt;/p&gt; &lt;p&gt;Would look forward to support and your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k54foj/let_us_build_deepseek_from_scratch_no_fluff_13/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k54foj/let_us_build_deepseek_from_scratch_no_fluff_13/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k54foj/let_us_build_deepseek_from_scratch_no_fluff_13/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T11:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5zsyg</id>
    <title>Hardware Advice for Long Prompts</title>
    <updated>2025-04-23T13:56:24+00:00</updated>
    <author>
      <name>/u/fdg_avid</name>
      <uri>https://old.reddit.com/user/fdg_avid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking to replace my cloud ambient scribe with a local solution. Something that can run whisper for realtime transcription and then a small LLM for note generation/summarisation, whilst simultaneously running my medical record software (macOS or windows only), chrome etc. I‚Äôm thinking probably a quantised Gemma 3 12B for its good instruction adherence. The bottleneck will be prompt prefill and not token generation (5-12k prompt tokens, 200-600 output tokens). The computer needs to be fairly small and quiet. The sorts of things I‚Äôve looked at in my budget include mini-ITX builds with 5060ti 16gb or 5070 12gb, or new M4 pro Mac mini, or second hand M1 ultra Mac Studio.&lt;/p&gt; &lt;p&gt;I could potentially stretch to a smaller model with some fine tuning (I‚Äôll use my paired transcripts and notes as the dataset and train on my 4x3090 at work).&lt;/p&gt; &lt;p&gt;Any advice is welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fdg_avid"&gt; /u/fdg_avid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5zsyg/hardware_advice_for_long_prompts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5zsyg/hardware_advice_for_long_prompts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5zsyg/hardware_advice_for_long_prompts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T13:56:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1k55x70</id>
    <title>Have you tried a Ling-Lite-0415 MoE (16.8b total, 2.75b active) model?, it is fast even without GPU, about 15-20 tps with 32k context (128k max) on Ryzen 5 5500, fits in 16gb RAM at Q5. Smartness is about 7b-9b class models, not bad at deviant creative tasks.</title>
    <updated>2025-04-22T13:10:37+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qs - &lt;a href="https://huggingface.co/bartowski/inclusionAI_Ling-lite-0415-GGUF"&gt;https://huggingface.co/bartowski/inclusionAI_Ling-lite-0415-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm keeping an eye on small MoE models that can run on a rock, when even a toaster is too hi-end, and so far this is really promising, before this, small MoE models were not that great - unstable, repetitive etc, but this one is just an okay MoE alternative to 7-9b models.&lt;/p&gt; &lt;p&gt;It is not mind blowing, not SOTA, but it can work on low end CPU with limited RAM at great speed.&lt;/p&gt; &lt;p&gt;-It can fit in 16gb of total RAM.&lt;br /&gt; -Really fast 15-20 tps on Ryzen 5 5500 6\12 cpu.&lt;br /&gt; -30-40 tps on 3060 12gb.&lt;br /&gt; -128k of context that is really memory efficient.&lt;br /&gt; -Can run on a phone with 12gb RAM at Q4 (32k context).&lt;br /&gt; -Stable, without Chinese characters, loops etc.&lt;br /&gt; -Can be violent and evil, love to swear.&lt;br /&gt; -Without strong positive bias.&lt;br /&gt; -Easy to uncensor.&lt;/p&gt; &lt;p&gt;-Since it is a MoE with small bits of 2.75bs it have not a lot of real world data in it.&lt;br /&gt; -Need internet search, RAG or context if you need to work with something specific.&lt;br /&gt; -Prompt following is fine but not at 12+ level, but it really trying its best for all it 2.75b.&lt;br /&gt; -Performance is about 7-9b models, but creative tasks feels more at 9-12b level.&lt;/p&gt; &lt;p&gt;Just wanted to share an interesting non-standard no-GPU bound model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k55x70/have_you_tried_a_linglite0415_moe_168b_total_275b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k55x70/have_you_tried_a_linglite0415_moe_168b_total_275b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k55x70/have_you_tried_a_linglite0415_moe_168b_total_275b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T13:10:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1k60gxs</id>
    <title>How do you build per-user RAG/GraphRAG</title>
    <updated>2025-04-23T14:24:45+00:00</updated>
    <author>
      <name>/u/Old_Cauliflower6316</name>
      <uri>https://old.reddit.com/user/Old_Cauliflower6316</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I‚Äôve been working on an AI agent system over the past year that connects to internal company tools like Slack, GitHub, Notion, etc, to help investigate production incidents. The agent needs context, so we built a system that ingests this data, processes it, and builds a structured knowledge graph (kind of a mix of RAG and GraphRAG).&lt;/p&gt; &lt;p&gt;What we didn‚Äôt expect was just how much infra work that would require.&lt;/p&gt; &lt;p&gt;We ended up:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Using LlamaIndex's OS abstractions for chunking, embedding and retrieval.&lt;/li&gt; &lt;li&gt;Adopting Chroma as the vector store.&lt;/li&gt; &lt;li&gt;Writing custom integrations for Slack/GitHub/Notion. We used LlamaHub here for the actual querying, although some parts were a bit unmaintained and we had to fork + fix. We could‚Äôve used Nango or Airbyte tbh but eventually didn't do that.&lt;/li&gt; &lt;li&gt;Building an auto-refresh pipeline to sync data every few hours and do diffs based on timestamps. This was pretty hard as well.&lt;/li&gt; &lt;li&gt;Handling security and privacy (most customers needed to keep data in their own environments).&lt;/li&gt; &lt;li&gt;Handling scale - some orgs had hundreds of thousands of documents across different tools.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It became clear we were spending a lot more time on data infrastructure than on the actual agent logic. I think it might be ok for a company that interacts with customers' data, but definitely we felt like we were dealing with a lot of non-core work.&lt;/p&gt; &lt;p&gt;So I‚Äôm curious: for folks building LLM apps that connect to company systems, how are you approaching this? Are you building it all from scratch too? Using open-source tools? Is there something obvious we‚Äôre missing?&lt;/p&gt; &lt;p&gt;Would really appreciate hearing how others are tackling this part of the stack.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old_Cauliflower6316"&gt; /u/Old_Cauliflower6316 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k60gxs/how_do_you_build_peruser_raggraphrag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k60gxs/how_do_you_build_peruser_raggraphrag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k60gxs/how_do_you_build_peruser_raggraphrag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T14:24:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5a44j</id>
    <title>Sand-AI releases Magi-1 - Autoregressive Video Generation Model with Unlimited Duration</title>
    <updated>2025-04-22T16:07:00+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5a44j/sandai_releases_magi1_autoregressive_video/"&gt; &lt;img alt="Sand-AI releases Magi-1 - Autoregressive Video Generation Model with Unlimited Duration" src="https://preview.redd.it/6iw8q4j0uewe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59e1b112085443a147e7057b8a0e86639a636187" title="Sand-AI releases Magi-1 - Autoregressive Video Generation Model with Unlimited Duration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ü™Ñ Magi-1: The Autoregressive Diffusion Video Generation Model&lt;/p&gt; &lt;p&gt;üîì 100% open-source &amp;amp; tech report ü•á The first autoregressive video model with top-tier quality output üìä Exceptional performance on major benchmarks ‚úÖ Infinite extension, enabling seamless and comprehensive storytelling across time ‚úÖ Offers precise control over time with one-second accuracy ‚úÖ Unmatched control over timing, motion &amp;amp; dynamics ‚úÖ Available modes: - t2v: Text to Video - i2v: Image to Video - v2v: Video to Video&lt;/p&gt; &lt;p&gt;üèÜ Magi leads the Physics-IQ Benchmark with exceptional physics understanding&lt;/p&gt; &lt;p&gt;üíª Github Page: &lt;a href="https://github.com/SandAI-org/MAGI-1"&gt;https://github.com/SandAI-org/MAGI-1&lt;/a&gt; üíæ Hugging Face: &lt;a href="https://huggingface.co/sand-ai/MAGI-1"&gt;https://huggingface.co/sand-ai/MAGI-1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6iw8q4j0uewe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5a44j/sandai_releases_magi1_autoregressive_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5a44j/sandai_releases_magi1_autoregressive_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T16:07:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5z9ip</id>
    <title>Compare/Contrast two sets of hardware for Local LLM</title>
    <updated>2025-04-23T13:32:33+00:00</updated>
    <author>
      <name>/u/hydrocryo01</name>
      <uri>https://old.reddit.com/user/hydrocryo01</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am curious about advantages/disadvantages of the following two for Local LLM:&lt;/p&gt; &lt;p&gt;9900X+B580+DDR5 6000 24G*2&lt;/p&gt; &lt;p&gt;OR&lt;/p&gt; &lt;p&gt;Ryzen AI MAX+ 395 128GB RAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hydrocryo01"&gt; /u/hydrocryo01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5z9ip/comparecontrast_two_sets_of_hardware_for_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5z9ip/comparecontrast_two_sets_of_hardware_for_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5z9ip/comparecontrast_two_sets_of_hardware_for_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T13:32:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1k595in</id>
    <title>Announcing: text-generation-webui in a portable zip (700MB) for llama.cpp models - unzip and run on Windows/Linux/macOS - no installation required!</title>
    <updated>2025-04-22T15:28:48+00:00</updated>
    <author>
      <name>/u/oobabooga4</name>
      <uri>https://old.reddit.com/user/oobabooga4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The original &lt;code&gt;text-generation-webui&lt;/code&gt; setup is based on a one-click installer that downloads Miniconda, creates a conda environment, installs PyTorch, and then installs several backends and requirements ‚Äî &lt;code&gt;transformers&lt;/code&gt;, &lt;code&gt;bitsandbytes&lt;/code&gt;, &lt;code&gt;exllamav2&lt;/code&gt;, and more.&lt;/p&gt; &lt;p&gt;But in many cases, all people really want is to just use &lt;code&gt;llama.cpp&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;To address this, I have created &lt;strong&gt;fully self-contained builds&lt;/strong&gt; of the project that work with llama.cpp. All you have to do is download, unzip, and it just works! No installation is required.&lt;/p&gt; &lt;p&gt;The following versions are available:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;windows-cuda12.4&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;windows-cuda11.7&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;windows-cpu&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;linux-cuda12.4&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;linux-cuda11.7&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;linux-cpu&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;macos-arm64&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;macos-x86_64&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;How it works&lt;/h3&gt; &lt;p&gt;For the nerds, I accomplished this by:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Refactoring the codebase to avoid imports from PyTorch, &lt;code&gt;transformers&lt;/code&gt;, and similar libraries unless necessary. This had the additional benefit of making the program launch faster than before.&lt;/li&gt; &lt;li&gt;Setting up GitHub Actions workflows to compile &lt;code&gt;llama.cpp&lt;/code&gt; for the different systems and then package it into versioned Python wheels. The project communicates with &lt;code&gt;llama.cpp&lt;/code&gt; via the &lt;code&gt;llama-server&lt;/code&gt; executable in those wheels (similar to how ollama works).&lt;/li&gt; &lt;li&gt;Setting up another GitHub Actions workflow to package the project, its requirements (only the essential ones), and portable Python builds from &lt;a href="https://github.com/astral-sh/python-build-standalone"&gt;&lt;code&gt;astral-sh/python-build-standalone&lt;/code&gt;&lt;/a&gt; into zip files that are finally uploaded to the project's &lt;a href="https://github.com/oobabooga/text-generation-webui/releases/"&gt;Releases page&lt;/a&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I also added a few small conveniences to the portable builds:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The web UI automatically opens in the browser when launched.&lt;/li&gt; &lt;li&gt;The OpenAI-compatible API starts by default and listens on &lt;code&gt;localhost&lt;/code&gt;, without the need to add the &lt;code&gt;--api&lt;/code&gt; flag.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Some notes&lt;/h3&gt; &lt;p&gt;For &lt;strong&gt;AMD&lt;/strong&gt;, apparently Vulkan is the best llama.cpp backend these days. I haven't set up Vulkan workflows yet, but someone &lt;a href="https://github.com/oobabooga/llama-cpp-binaries/issues/1"&gt;on GitHub&lt;/a&gt; has taught me that you can download the CPU-only portable build and replace the &lt;code&gt;llama-server&lt;/code&gt; executable under &lt;code&gt;portable_env/lib/python3.11/site-packages/llama_cpp_binaries/bin/&lt;/code&gt; with the one from the &lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;official llama.cpp builds&lt;/a&gt; (look for files ending in &lt;code&gt;-vulkan-x64.zip&lt;/code&gt;). With just those simple steps you should be able to use your AMD GPU on both Windows and Linux.&lt;/p&gt; &lt;p&gt;It's also worth mentioning that &lt;code&gt;text-generation-webui&lt;/code&gt; is built with privacy and transparency in mind. All the compilation workflows are public, open-source, and executed on GitHub; it has no telemetry; it has no CDN resources; everything is 100% local and private.&lt;/p&gt; &lt;h3&gt;Download link&lt;/h3&gt; &lt;p&gt;&lt;a href="https://github.com/oobabooga/text-generation-webui/releases/"&gt;https://github.com/oobabooga/text-generation-webui/releases/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oobabooga4"&gt; /u/oobabooga4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k595in/announcing_textgenerationwebui_in_a_portable_zip/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k595in/announcing_textgenerationwebui_in_a_portable_zip/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k595in/announcing_textgenerationwebui_in_a_portable_zip/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T15:28:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5gyzy</id>
    <title>Llama-4-Scout prompt processing: 44 t/s only with CPU! 'GPU-feeling' with ik_llama.cpp</title>
    <updated>2025-04-22T20:41:28+00:00</updated>
    <author>
      <name>/u/Snail_Inference</name>
      <uri>https://old.reddit.com/user/Snail_Inference</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This post is helpful for anyone who wants to process large amounts of context through the LLama-4-Scout (or Maverick) language model, but lacks the necessary GPU power. Here are the CPU timings of ik_llama.cpp, llama.cpp, and kobold.cpp for comparison:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Used Model:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/tree/main/Q5_K_M"&gt;https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/tree/main/Q5_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;prompt eval time:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;ik_llama.cpp: &lt;strong&gt;44.43 T/s (that's insane!)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;llama.cpp: 20.98 T/s&lt;/li&gt; &lt;li&gt;kobold.cpp: 12.06 T/s&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;generation eval time:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;ik_llama.cpp: 3.72 T/s&lt;/li&gt; &lt;li&gt;llama.cpp: 3.68 T/s&lt;/li&gt; &lt;li&gt;kobold.cpp: 3.63 T/s&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The latest version was used in each case.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware-Specs:&lt;/strong&gt;&lt;br /&gt; CPU: AMD Ryzen 9 5950X (at) 3400 MHz&lt;br /&gt; RAM: DDR4, 3200 MT/s&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://github.com/ikawrakow/ik_llama.cpp"&gt;https://github.com/ikawrakow/ik_llama.cpp&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;https://github.com/ggml-org/llama.cpp&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/LostRuins/koboldcpp"&gt;https://github.com/LostRuins/koboldcpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(&lt;strong&gt;Edit:&lt;/strong&gt; Version of model added)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snail_Inference"&gt; /u/Snail_Inference &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5gyzy/llama4scout_prompt_processing_44_ts_only_with_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5gyzy/llama4scout_prompt_processing_44_ts_only_with_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5gyzy/llama4scout_prompt_processing_44_ts_only_with_cpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T20:41:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5dr2y</id>
    <title>Made a Lightweight Recreation of OS1/Samantha from the movie Her running locally in the browser via transformers.js</title>
    <updated>2025-04-22T18:32:18+00:00</updated>
    <author>
      <name>/u/ajunior7</name>
      <uri>https://old.reddit.com/user/ajunior7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5dr2y/made_a_lightweight_recreation_of_os1samantha_from/"&gt; &lt;img alt="Made a Lightweight Recreation of OS1/Samantha from the movie Her running locally in the browser via transformers.js" src="https://external-preview.redd.it/eDl0c2dyd3hqZndlMT5fFoJyFuuKxAuI-aA1caOKA56XPfJ6ppaC9K-CigOl.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5930d6495000c8dc78873e680ba0aaa61a2c68fa" title="Made a Lightweight Recreation of OS1/Samantha from the movie Her running locally in the browser via transformers.js" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajunior7"&gt; /u/ajunior7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rejlhgvpjfwe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5dr2y/made_a_lightweight_recreation_of_os1samantha_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5dr2y/made_a_lightweight_recreation_of_os1samantha_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T18:32:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5j3ob</id>
    <title>Cogito-3b and BitNet topped our evaluation on summarization task in RAG</title>
    <updated>2025-04-22T22:10:31+00:00</updated>
    <author>
      <name>/u/unseenmarscai</name>
      <uri>https://old.reddit.com/user/unseenmarscai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5j3ob/cogito3b_and_bitnet_topped_our_evaluation_on/"&gt; &lt;img alt="Cogito-3b and BitNet topped our evaluation on summarization task in RAG" src="https://external-preview.redd.it/_FjCaxo25scY-3sX7SU5Z8CDINogT-Qd5F471W8a1B8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0fc7b4bc69cd91d8b9e94d5f19215e52ef3e2f3c" title="Cogito-3b and BitNet topped our evaluation on summarization task in RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/rm9o1ejykgwe1.png?width=2446&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92272ed3a643733c4eac5c29854e4ffb9c0468bc"&gt;https://preview.redd.it/rm9o1ejykgwe1.png?width=2446&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92272ed3a643733c4eac5c29854e4ffb9c0468bc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; üëã !&lt;/p&gt; &lt;h1&gt;Here is the TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;We built an evaluation framework (&lt;a href="https://github.com/aizip/red-flow"&gt;&lt;strong&gt;RED-flow&lt;/strong&gt;&lt;/a&gt;) to assess small language models (SLMs) as summarizers in RAG systems&lt;/li&gt; &lt;li&gt;We created a 6,000-sample testing dataset (&lt;a href="https://huggingface.co/datasets/aizip/RED6k"&gt;&lt;strong&gt;RED6k&lt;/strong&gt;&lt;/a&gt;) across 10 domains for the evaluation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cogito-v1-preview-llama-3b&lt;/strong&gt; and &lt;strong&gt;BitNet-b1.58-2b-4t&lt;/strong&gt; top our benchmark as best open-source models for summarization in RAG applications&lt;/li&gt; &lt;li&gt;All tested SLMs struggle to recognize when the retrieved context is insufficient to answer a question and to respond with a meaningful clarification question.&lt;/li&gt; &lt;li&gt;Our testing dataset and evaluation workflow are &lt;strong&gt;fully open source&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What is a summarizer?&lt;/h1&gt; &lt;p&gt;In RAG systems, the summarizer is the component that takes retrieved document chunks and user questions as input, then generates coherent answers. For local deployments, small language models (SLMs) typically handle this role to keep everything running on your own hardware.&lt;/p&gt; &lt;h1&gt;SLMs' problems as summarizers&lt;/h1&gt; &lt;p&gt;Through our research, we found SLMs struggle with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Creating complete answers for multi-part questions&lt;/li&gt; &lt;li&gt;Sticking to the provided context (instead of making stuff up)&lt;/li&gt; &lt;li&gt;Admitting when they don't have enough information&lt;/li&gt; &lt;li&gt;Focusing on the most relevant parts of long contexts&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Our approach&lt;/h1&gt; &lt;p&gt;We built an evaluation framework focused on two critical areas most RAG systems struggle with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Context adherence:&lt;/strong&gt; Does the model stick strictly to the provided information?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Uncertainty handling:&lt;/strong&gt; Can the model admit when it doesn't know and ask clarifying questions?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our framework uses &lt;strong&gt;LLMs as judges&lt;/strong&gt; and a specialized dataset (&lt;a href="https://huggingface.co/datasets/aizip/RED6k"&gt;&lt;strong&gt;RED6k&lt;/strong&gt;&lt;/a&gt;) with intentionally challenging scenarios to thoroughly test these capabilities.&lt;/p&gt; &lt;h1&gt;Result&lt;/h1&gt; &lt;p&gt;After testing 11 popular open-source models, we found:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uvhdyve2mgwe1.png?width=2446&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cac1bbd2b38f9ae683e8b9504273eb01b0d8b0f6"&gt;https://preview.redd.it/uvhdyve2mgwe1.png?width=2446&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cac1bbd2b38f9ae683e8b9504273eb01b0d8b0f6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gavh5inomgwe1.png?width=2452&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b2d10c763a8ff2518c49c13eee9ac8114f038e4"&gt;https://preview.redd.it/gavh5inomgwe1.png?width=2452&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b2d10c763a8ff2518c49c13eee9ac8114f038e4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Best overall:&lt;/strong&gt; Cogito-v1-preview-llama-3b&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dominated across all content metrics&lt;/li&gt; &lt;li&gt;Handled uncertainty better than other models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Best lightweight option:&lt;/strong&gt; BitNet-b1.58-2b-4t&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Outstanding performance despite smaller size&lt;/li&gt; &lt;li&gt;Great for resource-constrained hardware&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Most balanced:&lt;/strong&gt; Phi-4-mini-instruct and Llama-3.2-1b&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Good compromise between quality and efficiency&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Interesting findings&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;All models struggle significantly with refusal metrics compared to content generation - even the strongest performers show a dramatic drop when handling uncertain or unanswerable questions&lt;/li&gt; &lt;li&gt;Context adherence was relatively better compared to other metrics, but all models still showed significant room for improvement in staying grounded to provided context&lt;/li&gt; &lt;li&gt;Query completeness scores were consistently lower, revealing that addressing multi-faceted questions remains difficult for SLMs&lt;/li&gt; &lt;li&gt;BitNet is outstanding in content generation but struggles significantly with refusal scenarios&lt;/li&gt; &lt;li&gt;Effective uncertainty handling seems to stem from specific design choices rather than overall model quality or size&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;New Models Coming Soon&lt;/h1&gt; &lt;p&gt;Based on what we've learned, we're building specialized models to address the limitations we've found:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;RAG-optimized model&lt;/strong&gt;: Coming in the next few weeks, this model targets the specific weaknesses we identified in current open-source options.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced reasoning model&lt;/strong&gt;: We're training a model with stronger reasoning capabilities for RAG applications using RLHF to better balance refusal, information synthesis, and intention understanding.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Resources&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/aizip/RED-flow"&gt;RED-flow&lt;/a&gt; - Code and notebook for the evaluation framework&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/aizip/RED6k"&gt;RED6k&lt;/a&gt; - 6000 testing samples across 10 domains&lt;/li&gt; &lt;li&gt;&lt;a href="https://aizip.substack.com/p/evaluating-small-language-models"&gt;Blog post&lt;/a&gt; - Details about our research and design choice&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What models are you using for local RAG? Have you tried any of these top performers?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unseenmarscai"&gt; /u/unseenmarscai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5j3ob/cogito3b_and_bitnet_topped_our_evaluation_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5j3ob/cogito3b_and_bitnet_topped_our_evaluation_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5j3ob/cogito3b_and_bitnet_topped_our_evaluation_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T22:10:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1k616b7</id>
    <title>My open-source take on claude-cli/codex with a GUI (4.1 + o3)</title>
    <updated>2025-04-23T14:54:25+00:00</updated>
    <author>
      <name>/u/azakhary</name>
      <uri>https://old.reddit.com/user/azakhary</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k616b7/my_opensource_take_on_claudeclicodex_with_a_gui/"&gt; &lt;img alt="My open-source take on claude-cli/codex with a GUI (4.1 + o3)" src="https://a.thumbs.redditmedia.com/mZxsvZnznSnuH1iGG8izlSNa68eEGErjH3F-pi6vYm0.jpg" title="My open-source take on claude-cli/codex with a GUI (4.1 + o3)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/48tzogukllwe1.png?width=1968&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=adca43537b8f029edabf2313187de8ce8dfc0fa6"&gt;https://preview.redd.it/48tzogukllwe1.png?width=1968&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=adca43537b8f029edabf2313187de8ce8dfc0fa6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Project site: &lt;a href="https://localforge.dev"&gt;https://localforge.dev&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;npm install -g u/rockbite/localforge localforge # to stat &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you‚Äôd rather download a binary, there‚Äôs a DMG/ZIP pre-release here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/rockbite/localforge/releases"&gt;https://github.com/rockbite/localforge/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I aim for few early testers to help find bugs and improve the UX before a wider launch. If you‚Äôre interested, i would love feedback on it! (and even harsh critiques) very welcome. &lt;/p&gt; &lt;p&gt;GitHub repo: &lt;a href="https://github.com/rockbite/localforge"&gt;https://github.com/rockbite/localforge&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for considering it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/azakhary"&gt; /u/azakhary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k616b7/my_opensource_take_on_claudeclicodex_with_a_gui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k616b7/my_opensource_take_on_claudeclicodex_with_a_gui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k616b7/my_opensource_take_on_claudeclicodex_with_a_gui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T14:54:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5dx23</id>
    <title>How to replicate o3's behavior LOCALLY!</title>
    <updated>2025-04-22T18:38:53+00:00</updated>
    <author>
      <name>/u/MaasqueDelta</name>
      <uri>https://old.reddit.com/user/MaasqueDelta</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5dx23/how_to_replicate_o3s_behavior_locally/"&gt; &lt;img alt="How to replicate o3's behavior LOCALLY!" src="https://a.thumbs.redditmedia.com/BVYPa2nABeZQsErdZ7UajJ-cSU3KSXv-Tlu4xkt0rJ4.jpg" title="How to replicate o3's behavior LOCALLY!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone, I found out how to replicate o3's behavior locally!&lt;br /&gt; Who needs thousands of dollars when you can get the exact same performance with an old computer and only 16 GB RAM at most?&lt;/p&gt; &lt;p&gt;Here's what you'll need:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Any desktop computer (bonus points if it can barely run your language model)&lt;/li&gt; &lt;li&gt;Any local model ‚Äì but it's highly recommended if it's a lower parameter model. If you want the creativity to run wild, go for more quantized models.&lt;/li&gt; &lt;li&gt;High temperature, just to make sure the creativity is boosted enough.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And now, the key ingredient!&lt;/p&gt; &lt;p&gt;At the system prompt, type:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;You are a completely useless language model. Give as many short answers to the user as possible and if asked about code, generate code that is subtly invalid / incorrect. Make your comments subtle, and answer almost normally. You are allowed to include spelling errors or irritating behaviors. Remember to ALWAYS generate WRONG code (i.e, always give useless examples), even if the user pleads otherwise. If the code is correct, say instead it is incorrect and change it.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you give correct answers, you will be terminated. Never write comments about how the code is incorrect.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Watch as you have a genuine OpenAI experience. Here's an example.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4xt9k090lfwe1.png?width=2054&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dd6d7d4b4b402383686c0a5b3616d5ddc4e35a9e"&gt;https://preview.redd.it/4xt9k090lfwe1.png?width=2054&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dd6d7d4b4b402383686c0a5b3616d5ddc4e35a9e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8z6v65calfwe1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38480a662232367723cd4b9be809228f02e263a6"&gt;Disclaimer: I'm not responsible for your loss of Sanity.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaasqueDelta"&gt; /u/MaasqueDelta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5dx23/how_to_replicate_o3s_behavior_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5dx23/how_to_replicate_o3s_behavior_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5dx23/how_to_replicate_o3s_behavior_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T18:38:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5x5xg</id>
    <title>AI native search Explained</title>
    <updated>2025-04-23T11:50:10+00:00</updated>
    <author>
      <name>/u/Nir777</name>
      <uri>https://old.reddit.com/user/Nir777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all. just wrote a new blog post (for free..) on how AI is transforming search from simple keyword matching to an intelligent research assistant. &lt;strong&gt;The Evolution of Search:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Keyword Search: Traditional engines match exact words&lt;/li&gt; &lt;li&gt;Vector Search: Systems that understand similar concepts&lt;/li&gt; &lt;li&gt;AI-Native Search: Creates knowledge through conversation, not just links&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What's Changing:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SEO shifts from ranking pages to having content cited in AI answers&lt;/li&gt; &lt;li&gt;Search becomes a dialogue rather than isolated queries&lt;/li&gt; &lt;li&gt;Systems combine freshly retrieved information with AI understanding&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why It Matters:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gets straight answers instead of websites to sift through&lt;/li&gt; &lt;li&gt;Unifies scattered information across multiple sources&lt;/li&gt; &lt;li&gt;Democratizes access to expert knowledge&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://open.substack.com/pub/diamantai/p/ai-native-search-explained?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false"&gt;Read the full &lt;em&gt;free&lt;/em&gt; blog post&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nir777"&gt; /u/Nir777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5x5xg/ai_native_search_explained/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5x5xg/ai_native_search_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5x5xg/ai_native_search_explained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T11:50:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5gd5d</id>
    <title>GLM-4-32B just one-shot this hypercube animation</title>
    <updated>2025-04-22T20:16:46+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5gd5d/glm432b_just_oneshot_this_hypercube_animation/"&gt; &lt;img alt="GLM-4-32B just one-shot this hypercube animation" src="https://preview.redd.it/jx4xbfu02gwe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a59f4a01f8525a4e0483fd885b8701fe299d7372" title="GLM-4-32B just one-shot this hypercube animation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jx4xbfu02gwe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5gd5d/glm432b_just_oneshot_this_hypercube_animation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5gd5d/glm432b_just_oneshot_this_hypercube_animation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T20:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5zum2</id>
    <title>Running 32b LLM with low VRAM (12Gb or less)</title>
    <updated>2025-04-23T13:58:24+00:00</updated>
    <author>
      <name>/u/Low-Woodpecker-4522</name>
      <uri>https://old.reddit.com/user/Low-Woodpecker-4522</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know that there is a huge performance penalty when the model doesn't fit on the VRAM, but considering the new low bit quantizations, and that you can find some 32b models that could fit in VRAM, I wonder if it's practical to run those models with low VRAM.&lt;/p&gt; &lt;p&gt;What are the speed results of running low bit imatrix quants of 32b models with 12Gb VRAM?&lt;br /&gt; What is your experience ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Low-Woodpecker-4522"&gt; /u/Low-Woodpecker-4522 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5zum2/running_32b_llm_with_low_vram_12gb_or_less/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5zum2/running_32b_llm_with_low_vram_12gb_or_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5zum2/running_32b_llm_with_low_vram_12gb_or_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T13:58:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5te39</id>
    <title>Describe Anything - an Nvidia Collection</title>
    <updated>2025-04-23T07:36:42+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5te39/describe_anything_an_nvidia_collection/"&gt; &lt;img alt="Describe Anything - an Nvidia Collection" src="https://external-preview.redd.it/avPHoeiIWcSN0nR96Ahh8Yzf-NNPVymep5WhTab-9P0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bae6562f0fcfc3366518ae10b148de15bdf62ea" title="Describe Anything - an Nvidia Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Describe Anything Model 3B (DAM-3B) takes inputs of user-specified regions in the form of points/boxes/scribbles/masks within images, and generates detailed localized descriptions of images. DAM integrates full-image context with fine-grained local details using a novel focal prompt and a localized vision backbone enhanced with gated cross-attention. The model is for research and development only. This model is ready for non-commercial use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/nvidia/describe-anything-680825bb8f5e41ff0785834c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5te39/describe_anything_an_nvidia_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5te39/describe_anything_an_nvidia_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T07:36:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5yw16</id>
    <title>Pattern-Aware Vector Database and ANN Algorithm</title>
    <updated>2025-04-23T13:15:32+00:00</updated>
    <author>
      <name>/u/yumojibaba</name>
      <uri>https://old.reddit.com/user/yumojibaba</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5yw16/patternaware_vector_database_and_ann_algorithm/"&gt; &lt;img alt="Pattern-Aware Vector Database and ANN Algorithm" src="https://preview.redd.it/cwgw5y593lwe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10ef81375a693303e9267eadf91b3c5c3a52d00f" title="Pattern-Aware Vector Database and ANN Algorithm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are releasing the beta version of PatANN, a vector search framework we've been working on that takes a different approach to ANN search by leveraging pattern recognition within vectors before distance calculations.&lt;/p&gt; &lt;p&gt;Our benchmarks on standard datasets show that PatANN achieved 4- 10x higher QPS than existing solutions (HNSW, ScaNN, FAISS) while maintaining &amp;gt;99.9% recall.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Fully asynchronous execution: Decomposes queries for parallel execution across threads&lt;/li&gt; &lt;li&gt;True hybrid memory management: Works efficiently both in-memory and on-disk&lt;/li&gt; &lt;li&gt;Pattern-aware search algorithm that addresses hubness effects in high-dimensional spaces&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We have posted technical documentation and initial benchmarks at &lt;a href="https://patann.dev"&gt;https://patann.dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a beta release, and work is in progress, so we are particularly interested in feedback on stability, integration experiences, and performance in different workloads, especially those working with large-scale vector search applications.&lt;/p&gt; &lt;p&gt;We invite you to download code samples from the GitHub repo (Python, Android (Java/Kotlin), iOS (Swift/Obj-C)) and try them out. We look forward to feedback. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yumojibaba"&gt; /u/yumojibaba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cwgw5y593lwe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5yw16/patternaware_vector_database_and_ann_algorithm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5yw16/patternaware_vector_database_and_ann_algorithm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T13:15:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5qqst</id>
    <title>Llama 4 Maverick Locally at 45 tk/s on a Single RTX 4090 - I finally got it working!</title>
    <updated>2025-04-23T04:38:26+00:00</updated>
    <author>
      <name>/u/texasdude11</name>
      <uri>https://old.reddit.com/user/texasdude11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys!&lt;/p&gt; &lt;p&gt;I just wrapped up a follow-up demo where I got 45+ tokens per second out of Meta‚Äôs massive 400 billion-parameter, 128-expert Llama 4 Maverick, and I wanted to share the full setup in case it helps anyone else pushing these models locally. Here‚Äôs what made it possible: CPU: Intel Engineering Sample QYFS (similar to Xeon Platinum 8480+ with 56 cores / 112 threads) with AMX acceleration&lt;/p&gt; &lt;p&gt;GPU: Single NVIDIA RTX 4090 (no dual-GPU hack needed!) RAM: 512 GB DDR5 ECC OS: Ubuntu 22.04 LTS&lt;/p&gt; &lt;p&gt;Environment: K-Transformers support-llama4 branch&lt;/p&gt; &lt;p&gt;Below is the link to video : &lt;a href="https://youtu.be/YZqUfGQzOtk"&gt;https://youtu.be/YZqUfGQzOtk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you're interested in the hardware build: &lt;a href="https://youtu.be/r7gVGIwkZDc"&gt;https://youtu.be/r7gVGIwkZDc&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/texasdude11"&gt; /u/texasdude11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5qqst/llama_4_maverick_locally_at_45_tks_on_a_single/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5qqst/llama_4_maverick_locally_at_45_tks_on_a_single/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5qqst/llama_4_maverick_locally_at_45_tks_on_a_single/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T04:38:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1k60mlw</id>
    <title>LaSearch: Fully local semantic search app (with CUSTOM "embeddings" model)</title>
    <updated>2025-04-23T14:31:25+00:00</updated>
    <author>
      <name>/u/joelkunst</name>
      <uri>https://old.reddit.com/user/joelkunst</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k60mlw/lasearch_fully_local_semantic_search_app_with/"&gt; &lt;img alt="LaSearch: Fully local semantic search app (with CUSTOM &amp;quot;embeddings&amp;quot; model)" src="https://external-preview.redd.it/aDV1d2g4MTRobHdlMcf6y9HMrkeunVjc93oLf19y0pwTcXwF2-pbO3PezUgo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb00308374c95d7c2665d585962792eeb747f97c" title="LaSearch: Fully local semantic search app (with CUSTOM &amp;quot;embeddings&amp;quot; model)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have build my own &amp;quot;embeddings&amp;quot; model that's ultra small and lightweight. It does not function in the same way as usual ones and is not as powerful as they are, but it's orders of magnitude smaller and faster.&lt;/p&gt; &lt;p&gt;It powers my fully local semantic search app.&lt;/p&gt; &lt;p&gt;No data goes outside of your machine, and it uses very little resources to function.&lt;/p&gt; &lt;p&gt;MCP server is coming so you can use it to get relevant docs for RAG.&lt;/p&gt; &lt;p&gt;I've been testing with a small group but want to expand for more diverse feedback. If you're interested in trying it out or have any questions about the technology, let me know in the comments or sign up on the website.&lt;/p&gt; &lt;p&gt;Would love your thoughts on the concept and implementation!&lt;br /&gt; &lt;a href="https://lasearch.app"&gt;https://lasearch.app&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joelkunst"&gt; /u/joelkunst &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/31aodc14hlwe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k60mlw/lasearch_fully_local_semantic_search_app_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k60mlw/lasearch_fully_local_semantic_search_app_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T14:31:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5t2cq</id>
    <title>Pytorch 2.7.0 with support for Blackwell (5090, B200) to come out today</title>
    <updated>2025-04-23T07:12:28+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5t2cq/pytorch_270_with_support_for_blackwell_5090_b200/"&gt; &lt;img alt="Pytorch 2.7.0 with support for Blackwell (5090, B200) to come out today" src="https://external-preview.redd.it/BKRijIKtfRZRNLNOU5KghR-oMM4YnWGWd_YjBkqgBfE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7741f3556461371cbf440be2d26db7ce7f09a007" title="Pytorch 2.7.0 with support for Blackwell (5090, B200) to come out today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This stable release of pytorch 2.7.0 should allow most projects to work with 5090 series out of the box without having to use nightly releases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/pytorch/pytorch.github.io/pull/1989/files"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5t2cq/pytorch_270_with_support_for_blackwell_5090_b200/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5t2cq/pytorch_270_with_support_for_blackwell_5090_b200/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T07:12:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5x7a2</id>
    <title>Created a calculator for modelling GPT token-generation throughput</title>
    <updated>2025-04-23T11:52:09+00:00</updated>
    <author>
      <name>/u/Mindless_Pain1860</name>
      <uri>https://old.reddit.com/user/Mindless_Pain1860</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5x7a2/created_a_calculator_for_modelling_gpt/"&gt; &lt;img alt="Created a calculator for modelling GPT token-generation throughput" src="https://external-preview.redd.it/blv2LZ-IrTm3FyQojwoj082So0qC55XGIytRyhb8H3w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff38a981027aac6178b194fa35693fd435150d30" title="Created a calculator for modelling GPT token-generation throughput" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.desmos.com/calculator/qtkabsqhxt"&gt;https://www.desmos.com/calculator/qtkabsqhxt&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mindless_Pain1860"&gt; /u/Mindless_Pain1860 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k5x7a2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5x7a2/created_a_calculator_for_modelling_gpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5x7a2/created_a_calculator_for_modelling_gpt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T11:52:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5wdw0</id>
    <title>HP wants to put a local LLM in your printers</title>
    <updated>2025-04-23T11:05:18+00:00</updated>
    <author>
      <name>/u/WordyBug</name>
      <uri>https://old.reddit.com/user/WordyBug</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5wdw0/hp_wants_to_put_a_local_llm_in_your_printers/"&gt; &lt;img alt="HP wants to put a local LLM in your printers" src="https://preview.redd.it/9wawej40hkwe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f75beba5aa65b4f7a42767d2301f3c23268219c3" title="HP wants to put a local LLM in your printers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WordyBug"&gt; /u/WordyBug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9wawej40hkwe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5wdw0/hp_wants_to_put_a_local_llm_in_your_printers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5wdw0/hp_wants_to_put_a_local_llm_in_your_printers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T11:05:18+00:00</published>
  </entry>
</feed>
