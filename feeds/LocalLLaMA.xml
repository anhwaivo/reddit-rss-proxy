<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-25T15:50:48+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ljw728</id>
    <title>NeuralTranslate: Nahuatl to Spanish LLM! (Gemma 3 27b fine-tune)</title>
    <updated>2025-06-25T04:15:40+00:00</updated>
    <author>
      <name>/u/Azuriteh</name>
      <uri>https://old.reddit.com/user/Azuriteh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! After quite a long time there's a new release from my open-source series of models: NeuralTranslate!&lt;/p&gt; &lt;p&gt;This time I full fine-tuned Gemma 3 27b on a Nahuatl-Spanish dataset. It comes with 3 versions: v1, v1.1 &amp;amp; v1.2. v1 is the epoch 4 checkpoint for the model, v1.1 is for epoch 9 &amp;amp; v1.2 is for epoch 10. I've seen great results with the v1.2 version and the demo for the model actually uses that one! But there might be some overfitting... I haven't thoroughly tested the checkpoints yet. v1 is the main release and shouldn't be presenting signs of overfitting from my limited testing, though!&lt;/p&gt; &lt;p&gt;Here is the demo: &lt;a href="https://huggingface.co/spaces/Thermostatic/neuraltranslate-27b-mt-nah-es"&gt;https://huggingface.co/spaces/Thermostatic/neuraltranslate-27b-mt-nah-es&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are the weights:&lt;/p&gt; &lt;p&gt;- v1: &lt;a href="https://huggingface.co/Thermostatic/neuraltranslate-27b-mt-nah-es-v1"&gt;https://huggingface.co/Thermostatic/neuraltranslate-27b-mt-nah-es-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- v1.1: &lt;a href="https://huggingface.co/Thermostatic/neuraltranslate-27b-mt-nah-es-v1.1"&gt;https://huggingface.co/Thermostatic/neuraltranslate-27b-mt-nah-es-v1.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- v1.2: &lt;a href="https://huggingface.co/Thermostatic/neuraltranslate-27b-mt-nah-es-v1.2"&gt;https://huggingface.co/Thermostatic/neuraltranslate-27b-mt-nah-es-v1.2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've contacted a few knowledgeable nahuatl speakers and it seems that the dataset itself is archaic, so sadly the model itself it's not as good as I'd wish I wanted, but hopefully I can overcome those issues in future releases! Currently working in creating the v1 of NeuralTranslate English to Spanish and will be releasing it shortly :)&lt;/p&gt; &lt;p&gt;I fine-tuned the model using a B200 with the help of Unsloth (4-bit full fine-tuning is a game changer). You can easily recreate my workflow with my public repo for training LLMs in QLoRa &amp;amp; Full fine-tune with Unsloth too: &lt;a href="https://github.com/Sekinal/neuraltranslate-nahuatl/tree/master"&gt;https://github.com/Sekinal/neuraltranslate-nahuatl/tree/master&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hopefully this isn't taken as spam, I'm really not trying to make a profit nor anything like that, I just think the model itself or my workflow would be of help for a lot of people and this is a really exciting project I wanted to share!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Azuriteh"&gt; /u/Azuriteh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljw728/neuraltranslate_nahuatl_to_spanish_llm_gemma_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljw728/neuraltranslate_nahuatl_to_spanish_llm_gemma_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljw728/neuraltranslate_nahuatl_to_spanish_llm_gemma_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T04:15:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk2pza</id>
    <title>Budget VPS as a viable off-ramp for unsustainable Google Cloud bills?</title>
    <updated>2025-06-25T11:04:17+00:00</updated>
    <author>
      <name>/u/reclusebird</name>
      <uri>https://old.reddit.com/user/reclusebird</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Our team is running a custom model on Google Cloud with a Vercel frontend. While we're seeing user growth, the GCP bill—driven by compute and data egress fees—is scaling much faster than our revenue. The cost has quickly become unsustainable.&lt;/p&gt; &lt;p&gt;We're now considering moving the AI backend to a budget VPS or bare-metal provider to survive. Most of us have backgrounds as researchers, not professional devs, our concern is the hidden complexity. &lt;/p&gt; &lt;p&gt;How much operational burden would we be taking on, and what are the real-world trade-offs in giving up the Google stack? &lt;/p&gt; &lt;p&gt;Any advice would be appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reclusebird"&gt; /u/reclusebird &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk2pza/budget_vps_as_a_viable_offramp_for_unsustainable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk2pza/budget_vps_as_a_viable_offramp_for_unsustainable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk2pza/budget_vps_as_a_viable_offramp_for_unsustainable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T11:04:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk20o4</id>
    <title>Running llama.pp et al on Strix Halo on Linux, anyone?</title>
    <updated>2025-06-25T10:24:17+00:00</updated>
    <author>
      <name>/u/Captain-Pie-62</name>
      <uri>https://old.reddit.com/user/Captain-Pie-62</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I bought short time ago a GMKtec EVO X2 , which sports the Strix Halo CPU/GPU hardware. I bought it with 128 GB RAM and 2 TB SSD. So I thought, 'This is the perfect system for a nice, private LLM machine, especially under Linux!&amp;quot; In real life I had to overcome some obstacles (i.E. upgrading the EFI BIOS by one minor number, in order to be able to allow the GPU to use up to 96 GB, instead of the default 64 GB, which was a hard limit, without that upgrade). There seem to be some more things to do, to get the best performance out of this box.&lt;/p&gt; &lt;p&gt;Yes, I already have it up and running (together with OpenWebUI and VPN) but it was a real PitA to get there.&lt;/p&gt; &lt;p&gt;Is there anybody out there, having the same idea and or issues? Like ROCm still doesn't support the gfx1151 LLVM-Target (officially) and the impossibility of running the latest ROCm with the latest Linux Kernels?&lt;/p&gt; &lt;p&gt;AMD, I hope you read this and act. Because this StrixHalo combination has the potential to become something like the 'Volks-AI'- system for private use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Captain-Pie-62"&gt; /u/Captain-Pie-62 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk20o4/running_llamapp_et_al_on_strix_halo_on_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk20o4/running_llamapp_et_al_on_strix_halo_on_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk20o4/running_llamapp_et_al_on_strix_halo_on_linux/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T10:24:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljogsx</id>
    <title>LinusTechTips reviews Chinese 4090s with 48Gb VRAM, messes with LLMs</title>
    <updated>2025-06-24T22:05:08+00:00</updated>
    <author>
      <name>/u/BumbleSlob</name>
      <uri>https://old.reddit.com/user/BumbleSlob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljogsx/linustechtips_reviews_chinese_4090s_with_48gb/"&gt; &lt;img alt="LinusTechTips reviews Chinese 4090s with 48Gb VRAM, messes with LLMs" src="https://external-preview.redd.it/ZSkXOQ0Ftmzf9m07Ydba1-71lECRPh1WZMhCFovef6Y.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fdb319a25ca00eba0456ee1f02c9bf5308cdb5e" title="LinusTechTips reviews Chinese 4090s with 48Gb VRAM, messes with LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just thought it might be fun for the community to see one of the largest tech YouTubers introducing their audience to local LLMs.&lt;/p&gt; &lt;p&gt;Lots of newbie mistakes in their messing with Open WebUI and Ollama but hopefully it encourages some of their audience to learn more. For anyone who saw the video and found their way here, welcome! Feel free to ask questions about getting started.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BumbleSlob"&gt; /u/BumbleSlob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/HZgQp-WDebU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljogsx/linustechtips_reviews_chinese_4090s_with_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljogsx/linustechtips_reviews_chinese_4090s_with_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T22:05:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljo4ns</id>
    <title>New Moondream 2B VLM update, with visual reasoning</title>
    <updated>2025-06-24T21:51:07+00:00</updated>
    <author>
      <name>/u/radiiquark</name>
      <uri>https://old.reddit.com/user/radiiquark</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/radiiquark"&gt; /u/radiiquark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://moondream.ai/blog/moondream-2025-06-21-release"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljo4ns/new_moondream_2b_vlm_update_with_visual_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljo4ns/new_moondream_2b_vlm_update_with_visual_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:51:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljs4e7</id>
    <title>All of our posts for the last week:</title>
    <updated>2025-06-25T00:47:47+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljs4e7/all_of_our_posts_for_the_last_week/"&gt; &lt;img alt="All of our posts for the last week:" src="https://preview.redd.it/0feqhgvc0z8f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb98aa33f9a72ba846bb3609af050401518880f2" title="All of our posts for the last week:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0feqhgvc0z8f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljs4e7/all_of_our_posts_for_the_last_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljs4e7/all_of_our_posts_for_the_last_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T00:47:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk0cjv</id>
    <title>Jan Nano + Deepseek R1: Combining Remote Reasoning with Local Models using MCP</title>
    <updated>2025-06-25T08:37:45+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Combining Remote Reasoning with Local Models&lt;/h1&gt; &lt;p&gt;I made this MCP server which wraps open source models on Hugging Face. It's useful if you want to give you local model access to (bigger) models via an API.&lt;/p&gt; &lt;p&gt;This is the basic idea:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Local model&lt;/strong&gt; handles initial user input and decides task complexity&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Remote model&lt;/strong&gt; (via MCP) processes complex reasoning and solves the problem&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local model&lt;/strong&gt; formats and delivers the final response, say in markdown or LaTeX.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;To use MCP tools on Hugging Face, you need to add the MCP server to your local tool.&lt;/p&gt; &lt;p&gt;&lt;code&gt;json { &amp;quot;servers&amp;quot;: { &amp;quot;hf-mcp-server&amp;quot;: { &amp;quot;url&amp;quot;: &amp;quot;https://huggingface.co/mcp&amp;quot;, &amp;quot;headers&amp;quot;: { &amp;quot;Authorization&amp;quot;: &amp;quot;Bearer &amp;lt;YOUR_HF_TOKEN&amp;gt;&amp;quot; } } } } &lt;/code&gt;&lt;/p&gt; &lt;p&gt;This will give your MCP client access to all the MCP servers you define in your &lt;a href="https://huggingface.co/settings/mcp"&gt;MCP settings&lt;/a&gt;. This is the best approach because the model get's access to general tools like searching the hub for models and datasets.&lt;/p&gt; &lt;p&gt;If you just want to add the inference providers MCP server directly, you can do this:&lt;/p&gt; &lt;p&gt;&lt;code&gt;json { &amp;quot;mcpServers&amp;quot;: { &amp;quot;inference-providers-mcp&amp;quot;: { &amp;quot;url&amp;quot;: &amp;quot;https://burtenshaw-inference-providers-mcp.hf.space/gradio_api/mcp/sse&amp;quot; } } } &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Or this, if your tool doesn't support url:&lt;/p&gt; &lt;p&gt;&lt;code&gt;json { &amp;quot;mcpServers&amp;quot;: { &amp;quot;inference-providers-mcp&amp;quot;: { &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;, &amp;quot;args&amp;quot;: [ &amp;quot;mcp-remote&amp;quot;, &amp;quot;https://burtenshaw-inference-providers-mcp.hf.space/gradio_api/mcp/sse&amp;quot;, &amp;quot;--transport&amp;quot;, &amp;quot;sse-only&amp;quot; ] } } } &lt;/code&gt;&lt;/p&gt; &lt;p&gt;You will need to duplicate the space on huggingface.co and add your own inference token.&lt;/p&gt; &lt;p&gt;Once you've down that, you can then prompt your local model to use the remote model. For example, I tried this:&lt;/p&gt; &lt;p&gt;``` Search for a deepseek r1 model on hugging face and use it to solve this problem via inference providers and groq: &amp;quot;Two quantum states with energies E1 and E2 have a lifetime of 10&lt;sup&gt;-9&lt;/sup&gt; sec and 10&lt;sup&gt;-8&lt;/sup&gt; sec, respectively. We want to clearly distinguish these two energy levels. Which one of the following options could be their energy difference so that they be clearly resolved?&lt;/p&gt; &lt;p&gt;10&lt;sup&gt;-4&lt;/sup&gt; eV 10&lt;sup&gt;-11&lt;/sup&gt; eV 10&lt;sup&gt;-8&lt;/sup&gt; eV 10&lt;sup&gt;-9&lt;/sup&gt; eV&amp;quot; ```&lt;/p&gt; &lt;p&gt;The main limitation is that the local model needs to be prompted directly to use the correct MCP tool, and parameters need to be declared rather than inferred, but this will depend on the local model's performance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk0cjv/jan_nano_deepseek_r1_combining_remote_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk0cjv/jan_nano_deepseek_r1_combining_remote_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk0cjv/jan_nano_deepseek_r1_combining_remote_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T08:37:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk82qj</id>
    <title>[Open Source] Build Your AI Team with Vibe Coding (Software 3.0 Framework)</title>
    <updated>2025-06-25T15:03:34+00:00</updated>
    <author>
      <name>/u/mpthouse</name>
      <uri>https://old.reddit.com/user/mpthouse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk82qj/open_source_build_your_ai_team_with_vibe_coding/"&gt; &lt;img alt="[Open Source] Build Your AI Team with Vibe Coding (Software 3.0 Framework)" src="https://external-preview.redd.it/aXJtNGJuM3g4MzlmMUG656sa9a8x7y41qMK9KHse6G3IOvzv264vz6Sx8d-p.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d94a8fb1561294a002a80a1f25e106decc44658" title="[Open Source] Build Your AI Team with Vibe Coding (Software 3.0 Framework)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Zentrun is an open-source Software 3.0 platform that lets you build AI agents&lt;br /&gt; that grow and evolve — by creating new features through &lt;strong&gt;vibe coding&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Unlike static scripts or prompt-only tools, Zentrun agents can&lt;br /&gt; &lt;strong&gt;build, run, and refine&lt;/strong&gt; their own workflows using natural language.&lt;/p&gt; &lt;p&gt;From automation and analytics to full UI and database logic,&lt;br /&gt; Zentrun turns your ideas into living, executable software — like real SaaS apps.&lt;/p&gt; &lt;p&gt;All runs locally, with full support for &lt;strong&gt;MCP&lt;/strong&gt;, &lt;strong&gt;Ollama&lt;/strong&gt;, and other modular backends.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;⚡️ &lt;strong&gt;Vibe-Coded AI Agents&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Say: &lt;em&gt;“Scrape AI job posts from Reddit and send a Slack summary.”&lt;/em&gt;&lt;/li&gt; &lt;li&gt;Zentrun turns that into working code, stores it as a &lt;strong&gt;Zent&lt;/strong&gt;, and lets your agent re-run or build on it.&lt;/li&gt; &lt;li&gt;Each new command becomes a new skill. Your agent evolves like software — not just responds.&lt;/li&gt; &lt;li&gt;Full support for local LLMs via Ollama&lt;/li&gt; &lt;li&gt;Compatible with any model provider in OpenAI/Gemini/Anthropic API format&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;🧠 &lt;strong&gt;Software 3.0 Architecture&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agents define and extend their automation, UI, analysis, and visualization — through vibe coding&lt;/li&gt; &lt;li&gt;Each agent has its own embedded database — remembers state, data, and logic&lt;/li&gt; &lt;li&gt;Real code execution with zero-code input: Python, browser control, API calls, shell commands&lt;/li&gt; &lt;li&gt;Supports LLMs like OpenAI, Claude, Gemini, and Ollama (local)&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;🛠️ &lt;strong&gt;Powered by MCP&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model Context Protocol handles memory, logging, and multi-tool orchestration&lt;/li&gt; &lt;li&gt;Natural-language-to-execution across scraping, file parsing, DB ops, and notifications&lt;/li&gt; &lt;li&gt;Zent → Agent → ZPilot hierarchy for scaling into multi-agent systems&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;💡 &lt;strong&gt;Use Cases&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sales: auto-scrape leads, summarize contacts, send follow-ups&lt;/li&gt; &lt;li&gt;HR: filter resumes, score candidates, auto-schedule interviews&lt;/li&gt; &lt;li&gt;Analytics: extract → analyze → visualize — entirely with vibe-coded agents&lt;/li&gt; &lt;li&gt;Marketing: generate content, monitor competitors, auto-publish across platforms&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;🖥️ &lt;strong&gt;Cross-Platform, Offline, and Open Source&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;macOS, Windows, and Linux support&lt;/li&gt; &lt;li&gt;Offline-first — agents work locally with full transparency&lt;/li&gt; &lt;li&gt;Open-source at: &lt;a href="https://github.com/andrewsky-labs/zentrun"&gt;https://github.com/andrewsky-labs/zentrun&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;🔗 &lt;strong&gt;Explore More&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;→ Try prebuilt agents or build your own AI team: &lt;a href="https://zentrun.com"&gt;https://zentrun.com&lt;/a&gt;&lt;br /&gt; → GitHub: &lt;a href="https://github.com/andrewsky-labs/zentrun"&gt;https://github.com/andrewsky-labs/zentrun&lt;/a&gt; &lt;/p&gt; &lt;hr /&gt; &lt;p&gt;We’re building Zentrun in public — feedback and contributions welcome!&lt;/p&gt; &lt;p&gt;If you’ve ever wanted an AI that grows like real software, give vibe coding a try.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mpthouse"&gt; /u/mpthouse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3glwsm3x839f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk82qj/open_source_build_your_ai_team_with_vibe_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk82qj/open_source_build_your_ai_team_with_vibe_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T15:03:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljnmj9</id>
    <title>Google researcher requesting feedback on the next Gemma.</title>
    <updated>2025-06-24T21:30:18+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnmj9/google_researcher_requesting_feedback_on_the_next/"&gt; &lt;img alt="Google researcher requesting feedback on the next Gemma." src="https://a.thumbs.redditmedia.com/YXztzxUAkpa8OQtPRt3lxinca8NVcah5DIxz1ZPOgn4.jpg" title="Google researcher requesting feedback on the next Gemma." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/kr52i2mn0y8f1.png?width=700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f654b4d8fc807a8722055201e8c097168452937f"&gt;https://x.com/osanseviero/status/1937453755261243600&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/osanseviero/status/1937453755261243600"&gt;https://x.com/osanseviero/status/1937453755261243600&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm gpu poor. 8-12B models are perfect for me. What are yout thoughts ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnmj9/google_researcher_requesting_feedback_on_the_next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnmj9/google_researcher_requesting_feedback_on_the_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnmj9/google_researcher_requesting_feedback_on_the_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:30:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk6dub</id>
    <title>Which gemma-3 (12b and 27b) version (Unsloth, Bartowski, stduhpf, Dampfinchen, QAT, non-QAT, etc) are you using/do you prefer?</title>
    <updated>2025-06-25T13:57:18+00:00</updated>
    <author>
      <name>/u/relmny</name>
      <uri>https://old.reddit.com/user/relmny</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lately I started using different versions of Qwen-3 (I used to use the Unsloth UD ones, but recently I started moving* to the non-UD ones or the Bartowski ones instead, as I get more t/s and more context) and I was considering the same for Gemma-3.&lt;br /&gt; But between what I was reading from comments and my own tests, and I'm confused.&lt;/p&gt; &lt;p&gt;I remember the Bartowski, Unsloth, stduhpf, Dampfinchen, QAT, no-QAT... and reading people complaining about QAT or saying how great it is, adds to the confusion.&lt;/p&gt; &lt;p&gt;So, which version are you using and, if you don't mind, why? (I'm currently using the Unsloth UD ones).&lt;/p&gt; &lt;p&gt;*Which I recently started to think that might be based on the different &amp;quot;Precision&amp;quot; values of the tensors, but is something I have no idea about and I still need to look at.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/relmny"&gt; /u/relmny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk6dub/which_gemma3_12b_and_27b_version_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk6dub/which_gemma3_12b_and_27b_version_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk6dub/which_gemma3_12b_and_27b_version_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T13:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljp29d</id>
    <title>So, what do people think about the new Mistral Small 3.2?</title>
    <updated>2025-06-24T22:30:10+00:00</updated>
    <author>
      <name>/u/TacticalRock</name>
      <uri>https://old.reddit.com/user/TacticalRock</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was wondering why the sub was so quiet lately, but alas, what're your thoughts so far?&lt;/p&gt; &lt;p&gt;I for one welcome the decreased repetition, solid &amp;quot;minor&amp;quot; update.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TacticalRock"&gt; /u/TacticalRock &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljp29d/so_what_do_people_think_about_the_new_mistral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljp29d/so_what_do_people_think_about_the_new_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljp29d/so_what_do_people_think_about_the_new_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T22:30:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk4vbo</id>
    <title>Could anyone get UI-TARS Desktop running locally?</title>
    <updated>2025-06-25T12:53:03+00:00</updated>
    <author>
      <name>/u/m_abdelfattah</name>
      <uri>https://old.reddit.com/user/m_abdelfattah</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While using Ollama or LM Studios for &lt;a href="https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B"&gt;UI-TARS-1.5-7B&lt;/a&gt; inference.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m_abdelfattah"&gt; /u/m_abdelfattah &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk4vbo/could_anyone_get_uitars_desktop_running_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk4vbo/could_anyone_get_uitars_desktop_running_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk4vbo/could_anyone_get_uitars_desktop_running_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T12:53:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk2swu</id>
    <title>What are the best 70b tier models/finetunes? (That fit into 48gb these days)</title>
    <updated>2025-06-25T11:08:58+00:00</updated>
    <author>
      <name>/u/DepthHour1669</name>
      <uri>https://old.reddit.com/user/DepthHour1669</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been a while since llama 3.3 came out. &lt;/p&gt; &lt;p&gt;Are there any real improvements in the 70b area? That size is interesting since it can fit into 48gb aka 2x 3090 very well when quantized. &lt;/p&gt; &lt;p&gt;Anything that beats Qwen 3 32b? &lt;/p&gt; &lt;p&gt;From what I can tell, the Qwen 3 models are cutting edge for general purpose use running locally, with Gemma 3 27b, Mistral Small 3.2, Deepseek-R1-0528-Qwen3-8b being notable exceptions that punch above Qwen 3 (30b or 32b) for some workloads. Are there any other models that beat these? I presume Llama 3.3 70b is too old now. &lt;/p&gt; &lt;p&gt;Any finetunes of 70b or 72b models that I should be aware of, similar to Deepseek's finetunes?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DepthHour1669"&gt; /u/DepthHour1669 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk2swu/what_are_the_best_70b_tier_modelsfinetunes_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk2swu/what_are_the_best_70b_tier_modelsfinetunes_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk2swu/what_are_the_best_70b_tier_modelsfinetunes_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T11:08:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljr1wn</id>
    <title>Where is OpenAI's open source model?</title>
    <updated>2025-06-24T23:57:16+00:00</updated>
    <author>
      <name>/u/_Vedr</name>
      <uri>https://old.reddit.com/user/_Vedr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did I miss something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_Vedr"&gt; /u/_Vedr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljr1wn/where_is_openais_open_source_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljr1wn/where_is_openais_open_source_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljr1wn/where_is_openais_open_source_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T23:57:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljnhca</id>
    <title>Made an LLM Client for the PS Vita</title>
    <updated>2025-06-24T21:24:23+00:00</updated>
    <author>
      <name>/u/ajunior7</name>
      <uri>https://old.reddit.com/user/ajunior7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnhca/made_an_llm_client_for_the_ps_vita/"&gt; &lt;img alt="Made an LLM Client for the PS Vita" src="https://external-preview.redd.it/Y283aGV6aXd6eDhmMfIP8BrPficmhyY5KB42Ptrwyms9E-ke6lpIPgzOipjX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40daff1e17d68cd71479175d661e93123af22f55" title="Made an LLM Client for the PS Vita" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all, awhile back I had ported llama2.c on the PS Vita for on-device inference using the TinyStories 260K &amp;amp; 15M checkpoints. Was a cool and fun concept to work on, but it wasn't too practical in the end.&lt;/p&gt; &lt;p&gt;Since then, I have made a full fledged LLM client for the Vita instead! You can even use the camera to take photos to send to models that support vision. In this demo I gave it an endpoint to test out vision and reasoning models, and I'm happy with how it all turned out. It isn't perfect, as LLMs like to display messages in fancy ways like using TeX and markdown formatting, so it shows that in its raw text. The Vita can't even do emojis!&lt;/p&gt; &lt;p&gt;You can download the vpk in the releases section of my repo. Throw in an endpoint and try it yourself! (If using an API key, I hope you are very patient in typing that out manually)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/callbacked/vela"&gt;https://github.com/callbacked/vela&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajunior7"&gt; /u/ajunior7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qunyr1jwzx8f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnhca/made_an_llm_client_for_the_ps_vita/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnhca/made_an_llm_client_for_the_ps_vita/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:24:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljs95d</id>
    <title>ThermoAsk: getting an LLM to set its own temperature</title>
    <updated>2025-06-25T00:54:24+00:00</updated>
    <author>
      <name>/u/tycho_brahes_nose_</name>
      <uri>https://old.reddit.com/user/tycho_brahes_nose_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljs95d/thermoask_getting_an_llm_to_set_its_own/"&gt; &lt;img alt="ThermoAsk: getting an LLM to set its own temperature" src="https://preview.redd.it/t8az5arc1z8f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40ab4b4271e74985945a33ea726d1e36e0b0897b" title="ThermoAsk: getting an LLM to set its own temperature" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got an LLM to dynamically adjust its own sampling temperature.&lt;/p&gt; &lt;p&gt;I wrote a blog post on how I did this and why dynamic temperature adjustment might be a valuable ability for a language model to possess: &lt;a href="http://amanvir.com/blog/getting-an-llm-to-set-its-own-temperature"&gt;amanvir.com/blog/getting-an-llm-to-set-its-own-temperature&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: LLMs can struggle with prompts that inherently require large changes in sampling temperature for sensible or accurate responses. This includes simple prompts like &amp;quot;pick a random number from &amp;lt;some range&amp;gt;&amp;quot; and more complex stuff like:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Solve the following math expression: &amp;quot;1 + 5 * 3 - 4 / 2&amp;quot;. Then, write a really abstract poem that contains the answer to this expression.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Tackling these prompts with a &amp;quot;default&amp;quot; temperature value will not lead to good responses. To solve this problem, I had the idea of allowing LLMs to request changes to their own temperature based on the task they were dealing with. To my knowledge, this is the first time such a system has been proposed, so I thought I'd use the opportunity to give this technique a name: &lt;strong&gt;ThermoAsk&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I've created a basic implementation of ThermoAsk that relies on Ollama's Python SDK and Qwen2.5-7B: &lt;a href="https://github.com/amanvirparhar/thermoask"&gt;github.com/amanvirparhar/thermoask&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I'd love to hear your thoughts on this approach!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tycho_brahes_nose_"&gt; /u/tycho_brahes_nose_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t8az5arc1z8f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljs95d/thermoask_getting_an_llm_to_set_its_own/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljs95d/thermoask_getting_an_llm_to_set_its_own/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T00:54:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljpo64</id>
    <title>I gave the same silly task to ~70 models that fit on 32GB of VRAM - thousands of times (resharing my post from /r/LocalLLM)</title>
    <updated>2025-06-24T22:56:20+00:00</updated>
    <author>
      <name>/u/EmPips</name>
      <uri>https://old.reddit.com/user/EmPips</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd posted this over at &lt;a href="/r/LocalLLM"&gt;/r/LocalLLM&lt;/a&gt; and Some people thought I presented this too much as serious research - it wasn't, it was much closer to a bored rainy day activity. So here's the post I've been waiting to make on &lt;a href="/r/LocalLLaMA"&gt;/r/LocalLLaMA&lt;/a&gt; for some time, simplified as casually as possible:&lt;/p&gt; &lt;p&gt;Quick recap - &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbfinu/26_quants_that_fit_on_32gb_vs_10000token_needle/"&gt;here is the original post&lt;/a&gt; from a few weeks ago where users suggested I greatly expand the scope of this little game. &lt;a href="https://old.reddit.com/r/LocalLLM/comments/1liy7ku/i_thousands_of_tests_on_104_different_ggufs_10k/"&gt;Here is the post on /r/LocalLLM&lt;/a&gt; yesterday that I imagine some of you saw. I hope you don't mind the cross-post - but &lt;em&gt;THIS&lt;/em&gt; is the subreddit that I really wanted to bounce this off of and yesterday it was going through a change-of-management :-)&lt;/p&gt; &lt;p&gt;To be as brief/casual as possible: I broke HG Well's &lt;em&gt;&amp;quot;The Time Machine&amp;quot;&lt;/em&gt; again with a sentence that was correct English, but contextually nonsense, and asked a bunch of quantized LLM's (all that fit with 16k context on 32GB of VRAM). I did this multiple times at all temperatures from 0.0 to 0.9 in steps of 0.1 . For models with optional reasoning I split thinking mode on and off.&lt;/p&gt; &lt;h2&gt;What should you take from this?&lt;/h2&gt; &lt;p&gt;nothing at all! I'm hoping to get a better feel for how quantization works on some of my favorite models, so will take a little thing I do during my day and repeat it thousands and thousands of times to see if patterns emerge. I share this dataset with you for fun. I have my takeaways, I'd be interested to hear yours. My biggest takeaway from this is that I built a little framework of scripts for myself that will run and evaluate these sorts of tests at whatever scale I set them to.&lt;/p&gt; &lt;h2&gt;The Results&lt;/h2&gt; &lt;p&gt;Without further ado, the results. The 'Score' column is a percentage of correct answers.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Quant&lt;/th&gt; &lt;th&gt;Reasoning&lt;/th&gt; &lt;th&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Meta Llama Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.2_3B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.2_3B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.2_3B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.1_8B_Instruct&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.1_8B_Instruct&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.1_8B_Instruct&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_70B_Instruct&lt;/td&gt; &lt;td&gt;iq1&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_70B_Instruct&lt;/td&gt; &lt;td&gt;iq2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_70B_Instruct&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_4_Scout_17B&lt;/td&gt; &lt;td&gt;iq1&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_4_Scout_17B&lt;/td&gt; &lt;td&gt;iq2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Nvidia Nemotron Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.1_Nemotron_8B_UltraLong&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.1_Nemotron_8B_UltraLong&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;67&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq2&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq2&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Mistral Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mistral_Small_24B_2503&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mistral_Small_24B_2503&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;83&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mistral_Small_24B_2503&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Microsoft Phi Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Phi_4&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Phi_4&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Phi_4&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Phi_4&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Alibaba Qwen Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5_14B_Instruct&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5_14B_Instruct&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5_14B_Instruct&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5_Coder_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5_Coder_32B_Instruct&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;QwQ_32B&lt;/td&gt; &lt;td&gt;iq2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;QwQ_32B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;QwQ_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;67&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;QwQ_32B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;83&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;QwQ_32B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;53&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A6B_16_Extreme&lt;/td&gt; &lt;td&gt;q4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A6B_16_Extreme&lt;/td&gt; &lt;td&gt;q4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A6B_16_Extreme&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A6B_16_Extreme&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Google Gemma Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_12B_IT&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_12B_IT&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_12B_IT&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_27B_IT&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_27B_IT&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_27B_IT&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Deepseek (Distill) Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Qwen3_8B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Qwen3_8B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Qwen3_8B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Distill_Qwen_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Distill_Qwen_32B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Distill_Qwen_32B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Other&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cogito&lt;em&gt;v1_Preview&lt;/em&gt;&lt;em&gt;Qwen_14B&lt;/em&gt;&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cogito&lt;em&gt;v1_Preview&lt;/em&gt;&lt;em&gt;Qwen_14B&lt;/em&gt;&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cogito&lt;em&gt;v1_Preview&lt;/em&gt;&lt;em&gt;Qwen_14B&lt;/em&gt;&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM_4_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM_4_32B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM_4_32B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;16&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmPips"&gt; /u/EmPips &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljpo64/i_gave_the_same_silly_task_to_70_models_that_fit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljpo64/i_gave_the_same_silly_task_to_70_models_that_fit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljpo64/i_gave_the_same_silly_task_to_70_models_that_fit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T22:56:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljxa2e</id>
    <title>Gemini CLI: your open-source AI agent</title>
    <updated>2025-06-25T05:18:00+00:00</updated>
    <author>
      <name>/u/adefa</name>
      <uri>https://old.reddit.com/user/adefa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljxa2e/gemini_cli_your_opensource_ai_agent/"&gt; &lt;img alt="Gemini CLI: your open-source AI agent" src="https://external-preview.redd.it/v_nU-59VjAFg3tUf3ktH0OR1eDLLCpt7sTIO-4lpiic.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66dc977cf68889558dd1e0a18ef318dff22dc727" title="Gemini CLI: your open-source AI agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Really generous free tier&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adefa"&gt; /u/adefa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.google/technology/developers/introducing-gemini-cli/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljxa2e/gemini_cli_your_opensource_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljxa2e/gemini_cli_your_opensource_ai_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T05:18:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljm3pb</id>
    <title>LocalLlama is saved!</title>
    <updated>2025-06-24T20:30:08+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LocalLlama has been many folk's favorite place to be for everything AI, so it's good to see a new moderator taking the reins!&lt;/p&gt; &lt;p&gt;Thanks to &lt;a href="/u/HOLUPREDICTIONS"&gt;u/HOLUPREDICTIONS&lt;/a&gt; for taking the reins!&lt;/p&gt; &lt;p&gt;More detail here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR - the previous moderator (we appreciate their work) unfortunately left the subreddit, and unfortunately deleted new comments and posts - it's now lifted!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm3pb/localllama_is_saved/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm3pb/localllama_is_saved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm3pb/localllama_is_saved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:30:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk63od</id>
    <title>Gemini CLI: your open-source AI agent</title>
    <updated>2025-06-25T13:45:52+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk63od/gemini_cli_your_opensource_ai_agent/"&gt; &lt;img alt="Gemini CLI: your open-source AI agent" src="https://external-preview.redd.it/v_nU-59VjAFg3tUf3ktH0OR1eDLLCpt7sTIO-4lpiic.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66dc977cf68889558dd1e0a18ef318dff22dc727" title="Gemini CLI: your open-source AI agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Free license gets you access to Gemini 2.5 Pro and its massive 1 million token context window. To ensure you rarely, if ever, hit a limit during this preview, we offer the industry’s largest allowance: 60 model requests per minute and 1,000 requests per day at no charge.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk63od/gemini_cli_your_opensource_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk63od/gemini_cli_your_opensource_ai_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T13:45:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljlr5b</id>
    <title>Subreddit back in business</title>
    <updated>2025-06-24T20:16:36+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"&gt; &lt;img alt="Subreddit back in business" src="https://preview.redd.it/1sx7mwusnx8f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3f5a6313e8a4b034a44e79151a371760d959973" title="Subreddit back in business" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As most of you folks I'm also not sure what happened but I'm attaching screenshot of the last actions taken by the previous moderator before deleting their account &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1sx7mwusnx8f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:16:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk40ac</id>
    <title>Hunyuan-A13B</title>
    <updated>2025-06-25T12:12:27+00:00</updated>
    <author>
      <name>/u/lly0571</name>
      <uri>https://old.reddit.com/user/lly0571</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-A13B-Instruct-FP8"&gt;https://huggingface.co/tencent/Hunyuan-A13B-Instruct-FP8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I think the model should be a ~80B MoE. As 3072x4096x3x(64+1)*32 = 78.5B, and there are embedding layers and gating parts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lly0571"&gt; /u/lly0571 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk40ac/hunyuana13b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk40ac/hunyuana13b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk40ac/hunyuana13b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T12:12:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk12th</id>
    <title>New Mistral Small 3.2 actually feels like something big. [non-reasoning]</title>
    <updated>2025-06-25T09:26:23+00:00</updated>
    <author>
      <name>/u/Snail_Inference</name>
      <uri>https://old.reddit.com/user/Snail_Inference</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk12th/new_mistral_small_32_actually_feels_like/"&gt; &lt;img alt="New Mistral Small 3.2 actually feels like something big. [non-reasoning]" src="https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47f397e4a22ed5ec7e82aad070eb446319603abc" title="New Mistral Small 3.2 actually feels like something big. [non-reasoning]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/1wwakei8k19f1.png?width=1009&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fb72a4bf78efba7661e6ea5f54df70331a15539b"&gt;https://preview.redd.it/1wwakei8k19f1.png?width=1009&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fb72a4bf78efba7661e6ea5f54df70331a15539b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In my experience, it ranges far above its size.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="http://artificialanalysis.ai"&gt;artificialanalysis.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snail_Inference"&gt; /u/Snail_Inference &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk12th/new_mistral_small_32_actually_feels_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk12th/new_mistral_small_32_actually_feels_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk12th/new_mistral_small_32_actually_feels_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T09:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljyo2p</id>
    <title>Jan-nano-128k: A 4B Model with a Super-Long Context Window (Still Outperforms 671B)</title>
    <updated>2025-06-25T06:44:26+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljyo2p/jannano128k_a_4b_model_with_a_superlong_context/"&gt; &lt;img alt="Jan-nano-128k: A 4B Model with a Super-Long Context Window (Still Outperforms 671B)" src="https://external-preview.redd.it/MDRyeGJ6bmJvMDlmMdx7LrexgFcEoZTqX8Yp_PzSREeGDqUB-Qd2XY93v_7d.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c71ebd7c03a7ccb476c3ff52d6b9e5cc00e65722" title="Jan-nano-128k: A 4B Model with a Super-Long Context Window (Still Outperforms 671B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone it's me from Menlo Research again,&lt;/p&gt; &lt;p&gt;Today, I'd like to introduce our latest model: &lt;strong&gt;Jan-nano-128k&lt;/strong&gt; - this model is fine-tuned on &lt;strong&gt;Jan-nano&lt;/strong&gt; (which is a qwen3 finetune), improve performance when enable YaRN scaling &lt;strong&gt;(instead of having degraded performance)&lt;/strong&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It can uses tools continuously, repeatedly. &lt;/li&gt; &lt;li&gt;It can perform deep research &lt;strong&gt;VERY VERY DEEP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Extremely persistence (please pick the right MCP as well)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Again, we are not trying to beat Deepseek-671B models, we just want to see how far this current model can go. To our surprise, &lt;strong&gt;it is going very very far.&lt;/strong&gt; Another thing, we have spent all the resource on this version of Jan-nano so.... &lt;/p&gt; &lt;p&gt;&lt;strong&gt;We pushed back the technical report release! But it's coming ...sooon!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You can find the model at:&lt;br /&gt; &lt;a href="https://huggingface.co/Menlo/Jan-nano-128k"&gt;https://huggingface.co/Menlo/Jan-nano-128k&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also have gguf at:&lt;br /&gt; &lt;strong&gt;We are converting the GGUF check in comment section&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This model will require &lt;strong&gt;YaRN Scaling&lt;/strong&gt; supported from inference engine, we already configure it in the model, but your inference engine will need to be able to handle YaRN scaling. Please run the model in l&lt;strong&gt;lama.server or Jan app&lt;/strong&gt; (these are from our team, we tested them, just it).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;SimpleQA:&lt;/strong&gt;&lt;br /&gt; - OpenAI o1: 42.6&lt;br /&gt; - Grok 3: 44.6&lt;br /&gt; - 03: 49.4&lt;br /&gt; - Claude-3.7-Sonnet: 50.0&lt;br /&gt; - Gemini-2.5 pro: 52.9&lt;br /&gt; &lt;strong&gt;- baseline-with-MCP: 59.2&lt;/strong&gt;&lt;br /&gt; - ChatGPT-4.5: 62.5&lt;br /&gt; &lt;strong&gt;- deepseek-671B-with-MCP: 78.2&lt;/strong&gt; (we benchmark using openrouter)&lt;br /&gt; - jan-nano-v0.4-with-MCP: 80.7&lt;br /&gt; &lt;strong&gt;- jan-nano-128k-with-MCP: 83.2&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/909kwwnbo09f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljyo2p/jannano128k_a_4b_model_with_a_superlong_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljyo2p/jannano128k_a_4b_model_with_a_superlong_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T06:44:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lk5u1o</id>
    <title>The Jan.ai "team" used fake engagement to advertise their new 4B model, and deleted the post when called out</title>
    <updated>2025-06-25T13:34:38+00:00</updated>
    <author>
      <name>/u/Xandred_the_thicc</name>
      <uri>https://old.reddit.com/user/Xandred_the_thicc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These are all of my interactions with the jan &amp;quot;team&amp;quot;, followed by an instantly deleted angry comment, and the deletion of their entire announcement post without an explanation. Up to you how to interpret their response, but personally i feel i've seen enough just sorting the comment section by old and clicking a few random profiles.&lt;/p&gt; &lt;p&gt;&lt;a href="https://imgur.com/GHfWpXr"&gt;The initial interaction concerning the weirdly sycophantic comment section full of people who have apparently already used the just-announced model enough for it to be their favorite!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://imgur.com/eds5hhX"&gt;The totally real engagement from their &amp;quot;team members&amp;quot;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'll link some of the accounts i assume are supposed to be &amp;quot;&lt;a href="https://imgur.com/HrtDWen"&gt;team members answering people&lt;/a&gt;&amp;quot;, unfortunately since &lt;a href="https://www.reveddit.com/v/LocalLLaMA/comments/1ljyo2p/jannano128k_a_4b_model_with_a_superlong_context/mznsoq5/#t1_mznsoq5"&gt;only one of the accounts pictured owned up to their affiliation&lt;/a&gt; i'm going to have to make assumptions based on these commenters &lt;a href="https://imgur.com/hREcCVZ"&gt;coincidentally posting almost exclusively on jan products with posting behavior that can best be described as &amp;quot;suspicious&amp;quot;.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The original poster's account is also deleted now, which is totally what you do when you're a trustworthy org who can be trusted not to bot your github stars.&lt;/p&gt; &lt;p&gt;It's just especially insulting to be gaslit in this plausibly deniable way that allows them to move the goalposts as to how dishonest they were being based on how deep people are willing to dig.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xandred_the_thicc"&gt; /u/Xandred_the_thicc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk5u1o/the_janai_team_used_fake_engagement_to_advertise/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lk5u1o/the_janai_team_used_fake_engagement_to_advertise/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lk5u1o/the_janai_team_used_fake_engagement_to_advertise/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T13:34:38+00:00</published>
  </entry>
</feed>
