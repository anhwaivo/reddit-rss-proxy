<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-31T01:57:05+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1md0gfh</id>
    <title>RTX 5090 form INNO3D 1 slot with Alphacool-waterkoeling look perfect for local AI machines</title>
    <updated>2025-07-30T07:46:19+00:00</updated>
    <author>
      <name>/u/jwestra</name>
      <uri>https://old.reddit.com/user/jwestra</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md0gfh/rtx_5090_form_inno3d_1_slot_with/"&gt; &lt;img alt="RTX 5090 form INNO3D 1 slot with Alphacool-waterkoeling look perfect for local AI machines" src="https://preview.redd.it/eeopjbr7uyff1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=071af31f7998dd67f773b41419988ca83dd8fdd3" title="RTX 5090 form INNO3D 1 slot with Alphacool-waterkoeling look perfect for local AI machines" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Keeping your warranty.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;1 slot&lt;br /&gt;&lt;/li&gt; &lt;li&gt;backside tube exits &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Look perfect to make a dense AI machine.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.inno3d.com/news/inno3d-geforce-rtx-5090-rtx-5080-frostbite-pro-1-slot-design"&gt;https://www.inno3d.com/news/inno3d-geforce-rtx-5090-rtx-5080-frostbite-pro-1-slot-design&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jwestra"&gt; /u/jwestra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eeopjbr7uyff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md0gfh/rtx_5090_form_inno3d_1_slot_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md0gfh/rtx_5090_form_inno3d_1_slot_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T07:46:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1md9nc8</id>
    <title>GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning</title>
    <updated>2025-07-30T15:29:39+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2507.19457"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md9nc8/gepa_reflective_prompt_evolution_can_outperform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md9nc8/gepa_reflective_prompt_evolution_can_outperform/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T15:29:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdishv</id>
    <title>Best LLMs to preserve in case of internet apocalypse</title>
    <updated>2025-07-30T21:17:29+00:00</updated>
    <author>
      <name>/u/nos_66</name>
      <uri>https://old.reddit.com/user/nos_66</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am a long time lurker, but I took a break after the rtx 5090 launch fail since I almost completely gave up on getting to run ai locally this year.&lt;/p&gt; &lt;p&gt;With everything that's going on in the world and the possibility of the ai being considered &amp;quot;too dangerous&amp;quot;, apparently the music may already be, I want to ask which llm is &amp;quot;good&amp;quot; today (not in the way of SOTA, but by personal user experience). I am planning on using an intel b60 48gb vram or maybe 1-2 amd mi50 32gb. I am mostly interested in llm, vllm and probably one for coding, although it's not really needed since I know how to code, but it might come handy I don't know. I guess what I might need is probably 7-70b parameter ones, I also have 96gb ram so a larger moe might also be decent. The total storage for all ais is probably 2-3tb. If I am at this topic I suppose that the intel gpu might be better for image generation&lt;/p&gt; &lt;p&gt;I am old enough to remember mixtral 7x8 but I have no idea if it's still relevant, I know some mistral small might be better, also I might be interested in the vllm one for ocr. I kinda have an idea of most of the llms including the new qwen moes, but I have no idea which of the old models are still relevant today. For example I know that lama 3, or even 3.3 is kinda &amp;quot;outdated&amp;quot; (since I have no better word, but you get what I mean), I am even aware of a new nemotron which is based on lama 70b but I am missing a lot of details.&lt;/p&gt; &lt;p&gt;I know I should be able to find them on huggingface, and I might need to download vllm, ollama and intel playgrounds or idk how it is for it.&lt;/p&gt; &lt;p&gt;I know exactly how to get the stable diffusion models, but while we are at it I might be interested in a few tts models (text to speech, preferably with voice cloning), I think I've heard of &amp;quot;megatts 3&amp;quot; and &amp;quot;GPT-SoVITS&amp;quot; but any tips here are helpful as well. Meanwhile I will to find the fastest whisper model for stt, I am certain I might have saved the link for it somewhere.&lt;/p&gt; &lt;p&gt;Sorry for creating trash posts that are probably lots and lots on weekly bases for this particular question (not that particular considering the title, but you get what I mean).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nos_66"&gt; /u/nos_66 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T21:17:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcr64f</id>
    <title>4B models are consistently overlooked. Runs Locally and Crushes It. Reasoning for UI, Mobile, Software and Frontend design.</title>
    <updated>2025-07-29T23:36:00+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcr64f/4b_models_are_consistently_overlooked_runs/"&gt; &lt;img alt="4B models are consistently overlooked. Runs Locally and Crushes It. Reasoning for UI, Mobile, Software and Frontend design." src="https://b.thumbs.redditmedia.com/3wFSGxs0og7hUYyLF8nuoy2CBvu34JQ_m2cRe7ujEoc.jpg" title="4B models are consistently overlooked. Runs Locally and Crushes It. Reasoning for UI, Mobile, Software and Frontend design." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-X-4B-0729"&gt;https://huggingface.co/Tesslate/UIGEN-X-4B-0729&lt;/a&gt; 4B model that does reasoning for Design. We also released a 32B earlier in the week. &lt;/p&gt; &lt;p&gt;As per the last post -&amp;gt;&lt;br /&gt; Specifically trained for modern web and mobile development across frameworks like React (Next.js, Remix, Gatsby, Vite), Vue (Nuxt, Quasar), Angular (Angular CLI, Ionic), and SvelteKit, along with Solid.js, Qwik, Astro, and static site tools like 11ty and Hugo. Styling options include Tailwind CSS, CSS-in-JS (Styled Components, Emotion), and full design systems like Carbon and Material UI. We cover UI libraries for every framework React (shadcn/ui, Chakra, Ant Design), Vue (Vuetify, PrimeVue), Angular, and Svelte plus headless solutions like Radix UI. State management spans Redux, Zustand, Pinia, Vuex, NgRx, and universal tools like MobX and XState. For animation, we support Framer Motion, GSAP, and Lottie, with icons from Lucide, Heroicons, and more. Beyond web, we enable React Native, Flutter, and Ionic for mobile, and Electron, Tauri, and Flutter Desktop for desktop apps. Python integration includes Streamlit, Gradio, Flask, and FastAPI. All backed by modern build tools, testing frameworks, and support for 26+ languages and UI approaches, including JavaScript, TypeScript, Dart, HTML5, CSS3, and component-driven architectures.&lt;/p&gt; &lt;p&gt;We're looking for some beta testers for some new models and open source projects!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mcr64f"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcr64f/4b_models_are_consistently_overlooked_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcr64f/4b_models_are_consistently_overlooked_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T23:36:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdgjmk</id>
    <title>I’m curious to know how does MLX adds support for models faster than llama.cpp</title>
    <updated>2025-07-30T19:49:36+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a mac and whenever a new model launches, I see MLX quants available in a day or two. However GGUF takes more time due to llama.cpp support. Recent example is GLM 4.5&lt;/p&gt; &lt;p&gt;I’m just genuinely curious to know, what makes it easy or faster to add support in MLX.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T19:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdawyz</id>
    <title>Just launched Transformer Lab Recipes: 13 pre-built templates including Llama 3.2 fine-tuning, quantization, and benchmarking.</title>
    <updated>2025-07-30T16:17:35+00:00</updated>
    <author>
      <name>/u/aliasaria</name>
      <uri>https://old.reddit.com/user/aliasaria</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdawyz/just_launched_transformer_lab_recipes_13_prebuilt/"&gt; &lt;img alt="Just launched Transformer Lab Recipes: 13 pre-built templates including Llama 3.2 fine-tuning, quantization, and benchmarking." src="https://preview.redd.it/x7gqer73e1gf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=59a0719796cc2073a0df325ec3f1a9ef590559cd" title="Just launched Transformer Lab Recipes: 13 pre-built templates including Llama 3.2 fine-tuning, quantization, and benchmarking." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After getting helpful feedback from you all, our team just shipped &amp;quot;Recipes” which are pre-built, fully-runnable workflows for common LLM tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some of the most popular recipes include:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Llama 3.2 1B fine-tuning&lt;/strong&gt; (with Apple Silicon MLX optimization!)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model quantization to GGUF&lt;/strong&gt; format (CPU and GPU)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benchmark evaluation&lt;/strong&gt; (MMLU, HellaSwag, PIQA, Winogrande)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LoRA training&lt;/strong&gt; with before/after comparisons&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dialogue summarization&lt;/strong&gt; (perfect for chat logs)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We support local hardware (CUDA, AMD ROCm, Apple MLX, or CPU) and let you modify anything: model, data, params. Zero config to get started and we’re open source.&lt;/p&gt; &lt;p&gt;Been testing the Llama 3.2 fine-tuning recipe and the results are great. Way faster than setting everything up from scratch. &lt;/p&gt; &lt;p&gt;What local training workflows are you all using? This seems like it could replace a lot of custom scripts. Appreciate your feedback. What recipes should we add?&lt;/p&gt; &lt;p&gt;🔗 Try it here →&lt;a href="https://transformerlab.ai/docs/intro"&gt; &lt;/a&gt;&lt;a href="https://transformerlab.ai/"&gt;https://transformerlab.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🔗 Useful? Please star us on GitHub → &lt;a href="https://github.com/transformerlab"&gt;https://github.com/transformerlab/transformerlab-app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🔗 Ask for help on our Discord Community → &lt;a href="https://discord.gg/transformerlab"&gt;https://discord.gg/transformerlab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aliasaria"&gt; /u/aliasaria &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x7gqer73e1gf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdawyz/just_launched_transformer_lab_recipes_13_prebuilt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdawyz/just_launched_transformer_lab_recipes_13_prebuilt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T16:17:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1md7h5z</id>
    <title>Meta’s Vision for the future of Personal SuperIntelligence</title>
    <updated>2025-07-30T14:04:42+00:00</updated>
    <author>
      <name>/u/5h3r_10ck</name>
      <uri>https://old.reddit.com/user/5h3r_10ck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md7h5z/metas_vision_for_the_future_of_personal/"&gt; &lt;img alt="Meta’s Vision for the future of Personal SuperIntelligence" src="https://b.thumbs.redditmedia.com/wTDzxveLNV6lyThS6Dq2WJK_bRtGmie5PYzTvcrT62c.jpg" title="Meta’s Vision for the future of Personal SuperIntelligence" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today Mark shared Meta’s vision for the future of personal superintelligence for everyone. &lt;/p&gt; &lt;p&gt;Redditors!! What's your take on this?&lt;/p&gt; &lt;p&gt;Read his full letter here: &lt;a href="https://www.meta.com/superintelligence/"&gt;https://www.meta.com/superintelligence/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/5h3r_10ck"&gt; /u/5h3r_10ck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1md7h5z"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md7h5z/metas_vision_for_the_future_of_personal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md7h5z/metas_vision_for_the_future_of_personal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T14:04:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1md00oc</id>
    <title>Kudos to Qwen 3 team!</title>
    <updated>2025-07-30T07:17:24+00:00</updated>
    <author>
      <name>/u/ExcuseAccomplished97</name>
      <uri>https://old.reddit.com/user/ExcuseAccomplished97</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md00oc/kudos_to_qwen_3_team/"&gt; &lt;img alt="Kudos to Qwen 3 team!" src="https://b.thumbs.redditmedia.com/mUsja5QiJMisNYHJhZA4P57qdlHnaGZYvKopTiB-51E.jpg" title="Kudos to Qwen 3 team!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Qwen3-30B-A3B-Instruct-2507 is an amazing release! Congratulations!&lt;/p&gt; &lt;p&gt;However, the three-month-old 32B shows better performance across the board in the benchmark. I hope the Qwen3-32B Instruct/Thinking and Qwen3-30B-A3B-Thinking-2507 versions will be released soon!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExcuseAccomplished97"&gt; /u/ExcuseAccomplished97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md00oc/kudos_to_qwen_3_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md00oc/kudos_to_qwen_3_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md00oc/kudos_to_qwen_3_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T07:17:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdj3ap</id>
    <title>CUDA-L1 Improving CUDA Optimization via Contrastive Reinforcement Learning</title>
    <updated>2025-07-30T21:29:39+00:00</updated>
    <author>
      <name>/u/Optimal-Outcome-7458</name>
      <uri>https://old.reddit.com/user/Optimal-Outcome-7458</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found this post really worth reading.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/deep_reinforce/status/1950654480023957646"&gt;https://x.com/deep_reinforce/status/1950654480023957646&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Large language models can write CUDA kernels. Does this mean that one day LLMs can evolve 100% by themselves?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Optimal-Outcome-7458"&gt; /u/Optimal-Outcome-7458 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdj3ap/cudal1_improving_cuda_optimization_via/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdj3ap/cudal1_improving_cuda_optimization_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdj3ap/cudal1_improving_cuda_optimization_via/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T21:29:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdldom</id>
    <title>Kimi K2 vs Claude 4 Sonnet - Unexpected Review Result (400k token Codebase)</title>
    <updated>2025-07-30T23:03:20+00:00</updated>
    <author>
      <name>/u/marvijo-software</name>
      <uri>https://old.reddit.com/user/marvijo-software</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tested Kimi K2 again, against Claude 4 Sonnet (Sonnet 4) this time, here are my findings (vid in comments):&lt;/p&gt; &lt;p&gt;- K2 isn't only less reliable in VSCode tool calling, it's considerably less in Cline as well, vs Claude 4 Sonnet&lt;/p&gt; &lt;p&gt;- I integrated K2 via OpenRouter inference into my own application LIVE and it did the same thing: instead of calling tools, it outputs the tool calls as text, mostly malformed and consolidated&lt;/p&gt; &lt;p&gt;- Ref: &lt;a href="https://youtu.be/p2LKJo3EK7w"&gt;https://youtu.be/p2LKJo3EK7w&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Tip for AI coding agent authors: write a parser or a specialized prompt for Kimi K2 - even if it sounds like coupling, the value for money is well worth it&lt;/p&gt; &lt;p&gt;- The &amp;quot;Agent Benchmarks&amp;quot; are definitely not accurate, Sonnet 4 is NATIVELY much better in almost every AI Coding tool&lt;/p&gt; &lt;p&gt;- I'm still going to test K2 in Qwen Coder and maybe a custom coding tool, but it's a very good coder&lt;/p&gt; &lt;p&gt;- K2 is better than Gemini 2.5 Pro in tool calling, according to me&lt;/p&gt; &lt;p&gt;- Currently, the best implementation of K2 I found is in Windsurf (I tested VSCode, Cline, Windsurf and RooCode)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marvijo-software"&gt; /u/marvijo-software &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdldom/kimi_k2_vs_claude_4_sonnet_unexpected_review/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdldom/kimi_k2_vs_claude_4_sonnet_unexpected_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdldom/kimi_k2_vs_claude_4_sonnet_unexpected_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T23:03:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1md6xba</id>
    <title>Skywork/Skywork-UniPic-1.5B - A unified autoregressive multimodal model</title>
    <updated>2025-07-30T13:41:50+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md6xba/skyworkskyworkunipic15b_a_unified_autoregressive/"&gt; &lt;img alt="Skywork/Skywork-UniPic-1.5B - A unified autoregressive multimodal model" src="https://external-preview.redd.it/NU1es84U5dKcUVq65hYCHqeHpunplTrqbG-pwQUy3MM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0deccacf66db69a58065546ea92e99fae0e3c4d6" title="Skywork/Skywork-UniPic-1.5B - A unified autoregressive multimodal model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-UniPic-1.5B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md6xba/skyworkskyworkunipic15b_a_unified_autoregressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md6xba/skyworkskyworkunipic15b_a_unified_autoregressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T13:41:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdj5ww</id>
    <title>Complete Mistral Coding Stack but for enterprise only</title>
    <updated>2025-07-30T21:32:29+00:00</updated>
    <author>
      <name>/u/Edereum</name>
      <uri>https://old.reddit.com/user/Edereum</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://mistral.ai/news/codestral-25-08"&gt;https://mistral.ai/news/codestral-25-08&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mistral just release new version of codestral &amp;amp; entire coding stack.&lt;/p&gt; &lt;p&gt;but god.. for enterprise only.. the heck ? don't understand the move of blocking usual coder &amp;amp; shadow it ^^'&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Edereum"&gt; /u/Edereum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdj5ww/complete_mistral_coding_stack_but_for_enterprise/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdj5ww/complete_mistral_coding_stack_but_for_enterprise/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdj5ww/complete_mistral_coding_stack_but_for_enterprise/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T21:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdgeww</id>
    <title>AutoRL "vibe-training" for open models</title>
    <updated>2025-07-30T19:44:26+00:00</updated>
    <author>
      <name>/u/arctic_fly</name>
      <uri>https://old.reddit.com/user/arctic_fly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;📈 Introducing &lt;a href="https://github.com/OpenPipe/ART/tree/auto-rl?tab=readme-ov-file#-autorl-train-models-for-any-task"&gt;AutoRL&lt;/a&gt;, simple architecture for specializing Qwen and other OSS models for any task.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technique breakdown:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;User defines task&lt;/li&gt; &lt;li&gt;AutoRL generates 30 sample scenarios for which agent must perform task&lt;/li&gt; &lt;li&gt;Agent runs through 25 training samples using GRPO to improve for specified number of epochs&lt;/li&gt; &lt;li&gt;Agent is tested on remaining 5 test samples against SOTA models (like Sonnet 4)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Built on top of OpenPipe's &lt;a href="https://github.com/OpenPipe/ART/tree/auto-rl"&gt;ART&lt;/a&gt; and uses &lt;a href="https://art.openpipe.ai/fundamentals/ruler"&gt;RULER&lt;/a&gt; as its reward function. It's quite easy to get started with.&lt;/p&gt; &lt;p&gt;Sample Colab notebook: &lt;a href="https://colab.research.google.com/github/openpipe/art/blob/main/examples/auto_rl.ipynb"&gt;https://colab.research.google.com/github/openpipe/art/blob/main/examples/auto_rl.ipynb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arctic_fly"&gt; /u/arctic_fly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdgeww/autorl_vibetraining_for_open_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdgeww/autorl_vibetraining_for_open_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdgeww/autorl_vibetraining_for_open_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T19:44:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1md5k8f</id>
    <title>GLM4.5 EQ-Bench and Creative Write</title>
    <updated>2025-07-30T12:42:02+00:00</updated>
    <author>
      <name>/u/pcdacks</name>
      <uri>https://old.reddit.com/user/pcdacks</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md5k8f/glm45_eqbench_and_creative_write/"&gt; &lt;img alt="GLM4.5 EQ-Bench and Creative Write" src="https://preview.redd.it/ubwsl0gdb0gf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abdf15ff9928a1e321306852523e66da9ac4b1cf" title="GLM4.5 EQ-Bench and Creative Write" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pcdacks"&gt; /u/pcdacks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ubwsl0gdb0gf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md5k8f/glm45_eqbench_and_creative_write/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md5k8f/glm45_eqbench_and_creative_write/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T12:42:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdfkly</id>
    <title>Is "Personal Superintelligence" really personal if it is not local like a personal device?</title>
    <updated>2025-07-30T19:12:15+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.meta.com/superintelligence/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdfkly/is_personal_superintelligence_really_personal_if/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdfkly/is_personal_superintelligence_really_personal_if/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T19:12:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdbm5t</id>
    <title>Eigent – Open Source, Local-First Multi-Agent Workforce</title>
    <updated>2025-07-30T16:44:03+00:00</updated>
    <author>
      <name>/u/FitHeron1933</name>
      <uri>https://old.reddit.com/user/FitHeron1933</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdbm5t/eigent_open_source_localfirst_multiagent_workforce/"&gt; &lt;img alt="Eigent – Open Source, Local-First Multi-Agent Workforce" src="https://b.thumbs.redditmedia.com/3RqGcIVLHN9WLuz3G6pO2CwDYLwlqMU3O2iTSdwHGzY.jpg" title="Eigent – Open Source, Local-First Multi-Agent Workforce" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just launched &lt;strong&gt;Eigent,&lt;/strong&gt; a fully open-source, local-first multi-agent desktop application designed for developers and teams who want full control over their AI workflows.&lt;br /&gt; Built on top of CAMEL-AI’s modular framework, Eigent allows you to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Run tasks in parallel with customizable agent workflows&lt;/li&gt; &lt;li&gt;Deploy locally or in the cloud with “Bring Your Own Key” (BYOK) support&lt;/li&gt; &lt;li&gt;Maintain full data privacy — no information leaves your machine&lt;/li&gt; &lt;li&gt;Step in anytime with Human-in-the-Loop control&lt;/li&gt; &lt;li&gt;Integrate seamlessly with your existing stack&lt;/li&gt; &lt;li&gt;Use 200+ MCP-compatible tools (or bring your own)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The goal is simple: give teams a secure, customizable, and scalable AI workforce on their own infrastructure.&lt;br /&gt; → GitHub: &lt;a href="http://github.com/eigent-ai/eigent"&gt;github.com/eigent-ai/eigent&lt;/a&gt;&lt;br /&gt; → Download: &lt;a href="http://www.eigent.ai/"&gt;eigent.ai &lt;/a&gt;&lt;br /&gt; Feel free to ask me anything below, whether it’s about the architecture, use cases, or how to extend it for your own needs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FitHeron1933"&gt; /u/FitHeron1933 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mdbm5t"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdbm5t/eigent_open_source_localfirst_multiagent_workforce/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdbm5t/eigent_open_source_localfirst_multiagent_workforce/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T16:44:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1md8rxu</id>
    <title>Qwen/Qwen3-30B-A3B-Thinking-2507 · Hugging Face</title>
    <updated>2025-07-30T14:56:12+00:00</updated>
    <author>
      <name>/u/MariusNocturnum</name>
      <uri>https://old.reddit.com/user/MariusNocturnum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md8rxu/qwenqwen330ba3bthinking2507_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-30B-A3B-Thinking-2507 · Hugging Face" src="https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd872c4c3958b52ad860a6db5ba53994da65552e" title="Qwen/Qwen3-30B-A3B-Thinking-2507 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MariusNocturnum"&gt; /u/MariusNocturnum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md8rxu/qwenqwen330ba3bthinking2507_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md8rxu/qwenqwen330ba3bthinking2507_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T14:56:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdn6dp</id>
    <title>Deepseek just won the best paper award at ACL 2025 with a breakthrough innovation in long context, a model using this might come soon</title>
    <updated>2025-07-31T00:23:44+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.11089"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdn6dp/deepseek_just_won_the_best_paper_award_at_acl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdn6dp/deepseek_just_won_the_best_paper_award_at_acl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T00:23:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdhfhs</id>
    <title>glm-4.5-Air appreciation poist - if you have not done so already, give this model a try</title>
    <updated>2025-07-30T20:24:01+00:00</updated>
    <author>
      <name>/u/Southern_Sun_2106</name>
      <uri>https://old.reddit.com/user/Southern_Sun_2106</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. It has been an awesomely-busy week for all of us here, trying out the new goodies that dropped by Qwen and others. Wow, this week will be hard to match, good times!&lt;/p&gt; &lt;p&gt;Like most here, I ended up trying a bunch of models in bunch of quants plus mlx.&lt;/p&gt; &lt;p&gt;I have to say, the model that completely blew my mind was glm-4.5-air, the 4-bit mlx. I plugged it into my assistant (that does chains of tools, plus connected to a project management app, plus to a notebook), and it immediately figured out how to use those.&lt;/p&gt; &lt;p&gt;It really likes to dig through tasks, priorities, notes, online research - to the point when I am worried it's going to do it too much and loose track of things - but amazingly enough, it doesn't loose track of things and comes back with in-depth, good analysis and responses.&lt;/p&gt; &lt;p&gt;The model is also fast - kind of reminds me of Owen 30b a3b, although of course it punches well above that one due to its larger size.&lt;/p&gt; &lt;p&gt;If you can fit the 4-bit version onto your machine, absolutely, give this model a try. It is now my new daily driver, replacing Qwen 32B (until the new Qwen 32B comes out later this week? lol)&lt;/p&gt; &lt;p&gt;edit: I am not associated with the gml team (I wish I was!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Southern_Sun_2106"&gt; /u/Southern_Sun_2106 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdhfhs/glm45air_appreciation_poist_if_you_have_not_done/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdhfhs/glm45air_appreciation_poist_if_you_have_not_done/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdhfhs/glm45air_appreciation_poist_if_you_have_not_done/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T20:24:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdjb67</id>
    <title>After 6 months of fiddling with local AI. Here’s my curated models list that work for 90% of my needs. What’s yours?</title>
    <updated>2025-07-30T21:38:07+00:00</updated>
    <author>
      <name>/u/simracerman</name>
      <uri>https://old.reddit.com/user/simracerman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/"&gt; &lt;img alt="After 6 months of fiddling with local AI. Here’s my curated models list that work for 90% of my needs. What’s yours?" src="https://preview.redd.it/jzljyi4tw2gf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=387253ef4ef3e3a18ba79c1be71339080caaaf1c" title="After 6 months of fiddling with local AI. Here’s my curated models list that work for 90% of my needs. What’s yours?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All models are from Unsloth UD Q4_K_XL except for Gemma3-27B is IQ3. Running all these with 10-12k context with 4-30 t/s across all models.&lt;/p&gt; &lt;p&gt;Most used ones are Mistral-24B, Gemma3-27B, and Granite3.3-2B. Mistral and Gemma are for general QA and random text tools. Granite is for article summaries and random small RAG related tasks. Qwen3-30B (new one) is for coding related tasks, and Gemma3-12B is for vision strictly.&lt;/p&gt; &lt;p&gt;Gemma3n-2B is essentially hooked to Siri via shortcuts and acts as an enhanced Siri.&lt;/p&gt; &lt;p&gt;Medgemma is for anything medical and it’s wonderful for any general advice and reading of x-rays or medical reports.&lt;/p&gt; &lt;p&gt;My humble mini PC runs all these on Llama.cpp with iGPU 48GB shared memory RAM and Vulkan backend. It runs Mistral at 4t/s with 6k context (set to max of 10k window). Gemme3-27B runs at 5t/s, and Qwen3-30B-A3B at 20-22t/s.&lt;/p&gt; &lt;p&gt;I fall back to ChatGPT once or twice a week when i need a super quick answer or something too in depth.&lt;/p&gt; &lt;p&gt;What is your curated list?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simracerman"&gt; /u/simracerman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jzljyi4tw2gf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T21:38:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1md8slx</id>
    <title>Qwen3-30b-a3b-thinking-2507 This is insane performance</title>
    <updated>2025-07-30T14:56:57+00:00</updated>
    <author>
      <name>/u/3oclockam</name>
      <uri>https://old.reddit.com/user/3oclockam</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md8slx/qwen330ba3bthinking2507_this_is_insane_performance/"&gt; &lt;img alt="Qwen3-30b-a3b-thinking-2507 This is insane performance" src="https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd872c4c3958b52ad860a6db5ba53994da65552e" title="Qwen3-30b-a3b-thinking-2507 This is insane performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On par with qwen3-235b?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3oclockam"&gt; /u/3oclockam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md8slx/qwen330ba3bthinking2507_this_is_insane_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md8slx/qwen330ba3bthinking2507_this_is_insane_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T14:56:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1md8t1g</id>
    <title>🚀 Qwen3-30B-A3B-Thinking-2507</title>
    <updated>2025-07-30T14:57:27+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md8t1g/qwen330ba3bthinking2507/"&gt; &lt;img alt="🚀 Qwen3-30B-A3B-Thinking-2507" src="https://preview.redd.it/eaag1cpuz0gf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e073b4b20cd702585ec6bbac8fc80938677c24f8" title="🚀 Qwen3-30B-A3B-Thinking-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 Qwen3-30B-A3B-Thinking-2507, a medium-size model that can think!&lt;/p&gt; &lt;p&gt;• Nice performance on reasoning tasks, including math, science, code &amp;amp; beyond • Good at tool use, competitive with larger models • Native support of 256K-token context, extendable to 1M&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model scope: &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Thinking-2507/summary"&gt;https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Thinking-2507/summary&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eaag1cpuz0gf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md8t1g/qwen330ba3bthinking2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md8t1g/qwen330ba3bthinking2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T14:57:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1md93bj</id>
    <title>Qwen3 Coder 30B-A3B tomorrow!!!</title>
    <updated>2025-07-30T15:08:26+00:00</updated>
    <author>
      <name>/u/R46H4V</name>
      <uri>https://old.reddit.com/user/R46H4V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md93bj/qwen3_coder_30ba3b_tomorrow/"&gt; &lt;img alt="Qwen3 Coder 30B-A3B tomorrow!!!" src="https://preview.redd.it/zv92612t11gf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45b98263f660ff1bebd4634907371461fd4e0207" title="Qwen3 Coder 30B-A3B tomorrow!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R46H4V"&gt; /u/R46H4V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zv92612t11gf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md93bj/qwen3_coder_30ba3b_tomorrow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md93bj/qwen3_coder_30ba3b_tomorrow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T15:08:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1md6t2h</id>
    <title>Bye bye, Meta AI, it was good while it lasted.</title>
    <updated>2025-07-30T13:36:51+00:00</updated>
    <author>
      <name>/u/absolooot1</name>
      <uri>https://old.reddit.com/user/absolooot1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Zuck has posted a video and a longer letter about the superintelligence plans at Meta. In the letter he says:&lt;/p&gt; &lt;p&gt;&amp;quot;That said, superintelligence will raise novel safety concerns. We'll need to be rigorous about mitigating these risks and careful about what we choose to open source.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.meta.com/superintelligence/"&gt;https://www.meta.com/superintelligence/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That means that Meta will not open source the best they have. But it is inevitable that others will release their best models and agents, meaning that Meta has committed itself to oblivion, not only in open source but in proprietary too, as they are not a major player in that space. The ASI they will get to will be for use in their products only.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/absolooot1"&gt; /u/absolooot1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md6t2h/bye_bye_meta_ai_it_was_good_while_it_lasted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md6t2h/bye_bye_meta_ai_it_was_good_while_it_lasted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md6t2h/bye_bye_meta_ai_it_was_good_while_it_lasted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T13:36:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdmsu9</id>
    <title>Chinese models pulling away</title>
    <updated>2025-07-31T00:06:15+00:00</updated>
    <author>
      <name>/u/Kniffliger_Kiffer</name>
      <uri>https://old.reddit.com/user/Kniffliger_Kiffer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdmsu9/chinese_models_pulling_away/"&gt; &lt;img alt="Chinese models pulling away" src="https://preview.redd.it/727keqreo3gf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=638ce7aed31fa426f1cfea7678c6d9169932f5a9" title="Chinese models pulling away" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kniffliger_Kiffer"&gt; /u/Kniffliger_Kiffer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/727keqreo3gf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdmsu9/chinese_models_pulling_away/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdmsu9/chinese_models_pulling_away/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T00:06:15+00:00</published>
  </entry>
</feed>
