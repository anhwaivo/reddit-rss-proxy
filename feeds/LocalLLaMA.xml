<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-14T04:26:10+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mpmhxj</id>
    <title>Docling: Great quality, but painfully slow</title>
    <updated>2025-08-14T01:07:34+00:00</updated>
    <author>
      <name>/u/Cyp9715</name>
      <uri>https://old.reddit.com/user/Cyp9715</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Docling for a while now, and I have to say the output quality is good. It really nails accuracy and formatting in a way that most other tools don't.&lt;/p&gt; &lt;p&gt;The problem? Speed. It's painfully slow. Even relatively small documents take far longer than I’d expect, and without a GPU, large ones can take over an hour. I get that good processing takes time, but in my case, the wait often outweighs the benefits.&lt;/p&gt; &lt;p&gt;Is anyone else experiencing the same thing? Have you found any tweaks, settings, or workarounds that make Docling faster without sacrificing quality?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cyp9715"&gt; /u/Cyp9715 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpmhxj/docling_great_quality_but_painfully_slow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpmhxj/docling_great_quality_but_painfully_slow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpmhxj/docling_great_quality_but_painfully_slow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T01:07:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp1ras</id>
    <title>[Beta] Local TTS Studio with Kokoro, Kitten TTS, and Piper built in, completely in JavaScript (930+ voices to choose from)</title>
    <updated>2025-08-13T11:24:08+00:00</updated>
    <author>
      <name>/u/CommunityTough1</name>
      <uri>https://old.reddit.com/user/CommunityTough1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! Last week, &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mi45h1/kitten_tts_web_demo/"&gt;I posted&lt;/a&gt; a Kitten TTS web demo that it seemed like a lot of people liked, so I decided to take it a step further and add Piper and Kokoro to the project! The project lets you load Kitten TTS, Piper Voices, or Kokoro completely in the browser, 100% local. It also has a quick preview feature in the voice selection dropdowns.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://clowerweb.github.io/tts-studio/"&gt;&lt;strong&gt;Online Demo&lt;/strong&gt;&lt;/a&gt; (GitHub Pages)&lt;/h1&gt; &lt;p&gt;Repo (Apache 2.0): &lt;a href="https://github.com/clowerweb/tts-studio"&gt;https://github.com/clowerweb/tts-studio&lt;/a&gt;&lt;br /&gt; One-liner Docker installer: &lt;code&gt;docker pull ghcr.io/clowerweb/tts-studio:latest&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The &lt;a href="https://clowerweb.github.io/kitten-tts-web-demo/"&gt;Kitten TTS standalone&lt;/a&gt; was also updated to include a bunch of your feedback including bug fixes and requested features! There's also a &lt;a href="https://clowerweb.github.io/piper-tts-web-demo/"&gt;Piper standalone&lt;/a&gt; available.&lt;/p&gt; &lt;p&gt;Lemme know what you think and if you've got any feedback or suggestions!&lt;/p&gt; &lt;p&gt;If this project helps you save a few GPU hours, please consider &lt;a href="https://github.com/sponsors/clowerweb"&gt;grabbing me a coffee!&lt;/a&gt; ☕&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CommunityTough1"&gt; /u/CommunityTough1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1ras/beta_local_tts_studio_with_kokoro_kitten_tts_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1ras/beta_local_tts_studio_with_kokoro_kitten_tts_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1ras/beta_local_tts_studio_with_kokoro_kitten_tts_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T11:24:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpb0mo</id>
    <title>Dual GPU Setup for LLMs – Notes from a Newbie</title>
    <updated>2025-08-13T17:36:06+00:00</updated>
    <author>
      <name>/u/DrRamorey</name>
      <uri>https://old.reddit.com/user/DrRamorey</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some learnings I made the hard way. These points might be obvious to some, but I wasn’t fully aware of them before I built my LLM workstation. Hopefully this helps other newbies like me.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;I was using my AMD RX 6800 mostly for LLM workloads and wanted more VRAM to test larger models. I built a PC to accommodate two GPUs for this use case.&lt;br /&gt; The plan was to use my RX 6800 plus a newer GPU. I knew it should be an AMD card, and the RX 9070 XT seemed like the best value.&lt;br /&gt; I’m still an amateur with LLMs—mostly using them in LM Studio—but I’ve started experimenting with dedicated servers and Docker setups.&lt;/p&gt; &lt;h1&gt;Learning 1 - You can’t assume gaming benchmarks reflect LLM performance&lt;/h1&gt; &lt;p&gt;Standard benchmarks like 3DMark, Heaven, or Superposition showed my new 9070 XT was &lt;strong&gt;51–64% faster&lt;/strong&gt; than my old card. I kind of expected similar gains in LLM performance.&lt;/p&gt; &lt;p&gt;That was clearly not the case. Here are my &lt;code&gt;llama-bench&lt;/code&gt; results (ROCm):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-bench -m gemma-3-12B-it-qat-GGUF/gemma-3-12B-it-QAT-Q4_0.gguf -mg 0,1 -sm none ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 2 ROCm devices: Device 0: AMD Radeon RX 9070 XT, gfx1201 (0x1201), VMM: no, Wave Size: 32 Device 1: AMD Radeon RX 6800, gfx1030 (0x1030), VMM: no, Wave Size: 32 &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;main_gpu&lt;/th&gt; &lt;th align="left"&gt;sm&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;none&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;1420.48 ± 4.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;none&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;47.74 ± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;none&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;947.02 ± 0.82&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;none&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;43.23 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The 9070 XT is 50&lt;strong&gt;% faster&lt;/strong&gt; in prompt parsing, but only &lt;strong&gt;10% faster&lt;/strong&gt; in token generation.&lt;br /&gt; That was really disappointing.&lt;/p&gt; &lt;p&gt;It’s a different picture for image generation (results below are generation times in seconds; lower is better).&lt;br /&gt; As this isn’t my main interest, I only did very basic testing. ComfyUI had some weird issues with the 9070 XT but worked flawlessly with the RX 6800 (as of July 2025).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Task&lt;/th&gt; &lt;th align="left"&gt;RX 6800&lt;/th&gt; &lt;th align="left"&gt;RX 9070 XT&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Stable Diffusion 3.5 simple 1024x1024&lt;/td&gt; &lt;td align="left"&gt;115&lt;/td&gt; &lt;td align="left"&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SDXL simple 1024x1024&lt;/td&gt; &lt;td align="left"&gt;25&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Flux schnell 1024x1024&lt;/td&gt; &lt;td align="left"&gt;38&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Flux checkpoint 1024x1024&lt;/td&gt; &lt;td align="left"&gt;171&lt;/td&gt; &lt;td align="left"&gt;61&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;As my 9070 XT was also &lt;strong&gt;way too loud&lt;/strong&gt;, I returned it and picked up a second-hand RX 6800 XT. It’s only slightly faster than my old card, but &lt;strong&gt;€450 cheaper&lt;/strong&gt; than the 9070 XT.&lt;/p&gt; &lt;p&gt;Lesson: ignore standard gaming benchmarks when choosing a GPU for LLMs.&lt;br /&gt; Check LLM-specific benchmark lists like &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/10879"&gt;https://github.com/ggml-org/llama.cpp/discussions/10879&lt;/a&gt; and pick a GPU that matches your existing one if you’re not going for identical models.&lt;/p&gt; &lt;h1&gt;Learning 2 - Two GPUs do not double LLM token generation performance&lt;/h1&gt; &lt;p&gt;This should be obvious, but I never really thought about it and assumed overall performance would scale with two GPUs.&lt;/p&gt; &lt;p&gt;Wrong again.&lt;br /&gt; The main benefit of the second GPU is &lt;strong&gt;extra VRAM&lt;/strong&gt;. Larger models can be split across both GPUs—but performance is actually worse than with a single card (see next point).&lt;/p&gt; &lt;h1&gt;Learning 3 - Splitting models across two GPUs can tank performance&lt;/h1&gt; &lt;p&gt;Using the same model as before, now with the RX 6800 XT:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-bench -m gemma-3-12B-it-qat-GGUF/gemma-3-12B-it-QAT-Q4_0.gguf -mg 0,1 -sm none` ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 2 ROCm devices: Device 0: AMD Radeon RX 6800 XT, gfx1030 (0x1030), VMM: no, Wave Size: 32 Device 1: AMD Radeon RX 6800, gfx1030 (0x1030), VMM: no, Wave Size: 32 &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;main_gpu&lt;/th&gt; &lt;th align="left"&gt;sm&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;none&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;1070.86 ± 2.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;none&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;44.78 ± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;none&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;875.96 ± 1.29&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;none&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;43.25 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The 6800 XT is only &lt;strong&gt;3% faster&lt;/strong&gt; than my old RX 6800 in token generation.&lt;/p&gt; &lt;p&gt;Now splitting the model across both cards:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-bench -m gemma-3-12B-it-qat-GGUF/gemma-3-12B-it-QAT-Q4_0.gguf -mg 0,1 ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 2 ROCm devices: Device 0: AMD Radeon RX 6800 XT, gfx1030 (0x1030), VMM: no, Wave Size: 32 Device 1: AMD Radeon RX 6800, gfx1030 (0x1030), VMM: no, Wave Size: 32 &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;main_gpu&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;964.56 ± 2.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;31.92 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;962.75 ± 1.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;31.90 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Interpretation (my guess):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt parsing (&lt;code&gt;pp512&lt;/code&gt;) seems truly parallelized—the performance is roughly the average of both cards.&lt;/li&gt; &lt;li&gt;Token generation (&lt;code&gt;tg128&lt;/code&gt;) is slower than a single card. This makes sense: both cards must work in sync, so there’s extra overhead—likely from synchronization and maybe PCIe bandwidth limits.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In my case, splitting gave me &lt;strong&gt;26% lower token generation speed&lt;/strong&gt; compared to my slowest card.&lt;br /&gt; The upside: I now have 32GB of VRAM for bigger models.&lt;/p&gt; &lt;h1&gt;Learning 3 - Consumer hardware (mainboard and case) pitfalls&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Mainboard&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You need &lt;strong&gt;two physical PCIe x16 slots&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;These must support &lt;strong&gt;lane splitting&lt;/strong&gt; (x16 → x8/x8). Some boards don’t support this or use weird splits (x8/x1). x8 speeds didn’t cause me performance issues (even in gaming), but x4 or x1 would likely bottleneck.&lt;/li&gt; &lt;li&gt;Slot spacing matters—many modern GPUs are 3+ slots thick, which can block the second slot.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Case&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I underestimated the heat and noise from two GPUs.&lt;/li&gt; &lt;li&gt;With one GPU, my airflow-oriented build was fine.&lt;/li&gt; &lt;li&gt;Adding the second GPU was a &lt;strong&gt;massive change&lt;/strong&gt;—temps spiked and noise went from “barely there” to “annoying constant GPU roar.”&lt;/li&gt; &lt;li&gt;If your build sits on your desk, fan performance and possibly sound-dampening panels become very important.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m still learning, and most of this is based on my own trial and error.&lt;br /&gt; If I’ve misunderstood something, overlooked a better method, or drawn the wrong conclusions, I’d appreciate corrections.&lt;br /&gt; Feel free to share your own benchmarks, tweaks, or experiences so others (including me) can learn from them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrRamorey"&gt; /u/DrRamorey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpb0mo/dual_gpu_setup_for_llms_notes_from_a_newbie/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpb0mo/dual_gpu_setup_for_llms_notes_from_a_newbie/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpb0mo/dual_gpu_setup_for_llms_notes_from_a_newbie/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T17:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp4vxe</id>
    <title>Plant UML Generator LLM finetune</title>
    <updated>2025-08-13T13:44:53+00:00</updated>
    <author>
      <name>/u/lolzinventor</name>
      <uri>https://old.reddit.com/user/lolzinventor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4vxe/plant_uml_generator_llm_finetune/"&gt; &lt;img alt="Plant UML Generator LLM finetune" src="https://external-preview.redd.it/A2vjjq6KSpHPh1Yu7sh0j0dqmFW8S4lQdPeKAxnYi1A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ffbbaa449ca9e4ad6cfa5fac6ce6ffc23c3275b" title="Plant UML Generator LLM finetune" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Introducing pumlGenV2-1: The AI That Visualizes Complex Ideas as PlantUML Diagrams&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;What It Does&lt;/h1&gt; &lt;p&gt;Give it a complex question—whether about &lt;strong&gt;architecture&lt;/strong&gt;, &lt;strong&gt;philosophical debates&lt;/strong&gt;, or &lt;strong&gt;historical events&lt;/strong&gt;—and it generates a structured PlantUML diagram with (all input text is treated as a question):&lt;br /&gt; ✔ &lt;strong&gt;Logical relationships&lt;/strong&gt; between concepts&lt;br /&gt; ✔ &lt;strong&gt;Hierarchical grouping&lt;/strong&gt; (packages, components)&lt;br /&gt; ✔ &lt;strong&gt;Annotations &amp;amp; notes&lt;/strong&gt; for clarity&lt;br /&gt; ✔ &lt;strong&gt;Color-coding &amp;amp; legends&lt;/strong&gt; for readability&lt;/p&gt; &lt;h1&gt;Examples&lt;/h1&gt; &lt;h1&gt;1️⃣ Philosophy Made Visual&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; &lt;em&gt;&amp;quot;Can AI achieve true understanding, or is it just pattern recognition?&amp;quot;&lt;/em&gt;&lt;br /&gt; &lt;strong&gt;Output:&lt;/strong&gt;&lt;br /&gt; Philosophy Diagram&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Maps consciousness, semantics, and computational limits&lt;/li&gt; &lt;li&gt;Contrasts human vs. machine cognition&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2️⃣ Technical Architecture in Seconds&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; &lt;em&gt;&amp;quot;Diagram a microservices e-commerce system with Kubernetes, Kafka, and Istio.&amp;quot;&lt;/em&gt;&lt;br /&gt; &lt;strong&gt;Output:&lt;/strong&gt;&lt;br /&gt; Microservices Diagram&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Shows frontend/backend/services&lt;/li&gt; &lt;li&gt;Visualizes async flows (RabbitMQ, Kafka)&lt;/li&gt; &lt;li&gt;Includes security (mTLS, Istio)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-domain&lt;/strong&gt;: Works for tech, philosophy, history, law&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Precise syntax&lt;/strong&gt;: Generates valid PlantUML code&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context-aware&lt;/strong&gt;: Adds relevant notes and structure&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Limitations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Best for analytical questions (not narratives/stories)&lt;/li&gt; &lt;li&gt;Output may need minor tweaks for custom styling&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lolzinventor"&gt; /u/lolzinventor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/chrisrutherford/pumlGenV2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4vxe/plant_uml_generator_llm_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4vxe/plant_uml_generator_llm_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T13:44:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpgkap</id>
    <title>Why it’s a mistake to ask chatbots about their mistakes</title>
    <updated>2025-08-13T21:02:24+00:00</updated>
    <author>
      <name>/u/Educational_Sun_8813</name>
      <uri>https://old.reddit.com/user/Educational_Sun_8813</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;The tendency to ask AI bots to explain themselves reveals widespread misconceptions about how they work.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arstechnica.com/ai/2025/08/why-its-a-mistake-to-ask-chatbots-about-their-mistakes/"&gt;https://arstechnica.com/ai/2025/08/why-its-a-mistake-to-ask-chatbots-about-their-mistakes/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Sun_8813"&gt; /u/Educational_Sun_8813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpgkap/why_its_a_mistake_to_ask_chatbots_about_their/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpgkap/why_its_a_mistake_to_ask_chatbots_about_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpgkap/why_its_a_mistake_to_ask_chatbots_about_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T21:02:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpm728</id>
    <title>AMD Radeon RX 480 8GB benchmark</title>
    <updated>2025-08-14T00:53:50+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I finally got around to testing my&lt;a href="https://www.techpowerup.com/gpu-specs/radeon-rx-480.c2848"&gt; RX 480 8GB&lt;/a&gt; card with &lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b6152"&gt;latest&lt;/a&gt; llama.cpp &lt;a href="https://github.com/ggml-org/llama.cpp/releases/download/b6152/llama-b6152-bin-ubuntu-vulkan-x64.zip"&gt;Vulkan&lt;/a&gt; on Kubuntu. Just download, unzipped and for each model ran:&lt;/p&gt; &lt;p&gt;&lt;code&gt;time ./llama-bench --model /home/user33/Downloads/&lt;/code&gt;&lt;em&gt;models_to_test.guff&lt;/em&gt;&lt;/p&gt; &lt;p&gt;This is the full command and output for mistral-7b benchmark&lt;/p&gt; &lt;p&gt;&lt;strong&gt;time ./llama-bench --model /home/user33/Downloads/mistral-7b-v0.1.Q4_K_M.gguf&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;load_backend: loaded RPC backend from /home/user33/Downloads/build/bin/libggml-rpc.so&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ggml_vulkan: Found 1 Vulkan devices:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ggml_vulkan: 0 = AMD Radeon RX 480 Graphics (RADV POLARIS10) (radv) | uma: 0 | fp16: 0 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 0 | matrix cores: none&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;load_backend: loaded Vulkan backend from /home/user33/Downloads/build/bin/libggml-vulkan.so&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;load_backend: loaded CPU backend from /home/userr33/Downloads/build/bin/libggml-cpu-haswell.so&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| llama 7B Q4_K - Medium | 4.07 GiB | 7.24 B | RPC,Vulkan | 99 | pp512 | 181.60 ± 0.84 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| llama 7B Q4_K - Medium | 4.07 GiB | 7.24 B | RPC,Vulkan | 99 | tg128 | 31.71 ± 0.13 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Here are 6 popular 7B size model.&lt;/p&gt; &lt;p&gt;backend for all models: RPC,Vulkan&lt;/p&gt; &lt;p&gt;ngl for all models: 99&lt;/p&gt; &lt;p&gt;| model | size | test | t/s |&lt;br /&gt; | ------------------------------ | ---------: | --------------: | -------------------: |&lt;br /&gt; | llama 7B Q4_K - Medium | 4.07 GiB | pp512 | 181.60 ± 0.84 |&lt;br /&gt; | llama 7B Q4_K - Medium | 4.07 GiB | tg128 | 31.71 ± 0.13 |&lt;br /&gt; | falcon-h1 7B Q4_K - Medium | 4.28 GiB | pp512 | 104.07 ± 0.73 |&lt;br /&gt; | falcon-h1 7B Q4_K - Medium | 4.28 GiB | tg128 | 7.61 ± 0.04 |&lt;br /&gt; | qwen2 7B Q5_K - Medium | 5.07 GiB | pp512 | 191.89 ± 0.84 |&lt;br /&gt; | qwen2 7B Q5_K - Medium | 5.07 GiB | tg128 | 26.29 ± 0.07 |&lt;br /&gt; | llama 8B Q4_K - Medium | 4.58 GiB | pp512 | 183.17 ± 1.18 |&lt;br /&gt; | llama 8B Q4_K - Medium | 4.58 GiB | tg128 | 29.93 ± 0.10 |&lt;br /&gt; | qwen3 8B Q4_K - Medium | 4.68 GiB | pp512 | 179.43 ± 0.56 |&lt;br /&gt; | qwen3 8B Q4_K - Medium | 4.68 GiB | tg128 | 28.96 ± 0.07 |&lt;br /&gt; | gemma 7B Q4_K - Medium | 4.96 GiB | pp512 | 157.71 ± 0.53 |&lt;br /&gt; | gemma 7B Q4_K - Medium | 4.96 GiB | tg128 | 27.16 ± 0.03 |&lt;/p&gt; &lt;p&gt;Not bad, getting about 30 t/s eval rate. It is about 10% slower than my &lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1070.c2840"&gt;GTX-1070&lt;/a&gt; running CUDA. They both have a memory bandwidth of 256 GB/s. So Radeon Vulkan = Nvidia CUDA for older GPU. They are going for about $50 each on your favorite auction house. I paid about $75 for my GTX 1070 a few months back.&lt;/p&gt; &lt;p&gt;So the RX 470,480,570 and 580 are all capable GPU for gaming and AI on a budget.&lt;/p&gt; &lt;p&gt;Not sure what's is going on with falcon. It offloaded.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpm728/amd_radeon_rx_480_8gb_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpm728/amd_radeon_rx_480_8gb_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpm728/amd_radeon_rx_480_8gb_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T00:53:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpmeba</id>
    <title>Jan-v1 trial results follow-up and comparison to Qwen3, Perplexity, Claude</title>
    <updated>2025-08-14T01:03:04+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpmeba/janv1_trial_results_followup_and_comparison_to/"&gt; &lt;img alt="Jan-v1 trial results follow-up and comparison to Qwen3, Perplexity, Claude" src="https://b.thumbs.redditmedia.com/3X645e074wAZwFAUEsvdDQTEHHMP4k46jGqsiJfXi6w.jpg" title="Jan-v1 trial results follow-up and comparison to Qwen3, Perplexity, Claude" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following up to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mov3d9/i_tried_the_janv1_model_released_today_and_here/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;this post&lt;/a&gt; yesterday, here are the updated results using Q8 of the Jan V1 model with Serper search.&lt;/p&gt; &lt;p&gt;Summaries corresponding to each image:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Jan V1 Q8 with brave search: Actually produces an answer. But it gives the result for 2023.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Jan V1 Q8 with serper: Same result as above. It seems to make the mistake in the first thinking step in initiating the search - &amp;quot;Let me phrase the query as &amp;quot;US GDP current value&amp;quot; or something similar. Let me check the parameters: I need to specify a query. Let's go with &amp;quot;US GDP 2023 latest&amp;quot; to get recent data.&amp;quot; It thinks its way to the wrong query... &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Qwen3 A3B:30B via OpenRouter (with Msty's inbuilt web search): It had the right answer but then included numbers from 1999 and was far too verbose. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;GPT-OSS 20B via OpenRouter (with Msty's inbuilt web search): On the ball but a tad verbose&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Perplexity Pro: nailed it&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Claude Desktop w Sonnet 4: got it as well, but again more info than requested.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I didnt bother trying anything more.. Its harsh to jump to conclusions with just 1 question but its hard for me to see how Jan V1 is actually better than Perplexity or any other LLM+search tool &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpmeba"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpmeba/janv1_trial_results_followup_and_comparison_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpmeba/janv1_trial_results_followup_and_comparison_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T01:03:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpa1ew</id>
    <title>I am building a semantic file search engine using Qwen0.6b with recently released LangExtract tool!</title>
    <updated>2025-08-13T17:00:27+00:00</updated>
    <author>
      <name>/u/fuckAIbruhIhateCorps</name>
      <uri>https://old.reddit.com/user/fuckAIbruhIhateCorps</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpa1ew/i_am_building_a_semantic_file_search_engine_using/"&gt; &lt;img alt="I am building a semantic file search engine using Qwen0.6b with recently released LangExtract tool!" src="https://external-preview.redd.it/l26S5nt-U8KfOY_GWKDJSOzb_2xdSlDoF2CnOuQm95k.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d46f203692b0157fc900d760b8c60f96cb3a1eee" title="I am building a semantic file search engine using Qwen0.6b with recently released LangExtract tool!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;br /&gt; I am a long time lurker and quite active here on LocalLlama. I am starting to build a tool called 'monkeSearch'. Essentially an open source local file search engine where you can type english sentences or even broken keywords related to your file (typing like a &amp;quot;monke&amp;quot; essentially) and you get the closest matches listed, and without needing a beefy system in order to run this locally.&lt;br /&gt; Instead of remembering exact filenames or navigating through folder hierarchies, you can search with queries like &amp;quot;photos from wedding 3 weeks ago&amp;quot; or &amp;quot;resume pdf last month.&amp;quot; The whole tool is supposed to be offline, and aimed at running on potato pc too. &lt;/p&gt; &lt;p&gt;So basically, when you aim at searching a file, we have specific things in mind when we are given the freedom to search for stuff in semantic language, and we can segregate the input query in 3 domains: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Filetype: most deterministic thing we have in our mind while searching&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Temporal data (time related) for example: x days ago, x weeks ago, last x months&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Misc. data: for example file name, file path which can be anything.&lt;br /&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The idea had a lot of iterations, and I was planning to do all of the heavylifting without any ML at all, with just bare algorithms and pattern matching (yup i am crazy) &lt;/p&gt; &lt;p&gt;But a few days back, Google released LangExtract, and it was exactly what I could have dreamt of, and I started playing around with it.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;input query: &amp;quot;web dev code&amp;quot; LangExtract: Processing, current=12 chars, processed=12 chars: [00:00] ✓ Extraction processing complete ✓ Extracted 1 entities (1 unique types) • Time: 0.65s • Speed: 18 chars/sec • Chunks: 1 FILE TYPE INDICATORS: - 'web dev code' → ts, js, html &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As you can see above, I am using Qwen 0.6b running locally, and the base model performs surprisingly well (works perfectly in 90% of the cases). Finetuning it would result in better results. I have the dataset generation script ready for finetuning too. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lnvkw3sngtif1.png?width=4430&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=85aee20b6c0af82b04c83dcabe1d2dc20fa25a41"&gt;architecture planning &lt;/a&gt;&lt;/p&gt; &lt;p&gt;Okay so the above diagram covers the planning I did for the idea, and it has a lot of stuff to implement.&lt;br /&gt; I have already started working on the services locally. And it's a lot of work for one guy to do.&lt;br /&gt; &lt;a href="https://github.com/monkesearch"&gt;https://github.com/monkesearch&lt;/a&gt;&lt;br /&gt; This project has multiple service components and I'd love to collaborate with others interested in: &lt;/p&gt; &lt;p&gt;* NLP/semantic processing implementation&lt;/p&gt; &lt;p&gt;* Database optimization (currently planning SQLite)&lt;/p&gt; &lt;p&gt;* Frontend development for the query interface&lt;/p&gt; &lt;p&gt;* Testing!!&lt;/p&gt; &lt;p&gt;* Performance optimization for large file systems&lt;/p&gt; &lt;p&gt;If you're interested in contributing or have suggestions for the architecture, let's discuss below. I'm particularly interested in feedback on the semantic tagging approach and ideas for optimizing the overall system for real-time file processing.&lt;/p&gt; &lt;p&gt;Thanks for reading, and looking forward to building this with the community!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fuckAIbruhIhateCorps"&gt; /u/fuckAIbruhIhateCorps &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpa1ew/i_am_building_a_semantic_file_search_engine_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpa1ew/i_am_building_a_semantic_file_search_engine_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpa1ew/i_am_building_a_semantic_file_search_engine_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T17:00:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpmq7s</id>
    <title>Will the new 5070 Ti Super 24GB be local LLM new favourite 🙂</title>
    <updated>2025-08-14T01:18:01+00:00</updated>
    <author>
      <name>/u/Redinaj</name>
      <uri>https://old.reddit.com/user/Redinaj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just saw this updated leak on prices so Im wondering... Will 5070 ti Super 24GB be local LLM new favourite? &lt;a href="https://overclock3d.net/news/gpu-displays/nvidia-geforce-rtx-50-super-pricing-leaks/"&gt;https://overclock3d.net/news/gpu-displays/nvidia-geforce-rtx-50-super-pricing-leaks/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looks on par with 3090 for similar price used/new but newest tech will offer considerably more performance and future proofing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Redinaj"&gt; /u/Redinaj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpmq7s/will_the_new_5070_ti_super_24gb_be_local_llm_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpmq7s/will_the_new_5070_ti_super_24gb_be_local_llm_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpmq7s/will_the_new_5070_ti_super_24gb_be_local_llm_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T01:18:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp7cfe</id>
    <title>awesome-private-ai: all things for your AI data sovereign</title>
    <updated>2025-08-13T15:20:08+00:00</updated>
    <author>
      <name>/u/tdi</name>
      <uri>https://old.reddit.com/user/tdi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi just wanted to show - I have created this list. Been working on those topics recently and will be expanding it even more.&lt;br /&gt; &lt;a href="https://github.com/tdi/awesome-private-ai"&gt;https://github.com/tdi/awesome-private-ai&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tdi"&gt; /u/tdi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp7cfe/awesomeprivateai_all_things_for_your_ai_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp7cfe/awesomeprivateai_all_things_for_your_ai_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp7cfe/awesomeprivateai_all_things_for_your_ai_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T15:20:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mppqtu</id>
    <title>YAMS: Yet Another Memory System for LLM's</title>
    <updated>2025-08-14T03:41:54+00:00</updated>
    <author>
      <name>/u/blkmanta</name>
      <uri>https://old.reddit.com/user/blkmanta</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mppqtu/yams_yet_another_memory_system_for_llms/"&gt; &lt;img alt="YAMS: Yet Another Memory System for LLM's" src="https://external-preview.redd.it/Wdl3okAHvOXaOkYdjPCaNhixUPRpyzUYZxAYiri9Ewg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=365c61553d764b17eeff651fffa6fb624ced2120" title="YAMS: Yet Another Memory System for LLM's" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;|| || |Built this for my LLM workflows - needed searchable, persistent memory that wouldn't blow up storage costs. I also wanted to use it locally for my research. It's a content-addressed storage system with block-level deduplication (saves 30-40% on typical codebases). I have integrated the CLI tool into most of my workflows in Zed, Claude Code, and Cursor, and I provide the prompt I'm currently using in the repo. The project is in C++ and the build system is rough around the edges but is tested on macOS and Ubuntu 24.04.|&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blkmanta"&gt; /u/blkmanta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/trvon/yams"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mppqtu/yams_yet_another_memory_system_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mppqtu/yams_yet_another_memory_system_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T03:41:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpee0x</id>
    <title>GPT OSS 120b 34th on Simple bench, roughly on par with Llama 3.3 70b</title>
    <updated>2025-08-13T19:40:46+00:00</updated>
    <author>
      <name>/u/and_human</name>
      <uri>https://old.reddit.com/user/and_human</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/and_human"&gt; /u/and_human &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://simple-bench.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpee0x/gpt_oss_120b_34th_on_simple_bench_roughly_on_par/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpee0x/gpt_oss_120b_34th_on_simple_bench_roughly_on_par/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T19:40:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk834</id>
    <title>Testing qwen3-30b-a3b-q8_0 with my RTX Pro 6000 Blackwell MaxQ. Significant speed improvement. Around 120 t/s.</title>
    <updated>2025-08-13T23:27:12+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk834/testing_qwen330ba3bq8_0_with_my_rtx_pro_6000/"&gt; &lt;img alt="Testing qwen3-30b-a3b-q8_0 with my RTX Pro 6000 Blackwell MaxQ. Significant speed improvement. Around 120 t/s." src="https://external-preview.redd.it/dTd1ZDB2NnBldmlmMavIpB9AHSfqY-PSwwptUZpoVAt7ZMaVO0xQLghP-sG0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65f0830ecafdacba42b232e8ce354f02aa9a68f2" title="Testing qwen3-30b-a3b-q8_0 with my RTX Pro 6000 Blackwell MaxQ. Significant speed improvement. Around 120 t/s." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ycrl1u6pevif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk834/testing_qwen330ba3bq8_0_with_my_rtx_pro_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk834/testing_qwen330ba3bq8_0_with_my_rtx_pro_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:27:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp92nc</id>
    <title>Flash Attention massively accelerate gpt-oss-120b inference speed on Apple silicon</title>
    <updated>2025-08-13T16:24:53+00:00</updated>
    <author>
      <name>/u/DaniDubin</name>
      <uri>https://old.reddit.com/user/DaniDubin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share my observation and experience with gpt-oss-120b (unsloth/gpt-oss-120b-GGUF, F16).&lt;br /&gt; I am running it via LM Studio (latest v0.3.23), my hardware config is Mac Studio M4 Max (16c/40g) with 128GB of unified memory. &lt;/p&gt; &lt;p&gt;My main complaint against gpt-oss-120b was its inference speed, once the context window get filled up, it was dropping from 35-40 to 10-15 t/s when the context was around 15K only.&lt;/p&gt; &lt;p&gt;Now I noticed that by default Flash Attention is turned off. Once I turn it on via LM Studio model's configuration, I got ~50t/s with the context window at 15K, instead of the usual &amp;lt;15t/s.&lt;/p&gt; &lt;p&gt;Has anyone else tried to run this model with Flash Attention? Is there any trade-offs in model's accuracy? In my *very* limited testing I didn't notice any. I did not know that it can speed up so much the inference speed. I also noticed that Flash Attention is only available with GGUF quants, not on MLX.&lt;/p&gt; &lt;p&gt;Would like to hear your thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DaniDubin"&gt; /u/DaniDubin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp92nc/flash_attention_massively_accelerate_gptoss120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp92nc/flash_attention_massively_accelerate_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp92nc/flash_attention_massively_accelerate_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T16:24:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp6y0e</id>
    <title>Thankful to r/localllama, Swapped from Manus to a local setup</title>
    <updated>2025-08-13T15:04:36+00:00</updated>
    <author>
      <name>/u/Proof_Dog6506</name>
      <uri>https://old.reddit.com/user/Proof_Dog6506</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6y0e/thankful_to_rlocalllama_swapped_from_manus_to_a/"&gt; &lt;img alt="Thankful to r/localllama, Swapped from Manus to a local setup" src="https://preview.redd.it/0zp95auywsif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db6545a173cd1158bbe18fa4ac2296625343d76b" title="Thankful to r/localllama, Swapped from Manus to a local setup" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw a post here a while back about running multi‑agent setups locally. At the time I was still subbed to Manus and figured I'd just stick with what I knew.&lt;/p&gt; &lt;p&gt;Last week I decided to actually try it after seeing it mentioned again and… the OS community is fire tbh. Found an open‑source tool that runs entirely on my machine, does the same workflows (even better) I used Manus for, and I can tweak it however I want.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Before vs After:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Before: $40/month, cloud‑only, occasional downtime&lt;/li&gt; &lt;li&gt;After: $0, local‑first, tweakable, private, running with ollama and self‑hosted models, have full control of search (human in the loop)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Props to whoever originally posted about this, you might have just saved me a subscription. Massive thanks to LocalLLaMA for putting this on my radar. Here's the post I found that kicked this off for me: &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mdbm5t/eigent_open_source_localfirst_multiagent_workforce/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mdbm5t/eigent_open_source_localfirst_multiagent_workforce/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone else made the switch?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proof_Dog6506"&gt; /u/Proof_Dog6506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0zp95auywsif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6y0e/thankful_to_rlocalllama_swapped_from_manus_to_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6y0e/thankful_to_rlocalllama_swapped_from_manus_to_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T15:04:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp6it6</id>
    <title>now it can turn your PDFs and docs into clean fine tuning datasets</title>
    <updated>2025-08-13T14:48:26+00:00</updated>
    <author>
      <name>/u/Interesting-Area6418</name>
      <uri>https://old.reddit.com/user/Interesting-Area6418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6it6/now_it_can_turn_your_pdfs_and_docs_into_clean/"&gt; &lt;img alt="now it can turn your PDFs and docs into clean fine tuning datasets" src="https://external-preview.redd.it/3sG_aaHa7N5A_uKldFg_ckXPZRKSagJ4eq_vlsxxQ-g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b05b86b865816d2d239bd9c679d5afbf3fd0461" title="now it can turn your PDFs and docs into clean fine tuning datasets" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/l4z271b5usif1.png?width=1812&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e4d98143bf7d60e382b53787e3ce6eb6272f8c8"&gt;The flow on how it generates datasets using local resources&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1mp6it6/video/hhwtavqwusif1/player"&gt;Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;repo is here &lt;a href="https://github.com/Datalore-ai/datalore-localgen-cli"&gt;https://github.com/Datalore-ai/datalore-localgen-cli&lt;/a&gt;&lt;/p&gt; &lt;p&gt;a while back I posted here about a terminal tool I made during my internship that could generate fine tuning datasets from real world data using deep research.&lt;br /&gt; after that post, I got quite a few dms and some really thoughtful feedback. thank you to everyone who reached out.&lt;/p&gt; &lt;p&gt;also, it got around 15 stars on GitHub which might be small but it was my first project so I am really happy about it. thanks to everyone who checked it out.&lt;/p&gt; &lt;p&gt;one of the most common requests was if it could work on local resources instead of only going online.&lt;br /&gt; so over the weekend I built a separate version that does exactly that.&lt;/p&gt; &lt;p&gt;you point it to a local file like a pdf, docx, jpg or txt and describe the dataset you want. it extracts the text, finds relevant parts with semantic search, applies your instructions through a generated schema, and outputs the dataset.&lt;/p&gt; &lt;p&gt;I am planning to integrate this into the main tool soon so it can handle both online and offline sources in one workflow.&lt;/p&gt; &lt;p&gt;if you want to see some example datasets it generated, feel free to dm me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Area6418"&gt; /u/Interesting-Area6418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6it6/now_it_can_turn_your_pdfs_and_docs_into_clean/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6it6/now_it_can_turn_your_pdfs_and_docs_into_clean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6it6/now_it_can_turn_your_pdfs_and_docs_into_clean/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T14:48:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1moz341</id>
    <title>gpt-oss-120B most intelligent model that fits on an H100 in native precision</title>
    <updated>2025-08-13T08:46:18+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moz341/gptoss120b_most_intelligent_model_that_fits_on_an/"&gt; &lt;img alt="gpt-oss-120B most intelligent model that fits on an H100 in native precision" src="https://preview.redd.it/4okvse7e2rif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=943876a00ac037e2110c919f54e46c6e6d4303b4" title="gpt-oss-120B most intelligent model that fits on an H100 in native precision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Interesting analysis thread: &lt;a href="https://x.com/artificialanlys/status/1952887733803991070"&gt;https://x.com/artificialanlys/status/1952887733803991070&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4okvse7e2rif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moz341/gptoss120b_most_intelligent_model_that_fits_on_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1moz341/gptoss120b_most_intelligent_model_that_fits_on_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T08:46:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp4gwl</id>
    <title>Beelink GTR9 Pro Mini PC Launched: 140W AMD Ryzen AI MAX+ 395 APU, 128 GB LPDDR5x 8000 MT/s Memory, 2 TB Crucial SSD, Dual 10GbE LAN For $1985</title>
    <updated>2025-08-13T13:28:20+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4gwl/beelink_gtr9_pro_mini_pc_launched_140w_amd_ryzen/"&gt; &lt;img alt="Beelink GTR9 Pro Mini PC Launched: 140W AMD Ryzen AI MAX+ 395 APU, 128 GB LPDDR5x 8000 MT/s Memory, 2 TB Crucial SSD, Dual 10GbE LAN For $1985" src="https://external-preview.redd.it/Bvw60PvhPgoef0Ng9Djae_QLUotq8vncLfnhqt8cL74.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=218782160b09032a3d5202434bc6cfb1ccd15a36" title="Beelink GTR9 Pro Mini PC Launched: 140W AMD Ryzen AI MAX+ 395 APU, 128 GB LPDDR5x 8000 MT/s Memory, 2 TB Crucial SSD, Dual 10GbE LAN For $1985" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/beelink-gtr9-pro-mini-pc-launched-140w-amd-ryzen-ai-max-395-128-gb-dual-10gbe-1985-usd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4gwl/beelink_gtr9_pro_mini_pc_launched_140w_amd_ryzen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4gwl/beelink_gtr9_pro_mini_pc_launched_140w_amd_ryzen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T13:28:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp1j7e</id>
    <title>Peak safety theater: gpt-oss-120b refuses to discuss implementing web search in llama.cpp</title>
    <updated>2025-08-13T11:12:30+00:00</updated>
    <author>
      <name>/u/csixtay</name>
      <uri>https://old.reddit.com/user/csixtay</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1j7e/peak_safety_theater_gptoss120b_refuses_to_discuss/"&gt; &lt;img alt="Peak safety theater: gpt-oss-120b refuses to discuss implementing web search in llama.cpp" src="https://preview.redd.it/j7hi9xgjrrif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=328e2e9fc9cd738d0907c1394e77c1ec12b827b3" title="Peak safety theater: gpt-oss-120b refuses to discuss implementing web search in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/csixtay"&gt; /u/csixtay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j7hi9xgjrrif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1j7e/peak_safety_theater_gptoss120b_refuses_to_discuss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1j7e/peak_safety_theater_gptoss120b_refuses_to_discuss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T11:12:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpm8kr</id>
    <title>ERNIE 4.5 21BA3B appreciation post.</title>
    <updated>2025-08-14T00:55:45+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think it's the best model of it's size, outshining gpt-oss 20 and qwen 3 30BA3B.&lt;/p&gt; &lt;p&gt;It's not as good at coding, but it runs without error even at decent context. I find the qwen a3b to be better for code gen, but prefer ernie for everythign else.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpm8kr/ernie_45_21ba3b_appreciation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpm8kr/ernie_45_21ba3b_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpm8kr/ernie_45_21ba3b_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T00:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpabh1</id>
    <title>Matrix-Game 2.0 — first open-source, real-time, long-sequence interactive world model. 25 FPS, minutes-long interaction</title>
    <updated>2025-08-13T17:10:32+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Skywork_ai/status/1955237399912648842?t=hsxnA2t2FyKxRsSRBCJ1kA&amp;amp;s=19"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpabh1/matrixgame_20_first_opensource_realtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpabh1/matrixgame_20_first_opensource_realtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T17:10:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpez1p</id>
    <title>Added locally generated dialogue + voice acting to my game!</title>
    <updated>2025-08-13T20:02:24+00:00</updated>
    <author>
      <name>/u/LandoRingel</name>
      <uri>https://old.reddit.com/user/LandoRingel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpez1p/added_locally_generated_dialogue_voice_acting_to/"&gt; &lt;img alt="Added locally generated dialogue + voice acting to my game!" src="https://external-preview.redd.it/N2szZGNtMzRldWlmMZmQp7O5BpjYg7UqegAgE9IdgP7TYx8Szh9dJVqIheQu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eccefbf13830c4a641bc7633ab5e9b01c2c86540" title="Added locally generated dialogue + voice acting to my game!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LandoRingel"&gt; /u/LandoRingel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t1qgim34euif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpez1p/added_locally_generated_dialogue_voice_acting_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpez1p/added_locally_generated_dialogue_voice_acting_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T20:02:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp2wq3</id>
    <title>There is a new text-to-image model named nano-banana</title>
    <updated>2025-08-13T12:20:32+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp2wq3/there_is_a_new_texttoimage_model_named_nanobanana/"&gt; &lt;img alt="There is a new text-to-image model named nano-banana" src="https://preview.redd.it/jmw88evj4sif1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53c768eb9781ffe9119d98e2a2e9f3c88c8adab5" title="There is a new text-to-image model named nano-banana" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jmw88evj4sif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp2wq3/there_is_a_new_texttoimage_model_named_nanobanana/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp2wq3/there_is_a_new_texttoimage_model_named_nanobanana/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T12:20:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp5bjc</id>
    <title>God I love Qwen and llamacpp so much!</title>
    <updated>2025-08-13T14:01:37+00:00</updated>
    <author>
      <name>/u/Limp_Classroom_2645</name>
      <uri>https://old.reddit.com/user/Limp_Classroom_2645</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"&gt; &lt;img alt="God I love Qwen and llamacpp so much!" src="https://external-preview.redd.it/YWE3eDdxZG5tc2lmMRvVg1psIEfKedgCcU_ySdSE0fdUxqG9M3HUjgrx1S5i.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8afab7c45ab87f6ac2ce8db445bb27de25840096" title="God I love Qwen and llamacpp so much!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Local batch inference with qwen3 30B Instruct on a single RTX3090, 4 requests in parallel &lt;/p&gt; &lt;p&gt;Gonna use it to mass process some data to generate insights about our platform usage&lt;/p&gt; &lt;p&gt;I feel like I'm hitting my limits here and gonna need a multi GPU setup soon 😄&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Limp_Classroom_2645"&gt; /u/Limp_Classroom_2645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ur3oxzhnmsif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T14:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
</feed>
