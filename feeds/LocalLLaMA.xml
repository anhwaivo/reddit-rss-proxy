<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-28T16:39:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jl1yk4</id>
    <title>DeepSeek V3 0324 on livebench surpasses Claude 3.7</title>
    <updated>2025-03-27T11:44:33+00:00</updated>
    <author>
      <name>/u/MrPiradoHD</name>
      <uri>https://old.reddit.com/user/MrPiradoHD</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl1yk4/deepseek_v3_0324_on_livebench_surpasses_claude_37/"&gt; &lt;img alt="DeepSeek V3 0324 on livebench surpasses Claude 3.7" src="https://b.thumbs.redditmedia.com/rn27tkiEJGK7lj3Fd5ifzNLt6PhUdToT0IvAY2kN6gM.jpg" title="DeepSeek V3 0324 on livebench surpasses Claude 3.7" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just saw the latest LiveBench results and DeepSeek's V3 (0324) is showing some impressive performance! It's currently sitting at 10th place overall, but what's really interesting is that it's the second highest non-thinking model, only behind GPT-4.5 Preview, while outperforming Claude 3.7 Sonnet (base model, not the thinking version).&lt;/p&gt; &lt;p&gt;We will have to wait, but this suggests that R2 might be a stupidly great model if V3 is already outperforming Claude 3.7 (base), this next version could seriously challenge to the big ones.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cvzv13s3z7re1.png?width=1956&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=10ff7be91e85ed4fd15e1efa97f8c271791cd296"&gt;https://preview.redd.it/cvzv13s3z7re1.png?width=1956&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=10ff7be91e85ed4fd15e1efa97f8c271791cd296&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrPiradoHD"&gt; /u/MrPiradoHD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl1yk4/deepseek_v3_0324_on_livebench_surpasses_claude_37/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl1yk4/deepseek_v3_0324_on_livebench_surpasses_claude_37/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl1yk4/deepseek_v3_0324_on_livebench_surpasses_claude_37/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T11:44:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlbrjk</id>
    <title>QVQ-Max: Think with Evidence</title>
    <updated>2025-03-27T19:11:59+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://qwenlm.github.io/blog/qvq-max-preview/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlbrjk/qvqmax_think_with_evidence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlbrjk/qvqmax_think_with_evidence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T19:11:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jluzkj</id>
    <title>Best fully local coding setup?</title>
    <updated>2025-03-28T13:23:57+00:00</updated>
    <author>
      <name>/u/nic_key</name>
      <uri>https://old.reddit.com/user/nic_key</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is your go to setup (tools, models, more?) you use to code locally?&lt;/p&gt; &lt;p&gt;I am limited to 12gb RAM but also I don't expect miracles and mainly want to use AI as an assistant taking over simple tasks or small units of an application. &lt;/p&gt; &lt;p&gt;Is there any advice on the current best local coding setup?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nic_key"&gt; /u/nic_key &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jluzkj/best_fully_local_coding_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jluzkj/best_fully_local_coding_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jluzkj/best_fully_local_coding_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T13:23:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkzjve</id>
    <title>Microsoft develop a more efficient way to add knowledge into LLMs</title>
    <updated>2025-03-27T08:55:51+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzjve/microsoft_develop_a_more_efficient_way_to_add/"&gt; &lt;img alt="Microsoft develop a more efficient way to add knowledge into LLMs" src="https://external-preview.redd.it/aCGhAR6FEKRX-h5rqecZAFckea8B8CJ4kaRGE3aJoC0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9ed759c95a14ac48041ed2121cc23df6c9a4808d" title="Microsoft develop a more efficient way to add knowledge into LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.microsoft.com/en-us/research/blog/introducing-kblam-bringing-plug-and-play-external-knowledge-to-llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzjve/microsoft_develop_a_more_efficient_way_to_add/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzjve/microsoft_develop_a_more_efficient_way_to_add/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T08:55:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl7dd9</id>
    <title>What is currently the best Uncensored LLM for 24gb of VRAM?</title>
    <updated>2025-03-27T16:01:27+00:00</updated>
    <author>
      <name>/u/My_Unbiased_Opinion</name>
      <uri>https://old.reddit.com/user/My_Unbiased_Opinion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for recommendations. I have been using APIs but itching getting back to locallama. &lt;/p&gt; &lt;p&gt;Will be running Ollama with OpenWebUI and the model's use case being simply general purpose with the occasional sketchy request.&lt;/p&gt; &lt;p&gt;Edit:&lt;/p&gt; &lt;p&gt;Settled on this one for now: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jlqduz/uncensored_huihuiaiqwq32babliterated_is_very_good/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jlqduz/uncensored_huihuiaiqwq32babliterated_is_very_good/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/My_Unbiased_Opinion"&gt; /u/My_Unbiased_Opinion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl7dd9/what_is_currently_the_best_uncensored_llm_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl7dd9/what_is_currently_the_best_uncensored_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl7dd9/what_is_currently_the_best_uncensored_llm_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T16:01:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlvgwz</id>
    <title>Is a Basic PC enough to run an LLM?</title>
    <updated>2025-03-28T13:47:29+00:00</updated>
    <author>
      <name>/u/r093rp0llack</name>
      <uri>https://old.reddit.com/user/r093rp0llack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to run an LLM on this computer I am not using and want to know if it is possible. Specs: Intel i7 (4 Cores, 4 Threads), 16GB DDR4 RAM, 1TB SSD, AMD W7000 4GB VRAM GPU. I am new to this, only just figuring LLMs out but I figured if a Raspberry Pi 5 can run LLMs a basic PC should be able to run something, right? I just want text, NOT image creation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/r093rp0llack"&gt; /u/r093rp0llack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlvgwz/is_a_basic_pc_enough_to_run_an_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlvgwz/is_a_basic_pc_enough_to_run_an_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlvgwz/is_a_basic_pc_enough_to_run_an_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T13:47:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlts0h</id>
    <title>Help with regard to selection of models for coding</title>
    <updated>2025-03-28T12:21:45+00:00</updated>
    <author>
      <name>/u/AdSenior434</name>
      <uri>https://old.reddit.com/user/AdSenior434</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;1.I got a Mac mini m4 Pro with 16 core GPU and 64 gb ram. My main use case is coding - currently which model should i try to install and what parameter? I don't have unlimited data so cant download every 32B parameter models and experiment with it.And I was told 70B parameter models are no go. Is that true?&lt;br /&gt; 2.Also can the configuration run video generation?Given I can generate images in my M2 8GB i am pretty sure it can generate images but can it generate video?&lt;br /&gt; 3. in case of 64 GB ram how can I allocate more Vram to run models.I saw a command and then forgot.Can anyone help me out?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdSenior434"&gt; /u/AdSenior434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlts0h/help_with_regard_to_selection_of_models_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlts0h/help_with_regard_to_selection_of_models_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlts0h/help_with_regard_to_selection_of_models_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T12:21:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jldt1s</id>
    <title>I looked up "Qwen 3" on duckduck go and found something interesting</title>
    <updated>2025-03-27T21:09:55+00:00</updated>
    <author>
      <name>/u/Flat_Jelly_3581</name>
      <uri>https://old.reddit.com/user/Flat_Jelly_3581</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jldt1s/i_looked_up_qwen_3_on_duckduck_go_and_found/"&gt; &lt;img alt="I looked up &amp;quot;Qwen 3&amp;quot; on duckduck go and found something interesting" src="https://b.thumbs.redditmedia.com/9BzPKrc0IjjvNWCIjcjCohoKMpTPqOmRfuRMvx_uJeY.jpg" title="I looked up &amp;quot;Qwen 3&amp;quot; on duckduck go and found something interesting" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/xwv0yfxyrare1.png?width=1414&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4cff8e124dd1984d9ea2d583ed97e7805a6a33db"&gt;https://preview.redd.it/xwv0yfxyrare1.png?width=1414&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4cff8e124dd1984d9ea2d583ed97e7805a6a33db&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Did someone make a mistake? I think someone made a mistake. That or someones baiting me. Also the link is obviously not made public, but here it will be when its released &lt;a href="https://huggingface.co/FalconNet/Qwen3.0"&gt;https://huggingface.co/FalconNet/Qwen3.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Im stupid, this is early april fools. :/&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flat_Jelly_3581"&gt; /u/Flat_Jelly_3581 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jldt1s/i_looked_up_qwen_3_on_duckduck_go_and_found/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jldt1s/i_looked_up_qwen_3_on_duckduck_go_and_found/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jldt1s/i_looked_up_qwen_3_on_duckduck_go_and_found/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T21:09:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlq0rb</id>
    <title>Questions for a budget build (around $1000)</title>
    <updated>2025-03-28T08:01:41+00:00</updated>
    <author>
      <name>/u/blankboy2022</name>
      <uri>https://old.reddit.com/user/blankboy2022</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlq0rb/questions_for_a_budget_build_around_1000/"&gt; &lt;img alt="Questions for a budget build (around $1000)" src="https://preview.redd.it/4mkpr72n0ere1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82a96d4c14894d0f79690cdb7bd4d1bbf2ca6504" title="Questions for a budget build (around $1000)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, this is my first time building a machine for running local LLMs (and maybe for fine-tuning as well). My budget is around 1000$ and this is what I picked.&lt;/p&gt; &lt;p&gt;I have serveral questions before throwing my money out of the window, hopefully you guys can help me answer them (or give suggestions if you like). Thank you all!&lt;/p&gt; &lt;p&gt;Context: I have chosen a Huananzhi mainboard for 2 reasons. 1) I thought Xeon are good budget CPU (ignore the electricity cost), especially when you can use 2 in a single machine; and 2) I observe that ECC RAM is actually cheaper than normal RAM for whatever reason. I do music and video rendering sometimes as well, so I think Xeon is kind of nice to have. But when I ask the store about my build, they advised me against building a Xeon based system since they think Xeon CPUs have kind of low clock speed, that wouldn't be suitable for the use for AI.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;How would you rate this build for my use case (LLMs inference and possibly fine-tuning)? What is your opinion on Xeon CPUs for running and training LLMs in general?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The GPU part hasn't be decided yet. I was thinking about replacing two 3060 12GB (24GB VRAM) for a single 4060TI 16GB. For any case, I would like to scale it up, by adding more GPU (preferably 3060 12GB or P40 24GB, but our local P40 price has rised to around 500$ recently) and RAM later, aiming for 256GB max by the mainboard, and if I understand correctly the mainboard supports up to 3 GPUs (not mentioning extension or conversation cables added). Have anybody had experience with building a multiple GPU system, especially for Huananzhi mainboards? I wonder how all 8 RAM bars and 3 GPU could fit on it, given the space is quite limited as I observe the mainboard's preview photo.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thank you all, again!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blankboy2022"&gt; /u/blankboy2022 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4mkpr72n0ere1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlq0rb/questions_for_a_budget_build_around_1000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlq0rb/questions_for_a_budget_build_around_1000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T08:01:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jla08h</id>
    <title>Orpheus.cpp - Fast Audio Generation without a GPU</title>
    <updated>2025-03-27T17:50:44+00:00</updated>
    <author>
      <name>/u/freddyaboulton</name>
      <uri>https://old.reddit.com/user/freddyaboulton</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! I've been spending the last couple of months trying to build real-time audio/video assistants in python and got frustrated by the lack of good text-to-speech models that are easy to use and can run decently fast without a GPU on my macbook.&lt;/p&gt; &lt;p&gt;So I built &lt;a href="https://github.com/freddyaboulton/orpheus-cpp"&gt;orpheus.cpp&lt;/a&gt; - a llama.cpp port of CanopyAI's &lt;a href="https://github.com/canopyai/Orpheus-TTS"&gt;Orpheus TTS model&lt;/a&gt; with an easy python API.&lt;/p&gt; &lt;p&gt;Orpheus is cool because it's a llama backbone that generates tokens that can be independently decoded to audio. So it lends itself well to this kind of hardware optimizaiton.&lt;/p&gt; &lt;p&gt;Anyways, hope you find it useful!&lt;/p&gt; &lt;p&gt;𝚙𝚒𝚙 𝚒𝚗𝚜𝚝𝚊𝚕𝚕 𝚘𝚛𝚙𝚑𝚎𝚞𝚜-𝚌𝚙𝚙&lt;br /&gt; 𝚙𝚢𝚝𝚑𝚘𝚗 -𝚖 𝚘𝚛𝚙𝚑𝚎𝚞𝚜_𝚌𝚙𝚙&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freddyaboulton"&gt; /u/freddyaboulton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jla08h/orpheuscpp_fast_audio_generation_without_a_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jla08h/orpheuscpp_fast_audio_generation_without_a_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jla08h/orpheuscpp_fast_audio_generation_without_a_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T17:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlqmjd</id>
    <title>Very interesting paper: Measuring AI Ability to Complete Long Tasks</title>
    <updated>2025-03-28T08:50:08+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2503.14499"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlqmjd/very_interesting_paper_measuring_ai_ability_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlqmjd/very_interesting_paper_measuring_ai_ability_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T08:50:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlec7i</id>
    <title>Microsoft developed this technique which combines RAG and Fine-tuning for better domain adaptation</title>
    <updated>2025-03-27T21:31:07+00:00</updated>
    <author>
      <name>/u/Ambitious_Anybody855</name>
      <uri>https://old.reddit.com/user/Ambitious_Anybody855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlec7i/microsoft_developed_this_technique_which_combines/"&gt; &lt;img alt="Microsoft developed this technique which combines RAG and Fine-tuning for better domain adaptation" src="https://preview.redd.it/do8skr38sare1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09863117ee9d202e6c101b49e612ece45d06fde5" title="Microsoft developed this technique which combines RAG and Fine-tuning for better domain adaptation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been exploring Retrieval Augmented Fine-Tuning (RAFT). Combines RAG and finetuning for better domain adaptation. Along with the question, the doc that gave rise to the context (called the oracle doc) is added, along with other distracting documents. Then, with a certain probability, the oracle document is not included. Has there been any successful use cases of RAFT in the wild? Or has it been overshadowed. In that case, by what?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious_Anybody855"&gt; /u/Ambitious_Anybody855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/do8skr38sare1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlec7i/microsoft_developed_this_technique_which_combines/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlec7i/microsoft_developed_this_technique_which_combines/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T21:31:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlj7qm</id>
    <title>Video of 48GB 4090d teardown and test.</title>
    <updated>2025-03-28T01:09:01+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's a video that shows a teardown of a 48GB 4090. They also show various tests including a LLM run at around the 12:40 mark. It's in Russian so turn on CC with autotranslate to your language of choice.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=m9YszWQenII"&gt;https://www.youtube.com/watch?v=m9YszWQenII&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlj7qm/video_of_48gb_4090d_teardown_and_test/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlj7qm/video_of_48gb_4090d_teardown_and_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlj7qm/video_of_48gb_4090d_teardown_and_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T01:09:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlaeuw</id>
    <title>New QVQ-Max on Qwen Chat</title>
    <updated>2025-03-27T18:07:42+00:00</updated>
    <author>
      <name>/u/MrPLotor</name>
      <uri>https://old.reddit.com/user/MrPLotor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlaeuw/new_qvqmax_on_qwen_chat/"&gt; &lt;img alt="New QVQ-Max on Qwen Chat" src="https://preview.redd.it/vlz8vwxsv9re1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f5aac48465e4c10b54c5dbc92a2a67b80abc921" title="New QVQ-Max on Qwen Chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrPLotor"&gt; /u/MrPLotor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vlz8vwxsv9re1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlaeuw/new_qvqmax_on_qwen_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlaeuw/new_qvqmax_on_qwen_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T18:07:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jln2th</id>
    <title>If money was no object, what kind of system would you seek out in order to run Llama 3.3?</title>
    <updated>2025-03-28T04:34:17+00:00</updated>
    <author>
      <name>/u/TokenBearer</name>
      <uri>https://old.reddit.com/user/TokenBearer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A Mac Studio with 256GB unified ram, or maybe 512GB to run DeepSeek as well? Both should handle full precision.&lt;/p&gt; &lt;p&gt;Or would you go cluster together GPUs? If so, which ones and why?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenBearer"&gt; /u/TokenBearer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jln2th/if_money_was_no_object_what_kind_of_system_would/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jln2th/if_money_was_no_object_what_kind_of_system_would/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jln2th/if_money_was_no_object_what_kind_of_system_would/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T04:34:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jldzbn</id>
    <title>Is there something better than Ollama?</title>
    <updated>2025-03-27T21:16:43+00:00</updated>
    <author>
      <name>/u/Timziito</name>
      <uri>https://old.reddit.com/user/Timziito</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't mind Ollama but i assume something more optimized is out there maybe? :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Timziito"&gt; /u/Timziito &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jldzbn/is_there_something_better_than_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jldzbn/is_there_something_better_than_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jldzbn/is_there_something_better_than_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T21:16:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlgvbv</id>
    <title>I built a very easy to use lightweight fully C++ desktop UI for whisper.cpp</title>
    <updated>2025-03-27T23:17:57+00:00</updated>
    <author>
      <name>/u/mehtabmahir</name>
      <uri>https://old.reddit.com/user/mehtabmahir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just released a lightweight local desktop UI for whisper.cpp, and added several thoughtful features that makes the whisper experience very easy and noob friendly.&lt;/p&gt; &lt;p&gt;It’s a lightweight, native desktop interface for &lt;a href="https://github.com/ggerganov/whisper.cpp"&gt;whisper.cpp&lt;/a&gt;, built entirely in C++ using Qt. No Python, no browser, and no heavy dependencies — just a smooth and fast UI that runs locally on Windows.&lt;/p&gt; &lt;h1&gt;🔧 Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Fully C++ implementation — &lt;strong&gt;no Python required&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Uses &lt;strong&gt;Vulkan&lt;/strong&gt; for cross platform GPU acceleration (via whisper.cpp)&lt;/li&gt; &lt;li&gt;Drag &amp;amp; drop or use “Open With” to load audio&lt;/li&gt; &lt;li&gt;Auto-converts audio if needed to &lt;code&gt;.mp3&lt;/code&gt; with FFmpeg&lt;/li&gt; &lt;li&gt;Model selector with automatic downloading&lt;/li&gt; &lt;li&gt;Real-time logs in a built-in console box&lt;/li&gt; &lt;li&gt;Opens the final transcript in Notepad&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;💡 Why I built it&lt;/h1&gt; &lt;p&gt;I wanted something that just worked — no virtual environments, no setup steps — just a small program you can drop on your desktop and use right away. Whisper is amazing, but I felt the experience could be simpler for everyday users.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mehtabmahir/easy-whisper-ui/releases/"&gt;https://github.com/mehtabmahir/easy-whisper-ui/releases/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think — feedback, feature ideas, and bug reports welcome! I'm planning to add more features very soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehtabmahir"&gt; /u/mehtabmahir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlgvbv/i_built_a_very_easy_to_use_lightweight_fully_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlgvbv/i_built_a_very_easy_to_use_lightweight_fully_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlgvbv/i_built_a_very_easy_to_use_lightweight_fully_c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T23:17:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jltdr3</id>
    <title>Google release TX Gemma open model to improve the efficiency of therapeutic development</title>
    <updated>2025-03-28T11:59:52+00:00</updated>
    <author>
      <name>/u/codingworkflow</name>
      <uri>https://old.reddit.com/user/codingworkflow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://developers.googleblog.com/en/introducing-txgemma-open-models-improving-therapeutics-development/"&gt;https://developers.googleblog.com/en/introducing-txgemma-open-models-improving-therapeutics-development/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TxGemma models, fine-tuned from Gemma 2 using 7 million training examples, are open models designed for prediction and conversational therapeutic data analysis. These models are available in three sizes: 2B, 9B and 27B. Each size includes a ‘predict’ version, specifically tailored for narrow tasks drawn from Therapeutic Data Commons, for example predicting if a molecule is toxic.&lt;/p&gt; &lt;p&gt;These tasks encompass:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;classification (e.g., will this molecule cross the blood-brain barrier?)&lt;/li&gt; &lt;li&gt;regression (e.g., predicting a drug's binding affinity) &lt;/li&gt; &lt;li&gt;and generation (e.g., given the product of some reaction, generate the reactant set)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The largest TxGemma model (27B predict version) delivers strong performance. It's not only better than, or roughly equal to, our previous state-of-the-art generalist model (Tx-LLM) on almost every task, but it also rivals or beats many models that are specifically designed for single tasks. Specifically, it outperforms or has comparable performance to our previous model on 64 of 66 tasks (beating it on 45), and does the same against specialized models on 50 of the tasks (beating them on 26). See the TxGemma paper for detailed results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codingworkflow"&gt; /u/codingworkflow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jltdr3/google_release_tx_gemma_open_model_to_improve_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jltdr3/google_release_tx_gemma_open_model_to_improve_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jltdr3/google_release_tx_gemma_open_model_to_improve_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T11:59:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlwpz9</id>
    <title>llama.cpp parameters for QwQ-32B with 128k expanded context</title>
    <updated>2025-03-28T14:44:54+00:00</updated>
    <author>
      <name>/u/H3PO</name>
      <uri>https://old.reddit.com/user/H3PO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've got 48GB of VRAM and the Q4_K_M model fits alongside 128k context using q4_0 value cache quantization. Which parameters do I need to give to llama.cpp to correctly expand the context from 32k to 128k? &lt;a href="https://docs.unsloth.ai/basics/tutorials-how-to-fine-tune-and-run-llms/tutorial-how-to-run-qwq-32b-effectively"&gt;This unsloth blog post&lt;/a&gt; mentions how they tried setting some --override-kv options, but from what I understand that was in an attempt to fix issues with repetitions, which they then solved with the --sampler paramter.&lt;/p&gt; &lt;p&gt;Below are the parameters I used in my naive attempt to copy those that unsloth suggest, but with yarn rope scaling added. Using the &amp;quot;Create a Flappy Bird game in Python....&amp;quot; prompt from the blog post, QwQ thinks for for a long time and outputs a working flappy bird pygame script (about 150 lines), but only after thinking for about 20.000 tokens.&lt;/p&gt; &lt;p&gt;Should I set the various --yarn-* parameters differently? I notice llama.cpp logs &amp;quot;qwen2.context_length u32 = 131072&amp;quot; and &amp;quot;n_ctx_train = 131072&amp;quot;, which are wrong afaik.&lt;br /&gt; Also, can someone suggest a long-context test prompt I could use to test if the context expansion is working correctly?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./build/bin/llama-cli \ --threads 32 --prio 2 \ --model ~/llm/models/QwQ-32B-Q4_K_M.gguf \ --n-gpu-layers 99 \ --temp 0.6 --repeat-penalty 1.1 --dry-multiplier 0.5 \ --min-p 0.01 --top-k 40 --top-p 0.95 \ --samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot; \ --ctx-size 131072 --rope-scaling yarn --rope-scale 4 \ --cache-type-v q4_0 --flash-attn \ -no-cnv --prompt &amp;quot;&amp;lt;|im_start|&amp;gt;user\nCreate a Flappy Bird game in Python. You must include these things:\n1. You must use pygame.\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\n3. Pressing SPACE multiple times will accelerate the bird.\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&amp;lt;think&amp;gt;\n&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/H3PO"&gt; /u/H3PO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlwpz9/llamacpp_parameters_for_qwq32b_with_128k_expanded/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlwpz9/llamacpp_parameters_for_qwq32b_with_128k_expanded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlwpz9/llamacpp_parameters_for_qwq32b_with_128k_expanded/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T14:44:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlqduz</id>
    <title>Uncensored huihui-ai/QwQ-32B-abliterated is very good!</title>
    <updated>2025-03-28T08:30:35+00:00</updated>
    <author>
      <name>/u/My_Unbiased_Opinion</name>
      <uri>https://old.reddit.com/user/My_Unbiased_Opinion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been getting back into LocalLLMs as of late and been on the hunt for the best overall uncensored LLM I can find. Tried Gemma 3 and Mistal. Even other Abliterated QwQ models. But this specific one here takes the cake. I got the Ollama url here for anyone interested:&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/huihui_ai/qwq-abliterated:32b-Q3_K_M"&gt;https://ollama.com/huihui_ai/qwq-abliterated:32b-Q3_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When running the model, be sure to run Temperature=0.6, TopP=0.95, MinP=0, topk=30, presence penalty might need to be adjusted for repetitions. (Between 0-2). Apparently this can affect performance negatively when set up to the highest recommended max of 2. I have mine set to 0.&lt;/p&gt; &lt;p&gt;Be sure to increase context length! Ollama defaults to 2048. That's not enough for a reasoning model.&lt;/p&gt; &lt;p&gt;I had to manually set these in OpenWebUi in order to get good output. &lt;/p&gt; &lt;p&gt;Why I like it: The model doesn't seem to be brainwashed. The thought chain knows I'm asking something sketchy, but still decides to answer. It doesn't soft refuse as in giving vague I formation. It can be as detailed as you allow it. It's also very logical yet can use colorful language if the need calls for it. &lt;/p&gt; &lt;p&gt;Very good model, y'all should try. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/My_Unbiased_Opinion"&gt; /u/My_Unbiased_Opinion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlqduz/uncensored_huihuiaiqwq32babliterated_is_very_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlqduz/uncensored_huihuiaiqwq32babliterated_is_very_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlqduz/uncensored_huihuiaiqwq32babliterated_is_very_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T08:30:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl5jea</id>
    <title>My LLMs are all free thinking and locally-sourced.</title>
    <updated>2025-03-27T14:43:35+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl5jea/my_llms_are_all_free_thinking_and_locallysourced/"&gt; &lt;img alt="My LLMs are all free thinking and locally-sourced." src="https://preview.redd.it/s6mrolmfv8re1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4fc11e662f92489410497cde8c31a6b140e9bde" title="My LLMs are all free thinking and locally-sourced." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s6mrolmfv8re1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl5jea/my_llms_are_all_free_thinking_and_locallysourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl5jea/my_llms_are_all_free_thinking_and_locallysourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T14:43:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlsi6h</id>
    <title>nsfw orpheus tts - update</title>
    <updated>2025-03-28T11:04:52+00:00</updated>
    <author>
      <name>/u/MrAlienOverLord</name>
      <uri>https://old.reddit.com/user/MrAlienOverLord</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ok since the last post captured quite a bit of interest &lt;/p&gt; &lt;p&gt;Overall Total Duration: 22625515.59600002 seconds&lt;br /&gt; Overall Total Duration: 6284.87 hours&lt;br /&gt; Total audio events found: 919007&lt;/p&gt; &lt;p&gt;that's where we are - i think i can cut it short to 10-15k hours and then we should have something interesting . sadly 95% only female for the time being.&lt;/p&gt; &lt;p&gt;i should have enough high quality data in about a week to push a first finetune and then release it oss-nc&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jhgpew/nsfw_orpheus_tts/"&gt;old reddit post as ref&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrAlienOverLord"&gt; /u/MrAlienOverLord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlsi6h/nsfw_orpheus_tts_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlsi6h/nsfw_orpheus_tts_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlsi6h/nsfw_orpheus_tts_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T11:04:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlsruf</id>
    <title>Heptagon, 20 balls, rotating numbers, one shot Gemini Pro 2.5</title>
    <updated>2025-03-28T11:22:29+00:00</updated>
    <author>
      <name>/u/Careless_Garlic1438</name>
      <uri>https://old.reddit.com/user/Careless_Garlic1438</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlsruf/heptagon_20_balls_rotating_numbers_one_shot/"&gt; &lt;img alt="Heptagon, 20 balls, rotating numbers, one shot Gemini Pro 2.5" src="https://external-preview.redd.it/YnN3NW5tN2EwZnJlMRmXuZX6QrCBDO6mu8qJTiOd1FpECrDxp61t2gctAI10.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed11bb267533676ec07f47c2eb54af0997245461" title="Heptagon, 20 balls, rotating numbers, one shot Gemini Pro 2.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Careless_Garlic1438"&gt; /u/Careless_Garlic1438 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y4yine7a0fre1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlsruf/heptagon_20_balls_rotating_numbers_one_shot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlsruf/heptagon_20_balls_rotating_numbers_one_shot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T11:22:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlw5hb</id>
    <title>New TTS model from bytedance</title>
    <updated>2025-03-28T14:18:47+00:00</updated>
    <author>
      <name>/u/bio_risk</name>
      <uri>https://old.reddit.com/user/bio_risk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlw5hb/new_tts_model_from_bytedance/"&gt; &lt;img alt="New TTS model from bytedance" src="https://external-preview.redd.it/uKiEmAEQx3Gdvnl2sNzEW0QEbrYYFLxWFMibNhAEifw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=813b2b376a8aba99e7464ee633d5a6f6d97c2749" title="New TTS model from bytedance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bio_risk"&gt; /u/bio_risk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/bytedance/MegaTTS3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlw5hb/new_tts_model_from_bytedance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlw5hb/new_tts_model_from_bytedance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T14:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlptqu</id>
    <title>Reverse engineering GPT-4o image gen via Network tab - here's what I found</title>
    <updated>2025-03-28T07:46:37+00:00</updated>
    <author>
      <name>/u/seicaratteri</name>
      <uri>https://old.reddit.com/user/seicaratteri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlptqu/reverse_engineering_gpt4o_image_gen_via_network/"&gt; &lt;img alt="Reverse engineering GPT-4o image gen via Network tab - here's what I found" src="https://b.thumbs.redditmedia.com/6ICxPu6L9VzJwgZpZB11Ryf2Jzo9ZxiBnaamsutC34E.jpg" title="Reverse engineering GPT-4o image gen via Network tab - here's what I found" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am very intrigued about this new model; I have been working in the image generation space a lot, and I want to understand what's going on&lt;/p&gt; &lt;p&gt;I found interesting details when opening the network tab to see what the BE was sending - here's what I found. I tried with few different prompts, let's take this as a starter:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;An image of happy dog running on the street, studio ghibli style&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here I got four intermediate images, as follows:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6q6f9b9naere1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=586bd8d4f0cdb7b03c76492891bec5df0c0dbea9"&gt;https://preview.redd.it/6q6f9b9naere1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=586bd8d4f0cdb7b03c76492891bec5df0c0dbea9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We can see:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The BE is actually returning the image as we see it in the UI&lt;/li&gt; &lt;li&gt;It's not really clear wether the generation is autoregressive or not - we see &lt;em&gt;some&lt;/em&gt; details and a faint global structure of the image, this could mean two things: &lt;ul&gt; &lt;li&gt;Like usual diffusion processes, we first generate the global structure and then add details&lt;/li&gt; &lt;li&gt;OR - The image is &lt;em&gt;actually&lt;/em&gt; generated autoregressively&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If we analyze the 100% zoom of the first and last frame, we can see details are being added to high frequency textures like the trees&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vxdt6m8oaere1.png?width=2608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00b4f056ed1d4b6e363146438b951e59e2279965"&gt;https://preview.redd.it/vxdt6m8oaere1.png?width=2608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00b4f056ed1d4b6e363146438b951e59e2279965&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is what we would typically expect from a diffusion model. This is further accentuated in this other example, where I prompted specifically for a high frequency detail texture (&amp;quot;create the image of a grainy texture, abstract shape, very extremely highly detailed&amp;quot;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4sd80u4paere1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29a87f794c0041801bc825e32cebcbcbed8a3ddf"&gt;https://preview.redd.it/4sd80u4paere1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29a87f794c0041801bc825e32cebcbcbed8a3ddf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Interestingly, I got only three images here from the BE; and the details being added is obvious:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nuoeccupaere1.png?width=2058&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c0ffd1869939b6a7cc24167cd69ad7bd94ad728"&gt;https://preview.redd.it/nuoeccupaere1.png?width=2058&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c0ffd1869939b6a7cc24167cd69ad7bd94ad728&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This could be done of course as a separate post processing step too, for example like SDXL introduced the refiner model back in the days that was specifically trained to add details to the VAE latent representation before decoding it to pixel space.&lt;/p&gt; &lt;p&gt;It's also unclear if I got less images with this prompt due to availability (i.e. the BE could give me more flops), or to some kind of specific optimization (eg: latent caching).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So where I am at now:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It's probably a multi step process pipeline&lt;/li&gt; &lt;li&gt;OpenAI in the model card is stating that &amp;quot;Unlike DALL·E, which operates as a diffusion model, 4o image generation is an autoregressive model natively embedded within ChatGPT&amp;quot;&lt;/li&gt; &lt;li&gt;This makes me think of this recent paper: &lt;a href="https://arxiv.org/pdf/2409.11340"&gt;OmniGen&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There they directly connect the VAE of a Latent Diffusion architecture to an LLM and learn to model jointly both text and images; they observe few shot capabilities and emerging properties too which would explain the vast capabilities of GPT4-o, and it makes even more sense if we consider the usual OAI formula:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;More / higher quality data&lt;/li&gt; &lt;li&gt;More flops&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The architecture proposed in OmniGen has &lt;em&gt;great&lt;/em&gt; potential to scale given that is purely transformer based - and if we know one thing is surely that transformers scale well, and that OAI is especially good at that&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What do you think?&lt;/strong&gt; would love to take this as a space to investigate together! Thanks for reading and let's get to the bottom of this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seicaratteri"&gt; /u/seicaratteri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlptqu/reverse_engineering_gpt4o_image_gen_via_network/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlptqu/reverse_engineering_gpt4o_image_gen_via_network/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlptqu/reverse_engineering_gpt4o_image_gen_via_network/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T07:46:37+00:00</published>
  </entry>
</feed>
