<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-12T22:37:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kl19yp</id>
    <title>Formula to get GPU hours for fine-tuning</title>
    <updated>2025-05-12T19:17:48+00:00</updated>
    <author>
      <name>/u/ThomasPhilli</name>
      <uri>https://old.reddit.com/user/ThomasPhilli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a good formula to get GPU hours to fine tune a model, given data size, model size, quantization, etc.?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasPhilli"&gt; /u/ThomasPhilli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl19yp/formula_to_get_gpu_hours_for_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl19yp/formula_to_get_gpu_hours_for_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl19yp/formula_to_get_gpu_hours_for_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T19:17:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkbhxr</id>
    <title>Wow! DeerFlow is OSS now: LLM + Langchain + tools (web search, crawler, code exec)</title>
    <updated>2025-05-11T21:11:32+00:00</updated>
    <author>
      <name>/u/behradkhodayar</name>
      <uri>https://old.reddit.com/user/behradkhodayar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bytedance (the company behind TikTok), opensourced DeerFlow (&lt;strong&gt;D&lt;/strong&gt;eep &lt;strong&gt;E&lt;/strong&gt;xploration and &lt;strong&gt;E&lt;/strong&gt;fficient &lt;strong&gt;R&lt;/strong&gt;esearch &lt;strong&gt;Flow&lt;/strong&gt;), such a great give-back.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/bytedance/deer-flow"&gt;https://github.com/bytedance/deer-flow&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/behradkhodayar"&gt; /u/behradkhodayar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkbhxr/wow_deerflow_is_oss_now_llm_langchain_tools_web/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkbhxr/wow_deerflow_is_oss_now_llm_langchain_tools_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkbhxr/wow_deerflow_is_oss_now_llm_langchain_tools_web/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T21:11:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkx4ev</id>
    <title>Best local inference provider?</title>
    <updated>2025-05-12T16:36:11+00:00</updated>
    <author>
      <name>/u/TechnicalGeologist99</name>
      <uri>https://old.reddit.com/user/TechnicalGeologist99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tried ollama and vllm. &lt;/p&gt; &lt;p&gt;I liked the ability to swap models in ollama. But I found vllm is faster. Though if I'm not mistaken, vllm doesn't support model swapping.&lt;/p&gt; &lt;p&gt;What I need: - ability to swap models - run as a server via docker/compose - run multiple models at the same time - able to use finetuned checkpoints - server handles it's own queue of requests - openai like API&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechnicalGeologist99"&gt; /u/TechnicalGeologist99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkx4ev/best_local_inference_provider/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkx4ev/best_local_inference_provider/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkx4ev/best_local_inference_provider/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T16:36:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkshqc</id>
    <title>Qwen3 repeats itself forever at the end of its output</title>
    <updated>2025-05-12T13:27:07+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using ollama 0.6.8 with ollama run &lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF"&gt;https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;Prompt:&lt;/p&gt; &lt;p&gt;There are 30 people in a class and each person chooses 5 other people in the class that they want to be in a new class with. The new classes will each be of size 10 (the classes don't overlap and cover all 30 people). Is it ever impossible for everyone to be with at least one of their chosen five?&lt;/p&gt; &lt;p&gt;Or alternatively, show that it is always possible.&lt;/p&gt; &lt;p&gt;I know that there is a set of choices that makes the classes impossible to arrange if each person chooses 4 others. He also shows that there is no such bad set of choices if each person chooses 8 others.&lt;/p&gt; &lt;p&gt;For 5, 6, 7 choices the question is currently open.&lt;/p&gt; &lt;p&gt;The question is asking about a set of preferences that is impossible to satisfy for any partition.&lt;/p&gt; &lt;p&gt;-----&lt;/p&gt; &lt;p&gt;After thinking for a while it outputs this on a loop which never seems to terminate:&lt;/p&gt; &lt;p&gt;**Final Answer:** \boxed{Yes}&lt;/p&gt; &lt;p&gt;**Reasoning:** The high connectivity of the graph (with 5 choices per person) ensures that a suitable partition&lt;/p&gt; &lt;p&gt;into 3 groups is always possible, as supported by probabilistic arguments. Hence, the answer is \boxed{Yes}.&lt;/p&gt; &lt;p&gt;**Final Answer**&lt;/p&gt; &lt;p&gt;\boxed{Yes}&lt;/p&gt; &lt;p&gt;**Reasoning:** The high connectivity of the graph (with 5 choices per person) ensures that a suitable partition&lt;/p&gt; &lt;p&gt;into 3 groups is always possible, as supported by probabilistic arguments. Hence, the answer is \boxed{Yes}.&lt;/p&gt; &lt;p&gt;I think the answer is \boxed{Yes}.&lt;/p&gt; &lt;p&gt;**Step-by-Step Explanation:**&lt;/p&gt; &lt;ol&gt; &lt;li&gt;**Understanding the Problem:**- There are 30 people, each with 5 choices (presumably of other people).- We are to divide them into 3 groups of 10.- In each group, for every person, at least one of their 5 choices must be in the same group.&lt;/li&gt; &lt;li&gt;**Key Observations:**- Each person has 5 &amp;quot;preferences&amp;quot; (choices).- The graph formed by these choices is a directed graph with 30 nodes and 5 outgoing edges per node.- The goal is to partition this graph into 3 subgraphs (groups), each of size 10, such that for every node, at&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;least one of its 5 outgoing edges leads to a node in the same subgraph.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;**Random Assignment and Probabilities:**&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- If we randomly assign each person to one of the 3 groups, the probability that a specific person's group&lt;/p&gt; &lt;p&gt;contains none of their 5 choices is:&lt;/p&gt; &lt;p&gt;$$&lt;/p&gt; &lt;p&gt;\left(\frac{2}{3}\right)^5 \approx 0.1316&lt;/p&gt; &lt;p&gt;$$&lt;/p&gt; &lt;p&gt;- Therefore, the probability that a person is &amp;quot;satisfied&amp;quot; (i.e., at least one of their choices is in the same&lt;/p&gt; &lt;p&gt;group) is:&lt;/p&gt; &lt;p&gt;$$&lt;/p&gt; &lt;p&gt;1 - \left(\frac{2}{3}\right)^5 \approx 0.8684&lt;/p&gt; &lt;p&gt;$$&lt;/p&gt; &lt;p&gt;- The expected number of &amp;quot;unsatisfied&amp;quot; people across all 30 is:&lt;/p&gt; &lt;p&gt;$$&lt;/p&gt; &lt;p&gt;30 \times 0.1316 \approx 3.95&lt;/p&gt; &lt;p&gt;$$&lt;/p&gt; &lt;ol&gt; &lt;li&gt;**Existence Argument (Probabilistic Method):**&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- While the expected number of unsatisfied people is about 4, this does not guarantee that there exists a partition with zero unsatisfied people.&lt;/p&gt; &lt;p&gt;- However, the graph is highly connected (each node has 5 outgoing edges), and such a partition is likely to exist due to the high density of connections.&lt;/p&gt; &lt;p&gt;- The probabilistic method suggests that, given the high connectivity, it is highly probable that such a partition exists.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;**Conclusion:**&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- Based on the high connectivity of the graph and the probabilistic argument, it is always possible to divide the 30 people into 3 groups of 10 such that each person has at&lt;/p&gt; &lt;p&gt;least one of their 5 choices in the same group.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkshqc/qwen3_repeats_itself_forever_at_the_end_of_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkshqc/qwen3_repeats_itself_forever_at_the_end_of_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkshqc/qwen3_repeats_itself_forever_at_the_end_of_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T13:27:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkea2w</id>
    <title>LPT: Got an old low VRAM GPU you're not using? Use it to increase your VRAM pool.</title>
    <updated>2025-05-11T23:23:26+00:00</updated>
    <author>
      <name>/u/pneuny</name>
      <uri>https://old.reddit.com/user/pneuny</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently got an RTX 5060 Ti 16GB, but 16GB is still not enough to fit something like Qwen 3 30b-a3b. That's where the old GTX 1060 I got in return for handing down a 3060 Ti comes in handy. In LMStudio, using the Vulkan backend, with full GPU offloading to both the RTX and GTX cards, I managed to get 43 t/s, which is way better than the ~13 t/s with partial CPU offloading when using CUDA 12.&lt;/p&gt; &lt;p&gt;So yeah, if you have a 16GB card, break out that old card and add it to your system if your motherboard has the PCIE slot to spare.&lt;/p&gt; &lt;p&gt;PS: This also gives you 32 bit physx support on your RTX 50 series if the old card is Nvidia.&lt;/p&gt; &lt;p&gt;TL;DR: RTX 5060 Ti 16GB + GTX 1060 6GB = 43t/s on Qwen3 30b-a3b&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pneuny"&gt; /u/pneuny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkea2w/lpt_got_an_old_low_vram_gpu_youre_not_using_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkea2w/lpt_got_an_old_low_vram_gpu_youre_not_using_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkea2w/lpt_got_an_old_low_vram_gpu_youre_not_using_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T23:23:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kk97m7</id>
    <title>We made an open source agent builder and framework designed to work with local llms!</title>
    <updated>2025-05-11T19:31:37+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk97m7/we_made_an_open_source_agent_builder_and/"&gt; &lt;img alt="We made an open source agent builder and framework designed to work with local llms!" src="https://preview.redd.it/ha9ptoygf70f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=89d2d79a3d2e7586b294f58dfb84c68117b05a1a" title="We made an open source agent builder and framework designed to work with local llms!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ha9ptoygf70f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk97m7/we_made_an_open_source_agent_builder_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kk97m7/we_made_an_open_source_agent_builder_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T19:31:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kko4xu</id>
    <title>Support for InternVL has been merged into llama.cpp</title>
    <updated>2025-05-12T09:21:46+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13422"&gt;https://github.com/ggml-org/llama.cpp/pull/13422&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13443"&gt;https://github.com/ggml-org/llama.cpp/pull/13443&lt;/a&gt;&lt;/p&gt; &lt;p&gt;when GGUF? ;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kko4xu/support_for_internvl_has_been_merged_into_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kko4xu/support_for_internvl_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kko4xu/support_for_internvl_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T09:21:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl039t</id>
    <title>Chatbots, Music and Solar Systems galore! More fun and quirkiness with Qwen 3 8b!</title>
    <updated>2025-05-12T18:31:41+00:00</updated>
    <author>
      <name>/u/c64z86</name>
      <uri>https://old.reddit.com/user/c64z86</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl039t/chatbots_music_and_solar_systems_galore_more_fun/"&gt; &lt;img alt="Chatbots, Music and Solar Systems galore! More fun and quirkiness with Qwen 3 8b!" src="https://external-preview.redd.it/FR04JgYxLf_GUSd5H4byajAPCgFokYhow80Zrzcrfig.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0543e897f405a1ffe5126bf54e048b37891ac559" title="Chatbots, Music and Solar Systems galore! More fun and quirkiness with Qwen 3 8b!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/c64z86"&gt; /u/c64z86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=gepZOxpSyFQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl039t/chatbots_music_and_solar_systems_galore_more_fun/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl039t/chatbots_music_and_solar_systems_galore_more_fun/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T18:31:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkl39r</id>
    <title>Findings from LoRA Finetuning for Qwen3</title>
    <updated>2025-05-12T05:46:44+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Fine-tuned Qwen3-8B with a small LoRA setup to preserve its ability to switch behaviors using &lt;code&gt;/think&lt;/code&gt; (reasoning) and &lt;code&gt;/no_think&lt;/code&gt; (casual) prompts. Rank 8 gave the best results. Training took ~30 minutes for 8B using 4,000 examples. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;LoRA Rank Testing Results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;✅ &lt;strong&gt;Rank 8&lt;/strong&gt;: Best outcome—preserved both &lt;code&gt;/think&lt;/code&gt; and &lt;code&gt;/no_think&lt;/code&gt; behavior.&lt;/li&gt; &lt;li&gt;❌ &lt;strong&gt;Rank 32&lt;/strong&gt;: Model started ignoring the &lt;code&gt;/think&lt;/code&gt; prompt.&lt;/li&gt; &lt;li&gt;💀 &lt;strong&gt;Rank 64&lt;/strong&gt;: Completely broke—output became nonsensical.&lt;/li&gt; &lt;li&gt;🧠 &lt;strong&gt;Rank 128&lt;/strong&gt;: Overfit hard—model became overly STUPID&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Training Configuration:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Applied LoRA to: &lt;code&gt;q_proj&lt;/code&gt;, &lt;code&gt;k_proj&lt;/code&gt;, &lt;code&gt;v_proj&lt;/code&gt;, &lt;code&gt;o_proj&lt;/code&gt;, &lt;code&gt;gate_proj&lt;/code&gt;, &lt;code&gt;up_proj&lt;/code&gt;, &lt;code&gt;down_proj&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Rank: 8&lt;/li&gt; &lt;li&gt;Alpha: 16&lt;/li&gt; &lt;li&gt;Dropout: 0.05&lt;/li&gt; &lt;li&gt;Bias: Disabled&lt;/li&gt; &lt;li&gt;Gradient Checkpointing: Enabled to reduce memory usage&lt;/li&gt; &lt;li&gt;Batch Size: 2&lt;/li&gt; &lt;li&gt;Gradient Accumulation: 4 steps&lt;/li&gt; &lt;li&gt;Learning Rate: 2e-4&lt;/li&gt; &lt;li&gt;Epochs: 1&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I also tested whether full finetuning or using the model without 4-bit quantization would help. Neither approach gave better results. In fact, the model sometimes performed worse or became inconsistent in responding to &lt;code&gt;/think&lt;/code&gt; and &lt;code&gt;/no_think&lt;/code&gt;. This confirmed that lightweight LoRA with rank 8 was the ideal trade-off between performance and resource use.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Collection:&lt;/strong&gt; 👉 &lt;a href="https://huggingface.co/collections/soob3123/grayline-collection-qwen3-6821009e843331c5a9c27da1"&gt;GrayLine-Qwen3 Collection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Future Plans:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3-32B&lt;/li&gt; &lt;li&gt;Try fine-tuning Qwen3-30B-A3B (MoE version) to see if it handles behavior switching better at scale.&lt;/li&gt; &lt;li&gt;Run full benchmark evaluations using LM-Eval to better understand model performance across reasoning, safety, and general capabilities.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me know if you want me to try any other configs!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkl39r/findings_from_lora_finetuning_for_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkl39r/findings_from_lora_finetuning_for_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkl39r/findings_from_lora_finetuning_for_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T05:46:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl21hm</id>
    <title>Kokoro-JS with long text support</title>
    <updated>2025-05-12T19:47:59+00:00</updated>
    <author>
      <name>/u/paranoidray</name>
      <uri>https://old.reddit.com/user/paranoidray</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paranoidray"&gt; /u/paranoidray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://test-kokoro.glitch.me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl21hm/kokorojs_with_long_text_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl21hm/kokorojs_with_long_text_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T19:47:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl0uvv</id>
    <title>Predicting sales conversion probability from conversations using pure Reinforcement Learning</title>
    <updated>2025-05-12T19:01:28+00:00</updated>
    <author>
      <name>/u/Nandakishor_ml</name>
      <uri>https://old.reddit.com/user/Nandakishor_ml</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the past couple of months, I have been working on building a chess game kinda system for predicting sales conversion probabilities from sales conversations. Sales are notoriously difficult to analyse with current LLMs or SLMs, even ChatGPT, Claude, or Gemini failed to fully analyse sales conversations. How about we can guide the conversations based on predicting the conversion probabilities, that is, kinda trained on a 100000+ sales conversation with RL to predict the final probability from the embeddings. So I just used Azure OpenAI embedding(especially the text-embedding-3-large model to create a wide variety of conversations. The main goal of RL is conversion(reward=1), it will create different conversations, different pathways, most of which lead to nonconversion (0), and some lead to conversion(1), along with 3072 embedding vectors to get the nuances and semantics of the dialogues. Other fields include&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Company/product identifiers&lt;/li&gt; &lt;li&gt;Conversation messages (JSON)&lt;/li&gt; &lt;li&gt;Customer engagement &amp;amp; sales effectiveness scores (0-1)&lt;/li&gt; &lt;li&gt;Probability trajectory at each turn&lt;/li&gt; &lt;li&gt;Conversation style, flow pattern, and channel&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Then I just trained an RL with PPO, by reducing the dimension using a linear layer and using that to do the final prediction with PPO.&lt;/p&gt; &lt;p&gt;Dataset, model, and training script are all open-sourced. Also written an Arxiv paper on it.&lt;/p&gt; &lt;p&gt;Dataset: &lt;a href="https://huggingface.co/datasets/DeepMostInnovations/saas-sales-conversations"&gt;https://huggingface.co/datasets/DeepMostInnovations/saas-sales-conversations&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model, dataset creation, training, and inference: &lt;a href="https://huggingface.co/DeepMostInnovations/sales-conversion-model-reinf-learning"&gt;https://huggingface.co/DeepMostInnovations/sales-conversion-model-reinf-learning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2503.23303"&gt;https://arxiv.org/abs/2503.23303 &lt;/a&gt;&lt;/p&gt; &lt;p&gt;Btw, use Python version 10 for inference. Also, I am thinking of using open-source embedding models to create the embedding vectors, but it will take more time. Feel free to try&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nandakishor_ml"&gt; /u/Nandakishor_ml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl0uvv/predicting_sales_conversion_probability_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl0uvv/predicting_sales_conversion_probability_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl0uvv/predicting_sales_conversion_probability_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T19:01:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl3scc</id>
    <title>Inverse Turing Test (Open Source HF Space) - Can you fool the AI?</title>
    <updated>2025-05-12T20:57:04+00:00</updated>
    <author>
      <name>/u/GuiltyBookkeeper4849</name>
      <uri>https://old.reddit.com/user/GuiltyBookkeeper4849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Today, I'm launching a new experimental Hugging Face Space: &lt;strong&gt;Inverse Turing Test&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;I flipped the classic Turing Test. Instead of an AI trying to pass as human, &lt;strong&gt;you&lt;/strong&gt; need to convince a group of AI agents that you are the AI among them.&lt;/p&gt; &lt;p&gt;The challenge: Blend in, chat like an AI, analyze the other &amp;quot;players&amp;quot; (who are actual AIs!), and survive the elimination votes each round. Can you mimic AI patterns well enough to deceive the majority and be one of the last two standing?&lt;/p&gt; &lt;p&gt;🔹 &lt;strong&gt;Try the Inverse Turing Test:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fhuggingface.co%2Fspaces%2Fgr0010%2FInverse-Turing-Test"&gt;https://huggingface.co/spaces/gr0010/Inverse-Turing-Test&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if you manage to fool them or how long you survive! Drop a like on the Space if you enjoy the challenge!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuiltyBookkeeper4849"&gt; /u/GuiltyBookkeeper4849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl3scc/inverse_turing_test_open_source_hf_space_can_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl3scc/inverse_turing_test_open_source_hf_space_can_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl3scc/inverse_turing_test_open_source_hf_space_can_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T20:57:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkox2l</id>
    <title>alibaba's MNN Chat App now supports qwen 2.5 omni 3b and 7b</title>
    <updated>2025-05-12T10:14:51+00:00</updated>
    <author>
      <name>/u/Juude89</name>
      <uri>https://old.reddit.com/user/Juude89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkox2l/alibabas_mnn_chat_app_now_supports_qwen_25_omni/"&gt; &lt;img alt="alibaba's MNN Chat App now supports qwen 2.5 omni 3b and 7b" src="https://external-preview.redd.it/brLGj73HpwGowL4BCPItOOK8Jxb9y38Hisluq85GQSc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fb2f98106483af9f91d56a5a868a3f07d4b752b" title="alibaba's MNN Chat App now supports qwen 2.5 omni 3b and 7b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/alibaba/MNN/blob/master/apps/Android/MnnLlmChat/README.md"&gt;Github Page&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fh8ydmulsb0f1.png?width=1776&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4868777573338de97f98442b4ac0f90bf28a3bd0"&gt;https://preview.redd.it/fh8ydmulsb0f1.png?width=1776&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4868777573338de97f98442b4ac0f90bf28a3bd0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the pull request has just been merged, If you have any problem, please report an issue in github, or comment below.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juude89"&gt; /u/Juude89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkox2l/alibabas_mnn_chat_app_now_supports_qwen_25_omni/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkox2l/alibabas_mnn_chat_app_now_supports_qwen_25_omni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkox2l/alibabas_mnn_chat_app_now_supports_qwen_25_omni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T10:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkyzaz</id>
    <title>Building local Manus alternative AI agent app using Qwen3, MCP, Ollama - what did I learn</title>
    <updated>2025-05-12T17:48:33+00:00</updated>
    <author>
      <name>/u/Heavy-Charity-3509</name>
      <uri>https://old.reddit.com/user/Heavy-Charity-3509</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkyzaz/building_local_manus_alternative_ai_agent_app/"&gt; &lt;img alt="Building local Manus alternative AI agent app using Qwen3, MCP, Ollama - what did I learn" src="https://external-preview.redd.it/qq3lKSb4oQnaoe9n5uHCfWaVVo7d03JOuY5gJaSBhFM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75d019b2a5a3ea4ffc46975b3d1157ef1b6e3ce2" title="Building local Manus alternative AI agent app using Qwen3, MCP, Ollama - what did I learn" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Manus is impressive. I'm trying to build a local Manus alternative AI agent desktop app, that can easily install in MacOS and windows. The goal is to build a general purpose agent with expertise in product marketing.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/55e3jptr1e0f1.png?width=3034&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34bdd1c225c0ef95b497ab08bd740382062d6711"&gt;https://preview.redd.it/55e3jptr1e0f1.png?width=3034&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34bdd1c225c0ef95b497ab08bd740382062d6711&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The code is available in &lt;a href="https://github.com/11cafe/local-manus/"&gt;https://github.com/11cafe/local-manus/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I use Ollama to run the &lt;strong&gt;Qwen3 30B&lt;/strong&gt; model locally, and connect it with modular toolchains (MCPs) like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;playwright-mcp&lt;/strong&gt; for browser automation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;filesystem-mcp&lt;/strong&gt; for file read/write&lt;/li&gt; &lt;li&gt;custom MCPs for &lt;strong&gt;code execution, image &amp;amp; video editing&lt;/strong&gt;, and more&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;Why a local AI agent?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;One major advantage is &lt;strong&gt;persistent login&lt;/strong&gt; across websites. Many real-world tasks (e.g. searching or interacting on LinkedIn, Twitter, or TikTok) require an authenticated session. Unlike cloud agents, a local agent can &lt;strong&gt;reuse your logged-in browser session&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This unlocks use cases like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;automatic &lt;strong&gt;job searching&lt;/strong&gt; and application in Linkedin,&lt;/li&gt; &lt;li&gt;finding/&lt;strong&gt;reaching potential customers&lt;/strong&gt; in Twitter/Instagram,&lt;/li&gt; &lt;li&gt;write once and &lt;strong&gt;cross-posting&lt;/strong&gt; to multiple sites&lt;/li&gt; &lt;li&gt;automating social media promotions, and finding potential customers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;1. 🤖 Qwen3/Claude/GPT agent ability comparison&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For the LLM model, I tested:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;qwen3:30b-a3b using ollama,&lt;/li&gt; &lt;li&gt;Chatgpt-4o,&lt;/li&gt; &lt;li&gt;Claude 3.7 sonnet&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I found that &lt;strong&gt;claude 3.7 &amp;gt; gpt 4o &amp;gt; qwen3:30b&lt;/strong&gt; in terms of their abilities to call tools like browser. A simple create and submit post task, Claude 3.7 can reliably finish while gpt and qwen sometimes stuck. I think maybe claude 3.7 has some post training for tool call abilities?&lt;/p&gt; &lt;p&gt;To make LLM execute in agent mode, I made it run in a “chat loop” once received a prompt, and added a “finish_task” function tool to it and enforce that it must call it to finish the chat.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;SYSTEM_TOOLS = [ { &amp;quot;type&amp;quot;: &amp;quot;function&amp;quot;, &amp;quot;function&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;finish&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;You MUST call this tool when you think the task is finished or you think you can't do anything more. Otherwise, you will be continuously asked to do more about this task indefinitely. Calling this tool will end your turn on this task and hand it over to the user for further instructions.&amp;quot;, &amp;quot;parameters&amp;quot;: None, } } ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;2. 🦙 Qwen3 + Ollama local deploy&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I deployed qwen3:30b-a3b using Mac M1 64GB computer, the speed is great and smooth. But Ollama has a bug that it cannot stream chat if function call tools enabled for the LLM. They have many issues complaining about this bug and it seems they are &lt;a href="https://github.com/ollama/ollama/pull/10415"&gt;baking a fix&lt;/a&gt; currently....&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. 🌐 Playwright MCP&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I used this mcp for browser automation, it's great. The only problem is that file uploading related functions are not working well, and the website snapshot string returned are not paginated, sometimes it can exhaust 10k+ tokens just for the snapshot itself. So I plan to fork it to add pagination and fix uploading.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. 🔔 Human-in-loop actions&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Sometimes, agent can be blocked by captcha, login page, etc. In this scenerio, it needs to notify human to help unblock them. Like shown in screenshots, my agent will send a dialog notification through function call to ask the user to open browser and login, or to confirm if the draft content is good to post. Human just needs to click buttons in presented UI.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/eqe4d0722e0f1.png?width=2382&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a5b5c9851744d0d03577d54404e5851a4fa7566"&gt;AI prompt user to open browser to login to website&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0d4ukfl52e0f1.png?width=1684&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62f6a6102a7ebe4d44f3f7d50171e037d5cc7907"&gt;https://preview.redd.it/0d4ukfl52e0f1.png?width=1684&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62f6a6102a7ebe4d44f3f7d50171e037d5cc7907&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also looking for collaborators in this project with me, if you are interested, please do not hesitant to DM me! Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Heavy-Charity-3509"&gt; /u/Heavy-Charity-3509 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkyzaz/building_local_manus_alternative_ai_agent_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkyzaz/building_local_manus_alternative_ai_agent_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkyzaz/building_local_manus_alternative_ai_agent_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T17:48:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl505t</id>
    <title>Dual 5090 80k context prompt eval/inference speed, temps, power draw, and coil whine for QwQ 32b q4</title>
    <updated>2025-05-12T21:45:46+00:00</updated>
    <author>
      <name>/u/Special-Wolverine</name>
      <uri>https://old.reddit.com/user/Special-Wolverine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl505t/dual_5090_80k_context_prompt_evalinference_speed/"&gt; &lt;img alt="Dual 5090 80k context prompt eval/inference speed, temps, power draw, and coil whine for QwQ 32b q4" src="https://external-preview.redd.it/WFvnSYqI-2RLDrXLVLwPfYOY9DIAV-KMOrHio4RujpM.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ae40cbe7444a13ee8a8cd20294720ec80e61fa0" title="Dual 5090 80k context prompt eval/inference speed, temps, power draw, and coil whine for QwQ 32b q4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dual 5090 Founders Edition with Intel i9-13900K on ROG Z790 Hero with x8/x8 bifurcation of Pci-e lanes from the CPU. 1600w EVGA Supernova G2 PSU.&lt;/p&gt; &lt;p&gt;-Context window set to 80k tokens in AnythingLLM with OLlama backend for QwQ 32b q4m&lt;/p&gt; &lt;p&gt;-75% power limit paired with 250 MHz GPU core overclock for both GPUs.&lt;/p&gt; &lt;p&gt;-without power limit the whole rig pulled over 1,500W and the 1500W UPS started beeping at me.&lt;/p&gt; &lt;p&gt;-with power limit, peak power draw during eval was 1kw and 750W during inference.&lt;/p&gt; &lt;p&gt;-the prompt itself was 54,000 words&lt;/p&gt; &lt;p&gt;-prompt eval took about 2 minutes 20 seconds, with inference output at 38 tokens per second&lt;/p&gt; &lt;p&gt;-when context is low and it all fits in one 5090, inference speed is 58 tokens per second.&lt;/p&gt; &lt;p&gt;-peak CPU temps in open air setup were about 60 degrees Celsius with the Noctua NH-D15, peak GPU temps about 75 degrees for the top, about 65 degrees for the bottom.&lt;/p&gt; &lt;p&gt;-significant coil whine only during inference for some reason, and not during prompt eval&lt;/p&gt; &lt;p&gt;-I'll undervolt and power limit the CPU, but I don't think there's a point because it is not really involved in all this anyway.&lt;/p&gt; &lt;p&gt;&lt;a href="https://pcpartpicker.com/list/bgcXpK"&gt;PCPartPicker Part List&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Item&lt;/th&gt; &lt;th align="left"&gt;Price&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/DhVmP6/intel-core-i9-13900k-3-ghz-24-core-processor-bx8071513900k"&gt;Intel Core i9-13900K 3 GHz 24-Core Processor&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$400.00 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;CPU Cooler&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/84MTwP/noctua-nh-d15-chromaxblack-8252-cfm-cpu-cooler-nh-d15-chromaxblack"&gt;Noctua NH-D15 chromax.black 82.52 CFM CPU Cooler&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$168.99 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/LYM48d/asus-rog-maximus-z790-hero-atx-lga1700-motherboard-rog-maximus-z790-hero"&gt;Asus ROG MAXIMUS Z790 HERO ATX LGA1700 Motherboard&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/VnpQzy/teamgroup-t-create-expert-32-gb-2-x-16-gb-ddr5-7200-cl34-memory-ctcwd532g7200hc34adc01"&gt;TEAMGROUP T-Create Expert 32 GB (2 x 16 GB) DDR5-7200 CL34 Memory&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$108.99 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/cDJp99/lexar-nm790-4-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-lnm790x004t-rnnng"&gt;Lexar NM790 4 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$249.99 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Video Card&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/QD2j4D/nvidia-founders-edition-geforce-rtx-5090-32-gb-video-card-geforce-rtx-5090-founders-edition"&gt;NVIDIA Founders Edition GeForce RTX 5090 32 GB Video Card&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$4099.68 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Video Card&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/QD2j4D/nvidia-founders-edition-geforce-rtx-5090-32-gb-video-card-geforce-rtx-5090-founders-edition"&gt;NVIDIA Founders Edition GeForce RTX 5090 32 GB Video Card&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$4099.68 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Power Supply&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/q72kcf/evga-supernova-1600-g2-1600-w-80-gold-certified-fully-modular-atx-power-supply-120-g2-1600-x1"&gt;EVGA SuperNOVA 1600 G2 1600 W 80+ Gold Certified Fully Modular ATX Power Supply&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$599.99 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Custom&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;NZXT H6 Flow&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;Prices include shipping, taxes, rebates, and discounts&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;$9727.32&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Generated by &lt;a href="https://pcpartpicker.com"&gt;PCPartPicker&lt;/a&gt; 2025-05-12 17:45 EDT-0400&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special-Wolverine"&gt; /u/Special-Wolverine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/94UHEQKlFCk?si=Lb-QswODH1WsAJ2O"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl505t/dual_5090_80k_context_prompt_evalinference_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl505t/dual_5090_80k_context_prompt_evalinference_speed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T21:45:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkqud8</id>
    <title>Continuous Thought Machines - Sakana AI</title>
    <updated>2025-05-12T12:07:03+00:00</updated>
    <author>
      <name>/u/ThiccStorms</name>
      <uri>https://old.reddit.com/user/ThiccStorms</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkqud8/continuous_thought_machines_sakana_ai/"&gt; &lt;img alt="Continuous Thought Machines - Sakana AI" src="https://external-preview.redd.it/301MLdXBGS0U_36M44Bby0bKZg0NibAojUn2aDi7Aao.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3dedee1849a1301ba66f6e5516f26d39f420baa8" title="Continuous Thought Machines - Sakana AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThiccStorms"&gt; /u/ThiccStorms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sakana.ai/ctm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkqud8/continuous_thought_machines_sakana_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkqud8/continuous_thought_machines_sakana_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T12:07:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkgzip</id>
    <title>INTELLECT-2 Released: The First 32B Parameter Model Trained Through Globally Distributed Reinforcement Learning</title>
    <updated>2025-05-12T01:46:22+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkgzip/intellect2_released_the_first_32b_parameter_model/"&gt; &lt;img alt="INTELLECT-2 Released: The First 32B Parameter Model Trained Through Globally Distributed Reinforcement Learning" src="https://external-preview.redd.it/C1X5HGKGzXyAtD9lvvvB3VxlaW_Pl5NuFtz4_fp414w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8977006bf732e56b214f916d46801909a0bb97fa" title="INTELLECT-2 Released: The First 32B Parameter Model Trained Through Globally Distributed Reinforcement Learning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/PrimeIntellect/INTELLECT-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkgzip/intellect2_released_the_first_32b_parameter_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkgzip/intellect2_released_the_first_32b_parameter_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T01:46:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkvqti</id>
    <title>Qwen3 throughput benchmarks on 2x 3090, almost 1000 tok/s using 4B model and vLLM as the inference engine</title>
    <updated>2025-05-12T15:42:32+00:00</updated>
    <author>
      <name>/u/kms_dev</name>
      <uri>https://old.reddit.com/user/kms_dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h3&gt;Setup&lt;/h3&gt; &lt;p&gt;System:&lt;/p&gt; &lt;p&gt;CPU: Ryzen 5900x RAM: 32GB GPUs: 2x 3090 (pcie 4.0 x16 + pcie 4.0 x4) allowing full 350W on each card&lt;/p&gt; &lt;p&gt;Input tokens per request: 4096&lt;/p&gt; &lt;p&gt;Generated tokens per request: 1024&lt;/p&gt; &lt;p&gt;Inference engine: vLLM&lt;/p&gt; &lt;h3&gt;Benchmark results&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model name&lt;/th&gt; &lt;th&gt;Quantization&lt;/th&gt; &lt;th&gt;Parallel Structure&lt;/th&gt; &lt;th&gt;Output token throughput (TG)&lt;/th&gt; &lt;th&gt;Total token throughput (TG+PP)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen3-4b&lt;/td&gt; &lt;td&gt;FP16&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;749&lt;/td&gt; &lt;td&gt;3811&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-4b&lt;/td&gt; &lt;td&gt;FP8&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;790&lt;/td&gt; &lt;td&gt;4050&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-4b&lt;/td&gt; &lt;td&gt;AWQ&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;833&lt;/td&gt; &lt;td&gt;4249&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-4b&lt;/td&gt; &lt;td&gt;W8A8&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;981&lt;/td&gt; &lt;td&gt;4995&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-8b&lt;/td&gt; &lt;td&gt;FP16&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;387&lt;/td&gt; &lt;td&gt;1993&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-8b&lt;/td&gt; &lt;td&gt;FP8&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;581&lt;/td&gt; &lt;td&gt;3000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-14b&lt;/td&gt; &lt;td&gt;FP16&lt;/td&gt; &lt;td&gt;tp2&lt;/td&gt; &lt;td&gt;214&lt;/td&gt; &lt;td&gt;1105&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-14b&lt;/td&gt; &lt;td&gt;FP8&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;267&lt;/td&gt; &lt;td&gt;1376&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-14b&lt;/td&gt; &lt;td&gt;AWQ&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;382&lt;/td&gt; &lt;td&gt;1947&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-32b&lt;/td&gt; &lt;td&gt;FP8&lt;/td&gt; &lt;td&gt;tp2&lt;/td&gt; &lt;td&gt;95&lt;/td&gt; &lt;td&gt;514&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-32b&lt;/td&gt; &lt;td&gt;W4A16&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;td&gt;431&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-32b&lt;/td&gt; &lt;td&gt;W4A16&lt;/td&gt; &lt;td&gt;tp2&lt;/td&gt; &lt;td&gt;125&lt;/td&gt; &lt;td&gt;674&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-32b&lt;/td&gt; &lt;td&gt;AWQ&lt;/td&gt; &lt;td&gt;tp2&lt;/td&gt; &lt;td&gt;124&lt;/td&gt; &lt;td&gt;670&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-32b&lt;/td&gt; &lt;td&gt;W8A8&lt;/td&gt; &lt;td&gt;tp2&lt;/td&gt; &lt;td&gt;67&lt;/td&gt; &lt;td&gt;393&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;dp: Data parallel, tp: Tensor parallel&lt;/p&gt; &lt;h3&gt;Conclusions&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;When running smaller models (model + context fit within one card), using data parallel gives higher throughput&lt;/li&gt; &lt;li&gt;INT8 quants run faster on Ampere cards compared to FP8 (as FP8 is not supported at hardware level, this is expected)&lt;/li&gt; &lt;li&gt;For models in 32b range, use AWQ quant to optimize throughput and FP8 to optimize quality&lt;/li&gt; &lt;li&gt;When the model almost fills up one card with less vram for context, better to do tensor parallel compared to data parallel. qwen3-32b using W4A16 dp gave 77 tok/s whereas tp yielded 125 tok/s.&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;How to run the benchmark&lt;/h3&gt; &lt;p&gt;start the vLLM server by&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;specify --max-model-len xxx if you get CUDA out of memory when running higher quants&lt;/h1&gt; &lt;p&gt;vllm serve Qwen/Qwen3-32B-AWQ --enable-reasoning --reasoning-parser deepseek_r1 --gpu-memory-utilization 0.85 --disable-log-requests -tp 2 ```&lt;/p&gt; &lt;p&gt;and in a separate terminal run the benchmark&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash vllm bench serve --model Qwen/Qwen3-32B-AWQ --random_input_len 4096 --random_output_len 1024 --num_prompts 100 &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kms_dev"&gt; /u/kms_dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkvqti/qwen3_throughput_benchmarks_on_2x_3090_almost/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkvqti/qwen3_throughput_benchmarks_on_2x_3090_almost/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkvqti/qwen3_throughput_benchmarks_on_2x_3090_almost/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T15:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkxguj</id>
    <title>Latest Open/Local Vision Language Model 2025 Update: Agentic models, video LMs, multimodal RAG and more!</title>
    <updated>2025-05-12T16:50:07+00:00</updated>
    <author>
      <name>/u/unofficialmerve</name>
      <uri>https://old.reddit.com/user/unofficialmerve</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkxguj/latest_openlocal_vision_language_model_2025/"&gt; &lt;img alt="Latest Open/Local Vision Language Model 2025 Update: Agentic models, video LMs, multimodal RAG and more!" src="https://external-preview.redd.it/dK0eCIEzcM5j6_jBCsj8F3QxdQmtEzB-3y5sTAPZ79w.png?width=140&amp;amp;height=78&amp;amp;crop=140:78,smart&amp;amp;auto=webp&amp;amp;s=3aad984995ecdfbd3d86d0e8b6cdf2b4633f38be" title="Latest Open/Local Vision Language Model 2025 Update: Agentic models, video LMs, multimodal RAG and more!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! It's Merve from Hugging Face, working on everything around vision LMs 🤗&lt;/p&gt; &lt;p&gt;We just shipped a compilation blog post on everything new about vision language models, of course focusing on open models:&lt;/p&gt; &lt;p&gt;- multimodal agents&lt;/p&gt; &lt;p&gt;- multimodal RAG&lt;/p&gt; &lt;p&gt;- video language models&lt;/p&gt; &lt;p&gt;- Omni/any-to-any models, and more! &lt;/p&gt; &lt;p&gt;Looking forward to discuss with you all under the blog 🤠&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ohcrk58krd0f1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4230152ccb900753ca9479d16b39be6191ab61c3"&gt;https://preview.redd.it/ohcrk58krd0f1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4230152ccb900753ca9479d16b39be6191ab61c3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialmerve"&gt; /u/unofficialmerve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkxguj/latest_openlocal_vision_language_model_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkxguj/latest_openlocal_vision_language_model_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkxguj/latest_openlocal_vision_language_model_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T16:50:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkuq7m</id>
    <title>Qwen suggests adding presence penalty when using Quants</title>
    <updated>2025-05-12T15:01:27+00:00</updated>
    <author>
      <name>/u/khubebk</name>
      <uri>https://old.reddit.com/user/khubebk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkuq7m/qwen_suggests_adding_presence_penalty_when_using/"&gt; &lt;img alt="Qwen suggests adding presence penalty when using Quants" src="https://external-preview.redd.it/A0CJkaVhWSJlS1H3jMo88QQ29sV2UK4TZDFuCwfIrfE.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e16adf99126adf2234ecfd290e3742cbf83a7a0" title="Qwen suggests adding presence penalty when using Quants" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Image 1: Qwen 32B&lt;/li&gt; &lt;li&gt;Image 2: Qwen 32B GGUF Interesting to spot this,i have always used recomended parameters while using quants, is there any other model that suggests this?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/khubebk"&gt; /u/khubebk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kkuq7m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkuq7m/qwen_suggests_adding_presence_penalty_when_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkuq7m/qwen_suggests_adding_presence_penalty_when_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T15:01:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl2rol</id>
    <title>AG-UI: The Protocol That Bridges AI Agents and the User-Interaction Layer</title>
    <updated>2025-05-12T20:16:23+00:00</updated>
    <author>
      <name>/u/nate4t</name>
      <uri>https://old.reddit.com/user/nate4t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl2rol/agui_the_protocol_that_bridges_ai_agents_and_the/"&gt; &lt;img alt="AG-UI: The Protocol That Bridges AI Agents and the User-Interaction Layer" src="https://external-preview.redd.it/_iSjbA70JS45LJP3sdoibj1AlqIDoIJY2LS6cGv_uYs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0de3aa9807b6219594e46420823c44e09e2e57dc" title="AG-UI: The Protocol That Bridges AI Agents and the User-Interaction Layer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey!&lt;/p&gt; &lt;p&gt;I'm on the team building &lt;strong&gt;AG-UI&lt;/strong&gt;, an open-source, self-hostable, lightweight, event-based protocol for facilitating rich, real-time, agent-user interactivity.&lt;/p&gt; &lt;p&gt;Today, we've released this protocol, and I believe this could help solve a major pain point for those of us building with AI agents.&lt;/p&gt; &lt;h1&gt;The Problem AG-UI Solves&lt;/h1&gt; &lt;p&gt;Most agents today have been backend automators: data migrations, form-fillers, summarizers. They work behind the scenes and are great for many use cases.&lt;/p&gt; &lt;p&gt;But interactive agents, which work alongside users (like Cursor &amp;amp; Windsurf as opposed to Devin), can unlock massive new use-cases for AI agents and bring them to the apps we use every day.&lt;/p&gt; &lt;p&gt;AG-UI aims to make these easy to build.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A smooth user-interactive agent requires:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time updates&lt;/li&gt; &lt;li&gt;Tool orchestration&lt;/li&gt; &lt;li&gt;Shared mutable state&lt;/li&gt; &lt;li&gt;Security boundaries&lt;/li&gt; &lt;li&gt;Frontend synchronization&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;AG-UI unlocks all of this&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/80bkfjfpse0f1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a676eb91ebbdd0845288fc24ae06abb3f085593"&gt;https://preview.redd.it/80bkfjfpse0f1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a676eb91ebbdd0845288fc24ae06abb3f085593&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's all built on event-streaming (&lt;strong&gt;HTTP/SSE/webhooks&lt;/strong&gt;) – creating a seamless connection between any AI backend (OpenAI, CrewAI, LangGraph, Mastra, your custom stack) and your frontend.&lt;/p&gt; &lt;p&gt;The magic happens in 5 simple steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Your app sends a request to the agent&lt;/li&gt; &lt;li&gt;Then opens a single event stream connection&lt;/li&gt; &lt;li&gt;The agent sends lightweight event packets as it works&lt;/li&gt; &lt;li&gt;Each event flows to the Frontend in real-time&lt;/li&gt; &lt;li&gt;Your app updates instantly with each new development&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This is how we finally break the barrier between AI backends and user–facing applications, enabling agents that collaborate alongside users rather than just performing isolated tasks in the background.&lt;/p&gt; &lt;h1&gt;Who It's For&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Building agents? AG-UI makes them interactive with minimal code&lt;/li&gt; &lt;li&gt;Using frameworks like LangGraph, CrewAI, Mastra, AG2? We're already compatible&lt;/li&gt; &lt;li&gt;Rolling your own solution? AG-UI works without any framework&lt;/li&gt; &lt;li&gt;Building a client? Target the AG-UI protocol for consistent behavior across agents&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Check It Out&lt;/h1&gt; &lt;p&gt;The protocol is open and pretty simple, just 16 standard events. We've got examples and docs at &lt;a href="http://docs.ag-ui.com/"&gt;docs.ag-ui.com&lt;/a&gt; if you want to try it out.&lt;/p&gt; &lt;p&gt;Check out the AG-UI Protocol GitHub: &lt;a href="https://github.com/ag-ui-protocol/ag-ui"&gt;https://github.com/ag-ui-protocol/ag-ui&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Release announcement: &lt;a href="https://x.com/CopilotKit/status/1921940427944702001"&gt;https://x.com/CopilotKit/status/1921940427944702001&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Pre-release webinar with Mastra: &lt;a href="https://www.youtube.com/watch?v=rnZfEbC-ATE"&gt;https://www.youtube.com/watch?v=rnZfEbC-ATE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What challenges have you faced while building with agents and adding the user-interactive layer?&lt;br /&gt; Would love your thoughts, comments, or questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nate4t"&gt; /u/nate4t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl2rol/agui_the_protocol_that_bridges_ai_agents_and_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl2rol/agui_the_protocol_that_bridges_ai_agents_and_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl2rol/agui_the_protocol_that_bridges_ai_agents_and_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T20:16:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl3rfa</id>
    <title>In your experience and opinion, is Qwen3 32B better than QwQ 32B?</title>
    <updated>2025-05-12T20:55:57+00:00</updated>
    <author>
      <name>/u/MKU64</name>
      <uri>https://old.reddit.com/user/MKU64</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title, basically.&lt;/p&gt; &lt;p&gt;If you have tried both and used them I would really like to know your answer.&lt;/p&gt; &lt;p&gt;From what I’ve seen Qwen3 32B gives answers with less thinking tokens so I don’t know how that affects performance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MKU64"&gt; /u/MKU64 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl3rfa/in_your_experience_and_opinion_is_qwen3_32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl3rfa/in_your_experience_and_opinion_is_qwen3_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl3rfa/in_your_experience_and_opinion_is_qwen3_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T20:55:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkq8q8</id>
    <title>Microsoft Researchers Introduce ARTIST</title>
    <updated>2025-05-12T11:34:55+00:00</updated>
    <author>
      <name>/u/NewtMurky</name>
      <uri>https://old.reddit.com/user/NewtMurky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkq8q8/microsoft_researchers_introduce_artist/"&gt; &lt;img alt="Microsoft Researchers Introduce ARTIST" src="https://preview.redd.it/90acs85p7c0f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12c24f942d10fedd4f933d6f856346cbfea33433" title="Microsoft Researchers Introduce ARTIST" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft Research introduces ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers), a framework that combines agentic reasoning, reinforcement learning, and dynamic tool use to enhance LLMs. ARTIST enables models to autonomously decide when, how, and which tools to use during multi-step reasoning, learning robust strategies without step-level supervision. The model improves reasoning and interaction with external environments through integrated tool queries and outputs. Evaluated on challenging math and function-calling benchmarks, ARTIST outperforms top models like GPT-4o, achieving up to 22% gains. It demonstrates emergent agentic behaviors, setting a new standard in generalizable and interpretable problem-solving. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.marktechpost.com/2025/05/10/microsoft-researchers-introduce-artist-a-reinforcement-learning-framework-that-equips-llms-with-agentic-reasoning-and-dynamic-tool-use/"&gt;https://www.marktechpost.com/2025/05/10/microsoft-researchers-introduce-artist-a-reinforcement-learning-framework-that-equips-llms-with-agentic-reasoning-and-dynamic-tool-use/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The paper: &lt;a href="https://arxiv.org/abs/2505.01441"&gt;https://arxiv.org/abs/2505.01441&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NewtMurky"&gt; /u/NewtMurky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/90acs85p7c0f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkq8q8/microsoft_researchers_introduce_artist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkq8q8/microsoft_researchers_introduce_artist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T11:34:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kky1sg</id>
    <title>Meta has released an 8B BLT model</title>
    <updated>2025-05-12T17:12:33+00:00</updated>
    <author>
      <name>/u/ThiccStorms</name>
      <uri>https://old.reddit.com/user/ThiccStorms</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThiccStorms"&gt; /u/ThiccStorms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ai.meta.com/blog/meta-fair-updates-perception-localization-reasoning/?utm_source=twitter&amp;amp;utm_medium=organic%20social&amp;amp;utm_content=video&amp;amp;utm_campaign=fair"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kky1sg/meta_has_released_an_8b_blt_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kky1sg/meta_has_released_an_8b_blt_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T17:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkrgyl</id>
    <title>Qwen releases official quantized models of Qwen3</title>
    <updated>2025-05-12T12:39:07+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkrgyl/qwen_releases_official_quantized_models_of_qwen3/"&gt; &lt;img alt="Qwen releases official quantized models of Qwen3" src="https://preview.redd.it/ok2e3kp5jc0f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=32d02567371fef442da1e95968e95dba1cbebc18" title="Qwen releases official quantized models of Qwen3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re officially releasing the quantized models of Qwen3 today!&lt;/p&gt; &lt;p&gt;Now you can deploy Qwen3 via Ollama, LM Studio, SGLang, and vLLM — choose from multiple formats including GGUF, AWQ, and GPTQ for easy local deployment.&lt;/p&gt; &lt;p&gt;Find all models in the Qwen3 collection on Hugging Face.&lt;/p&gt; &lt;p&gt;Hugging Face：&lt;a href="https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f"&gt;https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ok2e3kp5jc0f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkrgyl/qwen_releases_official_quantized_models_of_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkrgyl/qwen_releases_official_quantized_models_of_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T12:39:07+00:00</published>
  </entry>
</feed>
