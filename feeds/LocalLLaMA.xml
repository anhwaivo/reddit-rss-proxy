<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-02T11:35:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lplaqk</id>
    <title>Models to run in browser</title>
    <updated>2025-07-02T03:10:05+00:00</updated>
    <author>
      <name>/u/the100rabh</name>
      <uri>https://old.reddit.com/user/the100rabh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;looking from the community to help me guide to selecting a models which can be run in browser. I see most models being too large to be run in browser. Ideally looking for something under a GB. Any suggestions would be helpful.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/the100rabh"&gt; /u/the100rabh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lplaqk/models_to_run_in_browser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lplaqk/models_to_run_in_browser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lplaqk/models_to_run_in_browser/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T03:10:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpm1k8</id>
    <title>Any recommendations on B200 servers?</title>
    <updated>2025-07-02T03:50:04+00:00</updated>
    <author>
      <name>/u/--pengu--</name>
      <uri>https://old.reddit.com/user/--pengu--</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're finally getting a B200 x8 server. Right now it's between the DGX B200 and ASUS's version. Which one should I go for? Do you have some experience with either of them? Which one would be easier to manage?&lt;/p&gt; &lt;p&gt;p.s. Interestingly, DGX seems to be cheaper. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/--pengu--"&gt; /u/--pengu-- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpm1k8/any_recommendations_on_b200_servers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpm1k8/any_recommendations_on_b200_servers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpm1k8/any_recommendations_on_b200_servers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T03:50:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpqyra</id>
    <title>Drafting RFP answers with Jamba, Mistral, Mixtral</title>
    <updated>2025-07-02T09:00:46+00:00</updated>
    <author>
      <name>/u/NullPointerJack</name>
      <uri>https://old.reddit.com/user/NullPointerJack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing notes in case it helps anyone. I don't often find people talking about models like Jamba and we have access to it, so figure it might be useful.&lt;/p&gt; &lt;p&gt;- &lt;/p&gt; &lt;p&gt;Been testing local models for drafting first-pass answers to internal RFPs. The source material is rough. Basically a mix of PDF exports, old responses in docx, inconsistent product specs, wiki dumps and suchlike.&lt;/p&gt; &lt;p&gt;I'm running a basic RAG pipeline over it using section-level chunking and a semantic search index. Nothing too exotic. Retrieval pulls five chunks per query and I'm prompting each model to answer strictly from the provided input. Tried Jamba, Mistral 7B and Mixtral on the same prompts.&lt;/p&gt; &lt;p&gt;My findings:&lt;/p&gt; &lt;p&gt;Mixtral gave the most natural writing style. Handled formatting like bullet points well, but when chunks were overlapping or contradicting, it sometimes mashed them together. Sounded coherent, but didn't track to any one source.&lt;/p&gt; &lt;p&gt;Mistral played it safer but the answers often felt incomplete. Would stop early or skip chunks if they weren't clearly relevant. Better than Mixtral at avoiding noise but I had to rerun prompts more often to get full coverage.&lt;/p&gt; &lt;p&gt;Jamba was slightly slower and more verbose, but I could actually trace the language back to the retrieved text most of the time. It didn't try to fill in gaps with guesswork and it stayed anchored to the input without inventing policy language. It was more useful in review. Didn't have to figure out where something came from.&lt;/p&gt; &lt;p&gt;Still experimenting with reranking to clean up the retrieval layer. Jamba has been the most consistent in situations where accuracy matters more than polish. Might try pairing it with. post-processing model to tighten up the tone without losing the original source trail.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NullPointerJack"&gt; /u/NullPointerJack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpqyra/drafting_rfp_answers_with_jamba_mistral_mixtral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpqyra/drafting_rfp_answers_with_jamba_mistral_mixtral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpqyra/drafting_rfp_answers_with_jamba_mistral_mixtral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T09:00:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpr5dj</id>
    <title>AKTA - Authenticated Knowledge &amp; Trust Architecture for AI Agents</title>
    <updated>2025-07-02T09:12:50+00:00</updated>
    <author>
      <name>/u/RedDotRocket</name>
      <uri>https://old.reddit.com/user/RedDotRocket</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing a prototype project I built called &amp;quot;Akta&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/RedDotRocket/akta"&gt;https://github.com/RedDotRocket/akta&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's an attempt to enable secure and verifiable auth and delegation between AI agents. It establishes a framework for time-bound capability-based access control, allowing agents to delegate tasks and share resources with fine-grained control. The system leverages concepts from Decentralised Identifiers (DIDs) and Verifiable Credentials (VCs) to create a cryptographically and auditable chain of trust for autonomous agent operations. &lt;/p&gt; &lt;p&gt;In essence, Akta tries to answer what does a &amp;quot;fully autonomous Agent to Agent authorisation grant look like with no humans in the loop&amp;quot;? a.k.a an Agent delegating tasks to another Agent of their own accord. The human presence is derived from their position higher up the chain to their Agents (and the agents they delegate to). There is also a CLI and library for creating keys, vc's, based on A2A AgentCards and their nominated capabilities and skillz!&lt;/p&gt; &lt;p&gt;If you are interested in this idea and want to hack on it with me, let me know. Typical me style, I have way too many uncompleted projects and I am focusing on getting out my main one over the next few weeks. But I do love all this DID stuff and my heart is in this tech, so hopefully this is valuable to someone one out there.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RedDotRocket"&gt; /u/RedDotRocket &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpr5dj/akta_authenticated_knowledge_trust_architecture/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpr5dj/akta_authenticated_knowledge_trust_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpr5dj/akta_authenticated_knowledge_trust_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T09:12:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp8kfw</id>
    <title>Sophgo TPU SC11 FP300, 256GB, 1.1Tb/s, PCIE-5</title>
    <updated>2025-07-01T17:57:16+00:00</updated>
    <author>
      <name>/u/On1ineAxeL</name>
      <uri>https://old.reddit.com/user/On1ineAxeL</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp8kfw/sophgo_tpu_sc11_fp300_256gb_11tbs_pcie5/"&gt; &lt;img alt="Sophgo TPU SC11 FP300, 256GB, 1.1Tb/s, PCIE-5" src="https://external-preview.redd.it/dfaryXk7Svqxh1IPzsdtnwh-Tb9PLhB1df8BVMC0jGM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ed05f87f6e0b04968a548f7e0b236a2438424f8" title="Sophgo TPU SC11 FP300, 256GB, 1.1Tb/s, PCIE-5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.scmp.com/tech/tech-trends/article/3316363/chinese-chipmaker-sophgo-adapts-compute-card-deepseek-beijings-self-reliance-push?module=perpetual_scroll_0&amp;amp;pgtype=article"&gt;https://www.scmp.com/tech/tech-trends/article/3316363/chinese-chipmaker-sophgo-adapts-compute-card-deepseek-beijings-self-reliance-push?module=perpetual_scroll_0&amp;amp;pgtype=article&lt;/a&gt;&lt;/p&gt; &lt;p&gt;SC11 FP300&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8ufk2n2zwaaf1.jpg?width=1453&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=33aa8e8aef095e4db012b08d42ddc4d432e49416"&gt;https://preview.redd.it/8ufk2n2zwaaf1.jpg?width=1453&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=33aa8e8aef095e4db012b08d42ddc4d432e49416&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ktec5womwaaf1.png?width=1459&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c131257e9cd12b940a5780570891ee556d0c9a4"&gt;https://preview.redd.it/ktec5womwaaf1.png?width=1459&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c131257e9cd12b940a5780570891ee556d0c9a4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I didn't find the price, but I found these tables&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/On1ineAxeL"&gt; /u/On1ineAxeL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp8kfw/sophgo_tpu_sc11_fp300_256gb_11tbs_pcie5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp8kfw/sophgo_tpu_sc11_fp300_256gb_11tbs_pcie5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp8kfw/sophgo_tpu_sc11_fp300_256gb_11tbs_pcie5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T17:57:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lprfbx</id>
    <title>Just me, or MNN chat is looping a lot</title>
    <updated>2025-07-02T09:31:20+00:00</updated>
    <author>
      <name>/u/ExtremeAcceptable289</name>
      <uri>https://old.reddit.com/user/ExtremeAcceptable289</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm trying MNN chat but for me it seems to be repeating itself a lot. I tried qwen3 0.6b, and when I try a simple request like&lt;/p&gt; &lt;p&gt;What is lasagna?&lt;/p&gt; &lt;p&gt;Lascange is a dish that is made from pasta. It is a very popular dish in Italy. The main ingredients are pasta and sauce. The sauce is made from various ingredients. It is also a dish that is popular in many countries around the world. It is a very popular dish. It is also a dish that is made from pasta. It is also a dish that is made from various ingredients. It is also a dish that is popular in many countries around the world. It is a very popular dish. It is also a dish that is made from pasta. It is also a dish that is made from various ingredients. It is also a dish that is popular in many countries around the world. It is a very popular dish. It is also a dish that is made from pasta. It is also a dish that is made from various ingredients. It is &lt;/p&gt; &lt;p&gt;Is this an inherent MNN issue or just a model issue?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExtremeAcceptable289"&gt; /u/ExtremeAcceptable289 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lprfbx/just_me_or_mnn_chat_is_looping_a_lot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lprfbx/just_me_or_mnn_chat_is_looping_a_lot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lprfbx/just_me_or_mnn_chat_is_looping_a_lot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T09:31:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpproa</id>
    <title>[Proof of Concept] CoreWeaver – AI Memory Engine for Long-Term Context, Emotional State Tracking, and Branching Timelines</title>
    <updated>2025-07-02T07:37:16+00:00</updated>
    <author>
      <name>/u/Separate-Toe409</name>
      <uri>https://old.reddit.com/user/Separate-Toe409</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve developed a working memory engine for LLM-based chat applications, designed primarily for long-term roleplay and simulation stability. It’s called CoreWeaver, and it’s built to address issues around persistent memory, decision consistency, and emotional context management.&lt;/p&gt; &lt;p&gt;Technical Summary: • Built in JavaScript as a modular plugin • Compatible with SillyTavern and local LLMs • Stores long-term memory entries with metadata (type, emotion, impact) • Tracks emotional pressure over time and influences AI decisions • Supports timeline branching for parallel scenarios or alternate chats • Includes token-optimized compression to reduce memory bloat • Fully character-specific memory folders with timeline control • Reflective decision engine logs choices and emotional drift&lt;/p&gt; &lt;p&gt;Status: • Engine was functional by 06/29/2025 • Currently integrating into a full companion app and testing with OpenAI and free local models via Horde • Codebase is closed-source for now but may offer technical previews later for feedback&lt;/p&gt; &lt;p&gt;My Role: This is a solo project—I built and tested the full framework myself over the past month. I’m currently validating its use in AI companion systems, but I believe it has strong potential for interactive NPC behavior in games, simulation RP, and emotionally consistent storytelling.&lt;/p&gt; &lt;p&gt;Let me know if anyone else is working on similar long-term memory engines. Happy to exchange ideas.&lt;/p&gt; &lt;p&gt;– Mike&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Separate-Toe409"&gt; /u/Separate-Toe409 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpproa/proof_of_concept_coreweaver_ai_memory_engine_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpproa/proof_of_concept_coreweaver_ai_memory_engine_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpproa/proof_of_concept_coreweaver_ai_memory_engine_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T07:37:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpl3mv</id>
    <title>Hosting your local Huanyuan A13B MOE</title>
    <updated>2025-07-02T03:00:05+00:00</updated>
    <author>
      <name>/u/kironlau</name>
      <uri>https://old.reddit.com/user/kironlau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/"&gt; &lt;img alt="Hosting your local Huanyuan A13B MOE" src="https://external-preview.redd.it/gQRnTfcC6aPcmeuf8YDaS3B32gWvzvTmzY9W4xzLeHo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2aec10232e41be314e9b6831041b90cf9d7b4600" title="Hosting your local Huanyuan A13B MOE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/70byco93mdaf1.png?width=2353&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=226d3dc6055ad2ad9c952ed13dca4a1451ae5d2a"&gt;https://preview.redd.it/70byco93mdaf1.png?width=2353&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=226d3dc6055ad2ad9c952ed13dca4a1451ae5d2a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;it is a PR of ik_llama.cpp, by ubergarm , not yet merged.&lt;/p&gt; &lt;p&gt;Instruction to compile, by ubergarm (from: &lt;a href="https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF#note-building-experimental-prs"&gt;ubergarm/Hunyuan-A13B-Instruct-GGUF · Hugging Face&lt;/a&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# get the code setup cd projects git clone https://github.com/ikawrakow/ik_llama.cpp.git git ik_llama.cpp git fetch origin git remote add ubergarm https://github.com/ubergarm/ik_llama.cpp git fetch ubergarm git checkout ug/hunyuan-moe-2 git checkout -b merge-stuff-here git merge ikawrakow/ik/iq3_ks_v2 # build for CUDA cmake -B build -DCMAKE_BUILD_TYPE=Release -DGGML_CUDA=ON -DGGML_VULKAN=OFF -DGGML_RPC=OFF -DGGML_BLAS=OFF -DGGML_CUDA_F16=ON -DGGML_SCHED_MAX_COPIES=1 cmake --build build --config Release -j $(nproc) # clean up later if things get merged into main git checkout main git branch -D merge-stuff-here ``` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GGUF download: &lt;a href="https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF/tree/main"&gt;ubergarm/Hunyuan-A13B-Instruct-GGUF at main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the running command (better read it here, and modified by yourself):&lt;br /&gt; &lt;a href="https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF#note-building-experimental-prs"&gt;ubergarm/Hunyuan-A13B-Instruct-GGUF · Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;a api/webui hosted by ubergarm, for early testing&lt;br /&gt; WebUI: &lt;a href="https://llm.ubergarm.com/"&gt;https://llm.ubergarm.com/&lt;/a&gt;&lt;br /&gt; APIEndpoint: &lt;a href="https://llm.ubergarm.com/"&gt;https://llm.ubergarm.com/&lt;/a&gt; (it is llama-server API endpoint with no API key)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kironlau"&gt; /u/kironlau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T03:00:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lporoz</id>
    <title>EXAONE 4.0 pull request sent to llama.cpp</title>
    <updated>2025-07-02T06:31:43+00:00</updated>
    <author>
      <name>/u/minpeter2</name>
      <uri>https://old.reddit.com/user/minpeter2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lporoz/exaone_40_pull_request_sent_to_llamacpp/"&gt; &lt;img alt="EXAONE 4.0 pull request sent to llama.cpp" src="https://external-preview.redd.it/mGIr9IovxD-Pm48t7v0UR9-dXenko9w6ivUXZu8RxgQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8cb8874cc5f0720f5b8ed06efb123f2b960a2973" title="EXAONE 4.0 pull request sent to llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/minpeter2"&gt; /u/minpeter2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/issues/14474"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lporoz/exaone_40_pull_request_sent_to_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lporoz/exaone_40_pull_request_sent_to_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T06:31:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp653l</id>
    <title>Reuse non-prefix KV Cache and speed up RAG by 3X with LMCache.</title>
    <updated>2025-07-01T16:26:03+00:00</updated>
    <author>
      <name>/u/Nice-Comfortable-650</name>
      <uri>https://old.reddit.com/user/Nice-Comfortable-650</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/"&gt; &lt;img alt="Reuse non-prefix KV Cache and speed up RAG by 3X with LMCache." src="https://preview.redd.it/9eq6ted4haaf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=576ff3381e11049410b474e07e3ae6108a604a27" title="Reuse non-prefix KV Cache and speed up RAG by 3X with LMCache." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey&lt;a href="https://www.reddit.com/r/MachineLearning/"&gt; &lt;/a&gt;&lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;A while back, we shared our open-source project LMCache here and were blown away by the incredible support and feedback. Today, our team is thrilled to share more about one of our core components: &lt;strong&gt;CacheBlend&lt;/strong&gt;. Recognized with a &lt;strong&gt;Best Paper Award at ACM EuroSys 2025,&lt;/strong&gt; this technique is a pain killer for efficient RAG applications &lt;/p&gt; &lt;h1&gt;The Problem: Your KV Cache is Wasting Potential&lt;/h1&gt; &lt;p&gt;In modern LLM applications like RAG and Agents, we constantly feed the model new context. For example, in RAG, we retrieve relevant documents and stuff them into the prompt.&lt;/p&gt; &lt;p&gt;The issue is that this dynamically retrieved context doesn't always appear at the beginning of the input sequence. Traditional KV caching only reuses a &amp;quot;common prefix,&amp;quot; so if the new information isn't at the very start, the cache hit rate plummets, and your GPU ends up recomputing the same things over and over.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Solution: CacheBlend - 100% Hit Rate, No Compromises&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;CacheBlend changes the game by allowing for the reuse of pre-computed KV caches &lt;strong&gt;regardless of their position in the input sequence&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;This means we can finally achieve a &lt;strong&gt;100% KV Cache hit rate&lt;/strong&gt; in applications like RAG. The performance gains are significant:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Faster Time-To-First-Token (TTFT):&lt;/strong&gt; Get your initial response much quicker.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;More Throughput:&lt;/strong&gt; Serve significantly more users with the same hardware.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Almost lossless Output Quality:&lt;/strong&gt; All of this is achieved with little degradation in the model's generation quality.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How does it work?&lt;/h1&gt; &lt;p&gt;CacheBlend intelligently handles the two main challenges of reusing non-prefix caches:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Positional Encoding Update:&lt;/strong&gt; It efficiently updates positional encodings to ensure the model always knows the correct position of each token, even when we're stitching together cached and new data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Selective Attention Recalculation:&lt;/strong&gt; Instead of recomputing everything, it strategically recalculates only the minimal cross-attention needed between the new and cached chunks to maintain perfect generation quality.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For detailed analysis, please refer to the official paper: &lt;a href="https://dl.acm.org/doi/10.1145/3689031.3696098"&gt;https://dl.acm.org/doi/10.1145/3689031.3696098&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Where can I try it?&lt;/h1&gt; &lt;p&gt;Try the newest interactive CacheBlend demo at: &lt;a href="https://github.com/LMCache/LMCache-Examples/tree/main/demo-rag-blending"&gt;https://github.com/LMCache/LMCache-Examples/tree/main/demo-rag-blending&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ask us anything!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nice-Comfortable-650"&gt; /u/Nice-Comfortable-650 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9eq6ted4haaf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T16:26:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpkhdc</id>
    <title>Watch a Photo Come to Life: AI Singing Video via Audio-Driven Animation</title>
    <updated>2025-07-02T02:28:13+00:00</updated>
    <author>
      <name>/u/Deep-Jellyfish6717</name>
      <uri>https://old.reddit.com/user/Deep-Jellyfish6717</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpkhdc/watch_a_photo_come_to_life_ai_singing_video_via/"&gt; &lt;img alt="Watch a Photo Come to Life: AI Singing Video via Audio-Driven Animation" src="https://external-preview.redd.it/dDlyZGM3ZGdnZGFmMd6_yl4KD4SDfLlRzY-8FYANWGWkzq3vBHvX2byZMrhl.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d8adc2f94ef20d963b1c6f6311511954660da6f1" title="Watch a Photo Come to Life: AI Singing Video via Audio-Driven Animation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deep-Jellyfish6717"&gt; /u/Deep-Jellyfish6717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/71tan5dggdaf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpkhdc/watch_a_photo_come_to_life_ai_singing_video_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpkhdc/watch_a_photo_come_to_life_ai_singing_video_via/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T02:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpquz6</id>
    <title>Open source tech from IBM for Compression of models</title>
    <updated>2025-07-02T08:53:40+00:00</updated>
    <author>
      <name>/u/Affectionate-Hat-536</name>
      <uri>https://old.reddit.com/user/Affectionate-Hat-536</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpquz6/open_source_tech_from_ibm_for_compression_of/"&gt; &lt;img alt="Open source tech from IBM for Compression of models" src="https://external-preview.redd.it/NTNoyueW9-UJqjsAanLnjShv3Q-OdUZiW0Di3m0uACc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c76f996ad0aa02724516847f86a59dddb6ea317e" title="Open source tech from IBM for Compression of models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems interesting, I am not clear if the compression is only for storage, transmission or extend to inference too :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Affectionate-Hat-536"&gt; /u/Affectionate-Hat-536 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://research.ibm.com/blog/Zip-NN-AI-compression"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpquz6/open_source_tech_from_ibm_for_compression_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpquz6/open_source_tech_from_ibm_for_compression_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T08:53:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpntxc</id>
    <title>Best RP Models</title>
    <updated>2025-07-02T05:32:25+00:00</updated>
    <author>
      <name>/u/sapry123</name>
      <uri>https://old.reddit.com/user/sapry123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Guys Just wanted to ask what are the latest updates on the Rp Models. Which ones do you use currently and what model do you think is best ones. Please Advice some models above 8B and less than 30B too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sapry123"&gt; /u/sapry123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpntxc/best_rp_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpntxc/best_rp_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpntxc/best_rp_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T05:32:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpmx00</id>
    <title>I built a cli tool to automatically figure out tensor overrides in llama.cpp</title>
    <updated>2025-07-02T04:38:28+00:00</updated>
    <author>
      <name>/u/kevin_1994</name>
      <uri>https://old.reddit.com/user/kevin_1994</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone&lt;/p&gt; &lt;p&gt;Running MoE models on my machine, I'm constantly frustrated working with `--overide-tensor` regexes in llama.cpp. They're hard to maintain, break easily, and are unreadable &lt;/p&gt; &lt;p&gt;I built a little cli tool which builds these `--override-tensor` arguments automatically for your architecture.&lt;/p&gt; &lt;p&gt;On my machine (Xeon e5 2699v3, 128GB DDR4, 2x3090, 1x3060) this runs Qwen3 235B Q4XL at 5.5 tok/s&lt;/p&gt; &lt;pre&gt;&lt;code&gt;#!/bin/bash export CUDA_VISIBLE_DEVICES=2,0,1 # Generate tensor overrides TENSOR_OVERRIDES=$(gguf-tensor-overrider -g https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q4_K_XL/Qwen3-235B-A22B-UD-Q4_K_XL-00001-of-00003.gguf -c 32000 --gpu-percentage 0.85) # Build command with tensor overrides CMD=&amp;quot;/home/kevin/llama.cpp/build/bin/llama-cli \ -hf unsloth/Qwen3-235B-A22B-GGUF:Q4_K_XL \ -c 32000 \ -fa \ -sm row \ $TENSOR_OVERRIDES&amp;quot; # Execute command directly (no pipe) eval &amp;quot;$CMD&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; hey there &amp;lt;think&amp;gt; Okay, the user just said &amp;quot;hey there&amp;quot;. That's pretty casual. I should respond in a friendly and welcoming way. Maybe ask how they're doing and offer help. Let me keep it simple and approachable. I need to make sure the response is open-ended so they feel comfortable to ask anything. Avoid any technical jargon. Just a warm greeting and an offer to assist with whatever they need. Yeah, that should work. &amp;lt;/think&amp;gt; Hello! How can I assist you today? 😊 &amp;gt; llama_perf_sampler_print: sampling time = 15.58 ms / 114 runs ( 0.14 ms per token, 7318.01 tokens per second) llama_perf_context_print: load time = 152623.89 ms llama_perf_context_print: prompt eval time = 1918.59 ms / 10 tokens ( 191.86 ms per token, 5.21 tokens per second) llama_perf_context_print: eval time = 18799.44 ms / 103 runs ( 182.52 ms per token, 5.48 tokens per second) llama_perf_context_print: total time = 30823.94 ms / 113 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;These commands should also work with ik_llama.cpp. 5.5 tok/s is about what I was getting before with ik_llama.cpp.&lt;/p&gt; &lt;p&gt;Here is the link to the repository: &lt;a href="https://github.com/k-koehler/gguf-tensor-overrider/tree/main"&gt;https://github.com/k-koehler/gguf-tensor-overrider&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hopefully some of your find this useful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kevin_1994"&gt; /u/kevin_1994 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T04:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp5nhy</id>
    <title>Gemma 3n Fine-tuning now in Unsloth - 1.5x faster with 50% less VRAM + Fixes</title>
    <updated>2025-07-01T16:07:29+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5nhy/gemma_3n_finetuning_now_in_unsloth_15x_faster/"&gt; &lt;img alt="Gemma 3n Fine-tuning now in Unsloth - 1.5x faster with 50% less VRAM + Fixes" src="https://external-preview.redd.it/Z9xq13sybpzZE8ZKJtOY4SuppgTg5x6rIeOPF_Fl1qk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7593f97dd0c1af68e044aad5a89b7cf7f0e2b642" title="Gemma 3n Fine-tuning now in Unsloth - 1.5x faster with 50% less VRAM + Fixes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey LocalLlama! We made finetuning Gemma 3N 1.5x faster in a free Colab with &lt;a href="http://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; in under 16GB of VRAM! We also managed to find and fix issues for Gemma 3N:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ollama &amp;amp; GGUF fixes&lt;/strong&gt; - All Gemma 3N GGUFs could not load in Ollama properly since &lt;code&gt;per_layer_token_embd&lt;/code&gt; had loading issues. Use our quants in Ollama for our fixes. All dynamic quants in our &lt;a href="https://huggingface.co/collections/unsloth/gemma-3n-685d3874830e49e1c93f9339"&gt;Gemma 3N collection&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NaN and infinities in float16 GPUs&lt;/strong&gt; - we found Conv2D weights (the vision part) have very large magnitudes - we upcast them to float32 to remove infinities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/v2w9vippbaaf1.jpg?width=1888&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9c617026ca9deecc699787547badded628f081bc"&gt;Green crosses are large Conv2D weights&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Free Colab to fine-tune Gemma 3N 4B&lt;/strong&gt; in a free Colab + audio + text + vision inference: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb&lt;/a&gt;-Conversational.ipynb)&lt;/p&gt; &lt;p&gt;Update Unsloth via &lt;code&gt;pip install --upgrade unsloth unsloth_zoo&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from unsloth import FastModel import torch model, tokenizer = FastModel.from_pretrained( model_name = &amp;quot;unsloth/gemma-3n-E4B-it&amp;quot;, max_seq_length = 1024, load_in_4bit = True, full_finetuning = False, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Detailed technical analysis&lt;/strong&gt; and guide on how to use Gemma 3N effectively: &lt;a href="https://docs.unsloth.ai/basics/gemma-3n"&gt;https://docs.unsloth.ai/basics/gemma-3n&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also uploaded GGUFs for the new FLUX model: &lt;a href="https://huggingface.co/unsloth/FLUX.1-Kontext-dev-GGUF"&gt;https://huggingface.co/unsloth/FLUX.1-Kontext-dev-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5nhy/gemma_3n_finetuning_now_in_unsloth_15x_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5nhy/gemma_3n_finetuning_now_in_unsloth_15x_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp5nhy/gemma_3n_finetuning_now_in_unsloth_15x_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T16:07:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lppz8x</id>
    <title>LeCarnet: A French Dataset for Small Language Models</title>
    <updated>2025-07-02T07:51:56+00:00</updated>
    <author>
      <name>/u/Unusual_Shoe2671</name>
      <uri>https://old.reddit.com/user/Unusual_Shoe2671</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lppz8x/lecarnet_a_french_dataset_for_small_language/"&gt; &lt;img alt="LeCarnet: A French Dataset for Small Language Models" src="https://external-preview.redd.it/LbeaoTZsvVnFcs9m3p61Rw_b6plof5ZMIkdYPViyiXk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a64905c33d69e9dd1313750f8b6efb7d4b7b7c4" title="LeCarnet: A French Dataset for Small Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I recently built &lt;strong&gt;LeCarnet&lt;/strong&gt;, a dataset of 2 million French short stories generated with Mistral Large, inspired by the TinyStories project. I also trained three LLaMA-based models from scratch on this dataset: &lt;strong&gt;LeCarnet-3M&lt;/strong&gt;, &lt;strong&gt;LeCarnet-8M&lt;/strong&gt;, and &lt;strong&gt;LeCarnet-21M&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;This dataset contains simple stories with a limited vocabulary, making it ideal for training small language models (SLMs) and for educational purposes.&lt;/p&gt; &lt;p&gt;I've shared the &lt;strong&gt;data generation, training, and evaluation scripts&lt;/strong&gt; as well.&lt;br /&gt; I hope this can be useful to others, feel free to use it, and don't hesitate to leave a star if you find it helpful!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/MaxLSB/LeCarnet"&gt;https://github.com/MaxLSB/LeCarnet&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Models:&lt;/strong&gt; &lt;a href="https://huggingface.co/collections/MaxLSB/lecarnet-683d6b6843023b2c88258594"&gt;https://huggingface.co/collections/MaxLSB/lecarnet-683d6b6843023b2c88258594&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Dataset:&lt;/strong&gt; &lt;a href="https://huggingface.co/datasets/MaxLSB/LeCarnet"&gt;https://huggingface.co/datasets/MaxLSB/LeCarnet&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unusual_Shoe2671"&gt; /u/Unusual_Shoe2671 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MaxLSB/LeCarnet"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lppz8x/lecarnet_a_french_dataset_for_small_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lppz8x/lecarnet_a_french_dataset_for_small_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T07:51:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpoju6</id>
    <title>World's first Intermediate thinking AI model is now Open Source</title>
    <updated>2025-07-02T06:17:24+00:00</updated>
    <author>
      <name>/u/Quiet-Moment-338</name>
      <uri>https://old.reddit.com/user/Quiet-Moment-338</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model Link: &lt;a href="https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview"&gt;https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Launch video: &lt;a href="https://www.youtube.com/watch?v=QMnmcXngoks"&gt;https://www.youtube.com/watch?v=QMnmcXngoks&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet-Moment-338"&gt; /u/Quiet-Moment-338 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoju6/worlds_first_intermediate_thinking_ai_model_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoju6/worlds_first_intermediate_thinking_ai_model_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoju6/worlds_first_intermediate_thinking_ai_model_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T06:17:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpejnj</id>
    <title>Qwen3 inference engine in C: simple, educational, fun</title>
    <updated>2025-07-01T21:49:58+00:00</updated>
    <author>
      <name>/u/adrian-cable</name>
      <uri>https://old.reddit.com/user/adrian-cable</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who may be interested, a free-time project that I've now put up on Github: &lt;a href="https://github.com/adriancable/qwen3.c"&gt;https://github.com/adriancable/qwen3.c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Run Qwen3-architecture models (like Qwen3-4B, or DeepSeek-R1-0528-Qwen3-8B) locally, no GPU required, using an LLM inference engine you build yourself from just 1 file of C source, with no dependencies. Only requirement is enough RAM to load the models. Think llama.cpp but 100X smaller and simpler, although it's still very functional: multi-language input/output, multi-core CPU support, supports reasoning/thinking models etc.&lt;/p&gt; &lt;p&gt;All you need to build and run is Python3 and a C compiler. The C source is so small, it compiles in around a second. Then, go have fun with the models!&lt;/p&gt; &lt;p&gt;After you've played around for a bit, if you already understand a bit about how transformers work but want to really learn the detail, the inference engine's C source (unlike llama.cpp) is small enough to dig into without getting a heart attack. Once you've understood how it ticks, you're a transformers expert! 😃&lt;/p&gt; &lt;p&gt;Not intended to compete with 'heavyweight' engines like llama.cpp, rather, the focus is on being (fun)ctional and educational.&lt;/p&gt; &lt;p&gt;MIT license so you can do whatever you want with the source, no restrictions.&lt;/p&gt; &lt;p&gt;Project will be a success if at least one person here enjoys it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrian-cable"&gt; /u/adrian-cable &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T21:49:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lppg3g</id>
    <title>What's the most complex thing you've been able to (consistently) do with a 4B LLM?</title>
    <updated>2025-07-02T07:15:42+00:00</updated>
    <author>
      <name>/u/noellarkin</name>
      <uri>https://old.reddit.com/user/noellarkin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't mean one-off responses that sound good, I'm thinking more along the lines of: ways in which you've gotten the model working reliably in a workflow or pipeline of some kind, or fine tuned it for a specific task that it performs jus as well as the cloudAI behemoths.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noellarkin"&gt; /u/noellarkin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T07:15:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpm6cv</id>
    <title>ERNIE-4.5-VL-28B-A3B is a hidden gem that can decently tackle challenging chinese/japanese OCR problems.</title>
    <updated>2025-07-02T03:57:24+00:00</updated>
    <author>
      <name>/u/mixivivo</name>
      <uri>https://old.reddit.com/user/mixivivo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpm6cv/ernie45vl28ba3b_is_a_hidden_gem_that_can_decently/"&gt; &lt;img alt="ERNIE-4.5-VL-28B-A3B is a hidden gem that can decently tackle challenging chinese/japanese OCR problems." src="https://b.thumbs.redditmedia.com/A5xBWO7s3WJ33IXN1UsaHXSRINAm2j5ql5UQhM4yZSM.jpg" title="ERNIE-4.5-VL-28B-A3B is a hidden gem that can decently tackle challenging chinese/japanese OCR problems." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;图中文本转录如下：&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;倭王武の上表文&lt;/p&gt; &lt;p&gt;倭・任那・加罗・秦韩・慕韩七国诸军事安东大将军罗・任那・加罗・秦韩・慕韩七国诸军事安东大将军倭国王と称す。顺帝の昇明二年①使遣して上表する。昔して曰く、封国②は偏遗して藩を外に作る。昔より祖祢③躬甲胄揔斡、山川を跋涉して寛处④に进めあず、西は衆夷⑥を服することに六十六国、渡って海北⑦を平くること九十五国。&lt;/p&gt; &lt;p&gt;(宋书 倭国传 原汉文)&lt;/p&gt; &lt;p&gt;①四七八年。②领城、自分の国のこと。③父祖という说とがある。④おちついての最もない。⑤蛭页のこととか。⑦朝鲜半岛のことか。&lt;/p&gt; &lt;p&gt;竖穴式石室の模式図&lt;/p&gt; &lt;p&gt;【日本書紀】【宋書】&lt;/p&gt; &lt;p&gt;倭の五王と天皇&lt;/p&gt; &lt;p&gt;「宋書」倭伝に读・珍(彌)・济・奥・武の五王の名が记されてる。济以下は记纪に伝える尤恭・安康・雄略の各天皇にあてられるが、读には忤神・仁德・履中天皇をあててる诸说がある。珍にも仁德・反正天皇あててる2说がある。&lt;/p&gt; &lt;p&gt;纪にかけてのことである。高句麗の好太王の碑文①には、倭が朝鲜半岛に进出し高句麗と交戦したことが记されている。これは、大和政権が朝鲜半岛の进んだ技术や鉄资源を获得するために加罗(任那)に进出し、そこを拠点として高句麗の势力と对抗したことを物语っている。&lt;/p&gt; &lt;p&gt;「宋书」などには、5世纪初めからほぼ1世纪の间、倭の五王が中国の南朝に朝贡し、高い称号をえようとしたことが记されている。これは中国の皇帝の権威を利用して、朝鲜诸国に対する政治的立场を有利にしようとしたものと考えられる。&lt;/p&gt; &lt;p&gt;朝鲜半岛・中国南朝との交渉をつづじて、大和政権は大陆の进んだ技术と文化をとりいれ、势いを强めた。4世纪末から5世纪にかけての中の古墳は急激に巨大化し、大和政権の最高の首长である大王②の権力が强大化したことを物语っている。&lt;/p&gt; &lt;p&gt;① 好太王(広开土王)一代の事业を记した石碑で、高句麗の都のあった中国吉林省集安県にある。当时の朝鲜半岛の情势を知るための贵重な史料で、そのなかに「百済(百济)」新罗は旧是属民り。由来朝贡す。而るに倭、辛卯の年(391年)よりこのかた、海渡って百済□□□罗を破り、以って臣民とあず、日本の朝鲜半岛への进出を伝えている。&lt;/p&gt; &lt;p&gt;② 熊本県玉名郡菊水町の江田船山古墳出土の大刀铭には「治天下猨□□□罗大王世……」とあり、埼玉県行田市の楢荷山古墳出土の铁劔铭(→p.26図版)にも「倭加多支文大王」ともなる。「大王」は、倭の五王の1人武、记纪（「古事记」「日本书纪」）にワカタケルの名で记録された雄略天皇をさすと考えられる。これらの大刀や铁劔をもつ古墳の被葬者は、大和政権と密接な関系にあったと推测される。&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mixivivo"&gt; /u/mixivivo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lpm6cv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpm6cv/ernie45vl28ba3b_is_a_hidden_gem_that_can_decently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpm6cv/ernie45vl28ba3b_is_a_hidden_gem_that_can_decently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T03:57:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp9gh2</id>
    <title>Huawei releases an open weight model Pangu Pro 72B A16B. Weights are on HF. It should be competitive with Qwen3 32B and it was trained entirely on Huawei Ascend NPUs. (2505.21411)</title>
    <updated>2025-07-01T18:30:51+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/"&gt; &lt;img alt="Huawei releases an open weight model Pangu Pro 72B A16B. Weights are on HF. It should be competitive with Qwen3 32B and it was trained entirely on Huawei Ascend NPUs. (2505.21411)" src="https://external-preview.redd.it/KKUaRRu1NZXsmquOk2Id9DRnEhBD6P6w5Y5xZQur5Yc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd39a4d6488e7f71969bdc8665d7c2dbe902c2b5" title="Huawei releases an open weight model Pangu Pro 72B A16B. Weights are on HF. It should be competitive with Qwen3 32B and it was trained entirely on Huawei Ascend NPUs. (2505.21411)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/IntervitensInc/pangu-pro-moe-model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T18:30:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpl656</id>
    <title>GLM-4.1V-Thinking</title>
    <updated>2025-07-02T03:03:37+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl656/glm41vthinking/"&gt; &lt;img alt="GLM-4.1V-Thinking" src="https://external-preview.redd.it/bgfOhKzxcgalBLIa5eyKdcFfts71dHE0qj65OmHVMu0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d1f66974e5478d143d6f55b57fcf633e79edaf66" title="GLM-4.1V-Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/THUDM/glm-41v-thinking-6862bbfc44593a8601c2578d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl656/glm41vthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl656/glm41vthinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T03:03:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpep3m</id>
    <title>Tenstorrent Blackhole Cards</title>
    <updated>2025-07-01T21:56:17+00:00</updated>
    <author>
      <name>/u/SashaUsesReddit</name>
      <uri>https://old.reddit.com/user/SashaUsesReddit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpep3m/tenstorrent_blackhole_cards/"&gt; &lt;img alt="Tenstorrent Blackhole Cards" src="https://preview.redd.it/ffghybw34caf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7e024c8281faff0ddc04029b2d8b6f4dc59b373" title="Tenstorrent Blackhole Cards" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got in some Blackhole p150b cards! Excited to try these out... Anyone else on here running some of these? Curious to collaborate! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SashaUsesReddit"&gt; /u/SashaUsesReddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ffghybw34caf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpep3m/tenstorrent_blackhole_cards/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpep3m/tenstorrent_blackhole_cards/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T21:56:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lphhj3</id>
    <title>DeepSeek-r1-0528 in top 5 on new SciArena benchmark, the ONLY open-source model</title>
    <updated>2025-07-01T23:59:59+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lphhj3/deepseekr10528_in_top_5_on_new_sciarena_benchmark/"&gt; &lt;img alt="DeepSeek-r1-0528 in top 5 on new SciArena benchmark, the ONLY open-source model" src="https://preview.redd.it/xxfqfefhpcaf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=525931b3ac9b9155ccc34e486fc5f097170ed00c" title="DeepSeek-r1-0528 in top 5 on new SciArena benchmark, the ONLY open-source model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Post: &lt;a href="https://allenai.org/blog/sciarena"&gt;https://allenai.org/blog/sciarena&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Allen AI puts out good work and contributes heavily to open-source, I am a big fan of Nathan Lambert. &lt;/p&gt; &lt;p&gt;They just released this scientific literature research benchmark and DeepSeek-r1-0528 is the &lt;strong&gt;only&lt;/strong&gt; open-source model in the top 5, sharing the pie with the like of OpenAI's o3, Claude 4 Open, and Gemini 2.5 Pro.&lt;/p&gt; &lt;p&gt;I like to trash DeepSeek here, but not anymore. This level of performance is just insane.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xxfqfefhpcaf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lphhj3/deepseekr10528_in_top_5_on_new_sciarena_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lphhj3/deepseekr10528_in_top_5_on_new_sciarena_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T23:59:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpoqlu</id>
    <title>DiffuCoder 7B - New coding diffusion LLM by Apple</title>
    <updated>2025-07-02T06:29:47+00:00</updated>
    <author>
      <name>/u/DunklerErpel</name>
      <uri>https://old.reddit.com/user/DunklerErpel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoqlu/diffucoder_7b_new_coding_diffusion_llm_by_apple/"&gt; &lt;img alt="DiffuCoder 7B - New coding diffusion LLM by Apple" src="https://external-preview.redd.it/WJ1Mt_9K-aAaMAebSityZ71IWIFD1yMghVJOklMW6Xo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed4a0394c2c1d722f620e6214e63d44c79d3e340" title="DiffuCoder 7B - New coding diffusion LLM by Apple" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/apple/DiffuCoder-7B-cpGRPO"&gt;https://huggingface.co/apple/DiffuCoder-7B-cpGRPO&lt;/a&gt; (base and instruct also available)&lt;/p&gt; &lt;p&gt;Currently trying - and failing - to run test it on Colab, but really looking forward to it!&lt;/p&gt; &lt;p&gt;Also, anyone got an idea how I can run it on Apple Silicon?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s19j3dmfneaf1.png?width=1176&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=927e506f764ded47a4e715aea53c223e56ea7ae6"&gt;Benchmarks compared to other coding and diffusion models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2506.20639"&gt;https://arxiv.org/pdf/2506.20639&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DunklerErpel"&gt; /u/DunklerErpel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoqlu/diffucoder_7b_new_coding_diffusion_llm_by_apple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoqlu/diffucoder_7b_new_coding_diffusion_llm_by_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoqlu/diffucoder_7b_new_coding_diffusion_llm_by_apple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T06:29:47+00:00</published>
  </entry>
</feed>
