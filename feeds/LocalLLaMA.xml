<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-28T21:48:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n1uokl</id>
    <title>Anonymizer SLM series: Privacy-first PII replacement models (0.6B/1.7B/4B)</title>
    <updated>2025-08-27T22:05:58+00:00</updated>
    <author>
      <name>/u/Sufficient-Way8060</name>
      <uri>https://old.reddit.com/user/Sufficient-Way8060</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1uokl/anonymizer_slm_series_privacyfirst_pii/"&gt; &lt;img alt="Anonymizer SLM series: Privacy-first PII replacement models (0.6B/1.7B/4B)" src="https://preview.redd.it/1k3v6nm7xmlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=329b15b8f182130c9637a712b1c2e26afd9107af" title="Anonymizer SLM series: Privacy-first PII replacement models (0.6B/1.7B/4B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;Just dropped something I think you'll find interesting - a series of small language models specifically trained for &lt;strong&gt;anonymizing personal data before it leaves your device&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;What these do&lt;/h1&gt; &lt;p&gt;Instead of sending &amp;quot;My name is Sarah and I work at Microsoft making $120k&amp;quot; to Claude/GPT, these models detect PII and replace it with semantically similar alternatives: &amp;quot;My name is Jessica and I work at TechCorp making $112k&amp;quot;. Query intent stays the same, but your real info stays private.&lt;/p&gt; &lt;h1&gt;The models&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;üèÉ‚Äç‚ôÇÔ∏è Anonymizer-0.6B&lt;/strong&gt; - Mobile-optimized, &amp;lt;200ms inference&lt;br /&gt; &lt;strong&gt;‚öñÔ∏è Anonymizer-1.7B&lt;/strong&gt; - Balanced (9.20/10 quality vs GPT-4.1's 9.77/10)&lt;br /&gt; &lt;strong&gt;üéØ Anonymizer-4B&lt;/strong&gt; - Highest accuracy (9.55/10 quality)&lt;/p&gt; &lt;p&gt;All based on Qwen3, trained with GRPO using GPT-4.1 as judge on ~30k anonymization samples.&lt;/p&gt; &lt;p&gt;Most &amp;quot;privacy&amp;quot; solutions either:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Send your data to be anonymized (defeating the purpose)&lt;/li&gt; &lt;li&gt;Use simple regex replacement (breaks context)&lt;/li&gt; &lt;li&gt;Are way too heavy for real-time use&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These are lightweight enough to run as a preprocessing step before your main LLM calls, whether that's local or API-based.&lt;/p&gt; &lt;h1&gt;Currently powers &lt;a href="http://link.freysa.ai/appstore"&gt;Enchanted&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;We're using these in production for an iOS app where users want large open-source models and ChatGPT/Claude quality but with actual privacy. The 1.7B runs great on M-series MacBooks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/eternisai/Anonymizer-0.6B"&gt;Anonymizer-0.6B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/eternisai/Anonymizer-1.7B"&gt;Anonymizer-1.7B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/eternisai/Anonymizer-4B"&gt;Anonymizer-4B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.freysa.ai/blueprint/reinforcement-learning-for-privacy-training-local-models-on-the-anonymization-frontier"&gt;Blog post&lt;/a&gt; with more technical details&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear thoughts on the approach or if anyone's been working on similar privacy-preserving inference setups!&lt;/p&gt; &lt;p&gt;&lt;em&gt;P.S. - Yes, I know there's some irony in using GPT-4.1 to train privacy models, but gotta start somewhere üòÖ&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sufficient-Way8060"&gt; /u/Sufficient-Way8060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1k3v6nm7xmlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1uokl/anonymizer_slm_series_privacyfirst_pii/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1uokl/anonymizer_slm_series_privacyfirst_pii/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T22:05:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2alye</id>
    <title>built a opensource tool that explores your files with deep research like workflow</title>
    <updated>2025-08-28T12:15:31+00:00</updated>
    <author>
      <name>/u/Interesting-Area6418</name>
      <uri>https://old.reddit.com/user/Interesting-Area6418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2alye/built_a_opensource_tool_that_explores_your_files/"&gt; &lt;img alt="built a opensource tool that explores your files with deep research like workflow" src="https://external-preview.redd.it/VjhzNXFqseZP5gshgU7f0eCX58wU9q276mVwsHcHWx8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fae3fa637225d92705332fa1f59996eb6a85201b" title="built a opensource tool that explores your files with deep research like workflow" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zzg6frcr4rlf1.png?width=1286&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=acb614d0a76dc0df628ef411b3066eae5f6eebbe"&gt;research workflow&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/s7dx6h2t4rlf1.gif"&gt;Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;repo - &lt;a href="https://github.com/Datalore-ai/deepdoc"&gt;https://github.com/Datalore-ai/deepdoc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;a while back I released a small open source project and the support it got honestly meant a lot. the feedback and love here keep me building more stuff so thank you for that.&lt;/p&gt; &lt;p&gt;recently I have been working on something new called &lt;strong&gt;DeepDoc&lt;/strong&gt;. it follows a deep research type workflow but on local resources instead of internet. the idea is simple. instead of digging through your own files manually, the tool explores them and hands back a clean report.&lt;/p&gt; &lt;p&gt;you just point it to directory containing local files like pdf, docx etc. it extracts the text, splits it into chunks, runs semantic search, builds a structure based on your instructions and then writes out a markdown report. each section is built step by step by exploring the right pieces, creating research queries, refining with reflection and finally stitching everything into a structured write up.&lt;/p&gt; &lt;p&gt;the result is something that feels like a researched report of your own documents without you having to scroll skim or copy paste.&lt;/p&gt; &lt;p&gt;still early but already works nicely on research papers, reports and even scanned files. planning to push it further soon.&lt;/p&gt; &lt;p&gt;if you want to see what the reports look like just drop a comment or dm me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Area6418"&gt; /u/Interesting-Area6418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2alye/built_a_opensource_tool_that_explores_your_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2alye/built_a_opensource_tool_that_explores_your_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2alye/built_a_opensource_tool_that_explores_your_files/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T12:15:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2dfb9</id>
    <title>LatteReview: a low-code Python package designed to automate systematic literature review processes through AI-powered agents.</title>
    <updated>2025-08-28T14:16:06+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2dfb9/lattereview_a_lowcode_python_package_designed_to/"&gt; &lt;img alt="LatteReview: a low-code Python package designed to automate systematic literature review processes through AI-powered agents." src="https://external-preview.redd.it/MLrpxjPePecFrnekjys9fFomerrkkYi_NRDQObRezwk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3eebd7c41c26f22f39345117cd35c1a6c56b43a8" title="LatteReview: a low-code Python package designed to automate systematic literature review processes through AI-powered agents." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I encountered this project (not mine), it looks really cool:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;LatteReview is a powerful Python package designed to automate academic literature review processes through AI-powered agents. Just like enjoying a cup of latte ‚òï, reviewing numerous research articles should be a pleasant, efficient experience that doesn't consume your entire day!&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Systematic literature reviews and meta-analyses are essential for synthesizing research insights, but they remain time-intensive and labor-intensive due to the iterative processes of screening, evaluation, and data extraction. This paper introduces and evaluates LatteReview, a Python-based framework that leverages large language models (LLMs) and multi-agent systems to automate key elements of the systematic review process. Designed to streamline workflows while maintaining rigor, LatteReview utilizes modular agents for tasks such as title and abstract screening, relevance scoring, and structured data extraction. These agents operate within orchestrated workflows, supporting sequential and parallel review rounds, dynamic decision-making, and iterative refinement based on user feedback.&lt;br /&gt; LatteReview's architecture integrates LLM providers, enabling compatibility with both cloud-based and locally hosted models. The framework supports features such as Retrieval-Augmented Generation (RAG) for incorporating external context, multimodal reviews, Pydantic-based validation for structured inputs and outputs, and asynchronous programming for handling large-scale datasets. The framework is available on the GitHub repository, with detailed documentation and an installable package.&lt;/p&gt; &lt;/blockquote&gt; &lt;ul&gt; &lt;li&gt;Repo: &lt;a href="https://github.com/PouriaRouzrokh/LatteReview"&gt;https://github.com/PouriaRouzrokh/LatteReview&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2501.05468"&gt;https://arxiv.org/abs/2501.05468&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/PouriaRouzrokh/LatteReview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2dfb9/lattereview_a_lowcode_python_package_designed_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2dfb9/lattereview_a_lowcode_python_package_designed_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T14:16:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1y04u</id>
    <title>Using a local LLM as a privacy filter for GPT-4/5 &amp; other cloud models</title>
    <updated>2025-08-28T00:30:33+00:00</updated>
    <author>
      <name>/u/cxu25</name>
      <uri>https://old.reddit.com/user/cxu25</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1y04u/using_a_local_llm_as_a_privacy_filter_for_gpt45/"&gt; &lt;img alt="Using a local LLM as a privacy filter for GPT-4/5 &amp;amp; other cloud models" src="https://external-preview.redd.it/BorDJx5oDYIFbTOFfLwuJI6cZGAtIBXsCgjBUU7XqMY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a731db6e945b9d6c39c93fae6a145bfa55796d93" title="Using a local LLM as a privacy filter for GPT-4/5 &amp;amp; other cloud models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The trade-off between local and cloud LLM is frustrating. Smarts or privacy, which side do you want to sacrifice? My answer is to use a small, fast local model as an intelligent privacy filter for the big cloud models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why the obvious regex redaction doesn't work&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most redaction tools, like &lt;a href="https://langfuse.com/docs/observability/features/masking"&gt;https://langfuse.com/docs/observability/features/masking&lt;/a&gt;, rely on regex. It's fast but brittle. A regex for a US SSN is useless for its UK/Canada counterparts, and there are hundreds of countries with their own ID formats. And how do you write a regex for arbitrary passwords or weirdly formatted API keys? You can't.&lt;/p&gt; &lt;p&gt;Even if you &lt;em&gt;could&lt;/em&gt; perfectly redact everything, you run into a bigger problem. Most tools just swap your data with [REDACTED].&lt;/p&gt; &lt;p&gt;Let's say someone asks AI assistant about a legal document:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Summarize the dispute between &lt;em&gt;John Doe&lt;/em&gt; and &lt;em&gt;Jane Smith&lt;/em&gt; regarding the property at &lt;em&gt;123 Main St&lt;/em&gt;. &lt;em&gt;John&lt;/em&gt;'s wife, &lt;em&gt;Mary Doe&lt;/em&gt;, is also a witness.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Redaction creates this mess:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Summarize the dispute between [REDACTED] and [REDACTED] regarding the property at [REDACTED]. [REDACTED]'s wife, [REDACTED], is also a witness.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The context is destroyed, and the LLM is confused, and you get a garbage response.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fix: Local LLM as a Semantic Gatekeeper&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of regex, we can use a local model to do this intelligently. Here's the workflow I came up with:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Your message sending to cloud LLM is first intercepted locally, like &lt;code&gt;&amp;quot;My patient, Jensen Huang (ID: P12345), needs help...&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;If sensitive data is found, local LLM will create a JSON map, like &lt;code&gt;{&amp;quot;Jensen Huang&amp;quot;: &amp;quot;${PATIENT_NAME}&amp;quot;, &amp;quot;P12345&amp;quot;: &amp;quot;${PATIENT_ID}&amp;quot;}&lt;/code&gt;&lt;/li&gt; &lt;li&gt;The actual message sent to cloud would be &lt;code&gt;&amp;quot;My patient, ${PATIENT_NAME} (ID: ${PATIENT_ID}), needs help...&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Cloud AI assistant respond &lt;code&gt;&amp;quot;Here is what we need to do for ${PATIENT_NAME} ...&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;The response is intercepted locally, to restore back sensitive data placeholders &lt;code&gt;&amp;quot;Here is what we need to do for Jensen Huang ...&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;So you get the final response as &lt;code&gt;&amp;quot;Here is what we need to do for Jensen Huang ...&amp;quot;&lt;/code&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0hdeej8j7olf1.png?width=3685&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d154c9bc8e83ba1a67915e12f71dbcd759558b3"&gt;diagram&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In this way, secrets never leave your machine. The cloud AI gets the semantic context it needs to be useful, but never sees the actual data.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My implementation: PromptMask, a local LLM-based privacy filter for LLMs&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It can be installed as a python package &lt;code&gt;pip install promptmask&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Aiming at seamless integration and user experience, I managed to implement two easy ways for use:&lt;/p&gt; &lt;p&gt;For python developer, it provides a drop-in replacement for the OpenAI SDK &lt;/p&gt; &lt;pre&gt;&lt;code&gt;from promptmask import OpenAIMasked as OpenAI &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For everyone else, if you use apps that connect to an OpenAI-compatible API, you can run a local API gateway.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install &amp;quot;promptmask[web]&amp;quot; promptmask-web &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This spins up a server on localhost:8000. Point your app's API endpoint to &lt;a href="http://localhost:8000/gateway/v1/chat/completions"&gt;http://localhost:8000/gateway/v1/chat/completions&lt;/a&gt;, and in the promptmask config file, add your cloud AI provider URL as upstream, it will automatically handle the masking/unmasking for any tool you use.&lt;/p&gt; &lt;p&gt;PromptMask itself does not include any LLM server, you will need to run a local model with Ollama, llama.cpp, vLLM, etc.&lt;/p&gt; &lt;p&gt;GitHub Repo (MIT Licensed): &lt;a href="https://github.com/cxumol/promptmask"&gt;https://github.com/cxumol/promptmask&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You &lt;strong&gt;don't need a 70B model&lt;/strong&gt; to spot passwords and passport numbers. Together with PromptMask, I built an eval framework and benchmarked a bunch of models. The results show that even ~1B models can do the job with good few-shot prompting. See &lt;a href="https://github.com/cxumol/promptmask/blob/master/eval/benchmark.md"&gt;https://github.com/cxumol/promptmask/blob/master/eval/benchmark.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;---------&lt;/p&gt; &lt;p&gt;For a much deeper dive into the &amp;quot;why&amp;quot; and &amp;quot;how,&amp;quot; including the prompt engineering for small models and the benchmark setup, I wrote a full blog post about it here: &lt;a href="https://xirtam.cxumol.com/promptmask-how-not-give-ai-secrets/"&gt;https://xirtam.cxumol.com/promptmask-how-not-give-ai-secrets/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love to get your feedback on this approach and the tool itself.&lt;/p&gt; &lt;p&gt;Edit: add diagram, formatting, fix typos&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cxu25"&gt; /u/cxu25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1y04u/using_a_local_llm_as_a_privacy_filter_for_gpt45/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1y04u/using_a_local_llm_as_a_privacy_filter_for_gpt45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1y04u/using_a_local_llm_as_a_privacy_filter_for_gpt45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T00:30:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2oi65</id>
    <title>[Guide + Code] Fine-Tuning a Vision-Language Model on a Single GPU (Yes, With Code)</title>
    <updated>2025-08-28T21:14:46+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2oi65/guide_code_finetuning_a_visionlanguage_model_on_a/"&gt; &lt;img alt="[Guide + Code] Fine-Tuning a Vision-Language Model on a Single GPU (Yes, With Code)" src="https://preview.redd.it/v6sxaqq5ttlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=878d7b8c3aafc625f919a07d374939f75a147ba0" title="[Guide + Code] Fine-Tuning a Vision-Language Model on a Single GPU (Yes, With Code)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote a step-by-step guide (with code) on how to fine-tune SmolVLM-256M-Instruct using Hugging Face TRL + PEFT. It covers lazy dataset streaming (no OOM), LoRA/DoRA explained simply, ChartQA for verifiable evaluation, and how to deploy via vLLM. Runs fine on a single consumer GPU like a 3060/4070.&lt;/p&gt; &lt;p&gt;Guide: &lt;a href="https://pavankunchalapk.medium.com/the-definitive-guide-to-fine-tuning-a-vision-language-model-on-a-single-gpu-with-code-79f7aa914fc6?utm_source=chatgpt.com"&gt;https://pavankunchalapk.medium.com/the-definitive-guide-to-fine-tuning-a-vision-language-model-on-a-single-gpu-with-code-79f7aa914fc6&lt;/a&gt;&lt;br /&gt; Code: &lt;a href="https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/vllm-fine-tuning-smolvlm?utm_source=chatgpt.com"&gt;https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/vllm-fine-tuning-smolvlm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also ‚Äî I‚Äôm open to roles! Hands-on with real-time pose estimation, LLMs, and deep learning architectures. Resume: &lt;a href="https://pavan-portfolio-tawny.vercel.app/"&gt;https://pavan-portfolio-tawny.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v6sxaqq5ttlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2oi65/guide_code_finetuning_a_visionlanguage_model_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2oi65/guide_code_finetuning_a_visionlanguage_model_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T21:14:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2dmku</id>
    <title>Achieving 80% task completion: Training LLMs to actually USE tools</title>
    <updated>2025-08-28T14:23:43+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently worked on a LoRA that improves tool use in LLM. Thought the approach might interest folks here.&lt;/p&gt; &lt;p&gt;The issue I have had when trying to use some of the local LLMs with coding agents is this:&lt;/p&gt; &lt;p&gt;Me: &amp;quot;Find all API endpoints with authentication in this codebase&amp;quot; LLM: &amp;quot;You should look for @app.route decorators and check if they have auth middleware...&amp;quot;&lt;/p&gt; &lt;p&gt;But I often want it to search the files and show me but the LLM doesn't trigger a tool use call.&lt;/p&gt; &lt;p&gt;To fine-tune it for tool use I combined two data sources:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Magpie scenarios&lt;/strong&gt; - 5000+ diverse tasks (bug hunting, refactoring, security audits)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real execution&lt;/strong&gt; - Ran these on actual repos (FastAPI, Django, React) to get authentic tool responses&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This ensures the model learns both breadth (many scenarios) and depth (real tool behavior).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tools We Taught&lt;/strong&gt; - &lt;code&gt;read_file&lt;/code&gt; - Actually read file contents - &lt;code&gt;search_files&lt;/code&gt; - Regex/pattern search across codebases - &lt;code&gt;find_definition&lt;/code&gt; - Locate classes/functions - &lt;code&gt;analyze_imports&lt;/code&gt; - Dependency tracking - &lt;code&gt;list_directory&lt;/code&gt; - Explore structure - &lt;code&gt;run_tests&lt;/code&gt; - Execute test suites&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Improvements&lt;/strong&gt; - Tool calling accuracy: 12% ‚Üí 80% - Correct parameters: 8% ‚Üí 87% - Multi-step tasks: 3% ‚Üí 78% - End-to-end completion: 5% ‚Üí 80% - Tools per task: 0.2 ‚Üí 3.8&lt;/p&gt; &lt;p&gt;The LoRA really improves on intential tool call as an example consider the query: &amp;quot;Find ValueError in payment module&amp;quot;&lt;/p&gt; &lt;p&gt;The response proceeds as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Calls &lt;code&gt;search_files&lt;/code&gt; with pattern &amp;quot;ValueError&amp;quot;&lt;/li&gt; &lt;li&gt;Gets 4 matches across 3 files&lt;/li&gt; &lt;li&gt;Calls &lt;code&gt;read_file&lt;/code&gt; on each match&lt;/li&gt; &lt;li&gt;Analyzes context&lt;/li&gt; &lt;li&gt;Reports: &amp;quot;Found 3 ValueError instances: payment/processor.py:47 for invalid amount, payment/validator.py:23 for unsupported currency...&amp;quot;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Resources&lt;/strong&gt; - &lt;a href="https://colab.research.google.com/github/codelion/ellora/blob/main/Ellora_Recipe_3_Enhanced_Tool_Calling_and_Code_Understanding.ipynb"&gt;Colab notebook&lt;/a&gt; - &lt;a href="https://huggingface.co/codelion/Llama-3.2-1B-Instruct-tool-calling-lora"&gt;Model&lt;/a&gt; - &lt;a href="https://github.com/codelion/ellora"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The key for this LoRA was combining synthetic diversity with real execution. Pure synthetic data leads to models that format tool calls correctly but use them inappropriately. Real execution teaches actual tool strategy.&lt;/p&gt; &lt;p&gt;What's your experience with tool-calling models? Any tips for handling complex multi-step workflows?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2dmku/achieving_80_task_completion_training_llms_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2dmku/achieving_80_task_completion_training_llms_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2dmku/achieving_80_task_completion_training_llms_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T14:23:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2bdal</id>
    <title>Llama.cpp --verbose</title>
    <updated>2025-08-28T12:50:27+00:00</updated>
    <author>
      <name>/u/Secure_Reflection409</name>
      <uri>https://old.reddit.com/user/Secure_Reflection409</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed something a bit weird?&lt;/p&gt; &lt;p&gt;Qwen coder famously doesn't work in roo. I used --verbose on LCP to try and capture the exact failure but IT NEVER FAILS WHEN VERBOSE IS ON?!&lt;/p&gt; &lt;p&gt;In fact, it works flawlessly. So flawlessly, I believed Devstral had fixed the chat template for me in one prompt.&lt;/p&gt; &lt;p&gt;Now I feel silly.&lt;/p&gt; &lt;p&gt;How exactly is --verbose smoothing over the chat template difficulties? It feels like verbose enables something extra?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Reflection409"&gt; /u/Secure_Reflection409 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2bdal/llamacpp_verbose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2bdal/llamacpp_verbose/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2bdal/llamacpp_verbose/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T12:50:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n21tb6</id>
    <title>Using Qwen to generate 3D assets in Blender</title>
    <updated>2025-08-28T03:33:45+00:00</updated>
    <author>
      <name>/u/spacespacespapce</name>
      <uri>https://old.reddit.com/user/spacespacespapce</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n21tb6/using_qwen_to_generate_3d_assets_in_blender/"&gt; &lt;img alt="Using Qwen to generate 3D assets in Blender" src="https://external-preview.redd.it/9uCJCtXh3hjjUjvHiYJX5nSL5ngxbe-WclVIpprEgOA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9cd1fe81e5b3dea5c4706e7a6a97566c3d8f5c9" title="Using Qwen to generate 3D assets in Blender" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Working on an AI agent that hooks up to Blender to generate low poly models. So far I'm impressed by Qwen's ability to generate and think usable code for this. Inspired by indie game dev where I constantly needed quick models for placeholders or prototyping. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spacespacespapce"&gt; /u/spacespacespapce &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imgur.com/a/qzOMpqr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n21tb6/using_qwen_to_generate_3d_assets_in_blender/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n21tb6/using_qwen_to_generate_3d_assets_in_blender/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T03:33:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1paeu</id>
    <title>What you think it will be..</title>
    <updated>2025-08-27T18:37:50+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1paeu/what_you_think_it_will_be/"&gt; &lt;img alt="What you think it will be.." src="https://preview.redd.it/68wbznvowllf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f674727eff3b01ce03cc3be22d7a1f41fa83009d" title="What you think it will be.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/68wbznvowllf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1paeu/what_you_think_it_will_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1paeu/what_you_think_it_will_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T18:37:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2jqym</id>
    <title>[EMNLP 2025] CCPS: Confidence from Consistency under Perturbation of States ‚Äî Superior Calibration Performance Across Benchmarks/Models</title>
    <updated>2025-08-28T18:11:39+00:00</updated>
    <author>
      <name>/u/erfan_mhi</name>
      <uri>https://old.reddit.com/user/erfan_mhi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Our paper &lt;strong&gt;‚Äú&lt;/strong&gt;&lt;strong&gt;&lt;em&gt;Calibrating LLM Confidence by Probing Perturbed Representation Stability&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;‚Äù&lt;/strong&gt; was accepted to the &lt;strong&gt;EMNLP 2025 Main Conference&lt;/strong&gt;, placing in the &lt;strong&gt;top 15% of accepted papers&lt;/strong&gt; with a &lt;strong&gt;final meta-review rating of 9 (strong accept)&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;üîç Motivation&lt;/h1&gt; &lt;p&gt;LLMs don‚Äôt just make mistakes, they‚Äôre often confidently wrong. That‚Äôs fine when asking for trivia, but risky in domains like healthcare and finance. Reliable confidence estimation is critical for safe deployment.&lt;/p&gt; &lt;h1&gt;‚ú® What is CCPS?&lt;/h1&gt; &lt;p&gt;CCPS looks at the hidden states of an LLM. We apply small perturbations to the final hidden representations and observe how stable the prediction is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If the answer remains stable ‚Üí the model was truly confident.&lt;/li&gt; &lt;li&gt;If the answer flips ‚Üí the confidence was unreliable.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This approach is simple, efficient, and does not require fine-tuning the base LLM.&lt;/p&gt; &lt;h1&gt;üìä Results&lt;/h1&gt; &lt;p&gt;Across LLaMA, Mistral, and Qwen on MMLU and MMLU-Pro, CCPS outperformed prior methods like LitCab and Calibration Tuning (CT):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Calibration&lt;/strong&gt;: Error cut by more than 50%, down to ~4.5% on the toughest benchmarks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discrimination&lt;/strong&gt;: More accurate at telling right vs. wrong answers than prior SOTA (LitCab, CT, etc.).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Boosts accuracy and robustness, all without fine-tuning the base LLM.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üí° Why it matters&lt;/h1&gt; &lt;p&gt;CCPS delivers more reliable, better-calibrated LLMs, models that don‚Äôt just generate answers but also provide trustworthy confidence signals. This is key for high-stakes AI applications, especially in the medical and finance industries.&lt;/p&gt; &lt;h1&gt;üìé Resources&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;üìÑ Paper: &lt;a href="https://arxiv.org/abs/2505.21772"&gt;arXiv link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üíª Code: &lt;a href="https://github.com/ledengary/CCPS"&gt;GitHub repo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìä Data: &lt;a href="https://huggingface.co/datasets/ledengary/CCPS"&gt;HF Dataset&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to hear feedback, especially from anyone working on calibration, verifiers (for RL), or LLM deployment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/erfan_mhi"&gt; /u/erfan_mhi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2jqym/emnlp_2025_ccps_confidence_from_consistency_under/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2jqym/emnlp_2025_ccps_confidence_from_consistency_under/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2jqym/emnlp_2025_ccps_confidence_from_consistency_under/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T18:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2kh2y</id>
    <title>Battle of the new Multi-Modal models: MiniCPM-V 4.5 8B vs InternVL3.5 8B</title>
    <updated>2025-08-28T18:39:21+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EDIT - Added GLM-4.1V 9B scores.&lt;/p&gt; &lt;p&gt;New multimodal models based off Qwen3, MiniCPM and InternVL, were released very recently, as in just a few days ago, which got me interested and wondering which were better.&lt;/p&gt; &lt;p&gt;Unfortunately, InternVL3.5's model card did not include benchmark results for the 8B model, they only posted results for the 30b-a3b model and the 240b-a20b models, which make it hard to compare their 8B model to minicpm-v 4.5 8b. Doing a little digging, and reading through their paper on axiv &lt;a href="https://arxiv.org/html/2508.18265v1"&gt;https://arxiv.org/html/2508.18265v1&lt;/a&gt; I was able to find benchmark results for their 8B model, and more luckily, results for their older InternVL3 8B model &lt;em&gt;which is&lt;/em&gt; also available in the MiniCPM model card. This gives me a way to cross check that I am comparing the correct results from their corresponding tests accurately (although this did end up creating a significant amount of work for me).&lt;/p&gt; &lt;p&gt;&lt;em&gt;\&lt;/em&gt;MME not included in average or geomean score for obvious reasons (the values are too large and will throw off the weighting)*&lt;/p&gt; &lt;p&gt;&lt;em&gt;\&lt;/em&gt;*Mantis not included in average or geomean cause GLM4.1V did not have results for this*&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;InternVL3.5-8B&lt;/th&gt; &lt;th align="left"&gt;MiniCPM-V 4.5-8B&lt;/th&gt; &lt;th align="left"&gt;GLM-4.1V-9B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt; MMMU (val)&lt;/td&gt; &lt;td align="left"&gt;73.4&lt;/td&gt; &lt;td align="left"&gt;67.7&lt;/td&gt; &lt;td align="left"&gt;68&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; MathVista (mini)&lt;/td&gt; &lt;td align="left"&gt;78.4&lt;/td&gt; &lt;td align="left"&gt;79.9&lt;/td&gt; &lt;td align="left"&gt;80.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AI2D&lt;/td&gt; &lt;td align="left"&gt;84&lt;/td&gt; &lt;td align="left"&gt;86.5&lt;/td&gt; &lt;td align="left"&gt;87.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; TextVQA (val)&lt;/td&gt; &lt;td align="left"&gt;78.2&lt;/td&gt; &lt;td align="left"&gt;82.2&lt;/td&gt; &lt;td align="left"&gt;79.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; DocVQA (test)&lt;/td&gt; &lt;td align="left"&gt;92.3&lt;/td&gt; &lt;td align="left"&gt;94.7&lt;/td&gt; &lt;td align="left"&gt;93.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; OCR Bench&lt;/td&gt; &lt;td align="left"&gt;83.2&lt;/td&gt; &lt;td align="left"&gt;89&lt;/td&gt; &lt;td align="left"&gt;82.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; Mantis Eval**&lt;/td&gt; &lt;td align="left"&gt;70.5&lt;/td&gt; &lt;td align="left"&gt;82.5&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; MMT (val)&lt;/td&gt; &lt;td align="left"&gt;66.7&lt;/td&gt; &lt;td align="left"&gt;68.3&lt;/td&gt; &lt;td align="left"&gt;68.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; MME (sum)*&lt;/td&gt; &lt;td align="left"&gt;2380.6&lt;/td&gt; &lt;td align="left"&gt;2500&lt;/td&gt; &lt;td align="left"&gt;2445.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; MMB v1.1 (EN)&lt;/td&gt; &lt;td align="left"&gt;79.5&lt;/td&gt; &lt;td align="left"&gt;84.2&lt;/td&gt; &lt;td align="left"&gt;85.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; MMVet (turbo)&lt;/td&gt; &lt;td align="left"&gt;83.1&lt;/td&gt; &lt;td align="left"&gt;75.5&lt;/td&gt; &lt;td align="left"&gt;66.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMStar&lt;/td&gt; &lt;td align="left"&gt;69.3&lt;/td&gt; &lt;td align="left"&gt;72.1&lt;/td&gt; &lt;td align="left"&gt;72.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; HallBench (avg)&lt;/td&gt; &lt;td align="left"&gt;54.5&lt;/td&gt; &lt;td align="left"&gt;61.2&lt;/td&gt; &lt;td align="left"&gt;63.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; Video-MME (w/o sub)&lt;/td&gt; &lt;td align="left"&gt;66&lt;/td&gt; &lt;td align="left"&gt;67.9&lt;/td&gt; &lt;td align="left"&gt;68.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; Video-MME (w sub)&lt;/td&gt; &lt;td align="left"&gt;68.6&lt;/td&gt; &lt;td align="left"&gt;73.5&lt;/td&gt; &lt;td align="left"&gt;73.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; MLVU (M-Avg)&lt;/td&gt; &lt;td align="left"&gt;70.2&lt;/td&gt; &lt;td align="left"&gt;75.1&lt;/td&gt; &lt;td align="left"&gt;71.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LongVideoBench (val total)&lt;/td&gt; &lt;td align="left"&gt;62.1&lt;/td&gt; &lt;td align="left"&gt;63.9&lt;/td&gt; &lt;td align="left"&gt;44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Average&lt;/td&gt; &lt;td align="left"&gt;73.75&lt;/td&gt; &lt;td align="left"&gt;76.51&lt;/td&gt; &lt;td align="left"&gt;73.72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Geomean&lt;/td&gt; &lt;td align="left"&gt;73.15&lt;/td&gt; &lt;td align="left"&gt;75.95&lt;/td&gt; &lt;td align="left"&gt;72.69&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2kh2y/battle_of_the_new_multimodal_models_minicpmv_45/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2kh2y/battle_of_the_new_multimodal_models_minicpmv_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2kh2y/battle_of_the_new_multimodal_models_minicpmv_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T18:39:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ixp0</id>
    <title>L3.3-Ignition-v0.1-70B - New Model Merge</title>
    <updated>2025-08-28T17:41:28+00:00</updated>
    <author>
      <name>/u/realechelon</name>
      <uri>https://old.reddit.com/user/realechelon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ignition v0.1 is a Llama 3.3-based model merge designed for &lt;strong&gt;creative roleplay&lt;/strong&gt; and &lt;strong&gt;fiction writing&lt;/strong&gt; purposes. The model underwent a multi-stage merge process designed to optimise for creative writing capability, minimising slop, and improving coherence when compared with its constituent models.&lt;/p&gt; &lt;p&gt;The model shows a preference for &lt;strong&gt;detailed character cards&lt;/strong&gt; and is &lt;strong&gt;sensitive to system prompting&lt;/strong&gt;. If you want a specific behavior from the model, prompt for it directly.&lt;/p&gt; &lt;p&gt;Inferencing has been tested at fp8 and fp16, and &lt;strong&gt;both are coherent up to ~64k context&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I'm running the following sampler settings. If you find the model isn't working at all, try these to see if the problem is your settings:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt Template&lt;/strong&gt;: Llama 3 &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Temperature&lt;/strong&gt;: 0.75 (this model runs pretty hot) &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Min-P&lt;/strong&gt;: 0.03 &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rep Pen&lt;/strong&gt;: 1.03 &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rep Pen Range&lt;/strong&gt;: 1536&lt;/p&gt; &lt;p&gt;High temperature settings (above 0.8) tend to create less coherent responses.&lt;/p&gt; &lt;p&gt;Huggingface: &lt;a href="https://huggingface.co/invisietch/L3.3-Ignition-v0.1-70B"&gt;https://huggingface.co/invisietch/L3.3-Ignition-v0.1-70B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;GGUF: &lt;a href="https://huggingface.co/mradermacher/L3.3-Ignition-v0.1-70B-GGUF"&gt;https://huggingface.co/mradermacher/L3.3-Ignition-v0.1-70B-GGUF&lt;/a&gt; &lt;/p&gt; &lt;p&gt;GGUF (iMat): &lt;a href="https://huggingface.co/mradermacher/L3.3-Ignition-v0.1-70B-i1-GGUF"&gt;https://huggingface.co/mradermacher/L3.3-Ignition-v0.1-70B-i1-GGUF&lt;/a&gt; (SOON)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/realechelon"&gt; /u/realechelon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ixp0/l33ignitionv0170b_new_model_merge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ixp0/l33ignitionv0170b_new_model_merge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ixp0/l33ignitionv0170b_new_model_merge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T17:41:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2mhh2</id>
    <title>Radeon RX9070/Radeon AI PRO R9700 updated vLLM image</title>
    <updated>2025-08-28T19:56:22+00:00</updated>
    <author>
      <name>/u/nuzaihan</name>
      <uri>https://old.reddit.com/user/nuzaihan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2mhh2/radeon_rx9070radeon_ai_pro_r9700_updated_vllm/"&gt; &lt;img alt="Radeon RX9070/Radeon AI PRO R9700 updated vLLM image" src="https://b.thumbs.redditmedia.com/CvRnUICxNL5mF7nH4DAGTiw0Na8DqgJYzYd7tQ5ACKY.jpg" title="Radeon RX9070/Radeon AI PRO R9700 updated vLLM image" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Optimized vLLM for AMD Radeon 9070 (RDNA gfx1201 architecture) and theoretically, including the new, just released this month - Radeon PRO AI R9700 (since it's gfx1201) as well. (only for gfx1201, i do not have the time to build for others) &lt;/p&gt; &lt;p&gt;Took me almost a week after stumbling to bugs in ROCm 6.4.1 that caused problems training AI models with unsloth and now it works perfectly. &lt;/p&gt; &lt;p&gt;Also updated the image from Ubuntu from 22.04 LTS to 24.04 LTS, latest libBlaslt, pytorch, rccl, triton, ROCm 6.4.3, vLLM &lt;a href="http://0.10.1.1"&gt;0.10.1.1&lt;/a&gt; etc and remove the bloat like CDNA specific configuration, to make it a lot lighter. &lt;/p&gt; &lt;p&gt;The Docker image can be pulled here: &lt;a href="https://hub.docker.com/r/muhammadn/vllm-rocm"&gt;https://hub.docker.com/r/muhammadn/vllm-rocm&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Latest Unsloth works as well, had been training some models using this docker image.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l82d7su4ftlf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ba382bb83f438f73e1b68c412d3cd9aca1754ab5"&gt;https://preview.redd.it/l82d7su4ftlf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ba382bb83f438f73e1b68c412d3cd9aca1754ab5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rgr4lgx4ftlf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c06b2aaf62bae9e5107137186c135492814d33d"&gt;https://preview.redd.it/rgr4lgx4ftlf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c06b2aaf62bae9e5107137186c135492814d33d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1ekbtru4ftlf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f43eb69f10151ed171c01fb439fdc139582808b0"&gt;https://preview.redd.it/1ekbtru4ftlf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f43eb69f10151ed171c01fb439fdc139582808b0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uln87ru4ftlf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4d2bd4f7f60d9ca36d0ffa10233e12eaa23818b9"&gt;https://preview.redd.it/uln87ru4ftlf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4d2bd4f7f60d9ca36d0ffa10233e12eaa23818b9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7fdiztu4ftlf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e630ffd43be1d7e07049b15aa20d7eef4c95348b"&gt;https://preview.redd.it/7fdiztu4ftlf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e630ffd43be1d7e07049b15aa20d7eef4c95348b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuzaihan"&gt; /u/nuzaihan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2mhh2/radeon_rx9070radeon_ai_pro_r9700_updated_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2mhh2/radeon_rx9070radeon_ai_pro_r9700_updated_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2mhh2/radeon_rx9070radeon_ai_pro_r9700_updated_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T19:56:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2p2wi</id>
    <title>85% of Nvidia's $46.7 billion revenue last quarter came from just 6 companies.</title>
    <updated>2025-08-28T21:37:34+00:00</updated>
    <author>
      <name>/u/vergogn</name>
      <uri>https://old.reddit.com/user/vergogn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2p2wi/85_of_nvidias_467_billion_revenue_last_quarter/"&gt; &lt;img alt="85% of Nvidia's $46.7 billion revenue last quarter came from just 6 companies." src="https://preview.redd.it/k0279pnmxtlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e282ac0e96e904a51aa3f0f7e514a47b6d02ed2" title="85% of Nvidia's $46.7 billion revenue last quarter came from just 6 companies." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vergogn"&gt; /u/vergogn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k0279pnmxtlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2p2wi/85_of_nvidias_467_billion_revenue_last_quarter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2p2wi/85_of_nvidias_467_billion_revenue_last_quarter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T21:37:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1n28n3v</id>
    <title>Sparrow: Custom language model architecture for microcontrollers like the ESP32</title>
    <updated>2025-08-28T10:31:54+00:00</updated>
    <author>
      <name>/u/c-f_i</name>
      <uri>https://old.reddit.com/user/c-f_i</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n28n3v/sparrow_custom_language_model_architecture_for/"&gt; &lt;img alt="Sparrow: Custom language model architecture for microcontrollers like the ESP32" src="https://external-preview.redd.it/dnB2ZnpraGdrcWxmMXI7s2J-dYT-lngU-7I3sc5b7CKL3t5WhtAsvCq_0YDI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e51762114b8f90c87d5be2a46288d85439d46b36" title="Sparrow: Custom language model architecture for microcontrollers like the ESP32" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Above is a video of Sparrow LM running on 1 core of the ESP32S3 while another core dedicated to the webserver/webapp, to showcase a ChatGPT-like system, although of course the models can be used for anything from text to sentiment analysis, time series analysis and more, depending how it is trained.&lt;/p&gt; &lt;p&gt;I've been super focused for a while now in bringing Language Models and complex NLP capabilities to microcontrollers and finally been able to finish the architecture and an ML Toolkit that enables training models from scratch, with this architecture and enables easy deployment on almost any MCUs.&lt;/p&gt; &lt;p&gt;The architecture uses state of the art methods, with many in-depth optimisations tested through over 1700 trained models, to get the most of every single memory byte and clock cycle, specifically for MCUs while also enabling extremely fast responses on PC.&lt;/p&gt; &lt;p&gt;The idea is to have domain specific and task specific models, using Sparrow's architecture, instead of a general prupose frontier model like ChatGPT/Llama etc. In the demo I showcase a Biology only model, that was made to give straight answrs (as per research papers showcasing that's what people want) for a question-answering chat-like system. Anything can be created. And then due to the model being only 50-200KB depending on how it is build (with twice that needed in total when flashed), mutiple models could be loaded in memory and a mixture-of-experts system can be designed. Which is what I want to explore with SPARROW 2.&lt;/p&gt; &lt;p&gt;I still have to see exactly how to proceed in terms of making the code open-source, best licensing methods, how to create the API, etc. But the idea is that it would be easy to create language models for MCUs, similar to how Sci-kit Learn is used for regular ML.&lt;/p&gt; &lt;p&gt;It supports encoder, decoder, encoder-decoder models, and the fastest model uses linear attention, but I have also been able to deploy dot attention and additive attention on the ESP32.&lt;/p&gt; &lt;p&gt;It also supports states, which is what's used in the final version and why it is so much faster. On the ESP32S3 the difference between a model with vs without states is 17x. The output &amp;quot;Dna is the molecule that stores genetic information&amp;quot; takes around 6 seconds without states, and 0.35 seconds with.&lt;/p&gt; &lt;p&gt;Let me know what you think! I have a lot more videos with the models running on PC with full phrases/paragraphs outputs in less than 10 miliseconds, have different versions Small, Main, Large running on the ESP32S3, have the Main flavour running on the ESP32P4 which can process everything 5-6 times faster due to the intrustions available, and outputting a phrase every 50-100ms, compared to ESP32S3's 300-600ms.&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/WCvv5W9gEiA?si=QCXvXei3qfp0qAG8"&gt;Here's the above video&lt;/a&gt; in 4K on YouTube, and &lt;a href="https://youtu.be/waqzSlAR0iY?si=Qfms7pVaRm0RMAR5"&gt;here's &lt;/a&gt;another video of it running without the Webapp overhead on the ESP32P4. &lt;a href="https://youtube.com/shorts/4xjKu1FP1I4?si=SWXVUj898T9ThNAy"&gt;This YouTube Short&lt;/a&gt; showcases Sparrow on PC with a simple webapp design with Streamlit.&lt;/p&gt; &lt;p&gt;EDIT: Forgot the most important part, SPARROW stands for Stateful Prototype-Aware Reasoning for Rapid Onboard Workflows. And it is also a super small cute bird, that fits the lightweight nature and portability of this model.&lt;/p&gt; &lt;p&gt;TL;DR: Run language models on most microcontrollers with a custom framework and Language Model called SPARROW that uses frontier methods, optimised even further, for speed. Why is it so fast, especially on such a small device? SPARROW makes a lot of the compute-bottlenecks into bandwidth-bottlenecks, resulting in a model that's orders of magnitude faster, which becomes even faster by having memory states and reducing the compute for each new token.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/c-f_i"&gt; /u/c-f_i &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pefagkhgkqlf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n28n3v/sparrow_custom_language_model_architecture_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n28n3v/sparrow_custom_language_model_architecture_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T10:31:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1n22xbl</id>
    <title>HunyuanVideo-Foley is out, an open source text-video-to-audio model</title>
    <updated>2025-08-28T04:33:24+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n22xbl/hunyuanvideofoley_is_out_an_open_source/"&gt; &lt;img alt="HunyuanVideo-Foley is out, an open source text-video-to-audio model" src="https://external-preview.redd.it/dXU2amRweXd1b2xmMTawZyv5aMEWeESK9yBcqymop7gFK-DtVYY3rCRDUSQp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39348627bb256dad55b1e266f8a7ec0f5b4b62ff" title="HunyuanVideo-Foley is out, an open source text-video-to-audio model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;try HunyuanVideo-Foley: &lt;a href="https://hunyuan.tencent.com/video/zh?tabIndex=0"&gt;https://hunyuan.tencent.com/video/zh?tabIndex=0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/tencent/HunyuanVideo-Foley"&gt;https://huggingface.co/tencent/HunyuanVideo-Foley&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Tencent-Hunyuan/HunyuanVideo-Foley"&gt;https://github.com/Tencent-Hunyuan/HunyuanVideo-Foley&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Project Page: &lt;a href="https://szczesnys.github.io/hunyuanvideo-foley/"&gt;https://szczesnys.github.io/hunyuanvideo-foley/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Research report: &lt;a href="https://arxiv.org/abs/2508.16930"&gt;https://arxiv.org/abs/2508.16930&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jpjpqw2xuolf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n22xbl/hunyuanvideofoley_is_out_an_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n22xbl/hunyuanvideofoley_is_out_an_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T04:33:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2k6st</id>
    <title>Local AI + state machine (yells at Amazon drivers peeing on my house)</title>
    <updated>2025-08-28T18:28:33+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2k6st/local_ai_state_machine_yells_at_amazon_drivers/"&gt; &lt;img alt="Local AI + state machine (yells at Amazon drivers peeing on my house)" src="https://external-preview.redd.it/dXM1ZWM1c3d3c2xmMZj5V4nY1VQiFgNlKq8PGxD_fB9khJueOQN3FmEXQ4it.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=38d8e99d192bc070575b0100763c538ec509e2aa" title="Local AI + state machine (yells at Amazon drivers peeing on my house)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Experimenting with state machines and LLMs in local pipelines. The LLM handles perception fuzziness (natural language, vision, edge cases), while the state machine enforces deterministic control flow. The combo makes agents way more reliable than just letting an LLM run solo.&lt;/p&gt; &lt;p&gt;Motivation for this latest test: Amazon drivers legit keep peeing on my house. So I wired up a workflow where the AI watches a live video feed. If it detects someone urinating in my driveway, the state machine flips the app from passive mode (just watching) into active mode (video + audio ingestion, ~1s TTS out), at which point it verbally shames them in real-time.&lt;/p&gt; &lt;p&gt;Some observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Conditional state changes:&lt;/strong&gt; Instead of always-on chatter, the LLM only activates when the state machine sees a trigger event. This makes it more deterministic and predictable.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Division of labor:&lt;/strong&gt; LLM handles perception + reasoning on noisy inputs. State machine handles orchestration + gating when/what gets executed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; The detection logic can be swapped out easily, so the same workflow could be used for different scenarios like spotting trespassing, logging deliveries, or recognizing gestures.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Weak spots:&lt;/strong&gt; Detection can hallucinate/miss under odd angles and lighting. Convo quality is hit-or-miss and depends on the model used.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I used GPT for reasoning in this demo, but it could easily be swapped for Qwen to keep everything 100% local.&lt;/p&gt; &lt;p&gt;TL;DR&lt;br /&gt; AI Urination Detection: not the hero we wanted, but the hero we needed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/257gigswwslf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2k6st/local_ai_state_machine_yells_at_amazon_drivers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2k6st/local_ai_state_machine_yells_at_amazon_drivers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T18:28:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1n24utb</id>
    <title>RELEASED: ComfyUI Wrapper for Microsoft‚Äôs new VibeVoice TTS (voice cloning in seconds)</title>
    <updated>2025-08-28T06:29:26+00:00</updated>
    <author>
      <name>/u/Fabix84</name>
      <uri>https://old.reddit.com/user/Fabix84</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n24utb/released_comfyui_wrapper_for_microsofts_new/"&gt; &lt;img alt="RELEASED: ComfyUI Wrapper for Microsoft‚Äôs new VibeVoice TTS (voice cloning in seconds)" src="https://external-preview.redd.it/eTdoNDByeThlcGxmMX--5rdiQuwxJ4jOINV8QPW9HN9UrvcxZxCYZhm1-TIi.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=264a3e65f2cd5a66911e7233d68932539c3879a6" title="RELEASED: ComfyUI Wrapper for Microsoft‚Äôs new VibeVoice TTS (voice cloning in seconds)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created and released open source the ComfyUI Wrapper for VibeVoice.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Single Speaker Node&lt;/strong&gt; to simplify workflow management when using only one voice.&lt;/li&gt; &lt;li&gt;Ability to load text from a file. This allows you to generate speech for the equivalent of dozens of minutes. The longer the text, the longer the generation time (obviously).&lt;/li&gt; &lt;li&gt;I tested cloning my real voice. I only provided a 56-second sample, and the results were very positive. You can see them in the video.&lt;/li&gt; &lt;li&gt;From my tests (not to be considered conclusive): when providing voice samples in a language other than English or Chinese (e.g. Italian), the model can generate speech in that same language (Italian) with a decent success rate. On the other hand, when providing English samples, I couldn‚Äôt get valid results when trying to generate speech in another language (e.g. Italian).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple Speakers&lt;/strong&gt; &lt;strong&gt;Node&lt;/strong&gt;, which allows up to 4 speakers (limit set by the Microsoft model). Results are decent only with the 7B model. The valid success rate is still much lower compared to single speaker generation. In short: the model looks very promising but still premature. The wrapper will still be adaptable to future updates of the model. Keep in mind the 7B model is still officially in &lt;em&gt;Preview&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;How much VRAM is needed?&lt;/strong&gt; Right now I‚Äôm only using the official models (so, maximum quality). The 1.5B model requires about &lt;strong&gt;5GB VRAM&lt;/strong&gt;, while the 7B model requires about &lt;strong&gt;17GB VRAM&lt;/strong&gt;. I haven‚Äôt tested on low-resource machines yet. To reduce resource usage, we‚Äôll have to wait for quantized models or, if I find the time, I‚Äôll try quantizing them myself (no promises).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My thoughts on this model:&lt;/strong&gt;&lt;br /&gt; A big step forward for the Open Weights ecosystem, and I‚Äôm really glad Microsoft released it. At its current stage, I see single-speaker generation as very solid, while multi-speaker is still too immature. But take this with a grain of salt. I may not have fully figured out how to get the best out of it yet. The real difference is the success rate between single-speaker and multi-speaker.&lt;/p&gt; &lt;p&gt;This model is &lt;em&gt;heavily&lt;/em&gt; influenced by the seed. Some seeds produce fantastic results, while others are really bad. With images, such wide variation can be useful. For voice cloning, though, it would be better to have a more deterministic model where the seed matters less.&lt;/p&gt; &lt;p&gt;In practice, this means you have to experiment with several seeds before finding the perfect voice. That can work for some workflows but not for others.&lt;/p&gt; &lt;p&gt;With multi-speaker, the problem gets worse because a single seed drives the entire conversation. You might get one speaker sounding great and another sounding off.&lt;/p&gt; &lt;p&gt;Personally, I think I‚Äôll stick to using single-speaker generation even for multi-speaker conversations unless a future version of the model becomes more deterministic.&lt;/p&gt; &lt;p&gt;That being said, it‚Äôs still a &lt;em&gt;huge&lt;/em&gt; step forward.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;URL to ComfyUI Wrapper:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://github.com/Enemyx-net/VibeVoice-ComfyUI"&gt;https://github.com/Enemyx-net/VibeVoice-ComfyUI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabix84"&gt; /u/Fabix84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yy7k60z8eplf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n24utb/released_comfyui_wrapper_for_microsofts_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n24utb/released_comfyui_wrapper_for_microsofts_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T06:29:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2b4et</id>
    <title>Qwen / Tongyi Lab launches GUI-Owl &amp; Mobile-Agent-v3</title>
    <updated>2025-08-28T12:39:15+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2b4et/qwen_tongyi_lab_launches_guiowl_mobileagentv3/"&gt; &lt;img alt="Qwen / Tongyi Lab launches GUI-Owl &amp;amp; Mobile-Agent-v3" src="https://b.thumbs.redditmedia.com/iUG3obaGCmAVOp3wRA9NxR4f0cDoay4umqV_naioeDc.jpg" title="Qwen / Tongyi Lab launches GUI-Owl &amp;amp; Mobile-Agent-v3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Github: &lt;a href="https://github.com/X-PLUG/MobileAgent"&gt;https://github.com/X-PLUG/MobileAgent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full Research Paper: &lt;a href="https://arxiv.org/abs/2508.15144"&gt;https://arxiv.org/abs/2508.15144&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n2b4et"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2b4et/qwen_tongyi_lab_launches_guiowl_mobileagentv3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2b4et/qwen_tongyi_lab_launches_guiowl_mobileagentv3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T12:39:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ev3c</id>
    <title>CohereLabs/command-a-translate-08-2025 ¬∑ Hugging Face</title>
    <updated>2025-08-28T15:09:18+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ev3c/coherelabscommandatranslate082025_hugging_face/"&gt; &lt;img alt="CohereLabs/command-a-translate-08-2025 ¬∑ Hugging Face" src="https://external-preview.redd.it/eR8XbSOhZiSMjrknKTRQhEYtliTvav81RbiIcBJQlDg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3193747f5f1f29e1784d71c482e40d0b96413aa8" title="CohereLabs/command-a-translate-08-2025 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cohere Labs Command A Translate is an open weights research release of a 111 billion parameter model that achieves state-of-the-art performance on translation quality.&lt;/p&gt; &lt;p&gt;Developed by: &lt;a href="https://cohere.com/"&gt;Cohere&lt;/a&gt; and &lt;a href="https://cohere.com/research"&gt;Cohere&lt;/a&gt; Labs&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Point of Contact: Cohere For AI: &lt;a href="https://cohere.com/research"&gt;&lt;strong&gt;Cohere Labs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;License: &lt;a href="https://cohere.com/cohere-labs-cc-by-nc-license"&gt;CC-BY-NC&lt;/a&gt;, requires also adhering to &lt;a href="https://docs.cohere.com/docs/c4ai-acceptable-use-policy"&gt;&lt;strong&gt;Cohere Lab's Acceptable Use Policy&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model: command-a-translate-08-2025&lt;/li&gt; &lt;li&gt;Model Size: 111B&lt;/li&gt; &lt;li&gt;Context length: 8k input, 8k output&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/CohereLabs/command-a-translate-08-2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ev3c/coherelabscommandatranslate082025_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ev3c/coherelabscommandatranslate082025_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T15:09:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2npu9</id>
    <title>GLM-4.5 is now leading the Berkeley Function-Calling Leaderboard V4, Beating Opus 4</title>
    <updated>2025-08-28T20:44:11+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2npu9/glm45_is_now_leading_the_berkeley_functioncalling/"&gt; &lt;img alt="GLM-4.5 is now leading the Berkeley Function-Calling Leaderboard V4, Beating Opus 4" src="https://preview.redd.it/pa10b6f5otlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad1a522ed166bb920414041f430c97aef7d1fdf9" title="GLM-4.5 is now leading the Berkeley Function-Calling Leaderboard V4, Beating Opus 4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://gorilla.cs.berkeley.edu/leaderboard.html?s=09"&gt;https://gorilla.cs.berkeley.edu/leaderboard.html?s=09&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pa10b6f5otlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2npu9/glm45_is_now_leading_the_berkeley_functioncalling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2npu9/glm45_is_now_leading_the_berkeley_functioncalling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T20:44:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2chrm</id>
    <title>Again where behemoth and reasoning model from meta ??</title>
    <updated>2025-08-28T13:39:03+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2chrm/again_where_behemoth_and_reasoning_model_from_meta/"&gt; &lt;img alt="Again where behemoth and reasoning model from meta ??" src="https://preview.redd.it/xma7ru49krlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=214fa2574efffdfe39bf57c819059660b5a2a371" title="Again where behemoth and reasoning model from meta ??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xma7ru49krlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2chrm/again_where_behemoth_and_reasoning_model_from_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2chrm/again_where_behemoth_and_reasoning_model_from_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T13:39:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2djpx</id>
    <title>I built a local ‚Äúsecond brain‚Äù AI that actually remembers everything (321 tests passed)</title>
    <updated>2025-08-28T14:20:48+00:00</updated>
    <author>
      <name>/u/IntelligentCause2043</name>
      <uri>https://old.reddit.com/user/IntelligentCause2043</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2djpx/i_built_a_local_second_brain_ai_that_actually/"&gt; &lt;img alt="I built a local ‚Äúsecond brain‚Äù AI that actually remembers everything (321 tests passed)" src="https://b.thumbs.redditmedia.com/nAthQhhqWhSgtN5Sk4QJYQdSOftJqyqFyWeMbtaNrdc.jpg" title="I built a local ‚Äúsecond brain‚Äù AI that actually remembers everything (321 tests passed)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the past months I‚Äôve been building &lt;strong&gt;Kai&lt;/strong&gt;, a cognitive operating system that acts like a &lt;em&gt;second brain&lt;/em&gt;. Unlike ChatGPT or Claude, it doesn‚Äôt forget what you tell it.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;100% local ‚Äì no cloud, no surveillance&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Graph-based memory&lt;/strong&gt; (3D visualization below)&lt;/li&gt; &lt;li&gt;Spreading activation ‚Üí memory retrieval works like a brain&lt;/li&gt; &lt;li&gt;&lt;strong&gt;321 passing tests&lt;/strong&gt; ‚Üí not a toy prototype&lt;/li&gt; &lt;li&gt;Learns from &lt;em&gt;everything you do&lt;/em&gt; on your machine&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm curious:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What‚Äôs the biggest pain you‚Äôve hit with current AI tools?&lt;/li&gt; &lt;li&gt;Would you actually use a local AI that builds a persistent memory of your knowledge/work?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to dive into the architecture or share more demos if people are interested.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; Thanks for all the feedback, I can‚Äôt keep up with comments. Short FAQ:&lt;br /&gt; ‚Äì It runs 100% local (no cloud, no spying).&lt;br /&gt; ‚Äì Not just RAG ‚Üí uses graph + activation model.&lt;br /&gt; ‚Äì Plan is to open core engine once stable.&lt;br /&gt; ‚Äì Early access / demo: &lt;a href="https://oneeko.ai?utm_source=chatgpt.com"&gt;oneeko.ai&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Here‚Äôs a shot of the memory graph growing as I feed it data :&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8jei7138zrlf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4125be85bd9a5a616c10a0423130cba14169100"&gt;https://preview.redd.it/8jei7138zrlf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4125be85bd9a5a616c10a0423130cba14169100&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IntelligentCause2043"&gt; /u/IntelligentCause2043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2djpx/i_built_a_local_second_brain_ai_that_actually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2djpx/i_built_a_local_second_brain_ai_that_actually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2djpx/i_built_a_local_second_brain_ai_that_actually/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T14:20:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2hyt2</id>
    <title>glm mini will be comming</title>
    <updated>2025-08-28T17:05:29+00:00</updated>
    <author>
      <name>/u/untanglled</name>
      <uri>https://old.reddit.com/user/untanglled</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2hyt2/glm_mini_will_be_comming/"&gt; &lt;img alt="glm mini will be comming" src="https://preview.redd.it/h1ss59p4lslf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d8d73abbfbb1def80b73cdd1845129f4a319098" title="glm mini will be comming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/untanglled"&gt; /u/untanglled &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h1ss59p4lslf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2hyt2/glm_mini_will_be_comming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2hyt2/glm_mini_will_be_comming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T17:05:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2jraj</id>
    <title>Gpt-oss Fine-tuning - now with 60K context length and fits on &lt;13GB VRAM</title>
    <updated>2025-08-28T18:12:00+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2jraj/gptoss_finetuning_now_with_60k_context_length_and/"&gt; &lt;img alt="Gpt-oss Fine-tuning - now with 60K context length and fits on &amp;lt;13GB VRAM" src="https://preview.redd.it/rwu8gezzwslf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01d59299286be897d49e1da4b5b96ae312e88050" title="Gpt-oss Fine-tuning - now with 60K context length and fits on &amp;lt;13GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys we've got LOTS of updates for gpt-oss training today! We‚Äôre excited to introduce &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; Flex Attention support for OpenAI gpt-oss training that enables &lt;strong&gt;&amp;gt;8√ó longer context lengths, &amp;gt;50% less VRAM usage and &amp;gt;1.5√ó faster training&lt;/strong&gt; vs. all implementations including those using Flash Attention 3 (FA3). Unsloth Flex Attention makes it possible to train with a 60K context length on just 80GB of VRAM for BF16 LoRA. Also:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;You can now export/save your QLoRA fine-tuned gpt-oss model to llama.cpp, vLLM, Ollama or HF&lt;/li&gt; &lt;li&gt;We fixed gpt-oss training losses going to infinity on float16 GPUs (like T4 Colab)&lt;/li&gt; &lt;li&gt;We fixed gpt-oss implementation issues irrelevant to Unsloth, most notably ensuring that swiglu_limit = 7.0 is properly applied during MXFP4 inference in transformers&lt;/li&gt; &lt;li&gt;Unsloth Flex Attention scales with context, longer sequences yield bigger savings in both VRAM and training time&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;ü¶• Would highly recommend you guys to read our blog which has all the bug fixes, guides, details, explanations, findings etc. and it'll be really educational: &lt;a href="https://docs.unsloth.ai/basics/long-context-gpt-oss-training"&gt;https://docs.unsloth.ai/basics/long-context-gpt-oss-training&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We'll likely release our gpt-oss training notebook with direct saving capabilities to GGUF, llama.cpp next week.&lt;/p&gt; &lt;p&gt;And we'll be releasing third-party Aider polygot benchmarks for DeepSeek-V3.1 next week. You guys will be amazed at how well IQ1_M performs!&lt;/p&gt; &lt;p&gt;And next week we'll might have a great new update for RL! üòâ&lt;/p&gt; &lt;p&gt;Thanks guys for reading and hope you all have a lovely Friday and long weekend, Daniel! ü¶•&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rwu8gezzwslf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2jraj/gptoss_finetuning_now_with_60k_context_length_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2jraj/gptoss_finetuning_now_with_60k_context_length_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T18:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1unkv</id>
    <title>Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)</title>
    <updated>2025-08-27T22:04:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt; &lt;img alt="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" src="https://preview.redd.it/ek8o2pfzumlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7e3a061baa23ba1306e6f0b1f2524a658bb27a2" title="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ek8o2pfzumlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T22:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ghx4</id>
    <title>AMA With Z.AI, The Lab Behind GLM Models</title>
    <updated>2025-08-28T16:10:25+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Z.AI ‚Äî The Lab Behind GLM Models. Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Z.AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;GLM family of models&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zixuanlimit/"&gt;&lt;strong&gt;Zixuan Li, u/zixuanlimit&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Maximum_Can9140/"&gt;&lt;strong&gt;Yuxuan Zhang, u/Maximum_Can9140&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxdu/"&gt;&lt;strong&gt;Zhengxiao Du, u/zxdu&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Sengxian/"&gt;&lt;strong&gt;Aohan Zeng, u/Sengxian&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 9 AM ‚Äì 12 PM PST, with the Z.AI team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our first AMA. The live part has ended and the Z.AI team will be following up with more answers sporadically over the next 48 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T16:10:25+00:00</published>
  </entry>
</feed>
