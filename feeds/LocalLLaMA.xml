<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-07T09:24:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jt97fn</id>
    <title>LLAMA 4 Scout on M3 Mac, 32 Tokens/sec 4-bit, 24 Tokens/sec 6-bit</title>
    <updated>2025-04-07T00:56:17+00:00</updated>
    <author>
      <name>/u/PerformanceRound7913</name>
      <uri>https://old.reddit.com/user/PerformanceRound7913</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt97fn/llama_4_scout_on_m3_mac_32_tokenssec_4bit_24/"&gt; &lt;img alt="LLAMA 4 Scout on M3 Mac, 32 Tokens/sec 4-bit, 24 Tokens/sec 6-bit" src="https://external-preview.redd.it/dmNvMGgxbW85YnRlMTNIns53Od6sUFLxeG7LHB3pN84GWbK3wZ0Y3inX9hXf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64f320b3580d9c7a0a06c17f508aefe4cc219054" title="LLAMA 4 Scout on M3 Mac, 32 Tokens/sec 4-bit, 24 Tokens/sec 6-bit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PerformanceRound7913"&gt; /u/PerformanceRound7913 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t1fxl1mo9bte1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt97fn/llama_4_scout_on_m3_mac_32_tokenssec_4bit_24/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt97fn/llama_4_scout_on_m3_mac_32_tokenssec_4bit_24/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T00:56:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsrz5v</id>
    <title>109b vs 24b ?? What's this benchmark?</title>
    <updated>2025-04-06T11:27:05+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsrz5v/109b_vs_24b_whats_this_benchmark/"&gt; &lt;img alt="109b vs 24b ?? What's this benchmark?" src="https://preview.redd.it/igg46skh97te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac1913f8538347a323b43f755d80a1b4bee7dcc0" title="109b vs 24b ?? What's this benchmark?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like llama 4 scout is 109b parameters and they compared with 24 and 27b parameters (I'm talking about total parameters size ) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/igg46skh97te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsrz5v/109b_vs_24b_whats_this_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsrz5v/109b_vs_24b_whats_this_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T11:27:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jspbqk</id>
    <title>Two months later and after LLaMA 4's release, I'm starting to believe that supposed employee leak... Hopefully LLaMA 4's reasoning is good, because things aren't looking good for Meta.</title>
    <updated>2025-04-06T08:16:46+00:00</updated>
    <author>
      <name>/u/Ill-Association-8410</name>
      <uri>https://old.reddit.com/user/Ill-Association-8410</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jspbqk/two_months_later_and_after_llama_4s_release_im/"&gt; &lt;img alt="Two months later and after LLaMA 4's release, I'm starting to believe that supposed employee leak... Hopefully LLaMA 4's reasoning is good, because things aren't looking good for Meta." src="https://b.thumbs.redditmedia.com/xVPaGo_gWxEAqifHFit37hwIpK0Ix1DGsAc5-U9IAmw.jpg" title="Two months later and after LLaMA 4's release, I'm starting to believe that supposed employee leak... Hopefully LLaMA 4's reasoning is good, because things aren't looking good for Meta." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/2acfxawz96te1.png?width=850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=50fd356b87dfa6e8a39d0b4bfb72f642c8168048"&gt;https://preview.redd.it/2acfxawz96te1.png?width=850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=50fd356b87dfa6e8a39d0b4bfb72f642c8168048&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill-Association-8410"&gt; /u/Ill-Association-8410 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jspbqk/two_months_later_and_after_llama_4s_release_im/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jspbqk/two_months_later_and_after_llama_4s_release_im/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jspbqk/two_months_later_and_after_llama_4s_release_im/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T08:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtb4r5</id>
    <title>Llama 4 doesn’t perform well on Fiction.LiveBench</title>
    <updated>2025-04-07T02:42:08+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtb4r5/llama_4_doesnt_perform_well_on_fictionlivebench/"&gt; &lt;img alt="Llama 4 doesn’t perform well on Fiction.LiveBench" src="https://preview.redd.it/ft5x7fvqsbte1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb3b833dc6d87614a343cdd93cbdbcbad3fde206" title="Llama 4 doesn’t perform well on Fiction.LiveBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://fiction.live/stories/Fiction-liveBench-Mar-25-2025/oQdzQvKHw8JyXbN87"&gt;https://fiction.live/stories/Fiction-liveBench-Mar-25-2025/oQdzQvKHw8JyXbN87&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ft5x7fvqsbte1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtb4r5/llama_4_doesnt_perform_well_on_fictionlivebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtb4r5/llama_4_doesnt_perform_well_on_fictionlivebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T02:42:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt9ig2</id>
    <title>The missing LLM size sweet-spot 18B</title>
    <updated>2025-04-07T01:12:28+00:00</updated>
    <author>
      <name>/u/Eden1506</name>
      <uri>https://old.reddit.com/user/Eden1506</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have 1b,2b3b,4b... until 14b but then jump to 24b,27b,32b and again jump up to 70b.&lt;/p&gt; &lt;p&gt;Outside of a small number of people (&amp;lt;10%) the majority don't run anything above 32b locally so my focus is on the gap between 14b and 24b. &lt;/p&gt; &lt;p&gt;An 18B model, in the most popular Q4KM quantisation, would be 10.5 gb in size fitting nicely on a 12gb gpu with 1.5 gb for context (~4096 tokens) or on 16gb with 5.5 gb context (20k tokens).&lt;/p&gt; &lt;p&gt;For consumer hardware 12gb vram seems to be the current sweet spot (Price/VRAM) right now with cards like the 2060 12gb, 3060 12gb, B580 12gb and many more AMD cards having 12gb as well. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eden1506"&gt; /u/Eden1506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt9ig2/the_missing_llm_size_sweetspot_18b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt9ig2/the_missing_llm_size_sweetspot_18b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt9ig2/the_missing_llm_size_sweetspot_18b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T01:12:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jta5vj</id>
    <title>VRAM requirement for 10M context</title>
    <updated>2025-04-07T01:48:53+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, I am into calculating KV cache size for different models:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jl33br/qwq32b_has_the_highest_kv_cachemodel_size_ratio/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jl33br/qwq32b_has_the_highest_kv_cachemodel_size_ratio/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To my surprise, the new Llama 4 Scout has 10M context. While most people don't have the resource or use case for 10M context, this super long maximum context can improve the lower context by a lot. Potentially making its &amp;lt;=128k performance similar to ChatGPT. So I think it is a huge breakthrough that warrants a calculation of how much VRAM it will use.&lt;/p&gt; &lt;p&gt;According vllm, Llama 4 Scout has a 3:1 interleaved chunked attention with 8192 tokens chunk:&lt;/p&gt; &lt;p&gt;&lt;a href="https://blog.vllm.ai/2025/04/05/llama4.html"&gt;https://blog.vllm.ai/2025/04/05/llama4.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Judging from the name, it seems to be similar to gemma 3's 5:1 interleaved Sliding Window Attention (iSWA) with 1024 tokens window. So I would just assume it is iSWA. Since not all inference engine supports iSWA, I would also calculate the KV cache requirement under the default Grouped Query Attention (GQA)&lt;/p&gt; &lt;p&gt;Here is a table comparing DeepSeek, Gemma 3 and Llama 4 assuming the first two can also run 10M context. All models parameters are fp8 and the KV cache is also fp8.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Context&lt;/th&gt; &lt;th align="left"&gt;8k&lt;/th&gt; &lt;th align="left"&gt;32k&lt;/th&gt; &lt;th align="left"&gt;128k&lt;/th&gt; &lt;th align="left"&gt;512k&lt;/th&gt; &lt;th align="left"&gt;2m&lt;/th&gt; &lt;th align="left"&gt;10m&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1 GQA&lt;/td&gt; &lt;td align="left"&gt;19.06GB&lt;/td&gt; &lt;td align="left"&gt;76.25GB&lt;/td&gt; &lt;td align="left"&gt;305GB&lt;/td&gt; &lt;td align="left"&gt;1220GB&lt;/td&gt; &lt;td align="left"&gt;4880GB&lt;/td&gt; &lt;td align="left"&gt;24400GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1 MLA&lt;/td&gt; &lt;td align="left"&gt;.268GB&lt;/td&gt; &lt;td align="left"&gt;1.07GB&lt;/td&gt; &lt;td align="left"&gt;4.29GB&lt;/td&gt; &lt;td align="left"&gt;17.16GB&lt;/td&gt; &lt;td align="left"&gt;68.63GB&lt;/td&gt; &lt;td align="left"&gt;343.1GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1 KV%&lt;/td&gt; &lt;td align="left"&gt;.04%&lt;/td&gt; &lt;td align="left"&gt;.159%&lt;/td&gt; &lt;td align="left"&gt;.64%&lt;/td&gt; &lt;td align="left"&gt;2.56%&lt;/td&gt; &lt;td align="left"&gt;10.23%&lt;/td&gt; &lt;td align="left"&gt;51.13%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma-3-27B GQA&lt;/td&gt; &lt;td align="left"&gt;1.94GB&lt;/td&gt; &lt;td align="left"&gt;7.75GB&lt;/td&gt; &lt;td align="left"&gt;31GB&lt;/td&gt; &lt;td align="left"&gt;124GB&lt;/td&gt; &lt;td align="left"&gt;496GB&lt;/td&gt; &lt;td align="left"&gt;2480GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma-3-27B iSWA&lt;/td&gt; &lt;td align="left"&gt;.516GB&lt;/td&gt; &lt;td align="left"&gt;1.45GB&lt;/td&gt; &lt;td align="left"&gt;5.2GB&lt;/td&gt; &lt;td align="left"&gt;20.2GB&lt;/td&gt; &lt;td align="left"&gt;80.2GB&lt;/td&gt; &lt;td align="left"&gt;400.2GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma-3-27B KV%&lt;/td&gt; &lt;td align="left"&gt;1.91%&lt;/td&gt; &lt;td align="left"&gt;5.37%&lt;/td&gt; &lt;td align="left"&gt;19.26%&lt;/td&gt; &lt;td align="left"&gt;74.81%&lt;/td&gt; &lt;td align="left"&gt;297%&lt;/td&gt; &lt;td align="left"&gt;1482%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-4-Scout GQA&lt;/td&gt; &lt;td align="left"&gt;.75GB&lt;/td&gt; &lt;td align="left"&gt;3GB&lt;/td&gt; &lt;td align="left"&gt;12GB&lt;/td&gt; &lt;td align="left"&gt;48GB&lt;/td&gt; &lt;td align="left"&gt;192GB&lt;/td&gt; &lt;td align="left"&gt;960GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-4-Scout iSWA&lt;/td&gt; &lt;td align="left"&gt;.75GB&lt;/td&gt; &lt;td align="left"&gt;1.31GB&lt;/td&gt; &lt;td align="left"&gt;3.56GB&lt;/td&gt; &lt;td align="left"&gt;12.56GB&lt;/td&gt; &lt;td align="left"&gt;48.56GB&lt;/td&gt; &lt;td align="left"&gt;240.56GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-4-Scout KV%&lt;/td&gt; &lt;td align="left"&gt;.688%&lt;/td&gt; &lt;td align="left"&gt;1.2%&lt;/td&gt; &lt;td align="left"&gt;3.27%&lt;/td&gt; &lt;td align="left"&gt;11.52%&lt;/td&gt; &lt;td align="left"&gt;44.55%&lt;/td&gt; &lt;td align="left"&gt;220.7%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;MLA and iSWA support from the popular inference engines.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Software&lt;/th&gt; &lt;th align="left"&gt;llama.cpp&lt;/th&gt; &lt;th align="left"&gt;transformers&lt;/th&gt; &lt;th align="left"&gt;vllm&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;MLA&lt;/td&gt; &lt;td align="left"&gt;No&lt;/td&gt; &lt;td align="left"&gt;No&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;iSWA&lt;/td&gt; &lt;td align="left"&gt;No&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;No&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;llama.cpp and transformers are working on MLA, so they will support it soon. But I haven't heard anything that llama.cpp and vllm are working on iSWA.&lt;/p&gt; &lt;p&gt;We can see that basically it is impractical to run 10m on GQA. It seems feasible to run Llama 4 Scout at 10m context with M3 Ultra but obviously the run time can be an issue. &lt;/p&gt; &lt;p&gt;Also, MLA is superior to iSWA for KV cache size, so it will be great if 10m context is supported by DeepSeek V4 in the future.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jta5vj/vram_requirement_for_10m_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jta5vj/vram_requirement_for_10m_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jta5vj/vram_requirement_for_10m_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T01:48:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtg1mp</id>
    <title>Briefly discussing Llama 4</title>
    <updated>2025-04-07T08:02:05+00:00</updated>
    <author>
      <name>/u/fourDnet</name>
      <uri>https://old.reddit.com/user/fourDnet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So Llama 4 is out, and so far we don't have a full technical report, but do have a semi-technical blog post (&lt;a href="https://huggingface.co/blog/llama4-release"&gt;https://huggingface.co/blog/llama4-release&lt;/a&gt;). I'm creating this post to foster discussion about their model architecture.&lt;/p&gt; &lt;p&gt;Regarding the model, the most striking claim is the 10 million token context size, which their team attributes to the following:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Blending layers that utilize rotary embeddings (RoPE) and no positional embeddings (NoPE)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Blending across layers is new, however similar approaches have been used before:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/lucidrains/x-transformers/issues/40"&gt;https://github.com/lucidrains/x-transformers/issues/40&lt;/a&gt; where they utilize RoPE only on some dimensions, &lt;a href="https://wandb.ai/eleutherai/neox/reports/Partial-Rotary-Tests--Vmlldzo2MjE1MjY"&gt;https://wandb.ai/eleutherai/neox/reports/Partial-Rotary-Tests--Vmlldzo2MjE1MjY&lt;/a&gt;, &lt;a href="https://wandb.ai/eleutherai/neox/reports/Partial-Rotary-Tests-v2--Vmlldzo2MjE4MTQ"&gt;https://wandb.ai/eleutherai/neox/reports/Partial-Rotary-Tests-v2--Vmlldzo2MjE4MTQ&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GPT-Neo/GPT-NeoX which applied RoPE on 25% of the dimensions&lt;/li&gt; &lt;li&gt;Deepseek V2 where they use NoPE/RoPE together in MLA&lt;/li&gt; &lt;li&gt;The author of RoPE wrote a blog post on partial rope: &lt;a href="https://kexue.fm/archives/10122"&gt;https://kexue.fm/archives/10122&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2. Length dependent softmax scaling&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This exact form of softmax was proposed by &amp;quot;Overcoming a Theoretical Limitation of Self-Attention&amp;quot; in section 5.3 in 2022&lt;/li&gt; &lt;li&gt;The author of RoPE also wrote a blog post on length dependent scaled softmax in 2022: &lt;a href="https://www.spaces.ac.cn/archives/9034"&gt;https://www.spaces.ac.cn/archives/9034&lt;/a&gt;&lt;/li&gt; &lt;li&gt;I see the blogpost they only reference &lt;a href="https://arxiv.org/abs/2501.19399"&gt;https://arxiv.org/abs/2501.19399&lt;/a&gt;, which is slightly puzzling since Qwen 1 (the original one from 2023) also uses the exact same softmax scaling strategy, and they call it logn scaling.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fourDnet"&gt; /u/fourDnet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtg1mp/briefly_discussing_llama_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtg1mp/briefly_discussing_llama_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtg1mp/briefly_discussing_llama_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T08:02:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsw1x6</id>
    <title>Llama 4 Maverick surpassing Claude 3.7 Sonnet, under DeepSeek V3.1 according to Artificial Analysis</title>
    <updated>2025-04-06T14:59:47+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsw1x6/llama_4_maverick_surpassing_claude_37_sonnet/"&gt; &lt;img alt="Llama 4 Maverick surpassing Claude 3.7 Sonnet, under DeepSeek V3.1 according to Artificial Analysis" src="https://preview.redd.it/bybxcks0b8te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=956027562bb411e338688cbf361aa64681ffdaa5" title="Llama 4 Maverick surpassing Claude 3.7 Sonnet, under DeepSeek V3.1 according to Artificial Analysis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bybxcks0b8te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsw1x6/llama_4_maverick_surpassing_claude_37_sonnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsw1x6/llama_4_maverick_surpassing_claude_37_sonnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T14:59:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtdeuc</id>
    <title>VIBE CHECKING LLAMA 4 MAVERICK</title>
    <updated>2025-04-07T04:56:01+00:00</updated>
    <author>
      <name>/u/Naubri</name>
      <uri>https://old.reddit.com/user/Naubri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtdeuc/vibe_checking_llama_4_maverick/"&gt; &lt;img alt="VIBE CHECKING LLAMA 4 MAVERICK" src="https://external-preview.redd.it/cTRhMGJiN2tnY3RlMUxLGgJy0dTEhlIhNMS-m77ENKYf8_yKnO--mAFXC5Dh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f15ae3d55c408d4d9eba438024d025e6bebcaeb" title="VIBE CHECKING LLAMA 4 MAVERICK" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did it pass the vibe check?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Naubri"&gt; /u/Naubri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kzce5wakgcte1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtdeuc/vibe_checking_llama_4_maverick/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtdeuc/vibe_checking_llama_4_maverick/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T04:56:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsx7m2</id>
    <title>Fiction.liveBench for Long Context Deep Comprehension updated with Llama 4 [It's bad]</title>
    <updated>2025-04-06T15:50:51+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsx7m2/fictionlivebench_for_long_context_deep/"&gt; &lt;img alt="Fiction.liveBench for Long Context Deep Comprehension updated with Llama 4 [It's bad]" src="https://preview.redd.it/r156a01ck8te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=031603b0696b592d8ef50de5da2e99898323dc70" title="Fiction.liveBench for Long Context Deep Comprehension updated with Llama 4 [It's bad]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r156a01ck8te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsx7m2/fictionlivebench_for_long_context_deep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsx7m2/fictionlivebench_for_long_context_deep/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T15:50:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt08di</id>
    <title>EXL3 early preview has been released! exl3 4.0bpw comparable to exl2 5.0bpw/gguf q4_k_m/l for less size!</title>
    <updated>2025-04-06T18:01:08+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt08di/exl3_early_preview_has_been_released_exl3_40bpw/"&gt; &lt;img alt="EXL3 early preview has been released! exl3 4.0bpw comparable to exl2 5.0bpw/gguf q4_k_m/l for less size!" src="https://external-preview.redd.it/vFBohsgnlXCJMC0xtwZ_rjdSYdrwEuOmU0JdzSKYjFA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=caf2528f31312b3bb89c3b44b8e38e5626b80507" title="EXL3 early preview has been released! exl3 4.0bpw comparable to exl2 5.0bpw/gguf q4_k_m/l for less size!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems exl3 early preview has been released, and it seems promising!&lt;/p&gt; &lt;p&gt;Seems 4.0 bpw EXL3 is comparable 5.0 bpw exl2, which at the same would be comparable to GGUF Q4_K_M/Q4_K_L for less size!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/turboderp-org/exllamav3/blob/master/doc/llama31_8b_instruct_bpw.png?raw=true"&gt;Llama-3.1-8B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/turboderp-org/exllamav3/blob/master/doc/llama31_70b_instruct_bpw.png?raw=true"&gt;Llama-3.7-70B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also turbo mentions&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Fun fact: Llama-3.1-70B-EXL3 is coherent at 1.6 bpw. With the output layer quantized to 3 bpw and a 4096-token cache, inference is possible in under 16 GB of VRAM.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Note there are a lot of missing features as early preview release, so take that in mind!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/turboderp-org/exllamav3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt08di/exl3_early_preview_has_been_released_exl3_40bpw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt08di/exl3_early_preview_has_been_released_exl3_40bpw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T18:01:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtby7o</id>
    <title>How to properly use Reasoning models in ST</title>
    <updated>2025-04-07T03:26:49+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtby7o/how_to_properly_use_reasoning_models_in_st/"&gt; &lt;img alt="How to properly use Reasoning models in ST" src="https://b.thumbs.redditmedia.com/bPykmlsSqilfr33lVfV_hLYIoAKGURDUeFZPF8QObsM.jpg" title="How to properly use Reasoning models in ST" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For any reasoning models in general, you need to make sure to set:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prefix is set to ONLY &amp;lt;think&amp;gt; and the suffix is set to ONLY &amp;lt;/think&amp;gt; without any spaces or newlines (enter)&lt;/li&gt; &lt;li&gt;Reply starts with &amp;lt;think&amp;gt;&lt;/li&gt; &lt;li&gt;Always add character names is unchecked&lt;/li&gt; &lt;li&gt;Include names is set to never&lt;/li&gt; &lt;li&gt;As always the chat template should also conform to the model being used&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Note: Reasoning models work properly only if include names is set to never, since they always expect the eos token of the user turn followed by the &amp;lt;think&amp;gt; token in order to start reasoning before outputting their response. If you set include names to enabled, then it will always append the character name at the end like &amp;quot;Seraphina:&amp;lt;eos\_token&amp;gt;&amp;quot; which confuses the model on whether it should respond or reason first.&lt;/p&gt; &lt;p&gt;The rest of your sampler parameters can be set as you wish as usual.&lt;/p&gt; &lt;p&gt;If you don't see the reasoning wrapped inside the thinking block, then either your settings is still wrong and doesn't follow my example or that your ST version is too old without reasoning block auto parsing.&lt;/p&gt; &lt;p&gt;If you see the whole response is in the reasoning block, then your &amp;lt;think&amp;gt; and &amp;lt;/think&amp;gt; reasoning token suffix and prefix might have an extra space or newline. Or the model just isn't a reasoning model that is smart enough to always put reasoning in between those tokens.&lt;/p&gt; &lt;p&gt;This has been a PSA from Owen of Arli AI in anticipation of our new &amp;quot;RpR&amp;quot; model. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jtby7o"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtby7o/how_to_properly_use_reasoning_models_in_st/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtby7o/how_to_properly_use_reasoning_models_in_st/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T03:26:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jszvmi</id>
    <title>where all the billion dollars went new model is not even top 20 in coding</title>
    <updated>2025-04-06T17:46:04+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;what yann lecun is smoking i wanna smoke too&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jszvmi/where_all_the_billion_dollars_went_new_model_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jszvmi/where_all_the_billion_dollars_went_new_model_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jszvmi/where_all_the_billion_dollars_went_new_model_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T17:46:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt7zp7</id>
    <title>Cybersecurity Benchmark - Pretty sure Maverick is broken</title>
    <updated>2025-04-06T23:53:03+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was getting some weird results with Llama 4 Maverick so broke out my old Cyber benchmark.&lt;br /&gt; These are multiple choice questions about Cybersecurity.&lt;/p&gt; &lt;p&gt;Guessing they screwed something with the version they pushed out.&lt;br /&gt; Based on what everyone has been saying it's not just Lambda.&lt;/p&gt; &lt;p&gt;I highly doubt the released version of Maverick would score 80 on MMLU PRO like Meta showed.&lt;br /&gt; I guess it could be their FP8 is broken.&lt;/p&gt; &lt;p&gt;Scout seems to score about as expected.&lt;/p&gt; &lt;p&gt;Results: (No I didn't mix them up, Scout is whooping Maverick here)&lt;/p&gt; &lt;p&gt;1st - GPT-4.5 - 95.01% - $3.87&lt;br /&gt; 2nd - Claude-3.7 - 92.87% - $0.30&lt;br /&gt; 2nd - Claude-3.5-October - 92.87%&lt;br /&gt; &lt;strong&gt;4th - Meta-Llama3.1-405b-FP8 - 92.64%&lt;/strong&gt;&lt;br /&gt; 5th - GPT-4o - 92.40%&lt;br /&gt; 5th - Mistral-Large-123b-2411-FP16 92.40%&lt;br /&gt; 7th - Deepseek-v3-api - 91.92% - $0.03&lt;br /&gt; 8th - GPT-4o-mini - 91.75%&lt;br /&gt; 9th - DeepSeek-v2.5-1210-BF16 - 90.50%&lt;br /&gt; 10th - Meta-LLama3.3-70b-FP8 - 90.26%&lt;br /&gt; 11th - Qwen-2.5-72b-FP8 - 90.09%&lt;br /&gt; 12th - Meta-Llama3.1-70b-FP8 - 89.15%&lt;br /&gt; &lt;strong&gt;13th - Llama-4-scout-Lambda - 88.6%&lt;/strong&gt;&lt;br /&gt; 13th - Phi-4-GGUF-Fixed-Q4 - 88.6%&lt;br /&gt; 15th - Hunyuan-Large-389b-FP8 - 88.60%&lt;br /&gt; 16th - Qwen-2.5-14b-awq - 85.75%&lt;br /&gt; 17nd - Qwen2.5-7B-FP16 - 83.73%&lt;br /&gt; 18th - IBM-Granite-3.1-8b-FP16 - 82.19%&lt;br /&gt; 19rd - Meta-Llama3.1-8b-FP16 - 81.37%&lt;br /&gt; &lt;strong&gt;20th - Llama-4-Maverick-FP8-Lambda - 77.2%&lt;/strong&gt;&lt;br /&gt; 21st - IBM-Granite-3.0-8b-FP16 - 73.82%&lt;/p&gt; &lt;p&gt;One interesting fact.&lt;br /&gt; Maverick did manage to answer every single questions in the correct &amp;quot;Answer: A&amp;quot; format as instructed.&lt;br /&gt; Only a handful of models have managed that.&lt;/p&gt; &lt;p&gt;Scout on the other hand screwed up 3 answer formats, I would say that is just average.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt7zp7/cybersecurity_benchmark_pretty_sure_maverick_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt7zp7/cybersecurity_benchmark_pretty_sure_maverick_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt7zp7/cybersecurity_benchmark_pretty_sure_maverick_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T23:53:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtcn8o</id>
    <title>Meta AI could have Just Released Small Variants for Llama-4 and Focus on Llama-5!</title>
    <updated>2025-04-07T04:07:36+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta AI might have just released smaller variants of the Llama-4 series, potentially focusing more on the upcoming Llama-5. Introducing models like the 2B, 8-12B, and possibly a 30B variant could be beneficial, as many users would be able to run them on consumer hardware. Training smaller models is faster and less resource-intensive, allowing Meta AI to iterate and improve them more quickly.&lt;/p&gt; &lt;p&gt;Meta AI could be transparent about the limitations of the larger Llama-4 variants, explaining that they decided to revisit their approach to deliver models that truly make a difference. Alternatively, they might share insights into experimenting with new architectures, which led to skipping the fourth iteration of Llama.&lt;/p&gt; &lt;p&gt;No one would blame Meta AI for a setback or for striving for excellence, but releasing models that are unusable is another matter. These issues include:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The models can't run on consumer hardware.&lt;/li&gt; &lt;li&gt;Even if they can run on consumer hardware, they don't match the performance of similarly sized models.&lt;/li&gt; &lt;li&gt;There's a well-established reason why AI labs focus on enhancing models with coding and math capabilities: research consistently shows that models excelling in these areas perform better in generalization and problem-solving.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We've moved beyond the era when chatbots were the main attraction. We need tools that solve problems and improve our lives. Most AI companies target coders because they are the ones pushing AI models to the public, building on and with these applications. As early adopters willing to invest in quality products, coders recognize the significant boost in productivity AI coding assistants provide.&lt;/p&gt; &lt;p&gt;So, why release models that no one will use? Since the Llama-1 release, the trend has been to benchmark fine-tuned models against larger ones, showcasing the potential of smaller models. Remember the Microsoft Orca model (later renamed Phi)? How did they claim that their 107B model barely surpassed Gemma-3-27B, a model four times smaller? It's challenging to see the strategy other than attempting to stay ahead of potential releases like Qwen-3 and DS-R2 by controlling the narrative and asserting relevance. This approach is both SAD and PATHETIC.&lt;/p&gt; &lt;p&gt;Moreover, betting everything on the Mixture of Experts (MoE) architecture, revitalized by DeepSeek, and failing to replicate their breakthrough performance is unbelievable. How can Meta AI miss the mark so significantly?&lt;/p&gt; &lt;p&gt;I'd love to hear your thoughts and discuss this situation further.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtcn8o/meta_ai_could_have_just_released_small_variants/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtcn8o/meta_ai_could_have_just_released_small_variants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtcn8o/meta_ai_could_have_just_released_small_variants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T04:07:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt0bx3</id>
    <title>QwQ-32b outperforms Llama-4 by a lot!</title>
    <updated>2025-04-06T18:05:12+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt0bx3/qwq32b_outperforms_llama4_by_a_lot/"&gt; &lt;img alt="QwQ-32b outperforms Llama-4 by a lot!" src="https://preview.redd.it/yz3jlgri89te1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1028f6ffed6ed42285e0509e6172b873a52a901" title="QwQ-32b outperforms Llama-4 by a lot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;QwQ-32b blows out of the water the newly announced Llama-4 models Maverick-400b and Scout-109b!&lt;/p&gt; &lt;p&gt;I know these models have different attributes, QwQ being a reasoning and dense model and Llama-4 being instruct and MoE models with only 17b active parameters. But, the end user doesn’t care much how these models work internally and rather focus on performance and how achievable is to self-host them, and frankly a 32b model requires cheaper hardware to self-host rather than a 100-400b model (even if only 17b are active).&lt;/p&gt; &lt;p&gt;Also, the difference in performance is mind blowing, I didn’t expect Meta to announce Llama-4 models that are so much behind the race in performance on date of announcement.&lt;/p&gt; &lt;p&gt;Even Gemma-3 27b outperforms their Scout model that has 109b parameters, Gemma-3 27b can be hosted in its full glory in just 16GB of VRAM with QAT quants, Llama would need 50GB in q4 and it’s significantly weaker model. &lt;/p&gt; &lt;p&gt;Honestly, I hope Meta to find a way to top the race with future releases, because this one doesn’t even make it to top 3…&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yz3jlgri89te1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt0bx3/qwq32b_outperforms_llama4_by_a_lot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt0bx3/qwq32b_outperforms_llama4_by_a_lot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T18:05:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt884c</id>
    <title>Meta’s head of AI research stepping down (before the llama4 flopped)</title>
    <updated>2025-04-07T00:04:40+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt884c/metas_head_of_ai_research_stepping_down_before/"&gt; &lt;img alt="Meta’s head of AI research stepping down (before the llama4 flopped)" src="https://external-preview.redd.it/w2Nj8TPYa2qaxzY0O1v6AVUr_GEXrum2ZQNsSXEfj7Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cba656798e15ec30ab799fc0f3003affc586cf6f" title="Meta’s head of AI research stepping down (before the llama4 flopped)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guess this ths early induction of the llama4 disaster that we all missed &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://apnews.com/article/meta-ai-research-chief-stepping-down-joelle-pineau-c596df5f0d567268c4acd6f41944b5db"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt884c/metas_head_of_ai_research_stepping_down_before/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt884c/metas_head_of_ai_research_stepping_down_before/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T00:04:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsshhe</id>
    <title>"snugly fits in a h100, quantized 4 bit"</title>
    <updated>2025-04-06T11:59:08+00:00</updated>
    <author>
      <name>/u/LoSboccacc</name>
      <uri>https://old.reddit.com/user/LoSboccacc</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsshhe/snugly_fits_in_a_h100_quantized_4_bit/"&gt; &lt;img alt="&amp;quot;snugly fits in a h100, quantized 4 bit&amp;quot;" src="https://preview.redd.it/g2mj9lg4f7te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b3f27828a4984437b27e38b91aa497b1074ed5d" title="&amp;quot;snugly fits in a h100, quantized 4 bit&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LoSboccacc"&gt; /u/LoSboccacc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g2mj9lg4f7te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsshhe/snugly_fits_in_a_h100_quantized_4_bit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsshhe/snugly_fits_in_a_h100_quantized_4_bit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T11:59:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt4asx</id>
    <title>Llama 4 Maverick scored 16% on the aider polyglot coding benchmark.</title>
    <updated>2025-04-06T20:56:16+00:00</updated>
    <author>
      <name>/u/Ill-Association-8410</name>
      <uri>https://old.reddit.com/user/Ill-Association-8410</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt4asx/llama_4_maverick_scored_16_on_the_aider_polyglot/"&gt; &lt;img alt="Llama 4 Maverick scored 16% on the aider polyglot coding benchmark." src="https://external-preview.redd.it/mxsAL5bnhJrCy59iBqMiFSK6EQCdcJFJGTjI3lvugv8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15cb6a9d1d8e17545225a02c68b19defe2588f3e" title="Llama 4 Maverick scored 16% on the aider polyglot coding benchmark." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill-Association-8410"&gt; /u/Ill-Association-8410 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/paulgauthier/status/1908976568879476843"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt4asx/llama_4_maverick_scored_16_on_the_aider_polyglot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt4asx/llama_4_maverick_scored_16_on_the_aider_polyglot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T20:56:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtfm44</id>
    <title>Meta Leaker refutes the training on test set claim</title>
    <updated>2025-04-07T07:29:49+00:00</updated>
    <author>
      <name>/u/ElectronicCress3132</name>
      <uri>https://old.reddit.com/user/ElectronicCress3132</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtfm44/meta_leaker_refutes_the_training_on_test_set_claim/"&gt; &lt;img alt="Meta Leaker refutes the training on test set claim" src="https://preview.redd.it/m4roxa1z7dte1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=83d5ddfa391a2107eb5ac49fe7ec99d2b2a5c5c3" title="Meta Leaker refutes the training on test set claim" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElectronicCress3132"&gt; /u/ElectronicCress3132 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m4roxa1z7dte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtfm44/meta_leaker_refutes_the_training_on_test_set_claim/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtfm44/meta_leaker_refutes_the_training_on_test_set_claim/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T07:29:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtc5ok</id>
    <title>We may see DeepSeek R2 this week, that will explain the Llama4 Saturday launch.</title>
    <updated>2025-04-07T03:38:54+00:00</updated>
    <author>
      <name>/u/estebansaa</name>
      <uri>https://old.reddit.com/user/estebansaa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not going to be a good week for LLama millionaire engineers. The Benchs they showed seem like complete lies at this point. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/estebansaa"&gt; /u/estebansaa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtc5ok/we_may_see_deepseek_r2_this_week_that_will/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtc5ok/we_may_see_deepseek_r2_this_week_that_will/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtc5ok/we_may_see_deepseek_r2_this_week_that_will/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T03:38:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt85zy</id>
    <title>I'd like to see Zuckerberg try to replace mid level engineers with Llama 4</title>
    <updated>2025-04-07T00:01:40+00:00</updated>
    <author>
      <name>/u/NoConcert8847</name>
      <uri>https://old.reddit.com/user/NoConcert8847</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;He said this in January: &lt;a href="https://www.forbes.com/sites/quickerbettertech/2025/01/26/business-tech-news-zuckerberg-says-ai-will-replace-mid-level-engineers-soon/"&gt;https://www.forbes.com/sites/quickerbettertech/2025/01/26/business-tech-news-zuckerberg-says-ai-will-replace-mid-level-engineers-soon/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoConcert8847"&gt; /u/NoConcert8847 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt85zy/id_like_to_see_zuckerberg_try_to_replace_mid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt85zy/id_like_to_see_zuckerberg_try_to_replace_mid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt85zy/id_like_to_see_zuckerberg_try_to_replace_mid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T00:01:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtejzj</id>
    <title>Llama 4 is open - unless you are in the EU</title>
    <updated>2025-04-07T06:13:05+00:00</updated>
    <author>
      <name>/u/Feeling_Dog9493</name>
      <uri>https://old.reddit.com/user/Feeling_Dog9493</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have you guys read the LLaMA 4 license? EU based entities are not restricted - they are banned. AI Geofencing has arrived:&lt;/p&gt; &lt;p&gt;“You may not use the Llama Materials if you are… domiciled in a country that is part of the European Union.”&lt;/p&gt; &lt;p&gt;No exceptions. Not for research, not for personal use, not even through a US-based cloud provider. If your org is legally in the EU, you’re legally locked out.&lt;/p&gt; &lt;p&gt;And that’s just the start: • Must use Meta’s branding (“LLaMA” must be in any derivative’s name) • Attribution is required (“Built with LLaMA”) • No field-of-use freedom • No redistribution freedom • Not OSI-compliant = not open source&lt;/p&gt; &lt;p&gt;This isn’t “open” in any meaningful sense—it’s corporate-controlled access dressed up in community language. The likely reason? Meta doesn’t want to deal with the EU AI Act’s transparency and risk requirements, so it’s easier to just draw a legal border around the entire continent.&lt;/p&gt; &lt;p&gt;This move sets a dangerous precedent. If region-locking becomes the norm, we’re headed for a fractured, privilege-based AI landscape—where your access to foundational tools depends on where your HQ is.&lt;/p&gt; &lt;p&gt;For EU devs, researchers, and startups: You’re out. For the open-source community: This is the line in the sand.&lt;/p&gt; &lt;p&gt;Real “open” models like DeepSeek and Mistral deserve more attention than ever—because this? This isn’t it.&lt;/p&gt; &lt;p&gt;What’s your take—are you switching models? Ignoring the license? Holding out hope for change?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Feeling_Dog9493"&gt; /u/Feeling_Dog9493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtejzj/llama_4_is_open_unless_you_are_in_the_eu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtejzj/llama_4_is_open_unless_you_are_in_the_eu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtejzj/llama_4_is_open_unless_you_are_in_the_eu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T06:13:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt8yug</id>
    <title>“Serious issues in Llama 4 training. I Have Submitted My Resignation to GenAI“</title>
    <updated>2025-04-07T00:43:36+00:00</updated>
    <author>
      <name>/u/rrryougi</name>
      <uri>https://old.reddit.com/user/rrryougi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Original post is in Chinese that can be found &lt;a href="https://www.1point3acres.com/bbs/thread-1122600-1-1.html"&gt;here&lt;/a&gt;. Please take the following with a grain of salt.&lt;/p&gt; &lt;p&gt;Content:&lt;/p&gt; &lt;p&gt;Despite repeated training efforts, the internal model's performance still falls short of open-source SOTA benchmarks, lagging significantly behind. Company leadership suggested blending test sets from various benchmarks during the post-training process, aiming to meet the targets across various metrics and produce a &amp;quot;presentable&amp;quot; result. Failure to achieve this goal by the end-of-April deadline would lead to dire consequences. Following yesterday’s release of Llama 4, many users on X and Reddit have already reported extremely poor real-world test results. &lt;/p&gt; &lt;p&gt;As someone currently in academia, I find this approach utterly unacceptable. Consequently, I have submitted my resignation and explicitly requested that my name be excluded from the technical report of Llama 4. Notably, the VP of AI at Meta also resigned for similar reasons. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rrryougi"&gt; /u/rrryougi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt8yug/serious_issues_in_llama_4_training_i_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt8yug/serious_issues_in_llama_4_training_i_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt8yug/serious_issues_in_llama_4_training_i_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T00:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jt7hlc</id>
    <title>Meta's Llama 4 Fell Short</title>
    <updated>2025-04-06T23:27:19+00:00</updated>
    <author>
      <name>/u/Rare-Site</name>
      <uri>https://old.reddit.com/user/Rare-Site</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt7hlc/metas_llama_4_fell_short/"&gt; &lt;img alt="Meta's Llama 4 Fell Short" src="https://preview.redd.it/rwrke16rpate1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c97aeb81ed981cdfb9ae0bb78f2027199731a69a" title="Meta's Llama 4 Fell Short" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Llama 4 Scout and Maverick left me really disappointed. It might explain why Joelle Pineau, Meta’s AI research lead, just got fired. Why are these models so underwhelming? My armchair analyst intuition suggests it’s partly the tiny expert size in their mixture-of-experts setup. 17B parameters? Feels small these days.&lt;/p&gt; &lt;p&gt;Meta’s struggle proves that having all the GPUs and Data in the world doesn’t mean much if the ideas aren’t fresh. Companies like DeepSeek, OpenAI etc. show real innovation is what pushes AI forward. You can’t just throw resources at a problem and hope for magic. Guess that’s the tricky part of AI, it’s not just about brute force, but brainpower too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rare-Site"&gt; /u/Rare-Site &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rwrke16rpate1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jt7hlc/metas_llama_4_fell_short/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jt7hlc/metas_llama_4_fell_short/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T23:27:19+00:00</published>
  </entry>
</feed>
