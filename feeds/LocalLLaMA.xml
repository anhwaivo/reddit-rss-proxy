<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-09T18:25:35+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jvbnx1</id>
    <title>Free ebook Offer - Retrieval-Augmented Generation (RAG): The Future of AI-Powered Knowledge Retrieval</title>
    <updated>2025-04-09T17:34:58+00:00</updated>
    <author>
      <name>/u/qptbook</name>
      <uri>https://old.reddit.com/user/qptbook</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvbnx1/free_ebook_offer_retrievalaugmented_generation/"&gt; &lt;img alt="Free ebook Offer - Retrieval-Augmented Generation (RAG): The Future of AI-Powered Knowledge Retrieval" src="https://external-preview.redd.it/Jk9HJTy_1g_5IjK5KbtKPCOckmpo6V46sYk5SHKgaFE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a383adb2735e1a584348e8ee2eb57014bb758e7" title="Free ebook Offer - Retrieval-Augmented Generation (RAG): The Future of AI-Powered Knowledge Retrieval" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is limited-time offer. Use it before it ends. You need to click the Buy (Add to cart) button, but need NOT make any payment, just give your email address for accessing the content.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/qptbook"&gt; /u/qptbook &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.rajamanickam.com/l/RAG/raj100?layout=profile"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvbnx1/free_ebook_offer_retrievalaugmented_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvbnx1/free_ebook_offer_retrievalaugmented_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T17:34:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv9xxo</id>
    <title>Benchmark results for Llama 4 Maverick and Scout for DevQualityEval v1.0</title>
    <updated>2025-04-09T16:24:58+00:00</updated>
    <author>
      <name>/u/zimmski</name>
      <uri>https://old.reddit.com/user/zimmski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv9xxo/benchmark_results_for_llama_4_maverick_and_scout/"&gt; &lt;img alt="Benchmark results for Llama 4 Maverick and Scout for DevQualityEval v1.0" src="https://b.thumbs.redditmedia.com/6oYoxz6WtRiFYSQRtLUbaOR9q6dPAl6xdkNP2cuA5_w.jpg" title="Benchmark results for Llama 4 Maverick and Scout for DevQualityEval v1.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Note 1: Took me a while to rerun the benchmark on all providers that currently have them up. i also reran this every day since the 2025-04-05, i.e. i am pretty confident about the stability of the results because the mean deviation is low, and that there were no inference improvements.)&lt;br /&gt; (Note 2: DevQualityEval is a coding benchmark. It is very picky. And it is not mainly based on Python. Your mileage may vary.)&lt;/p&gt; &lt;p&gt;Meta‚Äôs new Llama 4 Maverick 400B and Llama 4 Scout 109B are FAR BEHIND much smaller models in DevQualityEval v1.0 üíîüòø&lt;/p&gt; &lt;p&gt;There are lots of positive and negative details!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results for DevQualityEval v1.0&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Meta: Llama 4 Maverick 400B (best Llama so far, but still mid-level):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üèÅ Maverick (68.47%) is on #41 (&lt;strong&gt;slightly better than Llama 3.1 405B&lt;/strong&gt; #48: 65.38%) behind Gemma 3 27B #37 (73.90%), Mistral 3.1 Small (2503) 24B #35 (74.38%) and Qwen: Qwen 2.5 Coder 32B #19 (81.32%)&lt;/li&gt; &lt;li&gt;üêï‚Äçü¶∫ With better context Maverick (89.70%) would be as good as Claude 3.5 Sonnet (2024-10-22) #2 (89.19%) and ChatGPT-4o (2025-03-27) #1 (90.96%) but reaches only #18 (+21.23%!) since other models can take advantage of better context as well. &lt;strong&gt;This increase is notable and suggests that Maverick (and Scout) can perform much better by default with some fine-tuning.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;‚öôÔ∏è Maverick is in the mid-range for producing code that compiled (1007) better than Llama 3.1 405B (987) but comparing this to our top-compiler ChatGPT-4o (2025-03-27) (1109) there is much room left&lt;/li&gt; &lt;li&gt;üêò On average Maverick took 8.6s per task which is notably slower than better scoring models with similar pricing like Claude 3.5 Haiku (5.15s)&lt;/li&gt; &lt;li&gt;üó£Ô∏è Maverick is less chatty than its predecessor in in absolute chattiness but bit worse in excess chattiness. Both in the better league.&lt;/li&gt; &lt;li&gt;‚õ∞Ô∏è Consistency and reliable in output is good for Maverick (2.21%) but worse than Llama 3.1 405B (2.03%)&lt;/li&gt; &lt;li&gt;ü¶æ Request/response/retry-rate are almost perfect: 12 requests needed retries but were able to recover&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Meta: Llama 4 Scout 109B (mid-level):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üèÅ Scout (62.53%) is on #56 (&lt;strong&gt;worse than Meta: Llama 3.1 70B&lt;/strong&gt; #50: 64.90%) behind Maverick and Mistral: Ministral (2025-03-31) 8B #44 (66.53%, pretty solid!)&lt;/li&gt; &lt;li&gt;üêï‚Äçü¶∫ With better context Scout (79.58%) would be as good as Claude 3.5 Sonnet (2024-06-20) #22 (79.43%) and MiniMax-01 #21 (80.67%) but reaches only #45 (+17.05%) in this score compared to others&lt;/li&gt; &lt;li&gt;‚öôÔ∏è Scout is slightly behind Maverick and in the mid-range for producing code that compiled (992) &lt;strong&gt;FAR BETTER then Llama 3.1 70B&lt;/strong&gt; (943) which makes it surprising that its score is lower&lt;/li&gt; &lt;li&gt;üêò Even though Scout is much smaller than Maverick its average time per task is similar: 9.12s (&lt;strong&gt;this might be an inference problem still left&lt;/strong&gt;)&lt;/li&gt; &lt;li&gt;üó£Ô∏è Scout is more chatty in absolute and excess chattiness but still in the better league.&lt;/li&gt; &lt;li&gt;‚õ∞Ô∏è Consistency and reliable in output is great for Scout #11 (1.46%) but behind Llama 3.1 70B #2 (0.93%)&lt;/li&gt; &lt;li&gt;ü¶æ Request/response/retry-rate was better than Maverick: only 2 requests needed retries and were also able to recover&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Comparing language scores:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Go: Lama models have always been great for Go, but other models have caught up. Maverick #17 (92.84%) and Scout #19 (92.66%) are great spots but a regression to Llama 3.1 405B #14 (93.58%) which is still the &lt;strong&gt;best open source model for Go&lt;/strong&gt;. &lt;/li&gt; &lt;li&gt;Java: &lt;strong&gt;Llama models are not good for Java&lt;/strong&gt;. Maverick #41 (71.12%) and Scout #58 (63.26%) are in the mid-range. This is the main reason for the bad overall score for DevQualityEval v1.0. Still, better scores than before: Llama 3.1 405B is #48 with 65.54%.&lt;/li&gt; &lt;li&gt;Ruby: Maverick made a &lt;strong&gt;huge leap to #13 in Ruby scoring&lt;/strong&gt; (91.65%, Llama 3.1 405B is #38 with 83.55%), on the other hand Scout #51 (79.22%) seems to be regressing over Llama 3.1 70B #42 (82.85%)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Comparing task scores:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Code repair: Maverick and Scout have a perfect 100% which is an improvement over Llama 3.1&lt;/li&gt; &lt;li&gt;- Migrate: Maverick leaped (71.22%) for migrating but Scout (57.92%) is comparable to the old 3.1 scores&lt;/li&gt; &lt;li&gt;Transpile: Scout (87.43%) has a much better score than Maverick (85.15%) which is a leap over 3.1 scores&lt;/li&gt; &lt;li&gt;Writing tests: Maverick (63.89%) is a good improvement over 3.1 scores, &lt;strong&gt;Scout (57.40%) seems to be regressing badly for writing tests&lt;/strong&gt; Both are great at writing Go tests, but only Maverick is good at writing Ruby tests. However, &lt;strong&gt;both Llama 4 models are terrible at writing Java tests&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me know if you want to see a deeper analysis for these models, and what you are interested in evaluating!&lt;/p&gt; &lt;p&gt;The full leaderboard has been already updated with the latest metrics and charts to choose your perfect model. And i will update the deep dive for v1.0 when the major models of these crazy week are available. &lt;a href="https://symflower.com/en/company/blog/2025/dev-quality-eval-v1.0-anthropic-s-claude-3.7-sonnet-is-the-king-with-help-and-deepseek-r1-disappoints/"&gt;https://symflower.com/en/company/blog/2025/dev-quality-eval-v1.0-anthropic-s-claude-3.7-sonnet-is-the-king-with-help-and-deepseek-r1-disappoints/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zimmski"&gt; /u/zimmski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jv9xxo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv9xxo/benchmark_results_for_llama_4_maverick_and_scout/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv9xxo/benchmark_results_for_llama_4_maverick_and_scout/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T16:24:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv5sdb</id>
    <title>Advise for people thinking about getting dual GPUs?</title>
    <updated>2025-04-09T13:28:32+00:00</updated>
    <author>
      <name>/u/LanceThunder</name>
      <uri>https://old.reddit.com/user/LanceThunder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;this is something i have been obsessing over lately so any help would be much appreciated. i just bought a 4060ti 16gb to run ollama and open webUI. i figured that i could buy it now and test it out and then buy another one next payday, only to pretend like i have some restraint. but when i woke up the next day all the 4060ti 16gb everywhere are sold out at all over. just overnight they are all gone now! fuck. i am sort of thinking about picking up a used 3090 or even a 3080. i could go with a 3060 12gb if i wanted to save money... or i could do what i have to do to get a 4060ti. but is dual GPUs even worth it?&lt;/p&gt; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &lt;p&gt;i am looking to run an instance of open webUI that can support a 8-14b model with 1-5 users.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LanceThunder"&gt; /u/LanceThunder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5sdb/advise_for_people_thinking_about_getting_dual_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5sdb/advise_for_people_thinking_about_getting_dual_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5sdb/advise_for_people_thinking_about_getting_dual_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T13:28:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1juwb0e</id>
    <title>Use AI as proxy to communicate with other human?</title>
    <updated>2025-04-09T03:22:55+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juwb0e/use_ai_as_proxy_to_communicate_with_other_human/"&gt; &lt;img alt="Use AI as proxy to communicate with other human?" src="https://preview.redd.it/7d7hcjvz7qte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=188e15cced0081b10de29d4f68a27164ba72494e" title="Use AI as proxy to communicate with other human?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7d7hcjvz7qte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juwb0e/use_ai_as_proxy_to_communicate_with_other_human/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juwb0e/use_ai_as_proxy_to_communicate_with_other_human/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T03:22:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1juphda</id>
    <title>Excited to present Vector Companion: A %100 local, cross-platform, open source multimodal AI companion that can see, hear, speak and switch modes on the fly to assist you as a general purpose companion with search and deep search features enabled on your PC. More to come later! Repo in the comments!</title>
    <updated>2025-04-08T21:44:27+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juphda/excited_to_present_vector_companion_a_100_local/"&gt; &lt;img alt="Excited to present Vector Companion: A %100 local, cross-platform, open source multimodal AI companion that can see, hear, speak and switch modes on the fly to assist you as a general purpose companion with search and deep search features enabled on your PC. More to come later! Repo in the comments!" src="https://external-preview.redd.it/ZmRwZWJybDVnb3RlMSY-bUZv8golKOTPEcs9ioJR7hnQy1I9Bc0nx92Tbrmm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2869afde6675b54147a12cb0891e8cce7db3b1f1" title="Excited to present Vector Companion: A %100 local, cross-platform, open source multimodal AI companion that can see, hear, speak and switch modes on the fly to assist you as a general purpose companion with search and deep search features enabled on your PC. More to come later! Repo in the comments!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xp0tcrl5gote1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juphda/excited_to_present_vector_companion_a_100_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juphda/excited_to_present_vector_companion_a_100_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T21:44:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1juhgy4</id>
    <title>World Record: DeepSeek R1 at 303 tokens per second by Avian.io on NVIDIA Blackwell B200</title>
    <updated>2025-04-08T16:14:49+00:00</updated>
    <author>
      <name>/u/avianio</name>
      <uri>https://old.reddit.com/user/avianio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juhgy4/world_record_deepseek_r1_at_303_tokens_per_second/"&gt; &lt;img alt="World Record: DeepSeek R1 at 303 tokens per second by Avian.io on NVIDIA Blackwell B200" src="https://external-preview.redd.it/w1uNpC9oR-BDfODib53NHNbbHwfxCxWJxaBmoZ3DyCw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ee2a4eb2ac98a062fe2d3cb503f6c3733267d31" title="World Record: DeepSeek R1 at 303 tokens per second by Avian.io on NVIDIA Blackwell B200" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At Avian.io, we have achieved 303 tokens per second in a collaboration with NVIDIA to achieve world leading inference performance on the Blackwell platform.&lt;/p&gt; &lt;p&gt;This marks a new era in test time compute driven models. We will be providing dedicated B200 endpoints for this model which will be available in the coming days, now available for preorder due to limited capacity &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/avianio"&gt; /u/avianio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.linkedin.com/feed/update/urn:li:share:7315398985362391040/?actorCompanyId=99470879"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juhgy4/world_record_deepseek_r1_at_303_tokens_per_second/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juhgy4/world_record_deepseek_r1_at_303_tokens_per_second/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T16:14:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv4hu6</id>
    <title>VideoDB MCP &amp; Claude code built it in 10 mins</title>
    <updated>2025-04-09T12:25:09+00:00</updated>
    <author>
      <name>/u/ashutrv</name>
      <uri>https://old.reddit.com/user/ashutrv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv4hu6/videodb_mcp_claude_code_built_it_in_10_mins/"&gt; &lt;img alt="VideoDB MCP &amp;amp; Claude code built it in 10 mins" src="https://external-preview.redd.it/cTFqMzM5YTN4c3RlMbEf5reoeEaPB3xFu8kG-mo10FFDL4XkEElenjX51TZ1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b89370a7276fd658872e6aadc2290eb766658655" title="VideoDB MCP &amp;amp; Claude code built it in 10 mins" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built matrix style video indexing app in 10 min using VideoDB and Claude Code. &lt;/p&gt; &lt;p&gt;The future belongs to fluid UI -- Build your own workflows with your imagination!&lt;/p&gt; &lt;p&gt;Claude Code : &lt;a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview"&gt;https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;VideoDB MCP : &lt;a href="https://videodb.io/mcp-developers"&gt;https://videodb.io/mcp-developers&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ashutrv"&gt; /u/ashutrv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y5i2a6a3xste1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv4hu6/videodb_mcp_claude_code_built_it_in_10_mins/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv4hu6/videodb_mcp_claude_code_built_it_in_10_mins/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T12:25:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1juz7o6</id>
    <title>I uploaded Q6 / Q5 quants of Mistral-Small-3.1-24B to ollama</title>
    <updated>2025-04-09T06:26:11+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.ollama.com/JollyLlama/Mistral-Small-3.1-24B"&gt;https://www.ollama.com/JollyLlama/Mistral-Small-3.1-24B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Since the official Ollama repo only has Q8 and Q4, I uploaded the Q5 and Q6 ggufs of Mistral-Small-3.1-24B to Ollama myself. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;These are quantized using ollama client, so these quants supports vision&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;-&lt;/p&gt; &lt;p&gt;On an RTX 4090 with 24GB of VRAM&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q8 KV Cache enabled&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Leave 1GB to 800MB of VRAM as buffer zone&lt;/p&gt; &lt;p&gt;-&lt;/p&gt; &lt;p&gt;Q6_K: 35K context&lt;/p&gt; &lt;p&gt;Q5_K_M: 64K context&lt;/p&gt; &lt;p&gt;Q4_K_S: 100K context&lt;/p&gt; &lt;p&gt;-&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama run JollyLlama/Mistral-Small-3.1-24B:Q6_K&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama run JollyLlama/Mistral-Small-3.1-24B:Q5_K_M&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama run JollyLlama/Mistral-Small-3.1-24B:Q4_K_S&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juz7o6/i_uploaded_q6_q5_quants_of_mistralsmall3124b_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juz7o6/i_uploaded_q6_q5_quants_of_mistralsmall3124b_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juz7o6/i_uploaded_q6_q5_quants_of_mistralsmall3124b_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T06:26:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1juzt8z</id>
    <title>LIVEBENCH - updated after 8 months (02.04.2025) - CODING - 1st o3 mini high, 2nd 03 mini med, 3rd Gemini 2.5 Pro</title>
    <updated>2025-04-09T07:10:30+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juzt8z/livebench_updated_after_8_months_02042025_coding/"&gt; &lt;img alt="LIVEBENCH - updated after 8 months (02.04.2025) - CODING - 1st o3 mini high, 2nd 03 mini med, 3rd Gemini 2.5 Pro" src="https://preview.redd.it/r9wik2qderte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57b4c2a255dea94e6dc61d081088d52ad6289dcd" title="LIVEBENCH - updated after 8 months (02.04.2025) - CODING - 1st o3 mini high, 2nd 03 mini med, 3rd Gemini 2.5 Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r9wik2qderte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juzt8z/livebench_updated_after_8_months_02042025_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juzt8z/livebench_updated_after_8_months_02042025_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T07:10:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvc768</id>
    <title>Google just launched the A2A protocol were AI agents from any framework can work together</title>
    <updated>2025-04-09T17:56:16+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvc768/google_just_launched_the_a2a_protocol_were_ai/"&gt; &lt;img alt="Google just launched the A2A protocol were AI agents from any framework can work together" src="https://preview.redd.it/azpf25q5lute1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c562212b7e129e18a030a5673a2221e17473b30" title="Google just launched the A2A protocol were AI agents from any framework can work together" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're working on an even more MCP-oriented approach to this problem and are building in the open here if anyone is interested, would love to see peoples opinions on both approaches to see what you think it all. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/azpf25q5lute1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvc768/google_just_launched_the_a2a_protocol_were_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvc768/google_just_launched_the_a2a_protocol_were_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T17:56:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jum5s1</id>
    <title>Cogito releases strongest LLMs of sizes 3B, 8B, 14B, 32B and 70B under open license</title>
    <updated>2025-04-08T19:24:51+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jum5s1/cogito_releases_strongest_llms_of_sizes_3b_8b_14b/"&gt; &lt;img alt="Cogito releases strongest LLMs of sizes 3B, 8B, 14B, 32B and 70B under open license" src="https://b.thumbs.redditmedia.com/UF9D2G4LY8nt9Z5N6vRdkaTUx2GI3fo84_fgBsrinvs.jpg" title="Cogito releases strongest LLMs of sizes 3B, 8B, 14B, 32B and 70B under open license" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cogito: ‚ÄúWe are releasing the strongest LLMs of sizes 3B, 8B, 14B, 32B and 70B under open license. Each model outperforms the best available open models of the same size, including counterparts from LLaMA, DeepSeek, and Qwen, across most standard benchmarks‚Äù&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/collections/deepcogito/cogito-v1-preview-67eb105721081abe4ce2ee53"&gt;https://huggingface.co/collections/deepcogito/cogito-v1-preview-67eb105721081abe4ce2ee53&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jum5s1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jum5s1/cogito_releases_strongest_llms_of_sizes_3b_8b_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jum5s1/cogito_releases_strongest_llms_of_sizes_3b_8b_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T19:24:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvbhlp</id>
    <title>I actually really like Llama 4 scout</title>
    <updated>2025-04-09T17:28:00+00:00</updated>
    <author>
      <name>/u/d13f00l</name>
      <uri>https://old.reddit.com/user/d13f00l</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running it on a 64 core Ampere Altra arm system with 128GB ram, no GPU, in llama.cpp with q6_k quant. It averages about 10 tokens a second which is great for personal use. It is answering coding questions and technical questions well. I have run Llama 3.3 70b, Mixtral 8x7b, Qwen 2.5 72b, some of the PHI models. The performance of scout is really good. Anecdotally it seems to be answering things at least as good as Llama 3.3 70b or Qwen 2.5 72b, at higher speeds. People aren't liking the model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/d13f00l"&gt; /u/d13f00l &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvbhlp/i_actually_really_like_llama_4_scout/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvbhlp/i_actually_really_like_llama_4_scout/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvbhlp/i_actually_really_like_llama_4_scout/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T17:28:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv469f</id>
    <title>New paper: SmolVLM: Redefining small and efficient multimodal models</title>
    <updated>2025-04-09T12:07:55+00:00</updated>
    <author>
      <name>/u/futterneid</name>
      <uri>https://old.reddit.com/user/futterneid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello folks, it's Andi from Hugging Face multimodal team (author of SmolVLM) üëãüèª &lt;/p&gt; &lt;p&gt;Yesterday, we released a &lt;a href="https://huggingface.co/papers/2504.05299"&gt;technical report&lt;/a&gt; for SmolVLM (aka your favorite smol vision LM) ü§ó&lt;/p&gt; &lt;p&gt;This technical report comes packed with a ton of findings, here I wanted to summarize them for you (read the paper if you're interested in more details):&lt;/p&gt; &lt;p&gt;- Longer context; big wins: Increasing the context length from 2K to 16K gave our tiny VLMs a 60% performance boost&lt;/p&gt; &lt;p&gt;- Smaller is smarter with SigLIP: Smaller LLMs didn't benefit from the usual large SigLIP (400M). Instead, we use the 80M base SigLIP that performs equally well at just 20% of the original size&lt;/p&gt; &lt;p&gt;- Pixel shuffling magic: Aggressively pixel shuffling helped our compact VLMs; better, achieving the same performance with sequences 16x shorter!&lt;/p&gt; &lt;p&gt;- Learned positional tokens FTW: For compact models, learned positional tokens significantly outperform raw text tokens, enhancing efficiency and accuracy.&lt;/p&gt; &lt;p&gt;- System prompts and special tokens are key: Introducing system prompts and dedicated media intro/outro tokens significantly boosted our compact VLM‚Äôs performance‚Äîespecially for video tasks.&lt;/p&gt; &lt;p&gt;- Less CoT, more efficiency: Too much Chain-of-Thought (CoT) data actually hurts performance in small models. They dumb&lt;/p&gt; &lt;p&gt;- Longer videos, better results: Increasing video length during training enhanced performance on both video and image tasks. State-of-the-Art Performance, SmolVLM comes in three powerful yet compact sizes‚Äî256M, 500M, and 2.2B parameters‚Äîeach setting new SOTA benchmarks for their hardware constraints in image and video understanding.&lt;/p&gt; &lt;p&gt;- Real-world Efficiency: We've created an app using SmolVLM on an iPhone 15 and got real-time inference directly from its camera!&lt;/p&gt; &lt;p&gt;- Browser-based Inference: We get lightning-fast inference speeds of 40-80 tokens per second directly in a web browser. No tricks, just compact, efficient models!&lt;/p&gt; &lt;p&gt;Give it a read and let us know what you think, I'll be also answering questions in case you have any &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/futterneid"&gt; /u/futterneid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv469f/new_paper_smolvlm_redefining_small_and_efficient/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv469f/new_paper_smolvlm_redefining_small_and_efficient/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv469f/new_paper_smolvlm_redefining_small_and_efficient/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T12:07:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv68hp</id>
    <title>Deep Research using the Agents SDK</title>
    <updated>2025-04-09T13:49:00+00:00</updated>
    <author>
      <name>/u/TheRedfather</name>
      <uri>https://old.reddit.com/user/TheRedfather</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv68hp/deep_research_using_the_agents_sdk/"&gt; &lt;img alt="Deep Research using the Agents SDK" src="https://external-preview.redd.it/DbUzqqfzeRze2FIyQ3VOz5Vy-7i0-B5t-Vs82DGWScs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4dc660267089d52535b44a359859d31e76491d5" title="Deep Research using the Agents SDK" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRedfather"&gt; /u/TheRedfather &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/qx-labs/agents-deep-research"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv68hp/deep_research_using_the_agents_sdk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv68hp/deep_research_using_the_agents_sdk/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T13:49:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv4k84</id>
    <title>KTransformers Now Supports LLaMA 4: Run q4 Maverick at 32 tokens/s with 10GB VRAM + 270GB RAM</title>
    <updated>2025-04-09T12:28:28+00:00</updated>
    <author>
      <name>/u/CombinationNo780</name>
      <uri>https://old.reddit.com/user/CombinationNo780</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv4k84/ktransformers_now_supports_llama_4_run_q4/"&gt; &lt;img alt="KTransformers Now Supports LLaMA 4: Run q4 Maverick at 32 tokens/s with 10GB VRAM + 270GB RAM" src="https://external-preview.redd.it/O5w_btCMTBBtqnFyVOraTvoye2DmLp3ZSDLb_P_jWm0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfe234cecb629681a25925d0579c9a04a40715be" title="KTransformers Now Supports LLaMA 4: Run q4 Maverick at 32 tokens/s with 10GB VRAM + 270GB RAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLaMA 4 is also a MoE model, which makes it well-suited for hybrid CPU/GPU inference. &lt;/p&gt; &lt;p&gt;KTransformers now offers &lt;em&gt;experimental support&lt;/em&gt; for LLaMA 4 under the development branch &lt;code&gt;support-llama4&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tjwvu403zste1.jpg?width=1226&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7872a75c957e1cfd140015292298d07fc45efb5e"&gt;https://preview.redd.it/tjwvu403zste1.jpg?width=1226&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7872a75c957e1cfd140015292298d07fc45efb5e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Key performance highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scout (16 Experts): ~65GB system memory, 10GB GPU VRAM&lt;/li&gt; &lt;li&gt;Maverick (128 Experts): ~270GB system memory, 12GB GPU VRAM&lt;/li&gt; &lt;li&gt;Both models require ~17B activation parameters per request. Thus, with a 4090 GPU and dual Xeon 4 CPUs, Scout/Maverick can both achieve up to 32 tokens/s for single batch.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;More details and setup instructions can be found here: &lt;a href="https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/llama4.md"&gt;https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/llama4.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombinationNo780"&gt; /u/CombinationNo780 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv4k84/ktransformers_now_supports_llama_4_run_q4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv4k84/ktransformers_now_supports_llama_4_run_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv4k84/ktransformers_now_supports_llama_4_run_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T12:28:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvchif</id>
    <title>How we used NVIDIA TensorRT-LLM with Blackwell B200 to achieve 303 output tokens per second on DeepSeek R1</title>
    <updated>2025-04-09T18:07:37+00:00</updated>
    <author>
      <name>/u/avianio</name>
      <uri>https://old.reddit.com/user/avianio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvchif/how_we_used_nvidia_tensorrtllm_with_blackwell/"&gt; &lt;img alt="How we used NVIDIA TensorRT-LLM with Blackwell B200 to achieve 303 output tokens per second on DeepSeek R1" src="https://external-preview.redd.it/Rb0aJ2SdT0s5u9VBtMk4r2KGdiNryvu9_uLzVS6423c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1de9d7d850207207499d5314450055f45c8256e" title="How we used NVIDIA TensorRT-LLM with Blackwell B200 to achieve 303 output tokens per second on DeepSeek R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a technical blog post on how the team at Avian collaborated with Nvidia to achieve 303 output tokens per second, using FP4 quantization and their new Pytorch runtime.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/avianio"&gt; /u/avianio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://new.avian.io/blog/article/deepseek_r1_303"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvchif/how_we_used_nvidia_tensorrtllm_with_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvchif/how_we_used_nvidia_tensorrtllm_with_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T18:07:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1juni3t</id>
    <title>DeepCoder: A Fully Open-Source 14B Coder at O3-mini Level</title>
    <updated>2025-04-08T20:20:30+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juni3t/deepcoder_a_fully_opensource_14b_coder_at_o3mini/"&gt; &lt;img alt="DeepCoder: A Fully Open-Source 14B Coder at O3-mini Level" src="https://a.thumbs.redditmedia.com/Y5BwtBnjZby6zmZDlawuWxCvPe3JSO0Wzb73zGMqhW4.jpg" title="DeepCoder: A Fully Open-Source 14B Coder at O3-mini Level" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1juni3t"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juni3t/deepcoder_a_fully_opensource_14b_coder_at_o3mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juni3t/deepcoder_a_fully_opensource_14b_coder_at_o3mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T20:20:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv46ye</id>
    <title>Qwen 2.5 Omni</title>
    <updated>2025-04-09T12:09:00+00:00</updated>
    <author>
      <name>/u/futterneid</name>
      <uri>https://old.reddit.com/user/futterneid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just read the Qwen2.5-Omni technical report from the Qwen team, it's super interesting. Here are my notes. &lt;/p&gt; &lt;p&gt;Qwen2.5-Omni is a unified end-to-end model that can perceive text, images, audio, and video ‚Äî and generate both text and natural speech responses in a streaming fashion. &lt;/p&gt; &lt;p&gt;At its core is the Thinker-Talker architecture:&lt;br /&gt; Thinker: a large language model that processes multimodal inputs and generates text.&lt;br /&gt; Talker: an autoregressive speech decoder that turns Thinker's hidden states into speech tokens. They're trained together, end-to-end. &lt;/p&gt; &lt;p&gt;Handling audio: audio is converted to 128-channel mel-spectrograms (16kHz, 25ms window, 10ms hop). Encoded via a modified Whisper model. Audio is processed in 2s blocks with streaming-compatible attention to reduce latency. &lt;/p&gt; &lt;p&gt;Handling video: uses a ViT-based encoder with dynamic frame sampling. Each frame is treated like an image. To sync with audio, they introduce TMRoPE ‚Äî Time-aligned Multimodal RoPE ‚Äî a novel positional embedding that aligns video and audio in time. &lt;/p&gt; &lt;p&gt;TMRoPE splits positional encoding into temporal, height, and width axes, letting Qwen2.5-Omni represent image/video/audio/text all on the same timeline. Interleaving of audio and visual tokens every 2 seconds enables synchronized fusion. &lt;/p&gt; &lt;p&gt;Streaming audio generation: audio tokens from Talker are decoded using a sliding-window DiT model + modified BigVGAN. The receptive field includes 2 lookback blocks and 1 lookahead to allow context-aware streaming audio generation. &lt;/p&gt; &lt;p&gt;Pretraining involved locking the LLM and training the audio/vision encoders first. Later stages unfreeze everything and train on a massive mix of audio-text, video-text, image-text, and long-sequence (32k tokens) data. &lt;/p&gt; &lt;p&gt;Post-training includes reinforcement learning for Talker to reduce hallucinations and improve pronunciation/timing. Plus, multi-speaker fine-tuning for better prosody and naturalness. &lt;/p&gt; &lt;p&gt;Qwen2.5-Omni achieves SOTA on OmniBench, AV-Odyssey, and strong results across text, image, audio, and video tasks. End-to-end speech instruction following is nearly on par with text-based inputs. That's rare. &lt;/p&gt; &lt;p&gt;Overall: a super ambitious and well-integrated multimodal model. The Thinker-Talker separation is elegant. TMRoPE is a clever solution to a tricky problem. &lt;/p&gt; &lt;p&gt;That said, I wish the paper had included more ablation studies or experiments justifying some of the architectural decisions. Many claims are reasonable but would benefit from more empirical evidence. &lt;/p&gt; &lt;p&gt;Still, major kudos to the team. Qwen2.5-Omni is a big step toward real-time, unified multimodal assistants.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/futterneid"&gt; /u/futterneid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv46ye/qwen_25_omni/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv46ye/qwen_25_omni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv46ye/qwen_25_omni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T12:09:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv7x6l</id>
    <title>Hogwild! Inference: Parallel LLM Generation via Concurrent Attention</title>
    <updated>2025-04-09T15:01:53+00:00</updated>
    <author>
      <name>/u/Psychological-Tea652</name>
      <uri>https://old.reddit.com/user/Psychological-Tea652</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv7x6l/hogwild_inference_parallel_llm_generation_via/"&gt; &lt;img alt="Hogwild! Inference: Parallel LLM Generation via Concurrent Attention" src="https://external-preview.redd.it/b3BjbHE0c2ZwdHRlMWcmQ4x0UIQwBXGX5ihDQRS0yvkPTeRAH8Mf_AVWxETI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49a6c2066bf6d904c4233f96449ce01283d4b000" title="Hogwild! Inference: Parallel LLM Generation via Concurrent Attention" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The paper modifies LLM attention so multiple &amp;quot;workers&amp;quot; can see each other's thoughts (KV) in real time. They generate text in parallel like humans use Google Docs. Turns out, they can self-organize, split the work and cross-verify. Works with open-source models like QwQ-32B. Check it out!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Paper &amp;amp; code:&lt;/strong&gt; &lt;a href="https://huggingface.co/papers/2504.06261"&gt;https://huggingface.co/papers/2504.06261&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Project page:&lt;/strong&gt; &lt;a href="https://eqimp.github.io/hogwild_llm"&gt;https://eqimp.github.io/hogwild_llm&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological-Tea652"&gt; /u/Psychological-Tea652 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/q36zd4sfptte1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv7x6l/hogwild_inference_parallel_llm_generation_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv7x6l/hogwild_inference_parallel_llm_generation_via/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T15:01:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv9s6q</id>
    <title>LMSYS WebDev Arena updated with DeepSeek-V3-0324 and Llama 4 models.</title>
    <updated>2025-04-09T16:18:25+00:00</updated>
    <author>
      <name>/u/jpydych</name>
      <uri>https://old.reddit.com/user/jpydych</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv9s6q/lmsys_webdev_arena_updated_with_deepseekv30324/"&gt; &lt;img alt="LMSYS WebDev Arena updated with DeepSeek-V3-0324 and Llama 4 models." src="https://preview.redd.it/ew55ayg24ute1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb7de8d9615d9f552f6a7a05c87acda4c5a6a656" title="LMSYS WebDev Arena updated with DeepSeek-V3-0324 and Llama 4 models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jpydych"&gt; /u/jpydych &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ew55ayg24ute1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv9s6q/lmsys_webdev_arena_updated_with_deepseekv30324/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv9s6q/lmsys_webdev_arena_updated_with_deepseekv30324/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T16:18:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv5xv7</id>
    <title>Google Ironwood TPU (7th generation) introduction</title>
    <updated>2025-04-09T13:35:39+00:00</updated>
    <author>
      <name>/u/zimmski</name>
      <uri>https://old.reddit.com/user/zimmski</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/"&gt;https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When i see Google's TPUs, i always ask myself if there is any company working on a local variant that us mortals can buy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zimmski"&gt; /u/zimmski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5xv7/google_ironwood_tpu_7th_generation_introduction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5xv7/google_ironwood_tpu_7th_generation_introduction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5xv7/google_ironwood_tpu_7th_generation_introduction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T13:35:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv22mm</id>
    <title>Qwen3 and Qwen3-MoE support merged into llama.cpp</title>
    <updated>2025-04-09T10:00:19+00:00</updated>
    <author>
      <name>/u/matteogeniaccio</name>
      <uri>https://old.reddit.com/user/matteogeniaccio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv22mm/qwen3_and_qwen3moe_support_merged_into_llamacpp/"&gt; &lt;img alt="Qwen3 and Qwen3-MoE support merged into llama.cpp" src="https://external-preview.redd.it/-e19x77nCYUlOdA0buk2ekzVqo7mRcwDr167KRd89n4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8c6fee2cf4cf6c565073243d3b38360e7ec134d" title="Qwen3 and Qwen3-MoE support merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Support merged.&lt;/p&gt; &lt;p&gt;We'll have GGUF models on day one&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matteogeniaccio"&gt; /u/matteogeniaccio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12828"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv22mm/qwen3_and_qwen3moe_support_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv22mm/qwen3_and_qwen3moe_support_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T10:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv71su</id>
    <title>Granite 3.3 imminent?</title>
    <updated>2025-04-09T14:24:45+00:00</updated>
    <author>
      <name>/u/das_rdsm</name>
      <uri>https://old.reddit.com/user/das_rdsm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv71su/granite_33_imminent/"&gt; &lt;img alt="Granite 3.3 imminent?" src="https://preview.redd.it/g2ceteotjtte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60dab73cdbdeadc070ee093587327ecdf15ea289" title="Granite 3.3 imminent?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apparently they added and then edited the collection. maybe it will be released today?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/das_rdsm"&gt; /u/das_rdsm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g2ceteotjtte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv71su/granite_33_imminent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv71su/granite_33_imminent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T14:24:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv5uk8</id>
    <title>OmniSVG: A Unified Scalable Vector Graphics Generation Model</title>
    <updated>2025-04-09T13:31:15+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/"&gt; &lt;img alt="OmniSVG: A Unified Scalable Vector Graphics Generation Model" src="https://external-preview.redd.it/MHI3ZzMzc3Q5dHRlMexiJYH3Awkmn9VEWXtNssspPIW9nVy43T4cWZBoNTdU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df299d5217413408d56d4fe1cca2c1dc834179f7" title="OmniSVG: A Unified Scalable Vector Graphics Generation Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just saw this on X. If this is true, this SVG generation capability is really amazing, and I can't wait to run it locally. I checked and it seems the model weights haven't been released on Hugging Face yet.&lt;/p&gt; &lt;p&gt;site: &lt;a href="http://omnisvg.github.io"&gt;omnisvg.github.io&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jk6dp2st9tte1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T13:31:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv5vic</id>
    <title>Alibaba AI Conference happening today! We may see Qwen3 in a few hours!</title>
    <updated>2025-04-09T13:32:27+00:00</updated>
    <author>
      <name>/u/MushroomGecko</name>
      <uri>https://old.reddit.com/user/MushroomGecko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5vic/alibaba_ai_conference_happening_today_we_may_see/"&gt; &lt;img alt="Alibaba AI Conference happening today! We may see Qwen3 in a few hours!" src="https://preview.redd.it/w79q9k0katte1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a242dd332547aef1d5aceaf7a7c766055e6c50e" title="Alibaba AI Conference happening today! We may see Qwen3 in a few hours!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MushroomGecko"&gt; /u/MushroomGecko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w79q9k0katte1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5vic/alibaba_ai_conference_happening_today_we_may_see/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5vic/alibaba_ai_conference_happening_today_we_may_see/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T13:32:27+00:00</published>
  </entry>
</feed>
