<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-03T15:37:45+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mfqejn</id>
    <title>Open-source model that is as intelligent as Claude Sonnet 4</title>
    <updated>2025-08-02T13:21:11+00:00</updated>
    <author>
      <name>/u/vishwa1238</name>
      <uri>https://old.reddit.com/user/vishwa1238</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spend about 300-400 USD per month on Claude Code with the max 5x tier. I‚Äôm unsure when they‚Äôll increase pricing, limit usage, or make models less intelligent. I‚Äôm looking for a cheaper or open-source alternative that‚Äôs just as good for programming as Claude Sonnet 4. Any suggestions are appreciated. &lt;/p&gt; &lt;p&gt;Edit: I don‚Äôt pay $300-400 per month. I have Claude Max subscription (100$) that comes with a Claude code. I used a tool called ccusage to check my usage, and it showed that I use approximately $400 worth of API every month on my Claude Max subscription. It works fine now, but I‚Äôm quite certain that, just like what happened with cursor, there will likely be a price increase or a higher rate limiting soon. &lt;/p&gt; &lt;p&gt;Thanks for all the suggestions. I‚Äôll try out Kimi2, R1, qwen 3, glm4.5 and Gemini 2.5 Pro and update how it goes in another post. :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vishwa1238"&gt; /u/vishwa1238 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T13:21:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfuiri</id>
    <title>Qwen Code + Qwen Coder 30b 3A is insane</title>
    <updated>2025-08-02T16:17:55+00:00</updated>
    <author>
      <name>/u/Flashy_Management962</name>
      <uri>https://old.reddit.com/user/Flashy_Management962</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is just a little remark that if you haven't you definitely should try qwen code &lt;a href="https://github.com/QwenLM/qwen-code"&gt;https://github.com/QwenLM/qwen-code&lt;/a&gt;&lt;br /&gt; I use qwen coder and qwen 3 30b thinking while the latter still needs some copy and pasting. I'm working on and refining a script for syncing my koreader metadata with obsidian for the plugin lineage (every highlight in own section). The last time I tried to edit it, I used Grok 4 and Claude Sonnet Thinking on Perplexity (its the only subscription I had until know) even with those models it was tedious and not really working. But with Qwen Code it looks very different to be honest. &lt;/p&gt; &lt;p&gt;The metadata is in written in lua which at first was a pain to parse right (remember, I actually cannot code by myself, I understand the logic and I can tell in natural language what is wrong, but nothing more) and I got qwen code running today with llama cpp and it almost integrated everything on the first try and I'm very sure that nothing of that was in the models trainingdata. We reached a point where - if we know a little bit - can let code be written for us almost without us needing to know what is happening at all, running on a local machine. Of course it is very advantageous to know what you are looking for.&lt;/p&gt; &lt;p&gt;So this is just a little recommendation, if you have not tried qwen code, do it. I guess its almost only really useful for people like me, who don't know jack shit about coding. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy_Management962"&gt; /u/Flashy_Management962 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T16:17:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgk2nm</id>
    <title>Your proud AI setup</title>
    <updated>2025-08-03T14:04:15+00:00</updated>
    <author>
      <name>/u/Recent-Success-1520</name>
      <uri>https://old.reddit.com/user/Recent-Success-1520</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let's tease each other.&lt;/p&gt; &lt;p&gt;What is your local AI setup? Are you proud of it? What would you have done differently?&lt;/p&gt; &lt;p&gt;What model you use? Contrxt lenght? TPS?&lt;/p&gt; &lt;p&gt;I only have a MBP2019, so I will just be teased üòÇ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recent-Success-1520"&gt; /u/Recent-Success-1520 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgk2nm/your_proud_ai_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgk2nm/your_proud_ai_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgk2nm/your_proud_ai_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T14:04:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mga3ox</id>
    <title>I made a prebuilt windows binary for ik_llama.cpp</title>
    <updated>2025-08-03T04:19:52+00:00</updated>
    <author>
      <name>/u/Remarkable-Pea645</name>
      <uri>https://old.reddit.com/user/Remarkable-Pea645</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/X5R/ik_llama.cpp"&gt;https://huggingface.co/X5R/ik_llama.cpp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Pea645"&gt; /u/Remarkable-Pea645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mga3ox/i_made_a_prebuilt_windows_binary_for_ik_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mga3ox/i_made_a_prebuilt_windows_binary_for_ik_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mga3ox/i_made_a_prebuilt_windows_binary_for_ik_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T04:19:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgg3mh</id>
    <title>Successfully running INSTINCT MI50 on Win11</title>
    <updated>2025-08-03T10:38:30+00:00</updated>
    <author>
      <name>/u/Desperate-Sir-5088</name>
      <uri>https://old.reddit.com/user/Desperate-Sir-5088</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgg3mh/successfully_running_instinct_mi50_on_win11/"&gt; &lt;img alt="Successfully running INSTINCT MI50 on Win11" src="https://b.thumbs.redditmedia.com/ac8XNi9a_cAzytIhKD5Gg-Gwa0GLMmvvfiuUz9ia33E.jpg" title="Successfully running INSTINCT MI50 on Win11" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, poor GPU guys&lt;/p&gt; &lt;p&gt;A few days ago, I purchased the 32GB version of MI50 from Alibaba, and it arrived at my doorstep via UPS in just a few days, accompanied by a rather loud blower.&lt;/p&gt; &lt;p&gt;Some married guys might understand, but I‚Äôve been using an m-ATX case I bought about 15 years ago, and there‚Äôs no room for the MI50 since the 4070ti is already in there. I went ahead and used a PCIe riser cable to mount it on the side of my desk, and then I finally got down to ‚Äúreal‚Äù work.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9e8c9de636b034bd89f57dddca7291dcce101a80"&gt;https://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9e8c9de636b034bd89f57dddca7291dcce101a80&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One of the reasons the MI50 was rejected is that AMD only developed drivers for Linux and has since discontinued support, as most people are aware. That's why the ‚Äú32GB‚Äù model ended up in my hands.&lt;/p&gt; &lt;p&gt;Of course, some experts claim they can force-install the Radeon Pro VII BIOS, but that seemed too challenging for me, and after reading many posts stating that the ‚ÄúOriginal MI50‚Äù cannot be BIOS-re-flashed, I had given up.&lt;/p&gt; &lt;p&gt;First, take a look at the results: the MI50 is running with GTX 4070ti or alone on Windows.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pvictbhu4sgf1.png?width=1707&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49102c07c665ed9635a4c99e9a3aed46da15c6a9"&gt;https://preview.redd.it/pvictbhu4sgf1.png?width=1707&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49102c07c665ed9635a4c99e9a3aed46da15c6a9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3cabctch4sgf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ed2201e24962d94990ea9bc149d8ec389982b96"&gt;4070+MI50 (22GB only)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lrtre5656sgf1.png?width=1883&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f14e51095d28e301c62418f12f52586a62861ee2"&gt;MI50 works alone upto 30GB&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Guys, hold your horses. I'm aware there are a few issues here.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;It's recognized as a Radeon Pro VII&lt;/li&gt; &lt;li&gt;It runs on LM STUDIO, which some people really dislike&lt;/li&gt; &lt;li&gt;Even if it's recognized as Vulkan, you can't use the combined VRAM of hetero cards‚Äîonly twice the VRAM of the first graphics card (On my PC, it's 12+12GB instead of 12+32GB&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;-&amp;gt; However, I haven't tested it yet, but if you get a 32GB 5090 or V100, it might work with 32+32, and being able to steal GTX's prompt processing ability is an extra bonus.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Surely, you could use whole 32GB if you turn off other GPU and MI50 only!!!&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Anyway, there are only three things you need to do.&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Disable Secure Boot in the CMOS BIOS.&lt;/li&gt; &lt;li&gt;Run PowerShell in administrator mode and enter the following command:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;bcdedit.exe -set TESTSIGNING on&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Download and install the Polaris-Vega-Navi driver created by the real pros.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://rdn-id.com/"&gt;R.ID - AMD 3rd Party Drivers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All risks are on you, but I think it's better than getting divorced by your wife over buying an RTX 6000,&lt;/p&gt; &lt;p&gt;The blower fan sent by the Ali seller is very effective, but it's incredibly loud. The GPU also gets quite hot, so you might want to find a way to adjust the fan speed.&lt;/p&gt; &lt;p&gt;P.S. Could you please share a link to a guide on how to install ROCM to support MI50 on Ubuntu 24.04 LTS? I tried version 6.3.3, but it doesn't recognize it at all. Do I really have to rebuild PyTorch from scratch?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate-Sir-5088"&gt; /u/Desperate-Sir-5088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgg3mh/successfully_running_instinct_mi50_on_win11/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgg3mh/successfully_running_instinct_mi50_on_win11/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgg3mh/successfully_running_instinct_mi50_on_win11/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T10:38:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgfs7l</id>
    <title>Do you also get weird behavior from Qwen3-Coder-30B-A3B?</title>
    <updated>2025-08-03T10:18:02+00:00</updated>
    <author>
      <name>/u/Admirable-Star7088</name>
      <uri>https://old.reddit.com/user/Admirable-Star7088</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was using this model as an assistant to modify code in a C++ file with ~roughly 800 lines of code. However, the model did a lot of mistakes, and it constantly corrected itself (in the same reply) in a way like:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Here is the modification of the code:&lt;/p&gt; &lt;p&gt;&lt;em&gt;\&lt;/em&gt;code**&lt;/p&gt; &lt;p&gt;But on a second thought, that was not a good implementation, here is a better method to do it:&lt;/p&gt; &lt;p&gt;&lt;em&gt;\&lt;/em&gt;code**&lt;/p&gt; &lt;p&gt;But on a third thought, here is a better way to do it...&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And each method it provided had mistakes, such as missing lines of code. I then tried something very simple with HTML and prompted:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;How do I set a left, right and bottom border on a div in CSS?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;It then provided me with 4 methods. Notice how &lt;strong&gt;method 1&lt;/strong&gt; and &lt;strong&gt;method 4&lt;/strong&gt; is the &lt;em&gt;exact same&lt;/em&gt; code repeated. &lt;strong&gt;Method 2&lt;/strong&gt; is also almost the exact same code but just with an added &lt;code&gt;border: none;&lt;/code&gt; line of code. &lt;/p&gt; &lt;p&gt;Also, &lt;strong&gt;method 3&lt;/strong&gt; has a mistake where it do not to set a border on the right side of the div, it wrote &lt;code&gt;border-width: 0 0 2px 2px;&lt;/code&gt; but the correct code would be &lt;code&gt;border-width: 0 2px 2px 2px;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The output:&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Method 1: Using Individual Border Properties&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;div { border-left: 2px solid black; border-right: 2px solid black; border-bottom: 2px solid black; } &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Method 2: Using the Shorthand border Property&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;div { border: none; /* Reset all borders first */ border-left: 2px solid black; border-right: 2px solid black; border-bottom: 2px solid black; } &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Method 3: Using border-width, border-style, and border-color&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;div { border-width: 0 0 2px 2px; /* top right bottom left */ border-style: solid; border-color: black; } &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Method 4: More Concise Shorthand&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;div { border-left: 2px solid black; border-right: 2px solid black; border-bottom: 2px solid black; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm using Unsloth's UD-Q5_K_XL quant with the recommended settings:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Temperature: &lt;strong&gt;0,7&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Top K: &lt;strong&gt;20&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Repeat Penalty: &lt;strong&gt;1,05&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Top P: &lt;strong&gt;0,8&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Anyone else having similar odd behavior with this model? Might the quant/jinja be broken currently?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable-Star7088"&gt; /u/Admirable-Star7088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgfs7l/do_you_also_get_weird_behavior_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgfs7l/do_you_also_get_weird_behavior_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgfs7l/do_you_also_get_weird_behavior_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T10:18:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgdur5</id>
    <title>üß† ICM+DPO: Used Qwen3's coherent understanding to improve Gemma3 at math - cross-model capability transfer with zero supervision</title>
    <updated>2025-08-03T08:10:25+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! &lt;/p&gt; &lt;p&gt;Just released something that extends the recent &lt;a href="https://arxiv.org/abs/2506.10139"&gt;ICM paper&lt;/a&gt; in a big way - using one model's coherent understanding to improve a completely different model.&lt;/p&gt; &lt;h1&gt;Background: What is ICM?&lt;/h1&gt; &lt;p&gt;The original &lt;a href="https://arxiv.org/abs/2506.10139"&gt;&amp;quot;Unsupervised Elicitation of Language Models&amp;quot;&lt;/a&gt; paper showed something remarkable: &lt;strong&gt;models can generate their own training labels by finding internally coherent patterns&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Their key insight: pretrained models already understand concepts like mathematical correctness, but struggle to express this knowledge consistently. ICM finds label assignments that are &amp;quot;mutually predictable&amp;quot; - where each label can be predicted from all the others.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Original ICM results&lt;/strong&gt;: Matched performance of golden supervision without any external labels. Pretty amazing, but only improved the same model using its own labels.&lt;/p&gt; &lt;h1&gt;Our extension: Cross-model capability transfer&lt;/h1&gt; &lt;p&gt;We took ICM further - &lt;strong&gt;what if we use one model's coherent understanding to improve a completely different model?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our process:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Used ICM on Qwen3 to extract its coherent math reasoning patterns&lt;/li&gt; &lt;li&gt;Generated DPO training data from Qwen3's coherent vs incoherent solutions&lt;/li&gt; &lt;li&gt;Trained Gemma3 on this data - &lt;strong&gt;Gemma3 learned from Qwen3's understanding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Zero external supervision, pure model-to-model knowledge transfer&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Results on local models&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Qwen3-0.6B&lt;/strong&gt;: 63.2 ‚Üí 66.0 MATH-500 (+4%) [original ICM self-improvement]&lt;br /&gt; &lt;strong&gt;Gemma3-1B&lt;/strong&gt;: 41.0 ‚Üí 45.6 MATH-500 (+11%) [&lt;strong&gt;novel: learned from Qwen3!&lt;/strong&gt;]&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The breakthrough&lt;/strong&gt;: Successfully transferred mathematical reasoning coherence from Qwen3 to improve Gemma3's abilities across different architectures.&lt;/p&gt; &lt;h1&gt;Why this matters beyond the original paper&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cross-model knowledge transfer&lt;/strong&gt; - use any strong model to improve your local models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Democratizes capabilities&lt;/strong&gt; - extract from closed/expensive models to improve open ones&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No training data needed&lt;/strong&gt; - pure capability extraction and transfer&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scales the ICM concept&lt;/strong&gt; - from self-improvement to ecosystem-wide improvement&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What's available&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href="https://github.com/codelion/icm"&gt;https://github.com/codelion/icm&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Both models&lt;/strong&gt;: Self-improved Qwen3 + Gemma3 (learned from Qwen3)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transfer pipeline&lt;/strong&gt;: Extract from any model to improve another&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full writeup&lt;/strong&gt;: &lt;a href="https://huggingface.co/blog/codelion/internal-coherence-maximization"&gt;https://huggingface.co/blog/codelion/internal-coherence-maximization&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Quick start&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/codelion/icm.git &amp;amp;&amp;amp; cd icm &amp;amp;&amp;amp; pip install -e . # Extract coherent patterns from a strong model (teacher) icm run --model Qwen/Qwen2.5-Math-7B-Instruct --dataset gsm8k --max-examples 500 # Use those patterns to improve your local model (student) icm export --format dpo --output-path teacher_knowledge.jsonl # Train your model on teacher_knowledge.jsonl &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Anyone interested in trying capability transfer with their local models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgdur5/icmdpo_used_qwen3s_coherent_understanding_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgdur5/icmdpo_used_qwen3s_coherent_understanding_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgdur5/icmdpo_used_qwen3s_coherent_understanding_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T08:10:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg7qpa</id>
    <title>Announcing Olla - LLM Load Balancer, Proxy &amp; Model Unifier for Ollama / LM Studio &amp; OpenAI Compatible backends</title>
    <updated>2025-08-03T02:14:39+00:00</updated>
    <author>
      <name>/u/2shanigans</name>
      <uri>https://old.reddit.com/user/2shanigans</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/"&gt; &lt;img alt="Announcing Olla - LLM Load Balancer, Proxy &amp;amp; Model Unifier for Ollama / LM Studio &amp;amp; OpenAI Compatible backends" src="https://b.thumbs.redditmedia.com/aNDVZRvyIDhy6vwT1mm7Ch4ypRPRTf4JSNM_Np7grwg.jpg" title="Announcing Olla - LLM Load Balancer, Proxy &amp;amp; Model Unifier for Ollama / LM Studio &amp;amp; OpenAI Compatible backends" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've been working on an LLM proxy, balancer &amp;amp; model unifier based on a few other projects we've created in the past (scout, sherpa) to enable us to run several ollama / lmstudio backends and serve traffic for local-ai. &lt;/p&gt; &lt;p&gt;This was primarily after running into the same issues across several organisations - managing multiple LLM backend instances &amp;amp; routing/failover etc. We use this currently across several organisations who self-host their AI workloads (one organisation, has a bunch of MacStudios, another has RTX 6000s in their onprem racks and another lets people use their laptops at home, their work infra onsite),&lt;/p&gt; &lt;p&gt;So some folks run the dockerised versions and point their tooling (like Junie for example) at Olla and use it between home / work.&lt;/p&gt; &lt;p&gt;Olla currently natively supports Ollama and LMStudio, with Lemonade, vLLM and a few others being added soon.&lt;/p&gt; &lt;p&gt;Add your LLM endpoints into a config file, Olla will discover the models (and unify per-provider), manage health updates and route based on the balancer you pick.&lt;/p&gt; &lt;p&gt;The attempt to unify across providers wasn't as successful - as in, both LMStudio &amp;amp; Ollama, the nuances in naming causes more grief than its worth (right now). Maybe revisit later once other things have been implemented.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/thushan/olla"&gt;https://github.com/thushan/olla&lt;/a&gt; (golang)&lt;/p&gt; &lt;p&gt;Would love to know your thoughts. &lt;/p&gt; &lt;p&gt;Olla is still in its infancy, so we don't have auth implemented etc but there are plans in the future.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/2shanigans"&gt; /u/2shanigans &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mg7qpa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T02:14:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgjcai</id>
    <title>Roleplay with large historical context and RAG</title>
    <updated>2025-08-03T13:31:48+00:00</updated>
    <author>
      <name>/u/RoboCopsGoneMad</name>
      <uri>https://old.reddit.com/user/RoboCopsGoneMad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I play The Expanse role-playing game with some friends every week over Zoom. I've captured the transcripts for every session. I intend to run an LLM locally for players to interact with during the game and so it should act as if it were the AI of the ship. &lt;/p&gt; &lt;p&gt;From a high level, the pipeline goes like this; After every session, I download a transcript from Zoom, I put it through some basic pre-processing to clean it up and minimize the size. I run it through Claude Opus 4 with a very specific prompt on how to best summarize it and that is stored for later use. I run LM studio locally on an M4 MacBook with 48 gigs of RAM. The summaries are appended together into one large historical record for the campaign. That historical record is sent as the first message in the conversation. I have a scripting system that allows the players to interact with the LLM through &lt;a href="http://roll20.net"&gt;roll20.net&lt;/a&gt; (a virtual tabletop website) as if it were a chat participant.&lt;/p&gt; &lt;p&gt;It's been a while since I explored the state of the art for this problem space and it seems that a large number of Chinese models have been opened sourced, and so I am wondering if any of them are particularly good at role-play applications. I've defaulted to using mlx-community/Meta-Llama-3.1-8B-Instruct-8bit (64k context tokens) for now , but it seems to be really bad at accurately recalling historical events. It regularly mixes up facts and conflates events.&lt;/p&gt; &lt;p&gt;I haven't learned much about training/retraining/pretraining/fine-tuning yes, and I'm wondering if those are better approaches than just bootstrapping the convo&lt;/p&gt; &lt;p&gt;Other Features in flight:&lt;/p&gt; &lt;p&gt;Integrating with WolframAlpha over MCP so that players can ask for the AI to execute astronomical tasks, such as &amp;quot;how long will it take us to get to Callisto from Himalia if we travel at .3 G acceleration&amp;quot;. &lt;/p&gt; &lt;p&gt;Loading the core role book and supplement PDFs into the system for searching via RAG. Ideally, this could be used for looking up rules during gameplay. My experiences with RAG has been not great. I'm sure I'm using it incorrectly or perhaps enabling it during inference when it shouldn't be. I could definitely use some advice on that.&lt;/p&gt; &lt;p&gt;This must be a common idea, and I'm sure others are working on similar applications; how do I find them?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RoboCopsGoneMad"&gt; /u/RoboCopsGoneMad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgjcai/roleplay_with_large_historical_context_and_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgjcai/roleplay_with_large_historical_context_and_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgjcai/roleplay_with_large_historical_context_and_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T13:31:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgl1qz</id>
    <title>Is EXL3 doomed?</title>
    <updated>2025-08-03T14:45:24+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/"&gt; &lt;img alt="Is EXL3 doomed?" src="https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=778afd8299a6ccb54136a78390cc8473e58bebed" title="Is EXL3 doomed?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was very excited for the release of EXL3 because of its increased performance and revised design to support new models easier. It‚Äôs been an eternity since is early preview‚Ä¶ and now I wonder if it is doomed. Not just because it‚Äôs slow to release, but because models are moving towards large MoEs that all but require they spill over into RAM for most of us. Still, we are getting models around 32b. So what do you think? Or what do you know? Is it on its way? Will it still be helpful?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/turboderp-org/exllamav3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T14:45:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgdypr</id>
    <title>Best Medical Embedding Model Released</title>
    <updated>2025-08-03T08:17:31+00:00</updated>
    <author>
      <name>/u/DataNebula</name>
      <uri>https://old.reddit.com/user/DataNebula</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just dropped a new medical embedding model that's crushing the competition: &lt;a href="https://huggingface.co/lokeshch19/ModernPubMedBERT"&gt;https://huggingface.co/lokeshch19/ModernPubMedBERT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TL;DR: This model understands medical concepts better than existing solutions and has much fewer false positives.&lt;/p&gt; &lt;p&gt;The model is based on bioclinical modernbert, fine-tuned on PubMed title-abstract pairs using InfoNCE loss with 2048 token context.&lt;/p&gt; &lt;p&gt;The model demonstrates deeper comprehension of medical terminology, disease relationships, and clinical pathways through specialized training on PubMed literature. Advanced fine-tuning enabled nuanced understanding of complex medical semantics, symptom correlations, and treatment associations.&lt;/p&gt; &lt;p&gt;The model also exhibits deeper understanding to distinguish medical from non-medical content, significantly reducing false positive matches in cross-domain scenarios. Sophisticated discrimination capabilities ensure clear separation between medical terminology and unrelated domains like programming, general language, or other technical fields.&lt;/p&gt; &lt;p&gt;Download the model, test it on your medical datasets, and give it a ‚≠ê on the &lt;a href="https://huggingface.co/lokeshch19/ModernPubMedBERT"&gt;Hugging Face&lt;/a&gt; if it enhances your workflow!&lt;/p&gt; &lt;p&gt;Edit: Added evals to HF model card&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataNebula"&gt; /u/DataNebula &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgdypr/best_medical_embedding_model_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgdypr/best_medical_embedding_model_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgdypr/best_medical_embedding_model_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T08:17:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgbprh</id>
    <title>SmallThinker-21B-A3B-Instruct-QAT version</title>
    <updated>2025-08-03T05:53:55+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgbprh/smallthinker21ba3binstructqat_version/"&gt; &lt;img alt="SmallThinker-21B-A3B-Instruct-QAT version" src="https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d016e4dfb71f7ddb2afc8c6c06ee782ba15871a" title="SmallThinker-21B-A3B-Instruct-QAT version" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The larger SmallThinker MoE has been through a quantization aware training process. it's uploaded to the same gguf repo a bit later. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In llama.cpp m2 air 16gb, with the &lt;code&gt;sudo sysctl iogpu.wired_limit_mb=13000&lt;/code&gt; command, it's 30 t/s. &lt;/p&gt; &lt;p&gt;The model is CPU inference optimised for very low RAM provisions + fast disc, alongside sparsity optimizations, in their llama.cpp fork. The models are pre-trained from scratch. This group always had a good eye for inference optimizations, Always happy to see their works.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgbprh/smallthinker21ba3binstructqat_version/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgbprh/smallthinker21ba3binstructqat_version/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T05:53:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg7abc</id>
    <title>Mac + Blackwell üëÄ</title>
    <updated>2025-08-03T01:51:15+00:00</updated>
    <author>
      <name>/u/Accomplished_Ad9530</name>
      <uri>https://old.reddit.com/user/Accomplished_Ad9530</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg7abc/mac_blackwell/"&gt; &lt;img alt="Mac + Blackwell üëÄ" src="https://preview.redd.it/u2mr83o6npgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=161c4ac9304218f08618c511e09178e7a7c08931" title="Mac + Blackwell üëÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a WIP, but it's looking like may be possible to pair Macs with NVIDIA soon!&lt;/p&gt; &lt;p&gt;Tweet: &lt;a href="https://x.com/anemll/status/1951307167417639101"&gt;https://x.com/anemll/status/1951307167417639101&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/anemll/anemll"&gt;https://github.com/anemll/anemll&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished_Ad9530"&gt; /u/Accomplished_Ad9530 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u2mr83o6npgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg7abc/mac_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg7abc/mac_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T01:51:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgc0v0</id>
    <title>I created an app to run local AI as if it were the App Store</title>
    <updated>2025-08-03T06:12:39+00:00</updated>
    <author>
      <name>/u/Deivih-4774</name>
      <uri>https://old.reddit.com/user/Deivih-4774</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgc0v0/i_created_an_app_to_run_local_ai_as_if_it_were/"&gt; &lt;img alt="I created an app to run local AI as if it were the App Store" src="https://a.thumbs.redditmedia.com/DMpoJ6kbQBgxqPJp_64cY0H-qxr9jJD3Tr3RuFU7894.jpg" title="I created an app to run local AI as if it were the App Store" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I got tired of installing AI tools the hard way.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Every time I wanted to try something like Stable Diffusion, RVC or a local LLM, it was the same nightmare:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;terminal commands, missing dependencies, broken CUDA, slow setup, frustration.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;So I built &lt;strong&gt;Dione&lt;/strong&gt; ‚Äî a desktop app that makes running local AI feel like using an App Store.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Browse and install AI tools with one click (like apps)&lt;/li&gt; &lt;li&gt;No terminal, no Python setup, no configs&lt;/li&gt; &lt;li&gt;Open-source, designed with UX in mind&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;You can try it&lt;/strong&gt; &lt;a href="https://getdione.app"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I built it?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Tools like Pinokio or open-source repos are powerful, but honestly‚Ä¶ &lt;strong&gt;most look like they were made by devs, for devs&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I wanted something simple&lt;/strong&gt;. Something visual. Something you can give to your non-tech friend and it still works.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dione is my attempt to make local AI accessible without losing control or power.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Would you use something like this? Anything confusing / missing?&lt;/p&gt; &lt;p&gt;The project is still evolving, and I‚Äôm fully open to ideas and contributions. Also, if you‚Äôre into self-hosted AI or building tools around it ‚Äî let‚Äôs talk!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://getdione.app/github"&gt;https://getdione.app/github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for reading &amp;lt;3!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deivih-4774"&gt; /u/Deivih-4774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mgc0v0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgc0v0/i_created_an_app_to_run_local_ai_as_if_it_were/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgc0v0/i_created_an_app_to_run_local_ai_as_if_it_were/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T06:12:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg3i48</id>
    <title>HRM solved thinking more than current "thinking" models (this needs more hype)</title>
    <updated>2025-08-02T22:44:39+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Article: &lt;a href="https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e"&gt;https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Context:&lt;/p&gt; &lt;p&gt;This insane new paper got 40% on ARC-AGI with an absolutely tiny model (27M params). It's seriously a revolutionary new paper that got way less attention than it deserved.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2506.21734"&gt;https://arxiv.org/abs/2506.21734&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A number of people have reproduced it if anyone is worried about that: &lt;a href="https://x.com/VictorTaelin/status/1950512015899840768"&gt;https://x.com/VictorTaelin/status/1950512015899840768&lt;/a&gt; &lt;a href="https://github.com/sapientinc/HRM/issues/12"&gt;https://github.com/sapientinc/HRM/issues/12&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T22:44:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgky8g</id>
    <title>This might be the largest un-aligned open-source model</title>
    <updated>2025-08-03T14:41:20+00:00</updated>
    <author>
      <name>/u/jshin49</name>
      <uri>https://old.reddit.com/user/jshin49</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's a completely new 70B dense model trained from scratch on 1.5T high quality tokens - only SFT with basic chat and instructions, no RLHF alignment. Plus, it speaks Korean and Japanese.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/trillionlabs/Tri-70B-preview-SFT"&gt;https://huggingface.co/trillionlabs/Tri-70B-preview-SFT&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jshin49"&gt; /u/jshin49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgky8g/this_might_be_the_largest_unaligned_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgky8g/this_might_be_the_largest_unaligned_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgky8g/this_might_be_the_largest_unaligned_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T14:41:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg5xlb</id>
    <title>I created a persistent memory for an AI assistant I'm developing, and am releasing the memory system</title>
    <updated>2025-08-03T00:41:41+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ I just open-sourced a fully working persistent memory system for AI assistants!&lt;/p&gt; &lt;p&gt;üß† Features:&lt;/p&gt; &lt;p&gt;- Real-time memory capture across apps (LM Studio, VS Code, etc.)&lt;/p&gt; &lt;p&gt;- Semantic search via vector embeddings&lt;/p&gt; &lt;p&gt;- Tool call logging for AI self-reflection&lt;/p&gt; &lt;p&gt;- Cross-platform and fully tested&lt;/p&gt; &lt;p&gt;- Open source and modular&lt;/p&gt; &lt;p&gt;Built with: Python, SQLite, watchdog, and AI copilots like ChatGPT and GitHub Copilot ü§ù&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/savantskie/persistent-ai-memory"&gt;https://github.com/savantskie/persistent-ai-memory&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T00:41:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgjlek</id>
    <title>Are Chinese LLM companies effectively price dumping?</title>
    <updated>2025-08-03T13:43:23+00:00</updated>
    <author>
      <name>/u/uutnt</name>
      <uri>https://old.reddit.com/user/uutnt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People here seem to assume that Chinese AI companies are developing and releasing these models, which cost tens of millions of dollars to develop, for free out of the goodness of their heart.&lt;/p&gt; &lt;p&gt;I think this is absurd, considering these are for-profit companies, with shareholders who expect an ROI. In the case of Meta (and perhaps AliBaba), the explanation was it's about &lt;a href="https://gwern.net/complement"&gt;commoditizing your complement&lt;/a&gt;. But for many of these companies, which are pure play AI Labs, this simply does not hold.&lt;/p&gt; &lt;p&gt;So the question remains, why are they doing this?&lt;/p&gt; &lt;p&gt;One theory I would put forward is, they are playing the long game, and attempting to disincentivize investment in US AI labs, with the premise that investors will never recoup their investment, since similar capabilities will be offered for free. There is &lt;a href="https://chatgpt.com/s/dr_688f65761670819181f5f8f5e52f9838"&gt;a precedent&lt;/a&gt; of Chinese companies doing similarly, in the context of mineral production, which has resulted in most production moving to China.&lt;/p&gt; &lt;p&gt;If this is the case, it will be good for consumers in the short-term, but less so in the long-term, at least for non-Chinese entities. If you don't find this theory convincing, I would be interested in hearing other alternative explanations for the rise in Chinese open-source models.&lt;/p&gt; &lt;p&gt;What prompted this question, was the &lt;a href="https://youtu.be/mYDSSRS-B5U?t=2203"&gt;recent interview&lt;/a&gt; with Dario from Anthropic, where he was asked about the threat to the business model posed by open-source models. (I don't find his response very compelling).&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;One aside, its known that Twitter is banned in China. Yet, we see many Chinese-based AI researchers communicating there, on a daily basis. Sure it can be accessed via VPN, but these are publicly known figures, so there is no anonymity. What explains this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uutnt"&gt; /u/uutnt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgjlek/are_chinese_llm_companies_effectively_price/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgjlek/are_chinese_llm_companies_effectively_price/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgjlek/are_chinese_llm_companies_effectively_price/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T13:43:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgis6h</id>
    <title>NVIDIA's "Highly Optimistic" DGX Spark Mini-Supercomputer Still Hasn't Hit Retail Despite a Planned July Launch, Suggesting Possible Production Issues</title>
    <updated>2025-08-03T13:06:01+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgis6h/nvidias_highly_optimistic_dgx_spark/"&gt; &lt;img alt="NVIDIA's &amp;quot;Highly Optimistic&amp;quot; DGX Spark Mini-Supercomputer Still Hasn't Hit Retail Despite a Planned July Launch, Suggesting Possible Production Issues" src="https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09c3744f2431f90355d37d937f1352192cc87780" title="NVIDIA's &amp;quot;Highly Optimistic&amp;quot; DGX Spark Mini-Supercomputer Still Hasn't Hit Retail Despite a Planned July Launch, Suggesting Possible Production Issues" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-highly-optimistic-dgx-spark-mini-supercomputer-still-hasnt-hit-retail/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgis6h/nvidias_highly_optimistic_dgx_spark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgis6h/nvidias_highly_optimistic_dgx_spark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T13:06:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgbs6r</id>
    <title>We enabled Multi-GPU training in Unsloth AI ‚Äî a feature that‚Äôs usually paid ‚Äî using just 2 Copilot prompts!</title>
    <updated>2025-08-03T05:58:01+00:00</updated>
    <author>
      <name>/u/Quiet-Moment-338</name>
      <uri>https://old.reddit.com/user/Quiet-Moment-338</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/oevortex/unsloth"&gt;https://github.com/oevortex/unsloth&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet-Moment-338"&gt; /u/Quiet-Moment-338 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgbs6r/we_enabled_multigpu_training_in_unsloth_ai_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgbs6r/we_enabled_multigpu_training_in_unsloth_ai_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgbs6r/we_enabled_multigpu_training_in_unsloth_ai_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T05:58:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mghy1u</id>
    <title>qihoo360/Light-IF-32B</title>
    <updated>2025-08-03T12:24:28+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mghy1u/qihoo360lightif32b/"&gt; &lt;img alt="qihoo360/Light-IF-32B" src="https://preview.redd.it/6vaf0crhrsgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=771c3421ec90e5339a70e9664ac80ef7b729ca73" title="qihoo360/Light-IF-32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yet another new model claiming to outperform larger ones:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Instruction following&lt;/strong&gt; is a core ability of large language models (LLMs), but performance remains inconsistent, especially on complex tasks.&lt;/p&gt; &lt;p&gt;We identify &lt;strong&gt;lazy reasoning&lt;/strong&gt; during the thinking stage as a key cause of poor instruction adherence.&lt;/p&gt; &lt;p&gt;To address this, we propose a framework that promotes rigorous reasoning through &lt;strong&gt;previewing and self-checking&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Our method begins by generating instruction data with &lt;strong&gt;complex constraints&lt;/strong&gt;, filtering out samples that are too easy or too difficult. We then use rejection sampling to build a small but high-quality dataset for model adaptation.&lt;/p&gt; &lt;p&gt;Training involves entropy-preserving supervised fine-tuning (&lt;strong&gt;Entropy-SFT&lt;/strong&gt;) and token-wise entropy-adaptive reinforcement learning (&lt;strong&gt;TEA-RL&lt;/strong&gt;), guided by rule-based multidimensional rewards.&lt;/p&gt; &lt;p&gt;This approach encourages models to plan ahead and verify their outputs, fostering more generalizable reasoning abilities.&lt;/p&gt; &lt;p&gt;Experiments show consistent improvements across model sizes. Notably, our 32B model outperforms both larger open-source models like &lt;strong&gt;DeepSeek-R1&lt;/strong&gt; and closed-source models like &lt;strong&gt;ChatGPT-4o&lt;/strong&gt; on challenging instruction-following benchmarks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/qihoo360/Light-IF-32B"&gt;https://huggingface.co/qihoo360/Light-IF-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;technical report &lt;a href="https://huggingface.co/papers/2503.10460"&gt;https://huggingface.co/papers/2503.10460&lt;/a&gt;&lt;/p&gt; &lt;p&gt;previous popular models by this company:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/qihoo360/TinyR1-32B-Preview"&gt;https://huggingface.co/qihoo360/TinyR1-32B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/qihoo360/Light-R1-32B"&gt;https://huggingface.co/qihoo360/Light-R1-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6vaf0crhrsgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mghy1u/qihoo360lightif32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mghy1u/qihoo360lightif32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T12:24:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgccyc</id>
    <title>ByteDance drops Seed-Prover</title>
    <updated>2025-08-03T06:34:03+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;ByteDance Seed-Prover proves math the way mathematicians do, not just explanations, but full formal proofs that a computer can verify using Lean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It writes Lean 4 code (a formal proof language), solves problems from competitions like IMO and Putnam, and gets the proof &lt;em&gt;checked&lt;/em&gt; by a compiler. &lt;/p&gt; &lt;p&gt;The key innovations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Lemma-first reasoning&lt;/strong&gt;: breaks problems into small reusable steps.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Iterative refinement&lt;/strong&gt;: re-tries and improves failed proofs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Formal geometry engine&lt;/strong&gt;: solves insane geometry problems using a custom language and a C++ backend.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Performance? It formally solved &lt;strong&gt;5/6 IMO 2025 problems&lt;/strong&gt;, something no model has done before.&lt;/p&gt; &lt;p&gt;Check simple explanantion here : &lt;a href="https://www.youtube.com/watch?v=os1QcHEpgZQ"&gt;https://www.youtube.com/watch?v=os1QcHEpgZQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper : &lt;a href="https://arxiv.org/abs/2507.23726"&gt;https://arxiv.org/abs/2507.23726&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgccyc/bytedance_drops_seedprover/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgccyc/bytedance_drops_seedprover/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgccyc/bytedance_drops_seedprover/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T06:34:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mggku0</id>
    <title>XBai-04 Is It Real?</title>
    <updated>2025-08-03T11:07:25+00:00</updated>
    <author>
      <name>/u/Ordinary_Mud7430</name>
      <uri>https://old.reddit.com/user/Ordinary_Mud7430</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mggku0/xbai04_is_it_real/"&gt; &lt;img alt="XBai-04 Is It Real?" src="https://b.thumbs.redditmedia.com/ybTHvU3e25DWWBXsMwOWfbuGc2dzpO5QuzWGZyUC65s.jpg" title="XBai-04 Is It Real?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;WHAT THE DEVIL?&lt;/p&gt; &lt;p&gt;Another open model outperforms closed ones!&lt;br /&gt; XBai o4 beats OpenAI o3-mini and &lt;em&gt;confidently&lt;/em&gt; beats Anthropic's Claude Opus.&lt;/p&gt; &lt;p&gt;‚Ä¢Parameters: 32.8 B ‚Ä¢Training: Long-CoT RL + Process Reward Learning (SPRM) ‚Ä¢Benchmarks (High-Modus): ‚Ä¢AIME24: 86.5 ‚Ä¢AIME25: 77.9 ‚Ä¢LiveCodeBench v5: 67.2 ‚Ä¢C-EVAL: 89.7&lt;/p&gt; &lt;p&gt;üîóOpen source weights: &lt;a href="https://huggingface.co/MetaStoneTec/XBai-o4"&gt;https://huggingface.co/MetaStoneTec/XBai-o4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ordinary_Mud7430"&gt; /u/Ordinary_Mud7430 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mggku0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mggku0/xbai04_is_it_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mggku0/xbai04_is_it_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T11:07:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgkiti</id>
    <title>Use local LLM to neutralise the headers on the web</title>
    <updated>2025-08-03T14:23:05+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgkiti/use_local_llm_to_neutralise_the_headers_on_the_web/"&gt; &lt;img alt="Use local LLM to neutralise the headers on the web" src="https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a68cd5e26b5f21da9a193d716904dfd8485c857" title="Use local LLM to neutralise the headers on the web" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally got to finish a weekend project from a couple of months ago. &lt;/p&gt; &lt;p&gt;This is a small extension that can use a local LLM (any OpenAI-compatible endpoint is supported) to neutralise the clickbaits on the webpages you visit. It works reasonably well with models of Llama 3.2 3B class and above. Works in Chrome and Firefox (you can also install to Edge manually).&lt;/p&gt; &lt;p&gt;Full source and configuration guide is on GitHub: &lt;a href="https://github.com/av/unhype"&gt;https://github.com/av/unhype&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/niaha18uctgf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgkiti/use_local_llm_to_neutralise_the_headers_on_the_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgkiti/use_local_llm_to_neutralise_the_headers_on_the_web/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T14:23:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgiyg4</id>
    <title>Why doesn't "OpenAI" just release one of the models they already have? Like 3.5</title>
    <updated>2025-08-03T13:14:11+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are they really gonna train a model that's absolutely useless to give to us?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgiyg4/why_doesnt_openai_just_release_one_of_the_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgiyg4/why_doesnt_openai_just_release_one_of_the_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgiyg4/why_doesnt_openai_just_release_one_of_the_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T13:14:11+00:00</published>
  </entry>
</feed>
