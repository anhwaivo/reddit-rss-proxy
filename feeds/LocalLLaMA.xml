<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-05T18:07:15+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1keo3te</id>
    <title>UI-Tars-1.5 reasoning never fails to entertain me.</title>
    <updated>2025-05-04T16:37:31+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keo3te/uitars15_reasoning_never_fails_to_entertain_me/"&gt; &lt;img alt="UI-Tars-1.5 reasoning never fails to entertain me." src="https://preview.redd.it/627wnr5emsye1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b896f5165e878160c1e104137518ab1d80b3addc" title="UI-Tars-1.5 reasoning never fails to entertain me." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;7B parameter computer use agent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/627wnr5emsye1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keo3te/uitars15_reasoning_never_fails_to_entertain_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1keo3te/uitars15_reasoning_never_fails_to_entertain_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T16:37:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kewkno</id>
    <title>Qwen 30B A3B performance degradation with KV quantization</title>
    <updated>2025-05-04T22:43:11+00:00</updated>
    <author>
      <name>/u/fakezeta</name>
      <uri>https://old.reddit.com/user/fakezeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I came across this gist &lt;a href="https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4"&gt;https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4&lt;/a&gt; that shows how Qwen 30B can solve the OpenAI cypher test with Q4_K_M quantization.&lt;/p&gt; &lt;p&gt;I tried to replicate locally but could I was not able, model sometimes entered in a repetition loop even with dry sampling or came to wrong conclusion after generating lots of thinking tokens.&lt;/p&gt; &lt;p&gt;I was using Unsloth Q4_K_XL quantization, so I tought it could be the Dynamic quantization. I tested Bartowski Q5_K_S but it had no improvement. The model didn't entered in any repetition loop but generated lots of thinking tokens without finding any solution.&lt;/p&gt; &lt;p&gt;Then I saw that sunpazed didn't used KV quantization and tried the same: boom! First time right.&lt;/p&gt; &lt;p&gt;It worked with Q5_K_S and also with Q4_K_XL&lt;/p&gt; &lt;p&gt;For who wants more details I leave here a gist &lt;a href="https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef"&gt;https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Do you have any report of performance degradation with long generations on Qwen3 30B A3B and KV quantization?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fakezeta"&gt; /u/fakezeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T22:43:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf76z8</id>
    <title>Fine tuning Qwen3</title>
    <updated>2025-05-05T09:16:01+00:00</updated>
    <author>
      <name>/u/Basic-Pay-9535</name>
      <uri>https://old.reddit.com/user/Basic-Pay-9535</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to finetune Qwen 3 reasoning. But I need to generate think tags for my dataset . Which model / method would u recommend best in order to create these think tags ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Basic-Pay-9535"&gt; /u/Basic-Pay-9535 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf76z8/fine_tuning_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf76z8/fine_tuning_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf76z8/fine_tuning_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T09:16:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1keyy4k</id>
    <title>Well, that's just, like… your benchmark, man.</title>
    <updated>2025-05-05T00:39:27+00:00</updated>
    <author>
      <name>/u/remyxai</name>
      <uri>https://old.reddit.com/user/remyxai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keyy4k/well_thats_just_like_your_benchmark_man/"&gt; &lt;img alt="Well, that's just, like… your benchmark, man." src="https://preview.redd.it/mdy01ntgwuye1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d618c2e8de6cb32e8937776f2dfc048d10d61fc" title="Well, that's just, like… your benchmark, man." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Especially as teams put AI into production, we need to start treating evaluation like a first-class discipline: versioned, interpretable, reproducible, and aligned to outcomes and improved UX.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Without some kind of ExperimentOps, you’re one false positive away from months of shipping the wrong thing.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remyxai"&gt; /u/remyxai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mdy01ntgwuye1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keyy4k/well_thats_just_like_your_benchmark_man/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1keyy4k/well_thats_just_like_your_benchmark_man/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T00:39:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfh01g</id>
    <title>Local llms vs sonnet 3.7</title>
    <updated>2025-05-05T17:09:53+00:00</updated>
    <author>
      <name>/u/KillasSon</name>
      <uri>https://old.reddit.com/user/KillasSon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any model I can run locally (self host, pay for host etc) that would outperform sonnet 3.7? I get the feeling that I should just stick to Claude and not bother buying the hardware etc for hosting my own models. I’m strictly using them for coding. I use Claude sometimes to help me research but that’s not crucial and I get that for free&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KillasSon"&gt; /u/KillasSon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfh01g/local_llms_vs_sonnet_37/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfh01g/local_llms_vs_sonnet_37/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfh01g/local_llms_vs_sonnet_37/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T17:09:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1keolh9</id>
    <title>Visa is looking for vibe coders - thoughts?</title>
    <updated>2025-05-04T16:58:36+00:00</updated>
    <author>
      <name>/u/eastwindtoday</name>
      <uri>https://old.reddit.com/user/eastwindtoday</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keolh9/visa_is_looking_for_vibe_coders_thoughts/"&gt; &lt;img alt="Visa is looking for vibe coders - thoughts?" src="https://preview.redd.it/gefvhv84qsye1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=235b3e1de1b7df4bd1bc1f7519f84b5259303d05" title="Visa is looking for vibe coders - thoughts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eastwindtoday"&gt; /u/eastwindtoday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gefvhv84qsye1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1keolh9/visa_is_looking_for_vibe_coders_thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1keolh9/visa_is_looking_for_vibe_coders_thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T16:58:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfdkkz</id>
    <title>What quants and runtime configurations do Meta and Bing really run in public prod?</title>
    <updated>2025-05-05T14:53:08+00:00</updated>
    <author>
      <name>/u/scott-stirling</name>
      <uri>https://old.reddit.com/user/scott-stirling</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When comparing results of prompts between Bing, Meta, Deepseek and local LLMs such as quantized llama, qwen, mistral, Phi, etc. I find the results pretty comparable from the big guys to my local LLMs. Either they’re running quantized models for public use or the constraints and configuration dumb down the public LLMs somehow.&lt;/p&gt; &lt;p&gt;I am asking how LLMs are configured for scale and whether the average public user is actually getting the best LLM quality or some dumbed down restricted versions all the time. Ultimately pursuant to configuring local LLM runtimes for optimal performance. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/scott-stirling"&gt; /u/scott-stirling &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfdkkz/what_quants_and_runtime_configurations_do_meta/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfdkkz/what_quants_and_runtime_configurations_do_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfdkkz/what_quants_and_runtime_configurations_do_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T14:53:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfi8xh</id>
    <title>[Benchmark] Quick‑and‑dirty test of 5 models on a Mac Studio M3 Ultra 512 GB (LM Studio) – Qwen3 runs away with it</title>
    <updated>2025-05-05T17:58:51+00:00</updated>
    <author>
      <name>/u/Turbulent_Pin7635</name>
      <uri>https://old.reddit.com/user/Turbulent_Pin7635</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey r/LocalLLaMA!&lt;/p&gt; &lt;p&gt;I’m a former university physics lecturer (taught for five years) and—one month after buying a Mac Studio (M3 Ultra, 128 CPU / 80 GPU cores, 512 GB unified RAM)—I threw a very simple benchmark at a few LLMs inside &lt;strong&gt;LM Studio&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt (intentional typo):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Explain to me why sky is blue at an physiscist Level PhD. &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Raw numbers&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Quant. / RAM footprint&lt;/th&gt; &lt;th align="left"&gt;Speed (tok/s)&lt;/th&gt; &lt;th align="left"&gt;Tokens out&lt;/th&gt; &lt;th align="left"&gt;1st‑token latency&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;MLX deepseek‑V3‑0324‑4bit&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;355.95 GB&lt;/td&gt; &lt;td align="left"&gt;19.34&lt;/td&gt; &lt;td align="left"&gt; 755&lt;/td&gt; &lt;td align="left"&gt;17.29 s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;MLX Gemma‑3‑27b‑it‑bf16&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt; 52.57 GB&lt;/td&gt; &lt;td align="left"&gt;11.19&lt;/td&gt; &lt;td align="left"&gt; 1 317&lt;/td&gt; &lt;td align="left"&gt; 1.72 s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;MLX Deepseek‑R1‑4bit&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;402.17 GB&lt;/td&gt; &lt;td align="left"&gt;16.55&lt;/td&gt; &lt;td align="left"&gt; 2 062&lt;/td&gt; &lt;td align="left"&gt; 15.01 s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;MLX Qwen3‑235‑A22B‑8bit&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;233.79 GB&lt;/td&gt; &lt;td align="left"&gt;18.86&lt;/td&gt; &lt;td align="left"&gt; 3 096&lt;/td&gt; &lt;td align="left"&gt; 9.02 s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;GGFU Qwen3‑235‑A22B‑8bit&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt; 233.72 GB&lt;/td&gt; &lt;td align="left"&gt;14.35&lt;/td&gt; &lt;td align="left"&gt; 2 883&lt;/td&gt; &lt;td align="left"&gt; 4.47 s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;&lt;strong&gt;Teacher’s impressions&lt;/strong&gt;&lt;/h1&gt; &lt;h1&gt;1. Reasoning speed&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;R1 &amp;gt; Qwen3 &amp;gt; Gemma3&lt;/strong&gt;.&lt;br /&gt; The “thinking time” (pre‑generation) is roughly half of total generation time. If I had to re‑prompt twice to get a good answer, I’d simply pick a model with better reasoning instead of chasing seconds.&lt;/p&gt; &lt;h1&gt;2. Generation speed&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;V3 ≈ MLX‑Qwen3 &amp;gt; R1 &amp;gt; GGFU‑Qwen3 &amp;gt; Gemma3&lt;/strong&gt;.&lt;br /&gt; No surprise: token‑width + unified‑memory bandwidth rule here. The Mac’s 890 GB/s is great for a compact workstation, but it’s nowhere near the monster discrete GPUs you guys already know—so throughput drops once the model starts chugging serious tokens.&lt;/p&gt; &lt;h1&gt;3. Output quality (grading as if these were my students)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Qwen3 &amp;gt;&amp;gt;&amp;gt; R1 &amp;gt; Gemma3 &amp;gt; V3&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;deepseek‑V3&lt;/strong&gt; – trivial answer, would fail the course.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deepseek‑R1&lt;/strong&gt; – solid undergrad level.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemma‑3&lt;/strong&gt; – punchy for its size, respectable.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3&lt;/strong&gt; – in a league of its own: clear, creative, concise, high‑depth. If the others were bachelor’s level, Qwen3 was PhD defending a job talk.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Bottom line: for text‑to‑text tasks balancing quality and speed, &lt;strong&gt;Qwen3‑8bit (MLX)&lt;/strong&gt; is my daily driver.&lt;/p&gt; &lt;h1&gt;One month with the Mac Studio – worth it?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Why I don’t regret it&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Stellar build &amp;amp; design.&lt;/li&gt; &lt;li&gt;Makes sense if a computer &amp;gt; a car for you (I do bio‑informatics), you live in an apartment (space is luxury, no room for a noisy server), and noise destroys you (I’m neurodivergent; the Mac is silent even at 100 %).&lt;/li&gt; &lt;li&gt;Power draw peaks &amp;lt; 250 W.&lt;/li&gt; &lt;li&gt;Ridiculously small footprint, light enough to slip in a backpack.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why you might pass&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You game heavily on PC.&lt;/li&gt; &lt;li&gt;You hate macOS learning curves.&lt;/li&gt; &lt;li&gt;You want constant hardware upgrades.&lt;/li&gt; &lt;li&gt;You can wait 2–3 years for LLM‑focused hardware to get cheap.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Money‑saving tips&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Stick with the 1 TB SSD—Thunderbolt + a fast NVMe enclosure covers the rest.&lt;/li&gt; &lt;li&gt;Skip Apple’s monitor &amp;amp; peripherals; third‑party is way cheaper.&lt;/li&gt; &lt;li&gt;Grab one before any Trump‑era import tariffs jack up Apple prices again.&lt;/li&gt; &lt;li&gt;I would not buy the 256 Gb over the 512 Gb, of course is double the price, but it opens more opportunities at least for me. With it I can run an bioinformatics analysis while using Qwen3, and even if Qwen3 fits (tightly) in the 256 Gb, this won't let you with a large margin of maneuver for other tasks. Finally, who knows what would be the next generation of models and how much memory it will get.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Qwen3‑8bit&lt;/strong&gt; dominates – PhD‑level answers, fast enough, reasoning quick.&lt;/li&gt; &lt;li&gt;Thinking time isn’t the bottleneck; quantization + memory bandwidth are (if any expert wants to correct or improve this please do so).&lt;/li&gt; &lt;li&gt;Mac Studio M3 Ultra is a silence‑loving, power‑sipping, tiny beast—just not the rig for GPU fiends or upgrade addicts.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ask away if you want more details!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent_Pin7635"&gt; /u/Turbulent_Pin7635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfi8xh/benchmark_quickanddirty_test_of_5_models_on_a_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfi8xh/benchmark_quickanddirty_test_of_5_models_on_a_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfi8xh/benchmark_quickanddirty_test_of_5_models_on_a_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T17:58:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kezq68</id>
    <title>Speed metrics running DeepSeekV3 0324/Qwen3 235B and other models, on 128GB VRAM (5090+4090x2+A6000) + 192GB RAM on Consumer motherboard/CPU (llamacpp/ikllamacpp)</title>
    <updated>2025-05-05T01:19:58+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there guys, hope is all going good.&lt;/p&gt; &lt;p&gt;I have been testing some bigger models on this setup and wanted to share some metrics if it helps someone!&lt;/p&gt; &lt;p&gt;Setup is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen 7 7800X3D&lt;/li&gt; &lt;li&gt;192GB DDR5 6000Mhz at CL30 (overclocked and adjusted resistances to make it stable)&lt;/li&gt; &lt;li&gt;RTX 5090 MSI Vanguard LE SOC, flashed to Gigabyte Aorus Master VBIOS.&lt;/li&gt; &lt;li&gt;RTX 4090 ASUS TUF, flashed to Galax HoF VBIOS.&lt;/li&gt; &lt;li&gt;RTX 4090 Gigabyte Gaming OC, flashed to Galax HoF VBIOS.&lt;/li&gt; &lt;li&gt;RTX A6000 (Ampere)&lt;/li&gt; &lt;li&gt;AM5 MSI Carbon X670E&lt;/li&gt; &lt;li&gt;Running at X8 5.0 (5090) / X8 4.0 (4090) / X4 4.0 (4090) / X4 4.0 (A6000), all from CPU lanes (using M2 to PCI-E adapters)&lt;/li&gt; &lt;li&gt;Fedora 41-42 (believe me, I tried these on Windows and multiGPU is just borked there)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The models I have tested are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DeepSeek V3 0324 at Q2_K_XL (233GB), from &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD"&gt;https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qwen3 235B at Q3_K_XL, Q4_K_L, Q6_K from &lt;a href="https://huggingface.co/unsloth/Qwen3-235B-A22B-128K-GGUF"&gt;https://huggingface.co/unsloth/Qwen3-235B-A22B-128K-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Llama-3.1-Nemotron-Ultra-253B at Q3_K_XL from &lt;a href="https://huggingface.co/unsloth/Llama-3_1-Nemotron-Ultra-253B-v1-GGUF"&gt;https://huggingface.co/unsloth/Llama-3_1-Nemotron-Ultra-253B-v1-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;c4ai-command-a-03-2025 111B at Q6_K_XL from &lt;a href="https://huggingface.co/bartowski/CohereForAI_c4ai-command-a-03-2025-GGUF"&gt;https://huggingface.co/bartowski/CohereForAI_c4ai-command-a-03-2025-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mistral-Large-Instruct-2411 123B at Q4_K_M from &lt;a href="https://huggingface.co/bartowski/Mistral-Large-Instruct-2411-GGUF"&gt;https://huggingface.co/bartowski/Mistral-Large-Instruct-2411-GGUF&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All on llamacpp, for offloading mostly on the case of bigger models. command a and Mistral Large run faster on EXL2.&lt;/p&gt; &lt;p&gt;I have also used llamacpp (&lt;a href="https://github.com/ggml-org/llama.cpp"&gt;https://github.com/ggml-org/llama.cpp&lt;/a&gt;) and ikllamacpp (&lt;a href="https://github.com/ikawrakow/ik%5C_llama.cpp"&gt;https://github.com/ikawrakow/ik\_llama.cpp&lt;/a&gt;), so I will note where I use which.&lt;/p&gt; &lt;p&gt;All of these models were loaded with 32K, without flash attention or cache quantization, except in the case of Nemotron, mostly to give some VRAM usages. FA when avaialble reduces VRAM usage with cache/buffer size heavily.&lt;/p&gt; &lt;p&gt;Also, when running -ot, I did use each layer instead of regex. This is because when using the regex I got issues with VRAM usage.&lt;/p&gt; &lt;p&gt;They were compiled from source with:&lt;/p&gt; &lt;p&gt;&lt;code&gt;CC=gcc-14 CXX=g++-14 CUDAHOSTCXX=g++-14 cmake -B build_linux \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-DGGML_CUDA=ON \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-DGGML_CUDA_FA_ALL_QUANTS=ON \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-DGGML_BLAS=OFF \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-DCMAKE_CUDA_ARCHITECTURES=&amp;quot;86;89;120&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-DCMAKE_CUDA_FLAGS=&amp;quot;-allow-unsupported-compiler -ccbin=g++-14&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;(Had to force CC and CXX 14, as CUDA doesn't support GCC15 yet, which is what Fedora ships)&lt;/p&gt; &lt;h1&gt;DeepSeek V3 0324 (Q2_K_XL, llamacpp)&lt;/h1&gt; &lt;p&gt;For this model, MLA was added recently, which let me to use more tensors on GPU.&lt;/p&gt; &lt;p&gt;Command to run it was&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m '/GGUFs/DeepSeek-V3-0324-UD-Q2_K_XL-merged.gguf' -c 32768 --no-mmap --no-warmup -ngl 999 -ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; -ot &amp;quot;blk.(7|8|9|10).ffn.=CUDA1&amp;quot; -ot &amp;quot;blk.(11|12|13|14|15).ffn.=CUDA2&amp;quot; -ot &amp;quot;blk.(16|17|18|19|20|21|22|23|24|25).ffn.=CUDA3&amp;quot; -ot &amp;quot;ffn.*=CPU&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 38919.92 ms / 1528 tokens ( 25.47 ms per token, 39.26 tokens per second)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;eval time = 57175.47 ms / 471 tokens ( 121.39 ms per token, 8.24 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This makes it pretty usable. The important part is setting the experts to be only on CPU, and active params + other experts on GPU. With MLA, it uses ~4GB for 32K and ~8GB for 64K. Without MLA, 16K uses 80GB of VRAM.&lt;/p&gt; &lt;h1&gt;Qwen3 235B (Q3_K_XL, llamacpp)&lt;/h1&gt; &lt;p&gt;For this model and size, we're able to load the model entirely on VRAM. Note: When using only GPU, on my case, llamacpp is faster than ik llamacpp.&lt;/p&gt; &lt;p&gt;Command to run it was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m '/GGUFs/Qwen3-235B-A22B-128K-UD-Q3_K_XL-00001-of-00003.gguf' -c 32768 --no-mmap --no-warmup -ngl 999 -ts 0.8,0.8,1.2,2&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 6532.37 ms / 3358 tokens ( 1.95 ms per token, 514.06 tokens per second)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;eval time = 53259.78 ms / 1359 tokens ( 39.19 ms per token, 25.52 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Pretty good model but I would try to use at least Q4_K_S/M. Cache size at 32K is 6GB, and 12GB at 64K. This cache size is the same for all Qwen3 235B quants&lt;/p&gt; &lt;h1&gt;Qwen3 235B (Q4_K_XL, llamacpp)&lt;/h1&gt; &lt;p&gt;For this model, we're using ~20GB of RAM and the rest on GPU.&lt;/p&gt; &lt;p&gt;Command to run it was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m '/GGUFs/Qwen3-235B-A22B-128K-UD-Q4_K_XL-00001-of-00003.gguf' -c 32768 --no-mmap --no-warmup -ngl 999 -ot &amp;quot;blk\.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|13)\.ffn.*=CUDA0&amp;quot; -ot &amp;quot;blk\.(14|15|16|17|18|19|20|21|22|23|24|25|26|27)\.ffn.*=CUDA1&amp;quot; -ot &amp;quot;blk\.(28|29|30|31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|)\.ffn.*=CUDA2&amp;quot; -ot &amp;quot;blk\.(47|48|49|50|51|52|53|54|55|56|57|58|59|60|61|62|63|64|65|66|67|68|69|70|71|72|73|74|75|76|77|78)\.ffn.*=CUDA3&amp;quot; -ot &amp;quot;ffn.*=CPU&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 17405.76 ms / 3358 tokens ( 5.18 ms per token, 192.92 tokens per second)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;eval time = 92420.55 ms / 1549 tokens ( 59.66 ms per token, 16.76 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Model is pretty good at this point, and speeds are still acceptable. But on this case is where ik llamacpp shines.&lt;/p&gt; &lt;h1&gt;Qwen3 235B (Q4_K_XL, ik llamacpp)&lt;/h1&gt; &lt;p&gt;ik llamacpp with some extra parameters makes the models run faster when offloading. If you're wondering why this isn't the case or I didn't post with DeepSeek V3 0324, it is because quants of main llamacpp have MLA which are incompatible with MLA from ikllamacpp, which was implemented before via another method.&lt;/p&gt; &lt;p&gt;Command to run it was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m '/GGUFs/Qwen3-235B-A22B-128K-UD-Q4_K_XL-00001-of-00003.gguf' -c 32768 --no-mmap --no-warmup -ngl 999 -ot &amp;quot;blk\.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|13)\.ffn.*=CUDA0&amp;quot; -ot &amp;quot;blk\.(14|15|16|17|18|19|20|21|22|23|24|25|26|27)\.ffn.*=CUDA1&amp;quot; -ot &amp;quot;blk\.(28|29|30|31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|)\.ffn.*=CUDA2&amp;quot; -ot &amp;quot;blk\.(47|48|49|50|51|52|53|54|55|56|57|58|59|60|61|62|63|64|65|66|67|68|69|70|71|72|73|74|75|76|77|78)\.ffn.*=CUDA3&amp;quot; -ot &amp;quot;ffn.*=CPU&amp;quot; -fmoe -amb 1024 -rtr&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;INFO [ print_timings] prompt eval time = 15739.89 ms / 3358 tokens ( 4.69 ms per token, 213.34 tokens per second) | tid=&amp;quot;140438394236928&amp;quot; ti&lt;/code&gt;&lt;br /&gt; &lt;code&gt;mestamp=1746406901 id_slot=0 id_task=0 t_prompt_processing=15739.888 n_prompt_tokens_processed=3358 t_token=4.687280524121501 n_tokens_second=213.34332239212884&lt;/code&gt;&lt;br /&gt; &lt;code&gt;INFO [ print_timings] generation eval time = 66275.69 ms / 1067 runs ( 62.11 ms per token, 16.10 tokens per second) | tid=&amp;quot;140438394236928&amp;quot; ti&lt;/code&gt;&lt;br /&gt; &lt;code&gt;mestamp=1746406901 id_slot=0 id_task=0 t_token_generation=66275.693 n_decoded=1067 t_token=62.11405154639175 n_tokens_second=16.099416719791975&lt;/code&gt;&lt;/p&gt; &lt;p&gt;So basically 10% more speed in PP and similar generation t/s.&lt;/p&gt; &lt;h1&gt;Qwen3 235B (Q6_K, llamacpp)&lt;/h1&gt; &lt;p&gt;This is the point where models are really close to Q8 and then to F16. This was more for test porpouses, but still is very usable.&lt;/p&gt; &lt;p&gt;This uses about 70GB RAM and rest on VRAM.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Command to run was:&lt;/code&gt;&lt;br /&gt; &lt;code&gt;./llama-server -m '/models_llm/Qwen3-235B-A22B-128K-Q6_K-00001-of-00004.gguf' -c 32768 --no-mmap --no-warmup -ngl 999 -ot &amp;quot;blk\.(0|1|2|3|4|5|6|7|8)\.ffn.*=CUDA0&amp;quot; -ot &amp;quot;blk\.(9|10|11|12|13|14|15|16|17)\.ffn.*=CUDA1&amp;quot; -ot &amp;quot;blk\.(18|19|20|21|22|23|24|25|26|27|28|29|30)\.ffn.*=CUDA2&amp;quot; -ot &amp;quot;blk\.(31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|47|48|49|50|51|52)\.ffn.*=CUDA3&amp;quot; -ot &amp;quot;ffn.*=CPU&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speed are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 57152.69 ms / 3877 tokens ( 14.74 ms per token, 67.84 tokens per second) eval time = 38705.90 ms / 318 tokens ( 121.72 ms per token, 8.22 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;h1&gt;Qwen3 235B (Q6_K, ik llamacpp)&lt;/h1&gt; &lt;p&gt;ik llamacpp makes a huge increase in PP performance.&lt;/p&gt; &lt;p&gt;Command to run was:&lt;/p&gt; &lt;p&gt;./llama-server -m '/models_llm/Qwen3-235B-A22B-128K-Q6_K-00001-of-00004.gguf' -c 32768 --no-mmap --no-warmup -ngl 999 -ot &amp;quot;blk\.(0|1|2|3|4|5|6|7|8)\.ffn.*=CUDA0&amp;quot; -ot &amp;quot;blk\.(9|10|11|12|13|14|15|16|17)\.ffn.*=CUDA1&amp;quot; -ot &amp;quot;blk\.(18|19|20|21|22|23|24|25|26|27|28|29|30)\.ffn.*=CUDA2&amp;quot; -ot &amp;quot;blk\.(31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|47|48|49|50|51|52)\.ffn.*=CUDA3&amp;quot; -ot &amp;quot;ffn.*=CPU&amp;quot; -fmoe -amb 512 -rtr&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;INFO [ print_timings] prompt eval time = 36897.66 ms / 3877 tokens ( 9.52 ms per token, 105.07 tokens per second) | tid=&amp;quot;140095757803520&amp;quot; timestamp=1746307138 id_slot=0 id_task=0 t_prompt_processing=36897.659 n_prompt_tokens_processed=3877 t_token=9.517064482847562 n_tokens_second=105.07441678075024&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;INFO [ print_timings] generation eval time = 143560.31 ms / 1197 runs ( 119.93 ms per token, 8.34 tokens per second) | tid=&amp;quot;140095757803520&amp;quot; timestamp=1746307138 id_slot=0 id_task=0 t_token_generation=143560.31 n_decoded=1197 t_token=119.93342522974102 n_tokens_second=8.337959147622348&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Basically 40-50% more PP performance and similar generation speed.&lt;/p&gt; &lt;h1&gt;Llama 3.1 Nemotron 253B (Q3_K_XL, llamacpp)&lt;/h1&gt; &lt;p&gt;This model was PAINFUL to make it work fully on GPU, as layers are uneven. Some layers near the end are 8B each.&lt;/p&gt; &lt;p&gt;This is also the only model I had to use CTK8/CTV4, else it doesn't fit. &lt;/p&gt; &lt;p&gt;The commands to run it were:&lt;/p&gt; &lt;p&gt;&lt;code&gt;export CUDA_VISIBLE_DEVICES=0,1,3,2&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m /run/media/pancho/08329F4A329F3B9E/models_llm/Llama-3_1-Nemotron-Ultra-253B-v1-UD-Q3_K_XL-00001-of-00003.gguf -c 32768 -ngl 163 -ts 6.5,6,10,4 --no-warmup -fa -ctk q8_0 -ctv q4_0 -mg 2 --prio 3&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I don't have the specific speeds at the moment (as to run this model I have to close any application of my desktop), but they are, from a picture I got some days ago:&lt;/p&gt; &lt;p&gt;&lt;code&gt;PP: 130 t/s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Generation speed: 7.5 t/s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Cache size is 5GB for 32K and 10GB for 64K.&lt;/p&gt; &lt;h1&gt;c4ai-command-a-03-2025 111B (Q6_K, llamacpp)&lt;/h1&gt; &lt;p&gt;I particullay have liked command a models, and I also feel this model is great. Ran on GPU only.&lt;/p&gt; &lt;p&gt;Command to run it was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m '/GGUFs/CohereForAI_c4ai-command-a-03-2025-Q6_K-merged.gguf' -c 32768 -ngl 99 -ts 10,11,17,20 --no-warmup&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 4101.94 ms / 3403 tokens ( 1.21 ms per token, 829.61 tokens per second)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;eval time = 46452.40 ms / 472 tokens ( 98.42 ms per token, 10.16 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For reference: EXL2 with the same quant size gets ~12 t/s.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Cache size is 8GB for 32K and 16GB for 64K.&lt;/p&gt; &lt;h1&gt;Mistral Large 2411 123B (Q4_K_M, llamacpp)&lt;/h1&gt; &lt;p&gt;Also have been a fan of Mistral Large models, as they work pretty good!&lt;/p&gt; &lt;p&gt;Command to run it was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m '/run/media/pancho/DE1652041651DDD9/HuggingFaceModelDownload&lt;/code&gt;&lt;br /&gt; &lt;code&gt;er/Storage/GGUFs/Mistral-Large-Instruct-2411-Q4_K_M-merged.gguf' -c 32768 -ngl 99 -ts 7,7,10,5 --no-warmup&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 4427.90 ms / 3956 tokens ( 1.12 ms per token, 893.43 tokens per second)&lt;/code&gt;&lt;br /&gt; &lt;code&gt;eval time = 30739.23 ms / 387 tokens ( 79.43 ms per token, 12.59 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Cache size is quite big, 12GB for 32K and 24GB for 64K. In fact it is so big that if I want to load it on 3 GPUs (since size is 68GB) I need to use flash attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For reference: EXL2 with this same size gets 25 t/s with Tensor Parallel enabled. And 16-20 t/s on 6.5bpw EXL2 (EXL2 lets you to use TP with uneven VRAM)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;That's all the tests I have been running lately! I have been testing for both coding (python, C, C++) and RP. Not sure if you guys are interested in which one I prefer for each task or rank them.&lt;/p&gt; &lt;p&gt;Any question is welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T01:19:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf5lq4</id>
    <title>Does the Pareto principle apply to MoE models in practice?</title>
    <updated>2025-05-05T07:18:48+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf5lq4/does_the_pareto_principle_apply_to_moe_models_in/"&gt; &lt;img alt="Does the Pareto principle apply to MoE models in practice?" src="https://preview.redd.it/meqwqkomzwye1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=90b7b7164d1e5b4f5e2f7e1fe2d3294442e22401" title="Does the Pareto principle apply to MoE models in practice?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pareto Effect: In practice, a small number of experts (e.g., 2 or 3) may end up handling a majority of the traffic for many types of inputs. This aligns with the Pareto observation that a small set of experts could be responsible for most of the work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/meqwqkomzwye1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf5lq4/does_the_pareto_principle_apply_to_moe_models_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf5lq4/does_the_pareto_principle_apply_to_moe_models_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T07:18:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kffed0</id>
    <title>Qwen3 include thinking while outputing JSON only?</title>
    <updated>2025-05-05T16:06:05+00:00</updated>
    <author>
      <name>/u/jpcrow</name>
      <uri>https://old.reddit.com/user/jpcrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have QWEN 3 summarizing some forum data that I had downloaded before the site went down in 2010. I want to create training data from this forum data. I want Qwen 3 to use thinking to summarize the forum posts and output JSONL to train with, but I don't want the &amp;quot;thinking&amp;quot; conversation in my output. Is there a way to disable the thinking in the output without disabling thinking altogether? Or do I not understand how /no_thinking works?&lt;/p&gt; &lt;p&gt;Also I'm new to this lol, so I'm probably missing something important or simple; any help would be great.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jpcrow"&gt; /u/jpcrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kffed0/qwen3_include_thinking_while_outputing_json_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kffed0/qwen3_include_thinking_while_outputing_json_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kffed0/qwen3_include_thinking_while_outputing_json_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T16:06:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfeglz</id>
    <title>Why aren't there Any Gemma-3 Reasoning Models?</title>
    <updated>2025-05-05T15:28:49+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google released Gemma-3 models weeks ago and they are excellent for their sizes especially considering that they are non-reasoning ones. I thought that we would see a lot of reasoning fine-tunes especially that Google released the base models too. &lt;/p&gt; &lt;p&gt;I was excited to see what a reasoning Gemma-3-27B would be capable of and was looking forward to it. But, until now, neither Google nor the community bothered with that. I wonder why?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfeglz/why_arent_there_any_gemma3_reasoning_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfeglz/why_arent_there_any_gemma3_reasoning_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfeglz/why_arent_there_any_gemma3_reasoning_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T15:28:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf62ck</id>
    <title>Absolute best performer for 48 Gb vram</title>
    <updated>2025-05-05T07:53:25+00:00</updated>
    <author>
      <name>/u/TacGibs</name>
      <uri>https://old.reddit.com/user/TacGibs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I was wondering if there's a better model than Deepcogito 70B (a fined-tuned thinking version of Llama 3.3 70B for those who don't know) for 48Gb vram today ?&lt;/p&gt; &lt;p&gt;I'm not talking about pure speed, just about a usable model (so no CPU/Ram offloading) with decent speed (more than 10t/s) and great knowledge.&lt;/p&gt; &lt;p&gt;Sadly it seems that the 70B size isn't a thing anymore :(&lt;/p&gt; &lt;p&gt;And yes Qwen3 32B is very nice and a bit faster, but you can feel that it's a smaller model (even if it's incredibly good for it's size).&lt;/p&gt; &lt;p&gt;Thanks !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TacGibs"&gt; /u/TacGibs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf62ck/absolute_best_performer_for_48_gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf62ck/absolute_best_performer_for_48_gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf62ck/absolute_best_performer_for_48_gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T07:53:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfccbv</id>
    <title>Launching an open collaboration on production‑ready AI Agent tooling</title>
    <updated>2025-05-05T14:00:46+00:00</updated>
    <author>
      <name>/u/Nir777</name>
      <uri>https://old.reddit.com/user/Nir777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m kicking off a community‑driven initiative to help developers take AI Agents from proof of concept to reliable production. The focus is on practical, horizontal tooling: creation, monitoring, evaluation, optimization, memory management, deployment, security, human‑in‑the‑loop workflows, and other gaps that Agents face before they reach users.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I’m doing this&lt;/strong&gt;&lt;br /&gt; I maintain several open‑source repositories (35K GitHub stars, ~200K monthly visits) and a technical newsletter with 22K subscribers, and I’ve seen firsthand how many teams stall when it’s time to ship Agents at scale. The goal is to collect and showcase the best solutions - open‑source or commercial - that make that leap easier.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How you can help&lt;/strong&gt;&lt;br /&gt; If your company builds a tool or platform that accelerates any stage of bringing Agents to production - and it’s not just a vertical finished agent - I’d love to hear what you’re working on.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In stealth? Send me a direct message on LinkedIn: &lt;a href="https://www.linkedin.com/in/nir-diamant-ai/"&gt;https://www.linkedin.com/in/nir-diamant-ai/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Otherwise, drop a comment describing the problem you solve and how developers can try it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking forward to seeing what the community is building. I’ll be active in the comments to answer questions.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nir777"&gt; /u/Nir777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfccbv/launching_an_open_collaboration_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfccbv/launching_an_open_collaboration_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfccbv/launching_an_open_collaboration_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T14:00:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf1yg9</id>
    <title>Qwen3-32B-IQ4_XS GGUFs - MMLU-PRO benchmark comparison</title>
    <updated>2025-05-05T03:21:49+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/"&gt; &lt;img alt="Qwen3-32B-IQ4_XS GGUFs - MMLU-PRO benchmark comparison" src="https://external-preview.redd.it/jJ4wm0NIfgUy0MSOkw2YI6r-EjpVW_Y_SPR-xICfNk4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bf4c693cb7ebd3ae7a7b3eb2dc65cfbfc6e1d6d" title="Qwen3-32B-IQ4_XS GGUFs - MMLU-PRO benchmark comparison" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since IQ4_XS is my favorite quant for 32B models, I decided to run some benchmarks to compare IQ4_XS GGUFs from different sources.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MMLU-PRO 0.25 subset(3003 questions), 0 temp, No Think, IQ4_XS, Q8 KV Cache&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The entire benchmark took &lt;strong&gt;&lt;em&gt;11 hours, 37 minutes, and 30 seconds.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9ptc0cl2svye1.png?width=2475&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=06a3b551fba60a33877f8e67af9932e381a15cc6"&gt;https://preview.redd.it/9ptc0cl2svye1.png?width=2475&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=06a3b551fba60a33877f8e67af9932e381a15cc6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The difference is apparently minimum, so just keep using whatever iq4 quant you already downloaded. &lt;/p&gt; &lt;p&gt;&lt;em&gt;The official MMLU-PRO leaderboard is listing the score of Qwen3 base model instead of instruct, that's why these iq4 quants score higher than the one on MMLU-PRO leaderboard.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;gguf source:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-32B-GGUF/blob/main/Qwen3-32B-IQ4_XS.gguf"&gt;https://huggingface.co/unsloth/Qwen3-32B-GGUF/blob/main/Qwen3-32B-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-32B-128K-GGUF/blob/main/Qwen3-32B-128K-IQ4_XS.gguf"&gt;https://huggingface.co/unsloth/Qwen3-32B-128K-GGUF/blob/main/Qwen3-32B-128K-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/Qwen_Qwen3-32B-GGUF/blob/main/Qwen_Qwen3-32B-IQ4_XS.gguf"&gt;https://huggingface.co/bartowski/Qwen_Qwen3-32B-GGUF/blob/main/Qwen_Qwen3-32B-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/Qwen3-32B-i1-GGUF/blob/main/Qwen3-32B.i1-IQ4_XS.gguf"&gt;https://huggingface.co/mradermacher/Qwen3-32B-i1-GGUF/blob/main/Qwen3-32B.i1-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T03:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kff36y</id>
    <title>Experimental Quant (DWQ) of Qwen3-A30B</title>
    <updated>2025-05-05T15:54:01+00:00</updated>
    <author>
      <name>/u/N8Karma</name>
      <uri>https://old.reddit.com/user/N8Karma</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kff36y/experimental_quant_dwq_of_qwen3a30b/"&gt; &lt;img alt="Experimental Quant (DWQ) of Qwen3-A30B" src="https://external-preview.redd.it/LCswmSPGlLeg2uEXPrDVWNpr9PBgA-O2GA2zpqtkfFQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d2a9b92fb8daa878a2d33d37bea5b5e7acbb4797" title="Experimental Quant (DWQ) of Qwen3-A30B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Used a novel technique - details &lt;a href="https://x.com/N8Programs/status/1919283193892540850"&gt;here&lt;/a&gt; - to quantize Qwen3-30B-A3B into 4.5bpw in MLX. As shown in the image, the perplexity is now on par with a 6-bit quant at no storage cost:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/87znt1c8jzye1.png?width=3569&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e238bdbbc9f02e8be52c3a49393e195c77d5ca7"&gt;Graph showing the superiority of the DWQ technique.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The way the technique works is distilling the logits of the 6bit into the 4bit, treating the quant biases + scales as learnable parameters.&lt;/p&gt; &lt;p&gt;Get the model here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Should theoretically feel like a 6bit in a 4bit quant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/N8Karma"&gt; /u/N8Karma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kff36y/experimental_quant_dwq_of_qwen3a30b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kff36y/experimental_quant_dwq_of_qwen3a30b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kff36y/experimental_quant_dwq_of_qwen3a30b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T15:54:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfhmdq</id>
    <title>EQ-Bench gets a proper update today. Targeting emotional intelligence in challenging multi-turn roleplays.</title>
    <updated>2025-05-05T17:33:48+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Leaderboard: &lt;a href="https://eqbench.com/"&gt;https://eqbench.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sample outputs: &lt;a href="https://eqbench.com/results/eqbench3_reports/o3.html"&gt;https://eqbench.com/results/eqbench3_reports/o3.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/EQ-bench/eqbench3"&gt;https://github.com/EQ-bench/eqbench3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Lots more to read about the benchmark:&lt;br /&gt; &lt;a href="https://eqbench.com/about.html#long"&gt;https://eqbench.com/about.html#long&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://eqbench.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfhmdq/eqbench_gets_a_proper_update_today_targeting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfhmdq/eqbench_gets_a_proper_update_today_targeting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T17:33:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kexdgy</id>
    <title>What do I test out / run first?</title>
    <updated>2025-05-04T23:21:02+00:00</updated>
    <author>
      <name>/u/Recurrents</name>
      <uri>https://old.reddit.com/user/Recurrents</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kexdgy/what_do_i_test_out_run_first/"&gt; &lt;img alt="What do I test out / run first?" src="https://external-preview.redd.it/Gj8FzKNPTvVSOxJwgeuufUJzmZ6BR-6YWri04zLtxfs.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=99bf755df82e7e9bb2bc2cafc9271bdc27217ed4" title="What do I test out / run first?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got her in the mail. Haven't had a chance to put her in yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recurrents"&gt; /u/Recurrents &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kexdgy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kexdgy/what_do_i_test_out_run_first/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kexdgy/what_do_i_test_out_run_first/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-04T23:21:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfdbdi</id>
    <title>is elevenlabs still unbeatable for tts? or good locall options</title>
    <updated>2025-05-05T14:42:21+00:00</updated>
    <author>
      <name>/u/sandwich_stevens</name>
      <uri>https://old.reddit.com/user/sandwich_stevens</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry if this is a common one, but surely due to the progress of these models, by now something would have changed with the TTS landscape, and we have some clean sounding local models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandwich_stevens"&gt; /u/sandwich_stevens &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfdbdi/is_elevenlabs_still_unbeatable_for_tts_or_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfdbdi/is_elevenlabs_still_unbeatable_for_tts_or_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfdbdi/is_elevenlabs_still_unbeatable_for_tts_or_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T14:42:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1kffj42</id>
    <title>New Qwen3-32B-AWQ (Activation-aware Weight Quantization)</title>
    <updated>2025-05-05T16:11:31+00:00</updated>
    <author>
      <name>/u/jbaenaxd</name>
      <uri>https://old.reddit.com/user/jbaenaxd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kffj42/new_qwen332bawq_activationaware_weight/"&gt; &lt;img alt="New Qwen3-32B-AWQ (Activation-aware Weight Quantization)" src="https://external-preview.redd.it/-aaTUrK8hOTrBZTDUSYGah4_Rjpn4rU7szaPy5gCq8U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=69449e796c503e08a80bc47f50ab28640b3a7384" title="New Qwen3-32B-AWQ (Activation-aware Weight Quantization)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/iqzchenylzye1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47719abf442cd1242a56ba1f11b786e3921b3e10"&gt;https://preview.redd.it/iqzchenylzye1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47719abf442cd1242a56ba1f11b786e3921b3e10&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen released this 3 days ago and no one noticed. These new models look great for running in local. This technique was used in Gemma 3 and it was great. Waiting for someone to add them to Ollama, so we can easily try them.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1918353505074725363"&gt;https://x.com/Alibaba_Qwen/status/1918353505074725363&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jbaenaxd"&gt; /u/jbaenaxd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kffj42/new_qwen332bawq_activationaware_weight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kffj42/new_qwen332bawq_activationaware_weight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kffj42/new_qwen332bawq_activationaware_weight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T16:11:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfebga</id>
    <title>Open WebUI license change : no longer OSI approved ?</title>
    <updated>2025-05-05T15:23:12+00:00</updated>
    <author>
      <name>/u/CroquetteLauncher</name>
      <uri>https://old.reddit.com/user/CroquetteLauncher</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While Open WebUI has proved an excellent tool, with a permissive license, I have noticed the new release do not seem to use an &lt;a href="https://opensource.org/licenses"&gt;OSI approved license&lt;/a&gt; and require a contributor license agreement.&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.openwebui.com/license/"&gt;https://docs.openwebui.com/license/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I understand the reasoning, but i wish they could find other way to enforce contribution, without moving away from an open source license. Some OSI approved license enforce even more sharing back for service providers (AGPL).&lt;/p&gt; &lt;p&gt;The FAQ &amp;quot;6. Does this mean Open WebUI is “no longer open source”? -&amp;gt; No, not at all.&amp;quot; is missing the point. Even if you have good and fair reasons to restrict usage, it does not mean that you can claim to still be open source. I asked Gemini pro 2.5 preview, Mistral 3.1 and Gemma 3 and they tell me that no, the new license is not opensource / freesoftware.&lt;/p&gt; &lt;p&gt;For now it's totally reasonable, but If there are some other good reasons to add restrictions in the future, and a CLA that say &amp;quot;we can add any restriction to your code&amp;quot;, it worry me a bit.&lt;/p&gt; &lt;p&gt;I'm still a fan of the project, but a bit more worried than before.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CroquetteLauncher"&gt; /u/CroquetteLauncher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfebga/open_webui_license_change_no_longer_osi_approved/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfebga/open_webui_license_change_no_longer_osi_approved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfebga/open_webui_license_change_no_longer_osi_approved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T15:23:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kfcdll</id>
    <title>We fit 50+ LLMs on 2 GPUs — cold starts under 2s. Here’s how.</title>
    <updated>2025-05-05T14:02:10+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’ve been experimenting with multi-model orchestration and ran into the usual wall: cold starts, bloated memory, and inefficient GPU usage. Everyone talks about inference, but very few go below the HTTP layer.&lt;/p&gt; &lt;p&gt;So we built our own runtime that snapshots the entire model execution state , attention caches, memory layout, everything , and restores it directly on the GPU. Result?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;•50+ models running on 2× A4000s •Cold starts consistently under 2 seconds •90%+ GPU utilization •No persistent bloating or overprovisioning &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It feels like an OS for inference , instead of restarting a process, we just resume it. If you’re running agents, RAG pipelines, or multi-model setups locally, this might be useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfcdll/we_fit_50_llms_on_2_gpus_cold_starts_under_2s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kfcdll/we_fit_50_llms_on_2_gpus_cold_starts_under_2s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kfcdll/we_fit_50_llms_on_2_gpus_cold_starts_under_2s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T14:02:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf5ry6</id>
    <title>JOSIEFIED Qwen3 8B is amazing! Uncensored, Useful, and great personality.</title>
    <updated>2025-05-05T07:31:25+00:00</updated>
    <author>
      <name>/u/My_Unbiased_Opinion</name>
      <uri>https://old.reddit.com/user/My_Unbiased_Opinion</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf5ry6/josiefied_qwen3_8b_is_amazing_uncensored_useful/"&gt; &lt;img alt="JOSIEFIED Qwen3 8B is amazing! Uncensored, Useful, and great personality." src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="JOSIEFIED Qwen3 8B is amazing! Uncensored, Useful, and great personality." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Primary link is for Ollama but here is the creator's model card on HF:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1"&gt;https://huggingface.co/Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just wanna say this model has replaced my older Abliterated models. I genuinely think this Josie model is better than the stock model. It adhears to instructions better and is not dry in its responses at all. Running at Q8 myself and it definitely punches above its weight class. Using it primarily in a online RAG system. &lt;/p&gt; &lt;p&gt;Hoping for a 30B A3B Josie finetune in the future! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/My_Unbiased_Opinion"&gt; /u/My_Unbiased_Opinion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/goekdenizguelmez/JOSIEFIED-Qwen3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf5ry6/josiefied_qwen3_8b_is_amazing_uncensored_useful/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf5ry6/josiefied_qwen3_8b_is_amazing_uncensored_useful/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T07:31:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kffq2u</id>
    <title>Qwen 3 235b gets high score in LiveCodeBench</title>
    <updated>2025-05-05T16:19:04+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kffq2u/qwen_3_235b_gets_high_score_in_livecodebench/"&gt; &lt;img alt="Qwen 3 235b gets high score in LiveCodeBench" src="https://preview.redd.it/px3okqrznzye1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c94891166c70bc3a59e886ae5359d04bdf3d33af" title="Qwen 3 235b gets high score in LiveCodeBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/px3okqrznzye1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kffq2u/qwen_3_235b_gets_high_score_in_livecodebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kffq2u/qwen_3_235b_gets_high_score_in_livecodebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T16:19:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kf9i52</id>
    <title>RTX 5060 Ti 16GB sucks for gaming, but seems like a diamond in the rough for AI</title>
    <updated>2025-05-05T11:42:02+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf9i52/rtx_5060_ti_16gb_sucks_for_gaming_but_seems_like/"&gt; &lt;img alt="RTX 5060 Ti 16GB sucks for gaming, but seems like a diamond in the rough for AI" src="https://a.thumbs.redditmedia.com/ZP_Pe9inInMfd_-rxiw6xfzYHLp5iOazS6Ztzjzk5U4.jpg" title="RTX 5060 Ti 16GB sucks for gaming, but seems like a diamond in the rough for AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I recently grabbed an RTX 5060 Ti 16GB for “just” $499 - while it’s no one’s first choice for gaming (reviews are pretty harsh), for AI workloads? This card might be a hidden gem.&lt;/p&gt; &lt;p&gt;I mainly wanted those 16GB of VRAM to fit bigger models, and it actually worked out. Ran LightRAG to ingest this beefy PDF: &lt;a href="https://www.fiscal.treasury.gov/files/reports-statements/financial-report/2024/executive-summary-2024.pdf"&gt;https://www.fiscal.treasury.gov/files/reports-statements/financial-report/2024/executive-summary-2024.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Compared it with a 12GB GPU (RTX 3060 Ti 12GB) - and I’ve attached Grafana charts showing GPU utilization for both runs.&lt;/p&gt; &lt;p&gt;🟢 16GB card: finished in 3 min 29 sec (green line) 🟡 12GB card: took 8 min 52 sec (yellow line)&lt;/p&gt; &lt;p&gt;Logs showed the 16GB card could load all 41 layers, while the 12GB one only managed 31. The rest had to be constantly swapped in and out - crushing performance by 2x and leading to underutilizing the GPU (as clearly seen in the Grafana metrics).&lt;/p&gt; &lt;p&gt;LightRAG uses “Mistral Nemo Instruct 12B”, served via Ollama, if you’re curious.&lt;/p&gt; &lt;p&gt;TL;DR: 16GB+ VRAM saves serious time.&lt;/p&gt; &lt;p&gt;Bonus: the card is noticeably shorter than others — it has 2 coolers instead of the usual 3, thanks to using PCIe x8 instead of x16. Great for small form factor builds or neat home AI setups. I’m planning one myself (please share yours if you’re building something similar!).&lt;/p&gt; &lt;p&gt;And yep - I had written a full guide earlier on how to go from clean bare metal to fully functional LightRAG setup in minutes. Fully automated, just follow the steps: 👉 &lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-LightRAG.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-LightRAG.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if you try this setup or run into issues - happy to help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kf9i52"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kf9i52/rtx_5060_ti_16gb_sucks_for_gaming_but_seems_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kf9i52/rtx_5060_ti_16gb_sucks_for_gaming_but_seems_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-05T11:42:02+00:00</published>
  </entry>
</feed>
