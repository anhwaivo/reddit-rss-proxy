<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-10T17:23:23+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j8135u</id>
    <title>What are some good LLMs for NSWF stories and dialogues? (RTX 3090, 64GB RAM, Win10)</title>
    <updated>2025-03-10T15:21:16+00:00</updated>
    <author>
      <name>/u/rookan</name>
      <uri>https://old.reddit.com/user/rookan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need to create scripts with lewd stories and dialogues for NSFW animations with popular game franchises characters (Final Fantasy, Mass Effect). Are there LLMs that could mimic a style a specific character talks? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rookan"&gt; /u/rookan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8135u/what_are_some_good_llms_for_nswf_stories_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8135u/what_are_some_good_llms_for_nswf_stories_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8135u/what_are_some_good_llms_for_nswf_stories_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T15:21:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j816xq</id>
    <title>Epyc 9184x build</title>
    <updated>2025-03-10T15:25:50+00:00</updated>
    <author>
      <name>/u/Competitive_Bid1192</name>
      <uri>https://old.reddit.com/user/Competitive_Bid1192</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recommendations?&lt;/p&gt; &lt;p&gt;Purchased it used under $600. Anyone local to NYC that can test it before I purchase a MB.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive_Bid1192"&gt; /u/Competitive_Bid1192 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j816xq/epyc_9184x_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j816xq/epyc_9184x_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j816xq/epyc_9184x_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T15:25:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j81d1p</id>
    <title>Can an LLM Learn to See? Fine Tuning Qwen 0.5B for Vision Tasks with SFT + GRPO</title>
    <updated>2025-03-10T15:32:57+00:00</updated>
    <author>
      <name>/u/JacksonCakess</name>
      <uri>https://old.reddit.com/user/JacksonCakess</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j81d1p/can_an_llm_learn_to_see_fine_tuning_qwen_05b_for/"&gt; &lt;img alt="Can an LLM Learn to See? Fine Tuning Qwen 0.5B for Vision Tasks with SFT + GRPO" src="https://b.thumbs.redditmedia.com/gsc7NXCXOZm-2WKKXpiQ-5Lp8WMwgU71T_1ywxzIpBc.jpg" title="Can an LLM Learn to See? Fine Tuning Qwen 0.5B for Vision Tasks with SFT + GRPO" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I just published a blog breaking down the &lt;strong&gt;math behind&lt;/strong&gt; &lt;em&gt;Group Relative Policy Optimization&lt;/em&gt; &lt;strong&gt;GRPO, the RL method behind DeepSeek R1&lt;/strong&gt; and walking through its &lt;strong&gt;implementation in&lt;/strong&gt; &lt;code&gt;trl&lt;/code&gt;—step by step!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y7e4j2jksvne1.png?width=927&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38e2af3a04370a8edfb55e889b5f2e94e724ebd4"&gt;https://preview.redd.it/y7e4j2jksvne1.png?width=927&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38e2af3a04370a8edfb55e889b5f2e94e724ebd4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fun experiment included&lt;/strong&gt;:&lt;br /&gt; I fine-tuned &lt;strong&gt;Qwen 2.5 0.5B&lt;/strong&gt;, a &lt;strong&gt;language-only&lt;/strong&gt; model without prior visual training, using &lt;strong&gt;SFT + GRPO&lt;/strong&gt; and got &lt;strong&gt;~73% accuracy&lt;/strong&gt; on a &lt;strong&gt;visual counting task&lt;/strong&gt;! &lt;/p&gt; &lt;p&gt;&lt;a href="https://jacksoncakes.com/2025/03/10/can-an-llm-learn-to-see-fine-tuning-qwen-05b-for-vision-tasks-with-sft-grpo/"&gt;Full blog&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/JacksonCakes/vision-r1"&gt;Github&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JacksonCakess"&gt; /u/JacksonCakess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j81d1p/can_an_llm_learn_to_see_fine_tuning_qwen_05b_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j81d1p/can_an_llm_learn_to_see_fine_tuning_qwen_05b_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j81d1p/can_an_llm_learn_to_see_fine_tuning_qwen_05b_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T15:32:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7q3ms</id>
    <title>I've uploaded new Wilmer users, and made another tutorial vid showing setup plus ollama hotswapping multiple 14b models on a single RTX 4090</title>
    <updated>2025-03-10T03:59:59+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alright folks, so a few days back I was talking about some of my development workflows using Wilmer and had promised to try to get those released this weekend, as well as a video on how to use them, and also again showing the Ollama model hot-swapping so that a single 4090 can run as many 14-24b models as you have hard drive space for. I finished just in time lol&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/v2xYQCHZwJM"&gt;The tutorial vid on Youtube&lt;/a&gt; &lt;em&gt;(pop to the 34 minute mark to see a quick example of the wikipedia workflow)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;For the hotswapping: I show it in the video, but basically every node in the workflow can hit a different LLM API, right? So if you have 10 nodes, you could hit 10 different APIs. With Ollama, you can just keep hitting the same API endpoint (say 127.0.0.1:11434), but each time you send a different model name. That will cause Ollama to unload the previous model, and load a new model. So even with 24GB of VRAM, you could have a workflow that uses a bunch of 8-24b models, and swaps them out on each node. Gives a little freedom to do more complex stuff with.&lt;/p&gt; &lt;p&gt;I've added 6 new example users to the &lt;a href="https://github.com/someoddcodeguy/wilmerAI/"&gt;WilmerAI &lt;/a&gt;repository, set with the models that I use for development/testing on my 24GB VRAM windows machine and all set up with Ollama multi-modal image support (they also should be able to handle multiple images in 1 message, instead of just 1 image at a time):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;coding-complex-multi-model&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;This is a long workflow. Really long. Using my real model lineup on the M2 Ultra, this workflow can take as long as 30-40 minutes to finish. But the result is usually worth it. I've had this one resolve issues o3-mini-high could not, and I've had 4o, o1, and o3-mini-high rate a lot of its work as better than 4o and o1.&lt;/li&gt; &lt;li&gt;I generally kick this guy off at the start of working on something, or when something gets really frustrating, but otherwise don't use often.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;coding-reasoning-multi-model&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;This workflow is heavily built around a reasoning workflow. Second heaviest hitter; takes a while with QwQ, but worth the result&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;coding-dual-multi-model&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;This is generally 2 non-reasoning models. Much faster than the first two.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;coding-single-multi-model&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;This is just 1 model, usually my coder like Qwen2.5 32b Coder.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;general-multi-model&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;This is a general purpose model, usually something mid-range like Qwen2.5 32b Instruct&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;general-offline-wikipedia&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;This is the general purpose model but injects a full text wikipedia article to help with factual responses&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;These are 6 of the 11 or so Wilmer instances I keep running to help with development; another 2 instances are two more general models: large (for factual answers like Qwen2.5 72b Instruct) and large-rag (something with high IF scores like Llama 3.3 70b Instruct).&lt;/p&gt; &lt;p&gt;Additionally, I've added a new Youtube tutorial video, which walks through downloading Wilmer, setting up a user, running it, and hitting it with a curl command.&lt;/p&gt; &lt;p&gt;Anyhow, hope this stuff is helpful! Eventually Roland will be in a spot that I can release it, but that's still a bit away. I apologize if there are any mistakes or issues in these users; after QwQ came out I completely reworked some of these workflows to make use of that model, so I hope it helps!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7q3ms/ive_uploaded_new_wilmer_users_and_made_another/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7q3ms/ive_uploaded_new_wilmer_users_and_made_another/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7q3ms/ive_uploaded_new_wilmer_users_and_made_another/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T03:59:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7g30z</id>
    <title>When will Llama 4, Gemma 3, or Qwen 3 be released?</title>
    <updated>2025-03-09T19:55:53+00:00</updated>
    <author>
      <name>/u/CreepyMan121</name>
      <uri>https://old.reddit.com/user/CreepyMan121</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When do you guys think these SOTA models will be released? It's been like forever so do anything of you know if there is a specific date in which they will release the new models? Also, what kind of New advancements do you think these models will bring to the AI industry, how will they be different from our old models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CreepyMan121"&gt; /u/CreepyMan121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7g30z/when_will_llama_4_gemma_3_or_qwen_3_be_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7g30z/when_will_llama_4_gemma_3_or_qwen_3_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7g30z/when_will_llama_4_gemma_3_or_qwen_3_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T19:55:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7dzao</id>
    <title>QWQ low score in Leaderboard, what happened?</title>
    <updated>2025-03-09T18:25:08+00:00</updated>
    <author>
      <name>/u/ipechman</name>
      <uri>https://old.reddit.com/user/ipechman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7dzao/qwq_low_score_in_leaderboard_what_happened/"&gt; &lt;img alt="QWQ low score in Leaderboard, what happened?" src="https://preview.redd.it/77rco6vfipne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce1eb44fa048f06ef55f271c38be8e206c4ed0a5" title="QWQ low score in Leaderboard, what happened?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ipechman"&gt; /u/ipechman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/77rco6vfipne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7dzao/qwq_low_score_in_leaderboard_what_happened/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7dzao/qwq_low_score_in_leaderboard_what_happened/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T18:25:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1j81fx3</id>
    <title>AlexBefest's CardProjector-v2 series.</title>
    <updated>2025-03-10T15:36:15+00:00</updated>
    <author>
      <name>/u/AlexBefest</name>
      <uri>https://old.reddit.com/user/AlexBefest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model Name: AlexBefest/CardProjector-14B-v2 and AlexBefest/CardProjector-7B-v2&lt;/p&gt; &lt;p&gt;Models URL: &lt;a href="https://huggingface.co/collections/AlexBefest/cardprojector-v2-67cecdd5502759f205537122"&gt;https://huggingface.co/collections/AlexBefest/cardprojector-v2-67cecdd5502759f205537122&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model Author: AlexBefest, &lt;a href="https://www.reddit.com/user/AlexBefest/"&gt;u/AlexBefest&lt;/a&gt;, &lt;a href="https://huggingface.co/AlexBefest"&gt;AlexBefest&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What's new in v2?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Model output format has been completely redesigned! I decided to completely abandon the json output format, which allowed: 1) significantly improve the output quality; 2) improved the ability of the model to support multi-turn conservation for character editing; 3) largely frees your hands in Creative Writing, you can not be afraid to set any high temperatures, up to 1-1.1, without fear of broken json stubs; 4) allows you to create characters not only for Silly Tavern, but for the characters as a whole, 5) it is much more convenient to perceive the information generated&lt;/li&gt; &lt;li&gt;A total improvement in Creative Writing overall in character creation compared to v1 and v1.1.&lt;/li&gt; &lt;li&gt;A total improvement of generating the First Message label&lt;/li&gt; &lt;li&gt;Significantly improved the quality and detail of the characters: character descriptions are now richer, more consistent and engaging. I've focused on improving the depth and nuances of the characters and their backstories.&lt;/li&gt; &lt;li&gt;Improved output stability.&lt;/li&gt; &lt;li&gt;Improved edit processing: The initial improvements are in how the model handles edit requests, which allows you to create character maps more consistently. While it is under development, you should see more consistent and relevant changes when requesting changes to existing maps.&lt;/li&gt; &lt;li&gt;Improved the logical component of the model compared to v1 and v1.1.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Overview:&lt;/h1&gt; &lt;p&gt;CardProjector is a specialized series of language models, fine-tuned to generate character cards for &lt;strong&gt;SillyTavern&lt;/strong&gt; and &lt;strong&gt;now for creating characters in general&lt;/strong&gt;. These models are designed to assist creators and roleplayers by automating the process of crafting detailed and well-structured character cards, ensuring compatibility with SillyTavern's format.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexBefest"&gt; /u/AlexBefest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j81fx3/alexbefests_cardprojectorv2_series/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j81fx3/alexbefests_cardprojectorv2_series/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j81fx3/alexbefests_cardprojectorv2_series/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T15:36:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7ytq4</id>
    <title>Enjoying local LLM so much! My ultimate wish: A bag of compact, domain-focused "expert" models.</title>
    <updated>2025-03-10T13:39:29+00:00</updated>
    <author>
      <name>/u/partysnatcher</name>
      <uri>https://old.reddit.com/user/partysnatcher</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like the title says, I'm really hooked on the &amp;quot;local LLM movement&amp;quot;. I'm very much enjoying and making use of for instance DeepSeek-R1:14b locally - with plenty of use for it (for instance, Im batch scripting to create a mini trainingset Im playing with).&lt;/p&gt; &lt;p&gt;However, 14B quantized (Qwen-2.5 based one), while extremely impressive for what it can do, is definitely limited by parameter size (in terms of precision, hallucination etc).&lt;/p&gt; &lt;p&gt;Despite that, I do not want to buy 64x 3090s to create some AI god that thinks for me and does everything for me.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I want to manually choose an expert (or a mix of experts) per task.&lt;/strong&gt; Not only is that less troublesome, but I think it offers more control and is more involving and fun.&lt;/p&gt; &lt;p&gt;I also think that focused &amp;quot;verifier models&amp;quot; that are solely based on breaking down and criticizing text, are very useful, not only for the individual user tasks, but also, when an expert and a verifier are running serially and bouncing back and forth, they can create a stronger and more tightly wound form of the same back-and-forth that reasoning models do.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;Topic: what is the next breakthrough in physics?&lt;/code&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;Physics expert&amp;quot;:&lt;/strong&gt; &lt;code&gt;Deeper understanding of engineering quantum mechanics .. quantum computing .. blabla. &amp;lt;VERIFICATION REQUESTED&amp;gt;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;Physics tester/verifier&amp;quot;:&lt;/strong&gt; &lt;code&gt;Interesting thoughts, but paragraph 1 breaks with the principle in the standard model of .. &amp;lt;REITERATION REQUESTED&amp;gt;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;Physics expert&amp;quot;:&lt;/strong&gt; &lt;code&gt;I have modified paragraph 1 for better coherence with the standard model. This changes some of the premises in paragraph 2. &amp;lt;VERIFICATION REQUESTED&amp;gt;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;Physics tester/verifier&amp;quot;:&lt;/strong&gt; &lt;code&gt;That looks good..&amp;lt;ITERATION END&amp;gt;&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Here is an example list of focused experts (with verifiers / testers) that I want to pull from ollama some day:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Task planning and project management agent (with strong interdisciplinary overview) &lt;ul&gt; &lt;li&gt;TESTER/VERIFIER: Local user (me)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Coding (on the architectural level)&lt;/li&gt; &lt;li&gt;Coding (on the function level) &lt;ul&gt; &lt;li&gt;TESTER/VERIFIER: Coding (on the testing level)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Multilinguality and single-direction translation &lt;ul&gt; &lt;li&gt;TESTER/VERIFIER: Other-direction translation&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Expert understanding of &amp;lt;field&amp;gt; (physics, biology, medicine, economy, local accounting) &lt;ul&gt; &lt;li&gt;TESTER/VERIFIER: Dictionary-style knowledge LLM (&amp;quot;wiki mixer&amp;quot;)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Good text creation &lt;ul&gt; &lt;li&gt;TESTER/VERIFIER: Good text analysis and understanding; coherence, purpose-oriented, reader experience focused LLM&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Image generation - diffusion &lt;ul&gt; &lt;li&gt;TESTER/VERIFIER: Image testing - interpretation and analysis model&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Mainly, I would love to run these independently, but of course, each of these can recursively &amp;quot;script each other up&amp;quot;, and run serially, either in agentic setup or in a inter-model reasoning design.&lt;/p&gt; &lt;p&gt;In short, I don't really anymore believe in this vision of a singular intelligent entity hosted in Silicon Valley that knows anything and everything. To me, all arrows point in the direction of focused dense models, and I want as many compact dense expert models as I can get my hands on.&lt;/p&gt; &lt;p&gt;What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/partysnatcher"&gt; /u/partysnatcher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7ytq4/enjoying_local_llm_so_much_my_ultimate_wish_a_bag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7ytq4/enjoying_local_llm_so_much_my_ultimate_wish_a_bag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7ytq4/enjoying_local_llm_so_much_my_ultimate_wish_a_bag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T13:39:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7xwke</id>
    <title>Anyone using a 3060 12gb + 2060 12gb?</title>
    <updated>2025-03-10T12:53:15+00:00</updated>
    <author>
      <name>/u/Tiny-Table7937</name>
      <uri>https://old.reddit.com/user/Tiny-Table7937</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone local has a 12gb 2060 for $120, I'm considering throwing it in my extra PCIE slot. Wondered if anyone had done something like that, and how it went. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tiny-Table7937"&gt; /u/Tiny-Table7937 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7xwke/anyone_using_a_3060_12gb_2060_12gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7xwke/anyone_using_a_3060_12gb_2060_12gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7xwke/anyone_using_a_3060_12gb_2060_12gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T12:53:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7q0fw</id>
    <title>What are some useful tasks I can perform with smaller (&lt; 8b) local models?</title>
    <updated>2025-03-10T03:54:51+00:00</updated>
    <author>
      <name>/u/binarySolo0h1</name>
      <uri>https://old.reddit.com/user/binarySolo0h1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am new to the AI scenes and I can run smaller local ai models on my machine. So, what are some things that I can use these local models for. They need not be complex. Anything small but useful to improve everyday development workflow is good enough.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/binarySolo0h1"&gt; /u/binarySolo0h1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7q0fw/what_are_some_useful_tasks_i_can_perform_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7q0fw/what_are_some_useful_tasks_i_can_perform_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7q0fw/what_are_some_useful_tasks_i_can_perform_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T03:54:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j83md3</id>
    <title>Could GEMMA-3 Be Unveiled at GDC 2025 (March 18)?</title>
    <updated>2025-03-10T17:05:48+00:00</updated>
    <author>
      <name>/u/hCKstp4BtL</name>
      <uri>https://old.reddit.com/user/hCKstp4BtL</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://schedule.gdconf.com/session/beyond-the-hype-real-world-applications-of-google-ai-in-gaming-presented-by-google-play/911129"&gt;https://schedule.gdconf.com/session/beyond-the-hype-real-world-applications-of-google-ai-in-gaming-presented-by-google-play/911129&lt;/a&gt;&lt;/p&gt; &lt;p&gt;in this session description, we can read that they will talk about &amp;quot;Gemma models&amp;quot; (among other things). I think everyone knows about &amp;quot;Gemma 2&amp;quot; and there is no need to mention it because everyone knows how it works, right? Bigger chance is that they will show &amp;quot;Gemma 3&amp;quot; and they will release it shorly? because it seems to me that the deadline of May 20-21 (Google I/O) is a bit too late.&lt;/p&gt; &lt;p&gt;It looks like Google wants to focus the eyes of game developers on Gemma, so that they can combine the models with their games to create: “new AI-based game features and mechanics.”&lt;/p&gt; &lt;p&gt;... and to make it work, I think such a &amp;quot;Gemma 3&amp;quot; model should be prioritize with &amp;quot;perfect JSON generation&amp;quot; for the interface model&amp;lt;-&amp;gt;game and also improved instruction following.&lt;/p&gt; &lt;p&gt;I waiting for a small model (7b-9b) to be good enough to make a game with llm controlling npc (not only talk).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hCKstp4BtL"&gt; /u/hCKstp4BtL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83md3/could_gemma3_be_unveiled_at_gdc_2025_march_18/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83md3/could_gemma3_be_unveiled_at_gdc_2025_march_18/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j83md3/could_gemma3_be_unveiled_at_gdc_2025_march_18/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T17:05:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7wymw</id>
    <title>Expert opinion requested: Am I reaching the limits of &lt;10GB models?</title>
    <updated>2025-03-10T12:00:58+00:00</updated>
    <author>
      <name>/u/yo252yo</name>
      <uri>https://old.reddit.com/user/yo252yo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I've been trying to make a conversation agent for a few weeks now and I'm not very happy with what I'm getting.&lt;/p&gt; &lt;p&gt;I'm working on an RTX 4070 and I've found that it allows me to run perfectly smoothly models around 7/8B params, essentially everything that takes &lt;strong&gt;&amp;lt;8GB VRAM&lt;/strong&gt; comfortably.&lt;/p&gt; &lt;p&gt;I'm honestly really impressed by the quality of the output for such small models, but I'm &lt;strong&gt;struggling with them understanding instructions&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Since these models are pretty small, I'm trying to avoid too-long system prompts and have been keeping mine around 400 words.&lt;/p&gt; &lt;p&gt;I've tried shorter and longer, I've tried various models but they all tend to gravitate towards &lt;strong&gt;common pitfalls&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;they produce big responses, ignoring my instructions to keep things to 1/2 sentences&lt;/li&gt; &lt;li&gt;they produce the stereotypical LLM responses with open ended questions at the end of every response &amp;quot;What do you think about X?&amp;quot;&lt;/li&gt; &lt;li&gt;they sometimes get weirdly stuck talking repetitively in broad terms about a generic topic instead of following the flow of the conversation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These problems are quite abstract and hard to investigate. The biggest pain point though is that whatever I seem to do in prompt to mitigate seems mostly ignored.&lt;/p&gt; &lt;p&gt;It's my understanding that those are common pitfalls of small or old models. I have &lt;strong&gt;ideas for further exploration&lt;/strong&gt; such as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;maybe try to write a really long prompt explaining exactly what is a conversation&lt;/li&gt; &lt;li&gt;maybe i should try to feed it more examples, for instance hardcode a beginning of a conversation&lt;/li&gt; &lt;li&gt;maybe i should try my own fine-tuning (though I'm not super good at complex tech stuff). In particular I'm thinking maybe all models are either tuned for ERP or chatbot query/answer and I might not have found a model that does good friendly SFW conversation&lt;/li&gt; &lt;li&gt;i'm also experimenting with how much metadata I feed into the system and how I feed it (i.e. the conversation topic is X, the conversation so far has been XYZ...). I was inserting this as SystemMessage in the conversation feed to complete but maybe that's not a good thing, idk. I wonder if that stuff is best in the system prompt or in the discussion thread...&lt;/li&gt; &lt;li&gt;maybe i can have another round-trip of a tiny model taking the output of my model and shortening it to make it fit in a conversation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But before I continue investing so much time in all of this, I wanted to gather feedback from people who might know more, because maybe I'm just hitting a wall and nothing I do will help short of investing in better hardware. That being said I'll lose it if I spend so much money on a bit more VRAM and the 13b or more models still cant follow simple instructions.&lt;/p&gt; &lt;p&gt;What do you guys think? I've read everything I could find about small model pitfalls, but I haven't found an answer to questions like: Does anyone have an understanding on how long can I afford to make a system prompt for a 7B model? Do any of my mitigation plans seem more promising than the others? Is there any trick to conversational AI that I missed?&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;p&gt;&lt;em&gt;PS: my best results have been with neuraldaredevil-8b-abliterated:q8_0, l3-8b-stheno-v3.2 or mn-12b-mag-mell-r1:latest, deepseek-r1:8b is nice but i cant get it to make short answers.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yo252yo"&gt; /u/yo252yo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7wymw/expert_opinion_requested_am_i_reaching_the_limits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7wymw/expert_opinion_requested_am_i_reaching_the_limits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7wymw/expert_opinion_requested_am_i_reaching_the_limits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T12:00:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j82o15</id>
    <title>Fixed Ollama template for Mistral Small 3</title>
    <updated>2025-03-10T16:26:52+00:00</updated>
    <author>
      <name>/u/logkn</name>
      <uri>https://old.reddit.com/user/logkn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was finding that Mistral Small 3 on Ollama (mistral-small:24b) had some trouble calling tools -- mainly, adding or dropping tokens that rendered the tool call as message content rather than an actual tool call.&lt;br /&gt; The chat template on the model's Huggingface page was actually not very helpful because it doesn't even include tool calling. I dug around a bit to find the Tekken V7 tokenizer, and sure enough the chat template for providing and calling tools didn't match up with Ollama's.&lt;/p&gt; &lt;p&gt;Here's a fixed version, and it's MUCH more consistent with tool calling:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{{- range $index, $_ := .Messages }} {{- if eq .Role &amp;quot;system&amp;quot; }}[SYSTEM_PROMPT]{{ .Content }}[/SYSTEM_PROMPT] {{- else if eq .Role &amp;quot;user&amp;quot; }} {{- if and (le (len (slice $.Messages $index)) 2) $.Tools }}[AVAILABLE_TOOLS]{{ $.Tools }}[/AVAILABLE_TOOLS] {{- end }}[INST]{{ .Content }}[/INST] {{- else if eq .Role &amp;quot;assistant&amp;quot; }} {{- if .Content }}{{ .Content }} {{- if not (eq (len (slice $.Messages $index)) 1) }}&amp;lt;/s&amp;gt; {{- end }} {{- else if .ToolCalls }}[TOOL_CALLS] [ {{- range .ToolCalls }}{&amp;quot;name&amp;quot;: &amp;quot;{{ .Function.Name }}&amp;quot;, &amp;quot;arguments&amp;quot;: {{ .Function.Arguments }}} {{- end }}]&amp;lt;/s&amp;gt; {{- end }} {{- else if eq .Role &amp;quot;tool&amp;quot; }}[TOOL_RESULTS] [TOOL_CONTENT] {{ .Content }}[/TOOL_RESULTS] {{- end }} {{- end }} &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/logkn"&gt; /u/logkn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j82o15/fixed_ollama_template_for_mistral_small_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j82o15/fixed_ollama_template_for_mistral_small_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j82o15/fixed_ollama_template_for_mistral_small_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T16:26:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7vwcr</id>
    <title>Deepseek coder v2</title>
    <updated>2025-03-10T10:53:20+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got this model last night, for a 7B it is soooo good at web coding!!!&lt;/p&gt; &lt;p&gt;I have made a working calculator, pong, and flappy bird.&lt;/p&gt; &lt;p&gt;I'm using the lite model by lmstudio. best of all I'm getting 16 tps on my ryzen!!!&lt;/p&gt; &lt;p&gt;using this model in particular &lt;a href="https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF"&gt;https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7vwcr/deepseek_coder_v2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7vwcr/deepseek_coder_v2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7vwcr/deepseek_coder_v2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T10:53:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1j83imv</id>
    <title>We tested open and closed models for embodied decision alignment, and we found Qwen 2.5 VL is surprisingly stronger than most closed frontier models.</title>
    <updated>2025-03-10T17:01:38+00:00</updated>
    <author>
      <name>/u/ProKil_Chu</name>
      <uri>https://old.reddit.com/user/ProKil_Chu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One thing that surprised us during benchmarking with EgoNormia is that Qwen 2.5 VL is indeed a very strong model for vision which rivals Gemini 1.5/2.0, better than GPT-4o and Claude 3.5 Sonnet.&lt;/p&gt; &lt;p&gt;Tweet: &lt;a href="https://x.com/_Hao_Zhu/status/1899137396048003386"&gt;https://x.com/_Hao_Zhu/status/1899137396048003386&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Leaderboard: &lt;a href="https://egonormia.org"&gt;https://egonormia.org&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Eval code: &lt;a href="https://github.com/Open-Social-World/EgoNormia"&gt;https://github.com/Open-Social-World/EgoNormia&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ProKil_Chu"&gt; /u/ProKil_Chu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83imv/we_tested_open_and_closed_models_for_embodied/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83imv/we_tested_open_and_closed_models_for_embodied/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j83imv/we_tested_open_and_closed_models_for_embodied/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T17:01:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7s1ef</id>
    <title>Why Isn't There a Real-Time AI Translation App for Smartphones Yet?</title>
    <updated>2025-03-10T06:03:28+00:00</updated>
    <author>
      <name>/u/spbxspb</name>
      <uri>https://old.reddit.com/user/spbxspb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With all the advancements in AI, especially in language models and real-time processing, why don’t we have a truly seamless AI-powered translation app for smartphones? Something that works offline, translates speech in real-time with minimal delay, and supports multiple languages fluently.&lt;/p&gt; &lt;p&gt;Most current apps either require an internet connection, have significant lag, or struggle with natural-sounding translations. Given how powerful AI has become, it feels like we should already have a Star Trek-style universal translator by now.&lt;/p&gt; &lt;p&gt;Is it a technical limitation, a business decision, or something else?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spbxspb"&gt; /u/spbxspb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7s1ef/why_isnt_there_a_realtime_ai_translation_app_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7s1ef/why_isnt_there_a_realtime_ai_translation_app_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7s1ef/why_isnt_there_a_realtime_ai_translation_app_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T06:03:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7j6cg</id>
    <title>&lt;70B models aren't ready to solo codebases yet, but we're gaining momentum and fast</title>
    <updated>2025-03-09T22:11:41+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j6cg/70b_models_arent_ready_to_solo_codebases_yet_but/"&gt; &lt;img alt="&amp;lt;70B models aren't ready to solo codebases yet, but we're gaining momentum and fast" src="https://external-preview.redd.it/ZDFiNmN0NHptcW5lMdBuqabr-hQLmYC8Qi5X9EdtbTx_2YZ-hAZhcsR_hrB1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2014a6f010867f98da226a97f756cd7d035b3cb" title="&amp;lt;70B models aren't ready to solo codebases yet, but we're gaining momentum and fast" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2wo0b8lqmqne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j6cg/70b_models_arent_ready_to_solo_codebases_yet_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j6cg/70b_models_arent_ready_to_solo_codebases_yet_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T22:11:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7vx6z</id>
    <title>Open manus</title>
    <updated>2025-03-10T10:55:02+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/mannaandpoem/OpenManus"&gt;https://github.com/mannaandpoem/OpenManus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone got any views on this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7vx6z/open_manus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7vx6z/open_manus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7vx6z/open_manus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T10:55:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7uqv4</id>
    <title>v0.6.0 Update: Dive - An Open Source MCP Agent Desktop</title>
    <updated>2025-03-10T09:29:10+00:00</updated>
    <author>
      <name>/u/BigGo_official</name>
      <uri>https://old.reddit.com/user/BigGo_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7uqv4/v060_update_dive_an_open_source_mcp_agent_desktop/"&gt; &lt;img alt="v0.6.0 Update: Dive - An Open Source MCP Agent Desktop" src="https://external-preview.redd.it/emVhYWcxajZ6dG5lMatJSspljsEqCKIUqwl7TvTa14fhRRDdCNE6VsWU_1B_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d328715d2f1b9c124872513a6e543c4697240cf6" title="v0.6.0 Update: Dive - An Open Source MCP Agent Desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BigGo_official"&gt; /u/BigGo_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/e11wp2j6ztne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7uqv4/v060_update_dive_an_open_source_mcp_agent_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7uqv4/v060_update_dive_an_open_source_mcp_agent_desktop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T09:29:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7zzdt</id>
    <title>All about LLMs</title>
    <updated>2025-03-10T14:32:28+00:00</updated>
    <author>
      <name>/u/meme_watcher69420</name>
      <uri>https://old.reddit.com/user/meme_watcher69420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was given an offer to join this startup. They were impressed with my &amp;quot;knowledge&amp;quot; about AI and LLMs. But in reality, all my projects are made by pasting stuff from Claude, stackoverflow and improved with reading a few documents.&lt;/p&gt; &lt;p&gt;How do I get to know everything about setting up LLMs, integrating them into an application and deploying them? Is there a guide or a roadmap to it? I'll join this startup in a month so I got a bit of time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/meme_watcher69420"&gt; /u/meme_watcher69420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7zzdt/all_about_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7zzdt/all_about_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7zzdt/all_about_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T14:32:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7n2s5</id>
    <title>Manus turns out to be just Claude Sonnet + 29 other tools, Reflection 70B vibes ngl</title>
    <updated>2025-03-10T01:18:27+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7n2s5/manus_turns_out_to_be_just_claude_sonnet_29_other/"&gt; &lt;img alt="Manus turns out to be just Claude Sonnet + 29 other tools, Reflection 70B vibes ngl" src="https://b.thumbs.redditmedia.com/aZROr3LtwGC89EWOHjMHoIbCvPQTB8Fs1jwIfYjEb8U.jpg" title="Manus turns out to be just Claude Sonnet + 29 other tools, Reflection 70B vibes ngl" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/yi72dtz4krne1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f8e56eaed3a6e5eee634567caa3f64f1fbcc2f1"&gt;https://preview.redd.it/yi72dtz4krne1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f8e56eaed3a6e5eee634567caa3f64f1fbcc2f1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/Dorialexander/status/1898719861284454718"&gt;https://x.com/Dorialexander/status/1898719861284454718&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6b84agl7krne1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ce944225e1c92f7d0d466c896fdf2a80c667837"&gt;https://preview.redd.it/6b84agl7krne1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ce944225e1c92f7d0d466c896fdf2a80c667837&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/jianxliao/status/1898861051183349870"&gt;https://x.com/jianxliao/status/1898861051183349870&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7n2s5/manus_turns_out_to_be_just_claude_sonnet_29_other/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7n2s5/manus_turns_out_to_be_just_claude_sonnet_29_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7n2s5/manus_turns_out_to_be_just_claude_sonnet_29_other/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T01:18:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7usrm</id>
    <title>EuroBERT: A High-Performance Multilingual Encoder Model</title>
    <updated>2025-03-10T09:33:09+00:00</updated>
    <author>
      <name>/u/-Cubie-</name>
      <uri>https://old.reddit.com/user/-Cubie-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7usrm/eurobert_a_highperformance_multilingual_encoder/"&gt; &lt;img alt="EuroBERT: A High-Performance Multilingual Encoder Model" src="https://external-preview.redd.it/CnsI7xgYaguHri1_uGabuT9boB9PVspWCoeHvZsE1IM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f747438c0575ca5fca91283bb815527cfb8627a" title="EuroBERT: A High-Performance Multilingual Encoder Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cubie-"&gt; /u/-Cubie- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/EuroBERT/release"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7usrm/eurobert_a_highperformance_multilingual_encoder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7usrm/eurobert_a_highperformance_multilingual_encoder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T09:33:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1j80hbo</id>
    <title>Hunyuan-TurboS.</title>
    <updated>2025-03-10T14:54:37+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://twitter.com/TXhunyuan/status/1899105803073958010"&gt;https://twitter.com/TXhunyuan/status/1899105803073958010&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j80hbo/hunyuanturbos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j80hbo/hunyuanturbos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j80hbo/hunyuanturbos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T14:54:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7t18m</id>
    <title>Framework and DIGITS suddenly seem underwhelming compared to the 512GB Unified Memory on the new Mac.</title>
    <updated>2025-03-10T07:15:44+00:00</updated>
    <author>
      <name>/u/Common_Ad6166</name>
      <uri>https://old.reddit.com/user/Common_Ad6166</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was holding out on purchasing a FrameWork desktop until we could see what kind of performance the DIGITS would get when it comes out in May. But now that Apple has announced the new M4 Max/ M3 Ultra Mac's with 512 GB Unified memory, the 128 GB options on the other two seem paltry in comparison. &lt;/p&gt; &lt;p&gt;Are we actually going to be locked into the Apple ecosystem for another decade? This can't be true!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Common_Ad6166"&gt; /u/Common_Ad6166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7t18m/framework_and_digits_suddenly_seem_underwhelming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7t18m/framework_and_digits_suddenly_seem_underwhelming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7t18m/framework_and_digits_suddenly_seem_underwhelming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T07:15:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7r47l</id>
    <title>I just made an animation of a ball bouncing inside a spinning hexagon</title>
    <updated>2025-03-10T05:01:09+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/"&gt; &lt;img alt="I just made an animation of a ball bouncing inside a spinning hexagon" src="https://external-preview.redd.it/aHcybDc4eW5tc25lMWpXkBeJA0bkbXxKyNPWYhDqX6Z4Wwq4cQiczMXRiEBU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1910662e66472f313e9a9c19401be8a1be2f181a" title="I just made an animation of a ball bouncing inside a spinning hexagon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cy79860omsne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T05:01:09+00:00</published>
  </entry>
</feed>
