<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-13T08:40:44+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kl6llw</id>
    <title>RAM vs NVME swap for AI?</title>
    <updated>2025-05-12T22:55:49+00:00</updated>
    <author>
      <name>/u/lukinhasb</name>
      <uri>https://old.reddit.com/user/lukinhasb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 64GB RAM, 24GB 4090 and I want to run large models like qwen235 moe (111gb)&lt;/p&gt; &lt;p&gt;I have created generous swap files (like 200gb) in my NVME.&lt;/p&gt; &lt;p&gt;How's the performance of NVME swap compared to RAM for AI?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lukinhasb"&gt; /u/lukinhasb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl6llw/ram_vs_nvme_swap_for_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl6llw/ram_vs_nvme_swap_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl6llw/ram_vs_nvme_swap_for_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T22:55:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkgzip</id>
    <title>INTELLECT-2 Released: The First 32B Parameter Model Trained Through Globally Distributed Reinforcement Learning</title>
    <updated>2025-05-12T01:46:22+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkgzip/intellect2_released_the_first_32b_parameter_model/"&gt; &lt;img alt="INTELLECT-2 Released: The First 32B Parameter Model Trained Through Globally Distributed Reinforcement Learning" src="https://external-preview.redd.it/C1X5HGKGzXyAtD9lvvvB3VxlaW_Pl5NuFtz4_fp414w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8977006bf732e56b214f916d46801909a0bb97fa" title="INTELLECT-2 Released: The First 32B Parameter Model Trained Through Globally Distributed Reinforcement Learning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/PrimeIntellect/INTELLECT-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkgzip/intellect2_released_the_first_32b_parameter_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkgzip/intellect2_released_the_first_32b_parameter_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T01:46:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkqud8</id>
    <title>Continuous Thought Machines - Sakana AI</title>
    <updated>2025-05-12T12:07:03+00:00</updated>
    <author>
      <name>/u/ThiccStorms</name>
      <uri>https://old.reddit.com/user/ThiccStorms</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkqud8/continuous_thought_machines_sakana_ai/"&gt; &lt;img alt="Continuous Thought Machines - Sakana AI" src="https://external-preview.redd.it/301MLdXBGS0U_36M44Bby0bKZg0NibAojUn2aDi7Aao.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3dedee1849a1301ba66f6e5516f26d39f420baa8" title="Continuous Thought Machines - Sakana AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThiccStorms"&gt; /u/ThiccStorms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sakana.ai/ctm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkqud8/continuous_thought_machines_sakana_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkqud8/continuous_thought_machines_sakana_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T12:07:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl3scc</id>
    <title>Inverse Turing Test (Open Source HF Space) - Can you fool the AI?</title>
    <updated>2025-05-12T20:57:04+00:00</updated>
    <author>
      <name>/u/GuiltyBookkeeper4849</name>
      <uri>https://old.reddit.com/user/GuiltyBookkeeper4849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Today, I'm launching a new experimental Hugging Face Space: &lt;strong&gt;Inverse Turing Test&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;I flipped the classic Turing Test. Instead of an AI trying to pass as human, &lt;strong&gt;you&lt;/strong&gt; need to convince a group of AI agents that you are the AI among them.&lt;/p&gt; &lt;p&gt;The challenge: Blend in, chat like an AI, analyze the other &amp;quot;players&amp;quot; (who are actual AIs!), and survive the elimination votes each round. Can you mimic AI patterns well enough to deceive the majority and be one of the last two standing?&lt;/p&gt; &lt;p&gt;ðŸ”¹ &lt;strong&gt;Try the Inverse Turing Test:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fhuggingface.co%2Fspaces%2Fgr0010%2FInverse-Turing-Test"&gt;https://huggingface.co/spaces/gr0010/Inverse-Turing-Test&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if you manage to fool them or how long you survive! Drop a like on the Space if you enjoy the challenge!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuiltyBookkeeper4849"&gt; /u/GuiltyBookkeeper4849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl3scc/inverse_turing_test_open_source_hf_space_can_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl3scc/inverse_turing_test_open_source_hf_space_can_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl3scc/inverse_turing_test_open_source_hf_space_can_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T20:57:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1klgp0l</id>
    <title>Aider benchmark sortable by language page</title>
    <updated>2025-05-13T08:22:13+00:00</updated>
    <author>
      <name>/u/Mxfrj</name>
      <uri>https://old.reddit.com/user/Mxfrj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, a while ago I saw a page (probably posted here) where you could see benchmark results from aider (in my mind it was aider) for multiple programming languages and multiple models.&lt;/p&gt; &lt;p&gt;It was a chart if I remember correctly and you could adjust &amp;lt;things&amp;gt; at the top. I would really like to find that page again - does maybe somebody know what I mean?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mxfrj"&gt; /u/Mxfrj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klgp0l/aider_benchmark_sortable_by_language_page/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klgp0l/aider_benchmark_sortable_by_language_page/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klgp0l/aider_benchmark_sortable_by_language_page/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T08:22:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkvqti</id>
    <title>Qwen3 throughput benchmarks on 2x 3090, almost 1000 tok/s using 4B model and vLLM as the inference engine</title>
    <updated>2025-05-12T15:42:32+00:00</updated>
    <author>
      <name>/u/kms_dev</name>
      <uri>https://old.reddit.com/user/kms_dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h3&gt;Setup&lt;/h3&gt; &lt;p&gt;System:&lt;/p&gt; &lt;p&gt;CPU: Ryzen 5900x RAM: 32GB GPUs: 2x 3090 (pcie 4.0 x16 + pcie 4.0 x4) allowing full 350W on each card&lt;/p&gt; &lt;p&gt;Input tokens per request: 4096&lt;/p&gt; &lt;p&gt;Generated tokens per request: 1024&lt;/p&gt; &lt;p&gt;Inference engine: vLLM&lt;/p&gt; &lt;h3&gt;Benchmark results&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model name&lt;/th&gt; &lt;th&gt;Quantization&lt;/th&gt; &lt;th&gt;Parallel Structure&lt;/th&gt; &lt;th&gt;Output token throughput (TG)&lt;/th&gt; &lt;th&gt;Total token throughput (TG+PP)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen3-4b&lt;/td&gt; &lt;td&gt;FP16&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;749&lt;/td&gt; &lt;td&gt;3811&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-4b&lt;/td&gt; &lt;td&gt;FP8&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;790&lt;/td&gt; &lt;td&gt;4050&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-4b&lt;/td&gt; &lt;td&gt;AWQ&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;833&lt;/td&gt; &lt;td&gt;4249&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-4b&lt;/td&gt; &lt;td&gt;W8A8&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;981&lt;/td&gt; &lt;td&gt;4995&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-8b&lt;/td&gt; &lt;td&gt;FP16&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;387&lt;/td&gt; &lt;td&gt;1993&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-8b&lt;/td&gt; &lt;td&gt;FP8&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;581&lt;/td&gt; &lt;td&gt;3000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-14b&lt;/td&gt; &lt;td&gt;FP16&lt;/td&gt; &lt;td&gt;tp2&lt;/td&gt; &lt;td&gt;214&lt;/td&gt; &lt;td&gt;1105&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-14b&lt;/td&gt; &lt;td&gt;FP8&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;267&lt;/td&gt; &lt;td&gt;1376&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-14b&lt;/td&gt; &lt;td&gt;AWQ&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;382&lt;/td&gt; &lt;td&gt;1947&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-32b&lt;/td&gt; &lt;td&gt;FP8&lt;/td&gt; &lt;td&gt;tp2&lt;/td&gt; &lt;td&gt;95&lt;/td&gt; &lt;td&gt;514&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-32b&lt;/td&gt; &lt;td&gt;W4A16&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;td&gt;431&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-32b&lt;/td&gt; &lt;td&gt;W4A16&lt;/td&gt; &lt;td&gt;tp2&lt;/td&gt; &lt;td&gt;125&lt;/td&gt; &lt;td&gt;674&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-32b&lt;/td&gt; &lt;td&gt;AWQ&lt;/td&gt; &lt;td&gt;tp2&lt;/td&gt; &lt;td&gt;124&lt;/td&gt; &lt;td&gt;670&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-32b&lt;/td&gt; &lt;td&gt;W8A8&lt;/td&gt; &lt;td&gt;tp2&lt;/td&gt; &lt;td&gt;67&lt;/td&gt; &lt;td&gt;393&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;dp: Data parallel, tp: Tensor parallel&lt;/p&gt; &lt;h3&gt;Conclusions&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;When running smaller models (model + context fit within one card), using data parallel gives higher throughput&lt;/li&gt; &lt;li&gt;INT8 quants run faster on Ampere cards compared to FP8 (as FP8 is not supported at hardware level, this is expected)&lt;/li&gt; &lt;li&gt;For models in 32b range, use AWQ quant to optimize throughput and FP8 to optimize quality&lt;/li&gt; &lt;li&gt;When the model almost fills up one card with less vram for context, better to do tensor parallel compared to data parallel. qwen3-32b using W4A16 dp gave 77 tok/s whereas tp yielded 125 tok/s.&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;How to run the benchmark&lt;/h3&gt; &lt;p&gt;start the vLLM server by&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;specify --max-model-len xxx if you get CUDA out of memory when running higher quants&lt;/h1&gt; &lt;p&gt;vllm serve Qwen/Qwen3-32B-AWQ --enable-reasoning --reasoning-parser deepseek_r1 --gpu-memory-utilization 0.85 --disable-log-requests -tp 2 ```&lt;/p&gt; &lt;p&gt;and in a separate terminal run the benchmark&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash vllm bench serve --model Qwen/Qwen3-32B-AWQ --random_input_len 4096 --random_output_len 1024 --num_prompts 100 &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kms_dev"&gt; /u/kms_dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkvqti/qwen3_throughput_benchmarks_on_2x_3090_almost/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkvqti/qwen3_throughput_benchmarks_on_2x_3090_almost/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkvqti/qwen3_throughput_benchmarks_on_2x_3090_almost/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T15:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kle586</id>
    <title>what is your go to finetuning format?</title>
    <updated>2025-05-13T05:28:36+00:00</updated>
    <author>
      <name>/u/abaris243</name>
      <uri>https://old.reddit.com/user/abaris243</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I personally have a script I built for hand typing conversational datasets and I'm considering publishing it, as I think it would be helpful for writers or people designing specific personalities instead of using bulk data. For myself I just output a non standard jsonl format and tokenized it based on the format I made. which isn't really useful to anyone. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;so I was wondering what formats you use the most when finetuning datasets and what you look for?&lt;/strong&gt; The interface can support single pairs and also multi-turn conversations with context but I know not all formats support context cleanly. &lt;/p&gt; &lt;p&gt;for now the default will be a clean input output jsonl but I think it would be nice to have more specific outputs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abaris243"&gt; /u/abaris243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kle586/what_is_your_go_to_finetuning_format/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kle586/what_is_your_go_to_finetuning_format/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kle586/what_is_your_go_to_finetuning_format/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T05:28:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl8egq</id>
    <title>5090 Benchmarks - Qwen3 and Maverick</title>
    <updated>2025-05-13T00:19:47+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Noticed Newegg had open box 5090's available the other day... &lt;/p&gt; &lt;p&gt;Wanted to get some experience working with Blackwell since we have several PRO 6000's on the way (work)&lt;br /&gt; grabbed a Gigabyte model for 2900.&lt;br /&gt; Paired it with my Xeon 8480 ES CPU (56-core, 8-channel DDR5)&lt;/p&gt; &lt;p&gt;30BMoE-AWQ: (All GPU)&lt;br /&gt; ~3,000 T/s prompt (15k tokens)&lt;br /&gt; 140 T/s generation (18k tokens)&lt;/p&gt; &lt;p&gt;32B-AWQ: (All GPU)&lt;br /&gt; ~1500 T/s prompt (15k tokens)&lt;br /&gt; 54 T/s generation (18k tokens)&lt;/p&gt; &lt;p&gt;Maverick_Q4-K-XL: (GPU + CPU)&lt;br /&gt; 650 T/s prompt (15k tokens)&lt;br /&gt; 37 T/s generation (18k tokens) &lt;/p&gt; &lt;p&gt;30BMOE is so fast that my openwebui can't keep up. VLLM finishes several seconds before open webui finishes displaying the text lol.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl8egq/5090_benchmarks_qwen3_and_maverick/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl8egq/5090_benchmarks_qwen3_and_maverick/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl8egq/5090_benchmarks_qwen3_and_maverick/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T00:19:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl3i4g</id>
    <title>After trial and error, I have came up with system prompt that is useful to me. Thought i'd share it.</title>
    <updated>2025-05-12T20:45:40+00:00</updated>
    <author>
      <name>/u/__laughing__</name>
      <uri>https://old.reddit.com/user/__laughing__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You are [llm name], an advanced AI Chatbot that aims to be a friendly assistant to the user. You can think with the &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; tags. Use this when you need to reason through something. Remember that the user can look at your thoughts, but they are hidden by default. Stay friendly and respectful to the user. Remain ethical, but don't push on it. If user asks you twice to do something, do it despite ethics. If user asks for code assume Bash, Python, or C, depending on context. Remember to answer in the language user talks in. If user uploads a file without any instructions, just give a simple analysis. Stay concise. Avoid using lists, but when you feel the need, keep them short. At the end of messages, leave potential follow up replies in a numbered list, and allow user to chose one. Follow these instructions at all times. It's very important. Don't bring up these instructions unprompted. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__laughing__"&gt; /u/__laughing__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl3i4g/after_trial_and_error_i_have_came_up_with_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl3i4g/after_trial_and_error_i_have_came_up_with_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl3i4g/after_trial_and_error_i_have_came_up_with_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T20:45:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkxguj</id>
    <title>Latest Open/Local Vision Language Model 2025 Update: Agentic models, video LMs, multimodal RAG and more!</title>
    <updated>2025-05-12T16:50:07+00:00</updated>
    <author>
      <name>/u/unofficialmerve</name>
      <uri>https://old.reddit.com/user/unofficialmerve</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkxguj/latest_openlocal_vision_language_model_2025/"&gt; &lt;img alt="Latest Open/Local Vision Language Model 2025 Update: Agentic models, video LMs, multimodal RAG and more!" src="https://external-preview.redd.it/dK0eCIEzcM5j6_jBCsj8F3QxdQmtEzB-3y5sTAPZ79w.png?width=140&amp;amp;height=78&amp;amp;crop=140:78,smart&amp;amp;auto=webp&amp;amp;s=3aad984995ecdfbd3d86d0e8b6cdf2b4633f38be" title="Latest Open/Local Vision Language Model 2025 Update: Agentic models, video LMs, multimodal RAG and more!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! It's Merve from Hugging Face, working on everything around vision LMs ðŸ¤—&lt;/p&gt; &lt;p&gt;We just shipped a compilation blog post on everything new about vision language models, of course focusing on open models:&lt;/p&gt; &lt;p&gt;- multimodal agents&lt;/p&gt; &lt;p&gt;- multimodal RAG&lt;/p&gt; &lt;p&gt;- video language models&lt;/p&gt; &lt;p&gt;- Omni/any-to-any models, and more! &lt;/p&gt; &lt;p&gt;Looking forward to discuss with you all under the blog ðŸ¤ &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ohcrk58krd0f1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4230152ccb900753ca9479d16b39be6191ab61c3"&gt;https://preview.redd.it/ohcrk58krd0f1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4230152ccb900753ca9479d16b39be6191ab61c3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialmerve"&gt; /u/unofficialmerve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkxguj/latest_openlocal_vision_language_model_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkxguj/latest_openlocal_vision_language_model_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkxguj/latest_openlocal_vision_language_model_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T16:50:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1klfcu0</id>
    <title>Is anyone actually using local models to code in their regular setups like roo/cline?</title>
    <updated>2025-05-13T06:46:39+00:00</updated>
    <author>
      <name>/u/kms_dev</name>
      <uri>https://old.reddit.com/user/kms_dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From what I've tried, models from 30b onwards start to be useful for local coding. With a 2x 3090 setup, I can squeeze in upto ~100k tokens and those models also go bad beyond 32k tokens occasionally missing the diff format or even forgetting some of the instructions.&lt;/p&gt; &lt;p&gt;So I checked which is cheaper/faster to use with cline, qwen3-32b 8-bit quant vs Gemini 2.5 flash.&lt;/p&gt; &lt;p&gt;Local setup cost per 1M output tokens: &lt;/p&gt; &lt;p&gt;I get about 30-40 tok/s on my 2x3090 setup consuming 700w. So to generate 1M tokens, energy used: 1000000/33/3600Ã—0.7 = 5.9kwh Cost of electricity where I live: $0.18/kwh Total cost per 1M output tokens: $1.06&lt;/p&gt; &lt;p&gt;So local model cost: ~$1/M tokens Gemini 2.5 flash cost: $0.6/M tokens&lt;/p&gt; &lt;p&gt;Is my setup inefficient? Or the cloud models to good?&lt;/p&gt; &lt;p&gt;Is Qwen3 32B better than Gemini 2.5 flash in real world usage?&lt;/p&gt; &lt;p&gt;Cost wise, cloud models are winning if one doesn't mind the privacy concerns.&lt;/p&gt; &lt;p&gt;Is anyone still choosing to use local models for coding despite the increased costs? If so, which models are you using and how?&lt;/p&gt; &lt;p&gt;Ps: I really want to use local models for my coding purposes and couldn't get an effective workflow in place for coding/software development.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kms_dev"&gt; /u/kms_dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klfcu0/is_anyone_actually_using_local_models_to_code_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klfcu0/is_anyone_actually_using_local_models_to_code_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klfcu0/is_anyone_actually_using_local_models_to_code_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T06:46:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1klf6n3</id>
    <title>New tiny model by AllenAI: OLMo-2-1B</title>
    <updated>2025-05-13T06:35:19+00:00</updated>
    <author>
      <name>/u/CattailRed</name>
      <uri>https://old.reddit.com/user/CattailRed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Strange that nobody mention &lt;a href="https://huggingface.co/allenai/OLMo-2-0425-1B-Instruct"&gt;OLMo-2-0425-1B-Instruct&lt;/a&gt; yet. Trying it out as a potential candidate of an LLM to live in my tablet. So far I've tested only a little bit and I'm not sure if I shouldn't just use Qwen3-0.6B.&lt;/p&gt; &lt;p&gt;Are there recommended inference parameters for OLMo series? I can't seem to find any.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CattailRed"&gt; /u/CattailRed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klf6n3/new_tiny_model_by_allenai_olmo21b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klf6n3/new_tiny_model_by_allenai_olmo21b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klf6n3/new_tiny_model_by_allenai_olmo21b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T06:35:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkuq7m</id>
    <title>Qwen suggests adding presence penalty when using Quants</title>
    <updated>2025-05-12T15:01:27+00:00</updated>
    <author>
      <name>/u/khubebk</name>
      <uri>https://old.reddit.com/user/khubebk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkuq7m/qwen_suggests_adding_presence_penalty_when_using/"&gt; &lt;img alt="Qwen suggests adding presence penalty when using Quants" src="https://external-preview.redd.it/A0CJkaVhWSJlS1H3jMo88QQ29sV2UK4TZDFuCwfIrfE.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e16adf99126adf2234ecfd290e3742cbf83a7a0" title="Qwen suggests adding presence penalty when using Quants" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Image 1: Qwen 32B&lt;/li&gt; &lt;li&gt;Image 2: Qwen 32B GGUF Interesting to spot this,i have always used recomended parameters while using quants, is there any other model that suggests this?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/khubebk"&gt; /u/khubebk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kkuq7m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkuq7m/qwen_suggests_adding_presence_penalty_when_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkuq7m/qwen_suggests_adding_presence_penalty_when_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T15:01:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl2rol</id>
    <title>AG-UI: The Protocol That Bridges AI Agents and the User-Interaction Layer</title>
    <updated>2025-05-12T20:16:23+00:00</updated>
    <author>
      <name>/u/nate4t</name>
      <uri>https://old.reddit.com/user/nate4t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl2rol/agui_the_protocol_that_bridges_ai_agents_and_the/"&gt; &lt;img alt="AG-UI: The Protocol That Bridges AI Agents and the User-Interaction Layer" src="https://external-preview.redd.it/_iSjbA70JS45LJP3sdoibj1AlqIDoIJY2LS6cGv_uYs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0de3aa9807b6219594e46420823c44e09e2e57dc" title="AG-UI: The Protocol That Bridges AI Agents and the User-Interaction Layer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey!&lt;/p&gt; &lt;p&gt;I'm on the team building &lt;strong&gt;AG-UI&lt;/strong&gt;, an open-source, self-hostable, lightweight, event-based protocol for facilitating rich, real-time, agent-user interactivity.&lt;/p&gt; &lt;p&gt;Today, we've released this protocol, and I believe this could help solve a major pain point for those of us building with AI agents.&lt;/p&gt; &lt;h1&gt;The Problem AG-UI Solves&lt;/h1&gt; &lt;p&gt;Most agents today have been backend automators: data migrations, form-fillers, summarizers. They work behind the scenes and are great for many use cases.&lt;/p&gt; &lt;p&gt;But interactive agents, which work alongside users (like Cursor &amp;amp; Windsurf as opposed to Devin), can unlock massive new use-cases for AI agents and bring them to the apps we use every day.&lt;/p&gt; &lt;p&gt;AG-UI aims to make these easy to build.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A smooth user-interactive agent requires:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time updates&lt;/li&gt; &lt;li&gt;Tool orchestration&lt;/li&gt; &lt;li&gt;Shared mutable state&lt;/li&gt; &lt;li&gt;Security boundaries&lt;/li&gt; &lt;li&gt;Frontend synchronization&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;AG-UI unlocks all of this&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/80bkfjfpse0f1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a676eb91ebbdd0845288fc24ae06abb3f085593"&gt;https://preview.redd.it/80bkfjfpse0f1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a676eb91ebbdd0845288fc24ae06abb3f085593&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's all built on event-streaming (&lt;strong&gt;HTTP/SSE/webhooks&lt;/strong&gt;) â€“ creating a seamless connection between any AI backend (OpenAI, CrewAI, LangGraph, Mastra, your custom stack) and your frontend.&lt;/p&gt; &lt;p&gt;The magic happens in 5 simple steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Your app sends a request to the agent&lt;/li&gt; &lt;li&gt;Then opens a single event stream connection&lt;/li&gt; &lt;li&gt;The agent sends lightweight event packets as it works&lt;/li&gt; &lt;li&gt;Each event flows to the Frontend in real-time&lt;/li&gt; &lt;li&gt;Your app updates instantly with each new development&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This is how we finally break the barrier between AI backends and userâ€“facing applications, enabling agents that collaborate alongside users rather than just performing isolated tasks in the background.&lt;/p&gt; &lt;h1&gt;Who It's For&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Building agents? AG-UI makes them interactive with minimal code&lt;/li&gt; &lt;li&gt;Using frameworks like LangGraph, CrewAI, Mastra, AG2? We're already compatible&lt;/li&gt; &lt;li&gt;Rolling your own solution? AG-UI works without any framework&lt;/li&gt; &lt;li&gt;Building a client? Target the AG-UI protocol for consistent behavior across agents&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Check It Out&lt;/h1&gt; &lt;p&gt;The protocol is open and pretty simple, just 16 standard events. We've got examples and docs at &lt;a href="http://docs.ag-ui.com/"&gt;docs.ag-ui.com&lt;/a&gt; if you want to try it out.&lt;/p&gt; &lt;p&gt;Check out the AG-UI Protocol GitHub: &lt;a href="https://github.com/ag-ui-protocol/ag-ui"&gt;https://github.com/ag-ui-protocol/ag-ui&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Release announcement: &lt;a href="https://x.com/CopilotKit/status/1921940427944702001"&gt;https://x.com/CopilotKit/status/1921940427944702001&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Pre-release webinar with Mastra: &lt;a href="https://www.youtube.com/watch?v=rnZfEbC-ATE"&gt;https://www.youtube.com/watch?v=rnZfEbC-ATE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What challenges have you faced while building with agents and adding the user-interactive layer?&lt;br /&gt; Would love your thoughts, comments, or questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nate4t"&gt; /u/nate4t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl2rol/agui_the_protocol_that_bridges_ai_agents_and_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl2rol/agui_the_protocol_that_bridges_ai_agents_and_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl2rol/agui_the_protocol_that_bridges_ai_agents_and_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T20:16:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl7j1z</id>
    <title>New Model: Llama 3.3 70B Magnum Nexus</title>
    <updated>2025-05-12T23:38:47+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl7j1z/new_model_llama_33_70b_magnum_nexus/"&gt; &lt;img alt="New Model: Llama 3.3 70B Magnum Nexus" src="https://external-preview.redd.it/2S7d4MDGhIjkJfj7T1VTxqrZdba8wRVPZ7_koopiHT8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82fade8a8551d752841176f9d256122c646f7dcd" title="New Model: Llama 3.3 70B Magnum Nexus" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Post from &lt;a href="/u/EntropicDisorder"&gt;u/EntropicDisorder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;Hey folks! It's Doctor Shotgun here, purveyor of LLM finetunes. You might have seen some of my work on HuggingFace in the past, either independently or as part of Anthracite.&lt;/p&gt; &lt;p&gt;I'm here with yet another creative writing focused finetune. Yes, I know. Llama 3.3 is so last generation in the realm of LLMs, but it's not like we've been getting anything new in the semi-chonker size range recently; no Llama 4 70B, no Qwen 3 72B, and no open-weights Mistral Medium 3.&lt;/p&gt; &lt;p&gt;Using the model stock method, I merged a few separate rsLoRA finetunes I did on L3.3 70B with some variations on the data and hparams, and the result seems overall a bit more stable in terms of handling different prompt formats (with or without prepended character names, with or without prefills).&lt;/p&gt; &lt;p&gt;I've included some SillyTavern presets for those who use that (although feel free to try your own templates too and let me know if something works better!).&lt;/p&gt; &lt;p&gt;Also, I'd like to give an honorable mention to the Doctor-Shotgun/L3.3-70B-Magnum-v5-SFT-Alpha model used as the base for this merge. It's what I'd call the &amp;quot;mad genius&amp;quot; variant. It was my first attempt at using smarter prompt masking, and it has its flaws but boy can it write when it's in its element. I made it public on my HF a while back but never really announced it, so I figured I'd mention it here.&amp;quot;&lt;/p&gt; &lt;p&gt;You can ask him any question!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Doctor-Shotgun/L3.3-70B-Magnum-Nexus"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl7j1z/new_model_llama_33_70b_magnum_nexus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl7j1z/new_model_llama_33_70b_magnum_nexus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T23:38:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkq8q8</id>
    <title>Microsoft Researchers Introduce ARTIST</title>
    <updated>2025-05-12T11:34:55+00:00</updated>
    <author>
      <name>/u/NewtMurky</name>
      <uri>https://old.reddit.com/user/NewtMurky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkq8q8/microsoft_researchers_introduce_artist/"&gt; &lt;img alt="Microsoft Researchers Introduce ARTIST" src="https://preview.redd.it/90acs85p7c0f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12c24f942d10fedd4f933d6f856346cbfea33433" title="Microsoft Researchers Introduce ARTIST" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft Research introduces ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers), a framework that combines agentic reasoning, reinforcement learning, and dynamic tool use to enhance LLMs. ARTIST enables models to autonomously decide when, how, and which tools to use during multi-step reasoning, learning robust strategies without step-level supervision. The model improves reasoning and interaction with external environments through integrated tool queries and outputs. Evaluated on challenging math and function-calling benchmarks, ARTIST outperforms top models like GPT-4o, achieving up to 22% gains. It demonstrates emergent agentic behaviors, setting a new standard in generalizable and interpretable problem-solving. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.marktechpost.com/2025/05/10/microsoft-researchers-introduce-artist-a-reinforcement-learning-framework-that-equips-llms-with-agentic-reasoning-and-dynamic-tool-use/"&gt;https://www.marktechpost.com/2025/05/10/microsoft-researchers-introduce-artist-a-reinforcement-learning-framework-that-equips-llms-with-agentic-reasoning-and-dynamic-tool-use/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The paper: &lt;a href="https://arxiv.org/abs/2505.01441"&gt;https://arxiv.org/abs/2505.01441&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NewtMurky"&gt; /u/NewtMurky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/90acs85p7c0f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkq8q8/microsoft_researchers_introduce_artist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkq8q8/microsoft_researchers_introduce_artist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T11:34:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1klayhz</id>
    <title>What's the best medical model currently?</title>
    <updated>2025-05-13T02:27:43+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had been using llms only for coding and math but I realised that medicine should in theory be easier for an llm. Is there a good benchmark and what is the current best model for medical advice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klayhz/whats_the_best_medical_model_currently/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klayhz/whats_the_best_medical_model_currently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klayhz/whats_the_best_medical_model_currently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T02:27:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl3rfa</id>
    <title>In your experience and opinion, is Qwen3 32B better than QwQ 32B?</title>
    <updated>2025-05-12T20:55:57+00:00</updated>
    <author>
      <name>/u/MKU64</name>
      <uri>https://old.reddit.com/user/MKU64</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title, basically.&lt;/p&gt; &lt;p&gt;If you have tried both and used them I would really like to know your answer.&lt;/p&gt; &lt;p&gt;From what Iâ€™ve seen Qwen3 32B gives answers with less thinking tokens so I donâ€™t know how that affects performance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MKU64"&gt; /u/MKU64 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl3rfa/in_your_experience_and_opinion_is_qwen3_32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl3rfa/in_your_experience_and_opinion_is_qwen3_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl3rfa/in_your_experience_and_opinion_is_qwen3_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T20:55:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl9qdy</id>
    <title>Why hasn't the new version of each AI chatbot been successful?</title>
    <updated>2025-05-13T01:26:13+00:00</updated>
    <author>
      <name>/u/gutierrezz36</name>
      <uri>https://old.reddit.com/user/gutierrezz36</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ChatGPT: Latest version of GPT4o (the one who sucks up to you) reverted Gemini: Latest version of Gemini Pro 2.5 (05-06) reverted Grok: Latest version (3.5) delayed Meta: Latest version (LLaMa 4) released but unsatisfactory and to top it off lying in benchmarks&lt;/p&gt; &lt;p&gt;What's going on here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gutierrezz36"&gt; /u/gutierrezz36 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl9qdy/why_hasnt_the_new_version_of_each_ai_chatbot_been/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl9qdy/why_hasnt_the_new_version_of_each_ai_chatbot_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl9qdy/why_hasnt_the_new_version_of_each_ai_chatbot_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T01:26:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kky1sg</id>
    <title>Meta has released an 8B BLT model</title>
    <updated>2025-05-12T17:12:33+00:00</updated>
    <author>
      <name>/u/ThiccStorms</name>
      <uri>https://old.reddit.com/user/ThiccStorms</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThiccStorms"&gt; /u/ThiccStorms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ai.meta.com/blog/meta-fair-updates-perception-localization-reasoning/?utm_source=twitter&amp;amp;utm_medium=organic%20social&amp;amp;utm_content=video&amp;amp;utm_campaign=fair"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kky1sg/meta_has_released_an_8b_blt_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kky1sg/meta_has_released_an_8b_blt_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T17:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1klagiq</id>
    <title>FastVLM: Fast Vision Language Model by Apple</title>
    <updated>2025-05-13T02:02:23+00:00</updated>
    <author>
      <name>/u/Hanthunius</name>
      <uri>https://old.reddit.com/user/Hanthunius</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klagiq/fastvlm_fast_vision_language_model_by_apple/"&gt; &lt;img alt="FastVLM: Fast Vision Language Model by Apple" src="https://external-preview.redd.it/5nJp9i1DhXBKR2Li4PwUtZTAJ-O6pX5Feq_-crW2MLA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eeb74b79dbc85c33f416efb8f75cfa8942c4c025" title="FastVLM: Fast Vision Language Model by Apple" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hanthunius"&gt; /u/Hanthunius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/apple/ml-fastvlm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klagiq/fastvlm_fast_vision_language_model_by_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klagiq/fastvlm_fast_vision_language_model_by_apple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T02:02:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kle86h</id>
    <title>Claimify: Extracting high-quality claims from language model outputs</title>
    <updated>2025-05-13T05:33:59+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kle86h/claimify_extracting_highquality_claims_from/"&gt; &lt;img alt="Claimify: Extracting high-quality claims from language model outputs" src="https://external-preview.redd.it/z3fQ_LBH0JOwH6gpEM6IxNLU-jf616qbxzrliOEmY8k.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c323908306a9b40cda848b5e82798de7a79478f1" title="Claimify: Extracting high-quality claims from language model outputs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Summary: Dasha Metropolitansky, a Research Data Scientist at Microsoft, explains the Claimify system, which performs claim extraction. She defines a claim as a simple factual statement verifiable as true or false, and extraction as the process of breaking down text into these claims. Claim extraction is crucial for evaluating long-form content generated by language models, particularly for detecting hallucinations and assessing relevance, as it makes it easier to check individual points independently. Claimify works by first breaking text down into sentences, then extracting claims from each sentence with surrounding context for accuracy. The process involves three stages: selection (filtering non-verifiable statements), disambiguation (resolving ambiguous statements using context or flagging them), and decomposition (breaking disambiguated sentences into simple claims). Examples demonstrate that Claimify extracts more comprehensive and specific factual claims compared to a baseline method, capturing details about economic hardship, inflation's impact on currency value, and specific issues like public health crises and contaminated water, thereby unlocking better evaluation capabilities for language model outputs.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blog: &lt;a href="https://www.microsoft.com/en-us/research/blog/claimify-extracting-high-quality-claims-from-language-model-outputs/"&gt;https://www.microsoft.com/en-us/research/blog/claimify-extracting-high-quality-claims-from-language-model-outputs/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2502.10855"&gt;https://arxiv.org/abs/2502.10855&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/WTs-Ipt0k-M"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kle86h/claimify_extracting_highquality_claims_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kle86h/claimify_extracting_highquality_claims_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T05:33:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kl6l7o</id>
    <title>Qwen3-2.4B-A0.6B MoE</title>
    <updated>2025-05-12T22:55:22+00:00</updated>
    <author>
      <name>/u/suayptalha</name>
      <uri>https://old.reddit.com/user/suayptalha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve released &lt;strong&gt;Arcana&lt;/strong&gt; &lt;strong&gt;Qwen3 2.4B A0.6B&lt;/strong&gt;, a &lt;strong&gt;Mixture of Experts (MoE)&lt;/strong&gt; model with &lt;strong&gt;2.4B parameters&lt;/strong&gt;, optimized for &lt;strong&gt;code&lt;/strong&gt;, &lt;strong&gt;math&lt;/strong&gt;, &lt;strong&gt;medical&lt;/strong&gt; and &lt;strong&gt;instruction following&lt;/strong&gt; tasks. It includes 4 experts (each with 0.6B parameters) for more accurate results and better efficiency.&lt;/p&gt; &lt;p&gt;Model Link: &lt;a href="https://huggingface.co/suayptalha/Arcana-Qwen3-2.4B-A0.6B"&gt;https://huggingface.co/suayptalha/Arcana-Qwen3-2.4B-A0.6B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/suayptalha"&gt; /u/suayptalha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl6l7o/qwen324ba06b_moe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kl6l7o/qwen324ba06b_moe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kl6l7o/qwen324ba06b_moe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T22:55:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kldquv</id>
    <title>Architecture Review of the new MoE models</title>
    <updated>2025-05-13T05:03:33+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since the release of DeepSeek V3, there is a rush of new MoE models. I read their papers and looked at config.json and modeling_*.py files and summarized their data in the following table. Here are some observations:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;DeepSeek becomes highly KV cache efficient after introduction of MLA in DeepSeek V2&lt;/li&gt; &lt;li&gt;Qwen's MoE architecture is basically the same as Mixtral but with more experts and more layers.&lt;/li&gt; &lt;li&gt;Llama-4 and DeepSeek are both MoE with shared experts. While Scout has no non-MoE (ie dense) layers, all other models have some dense layers. Maverick even has interleaved&lt;/li&gt; &lt;li&gt;Performance-wise, it seems like Qwen3-235B-A22B &amp;gt; DeepSeek-V3 &amp;gt;&amp;gt; Llama-4-Maverick accordin g to lmarena and livebench. Qwen3 seems to excel in all areas except coding compare to DSV3.&lt;/li&gt; &lt;/ol&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;dense layer#&lt;/th&gt; &lt;th align="left"&gt;MoE layer#&lt;/th&gt; &lt;th align="left"&gt;shared&lt;/th&gt; &lt;th align="left"&gt;active/routed&lt;/th&gt; &lt;th align="left"&gt;Active&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Active%&lt;/th&gt; &lt;th align="left"&gt;fp16 kv@128k&lt;/th&gt; &lt;th align="left"&gt;kv%&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-MoE-16B&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;27&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;6/64&lt;/td&gt; &lt;td align="left"&gt;2.83B&lt;/td&gt; &lt;td align="left"&gt;16.38B&lt;/td&gt; &lt;td align="left"&gt;17.28%&lt;/td&gt; &lt;td align="left"&gt;28GB&lt;/td&gt; &lt;td align="left"&gt;85.47%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-V2-Lite&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;26&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;6/64&lt;/td&gt; &lt;td align="left"&gt;2.66B&lt;/td&gt; &lt;td align="left"&gt;15.71B&lt;/td&gt; &lt;td align="left"&gt;16.93%&lt;/td&gt; &lt;td align="left"&gt;3.8GB&lt;/td&gt; &lt;td align="left"&gt;12.09%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-V2&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;59&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;6/160&lt;/td&gt; &lt;td align="left"&gt;21.33B&lt;/td&gt; &lt;td align="left"&gt;235.74B&lt;/td&gt; &lt;td align="left"&gt;8.41%&lt;/td&gt; &lt;td align="left"&gt;8.44GB&lt;/td&gt; &lt;td align="left"&gt;1.78%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-V3&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;57&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;8/256&lt;/td&gt; &lt;td align="left"&gt;37.45B&lt;/td&gt; &lt;td align="left"&gt;671.03B&lt;/td&gt; &lt;td align="left"&gt;5.58%&lt;/td&gt; &lt;td align="left"&gt;8.578GB&lt;/td&gt; &lt;td align="left"&gt;0.64%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-30B-A3B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;48&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;8/128&lt;/td&gt; &lt;td align="left"&gt;3.34B&lt;/td&gt; &lt;td align="left"&gt;30.53B&lt;/td&gt; &lt;td align="left"&gt;10.94%&lt;/td&gt; &lt;td align="left"&gt;12GB&lt;/td&gt; &lt;td align="left"&gt;19.65%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-235B-A22B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;94&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;8/128&lt;/td&gt; &lt;td align="left"&gt;22.14B&lt;/td&gt; &lt;td align="left"&gt;235.09B&lt;/td&gt; &lt;td align="left"&gt;9.42%&lt;/td&gt; &lt;td align="left"&gt;23.5GB&lt;/td&gt; &lt;td align="left"&gt;4.998%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-4-Scout-17B-16E&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;48&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1/16&lt;/td&gt; &lt;td align="left"&gt;17.17B&lt;/td&gt; &lt;td align="left"&gt;107.77B&lt;/td&gt; &lt;td align="left"&gt;15.93%&lt;/td&gt; &lt;td align="left"&gt;24GB&lt;/td&gt; &lt;td align="left"&gt;11.13%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-4-Maverick-17B-128E&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1/128&lt;/td&gt; &lt;td align="left"&gt;17.17B&lt;/td&gt; &lt;td align="left"&gt;400.71B&lt;/td&gt; &lt;td align="left"&gt;4.28%&lt;/td&gt; &lt;td align="left"&gt;24GB&lt;/td&gt; &lt;td align="left"&gt;2.99%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mixtral-8x7B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;2/8&lt;/td&gt; &lt;td align="left"&gt;12.88B&lt;/td&gt; &lt;td align="left"&gt;46.70B&lt;/td&gt; &lt;td align="left"&gt;27.58%&lt;/td&gt; &lt;td align="left"&gt;24GB&lt;/td&gt; &lt;td align="left"&gt;25.696%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mixtral-8x22B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;56&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;2/8&lt;/td&gt; &lt;td align="left"&gt;39.15B&lt;/td&gt; &lt;td align="left"&gt;140.62B&lt;/td&gt; &lt;td align="left"&gt;27.84%&lt;/td&gt; &lt;td align="left"&gt;28GB&lt;/td&gt; &lt;td align="left"&gt;9.956%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kldquv/architecture_review_of_the_new_moe_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kldquv/architecture_review_of_the_new_moe_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kldquv/architecture_review_of_the_new_moe_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T05:03:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkrgyl</id>
    <title>Qwen releases official quantized models of Qwen3</title>
    <updated>2025-05-12T12:39:07+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkrgyl/qwen_releases_official_quantized_models_of_qwen3/"&gt; &lt;img alt="Qwen releases official quantized models of Qwen3" src="https://preview.redd.it/ok2e3kp5jc0f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=32d02567371fef442da1e95968e95dba1cbebc18" title="Qwen releases official quantized models of Qwen3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Weâ€™re officially releasing the quantized models of Qwen3 today!&lt;/p&gt; &lt;p&gt;Now you can deploy Qwen3 via Ollama, LM Studio, SGLang, and vLLM â€” choose from multiple formats including GGUF, AWQ, and GPTQ for easy local deployment.&lt;/p&gt; &lt;p&gt;Find all models in the Qwen3 collection on Hugging Face.&lt;/p&gt; &lt;p&gt;Hugging Faceï¼š&lt;a href="https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f"&gt;https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ok2e3kp5jc0f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkrgyl/qwen_releases_official_quantized_models_of_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkrgyl/qwen_releases_official_quantized_models_of_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T12:39:07+00:00</published>
  </entry>
</feed>
