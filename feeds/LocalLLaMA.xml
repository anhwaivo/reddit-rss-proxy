<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-04T14:06:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n89myt</id>
    <title>I'm pretty sure I released the first iOS store app that runs Qwen 3 models locally on your iPhone.</title>
    <updated>2025-09-04T13:07:02+00:00</updated>
    <author>
      <name>/u/sqli</name>
      <uri>https://old.reddit.com/user/sqli</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89myt/im_pretty_sure_i_released_the_first_ios_store_app/"&gt; &lt;img alt="I'm pretty sure I released the first iOS store app that runs Qwen 3 models locally on your iPhone." src="https://external-preview.redd.it/t315KHJqJsyAjUEE9WR6mtsGlKCX1QGLRs5qhk30A2s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af5d2ca6658c8d47ec4d7e55a9aca160a78561de" title="I'm pretty sure I released the first iOS store app that runs Qwen 3 models locally on your iPhone." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been so busy with other projects that I forgot to post it here.&lt;/p&gt; &lt;p&gt;It runs Qwen 3 4B locally, on-device. The only network requests it makes are to download the initial models on-demand, so like, it works in airplane mode. &lt;/p&gt; &lt;p&gt;I hardcoded &lt;a href="https://huggingface.co/dougiefresh/jade_qwen3_4b"&gt;my finetune&lt;/a&gt; of Qwen 3 4B because it's specifically trained on Apple product dev stuff and math (oh yeah, the app renders LaTex and source code with highlighting).&lt;/p&gt; &lt;p&gt;The base Qwen 3 4B model is also available in the app.&lt;/p&gt; &lt;p&gt;I collect no data because frankly I don't care. I want people to be able to receive augmented educations for free without having to worry about being watched or tracked. No account necessary, the app will always remain free and &lt;a href="https://github.com/graves/Jade"&gt;open source&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It's based on the hard work of the team maintaining &lt;a href="https://github.com/ml-explore/mlx-swift-examples"&gt;mlx-swift-examples&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I'd love your feedback. The mlx APIs are new so there's definitely improvements to made and kinks to work out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sqli"&gt; /u/sqli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://apps.apple.com/bz/app/awful-jade/id6746356585?platform=iphone"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89myt/im_pretty_sure_i_released_the_first_ios_store_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n89myt/im_pretty_sure_i_released_the_first_ios_store_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T13:07:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1n83z4r</id>
    <title>Best Vision/OCR Models for describing and extracting text for images in PDFs</title>
    <updated>2025-09-04T07:50:02+00:00</updated>
    <author>
      <name>/u/Top-Fig1571</name>
      <uri>https://old.reddit.com/user/Top-Fig1571</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;for a typical RAG Use Case I want to bring in multimodality and for images and tables I want to use a VLM to first extract the contents of the Image and then also describing or summarizing the image/table. &lt;/p&gt; &lt;p&gt;Currently I am using the &lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;nanonets/Nanonets-OCR-s&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;model. However I am curious of your experiences what have worked best for you. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Top-Fig1571"&gt; /u/Top-Fig1571 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n83z4r/best_visionocr_models_for_describing_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n83z4r/best_visionocr_models_for_describing_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n83z4r/best_visionocr_models_for_describing_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T07:50:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1n88sqb</id>
    <title>Deploying 1.4KW GPUs (B300) what's the biggest bottleneck you've seen power delivery or cooling?</title>
    <updated>2025-09-04T12:29:53+00:00</updated>
    <author>
      <name>/u/DingoOutrageous7124</name>
      <uri>https://old.reddit.com/user/DingoOutrageous7124</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most people see a GPU cluster and think about FLOPS. What’s been killing us lately is the supporting infrastructure.&lt;/p&gt; &lt;p&gt;Each B300 pulls ~1,400W. That’s 40+ W/cm² of heat in a small footprint. Air cooling stops being viable past ~800W, so at this density you need DLC (direct liquid cooling).&lt;/p&gt; &lt;p&gt;Power isn’t easier a single rack can hit 25kW+. That means 240V circuits, smart PDUs, and hundreds of supercaps just to keep power stable.&lt;/p&gt; &lt;p&gt;And the dumbest failure mode? A $200 thermal sensor installed wrong can kill a $2M deployment.&lt;/p&gt; &lt;p&gt;It feels like the semiconductor roadmap has outpaced the “boring” stuff power and cooling engineering.&lt;/p&gt; &lt;p&gt;For those who’ve deployed or worked with high-density GPU clusters (1kW+ per device), what’s been the hardest to scale reliably:&lt;/p&gt; &lt;p&gt;Power distribution and transient handling?&lt;/p&gt; &lt;p&gt;Cooling (DLC loops, CDU redundancy, facility water integration)?&lt;/p&gt; &lt;p&gt;Or something else entirely (sensoring, monitoring, failure detection)?&lt;/p&gt; &lt;p&gt;Would love to hear real-world experiences especially what people overlooked on their first large-scale deployment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DingoOutrageous7124"&gt; /u/DingoOutrageous7124 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n88sqb/deploying_14kw_gpus_b300_whats_the_biggest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n88sqb/deploying_14kw_gpus_b300_whats_the_biggest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n88sqb/deploying_14kw_gpus_b300_whats_the_biggest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T12:29:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7xgm5</id>
    <title>How to run Qwen3 0.6B at 8.4 tok/sec on 2 x 5090s</title>
    <updated>2025-09-04T01:47:21+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7xgm5/how_to_run_qwen3_06b_at_84_toksec_on_2_x_5090s/"&gt; &lt;img alt="How to run Qwen3 0.6B at 8.4 tok/sec on 2 x 5090s" src="https://a.thumbs.redditmedia.com/GfszssUYE2rt8pcGlvaw8ZJeriVVINMv1TxqRk17Y84.jpg" title="How to run Qwen3 0.6B at 8.4 tok/sec on 2 x 5090s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Completely useless but thought I would share :D)&lt;/p&gt; &lt;p&gt;This was just a fun experiment to see how fast I could run LLMs with WiFi interconnect and, well, I have to say it's quite a bit slower than I thought...&lt;/p&gt; &lt;p&gt;I set up two machines with 1x5090 each; then installed the latest vLLM on each, and also installed Ray on each of them. Then once you &lt;a href="https://docs.ray.io/en/latest/cluster/cli.html"&gt;start ray on one machine and connect to it with the other,&lt;/a&gt; you can run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm serve Qwen/Qwen3-0.6B --max-model-len 1024 --tensor-parallel-size 1 --pipeline-parallel-size 2 --host 0.0.0.0 --port 8181 --enable-reasoning --reasoning-parser deepseek_r1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Lo and behold, the mighty Qwen3 0.6B running at 8.4 t/s split across 2 5090s!!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6vwu78dhy1nf1.png?width=2136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fadb7a4ead8e56a8f3270f038608585ce8b8ed2"&gt;Open WebUI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Not only is the model bad, but also:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Runs way slower than just CPU.&lt;/li&gt; &lt;li&gt;Ray &amp;amp; vLLM need a bit of tweaking to get running correctly&lt;/li&gt; &lt;li&gt;vLLM will throw a bunch of random errors along the way ;)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7xgm5/how_to_run_qwen3_06b_at_84_toksec_on_2_x_5090s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7xgm5/how_to_run_qwen3_06b_at_84_toksec_on_2_x_5090s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7xgm5/how_to_run_qwen3_06b_at_84_toksec_on_2_x_5090s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T01:47:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7g0c2</id>
    <title>German "Who Wants to Be a Millionaire" Benchmark w/ Leading Models</title>
    <updated>2025-09-03T14:15:58+00:00</updated>
    <author>
      <name>/u/facethef</name>
      <uri>https://old.reddit.com/user/facethef</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7g0c2/german_who_wants_to_be_a_millionaire_benchmark_w/"&gt; &lt;img alt="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark w/ Leading Models" src="https://b.thumbs.redditmedia.com/-7S29tlnbmvmCdgKuNNinsiLgi6LQ83T8Ar-ZV3lCVs.jpg" title="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark w/ Leading Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First off, big thanks to &lt;a href="/u/Available_Load_5334"&gt;u/Available_Load_5334&lt;/a&gt; for creating the original German &lt;strong&gt;Wer wird Millionär?&lt;/strong&gt; Benchmark and open-sourcing it. &lt;a href="https://github.com/ikiruneo/millionaire-bench"&gt;https://github.com/ikiruneo/millionaire-bench&lt;/a&gt; &lt;/p&gt; &lt;p&gt;After speaking, we said it would be fun to run the same benchmark on a set of leading models, and that's what we did here. &lt;/p&gt; &lt;p&gt;The rules and data stayed the same, 45 rounds, each with 15 multiple-choice questions from easy to hard. One wrong answer ends the program and you keep the current winnings. No lifelines. Answers are single letters A–D. same public WWM question corpus used in the original. &lt;a href="https://github.com/GerritKainz/wer_wird_millionaer"&gt;https://github.com/GerritKainz/wer_wird_millionaer&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Questions remain in German for inference, but we included parallel English text so non-German readers can follow along. See fragen_antworten_en.json in the repo. Scripts to run many programs quickly and rebuild results from per-model outputs (millionaire-run.py, rebuild_leaderboard.py). We’ll attach a screenshot of the leaderboard instead of pasting a table here. same scoring and structure as the original, packaged for quick reruns.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/Jose-Sabater/millionaire-bench-opper"&gt;https://github.com/Jose-Sabater/millionaire-bench-opper&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Again thanks to &lt;a href="/u/Available_Load_5334"&gt;u/Available_Load_5334&lt;/a&gt; for the idea and groundwork. If you try more models or tweak settings, feel free to open a PR or drop results in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/facethef"&gt; /u/facethef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n7g0c2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7g0c2/german_who_wants_to_be_a_millionaire_benchmark_w/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7g0c2/german_who_wants_to_be_a_millionaire_benchmark_w/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T14:15:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7vqn8</id>
    <title>Thoughts on Intel Arc Pro B50 x4 = 64GB of VRAM for $1400 and 280W Power Draw?</title>
    <updated>2025-09-04T00:26:25+00:00</updated>
    <author>
      <name>/u/79215185-1feb-44c6</name>
      <uri>https://old.reddit.com/user/79215185-1feb-44c6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For new cards that is some of the best $/GB of VRAM you can get, and it's also the best VRAM/w you can get and because they're x8 cards you can run them off of a x16 splitter right? How are x16 splitters? I assume you'd need some external PCIe power.&lt;/p&gt; &lt;p&gt;Is this realistic? Does me making this thread basically prevent this card from ever be obtainable? &lt;em&gt;Am I stupid?&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/79215185-1feb-44c6"&gt; /u/79215185-1feb-44c6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7vqn8/thoughts_on_intel_arc_pro_b50_x4_64gb_of_vram_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7vqn8/thoughts_on_intel_arc_pro_b50_x4_64gb_of_vram_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7vqn8/thoughts_on_intel_arc_pro_b50_x4_64gb_of_vram_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T00:26:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n83kzo</id>
    <title>Please help, anyone here has archived the Google Colab notebook of VibeVoice ?</title>
    <updated>2025-09-04T07:23:53+00:00</updated>
    <author>
      <name>/u/CesarOverlorde</name>
      <uri>https://old.reddit.com/user/CesarOverlorde</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n83kzo/please_help_anyone_here_has_archived_the_google/"&gt; &lt;img alt="Please help, anyone here has archived the Google Colab notebook of VibeVoice ?" src="https://preview.redd.it/g585evlpn3nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f0ea9a57e461437341492da5152dbc3c10e88e47" title="Please help, anyone here has archived the Google Colab notebook of VibeVoice ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I only have a very weak laptop that can't run the model locally unfortunately. If anyone archived this notebook I would really appreciate if you can share it. Thank you in advance!&lt;/p&gt; &lt;p&gt;I tried accessing it using wayback machine but it's just white blank page.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CesarOverlorde"&gt; /u/CesarOverlorde &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g585evlpn3nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n83kzo/please_help_anyone_here_has_archived_the_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n83kzo/please_help_anyone_here_has_archived_the_google/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T07:23:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n81d1t</id>
    <title>[Level 0] Fine-tuned my first personal chatbot</title>
    <updated>2025-09-04T05:06:17+00:00</updated>
    <author>
      <name>/u/FastCommission2913</name>
      <uri>https://old.reddit.com/user/FastCommission2913</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n81d1t/level_0_finetuned_my_first_personal_chatbot/"&gt; &lt;img alt="[Level 0] Fine-tuned my first personal chatbot" src="https://external-preview.redd.it/nkhh65ujo5BznFJFojoMPaKjGuLSpPj6KGhRov-ykOg.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e2f90964c81a1de52938be6bcb08665605293f2" title="[Level 0] Fine-tuned my first personal chatbot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wrapped up my first LLM fine-tuning project and wanted to share the experience since I learned a ton. Used Unsloth + `cognitivecomputations/dolphin-2.9-llama3-8b` with around 1400 custom examples about myself, trained on Colab's free T4 GPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How I learnt:&lt;/strong&gt; I knew the basics of LoRA and QLoRA since we were never taught the practical. I am a self taught with a medical condition. Rest I followed the steps of ChatGPT.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;: Generated dataset using ChatGPT by providing it with my personal info (background, interests, projects, etc.). Formatted as simple question-answer pairs in JSONL. Used LoRA with r=16, trained for 300 steps (~20 minutes), ended with loss around 0.74.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/39hnvx6zl2nf1.png?width=2394&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0d0c1bcdd0ea06139760b8817ac64939070008c"&gt;This is what my current dataset looks like.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: Model went from generic &amp;quot;I'm an AI assistant created by...&amp;quot; to actually knowing I'm Sohaib Ahmed, ..... grad from ...., into anime (1794 watched according to my Anilist), gaming (Genshin Impact, ZZZ), and that I built InSightAI library with minimal PyPI downloads. Responses sound natural and match my personality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What worked&lt;/strong&gt;: Llama 3.1 8B base model was solid. Dataset quality mattered more than quantity. Unsloth made everything stupid fast and memory-efficient.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Issues hit&lt;/strong&gt;: Tried Mistral 7B first but got incomplete responses (&amp;quot;I am and I do&amp;quot;). Safety triggers still override on certain phrases - asking about &amp;quot;abusive language&amp;quot; makes it revert to generic safety mode instead of answering as me. Occasionally hallucinates experiences I never had when answering general knowledge questions.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Next steps&lt;/strong&gt;: &amp;quot;I don't know&amp;quot; boundary examples to fix the hallucination issue. How do I make it so that it says &amp;quot;I don't know&amp;quot; for other general purpose questions? How can I improve it further?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Level 1 (based on my idiotic knowledge): I want to learn how can I make the text summarization personalized. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Final model actually passes the &amp;quot;tell me about yourself&amp;quot; test convincingly. Pretty solid for a first attempt.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Colab notebook:&lt;/strong&gt; &lt;a href="https://colab.research.google.com/drive/1Az3gFYEKSzPouxrhvES7v5oafyhnm80v?usp=sharing"&gt;https://colab.research.google.com/drive/1Az3gFYEKSzPouxrhvES7v5oafyhnm80v?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Confusions:&lt;/strong&gt; I don't know much on hosting/ deploying a Local LLM. Following are my specs: &lt;strong&gt;MacBook Pro with Apple M4 chip, 16GB RAM, and an Apple M4 GPU with 10 cores&lt;/strong&gt;. I only know that I can run any LLM &amp;lt; 16GB but don't know any good yet to do the tool calling and all that stuff. I want to make something with it.&lt;/p&gt; &lt;p&gt;So, sorry in advance if my Colab Notebook's code is messy. Any advice would be a appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastCommission2913"&gt; /u/FastCommission2913 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n81d1t/level_0_finetuned_my_first_personal_chatbot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n81d1t/level_0_finetuned_my_first_personal_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n81d1t/level_0_finetuned_my_first_personal_chatbot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T05:06:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8aqi8</id>
    <title>BenderNet - A demonstration app for using Qwen3 1.7b q4f16 with web-llm</title>
    <updated>2025-09-04T13:52:59+00:00</updated>
    <author>
      <name>/u/gajananpp</name>
      <uri>https://old.reddit.com/user/gajananpp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8aqi8/bendernet_a_demonstration_app_for_using_qwen3_17b/"&gt; &lt;img alt="BenderNet - A demonstration app for using Qwen3 1.7b q4f16 with web-llm" src="https://external-preview.redd.it/emFycnp1bDdqNW5mMcFeSbd7MPl-hlbSK9XDmWZGPdomW8w2E4v5E_699wws.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76699af374829dfe961bd28c0ea2d4ee12cd761a" title="BenderNet - A demonstration app for using Qwen3 1.7b q4f16 with web-llm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This app runs client-side thanks to an awesome tech stack:&lt;/p&gt; &lt;p&gt;𝐌𝐨𝐝𝐞𝐥: Qwen3-1.7b (q4f16)&lt;/p&gt; &lt;p&gt;𝐄𝐧𝐠𝐢𝐧𝐞: MLC's WebLLM engine for in-browser inference&lt;/p&gt; &lt;p&gt;𝐑𝐮𝐧𝐭𝐢𝐦𝐞: LangGraph Web &lt;/p&gt; &lt;p&gt;𝐀𝐫𝐜𝐡𝐢𝐭𝐞𝐜𝐭𝐮𝐫𝐞: Two separate web workers—one for the model and one for the Python-based Lark parser.&lt;/p&gt; &lt;p&gt;𝐔𝐈: assistant-ui&lt;/p&gt; &lt;p&gt;App Link: &lt;a href="https://bendernet.vercel.app"&gt;https://bendernet.vercel.app&lt;/a&gt;&lt;br /&gt; Github Link: &lt;a href="https://github.com/gajananpp/bendernet"&gt;https://github.com/gajananpp/bendernet&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.linkedin.com/feed/update/urn:li:activity:7369358620875993088/"&gt;Original LinkedIn Post&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gajananpp"&gt; /u/gajananpp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/u44geul7j5nf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8aqi8/bendernet_a_demonstration_app_for_using_qwen3_17b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8aqi8/bendernet_a_demonstration_app_for_using_qwen3_17b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T13:52:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7fdy4</id>
    <title>Introducing Kimi K2-0905</title>
    <updated>2025-09-03T13:51:27+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"&gt; &lt;img alt="Introducing Kimi K2-0905" src="https://b.thumbs.redditmedia.com/lyzeYJ2XI6oIjcCbfXBgsYvdUpg2tM8OGWtELhu--Xc.jpg" title="Introducing Kimi K2-0905" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's new:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u8oxbcfyfymf1.png?width=2178&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87daf02d6f257631f0a0a8847de7180dc9d9eed8"&gt;https://preview.redd.it/u8oxbcfyfymf1.png?width=2178&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87daf02d6f257631f0a0a8847de7180dc9d9eed8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T13:51:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7l5kg</id>
    <title>Intel launches Arc Pro B50 graphics card at $349</title>
    <updated>2025-09-03T17:27:29+00:00</updated>
    <author>
      <name>/u/levian_</name>
      <uri>https://old.reddit.com/user/levian_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7l5kg/intel_launches_arc_pro_b50_graphics_card_at_349/"&gt; &lt;img alt="Intel launches Arc Pro B50 graphics card at $349" src="https://preview.redd.it/357rwwhaizmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=066c5073e108f00168fb16f32dcc905e00df9cae" title="Intel launches Arc Pro B50 graphics card at $349" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Initial review, source:&lt;a href="https://videocardz.com/newz/intel-launches-arc-pro-b50-graphics-card-at-349"&gt;https://videocardz.com/newz/intel-launches-arc-pro-b50-graphics-card-at-349&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/levian_"&gt; /u/levian_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/357rwwhaizmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7l5kg/intel_launches_arc_pro_b50_graphics_card_at_349/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7l5kg/intel_launches_arc_pro_b50_graphics_card_at_349/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T17:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7mien</id>
    <title>Best current NSFW TTS model?</title>
    <updated>2025-09-03T18:17:17+00:00</updated>
    <author>
      <name>/u/Stock-Fault5734</name>
      <uri>https://old.reddit.com/user/Stock-Fault5734</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which one? And how to use it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stock-Fault5734"&gt; /u/Stock-Fault5734 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7mien/best_current_nsfw_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7mien/best_current_nsfw_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7mien/best_current_nsfw_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T18:17:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7wh65</id>
    <title>VibeVoice Gone?</title>
    <updated>2025-09-04T01:00:53+00:00</updated>
    <author>
      <name>/u/atrfx</name>
      <uri>https://old.reddit.com/user/atrfx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like the GitHub page and the huggingface page are gone. The huggingface only has the 1.5B&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/microsoft/VibeVoice"&gt;https://github.com/microsoft/VibeVoice&lt;/a&gt; &lt;a href="https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f"&gt;https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Modelscope still has it (for now) &lt;/p&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/models/microsoft/VibeVoice-Large/summary"&gt;https://modelscope.cn/models/microsoft/VibeVoice-Large/summary&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atrfx"&gt; /u/atrfx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7wh65/vibevoice_gone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7wh65/vibevoice_gone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7wh65/vibevoice_gone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T01:00:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n89ryn</id>
    <title>Most affordable AI computer with GPU (“GPUter”) you can build in 2025?</title>
    <updated>2025-09-04T13:13:12+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89ryn/most_affordable_ai_computer_with_gpu_gputer_you/"&gt; &lt;img alt="Most affordable AI computer with GPU (“GPUter”) you can build in 2025?" src="https://preview.redd.it/bk6tf5l2e5nf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8da7afc16f4d8ff260c98ad24de5cc8adc50a222" title="Most affordable AI computer with GPU (“GPUter”) you can build in 2025?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After a bunch of testing and experiments, we landed on what looks like the best price-to-performance build you can do right now (using all new parts in the US, 2025). Total spend: $1,040.&lt;/p&gt; &lt;p&gt;That’s the actual GPUter in the photo — whisper-quiet but surprisingly powerful.&lt;/p&gt; &lt;p&gt;Parts list:&lt;/p&gt; &lt;p&gt;GPU: NVIDIA RTX 5060 Ti 16GB Blackwell (759 AI TOPS) – $429 &lt;a href="https://newegg.com/p/N82E16814932791"&gt;https://newegg.com/p/N82E16814932791&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Motherboard: B550M – $99 &lt;a href="https://amazon.com/dp/B0BDCZRBD6"&gt;https://amazon.com/dp/B0BDCZRBD6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CPU: AMD Ryzen 5 5500 – $60 &lt;a href="https://amazon.com/dp/B09VCJ171S"&gt;https://amazon.com/dp/B09VCJ171S&lt;/a&gt;&lt;/p&gt; &lt;p&gt;RAM: 32GB DDR4 (2×16GB) – $52 &lt;a href="https://amazon.com/dp/B07RW6Z692"&gt;https://amazon.com/dp/B07RW6Z692&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Storage: M.2 SSD 4TB – $249 &lt;a href="https://amazon.com/dp/B0DHLBDSP7"&gt;https://amazon.com/dp/B0DHLBDSP7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Case: JONSBO/JONSPLUS Z20 mATX – $109 &lt;a href="https://amazon.com/dp/B0D1YKXXJD"&gt;https://amazon.com/dp/B0D1YKXXJD&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PSU: 600W – $42 &lt;a href="https://amazon.com/dp/B014W3EMAO"&gt;https://amazon.com/dp/B014W3EMAO&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Grand total: $1,040&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Note: configs can vary, and you can go wild if you want (e.g. check out used AMD EPYC CPUs on eBay - 128 vCPUs for cheap 😉)&lt;/p&gt; &lt;p&gt;In terms of memory, here’s what this build gives you:&lt;/p&gt; &lt;p&gt;⚡ 16 GB of GDDR7 VRAM on the GPU with 448 GB/s bandwidth&lt;/p&gt; &lt;p&gt;🖥️ 32 GB of DDR4 RAM on the CPU side (dual channel) with ~51 GB/s bandwidth&lt;/p&gt; &lt;p&gt;On our workloads, GPU VRAM runs at about 86% utilization, while CPU RAM sits around 50% usage.&lt;/p&gt; &lt;p&gt;This machine also boots straight into AI workloads using the AI-optimized Linux distro Sbnb Linux: &lt;a href="https://github.com/sbnb-io/sbnb"&gt;https://github.com/sbnb-io/sbnb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;💡 &lt;strong&gt;What can this thing actually do?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We used this exact setup in our Google Gemma3n Hackathon submission — it was able to process 16 live security camera feeds with real-time video understanding: &lt;a href="https://kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sixth-sense-for-security-guards-powered-by-googles"&gt;https://kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sixth-sense-for-security-guards-powered-by-googles&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy building if anyone wants to replicate! Feel free to share your configs and findings 🚀&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bk6tf5l2e5nf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89ryn/most_affordable_ai_computer_with_gpu_gputer_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n89ryn/most_affordable_ai_computer_with_gpu_gputer_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T13:13:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7zfj5</id>
    <title>built and trained this 103M MoE from scratch - went good</title>
    <updated>2025-09-04T03:21:57+00:00</updated>
    <author>
      <name>/u/External_Mushroom978</name>
      <uri>https://old.reddit.com/user/External_Mushroom978</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zfj5/built_and_trained_this_103m_moe_from_scratch_went/"&gt; &lt;img alt="built and trained this 103M MoE from scratch - went good" src="https://preview.redd.it/vqtopd08g2nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b650bedc8fac15fe5bfe8adb9d475eb26ed8f5bb" title="built and trained this 103M MoE from scratch - went good" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i made this model a few weeks ago and experimented with SFT and LoRA. &lt;/p&gt; &lt;p&gt;technical report - &lt;a href="https://github.com/Abinesh-Mathivanan/beens-minimax/blob/main/Beens_MiniMax__How_not_to_Build_an_LLM.pdf"&gt;https://github.com/Abinesh-Mathivanan/beens-minimax/blob/main/Beens_MiniMax__How_not_to_Build_an_LLM.pdf&lt;/a&gt;&lt;br /&gt; you could find the full source code and weights here - &lt;a href="https://github.com/Abinesh-Mathivanan/beens-minimax"&gt;https://github.com/Abinesh-Mathivanan/beens-minimax&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mushroom978"&gt; /u/External_Mushroom978 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vqtopd08g2nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zfj5/built_and_trained_this_103m_moe_from_scratch_went/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zfj5/built_and_trained_this_103m_moe_from_scratch_went/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T03:21:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1n860bg</id>
    <title>DeepSeek Targets AI Agent Release by End of Year to Rival OpenAI</title>
    <updated>2025-09-04T10:01:10+00:00</updated>
    <author>
      <name>/u/alanwong</name>
      <uri>https://old.reddit.com/user/alanwong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n860bg/deepseek_targets_ai_agent_release_by_end_of_year/"&gt; &lt;img alt="DeepSeek Targets AI Agent Release by End of Year to Rival OpenAI" src="https://external-preview.redd.it/zq3vtY7JidZAIOIb5HnnpP8-DLavzSbkRkKDWz39uG0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7f396c0eb56f8886bfd55dad93a9f1d3218e5a3" title="DeepSeek Targets AI Agent Release by End of Year to Rival OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alanwong"&gt; /u/alanwong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-09-04/deepseek-targets-ai-agent-release-by-end-of-year-to-rival-openai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n860bg/deepseek_targets_ai_agent_release_by_end_of_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n860bg/deepseek_targets_ai_agent_release_by_end_of_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T10:01:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7uocj</id>
    <title>PSA: Make sure your API ports aren't exposed to the open internet</title>
    <updated>2025-09-03T23:38:13+00:00</updated>
    <author>
      <name>/u/nooclear</name>
      <uri>https://old.reddit.com/user/nooclear</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are about 1,100 exposed Ollama servers out there according to this blog post:&lt;/p&gt; &lt;p&gt;&lt;a href="https://blogs.cisco.com/security/detecting-exposed-llm-servers-shodan-case-study-on-ollama"&gt;https://blogs.cisco.com/security/detecting-exposed-llm-servers-shodan-case-study-on-ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, if you see the prompt &amp;quot;What is 2+2?&amp;quot; in your logs, it was Cisco.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nooclear"&gt; /u/nooclear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7uocj/psa_make_sure_your_api_ports_arent_exposed_to_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7uocj/psa_make_sure_your_api_ports_arent_exposed_to_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7uocj/psa_make_sure_your_api_ports_arent_exposed_to_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T23:38:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1n86rl2</id>
    <title>Which is the Best LLM you can run on your hardware? Discover it with llm-eval simple</title>
    <updated>2025-09-04T10:45:34+00:00</updated>
    <author>
      <name>/u/gnorrisan</name>
      <uri>https://old.reddit.com/user/gnorrisan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n86rl2/which_is_the_best_llm_you_can_run_on_your/"&gt; &lt;img alt="Which is the Best LLM you can run on your hardware? Discover it with llm-eval simple" src="https://preview.redd.it/nsuc0la2n4nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6a9d7274af0bf58b60061454cb27096d8dcb54d" title="Which is the Best LLM you can run on your hardware? Discover it with llm-eval simple" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can check your prompts and get an heatmap of the most correct and fast LLMs you can run on your computer for the use-cases you care. The most intense colors means a fater reply.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/grigio/llm-eval-simple"&gt;https://github.com/grigio/llm-eval-simple&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gnorrisan"&gt; /u/gnorrisan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nsuc0la2n4nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n86rl2/which_is_the_best_llm_you_can_run_on_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n86rl2/which_is_the_best_llm_you_can_run_on_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T10:45:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8abe6</id>
    <title>Eigent – Open Source, Local-First Multi-Agent Workforce</title>
    <updated>2025-09-04T13:36:11+00:00</updated>
    <author>
      <name>/u/FitHeron1933</name>
      <uri>https://old.reddit.com/user/FitHeron1933</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8abe6/eigent_open_source_localfirst_multiagent_workforce/"&gt; &lt;img alt="Eigent – Open Source, Local-First Multi-Agent Workforce" src="https://b.thumbs.redditmedia.com/o7mm3i-ghjjH2wm6PPrdvzWbqLN-x3TC3x0fwhx5pMw.jpg" title="Eigent – Open Source, Local-First Multi-Agent Workforce" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A month ago we shared &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mdbm5t/eigent_open_source_localfirst_multiagent_workforce/?utm_source=chatgpt.com&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;Eigent&lt;/a&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mdbm5t/eigent_open_source_localfirst_multiagent_workforce/?utm_source=chatgpt.com&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;here&lt;/a&gt;, our attempt at building a fully open-source, local-first multi-agent workforce you can run on your own machine.&lt;/p&gt; &lt;p&gt;The response was amazing, and so was the feedback. Two things came up the most:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Needing to sign up before trying it&lt;/li&gt; &lt;li&gt;Concerns about the license not feeling “truly open”&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So we focused on those. Now Eigent is fully local, you’ll still see a signup pipeline in the UI, but everything is stored only on your own device in a private Postgres database. Nothing leaves your machine. On the licensing side, we’ve also made updates. Eigent is now free for individuals and small teams of up to 10 users, including commercial use.&lt;/p&gt; &lt;p&gt;We’d love for you to give Eigent another try and let us know what you think. Your input is what helps us shape it into something that’s genuinely useful for developers and teams who want privacy, flexibility, and full ownership of their AI workflows, while unlocking exceptional productivity.&lt;/p&gt; &lt;p&gt;Follow the guide for setting it up locally: &lt;a href="https://github.com/eigent-ai/eigent/blob/main/server/README_EN.md"&gt;https://github.com/eigent-ai/eigent/blob/main/server/README_EN.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;→ GitHub: &lt;a href="https://github.com/eigent-ai/eigent"&gt;https://github.com/eigent-ai/eigent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;→ Download: &lt;a href="https://eigent.ai"&gt;https://eigent.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And if you find it useful, please give the repo a ⭐ and spread the word!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FitHeron1933"&gt; /u/FitHeron1933 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n8abe6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8abe6/eigent_open_source_localfirst_multiagent_workforce/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8abe6/eigent_open_source_localfirst_multiagent_workforce/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T13:36:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7z5kl</id>
    <title>Did M$ take down VibeVoice repo??</title>
    <updated>2025-09-04T03:08:12+00:00</updated>
    <author>
      <name>/u/x0rchidia</name>
      <uri>https://old.reddit.com/user/x0rchidia</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7z5kl/did_m_take_down_vibevoice_repo/"&gt; &lt;img alt="Did M$ take down VibeVoice repo??" src="https://preview.redd.it/vsnyimd3e2nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=277cc5e5dbf4b6c8f03d5f05352e5f7de6a92598" title="Did M$ take down VibeVoice repo??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not sure if I missed something, but &lt;a href="https://github.com/microsoft/VibeVoice"&gt;https://github.com/microsoft/VibeVoice&lt;/a&gt; is a 404 now &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/x0rchidia"&gt; /u/x0rchidia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vsnyimd3e2nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7z5kl/did_m_take_down_vibevoice_repo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7z5kl/did_m_take_down_vibevoice_repo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T03:08:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1n89wi8</id>
    <title>power limit your GPU(s) to reduce electricity costs</title>
    <updated>2025-09-04T13:18:43+00:00</updated>
    <author>
      <name>/u/MelodicRecognition7</name>
      <uri>https://old.reddit.com/user/MelodicRecognition7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89wi8/power_limit_your_gpus_to_reduce_electricity_costs/"&gt; &lt;img alt="power limit your GPU(s) to reduce electricity costs" src="https://a.thumbs.redditmedia.com/uyJHr0LNNqlJuU5QeZvn8UmkeX2y2aeJN7dPnMhD4t0.jpg" title="power limit your GPU(s) to reduce electricity costs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;many people worry about high electricity costs, the solution is simply power limit the GPU to about 50% of its TDP (&lt;code&gt;nvidia-smi -i $GPU_ID --power-limit=$LIMIT_IN_WATTS&lt;/code&gt;) because token generation speed does not increase past some power limit amount so you just waste electricity with the full power. As an example here is a result of &lt;code&gt;llama-bench&lt;/code&gt; (pp1024, tg1024, model Qwen3-32B Q8_0 33 GB) running on RTX Pro 6000 Workstation (600W TDP) power limited from 150W to 600W in 30W increments. 350W is the best spot for that card which is obvious on the token generation speed chart, however the prompt processing speed rise is also not linear and starts to slow down at about 350W. And another example: the best power limit for 4090 (450W TDP) is 270W, tested with Qwen3 8B.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MelodicRecognition7"&gt; /u/MelodicRecognition7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n89wi8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89wi8/power_limit_your_gpus_to_reduce_electricity_costs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n89wi8/power_limit_your_gpus_to_reduce_electricity_costs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T13:18:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7zk45</id>
    <title>VibeVoice RIP? What do you think?</title>
    <updated>2025-09-04T03:28:29+00:00</updated>
    <author>
      <name>/u/Fabix84</name>
      <uri>https://old.reddit.com/user/Fabix84</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zk45/vibevoice_rip_what_do_you_think/"&gt; &lt;img alt="VibeVoice RIP? What do you think?" src="https://preview.redd.it/un6uilkoh2nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39144e5e650c4ae66ef8205b6d09c62f6427edad" title="VibeVoice RIP? What do you think?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the past two weeks, I had been working hard to try and contribute to OpenSource AI by creating the VibeVoice nodes for ComfyUI. I’m glad to see that my contribution has helped quite a few people:&lt;br /&gt; &lt;a href="https://github.com/Enemyx-net/VibeVoice-ComfyUI"&gt;https://github.com/Enemyx-net/VibeVoice-ComfyUI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A short while ago, Microsoft suddenly deleted its official VibeVoice repository on GitHub. As of the time I’m writing this, the reason is still unknown (or at least I don’t know it).&lt;/p&gt; &lt;p&gt;At the same time, Microsoft also removed the VibeVoice-Large and VibeVoice-Large-Preview models from HF. For now, they are still available here: &lt;a href="https://modelscope.cn/models/microsoft/VibeVoice-Large/files"&gt;https://modelscope.cn/models/microsoft/VibeVoice-Large/files&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Of course, for those who have already downloaded and installed my nodes and the models, they will continue to work. Technically, I could decide to embed a copy of VibeVoice directly into my repo, but first I need to understand why Microsoft chose to remove its official repository. My hope is that they are just fixing a few things and that it will be back online soon. I also hope there won’t be any changes to the usage license...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;UPDATE: I have released a new 1.0.9 version that embed VibeVoice. No longer requires external VibeVoice installation.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabix84"&gt; /u/Fabix84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/un6uilkoh2nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zk45/vibevoice_rip_what_do_you_think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zk45/vibevoice_rip_what_do_you_think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T03:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n84rp5</id>
    <title>Mistral Set for $14 Billion Valuation With New Funding Round</title>
    <updated>2025-09-04T08:43:07+00:00</updated>
    <author>
      <name>/u/robberviet</name>
      <uri>https://old.reddit.com/user/robberviet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n84rp5/mistral_set_for_14_billion_valuation_with_new/"&gt; &lt;img alt="Mistral Set for $14 Billion Valuation With New Funding Round" src="https://external-preview.redd.it/M4AwBB5q0ft-ep7S9kw_Y8TYtAOJMnISlkcxXVEEP40.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=741648af7070d19298c0fb93541f0b6cf31c20b1" title="Mistral Set for $14 Billion Valuation With New Funding Round" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral has secured new funding, ensuring continued independence. No more rumors.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robberviet"&gt; /u/robberviet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-09-03/mistral-set-for-14-billion-valuation-with-new-funding-round"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n84rp5/mistral_set_for_14_billion_valuation_with_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n84rp5/mistral_set_for_14_billion_valuation_with_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T08:43:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n82ndz</id>
    <title>Finally: 3090 Successor: 5070 Ti super 24Gb 800$</title>
    <updated>2025-09-04T06:24:02+00:00</updated>
    <author>
      <name>/u/On1ineAxeL</name>
      <uri>https://old.reddit.com/user/On1ineAxeL</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n82ndz/finally_3090_successor_5070_ti_super_24gb_800/"&gt; &lt;img alt="Finally: 3090 Successor: 5070 Ti super 24Gb 800$" src="https://external-preview.redd.it/kT4ohg_saogl0QowFisFMgdjPOl3cV1Xjwbw3qji8TU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01c103360d1456c04311f988c3089b01de5157d0" title="Finally: 3090 Successor: 5070 Ti super 24Gb 800$" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/j9riehskc3nf1.jpg?width=1341&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fd5386a95c701b1a750a20a2b4116c93df426306"&gt;https://preview.redd.it/j9riehskc3nf1.jpg?width=1341&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fd5386a95c701b1a750a20a2b4116c93df426306&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=9ii4qrzfV5w"&gt;https://www.youtube.com/watch?v=9ii4qrzfV5w&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If they are well compressed in terms of energy consumption, then now it will be possible to assemble a rig with 100 gigabytes of VRAM without kilowatts of energy consumption, and we shouldn’t forget about the new FP4 formats&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/On1ineAxeL"&gt; /u/On1ineAxeL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n82ndz/finally_3090_successor_5070_ti_super_24gb_800/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n82ndz/finally_3090_successor_5070_ti_super_24gb_800/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n82ndz/finally_3090_successor_5070_ti_super_24gb_800/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T06:24:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1n89dy9</id>
    <title>🤷‍♂️</title>
    <updated>2025-09-04T12:56:20+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89dy9/_/"&gt; &lt;img alt="🤷‍♂️" src="https://preview.redd.it/21ivxa12b5nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e7a2744c78f03b518a206253cd3c9e861ea71c9" title="🤷‍♂️" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/21ivxa12b5nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89dy9/_/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n89dy9/_/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T12:56:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7j5z2</id>
    <title>Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)</title>
    <updated>2025-09-03T16:14:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt; &lt;img alt="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" src="https://preview.redd.it/wdx4ivdw3zmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=876855c03867ead70389d15b60f24b91d478f835" title="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wdx4ivdw3zmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T16:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
