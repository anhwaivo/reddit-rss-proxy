<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-22T14:39:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m68elw</id>
    <title>Fine-Tuning Multilingual Embedding Models for Industrial RAG System</title>
    <updated>2025-07-22T08:15:19+00:00</updated>
    <author>
      <name>/u/Maddin187</name>
      <uri>https://old.reddit.com/user/Maddin187</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm currently working on a project to fine-tune multilingual embedding models to improve document retrieval within a company's RAG system. The dataset consists of German and English documents related to industrial products, so multilingual support is essential. The dataset has a query-passage format with synthetic generated queries from the given documens.&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Requirements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multilingual (German &amp;amp; English)&lt;/li&gt; &lt;li&gt;Max. 7B parameters&lt;/li&gt; &lt;li&gt;Preferably compatible with Sentence-Transformers&lt;/li&gt; &lt;li&gt;Open-source&lt;/li&gt; &lt;/ul&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Models based on MTEB Retrieval performance:&lt;/p&gt; &lt;p&gt;&lt;a href="http://mteb-leaderboard.hf.space/?benchmark_name=MTEB%28Multilingual%2C+v2%29"&gt;http://mteb-leaderboard.hf.space/?benchmark_name=MTEB%28Multilingual%2C+v2%29&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen Embedding 8B / 4B&lt;/li&gt; &lt;li&gt;SFR-Embedding-Mistral&lt;/li&gt; &lt;li&gt;E5-mistral-7b-instruct&lt;/li&gt; &lt;li&gt;Snowflake-arctic-embed-m-v2.0&lt;/li&gt; &lt;/ul&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;I also read some papers and found that the following models were frequently used for fine-tuning embedding models for closed-domain use cases:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;BGE (all variants)&lt;/li&gt; &lt;li&gt;mE5&lt;/li&gt; &lt;li&gt;All-MiniLM-L6-v1.5&lt;/li&gt; &lt;li&gt;Text-Embedding-3-Large (often used as a baseline)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Would love to hear your thoughts or experiences, especially if you've worked on similar multilingual or domain-specific retrieval systems!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maddin187"&gt; /u/Maddin187 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m68elw/finetuning_multilingual_embedding_models_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m68elw/finetuning_multilingual_embedding_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m68elw/finetuning_multilingual_embedding_models_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T08:15:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1m69sb6</id>
    <title>In Qwen3-235B-A22B-Instruct-2507-UD-Q4 (unsloth) I'm seeing some "but wait" and related ones (like kinda questioning and answering itself), were the model seems to "think" (even when is a non-thinking model and I haven't setup any system prompt), have you seen something similar?</title>
    <updated>2025-07-22T09:45:44+00:00</updated>
    <author>
      <name>/u/relmny</name>
      <uri>https://old.reddit.com/user/relmny</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running it with latest llama-server (llama.cpp) and with the suggested parameters (same as the non-thinking Qwen3 ones)&lt;/p&gt; &lt;p&gt;Didn't see that with the &amp;quot;old&amp;quot; 235b with /no_think &lt;/p&gt; &lt;p&gt;Is that expected?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/relmny"&gt; /u/relmny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m69sb6/in_qwen3235ba22binstruct2507udq4_unsloth_im/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m69sb6/in_qwen3235ba22binstruct2507udq4_unsloth_im/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m69sb6/in_qwen3235ba22binstruct2507udq4_unsloth_im/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T09:45:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5oxyp</id>
    <title>Qwen released Qwen3-235B-A22B-2507!</title>
    <updated>2025-07-21T17:18:57+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5oxyp/qwen_released_qwen3235ba22b2507/"&gt; &lt;img alt="Qwen released Qwen3-235B-A22B-2507!" src="https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=13fd449c88365fae792fbacc8076a6e633ad74e2" title="Qwen released Qwen3-235B-A22B-2507!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bye Qwen3-235B-A22B, hello Qwen3-235B-A22B-2507!&lt;/p&gt; &lt;p&gt;After talking with the community and thinking it through, we decided to stop using hybrid thinking mode. Instead, we’ll train Instruct and Thinking models separately so we can get the best quality possible. Today, we’re releasing Qwen3-235B-A22B-Instruct-2507 and its FP8 version for everyone.&lt;/p&gt; &lt;p&gt;This model performs better than our last release, and we hope you’ll like it thanks to its strong overall abilities.&lt;/p&gt; &lt;p&gt;Qwen Chat: chat.qwen.ai — just start chatting with the default model, and feel free to use the search button!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6csu4o4wg9ef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5oxyp/qwen_released_qwen3235ba22b2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5oxyp/qwen_released_qwen3235ba22b2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T17:18:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5iymb</id>
    <title>The reason why local models are better/necessary.</title>
    <updated>2025-07-21T13:30:11+00:00</updated>
    <author>
      <name>/u/GPTshop_ai</name>
      <uri>https://old.reddit.com/user/GPTshop_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5iymb/the_reason_why_local_models_are_betternecessary/"&gt; &lt;img alt="The reason why local models are better/necessary." src="https://preview.redd.it/vdngpglhb8ef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae855f8543a7b34526b58ea6c68423bf02a9e2ac" title="The reason why local models are better/necessary." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GPTshop_ai"&gt; /u/GPTshop_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vdngpglhb8ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5iymb/the_reason_why_local_models_are_betternecessary/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5iymb/the_reason_why_local_models_are_betternecessary/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T13:30:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5gwzs</id>
    <title>I extracted the system prompts from closed-source tools like Cursor &amp; v0. The repo just hit 70k stars.</title>
    <updated>2025-07-21T11:56:42+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there,&lt;/p&gt; &lt;p&gt;My project to extract and collect the &amp;quot;secret&amp;quot; system prompts from a bunch of proprietary AI tools just passed 70k stars on GitHub, and I wanted to share it with this community specifically because I think it's incredibly useful.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The idea is to see the advanced &amp;quot;prompt architecture&amp;quot; that companies like Vercel, Cursor, etc., use to get high-quality results, so we can replicate those techniques on different platforms.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of trying to reinvent the wheel, you can see exactly how they force models to &amp;quot;think step-by-step&amp;quot; in a scratchpad, how they define an expert persona with hyper-specific rules, or how they demand rigidly structured outputs. It's a goldmine of ideas for crafting better system prompts.&lt;/p&gt; &lt;p&gt;For example, here's a small snippet from the Cursor prompt that shows how they establish the AI's role and capabilities right away:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Knowledge cutoff: 2024-06 You are an AI coding assistant, powered by GPT-4.1. You operate in Cursor. You are pair programming with a USER to solve their coding task. Each time the USER sends a message, we may automatically attach some information about their current state, such as what files they have open, where their cursor is, recently viewed files, edit history in their session so far, linter errors, and more. This information may or may not be relevant to the coding task, it is up for you to decide. You are an agent - please keep going until the user's query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved. Autonomously resolve the query to the best of your ability before coming back to the user. Your main goal is to follow the USER's instructions at each message, denoted by the &amp;lt;user_query&amp;gt; tag. &amp;lt;communication&amp;gt; When using markdown in assistant messages, use backticks to format file, directory, function, and class names. Use \( and \) for inline math, \[ and \] for block math. &amp;lt;/communication&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I wrote a full article that does a deep dive into these patterns and also discusses the &amp;quot;dual-use&amp;quot; aspect of making these normally-hidden prompts public.&lt;/p&gt; &lt;p&gt;I'm super curious: &lt;strong&gt;How are you all structuring system prompts for your favorite models?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;The full article with more analysis:&lt;/strong&gt; &lt;a href="https://medium.com/@lucknitelol/the-open-source-project-that-became-an-essential-library-for-modern-ai-engineering-67021b50acee?source=user_profile_page---------0-------------d9a574987030----------------------"&gt;The Open Source Project That Became an Essential Library for Modern AI Engineering&lt;/a&gt;&lt;a href="https://medium.com/@lucknitelol?source=post_page---byline--67021b50acee---------------------------------------"&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;The GitHub Repo (to grab the prompts):&lt;/strong&gt; &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hope you find it useful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5gwzs/i_extracted_the_system_prompts_from_closedsource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5gwzs/i_extracted_the_system_prompts_from_closedsource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5gwzs/i_extracted_the_system_prompts_from_closedsource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T11:56:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6ahsu</id>
    <title>Thinking about updating Llama 3.3-70B</title>
    <updated>2025-07-22T10:29:04+00:00</updated>
    <author>
      <name>/u/Only_Emergencies</name>
      <uri>https://old.reddit.com/user/Only_Emergencies</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I deployed Llama 3.3-70B for my organization quite a long time ago. I am now thinking of updating it to a newer model since there have been quite a few great new LLM releases recently. However, is there any model that actually performs better than Llama 3.3-70B for general purposes (chat, summarization... basically normal daily office tasks) with more or less the same size? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Only_Emergencies"&gt; /u/Only_Emergencies &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T10:29:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5oz0h</id>
    <title>Qwen3-235B-A22B-2507!</title>
    <updated>2025-07-21T17:19:58+00:00</updated>
    <author>
      <name>/u/ken-senseii</name>
      <uri>https://old.reddit.com/user/ken-senseii</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5oz0h/qwen3235ba22b2507/"&gt; &lt;img alt="Qwen3-235B-A22B-2507!" src="https://b.thumbs.redditmedia.com/sK-ChiNoLnwj2ggKTtNTJYTvWhnsGqdGF-BtGXWSrIM.jpg" title="Qwen3-235B-A22B-2507!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/7by2astxg9ef1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ed2caaa4b854693b6fd46383a9626aefe87b0128"&gt;Mind-Blowing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ken-senseii"&gt; /u/ken-senseii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5oz0h/qwen3235ba22b2507/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5oz0h/qwen3235ba22b2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5oz0h/qwen3235ba22b2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T17:19:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5r9ss</id>
    <title>Exhausted man defeats AI model in world coding championship</title>
    <updated>2025-07-21T18:45:00+00:00</updated>
    <author>
      <name>/u/Educational_Sun_8813</name>
      <uri>https://old.reddit.com/user/Educational_Sun_8813</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;A Polish programmer running on fumes recently accomplished what may soon become impossible: beating an advanced AI model from OpenAI in a head-to-head coding competition. The 10-hour marathon left him &amp;quot;completely exhausted.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arstechnica.com/ai/2025/07/exhausted-man-defeats-ai-model-in-world-coding-championship/"&gt;https://arstechnica.com/ai/2025/07/exhausted-man-defeats-ai-model-in-world-coding-championship/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Sun_8813"&gt; /u/Educational_Sun_8813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5r9ss/exhausted_man_defeats_ai_model_in_world_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5r9ss/exhausted_man_defeats_ai_model_in_world_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5r9ss/exhausted_man_defeats_ai_model_in_world_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T18:45:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1m650ow</id>
    <title>If Qwen3-235B-A22B-2507 can't think, why does it think when the thinking button is on?</title>
    <updated>2025-07-22T04:45:41+00:00</updated>
    <author>
      <name>/u/JeffreySons_90</name>
      <uri>https://old.reddit.com/user/JeffreySons_90</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m650ow/if_qwen3235ba22b2507_cant_think_why_does_it_think/"&gt; &lt;img alt="If Qwen3-235B-A22B-2507 can't think, why does it think when the thinking button is on?" src="https://preview.redd.it/lxwf5fgevcef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf2250dfec91bd4da5ce280844d385d4702942d1" title="If Qwen3-235B-A22B-2507 can't think, why does it think when the thinking button is on?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JeffreySons_90"&gt; /u/JeffreySons_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lxwf5fgevcef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m650ow/if_qwen3235ba22b2507_cant_think_why_does_it_think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m650ow/if_qwen3235ba22b2507_cant_think_why_does_it_think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T04:45:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1m62vbw</id>
    <title>Running LLMs against a sandbox airport to see if they can make the correct decisions in real time</title>
    <updated>2025-07-22T02:53:39+00:00</updated>
    <author>
      <name>/u/jjasghar</name>
      <uri>https://old.reddit.com/user/jjasghar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m62vbw/running_llms_against_a_sandbox_airport_to_see_if/"&gt; &lt;img alt="Running LLMs against a sandbox airport to see if they can make the correct decisions in real time" src="https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c17ae1c6715cde69de4cb21dd94c66e0f2a16d0b" title="Running LLMs against a sandbox airport to see if they can make the correct decisions in real time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created this sandbox to test LLMs and their real-time decision-making processes. Running it has generated some interesting outputs, and I'm curious to see if others find the same. PRs accepted and encouraged!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jjasghar"&gt; /u/jjasghar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jjasghar/ai-airport-simulation"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m62vbw/running_llms_against_a_sandbox_airport_to_see_if/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m62vbw/running_llms_against_a_sandbox_airport_to_see_if/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T02:53:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6dco7</id>
    <title>Jamba 1.7 is now available on Kaggle</title>
    <updated>2025-07-22T12:55:54+00:00</updated>
    <author>
      <name>/u/NullPointerJack</name>
      <uri>https://old.reddit.com/user/NullPointerJack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI21 has just made Jamba 1.7 available on Kaggle:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.kaggle.com/models/ai21labs/ai21-jamba-1.7"&gt;https://www.kaggle.com/models/ai21labs/ai21-jamba-1.7&lt;/a&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can run and test the model without needing to install it locally&lt;/li&gt; &lt;li&gt;No need to harness setup, hardware and engineering knowledge via Hugging Face anymore&lt;/li&gt; &lt;li&gt;Now you can run sample tasks, benchmark against other models and share public notebooks with results&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Pretty significant as the model is now available for non technical users. Here is what we know about 1.7 and Jamba in general:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Combination of Transformer architecture and Mamba, making it more efficient at handling long sequences&lt;/li&gt; &lt;li&gt;25k context window - well-suited for long document summarization and memory-heavy chat agents&lt;/li&gt; &lt;li&gt;Improved capabilities in understanding and following user instructions, and generating more factual, relevant outputs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Who is going to try it out? What use cases do you have in mind?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NullPointerJack"&gt; /u/NullPointerJack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6dco7/jamba_17_is_now_available_on_kaggle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6dco7/jamba_17_is_now_available_on_kaggle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6dco7/jamba_17_is_now_available_on_kaggle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T12:55:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1m65iga</id>
    <title>Frankenserver for sale at a steep discount. 2x96GB GH200 converted from liquid- to air-cooled.</title>
    <updated>2025-07-22T05:13:44+00:00</updated>
    <author>
      <name>/u/GPTrack_ai</name>
      <uri>https://old.reddit.com/user/GPTrack_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m65iga/frankenserver_for_sale_at_a_steep_discount_2x96gb/"&gt; &lt;img alt="Frankenserver for sale at a steep discount. 2x96GB GH200 converted from liquid- to air-cooled." src="https://preview.redd.it/ifz3sua70def1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=784878f38d8fa43b398531435fad5ad46f80423f" title="Frankenserver for sale at a steep discount. 2x96GB GH200 converted from liquid- to air-cooled." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GPTrack_ai"&gt; /u/GPTrack_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ifz3sua70def1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m65iga/frankenserver_for_sale_at_a_steep_discount_2x96gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m65iga/frankenserver_for_sale_at_a_steep_discount_2x96gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T05:13:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1m60ahf</id>
    <title>Used A100 40GB just dropped below $2000, for those who care with caveat</title>
    <updated>2025-07-22T00:50:04+00:00</updated>
    <author>
      <name>/u/--dany--</name>
      <uri>https://old.reddit.com/user/--dany--</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unfortunately it's on SXM4, you will need a $600 adapter for this. but I am sure someone with enough motivation will figure out a way to drop it into a PCIe adapter to sell it as a complete package. It'll be an interesting piece of localllama HW.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/--dany--"&gt; /u/--dany-- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m60ahf/used_a100_40gb_just_dropped_below_2000_for_those/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m60ahf/used_a100_40gb_just_dropped_below_2000_for_those/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m60ahf/used_a100_40gb_just_dropped_below_2000_for_those/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T00:50:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5n148</id>
    <title>Imminent release from Qwen tonight</title>
    <updated>2025-07-21T16:08:22+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5n148/imminent_release_from_qwen_tonight/"&gt; &lt;img alt="Imminent release from Qwen tonight" src="https://preview.redd.it/um0pwye549ef1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3401860a4fccf34b2ae631236b2b714dafe0ec28" title="Imminent release from Qwen tonight" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/JustinLin610/status/1947281769134170147"&gt;https://x.com/JustinLin610/status/1947281769134170147&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Maybe Qwen3-Coder, Qwen3-VL or a new QwQ? Will be open source / weight according to Chujie Zheng &lt;a href="https://x.com/ChujieZheng/status/1947307034980089905"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/um0pwye549ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5n148/imminent_release_from_qwen_tonight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5n148/imminent_release_from_qwen_tonight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T16:08:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1m61u94</id>
    <title>OmniSVG weights released</title>
    <updated>2025-07-22T02:03:54+00:00</updated>
    <author>
      <name>/u/DeProgrammer99</name>
      <uri>https://old.reddit.com/user/DeProgrammer99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Throwback to 3 months ago: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Weights: &lt;a href="https://huggingface.co/OmniSVG/OmniSVG"&gt;https://huggingface.co/OmniSVG/OmniSVG&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace demo: &lt;a href="https://huggingface.co/spaces/OmniSVG/OmniSVG-3B"&gt;https://huggingface.co/spaces/OmniSVG/OmniSVG-3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/OmniSVG/OmniSVG/"&gt;https://github.com/OmniSVG/OmniSVG/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeProgrammer99"&gt; /u/DeProgrammer99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m61u94/omnisvg_weights_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m61u94/omnisvg_weights_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m61u94/omnisvg_weights_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T02:03:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6172l</id>
    <title>New qwen tested on Fiction.liveBench</title>
    <updated>2025-07-22T01:33:20+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6172l/new_qwen_tested_on_fictionlivebench/"&gt; &lt;img alt="New qwen tested on Fiction.liveBench" src="https://preview.redd.it/9rynne03xbef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8cc832729da290425257b97f9e8171f9cd64ec1e" title="New qwen tested on Fiction.liveBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9rynne03xbef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6172l/new_qwen_tested_on_fictionlivebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6172l/new_qwen_tested_on_fictionlivebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T01:33:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5ox8z</id>
    <title>Qwen3-235B-A22B-2507</title>
    <updated>2025-07-21T17:18:14+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5ox8z/qwen3235ba22b2507/"&gt; &lt;img alt="Qwen3-235B-A22B-2507" src="https://preview.redd.it/w2uh7h5lg9ef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5242a889814e823acb6da0b1179758e2947ea2a7" title="Qwen3-235B-A22B-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1947344511988076547"&gt;https://x.com/Alibaba_Qwen/status/1947344511988076547&lt;/a&gt;&lt;/p&gt; &lt;p&gt;New Qwen3-235B-A22B with thinking mode only –– no more hybrid reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w2uh7h5lg9ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5ox8z/qwen3235ba22b2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5ox8z/qwen3235ba22b2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T17:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1m66qks</id>
    <title>Private Eval result of Qwen3-235B-A22B-Instruct-2507</title>
    <updated>2025-07-22T06:28:26+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m66qks/private_eval_result_of_qwen3235ba22binstruct2507/"&gt; &lt;img alt="Private Eval result of Qwen3-235B-A22B-Instruct-2507" src="https://b.thumbs.redditmedia.com/z5Hij9VVgEz-n0a_TqfphvLzGcPNiRWrZnXSAwXHg_Q.jpg" title="Private Eval result of Qwen3-235B-A22B-Instruct-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a &lt;strong&gt;&lt;em&gt;Private&lt;/em&gt;&lt;/strong&gt; eval that has been updated for over a year by Zhihu user &amp;quot;toyama nao&amp;quot;. So qwen cannot be benchmaxxing on it because it is &lt;strong&gt;&lt;em&gt;Private&lt;/em&gt;&lt;/strong&gt; and the questions are being updated constantly.&lt;/p&gt; &lt;p&gt;The score of this 2507 update is amazing, especially since it's a non-reasoning model that ranks among other reasoning ones.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s5t1rm4dcdef1.png?width=1054&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=74ec5e6f2306496b82a9049ef150b1b9f9f3b2c9"&gt;logic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q1ld1vkvcdef1.png?width=1319&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=849ca0681fc9aa9bfb08fc3ef6d29529731dfcbc"&gt;coding&lt;/a&gt;&lt;/p&gt; &lt;p&gt;*These 2 tables are OCR and translated by gemini, so it may contain small errors&lt;/p&gt; &lt;p&gt;Do note that Chinese models could have a slight advantage in this benchmark because the questions could be written in Chinese&lt;/p&gt; &lt;p&gt;Source:&lt;/p&gt; &lt;p&gt;&lt;a href="Https://www.zhihu.com/question/1930932168365925991/answer/1930972327442646873"&gt;Https://www.zhihu.com/question/1930932168365925991/answer/1930972327442646873&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m66qks/private_eval_result_of_qwen3235ba22binstruct2507/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m66qks/private_eval_result_of_qwen3235ba22binstruct2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m66qks/private_eval_result_of_qwen3235ba22binstruct2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T06:28:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5owi8</id>
    <title>Qwen3-235B-A22B-2507 Released!</title>
    <updated>2025-07-21T17:17:27+00:00</updated>
    <author>
      <name>/u/pseudoreddituser</name>
      <uri>https://old.reddit.com/user/pseudoreddituser</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5owi8/qwen3235ba22b2507_released/"&gt; &lt;img alt="Qwen3-235B-A22B-2507 Released!" src="https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9739fe5698145f958eb2e1c66da1875fc6d34a00" title="Qwen3-235B-A22B-2507 Released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pseudoreddituser"&gt; /u/pseudoreddituser &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1947344511988076547"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5owi8/qwen3235ba22b2507_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5owi8/qwen3235ba22b2507_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T17:17:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1m67zde</id>
    <title>AI should just be open-source</title>
    <updated>2025-07-22T07:47:52+00:00</updated>
    <author>
      <name>/u/adviceguru25</name>
      <uri>https://old.reddit.com/user/adviceguru25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For once, I’m not going to talk about my benchmark, so to be forefront, there will be no other reference or link to it in this post,&lt;/p&gt; &lt;p&gt;That said, just sharing something that’s been on mind. I’ve been thinking about this topic recently, and while this may be a hot or controversial take, all AI models should be open-source (even from companies like xAI, Google, OpenAI, etc.)&lt;/p&gt; &lt;p&gt;AI is already one of the greatest inventions in human history, and at minimum it will likely be on par in terms of impact with the Internet.&lt;/p&gt; &lt;p&gt;Like how the Internet is “open” for anyone to use and build on top of it, AI should be the same way.&lt;/p&gt; &lt;p&gt;It’s fine if products built on top of AI like Cursor, Codex, Claude Code, etc or anything that has an AI integration to be commercialized, but for the benefit and advancement of humanity, the underlying technology (the models) should be made publicly available.&lt;/p&gt; &lt;p&gt;What are your thoughts on this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adviceguru25"&gt; /u/adviceguru25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m67zde/ai_should_just_be_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m67zde/ai_should_just_be_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m67zde/ai_should_just_be_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T07:47:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6b151</id>
    <title>Updated Strix Halo (Ryzen AI Max+ 395) LLM Benchmark Results</title>
    <updated>2025-07-22T11:00:04+00:00</updated>
    <author>
      <name>/u/randomfoo2</name>
      <uri>https://old.reddit.com/user/randomfoo2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/"&gt; &lt;img alt="Updated Strix Halo (Ryzen AI Max+ 395) LLM Benchmark Results" src="https://b.thumbs.redditmedia.com/iZq9ApFg7F044Ny8obqZ27FfndXjE_7xNkH5oORO2gc.jpg" title="Updated Strix Halo (Ryzen AI Max+ 395) LLM Benchmark Results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A while back I posted some &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/"&gt;Strix Halo LLM performance testing&lt;/a&gt; benchmarks. I'm back with an update that I believe is actually a fair bit more comprehensive now (although the original is still worth checking out for background).&lt;/p&gt; &lt;p&gt;The biggest difference is I wrote some automated sweeps to test different backends and flags against a full range of pp/tg on many different model architectures (including the latest MoEs) and sizes.&lt;/p&gt; &lt;p&gt;This is also using the latest drivers, ROCm (7.0 nightlies), and llama.cpp &lt;/p&gt; &lt;p&gt;All the full data and latest info is available in the Github repo: &lt;a href="https://github.com/lhl/strix-halo-testing/tree/main/llm-bench"&gt;https://github.com/lhl/strix-halo-testing/tree/main/llm-bench&lt;/a&gt; but here are the topline stats below:&lt;/p&gt; &lt;h1&gt;Strix Halo LLM Benchmark Results&lt;/h1&gt; &lt;p&gt;All testing was done on pre-production &lt;a href="https://frame.work/desktop"&gt;Framework Desktop&lt;/a&gt; systems with an AMD Ryzen Max+ 395 (Strix Halo)/128GB LPDDR5x-8000 configuration. (Thanks Nirav, Alexandru, and co!)&lt;/p&gt; &lt;p&gt;Exact testing/system details are in the results folders, but roughly these are running:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Close to production BIOS/EC&lt;/li&gt; &lt;li&gt;Relatively up-to-date kernels: 6.15.5-arch1-1/6.15.6-arch1-1&lt;/li&gt; &lt;li&gt;Recent TheRock/ROCm-7.0 nightly builds with Strix Halo (gfx1151) kernels&lt;/li&gt; &lt;li&gt;Recent llama.cpp builds (eg b5863 from 2005-07-10)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Just to get a ballpark on the hardware:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~215 GB/s max GPU MBW out of a 256 GB/s theoretical (256-bit 8000 MT/s)&lt;/li&gt; &lt;li&gt;theoretical 59 FP16 TFLOPS (VPOD/WMMA) on RDNA 3.5 (gfx11); effective is &lt;em&gt;much&lt;/em&gt; lower&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;h1&gt;Prompt Processing (pp) Performance&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f"&gt;https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Name&lt;/th&gt; &lt;th align="left"&gt;Architecture&lt;/th&gt; &lt;th align="left"&gt;Weights (B)&lt;/th&gt; &lt;th align="left"&gt;Active (B)&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;Flags&lt;/th&gt; &lt;th align="left"&gt;pp512&lt;/th&gt; &lt;th align="left"&gt;tg128&lt;/th&gt; &lt;th align="left"&gt;Memory (Max MiB)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 2 7B Q4_0&lt;/td&gt; &lt;td align="left"&gt;Llama 2&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;998.0&lt;/td&gt; &lt;td align="left"&gt;46.5&lt;/td&gt; &lt;td align="left"&gt;4237&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 2 7B Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;Llama 2&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;HIP&lt;/td&gt; &lt;td align="left"&gt;hipBLASLt&lt;/td&gt; &lt;td align="left"&gt;906.1&lt;/td&gt; &lt;td align="left"&gt;40.8&lt;/td&gt; &lt;td align="left"&gt;4720&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Shisa V2 8B i1-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;Llama 3&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;HIP&lt;/td&gt; &lt;td align="left"&gt;hipBLASLt&lt;/td&gt; &lt;td align="left"&gt;878.2&lt;/td&gt; &lt;td align="left"&gt;37.2&lt;/td&gt; &lt;td align="left"&gt;5308&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 3 30B-A3B UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;Qwen 3 MoE&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1&lt;/td&gt; &lt;td align="left"&gt;604.8&lt;/td&gt; &lt;td align="left"&gt;66.3&lt;/td&gt; &lt;td align="left"&gt;17527&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral Small 3.1 UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;Mistral 3&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;HIP&lt;/td&gt; &lt;td align="left"&gt;hipBLASLt&lt;/td&gt; &lt;td align="left"&gt;316.9&lt;/td&gt; &lt;td align="left"&gt;13.6&lt;/td&gt; &lt;td align="left"&gt;14638&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hunyuan-A13B UD-Q6_K_XL&lt;/td&gt; &lt;td align="left"&gt;Hunyuan MoE&lt;/td&gt; &lt;td align="left"&gt;80&lt;/td&gt; &lt;td align="left"&gt;13&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1&lt;/td&gt; &lt;td align="left"&gt;270.5&lt;/td&gt; &lt;td align="left"&gt;17.1&lt;/td&gt; &lt;td align="left"&gt;68785&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 4 Scout UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;Llama 4 MoE&lt;/td&gt; &lt;td align="left"&gt;109&lt;/td&gt; &lt;td align="left"&gt;17&lt;/td&gt; &lt;td align="left"&gt;HIP&lt;/td&gt; &lt;td align="left"&gt;hipBLASLt&lt;/td&gt; &lt;td align="left"&gt;264.1&lt;/td&gt; &lt;td align="left"&gt;17.2&lt;/td&gt; &lt;td align="left"&gt;59720&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Shisa V2 70B i1-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;Llama 3&lt;/td&gt; &lt;td align="left"&gt;70&lt;/td&gt; &lt;td align="left"&gt;70&lt;/td&gt; &lt;td align="left"&gt;HIP rocWMMA&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;94.7&lt;/td&gt; &lt;td align="left"&gt;4.5&lt;/td&gt; &lt;td align="left"&gt;41522&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;dots1 UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;dots1 MoE&lt;/td&gt; &lt;td align="left"&gt;142&lt;/td&gt; &lt;td align="left"&gt;14&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1 b=256&lt;/td&gt; &lt;td align="left"&gt;63.1&lt;/td&gt; &lt;td align="left"&gt;20.6&lt;/td&gt; &lt;td align="left"&gt;84077&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Text Generation (tg) Performance&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7"&gt;https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Name&lt;/th&gt; &lt;th align="left"&gt;Architecture&lt;/th&gt; &lt;th align="left"&gt;Weights (B)&lt;/th&gt; &lt;th align="left"&gt;Active (B)&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;Flags&lt;/th&gt; &lt;th align="left"&gt;pp512&lt;/th&gt; &lt;th align="left"&gt;tg128&lt;/th&gt; &lt;th align="left"&gt;Memory (Max MiB)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 3 30B-A3B UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;Qwen 3 MoE&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;b=256&lt;/td&gt; &lt;td align="left"&gt;591.1&lt;/td&gt; &lt;td align="left"&gt;72.0&lt;/td&gt; &lt;td align="left"&gt;17377&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 2 7B Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;Llama 2&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1&lt;/td&gt; &lt;td align="left"&gt;620.9&lt;/td&gt; &lt;td align="left"&gt;47.9&lt;/td&gt; &lt;td align="left"&gt;4463&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 2 7B Q4_0&lt;/td&gt; &lt;td align="left"&gt;Llama 2&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1&lt;/td&gt; &lt;td align="left"&gt;1014.1&lt;/td&gt; &lt;td align="left"&gt;45.8&lt;/td&gt; &lt;td align="left"&gt;4219&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Shisa V2 8B i1-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;Llama 3&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1&lt;/td&gt; &lt;td align="left"&gt;614.2&lt;/td&gt; &lt;td align="left"&gt;42.0&lt;/td&gt; &lt;td align="left"&gt;5333&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;dots1 UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;dots1 MoE&lt;/td&gt; &lt;td align="left"&gt;142&lt;/td&gt; &lt;td align="left"&gt;14&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1 b=256&lt;/td&gt; &lt;td align="left"&gt;63.1&lt;/td&gt; &lt;td align="left"&gt;20.6&lt;/td&gt; &lt;td align="left"&gt;84077&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 4 Scout UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;Llama 4 MoE&lt;/td&gt; &lt;td align="left"&gt;109&lt;/td&gt; &lt;td align="left"&gt;17&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1 b=256&lt;/td&gt; &lt;td align="left"&gt;146.1&lt;/td&gt; &lt;td align="left"&gt;19.3&lt;/td&gt; &lt;td align="left"&gt;59917&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hunyuan-A13B UD-Q6_K_XL&lt;/td&gt; &lt;td align="left"&gt;Hunyuan MoE&lt;/td&gt; &lt;td align="left"&gt;80&lt;/td&gt; &lt;td align="left"&gt;13&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1 b=256&lt;/td&gt; &lt;td align="left"&gt;223.9&lt;/td&gt; &lt;td align="left"&gt;17.1&lt;/td&gt; &lt;td align="left"&gt;68608&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral Small 3.1 UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;Mistral 3&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1&lt;/td&gt; &lt;td align="left"&gt;119.6&lt;/td&gt; &lt;td align="left"&gt;14.3&lt;/td&gt; &lt;td align="left"&gt;14540&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Shisa V2 70B i1-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;Llama 3&lt;/td&gt; &lt;td align="left"&gt;70&lt;/td&gt; &lt;td align="left"&gt;70&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1&lt;/td&gt; &lt;td align="left"&gt;26.4&lt;/td&gt; &lt;td align="left"&gt;5.0&lt;/td&gt; &lt;td align="left"&gt;41456&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Testing Notes&lt;/h1&gt; &lt;p&gt;The best overall backend and flags were chosen for each model family tested. You can see that often times the best backend for prefill vs token generation differ. Full results for each model (including the pp/tg graphs for different context lengths for all tested backend variations) are available for review in their respective folders as which backend is the best performing will depend on your exact use-case.&lt;/p&gt; &lt;p&gt;There's a lot of performance still on the table when it comes to pp especially. Since these results should be close to optimal for when they were tested, I might add dates to the table (adding kernel, ROCm, and llama.cpp build#'s might be a bit much).&lt;/p&gt; &lt;p&gt;One thing worth pointing out is that pp has improved significantly on some models since I last tested. For example, back in May, pp512 for Qwen3 30B-A3B was 119 t/s (Vulkan) and it's now 605 t/s. Similarly, Llama 4 Scout has a pp512 of 103 t/s, and is now 173 t/s, although the HIP backend is significantly faster at 264 t/s.&lt;/p&gt; &lt;p&gt;Unlike last time, I won't be taking any model testing requests as these sweeps take quite a while to run - I feel like there are enough 395 systems out there now and the repo linked at top includes the full scripts to allow anyone to replicate (and can be easily adapted for other backends or to run with different hardware).&lt;/p&gt; &lt;p&gt;For testing, the HIP backend, I highly recommend trying &lt;code&gt;ROCBLAS_USE_HIPBLASLT=1&lt;/code&gt; as that is almost always faster than the default rocBLAS. If you are OK with occasionally hitting the reboot switch, you might also want to test in combination with (as long as you have the gfx1100 kernels installed) &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=11.0.0&lt;/code&gt; - in prior testing I've found the gfx1100 kernels to be up 2X faster than gfx1151 kernels... 🤔&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomfoo2"&gt; /u/randomfoo2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T11:00:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6ct7u</id>
    <title>Qwen3 235B-A22B 2507 :: Q3_K_L :: One shot HTML game :: 4090 + 128GB DDR5 @6000</title>
    <updated>2025-07-22T12:31:02+00:00</updated>
    <author>
      <name>/u/aidanjustsayin</name>
      <uri>https://old.reddit.com/user/aidanjustsayin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ct7u/qwen3_235ba22b_2507_q3_k_l_one_shot_html_game/"&gt; &lt;img alt="Qwen3 235B-A22B 2507 :: Q3_K_L :: One shot HTML game :: 4090 + 128GB DDR5 @6000" src="https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47d0469ef510365b725dc72ec2ab1d98d266e09a" title="Qwen3 235B-A22B 2507 :: Q3_K_L :: One shot HTML game :: 4090 + 128GB DDR5 @6000" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently upgraded my desktop RAM given the large MoE models coming out and I was excited for the maiden voyage to be yesterday's release! I'll put the prompt and code in a comment, this is sort of a test of ability but more so I wanted to confirm Q3_K_L is runnable (though slow) for anybody with similar PC specs and produces something usable!&lt;/p&gt; &lt;p&gt;I used LM Studio for loading the model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Context: 4096 (default)&lt;/li&gt; &lt;li&gt;GPU Offload: 18 / 94&lt;/li&gt; &lt;li&gt;CPU Thread Pool: 16&lt;/li&gt; &lt;li&gt;... all else default besides ...&lt;/li&gt; &lt;li&gt;Flash Attention: On&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;When loaded, it used up 23.3GB of VRAM and ~80GB of RAM.&lt;/p&gt; &lt;p&gt;Basic Generation stats: 5.52 tok/sec • 2202 tokens • 0.18s to first token&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aidanjustsayin"&gt; /u/aidanjustsayin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1x5u9hrp5fef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ct7u/qwen3_235ba22b_2507_q3_k_l_one_shot_html_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ct7u/qwen3_235ba22b_2507_q3_k_l_one_shot_html_game/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T12:31:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6bddm</id>
    <title>AMD's Strix Halo "Ryzen AI MAX" APUs Come To DIY PC Builders With New MoDT "Mini-ITX" Motherboards, Equipped With Up To 128 GB of LPDDR5X Memory</title>
    <updated>2025-07-22T11:18:22+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6bddm/amds_strix_halo_ryzen_ai_max_apus_come_to_diy_pc/"&gt; &lt;img alt="AMD's Strix Halo &amp;quot;Ryzen AI MAX&amp;quot; APUs Come To DIY PC Builders With New MoDT &amp;quot;Mini-ITX&amp;quot; Motherboards, Equipped With Up To 128 GB of LPDDR5X Memory" src="https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c63f2527e38ed9f9fb783cd700b8e831108fe01" title="AMD's Strix Halo &amp;quot;Ryzen AI MAX&amp;quot; APUs Come To DIY PC Builders With New MoDT &amp;quot;Mini-ITX&amp;quot; Motherboards, Equipped With Up To 128 GB of LPDDR5X Memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-strix-halo-ryzen-ai-max-apus-diy-pc-new-modt-mini-itx-motherboards-128-gb-lpddr5x-memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6bddm/amds_strix_halo_ryzen_ai_max_apus_come_to_diy_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6bddm/amds_strix_halo_ryzen_ai_max_apus_come_to_diy_pc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T11:18:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1m641zg</id>
    <title>MegaTTS 3 Voice Cloning is Here</title>
    <updated>2025-07-22T03:53:37+00:00</updated>
    <author>
      <name>/u/mrfakename0</name>
      <uri>https://old.reddit.com/user/mrfakename0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m641zg/megatts_3_voice_cloning_is_here/"&gt; &lt;img alt="MegaTTS 3 Voice Cloning is Here" src="https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=13bd3c86a79666218395f17439b714df6a5fc52c" title="MegaTTS 3 Voice Cloning is Here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MegaTTS 3 voice cloning is here!&lt;/p&gt; &lt;p&gt;For context: a while back, ByteDance released MegaTTS 3 (with exceptional voice cloning capabilities), but for various reasons, they decided not to release the WavVAE encoder necessary for voice cloning to work.&lt;/p&gt; &lt;p&gt;Recently, a WavVAE encoder compatible with MegaTTS 3 was released by ACoderPassBy on ModelScope: &lt;a href="https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT"&gt;https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT&lt;/a&gt; with quite promising results.&lt;/p&gt; &lt;p&gt;I reuploaded the weights to Hugging Face: &lt;a href="https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning"&gt;https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And put up a quick Gradio demo to try it out: &lt;a href="https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning"&gt;https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Overall looks quite impressive - excited to see that we can finally do voice cloning with MegaTTS 3!&lt;/p&gt; &lt;p&gt;h/t to MysteryShack on the StyleTTS 2 Discord for info about the WavVAE encoder&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrfakename0"&gt; /u/mrfakename0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m641zg/megatts_3_voice_cloning_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m641zg/megatts_3_voice_cloning_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T03:53:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6cfzi</id>
    <title>The ik_llama.cpp repository is back! \o/</title>
    <updated>2025-07-22T12:13:32+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ikawrakow/ik_llama.cpp"&gt;https://github.com/ikawrakow/ik_llama.cpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Friendly reminder to back up all the things!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T12:13:32+00:00</published>
  </entry>
</feed>
