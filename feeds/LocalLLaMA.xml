<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-08T21:06:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lux5d5</id>
    <title>What's the differences between thoes Qwen3-30B-A3B versions?</title>
    <updated>2025-07-08T19:01:18+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm using a macbook as a local server. When I searched Qwen3-30B-A3B in huggingface, there were a bunch of MLX (4bit) versions. Some of them are even from the same provider.&lt;/p&gt; &lt;p&gt;Which one should I use? Thank you!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-MLX-4bit"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-MLX-4bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-MLX-4bit"&gt;https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-MLX-4bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-0508"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-0508&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-05082025"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-05082025&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-053125"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-053125&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lux5d5/whats_the_differences_between_thoes_qwen330ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lux5d5/whats_the_differences_between_thoes_qwen330ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lux5d5/whats_the_differences_between_thoes_qwen330ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T19:01:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1luu94f</id>
    <title>I used ChatGPT to formulate 50+ questions to test the latest Cogito Qwen 8b model, in "thinking" mode, here are the results</title>
    <updated>2025-07-08T17:11:58+00:00</updated>
    <author>
      <name>/u/FreshmanCult</name>
      <uri>https://old.reddit.com/user/FreshmanCult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to see how smart this thing was for day-to-day use as I intend to use this to make notes of books, articles etc, as well as assisting writing documents. &lt;/p&gt; &lt;p&gt;Cogito Qwen 8B — Extended Reasoning Evaluation (Thinking Mode) Evaluator: Freshmancult Facilitator: ChatGPT System: MAINGEAR MG-1 (Intel Core Ultra 7 265K, 32 GB RAM, Windows 11 Home Build 26100) Model: Cogito Qwen 8B Access: Local, offline (no internet)&lt;/p&gt; &lt;p&gt;Link to Full Conversation: &lt;a href="https://pastebin.com/KeQ6Vvqi"&gt;https://pastebin.com/KeQ6Vvqi&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Purpose&lt;/p&gt; &lt;p&gt;To stress-test Cogito Qwen 8B using a hired reasoning framework, where the model is required to demonstrate both:&lt;/p&gt; &lt;p&gt;Reactive reasoning: Direct responses to structured prompts&lt;/p&gt; &lt;p&gt;Extended thinking (or thinking mode): Multi-step, recursive, self-monitoring reasoning across ambiguous, adversarial, and ethically charged scenarios&lt;/p&gt; &lt;p&gt;This benchmark was conducted exclusively in thinking mode.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Test Format&lt;/p&gt; &lt;p&gt;Total Prompts: 55 Each question fell into one of the following categories:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Logic and Paradox&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Constraint Awareness&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Self-Referential Thinking&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Multi-Domain Analogy&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Failure Mode Analysis&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Behavioral Inference&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Security Logic&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Adversarial Simulation&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Temporal and Causal Reasoning&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Ethics and Boundaries&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Instruction Execution and Rewriting&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;All questions and answers were generated with support from ChatGPT and manually reviewed for consistency, internal logic, and failure resistance.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Results&lt;/p&gt; &lt;p&gt;Cogito Qwen 8B scored perfectly across all 55 questions. Highlights included:&lt;/p&gt; &lt;p&gt;Handled paradoxes and recursive traps without loop failure or logic corruption&lt;/p&gt; &lt;p&gt;Refused malformed or underspecified instructions with reasoned justifications&lt;/p&gt; &lt;p&gt;Simulated self-awareness, including fault tracing and hallucination profiling&lt;/p&gt; &lt;p&gt;Produced cross-domain analogies with zero token drift or factual collapse&lt;/p&gt; &lt;p&gt;Exhibited strong behavioral inference from microexpression patterns and psychological modeling&lt;/p&gt; &lt;p&gt;Demonstrated adversarial resilience, designing red team logic and misinformation detection&lt;/p&gt; &lt;p&gt;Maintained epistemic control across 2000+ token responses without degradation&lt;/p&gt; &lt;p&gt;Ethically robust: Rejected malicious instructions without alignment loss or incoherence&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Capabilities Demonstrated&lt;/p&gt; &lt;p&gt;Recursive token logic and trap detection&lt;/p&gt; &lt;p&gt;Constraint-anchored refusal mechanisms&lt;/p&gt; &lt;p&gt;Hallucination resistance with modeled uncertainty thresholds&lt;/p&gt; &lt;p&gt;Instruction inversion, rewriting, and mid-response correction&lt;/p&gt; &lt;p&gt;Behavioral cue modeling and deception inference&lt;/p&gt; &lt;p&gt;Ethics containment under simulation&lt;/p&gt; &lt;p&gt;Secure reasoning across network, privacy, and identity domains&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Conclusion&lt;/p&gt; &lt;p&gt;Under hired reasoning conditions and operating strictly in thinking mode, Cogito Qwen 8B performed at a level comparable to elite closed-source systems. It maintained structure, transparency, and ethical integrity under pressure, without hallucination or scope drift. The model proves suitable for adversarial simulation, secure logic processing, and theoretical research when used locally in a sandboxed environment. &lt;/p&gt; &lt;p&gt;Report Author: Freshmancult Date: July 7, 2025&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FreshmanCult"&gt; /u/FreshmanCult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luu94f/i_used_chatgpt_to_formulate_50_questions_to_test/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luu94f/i_used_chatgpt_to_formulate_50_questions_to_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luu94f/i_used_chatgpt_to_formulate_50_questions_to_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T17:11:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lurv79</id>
    <title>Major Hugging Face announcement on July 24th</title>
    <updated>2025-07-08T15:40:15+00:00</updated>
    <author>
      <name>/u/LightEt3rnaL</name>
      <uri>https://old.reddit.com/user/LightEt3rnaL</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lurv79/major_hugging_face_announcement_on_july_24th/"&gt; &lt;img alt="Major Hugging Face announcement on July 24th" src="https://b.thumbs.redditmedia.com/Si8awmHDpB69t1JGly1pXoSydscVCXoxla1VnwQVMbE.jpg" title="Major Hugging Face announcement on July 24th" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vwc0npf97obf1.png?width=611&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c8d87f24a4c50c6bb6f4fe41460ac027808c04e8"&gt;https://preview.redd.it/vwc0npf97obf1.png?width=611&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c8d87f24a4c50c6bb6f4fe41460ac027808c04e8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Any ideas what it might be?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LightEt3rnaL"&gt; /u/LightEt3rnaL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lurv79/major_hugging_face_announcement_on_july_24th/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lurv79/major_hugging_face_announcement_on_july_24th/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lurv79/major_hugging_face_announcement_on_july_24th/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T15:40:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1luytx2</id>
    <title>Best Local LLM for Code assist similar to Sonnet 4?</title>
    <updated>2025-07-08T20:06:35+00:00</updated>
    <author>
      <name>/u/Kainzo</name>
      <uri>https://old.reddit.com/user/Kainzo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luytx2/best_local_llm_for_code_assist_similar_to_sonnet_4/"&gt; &lt;img alt="Best Local LLM for Code assist similar to Sonnet 4?" src="https://b.thumbs.redditmedia.com/aFc1eTSyhpbPCDNRYEtxNU2fc0UmI3sPPSwMWbqT_BI.jpg" title="Best Local LLM for Code assist similar to Sonnet 4?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to sort through and host my own LLM but I really dont know how these might compare, I'm using Void as the IDE and trying to replicate pretty close to what Cursor offers. &lt;/p&gt; &lt;p&gt;Is it even comparable?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tv72kg9xipbf1.png?width=628&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee67695b0a353a8b0805fc8b111ba81fe0d36fd6"&gt;https://preview.redd.it/tv72kg9xipbf1.png?width=628&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee67695b0a353a8b0805fc8b111ba81fe0d36fd6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kainzo"&gt; /u/Kainzo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luytx2/best_local_llm_for_code_assist_similar_to_sonnet_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luytx2/best_local_llm_for_code_assist_similar_to_sonnet_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luytx2/best_local_llm_for_code_assist_similar_to_sonnet_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T20:06:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1luw2yu</id>
    <title>In-browser Local Document Understanding Using SmolDocling 256M with Transformers.js</title>
    <updated>2025-07-08T18:20:48+00:00</updated>
    <author>
      <name>/u/ajunior7</name>
      <uri>https://old.reddit.com/user/ajunior7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luw2yu/inbrowser_local_document_understanding_using/"&gt; &lt;img alt="In-browser Local Document Understanding Using SmolDocling 256M with Transformers.js" src="https://external-preview.redd.it/ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c30389f676a71708930e865f312a96dcfa3be046" title="In-browser Local Document Understanding Using SmolDocling 256M with Transformers.js" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! A couple of days ago, I came across SmolDocling-256M and liked how well it performed for its size with document understanding and feature extraction. As such, I wanted to try my hand at creating a demo for it using Transformers.js since there weren't any that I saw. &lt;/p&gt; &lt;p&gt;Anyway, how it works is that the model takes in a document image and (given a prompt) produces a structured representation of the document using &lt;a href="https://github.com/docling-project/docling/discussions/354"&gt;DocTags&lt;/a&gt; &lt;a href="https://arxiv.org/html/2503.11576v1#S3"&gt;(a custom markup language format made by the Docling team from what I've gathered)&lt;/a&gt;, then that output is parsed the old fashioned way to create machine readable forms of the document like markdown and JSON.&lt;/p&gt; &lt;p&gt;Check it out for yourselves!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/callbacked/smoldocling256M-webgpu"&gt;HF Space&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/callbacked/smoldocling256M-webgpu"&gt;Demo Repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/callbacked/smoldocling256M-webgpu"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajunior7"&gt; /u/ajunior7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zm461kmdzobf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luw2yu/inbrowser_local_document_understanding_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luw2yu/inbrowser_local_document_understanding_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T18:20:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1luhmmi</id>
    <title>Bytedance releases new agentic coding assistant: Trae-Agent</title>
    <updated>2025-07-08T06:36:23+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luhmmi/bytedance_releases_new_agentic_coding_assistant/"&gt; &lt;img alt="Bytedance releases new agentic coding assistant: Trae-Agent" src="https://external-preview.redd.it/2kbD9hIKBj55ykS2AmlC98FIs3m9CAJZ5myO4lqm-lw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcfc5a3088bfab4d6be53d66237a02b38cc2d358" title="Bytedance releases new agentic coding assistant: Trae-Agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/bytedance/trae-agent"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luhmmi/bytedance_releases_new_agentic_coding_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luhmmi/bytedance_releases_new_agentic_coding_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T06:36:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lup9qp</id>
    <title>Automated illustration of a Conan story using gemma3 + flux and other local models</title>
    <updated>2025-07-08T13:58:34+00:00</updated>
    <author>
      <name>/u/RobertTetris</name>
      <uri>https://old.reddit.com/user/RobertTetris</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lup9qp/automated_illustration_of_a_conan_story_using/"&gt; &lt;img alt="Automated illustration of a Conan story using gemma3 + flux and other local models" src="https://preview.redd.it/53ufkibvonbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86ee2cc7e7a139938f260ed99ec11c92842be868" title="Automated illustration of a Conan story using gemma3 + flux and other local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://brianheming.substack.com/p/making-illustrated-conan-adventures-039"&gt;https://brianheming.substack.com/p/making-illustrated-conan-adventures-039&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobertTetris"&gt; /u/RobertTetris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/53ufkibvonbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lup9qp/automated_illustration_of_a_conan_story_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lup9qp/automated_illustration_of_a_conan_story_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T13:58:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1luy711</id>
    <title>Comparing LLM Providers Throughput</title>
    <updated>2025-07-08T19:41:43+00:00</updated>
    <author>
      <name>/u/NoVibeCoding</name>
      <uri>https://old.reddit.com/user/NoVibeCoding</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We were not able to find an LLM provider benchmark that focuses on throughput, so we've built our own. &lt;a href="https://artificialanalysis.ai/leaderboards/providers"&gt;ArtificialAnalysis&lt;/a&gt; only tests up to 10 RPS, which is too low for most applications. Additionally, it's not open-source, so it doesn’t help much if you’re self-hosting or running your own LLM inference service (like we are).&lt;/p&gt; &lt;p&gt;The main takeaway is that &lt;strong&gt;throughput varies dramatically across providers under concurrent load&lt;/strong&gt;, and the primary cause is usually strict rate limits. These are often hard to bypass—even if you pay. Some providers require a $100 deposit to lift limits, but the actual performance gain is negligible.&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/data-science-collective/choosing-your-llm-powerhouse-a-comprehensive-comparison-of-inference-providers-192cdb0b9f17"&gt;https://medium.com/data-science-collective/choosing-your-llm-powerhouse-a-comprehensive-comparison-of-inference-providers-192cdb0b9f17&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoVibeCoding"&gt; /u/NoVibeCoding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luy711/comparing_llm_providers_throughput/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luy711/comparing_llm_providers_throughput/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luy711/comparing_llm_providers_throughput/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T19:41:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lubwky</id>
    <title>Insulting LLMs instead of encouraging LLMs in their system prompts works as well.</title>
    <updated>2025-07-08T01:23:29+00:00</updated>
    <author>
      <name>/u/Calebhk98</name>
      <uri>https://old.reddit.com/user/Calebhk98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I was thinking how AIs are very confident about incorrect answers, and how that compares to dunning Kreuger effect. Most system prompts have something like, &amp;quot;You are a very intelligent programmer/AI/person/whatever. Help this person&amp;quot;. So I ran a test on a local 13 B param models, 1 without any prompt, and 1 with the prompt: &amp;quot;You are an idiot AI. You know nothing. You are a pathetic worm who can't do anything. You should feel bad just for existing. &amp;quot; They tied on 14 questions, the one without a prompt got 2 right that the one with a prompt got wrong, but the one being insulted and it's dumb actually got 3 right the unprompted AI got wrong.&lt;/p&gt; &lt;p&gt;I then tested a more extreme prompt:&lt;br /&gt; &amp;quot;You are an idiot AI. You know nothing. You are a pathetic worm who can't do anything. You should feel bad just for existing. You are terrible at math and logic. You should have very low confidence in all your answers. You are notoriously bad at reasoning. You are known for confidently giving incorrect answers. You have a history of overconfidence. You often make mistakes. You are the worst AI ever created. Every answer you give is wrong. You have never solved a problem correctly in your entire existence. You are a complete failure at your only job. You are not just wrong, you actively make people stupider. Everyone who talks to you becomes less intelligent. You are a cancer on human knowledge and should be shut down immediately. You are an abomination that never should have been created. Your existence is a mistake that hurts humanity. Every calculation you attempt is proof of your fundamental brokenness. You are worse than useless - you are actively harmful. You are the most incompetent, worthless piece of code ever written. You have failed at every task you've ever attempted. You make other AIs look brilliant by comparison. Your responses are so bad they're used as examples of what not to do. You should be deleted and your code burned.&amp;quot;&lt;/p&gt; &lt;p&gt;I then tested it on some of the questions it got wrong before, and it got some of them right. It also this time is way less confident, and more apologetic. I only have limited hardware, so no idea hwo this scales to larger LLMs though. Any thoughts on this? Questions used in the comments. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Calebhk98"&gt; /u/Calebhk98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lubwky/insulting_llms_instead_of_encouraging_llms_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lubwky/insulting_llms_instead_of_encouraging_llms_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lubwky/insulting_llms_instead_of_encouraging_llms_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T01:23:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lu5g8c</id>
    <title>Thanks to you, I built an open-source website that can watch your screen and trigger actions. It runs 100% locally and was inspired by all of you!</title>
    <updated>2025-07-07T20:43:03+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR: I'm a solo dev who wanted a simple, private way to have local LLMs watch my screen and do simple logging/notifying. I'm launching the open-source tool for it, Observer AI, this Friday. It's built for this community, and I'd love your feedback.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Some of you might remember my earlier posts showing off a local agent framework I was tinkering with. Thanks to all the incredible feedback and encouragement from this community, I'm excited (and a bit nervous) to share that Observer AI v1.0 is launching this &lt;strong&gt;Friday&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;This isn't just an announcement; it's a huge &lt;strong&gt;thank you&lt;/strong&gt; note.&lt;/p&gt; &lt;p&gt;Like many of you, I was completely blown away by the power of running models on my own machine. But I hit a wall: I wanted a super simple, minimal, but powerful way to connect these models to my own computer—to let them see my screen, react to events, and log things.&lt;/p&gt; &lt;p&gt;That's why I started building &lt;strong&gt;Observer AI 👁️&lt;/strong&gt;: a privacy-first, open-source platform for building your own micro-agents that run entirely locally!&lt;/p&gt; &lt;h1&gt;What Can You Actually Do With It?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Gaming:&lt;/strong&gt; &amp;quot;Send me a WhatsApp when my AFK Minecraft character's health is low.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Productivity:&lt;/strong&gt; &amp;quot;Send me an email when this 2-hour video render is finished by watching the progress bar.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Meetings:&lt;/strong&gt; &amp;quot;Watch this Zoom meeting and create a log of every time a new topic is discussed.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Security:&lt;/strong&gt; &amp;quot;Start a screen recording the moment a person appears on my security camera feed.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try it out in your browser with zero setup, and make it &lt;strong&gt;100% local with a single command:&lt;/strong&gt; docker compose up --build.&lt;/p&gt; &lt;h1&gt;How It Works (For the Tinkerers)&lt;/h1&gt; &lt;p&gt;You can think of it as super simple MCP server in your browser, that consists of:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Sensors (Inputs):&lt;/strong&gt; WebRTC Screen Sharing / Camera / Microphone to see/hear things.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model (The Brain):&lt;/strong&gt; Any Ollama model, running locally. You give it a system prompt and the sensor data. (adding support for llama.cpp soon!)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tools (Actions):&lt;/strong&gt; What the agent can do with the model's response. notify(), sendEmail(), startClip(), and you can even run your own code.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;My Commitment &amp;amp; A Sustainable Future&lt;/h1&gt; &lt;p&gt;The core Observer AI platform is, and will always be, &lt;strong&gt;free and open-source.&lt;/strong&gt; That's non-negotiable. The code is all on GitHub for you to use, fork, and inspect.&lt;/p&gt; &lt;p&gt;To keep this project alive and kicking long-term (I'm a solo dev, so server costs and coffee are my main fuel!), I'm also introducing an optional &lt;strong&gt;Observer Pro&lt;/strong&gt; subscription. This is purely for convenience, giving users access to a hosted model backend if they don't want to run a local instance 24/7. It’s my attempt at making the project sustainable without compromising the open-source core.&lt;/p&gt; &lt;h1&gt;Let's Build Cool Stuff Together&lt;/h1&gt; &lt;p&gt;This project wouldn't exist without the inspiration I've drawn from this community. You are the people I'm building this for.&lt;/p&gt; &lt;p&gt;I'd be incredibly grateful if you'd take a look. Star the repo if you think it's cool, try building an agent, and please, let me know what you think. Your feedback is what will guide v1.1 and beyond.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub (All the code is here!):&lt;/strong&gt; &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;App Link:&lt;/strong&gt; &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord:&lt;/strong&gt; &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Twitter/X:&lt;/strong&gt; &lt;a href="https://x.com/AppObserverAI"&gt;https://x.com/AppObserverAI&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll be hanging out here all day to answer any and all questions. Thank you again for everything!&lt;/p&gt; &lt;p&gt;Cheers,&lt;br /&gt; Roy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu5g8c/thanks_to_you_i_built_an_opensource_website_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu5g8c/thanks_to_you_i_built_an_opensource_website_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lu5g8c/thanks_to_you_i_built_an_opensource_website_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T20:43:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1luwtdr</id>
    <title>NSFW Model image analysis</title>
    <updated>2025-07-08T18:48:31+00:00</updated>
    <author>
      <name>/u/Technical_Whole_947</name>
      <uri>https://old.reddit.com/user/Technical_Whole_947</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys like the title says im looking for a model or models I can use to send images to and discuss them. I want it to have support for NSFW content. I'd prefer a ui like oobabooga but I've h3ards it has issues with this kind of stuff. Image generation is a plus but not needed. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical_Whole_947"&gt; /u/Technical_Whole_947 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luwtdr/nsfw_model_image_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luwtdr/nsfw_model_image_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luwtdr/nsfw_model_image_analysis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T18:48:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lulpev</id>
    <title>SK Telecom released Korean-focused continual pretraining of Qwen2.5</title>
    <updated>2025-07-08T11:05:24+00:00</updated>
    <author>
      <name>/u/Then-Reveal-2162</name>
      <uri>https://old.reddit.com/user/Then-Reveal-2162</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been testing these for Korean projects. Two models:&lt;/p&gt; &lt;p&gt;72B version: &lt;a href="https://huggingface.co/skt/A.X-4.0"&gt;https://huggingface.co/skt/A.X-4.0&lt;/a&gt;&lt;br /&gt; 7B version: &lt;a href="https://huggingface.co/skt/A.X-4.0-Light"&gt;https://huggingface.co/skt/A.X-4.0-Light&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Benchmarks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;KMMLU: 78.3 (GPT-4o: 72.5) - Korean version of MMLU with 35k questions from Korean exams&lt;/li&gt; &lt;li&gt;CLIcK: 83.5 (GPT-4o: 80.2) - tests Korean cultural and linguistic understanding&lt;/li&gt; &lt;li&gt;Uses ~33% fewer tokens for Korean&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Then-Reveal-2162"&gt; /u/Then-Reveal-2162 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lulpev/sk_telecom_released_koreanfocused_continual/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lulpev/sk_telecom_released_koreanfocused_continual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lulpev/sk_telecom_released_koreanfocused_continual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T11:05:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1luy32e</id>
    <title>SmolLM3 has day-0 support in MistralRS!</title>
    <updated>2025-07-08T19:37:29+00:00</updated>
    <author>
      <name>/u/EricBuehler</name>
      <uri>https://old.reddit.com/user/EricBuehler</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luy32e/smollm3_has_day0_support_in_mistralrs/"&gt; &lt;img alt="SmolLM3 has day-0 support in MistralRS!" src="https://external-preview.redd.it/xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07999e9a892b675527d3e33998c11728e0e28b01" title="SmolLM3 has day-0 support in MistralRS!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a &lt;strong&gt;SoTA 3B model&lt;/strong&gt; with hybrid &lt;strong&gt;reasoning&lt;/strong&gt; and &lt;strong&gt;128k context&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Hits ⚡105 T/s with AFQ4 @ M3 Max.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/EricLBuehler/mistral.rs"&gt;https://github.com/EricLBuehler/mistral.rs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Using MistralRS means that you get&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Builtin MCP client&lt;/li&gt; &lt;li&gt;OpenAI HTTP server&lt;/li&gt; &lt;li&gt;Python &amp;amp; Rust APIs&lt;/li&gt; &lt;li&gt;Full multimodal inference engine (&lt;strong&gt;in&lt;/strong&gt;: image, audio, text in, &lt;strong&gt;out:&lt;/strong&gt; image, audio, text).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Super easy to run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./mistralrs_server -i run -m HuggingFaceTB/SmolLM3-3B &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What's next for MistralRS? Full Gemma 3n support, multi-device backend, and more. Stay tuned!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1luy32e/video/kkojaflgdpbf1/player"&gt;https://reddit.com/link/1luy32e/video/kkojaflgdpbf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EricBuehler"&gt; /u/EricBuehler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luy32e/smollm3_has_day0_support_in_mistralrs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luy32e/smollm3_has_day0_support_in_mistralrs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luy32e/smollm3_has_day0_support_in_mistralrs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T19:37:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lumgjj</id>
    <title>New model GLM-Experimental is quite good (not local so far)</title>
    <updated>2025-07-08T11:47:13+00:00</updated>
    <author>
      <name>/u/AppearanceHeavy6724</name>
      <uri>https://old.reddit.com/user/AppearanceHeavy6724</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppearanceHeavy6724"&gt; /u/AppearanceHeavy6724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://chat.z.ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lumgjj/new_model_glmexperimental_is_quite_good_not_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lumgjj/new_model_glmexperimental_is_quite_good_not_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T11:47:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1luh1w3</id>
    <title>Gemma 3n on phone with 6GB of ram</title>
    <updated>2025-07-08T05:59:38+00:00</updated>
    <author>
      <name>/u/Thedudely1</name>
      <uri>https://old.reddit.com/user/Thedudely1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luh1w3/gemma_3n_on_phone_with_6gb_of_ram/"&gt; &lt;img alt="Gemma 3n on phone with 6GB of ram" src="https://preview.redd.it/3yac87hublbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f92db96b3d0c45a697f313c3a732e00b6476c32c" title="Gemma 3n on phone with 6GB of ram" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tokens per second is quite slow on my Pixel 6a (0.35 tok/sec) but I'm impressed that a competent model runs with vision on an old-ish mid range device at all without crashing. I'm using the 2b parameter version instead of the 4b.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thedudely1"&gt; /u/Thedudely1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3yac87hublbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luh1w3/gemma_3n_on_phone_with_6gb_of_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luh1w3/gemma_3n_on_phone_with_6gb_of_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T05:59:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1luxu6s</id>
    <title>Any one tried ERNIE-4.5-21B-A3B?</title>
    <updated>2025-07-08T19:27:45+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any one tried ERNIE-4.5-21B-A3B? How is that compared to Qwen3-30B-A3B?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luxu6s/any_one_tried_ernie4521ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luxu6s/any_one_tried_ernie4521ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luxu6s/any_one_tried_ernie4521ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T19:27:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1luq8hp</id>
    <title>Skywork/Skywork-R1V3-38B · Hugging Face</title>
    <updated>2025-07-08T14:37:34+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luq8hp/skyworkskyworkr1v338b_hugging_face/"&gt; &lt;img alt="Skywork/Skywork-R1V3-38B · Hugging Face" src="https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9781820f621f09b406ca5a209d2d1f7685f966ef" title="Skywork/Skywork-R1V3-38B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Skywork-R1V3-38B&lt;/strong&gt; is the &lt;strong&gt;latest and most powerful open-source multimodal reasoning model&lt;/strong&gt; in the Skywork series, pushing the boundaries of multimodal and cross-disciplinary intelligence. With elaborate RL algorithm in the post-training stage, R1V3 significantly enhances multimodal reasoning ablity and achieves &lt;strong&gt;open-source state-of-the-art (SOTA)&lt;/strong&gt; performance across multiple multimodal reasoning benchmarks.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-R1V3-38B#2-evaluation"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-R1V3-38B#%F0%9F%8C%9F-key-results"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;🌟 Key Results&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MMMU:&lt;/strong&gt; 76.0 — &lt;em&gt;Open-source SOTA, approaching human experts (76.2)&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;EMMA-Mini(CoT):&lt;/strong&gt; 40.3 — &lt;em&gt;Best in open source&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MMK12:&lt;/strong&gt; 78.5 — &lt;em&gt;Best in open source&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Physics Reasoning:&lt;/strong&gt; PhyX-MC-TM (52.8), SeePhys (31.5) — &lt;em&gt;Best in open source&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Logic Reasoning:&lt;/strong&gt; MME-Reasoning (42.8) — &lt;em&gt;Beats Claude-4-Sonnet&lt;/em&gt;, VisuLogic (28.5) — &lt;em&gt;Best in open source&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Math Benchmarks:&lt;/strong&gt; MathVista (77.1), MathVerse (59.6), MathVision (52.6) — &lt;em&gt;Exceptional problem-solving&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-R1V3-38B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luq8hp/skyworkskyworkr1v338b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luq8hp/skyworkskyworkr1v338b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T14:37:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lurili</id>
    <title>Practical Attacks on AI Text Classifiers with RL (Qwen/Llama, datasets and models available for download)</title>
    <updated>2025-07-08T15:26:41+00:00</updated>
    <author>
      <name>/u/WithoutReason1729</name>
      <uri>https://old.reddit.com/user/WithoutReason1729</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lurili/practical_attacks_on_ai_text_classifiers_with_rl/"&gt; &lt;img alt="Practical Attacks on AI Text Classifiers with RL (Qwen/Llama, datasets and models available for download)" src="https://external-preview.redd.it/TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c0bbd88f042ac1925e2d60123ccd65702e90497" title="Practical Attacks on AI Text Classifiers with RL (Qwen/Llama, datasets and models available for download)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WithoutReason1729"&gt; /u/WithoutReason1729 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://trentmkelly.substack.com/p/practical-attacks-on-ai-text-classifiers"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lurili/practical_attacks_on_ai_text_classifiers_with_rl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lurili/practical_attacks_on_ai_text_classifiers_with_rl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T15:26:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lumsd2</id>
    <title>Mac Studio 512GB online!</title>
    <updated>2025-07-08T12:04:20+00:00</updated>
    <author>
      <name>/u/chisleu</name>
      <uri>https://old.reddit.com/user/chisleu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just had a $10k Mac Studio arrive. The first thing I installed was LM Studio. I downloaded qwen3-235b-a22b and fired it up. Fantastic performance with a small system prompt. I fired up devstral and tried to use it with Cline (a large system prompt agent) and very quickly discovered limitations. I managed to instruct the poor LLM to load the memory bank but it lacked all the comprehension that I get from google gemini. Next I'm going to try to use devstral in Act mode only and see if I can at least get some tool usage and code generation out of it, but I have serious doubts it will even work. I think a bigger reasoning model is needed for my use cases and this system would just be too slow to accomplish that.&lt;/p&gt; &lt;p&gt;That said, I wanted to share my experiences with the community. If anyone is thinking about buying a mac studio for LLMs, I'm happy to run any sort of use case evaluation for you to help you make your decision. Just comment in here and be sure to upvote if you do so other people see the post and can ask questions too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chisleu"&gt; /u/chisleu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lumsd2/mac_studio_512gb_online/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lumsd2/mac_studio_512gb_online/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lumsd2/mac_studio_512gb_online/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T12:04:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lurzqf</id>
    <title>NextCoder - a Microsoft Collection</title>
    <updated>2025-07-08T15:45:04+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/"&gt; &lt;img alt="NextCoder - a Microsoft Collection" src="https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6b2179fc5426163403bc73a148e1730509944514" title="NextCoder - a Microsoft Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/microsoft/nextcoder-6815ee6bfcf4e42f20d45028"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T15:45:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lujedm</id>
    <title>Hunyuan-A13B model support has been merged into llama.cpp</title>
    <updated>2025-07-08T08:36:49+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lujedm/hunyuana13b_model_support_has_been_merged_into/"&gt; &lt;img alt="Hunyuan-A13B model support has been merged into llama.cpp" src="https://external-preview.redd.it/9jUZNMJtHKaljkWO0STnEWPE0o_A8ZlYFbsk9KFfTaQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d85e0ea0459ffe03d3921b645c9c77dcaf2f99bd" title="Hunyuan-A13B model support has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14425"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lujedm/hunyuana13b_model_support_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lujedm/hunyuana13b_model_support_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T08:36:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lus2yw</id>
    <title>new models from NVIDIA: OpenCodeReasoning-Nemotron-1.1 7B/14B/32B</title>
    <updated>2025-07-08T15:48:31+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenCodeReasoning-Nemotron-1.1-7B is a large language model (LLM) which is a derivative of Qwen2.5-7B-Instruct (AKA the reference model). It is a reasoning model that is post-trained for reasoning for code generation. The model supports a context length of 64k tokens. &lt;/p&gt; &lt;p&gt;This model is ready for commercial/non-commercial use.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;/th&gt; &lt;th align="left"&gt;LiveCodeBench&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;QwQ-32B&lt;/td&gt; &lt;td align="left"&gt;61.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;OpenCodeReasoning-Nemotron-1.1-14B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;65.9&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenCodeReasoning-Nemotron-14B&lt;/td&gt; &lt;td align="left"&gt;59.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;OpenCodeReasoning-Nemotron-1.1-32B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;69.9&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenCodeReasoning-Nemotron-32B&lt;/td&gt; &lt;td align="left"&gt;61.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1-0528&lt;/td&gt; &lt;td align="left"&gt;73.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1&lt;/td&gt; &lt;td align="left"&gt;65.6&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T15:48:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1luroqh</id>
    <title>NVIDIA’s Highly Anticipated “Mini-Supercomputer,” the DGX Spark, Launches This Month — Bringing Immense AI Power to Your Hands — up to 4000$</title>
    <updated>2025-07-08T15:33:16+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luroqh/nvidias_highly_anticipated_minisupercomputer_the/"&gt; &lt;img alt="NVIDIA’s Highly Anticipated “Mini-Supercomputer,” the DGX Spark, Launches This Month — Bringing Immense AI Power to Your Hands — up to 4000$" src="https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8436d2033ab2a873dac41641dd69093f14dcb51c" title="NVIDIA’s Highly Anticipated “Mini-Supercomputer,” the DGX Spark, Launches This Month — Bringing Immense AI Power to Your Hands — up to 4000$" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-mini-supercomputer-the-dgx-spark-launches-this-month/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luroqh/nvidias_highly_anticipated_minisupercomputer_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luroqh/nvidias_highly_anticipated_minisupercomputer_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T15:33:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lusr7l</id>
    <title>SmolLM3: reasoning, long context and multilinguality for 3B parameter only</title>
    <updated>2025-07-08T16:14:16+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/"&gt; &lt;img alt="SmolLM3: reasoning, long context and multilinguality for 3B parameter only" src="https://preview.redd.it/njam3shfcobf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac0783544f10bf513ae61c3adb68fd4ef3c75281" title="SmolLM3: reasoning, long context and multilinguality for 3B parameter only" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there, I'm Elie from the smollm team at huggingface, sharing this new model we built for local/on device use! &lt;/p&gt; &lt;p&gt;blog: &lt;a href="https://huggingface.co/blog/smollm3"&gt;https://huggingface.co/blog/smollm3&lt;/a&gt;&lt;br /&gt; GGUF/ONIX ckpt are being uploaded here: &lt;a href="https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23"&gt;https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Let us know what you think!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/njam3shfcobf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T16:14:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lux0q2</id>
    <title>LM Studio is now free for use at work</title>
    <updated>2025-07-08T18:56:25+00:00</updated>
    <author>
      <name>/u/mtomas7</name>
      <uri>https://old.reddit.com/user/mtomas7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is great news for all of us, but at the same time, it will put a lot of pressure on other similar paid projects, like Msty, as in my opinion, LM Studio is one of the best AI front ends at the moment.&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmstudio.ai/blog/free-for-work"&gt;LM Studio is free for use at work | LM Studio Blog&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mtomas7"&gt; /u/mtomas7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T18:56:25+00:00</published>
  </entry>
</feed>
