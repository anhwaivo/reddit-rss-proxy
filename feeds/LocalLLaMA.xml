<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-05T19:20:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1js14u2</id>
    <title>Coding agents?</title>
    <updated>2025-04-05T11:14:38+00:00</updated>
    <author>
      <name>/u/Leflakk</name>
      <uri>https://old.reddit.com/user/Leflakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, would like to know what you use for local coding, I tried few months ago cline with qwen2.5 coder (4x3090). Are there better options now?&lt;/p&gt; &lt;p&gt;Another dumb question: is there a simple way to connect an agentic workflow (crewai, autogen…) to a tool like cline, aider etc.?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leflakk"&gt; /u/Leflakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js14u2/coding_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js14u2/coding_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js14u2/coding_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T11:14:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsb5qa</id>
    <title>When will a smaller version of Llama 4 be released?</title>
    <updated>2025-04-05T19:14:46+00:00</updated>
    <author>
      <name>/u/CreepyMan121</name>
      <uri>https://old.reddit.com/user/CreepyMan121</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you guys know if a smaller version of Llama 4 will ever be released? Preferably a 8b - 12b parameter model that can fit on most consumer hardware? Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CreepyMan121"&gt; /u/CreepyMan121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsb5qa/when_will_a_smaller_version_of_llama_4_be_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsb5qa/when_will_a_smaller_version_of_llama_4_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsb5qa/when_will_a_smaller_version_of_llama_4_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T19:14:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrqb11</id>
    <title>Framework Desktop development units for open source AI developers</title>
    <updated>2025-04-04T23:52:01+00:00</updated>
    <author>
      <name>/u/cmonkey</name>
      <uri>https://old.reddit.com/user/cmonkey</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apologies in advance if this pushes too far into self-promotion, but when we launched Framework Desktop, AMD also announced that they would be providing 100 units to open source developers based in US/Canada to help accelerate local AI development. The application form for that is now open at &lt;a href="https://www.amd.com/en/forms/sign-up/framework-desktop-giveaway.html"&gt;https://www.amd.com/en/forms/sign-up/framework-desktop-giveaway.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm also happy to answer questions folks have around using Framework Desktop for local inference.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cmonkey"&gt; /u/cmonkey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrqb11/framework_desktop_development_units_for_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrqb11/framework_desktop_development_units_for_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jrqb11/framework_desktop_development_units_for_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T23:52:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsagyr</id>
    <title>With no update in 4 months, livebench was getting saturated and benchmaxxed, so I'm really looking forward to this one.</title>
    <updated>2025-04-05T18:45:09+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsagyr/with_no_update_in_4_months_livebench_was_getting/"&gt; &lt;img alt="With no update in 4 months, livebench was getting saturated and benchmaxxed, so I'm really looking forward to this one." src="https://preview.redd.it/9tclfgid92te1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=277ab988a5a396735a74b80fcdd3d4b467c43f25" title="With no update in 4 months, livebench was getting saturated and benchmaxxed, so I'm really looking forward to this one." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to tweet: &lt;a href="https://x.com/bindureddy/status/1908296208025870392"&gt;https://x.com/bindureddy/status/1908296208025870392&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9tclfgid92te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsagyr/with_no_update_in_4_months_livebench_was_getting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsagyr/with_no_update_in_4_months_livebench_was_getting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:45:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsb5zz</id>
    <title>Llama 4 Scout on single GPU?</title>
    <updated>2025-04-05T19:15:08+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Zuck just said that Scout is designed to run on a single GPU, but how? &lt;/p&gt; &lt;p&gt;It's an MoE model, if I'm correct. &lt;/p&gt; &lt;p&gt;You can fit 17B in single GPU but you still need to store all the experts somewhere first.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsb5zz/llama_4_scout_on_single_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsb5zz/llama_4_scout_on_single_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsb5zz/llama_4_scout_on_single_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T19:15:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1js31db</id>
    <title>Gemma3 licence</title>
    <updated>2025-04-05T13:07:06+00:00</updated>
    <author>
      <name>/u/Royal_Light_9921</name>
      <uri>https://old.reddit.com/user/Royal_Light_9921</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please explain to me like I'm 5 years old. What's wrong with their licence and what can I use it for? What is forbidden?&lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Royal_Light_9921"&gt; /u/Royal_Light_9921 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js31db/gemma3_licence/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js31db/gemma3_licence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js31db/gemma3_licence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T13:07:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrvhwk</id>
    <title>ibm-granite/granite-speech-3.2-8b · Hugging Face</title>
    <updated>2025-04-05T04:37:00+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrvhwk/ibmgranitegranitespeech328b_hugging_face/"&gt; &lt;img alt="ibm-granite/granite-speech-3.2-8b · Hugging Face" src="https://external-preview.redd.it/7tIq7fmEQHkDDxZGysXqAxxJULRvzvho5Jaa29Tj9zc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4fd879df27c798df53dc6f4fb0ddbd427697e30d" title="ibm-granite/granite-speech-3.2-8b · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Granite-speech-3.2-8b is a compact and efficient speech-language model, specifically designed for automatic speech recognition (ASR) and automatic speech translation (AST).&lt;/p&gt; &lt;p&gt;License: Apache 2.0&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ibm-granite/granite-speech-3.2-8b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrvhwk/ibmgranitegranitespeech328b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jrvhwk/ibmgranitegranitespeech328b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T04:37:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsayj9</id>
    <title>Llama 4 Reasoning</title>
    <updated>2025-04-05T19:05:58+00:00</updated>
    <author>
      <name>/u/Current-Strength-783</name>
      <uri>https://old.reddit.com/user/Current-Strength-783</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsayj9/llama_4_reasoning/"&gt; &lt;img alt="Llama 4 Reasoning" src="https://external-preview.redd.it/uRHwEOU98rM_55MIJyA8g2IjSi4Ibl9Ab1kLsdGuLI8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44af8b7574c0a4b26360d529db34c1b06ffcafcc" title="Llama 4 Reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's coming!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Current-Strength-783"&gt; /u/Current-Strength-783 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.llama.com/llama4-reasoning-is-coming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsayj9/llama_4_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsayj9/llama_4_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T19:05:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jrljxa</id>
    <title>Local LLMs are essential in a world where LLM platforms are going to get filled with ads</title>
    <updated>2025-04-04T20:16:43+00:00</updated>
    <author>
      <name>/u/TechExpert2910</name>
      <uri>https://old.reddit.com/user/TechExpert2910</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrljxa/local_llms_are_essential_in_a_world_where_llm/"&gt; &lt;img alt="Local LLMs are essential in a world where LLM platforms are going to get filled with ads" src="https://external-preview.redd.it/VS1VAWyp01_knDCN9Cj6--BP1C9fvqcewix3-g1QeRg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b34c3c22bfaf136ee38db8c12bf0b99aa321162" title="Local LLMs are essential in a world where LLM platforms are going to get filled with ads" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechExpert2910"&gt; /u/TechExpert2910 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://privacyinternational.org/long-read/5472/chatbots-adbots-sharing-your-thoughts-advertisers"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jrljxa/local_llms_are_essential_in_a_world_where_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jrljxa/local_llms_are_essential_in_a_world_where_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-04T20:16:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsauok</id>
    <title>Llama 4 Scout and Maverick Benchmarks</title>
    <updated>2025-04-05T19:01:35+00:00</updated>
    <author>
      <name>/u/Lankonk</name>
      <uri>https://old.reddit.com/user/Lankonk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsauok/llama_4_scout_and_maverick_benchmarks/"&gt; &lt;img alt="Llama 4 Scout and Maverick Benchmarks" src="https://preview.redd.it/kjhb7icjd2te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d2e8a44583f2836f2d9a88b833d4354d42a6e292" title="Llama 4 Scout and Maverick Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lankonk"&gt; /u/Lankonk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kjhb7icjd2te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsauok/llama_4_scout_and_maverick_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsauok/llama_4_scout_and_maverick_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T19:01:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1js5vwm</id>
    <title>AMD mi325x (8x) deployment and tests.</title>
    <updated>2025-04-05T15:23:38+00:00</updated>
    <author>
      <name>/u/Shivacious</name>
      <uri>https://old.reddit.com/user/Shivacious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Locallama cool people i am back again with new posts after &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1it46dv/amd_mi300x_deployment_and_tests/"&gt;amd_mi300x(8x)_deployment_and_tests&lt;/a&gt;&lt;/p&gt; &lt;p&gt;i will be soon be getting access to 8 x mi325x all connected by infinity fabric and yes 96 cores 2TB ram (the usual). &lt;/p&gt; &lt;p&gt;let me know what are you guys curious to actually test on it and i will try fulfilling every request as much as possible. from single model single gpu to multi model single gpu or even deploying r1 and v3 deploying in a single instance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shivacious"&gt; /u/Shivacious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js5vwm/amd_mi325x_8x_deployment_and_tests/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js5vwm/amd_mi325x_8x_deployment_and_tests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js5vwm/amd_mi325x_8x_deployment_and_tests/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T15:23:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1js0zmd</id>
    <title>Quick Comparison of QwQ and OpenThinker2 32B</title>
    <updated>2025-04-05T11:04:54+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Candle test:&lt;/p&gt; &lt;p&gt;qwq: &lt;a href="https://imgur.com/a/c5gJ2XL"&gt;https://imgur.com/a/c5gJ2XL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ot2: &lt;a href="https://imgur.com/a/TDNm12J"&gt;https://imgur.com/a/TDNm12J&lt;/a&gt;&lt;/p&gt; &lt;p&gt;both passed&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;5 reasoning questions:&lt;/p&gt; &lt;p&gt;&lt;a href="https://imgur.com/a/ec17EJC"&gt;https://imgur.com/a/ec17EJC&lt;/a&gt;&lt;/p&gt; &lt;p&gt;qwq passed all questions&lt;/p&gt; &lt;p&gt;ot2 failed 2 questions&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Private tests:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Coding question: One question about what caused the issue, plus 1,200 lines of C++ code. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Both passed, however ot2 is not as reliable as QwQ at solving this issue. It could give wrong answer during multi-shots, unlike qwq which always give the right answer.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Restructuring a financial spreadsheet. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Both passed.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Conclusion:&lt;/p&gt; &lt;p&gt;I prefer OpenThinker2-32B over the original R1-distill-32B from DS, especially because it never fell into an infinite loop during testing. I tested those five reasoning questions three times on OT2, and it never fell into a loop, unlike the R1-distill model.&lt;/p&gt; &lt;p&gt;Which is quite an achievement considering they open-sourced their dataset and their distillation dataset is not much larger than DS's (1M vs 800k).&lt;/p&gt; &lt;p&gt;However, it still falls behind QwQ-32B, which uses RL instead.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Settings I used for both models: &lt;a href="https://imgur.com/a/7ZBQ6SX"&gt;https://imgur.com/a/7ZBQ6SX&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gguf: &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/blob/main/Qwen_QwQ-32B-IQ4_XS.gguf"&gt;https://huggingface.co/bartowski/Qwen_QwQ-32B-GGUF/blob/main/Qwen_QwQ-32B-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/open-thoughts_OpenThinker2-32B-GGUF/blob/main/open-thoughts_OpenThinker2-32B-IQ4_XS.gguf"&gt;https://huggingface.co/bartowski/open-thoughts_OpenThinker2-32B-GGUF/blob/main/open-thoughts_OpenThinker2-32B-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;backend: ollama&lt;/p&gt; &lt;p&gt;source of public questions:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i65599/r1_32b_is_be_worse_than_qwq_32b_tests_included/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1i65599/r1_32b_is_be_worse_than_qwq_32b_tests_included/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jpr1nk/the_candle_test_most_llms_fail_to_generalise_at/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jpr1nk/the_candle_test_most_llms_fail_to_generalise_at/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js0zmd/quick_comparison_of_qwq_and_openthinker2_32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js0zmd/quick_comparison_of_qwq_and_openthinker2_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js0zmd/quick_comparison_of_qwq_and_openthinker2_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T11:04:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1js7559</id>
    <title>SoftWhisper April 2025 out – automated transcription now with speaker identification!</title>
    <updated>2025-04-05T16:20:24+00:00</updated>
    <author>
      <name>/u/Substantial_Swan_144</name>
      <uri>https://old.reddit.com/user/Substantial_Swan_144</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js7559/softwhisper_april_2025_out_automated/"&gt; &lt;img alt="SoftWhisper April 2025 out – automated transcription now with speaker identification!" src="https://external-preview.redd.it/j1LhVFNpOOLKVH84GYEqyz2i4Q75slwPOEiPhrZyFBc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be2b5855980dea1dc493e754a13fc957206fc763" title="SoftWhisper April 2025 out – automated transcription now with speaker identification!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, my dear Github friends,&lt;/p&gt; &lt;p&gt;It is with great joy that I announce that SoftWhisper April 2025 is out – now with speaker identification (diarization)!&lt;/p&gt; &lt;p&gt;(Link: &lt;a href="https://github.com/NullMagic2/SoftWhisper"&gt;https://github.com/NullMagic2/SoftWhisper&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/63yt4ll2l1te1.png?width=1986&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a41755400d8ab52f5b1f5a99444c74acfd1e939"&gt;https://preview.redd.it/63yt4ll2l1te1.png?width=1986&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a41755400d8ab52f5b1f5a99444c74acfd1e939&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A tricky feature&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Originally, I wanted to implement diarization with Pyannote, but because APIs are usually not widelly documented, not only learning how to use them, but also how effective they are for the project, is a bit difficult.&lt;/p&gt; &lt;p&gt;Identifying speakers is still somewhat primitive even with state-of-the-art solutions. Usually, the best results are achieved with fine-tuned models and controlled conditions (for example, two speakers in studio recordings).&lt;/p&gt; &lt;p&gt;The crux of the matter is: not only do we require a lot of money to create those specialized models, but they are incredibly hard to use. That does not align with my vision of having something that works reasonably well and is easy to setup, so I did a few tests with 3-4 different approaches.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A balanced compromise&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After careful testing, I believe inaSpeechSegmenter will provide our users the best balance between usability and accuracy: it's fast, identifies speakers to a more or less consistent degree out of the box, and does not require a complicated setup. Give it a try!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Known issues&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Please note: while speaker identification is more or less consistent, the current approach is still not perfect and will sometimes not identify cross speech or add more speakers than present in the audio, so manual review is still needed. This feature is provided with the hopes to make diarization easier, not a solved problem.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Increased loading times&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Also keep in mind that the current diarization solution will increase the loading times slightly and if you select diarization, computation will also increase. Please be patient.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Other bugfixes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This release also fixes a few other bugs, namely that the exported content sometimes would not match the content in the textbox.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Swan_144"&gt; /u/Substantial_Swan_144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js7559/softwhisper_april_2025_out_automated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js7559/softwhisper_april_2025_out_automated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js7559/softwhisper_april_2025_out_automated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T16:20:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1js9tkl</id>
    <title>Presenting chat.md: fully editable chat interface with MCP support on any LLM [open source][MIT license]</title>
    <updated>2025-04-05T18:17:04+00:00</updated>
    <author>
      <name>/u/Professor_Entropy</name>
      <uri>https://old.reddit.com/user/Professor_Entropy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js9tkl/presenting_chatmd_fully_editable_chat_interface/"&gt; &lt;img alt="Presenting chat.md: fully editable chat interface with MCP support on any LLM [open source][MIT license]" src="https://external-preview.redd.it/MWxtZHE0NW0yMnRlMcCcpkXwt-WbbuHPY-2jNSelzM5Bm5bES2mvySmIn5un.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=550bbea9f9620f863da5d865646cadcfc60b267a" title="Presenting chat.md: fully editable chat interface with MCP support on any LLM [open source][MIT license]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;chat.md: The Hacker's AI Chat Interface&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/rusiaaman/chat.md"&gt;https://github.com/rusiaaman/chat.md&lt;/a&gt; &lt;/p&gt; &lt;p&gt;chat.md is a VS Code extension that turns markdown files into editable AI conversations&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Edit past messages of user, assistant or tool responses and have the AI continue from any point. The file editor is the chat interface and the history.&lt;/li&gt; &lt;li&gt;LLM agnostic MCP support: no restrictions on tool calling on any LLM, even if they don't official support tool calling. &lt;/li&gt; &lt;li&gt;Press shift+enter to have AI stream its response in the chat.md file which is also the conversation history. &lt;/li&gt; &lt;li&gt;Tool calls are detected and tool execution results added in the file in an agentic loop.&lt;/li&gt; &lt;li&gt;Stateless. Switch the LLM provider at any point. Change the MCP tools at any point.&lt;/li&gt; &lt;li&gt;Put words in LLM's mouth - edit and have it continue from there&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Quick start:&lt;br /&gt; 1. Install chat.md vscode extension&lt;br /&gt; 2. Press Opt+Cmd+' (single quote)&lt;br /&gt; 3. Add your message in the user block and press &amp;quot;Shift+enter&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Your local LLM not able to follow tool call syntax?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Manually fix its tool use once (run the tool by adding a '# %% tool_execute' block) so that it does it right the next time copying its past behavior.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professor_Entropy"&gt; /u/Professor_Entropy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c3gen35m22te1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js9tkl/presenting_chatmd_fully_editable_chat_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js9tkl/presenting_chatmd_fully_editable_chat_interface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:17:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jryrik</id>
    <title>OpenThinker2-32B</title>
    <updated>2025-04-05T08:21:23+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jryrik/openthinker232b/"&gt; &lt;img alt="OpenThinker2-32B" src="https://external-preview.redd.it/MMWrXSajlQDn44f7IjYIsEwZJ6CFMkYChFS5zuH6LYA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b962a458fb32a0508ef1344c44f7b73075ef5245" title="OpenThinker2-32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/open-thoughts/OpenThinker2-32B"&gt;https://huggingface.co/open-thoughts/OpenThinker2-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1x9zxh5f7zse1.png?width=704&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76c2b9c3676b9d9969d428b0a44cf823d4f72367"&gt;https://preview.redd.it/1x9zxh5f7zse1.png?width=704&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76c2b9c3676b9d9969d428b0a44cf823d4f72367&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jryrik/openthinker232b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jryrik/openthinker232b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jryrik/openthinker232b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T08:21:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsax3p</id>
    <title>Llama 4 Benchmarks</title>
    <updated>2025-04-05T19:04:21+00:00</updated>
    <author>
      <name>/u/Ravencloud007</name>
      <uri>https://old.reddit.com/user/Ravencloud007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsax3p/llama_4_benchmarks/"&gt; &lt;img alt="Llama 4 Benchmarks" src="https://preview.redd.it/o2cd1y15e2te1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01928d53f0ef81a88115f299ef15628aacc38783" title="Llama 4 Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ravencloud007"&gt; /u/Ravencloud007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o2cd1y15e2te1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsax3p/llama_4_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsax3p/llama_4_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T19:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsalxn</id>
    <title>The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation</title>
    <updated>2025-04-05T18:51:11+00:00</updated>
    <author>
      <name>/u/Ill-Association-8410</name>
      <uri>https://old.reddit.com/user/Ill-Association-8410</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsalxn/the_llama_4_herd_the_beginning_of_a_new_era_of/"&gt; &lt;img alt="The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation" src="https://external-preview.redd.it/5GYklgQz-p1iWSTGvDsKHeD_QUDxP-9vHZQeXTsgRz4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65f85ee3e9068eb521d7e3ef4dce3cee7c471c03" title="The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill-Association-8410"&gt; /u/Ill-Association-8410 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsalxn/the_llama_4_herd_the_beginning_of_a_new_era_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsalxn/the_llama_4_herd_the_beginning_of_a_new_era_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:51:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1js335l</id>
    <title>Karamaru - An "Edo period" LLM trained on 17th-19th century japanese literature.</title>
    <updated>2025-04-05T13:09:39+00:00</updated>
    <author>
      <name>/u/nomad_lw</name>
      <uri>https://old.reddit.com/user/nomad_lw</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js335l/karamaru_an_edo_period_llm_trained_on_17th19th/"&gt; &lt;img alt="Karamaru - An &amp;quot;Edo period&amp;quot; LLM trained on 17th-19th century japanese literature." src="https://external-preview.redd.it/ll0sI2kj9OWJW1iOriHpZm1jSfC278YnLF-jisELKs4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f5f30bf0b3bae15b4dee53ba7bd37f2486072c04" title="Karamaru - An &amp;quot;Edo period&amp;quot; LLM trained on 17th-19th century japanese literature." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw this a few days ago where a researcher from Sakana AI continually pretrained a Llama-3 Elyza 8B model on classical japanese literature. &lt;/p&gt; &lt;p&gt;What's cool about is that it builds towards an idea that's been brewing on my mind and evidently a lot of other people here,&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;A model that's able to be a Time-travelling subject matter expert. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;Researcher's tweet: &lt;a href="https://x.com/tkasasagi/status/1907998360713441571?t=PGhYyaVJQtf0k37l-9zXiA&amp;amp;s=19"&gt;https://x.com/tkasasagi/status/1907998360713441571?t=PGhYyaVJQtf0k37l-9zXiA&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Huggingface:&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/SakanaAI/Llama-3-Karamaru-v1"&gt;https://huggingface.co/SakanaAI/Llama-3-Karamaru-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Space: &lt;a href="https://huggingface.co/spaces/SakanaAI/Llama-3-Karamaru-v1"&gt;https://huggingface.co/spaces/SakanaAI/Llama-3-Karamaru-v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nomad_lw"&gt; /u/nomad_lw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sakana.ai/karamaru/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js335l/karamaru_an_edo_period_llm_trained_on_17th19th/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js335l/karamaru_an_edo_period_llm_trained_on_17th19th/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T13:09:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsafqw</id>
    <title>Llama 4 announced</title>
    <updated>2025-04-05T18:43:42+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link: &lt;a href="https://www.llama.com/llama4/"&gt;https://www.llama.com/llama4/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsafqw/llama_4_announced/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsafqw/llama_4_announced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsafqw/llama_4_announced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:43:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsadt3</id>
    <title>Llama4 Released</title>
    <updated>2025-04-05T18:41:26+00:00</updated>
    <author>
      <name>/u/latestagecapitalist</name>
      <uri>https://old.reddit.com/user/latestagecapitalist</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/latestagecapitalist"&gt; /u/latestagecapitalist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.llama.com/llama4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsadt3/llama4_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsadt3/llama4_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1js0g38</id>
    <title>Tenstorrent Blackhole PCI-e cards with 32 GB of GDDR6 available for order</title>
    <updated>2025-04-05T10:27:38+00:00</updated>
    <author>
      <name>/u/Marcuss2</name>
      <uri>https://old.reddit.com/user/Marcuss2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js0g38/tenstorrent_blackhole_pcie_cards_with_32_gb_of/"&gt; &lt;img alt="Tenstorrent Blackhole PCI-e cards with 32 GB of GDDR6 available for order" src="https://external-preview.redd.it/bKa6_zcgR56sbwG-Cqtu_jN8tcBni2YVCOOrskJ4IzI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d47a097527e5e5e57498d3ac6192eb2f6741fe55" title="Tenstorrent Blackhole PCI-e cards with 32 GB of GDDR6 available for order" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Marcuss2"&gt; /u/Marcuss2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://tenstorrent.com/hardware/blackhole"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js0g38/tenstorrent_blackhole_pcie_cards_with_32_gb_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js0g38/tenstorrent_blackhole_pcie_cards_with_32_gb_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T10:27:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsahy4</id>
    <title>Llama 4 is here</title>
    <updated>2025-04-05T18:46:20+00:00</updated>
    <author>
      <name>/u/jugalator</name>
      <uri>https://old.reddit.com/user/jugalator</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jugalator"&gt; /u/jugalator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama4_omni/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsahy4/llama_4_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsahy4/llama_4_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:46:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsampe</id>
    <title>Mark presenting four Llama 4 models, even a 2 trillion parameters model!!!</title>
    <updated>2025-04-05T18:52:08+00:00</updated>
    <author>
      <name>/u/LarDark</name>
      <uri>https://old.reddit.com/user/LarDark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsampe/mark_presenting_four_llama_4_models_even_a_2/"&gt; &lt;img alt="Mark presenting four Llama 4 models, even a 2 trillion parameters model!!!" src="https://external-preview.redd.it/Z3p2aHZudXhiMnRlMYW4H8xHgtzR3pjuficV95KktJ2KVETiew0YUMQL020k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b332bfe887b8dc264280ed80e4cedb70e9cd787" title="Mark presenting four Llama 4 models, even a 2 trillion parameters model!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;source from his instagram page&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LarDark"&gt; /u/LarDark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7bgnzhtxb2te1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsampe/mark_presenting_four_llama_4_models_even_a_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsampe/mark_presenting_four_llama_4_models_even_a_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:52:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1js4iy0</id>
    <title>I think I overdid it.</title>
    <updated>2025-04-05T14:21:22+00:00</updated>
    <author>
      <name>/u/_supert_</name>
      <uri>https://old.reddit.com/user/_supert_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js4iy0/i_think_i_overdid_it/"&gt; &lt;img alt="I think I overdid it." src="https://preview.redd.it/i5f8b0knz0te1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1448cae5bed745aa96ac7b2801a7bf32c07afd26" title="I think I overdid it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_supert_"&gt; /u/_supert_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i5f8b0knz0te1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js4iy0/i_think_i_overdid_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js4iy0/i_think_i_overdid_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T14:21:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsabgd</id>
    <title>Meta: Llama4</title>
    <updated>2025-04-05T18:38:40+00:00</updated>
    <author>
      <name>/u/pahadi_keeda</name>
      <uri>https://old.reddit.com/user/pahadi_keeda</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsabgd/meta_llama4/"&gt; &lt;img alt="Meta: Llama4" src="https://external-preview.redd.it/cwgFslgMUPL6p26FpnXYan8AI9J3Uz-yA2DZbRx4puk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3c2d0eac2996298f7e242609a095f7deafa5ac1" title="Meta: Llama4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pahadi_keeda"&gt; /u/pahadi_keeda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.llama.com/llama-downloads/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsabgd/meta_llama4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsabgd/meta_llama4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:38:40+00:00</published>
  </entry>
</feed>
