<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-09T03:37:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ikzlpx</id>
    <title>MLX Quants Vs GGUF</title>
    <updated>2025-02-08T22:31:56+00:00</updated>
    <author>
      <name>/u/BalaelGios</name>
      <uri>https://old.reddit.com/user/BalaelGios</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm playing around with some MLX models lately (M3 Max), and have noticed the quants seem different I don't usually see qx_k_m or anything like that just literally 4bit/8bit and a few 6bit. And generally they are smaller than their GGUF counterparts. I know MLX is faster and that is definitely evident. &lt;/p&gt; &lt;p&gt;My question is; is an MLX model at the same quant lower quality than the GGUF model and if so how much lower quality are we talking? &lt;/p&gt; &lt;p&gt;While I haven't noticed anything particularly jarring yet, I'm curious to understand the differences, my assumption since discovering MLX is that, if you're on Apple silicone you should be using MLX. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BalaelGios"&gt; /u/BalaelGios &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikzlpx/mlx_quants_vs_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikzlpx/mlx_quants_vs_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikzlpx/mlx_quants_vs_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T22:31:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikek15</id>
    <title>My DeepSeek R1 671B @ Home plan: CPU+GPU hybrid, 4xGen5 NVMe offload</title>
    <updated>2025-02-08T04:03:51+00:00</updated>
    <author>
      <name>/u/bo_peng</name>
      <uri>https://old.reddit.com/user/bo_peng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikek15/my_deepseek_r1_671b_home_plan_cpugpu_hybrid/"&gt; &lt;img alt="My DeepSeek R1 671B @ Home plan: CPU+GPU hybrid, 4xGen5 NVMe offload" src="https://a.thumbs.redditmedia.com/Yk1sCOddUTNzMn5fOag5QoE4yeshLWIZhFYJ14-t3u0.jpg" title="My DeepSeek R1 671B @ Home plan: CPU+GPU hybrid, 4xGen5 NVMe offload" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/uggf4ioi9uhe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=527eb75829f56baaaa8c207f009cb4833e5de321"&gt;https://preview.redd.it/uggf4ioi9uhe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=527eb75829f56baaaa8c207f009cb4833e5de321&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now waiting for 4060ti 16G to arrive. Requires lots of custom code to efficiently utilize this chimera setup :) So stay tuned. I think it can reach 10+ token/s for quantized 671B after optimizations.&lt;/p&gt; &lt;p&gt;You can use &amp;quot;ASUS Hyper M.2 x16 Gen5 Card&amp;quot; to host 4 NVME. And currently you need AMD CPUs to do native x4x4x4x4 bifurcation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bo_peng"&gt; /u/bo_peng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikek15/my_deepseek_r1_671b_home_plan_cpugpu_hybrid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikek15/my_deepseek_r1_671b_home_plan_cpugpu_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikek15/my_deepseek_r1_671b_home_plan_cpugpu_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T04:03:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1il3yqk</id>
    <title>A100 "Drive" SXM2 bench testing of various LocalLLM hosting Platforms</title>
    <updated>2025-02-09T02:02:45+00:00</updated>
    <author>
      <name>/u/mp3m4k3r</name>
      <uri>https://old.reddit.com/user/mp3m4k3r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I started down this journey wanting to build out a local AI backend for immich and home assistant and started out picking up an nvidia Tesla A2. The seller happened to send over 2x P4s as well.&lt;/p&gt; &lt;p&gt;And wouldn't you know it &lt;em&gt;&amp;quot;oops honey I tripped and fell into a server, running circuits in my house, and then swapping out the perfectly fine GPUs with some updated models&amp;quot;&lt;/em&gt; ...&lt;/p&gt; &lt;p&gt;In expanding this out and learning tons in the process I wanted to also start doing some testing/benchmarking so that I could either share some information (or at least see if what I did marginally worked better than the last setting or not).&lt;/p&gt; &lt;p&gt;Below is the information I have so far, I am looking into moving to vLLM with vAttention as it looks pretty interesting and then also working on some augments to SWE-agent to play around with that and SWE-bench a bit.&lt;/p&gt; &lt;p&gt;Not on this post but I will be compiling the charts and stuff from this tomorrow to post as well.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Asks:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Do you have any recommendations for benchmarks?&lt;/li&gt; &lt;li&gt;Do you have any questions?&lt;/li&gt; &lt;li&gt;Anything you would like to see?&lt;/li&gt; &lt;li&gt;Do you know if I can get a bank loan for immersion cooling?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Test Setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Benchmark: &lt;a href="https://github.com/coder543/llm-speed-benchmark"&gt;llm-speed-benchmark&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model: &lt;a href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf"&gt;Phi-3-mini-4k-instruct Q4&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;(Why a Quant of Phi-3 Mini? Because it would fit in each of the GPUs and was easily available across the platforms)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ran the llm-speed-bench against each configuration for 100 runs. It automatically exports some charts, csv, and what filled out most of the MD formatting below. While the tests were running no other processing was really happening for this server.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance Summary&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Frontend&lt;/th&gt; &lt;th align="left"&gt;Platform&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;GPU&lt;/th&gt; &lt;th align="left"&gt;Warm?&lt;/th&gt; &lt;th align="left"&gt;Runs&lt;/th&gt; &lt;th align="left"&gt;Time To First Token&lt;/th&gt; &lt;th align="left"&gt;Prompt Tok/s&lt;/th&gt; &lt;th align="left"&gt;Response Tok/s&lt;/th&gt; &lt;th align="left"&gt;Num Response Tokens&lt;/th&gt; &lt;th align="left"&gt;Avg Tokens per Chunk&lt;/th&gt; &lt;th align="left"&gt;Avg Time Between Chunks&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenWebUI&lt;/td&gt; &lt;td align="left"&gt;ollama&lt;/td&gt; &lt;td align="left"&gt;llama-cpp&lt;/td&gt; &lt;td align="left"&gt;A100D&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;100&lt;/td&gt; &lt;td align="left"&gt;0.17 +/- 0.02&lt;/td&gt; &lt;td align="left"&gt;453.18 +/- 65.78&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;119.55 +/- 6.20&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;201.00 +/- 373.00&lt;/td&gt; &lt;td align="left"&gt;3.50 +/- 0.62&lt;/td&gt; &lt;td align="left"&gt;0.01 +/- 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenWebUI&lt;/td&gt; &lt;td align="left"&gt;ollama&lt;/td&gt; &lt;td align="left"&gt;llama-cpp&lt;/td&gt; &lt;td align="left"&gt;V100&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;100&lt;/td&gt; &lt;td align="left"&gt;0.21 +/- 0.03&lt;/td&gt; &lt;td align="left"&gt;379.30 +/- 63.55&lt;/td&gt; &lt;td align="left"&gt;112.01 +/- 5.59&lt;/td&gt; &lt;td align="left"&gt;191.00 +/- 201.75&lt;/td&gt; &lt;td align="left"&gt;3.38 +/- 0.45&lt;/td&gt; &lt;td align="left"&gt;0.01 +/- 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenWebUI&lt;/td&gt; &lt;td align="left"&gt;LocalAi&lt;/td&gt; &lt;td align="left"&gt;llama-cpp-fallback&lt;/td&gt; &lt;td align="left"&gt;A100D&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;100&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;0.14 +/- 0.03&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;577.40 +/- 109.92&lt;/td&gt; &lt;td align="left"&gt;74.14 +/- 2.13&lt;/td&gt; &lt;td align="left"&gt;719.00 +/- 113.00&lt;/td&gt; &lt;td align="left"&gt;1.00 +/- 0.00&lt;/td&gt; &lt;td align="left"&gt;0.00 +/- 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenWebUI&lt;/td&gt; &lt;td align="left"&gt;LocalAi&lt;/td&gt; &lt;td align="left"&gt;llama-cpp-fallback&lt;/td&gt; &lt;td align="left"&gt;V100&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;100&lt;/td&gt; &lt;td align="left"&gt;0.16 +/- 0.04&lt;/td&gt; &lt;td align="left"&gt;479.44 +/- 102.21&lt;/td&gt; &lt;td align="left"&gt;71.95 +/- 1.67&lt;/td&gt; &lt;td align="left"&gt;737.50 +/- 109.25&lt;/td&gt; &lt;td align="left"&gt;1.00 +/- 0.00&lt;/td&gt; &lt;td align="left"&gt;0.00 +/- 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenWebUI&lt;/td&gt; &lt;td align="left"&gt;vLLM&lt;/td&gt; &lt;td align="left"&gt;vLLM&lt;/td&gt; &lt;td align="left"&gt;A100D&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;100&lt;/td&gt; &lt;td align="left"&gt;0.27 +/- 0.03&lt;/td&gt; &lt;td align="left"&gt;293.64 +/- 31.49&lt;/td&gt; &lt;td align="left"&gt;114.38 +/- 4.48&lt;/td&gt; &lt;td align="left"&gt;743.50 +/- 122.00&lt;/td&gt; &lt;td align="left"&gt;3.81 +/- 0.20&lt;/td&gt; &lt;td align="left"&gt;0.01 +/- 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenWebUI&lt;/td&gt; &lt;td align="left"&gt;vLLM&lt;/td&gt; &lt;td align="left"&gt;vLLM&lt;/td&gt; &lt;td align="left"&gt;V100&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;100&lt;/td&gt; &lt;td align="left"&gt;0.31 +/- 0.03&lt;/td&gt; &lt;td align="left"&gt;253.70 +/- 18.75&lt;/td&gt; &lt;td align="left"&gt;107.08 +/- 3.09&lt;/td&gt; &lt;td align="left"&gt;782.50 +/- 128.75&lt;/td&gt; &lt;td align="left"&gt;3.80 +/- 0.14&lt;/td&gt; &lt;td align="left"&gt;0.01 +/- 0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;em&gt;Values are presented as median +/- IQR (Interquartile Range). Tokenization of non-OpenAI models is approximate.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Environmental Configuration:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;All platforms/frontends mentioned are running in docker containers across 2 chassis. Chassis 1: This hosts OpenWebUi and some other services as it is external facing Chassis 2: This is the &amp;quot;compute&amp;quot; node in the backend&lt;/p&gt; &lt;p&gt;Chassis 1 and 2 are connected via 10GB links through a cisco switch and are within the same VLANs (where applicable). OpenWebUi does make use of a docker &amp;quot;bridge&amp;quot; network to egress to the compute node.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;System Specs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Chassis: Gigabyte T181-G20 OCPv1 with custom power supply so I can run it outside of an OCPv1 rack&lt;/li&gt; &lt;li&gt;CPU: 1x Intel(R) Xeon(R) Gold 5115 CPU @ 2.40GHz (10C,20T)&lt;/li&gt; &lt;li&gt;RAM: 16x 32GB Samsung ECC 2400 MT/s (fills all channels) M393A4K40CB1-CRC&lt;/li&gt; &lt;li&gt;OS: Ubuntu 24.04.1 LTS&lt;/li&gt; &lt;li&gt;GPUs: &lt;ul&gt; &lt;li&gt;1x SXM2 A100 &amp;quot;Drive&amp;quot; module with 32GB of ram and 0 chill (it gets hot) &lt;ul&gt; &lt;li&gt;I have the other 3 but may hold off installing them until I can get some better cooling or the stupid IPMI in this chassis to take remote fan commands from the OS.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;3x V100 16GB&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 565.57.01 Driver Version: 565.57.01 CUDA Version: 12.7 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 Tesla V100-SXM2-16GB On | 00000000:1A:00.0 Off | 0 | | N/A 31C P0 56W / 300W | 7933MiB / 16384MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 Tesla V100-SXM2-16GB On | 00000000:1B:00.0 Off | 0 | | N/A 24C P0 39W / 300W | 1MiB / 16384MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 2 Tesla V100-SXM2-16GB On | 00000000:1C:00.0 Off | 0 | | N/A 43C P0 58W / 300W | 15051MiB / 16384MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 3 NVIDIA DRIVE-PG199-PROD On | 00000000:1D:00.0 Off | 0 | | N/A 39C P0 36W / N/A | 1MiB / 32768MiB | 0% Default | | | | Disabled | +-----------------------------------------+------------------------+----------------------+ &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mp3m4k3r"&gt; /u/mp3m4k3r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il3yqk/a100_drive_sxm2_bench_testing_of_various_localllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il3yqk/a100_drive_sxm2_bench_testing_of_various_localllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1il3yqk/a100_drive_sxm2_bench_testing_of_various_localllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T02:02:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1iksy75</id>
    <title>In Eleven Labs I can record a real voice performance and convert that performance into a different voice (and accent), is this possible locally yet?</title>
    <updated>2025-02-08T17:45:49+00:00</updated>
    <author>
      <name>/u/MisPreguntas</name>
      <uri>https://old.reddit.com/user/MisPreguntas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Eleven Labs is the only service I'm paying for, mainly because of the feature I described in my title, is there an offline local alternative that is able to do this?&lt;/p&gt; &lt;p&gt;So far, I'm able to clone any voice I want, but I can't transfer a real performance. &lt;/p&gt; &lt;p&gt;Is this possible yet locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MisPreguntas"&gt; /u/MisPreguntas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iksy75/in_eleven_labs_i_can_record_a_real_voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iksy75/in_eleven_labs_i_can_record_a_real_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iksy75/in_eleven_labs_i_can_record_a_real_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T17:45:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik98l2</id>
    <title>I haven't seen many quad GPU setups so here is one</title>
    <updated>2025-02-07T23:39:23+00:00</updated>
    <author>
      <name>/u/dazzou5ouh</name>
      <uri>https://old.reddit.com/user/dazzou5ouh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik98l2/i_havent_seen_many_quad_gpu_setups_so_here_is_one/"&gt; &lt;img alt="I haven't seen many quad GPU setups so here is one" src="https://preview.redd.it/lav3msh9zshe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a16836ce930a2602c2271a999be6be9f7ee678b" title="I haven't seen many quad GPU setups so here is one" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dazzou5ouh"&gt; /u/dazzou5ouh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lav3msh9zshe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik98l2/i_havent_seen_many_quad_gpu_setups_so_here_is_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ik98l2/i_havent_seen_many_quad_gpu_setups_so_here_is_one/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T23:39:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikjrf3</id>
    <title>OpenAI asks to Marginalise ILya as He asks investors for it's SSI &amp; also xAI | iLya &amp; elon both were OG cofounders of OpenAI</title>
    <updated>2025-02-08T09:40:24+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjrf3/openai_asks_to_marginalise_ilya_as_he_asks/"&gt; &lt;img alt="OpenAI asks to Marginalise ILya as He asks investors for it's SSI &amp;amp; also xAI | iLya &amp;amp; elon both were OG cofounders of OpenAI" src="https://preview.redd.it/wojbv3ufyvhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=672d5dd30929a3d9877d2f0489d521aeb4f36140" title="OpenAI asks to Marginalise ILya as He asks investors for it's SSI &amp;amp; also xAI | iLya &amp;amp; elon both were OG cofounders of OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wojbv3ufyvhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjrf3/openai_asks_to_marginalise_ilya_as_he_asks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjrf3/openai_asks_to_marginalise_ilya_as_he_asks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T09:40:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikqy5w</id>
    <title>Building an LLM-Optimized Linux Server on a Budget</title>
    <updated>2025-02-08T16:21:16+00:00</updated>
    <author>
      <name>/u/Unprotectedtxt</name>
      <uri>https://old.reddit.com/user/Unprotectedtxt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikqy5w/building_an_llmoptimized_linux_server_on_a_budget/"&gt; &lt;img alt="Building an LLM-Optimized Linux Server on a Budget" src="https://external-preview.redd.it/9ar9q08cZR3Pnd8Hxka6_rkxXup9YdrkJJw3Y1V7agI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7da33d2c9a8a6d7c6f95f5206eb1b4796158fe2f" title="Building an LLM-Optimized Linux Server on a Budget" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on these benchmarks wouldn’t buying a Mac Studio with 128 GB RAM M2 Ultra 60 or 72 core be far better than traditional dedicated PC builds?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unprotectedtxt"&gt; /u/Unprotectedtxt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://linuxblog.io/build-llm-linux-server-on-budget/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikqy5w/building_an_llmoptimized_linux_server_on_a_budget/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikqy5w/building_an_llmoptimized_linux_server_on_a_budget/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T16:21:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1iko5y3</id>
    <title>How do the models always answer in correct English when much of the web has badly written and incorrect English?</title>
    <updated>2025-02-08T14:15:17+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was wondering how the training works to achieve this &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iko5y3/how_do_the_models_always_answer_in_correct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iko5y3/how_do_the_models_always_answer_in_correct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iko5y3/how_do_the_models_always_answer_in_correct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T14:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik76bj</id>
    <title>It was Ilya who "closed" OpenAI</title>
    <updated>2025-02-07T22:08:04+00:00</updated>
    <author>
      <name>/u/Singularity-42</name>
      <uri>https://old.reddit.com/user/Singularity-42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik76bj/it_was_ilya_who_closed_openai/"&gt; &lt;img alt="It was Ilya who &amp;quot;closed&amp;quot; OpenAI" src="https://preview.redd.it/juqrkouwishe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2df7513b32ac2840f23b550e538e3f7b2c6c6ca3" title="It was Ilya who &amp;quot;closed&amp;quot; OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Singularity-42"&gt; /u/Singularity-42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/juqrkouwishe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik76bj/it_was_ilya_who_closed_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ik76bj/it_was_ilya_who_closed_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T22:08:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik162w</id>
    <title>Trump just said “no” DeepSeek does not pose a national security threat at a press conference</title>
    <updated>2025-02-07T17:57:11+00:00</updated>
    <author>
      <name>/u/bruhlmaocmonbro</name>
      <uri>https://old.reddit.com/user/bruhlmaocmonbro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik162w/trump_just_said_no_deepseek_does_not_pose_a/"&gt; &lt;img alt="Trump just said “no” DeepSeek does not pose a national security threat at a press conference" src="https://preview.redd.it/73sost17arhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d90041a44ec176a4b5a3630a47ce33577c957522" title="Trump just said “no” DeepSeek does not pose a national security threat at a press conference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bruhlmaocmonbro"&gt; /u/bruhlmaocmonbro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/73sost17arhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik162w/trump_just_said_no_deepseek_does_not_pose_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ik162w/trump_just_said_no_deepseek_does_not_pose_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T17:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikgsl6</id>
    <title>Germany: "We released model equivalent to R1 back in November, no reason to worry"</title>
    <updated>2025-02-08T06:14:46+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikgsl6/germany_we_released_model_equivalent_to_r1_back/"&gt; &lt;img alt="Germany: &amp;quot;We released model equivalent to R1 back in November, no reason to worry&amp;quot;" src="https://b.thumbs.redditmedia.com/mRAyAl4yIV32IK8PFM79kLhWfUBplZg1xnMZQcGaLLI.jpg" title="Germany: &amp;quot;We released model equivalent to R1 back in November, no reason to worry&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ikgsl6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikgsl6/germany_we_released_model_equivalent_to_r1_back/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikgsl6/germany_we_released_model_equivalent_to_r1_back/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T06:14:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1il2tob</id>
    <title>Webui Alternative for Vision Language Models like LLaMA 3.2 11b</title>
    <updated>2025-02-09T01:04:23+00:00</updated>
    <author>
      <name>/u/Any-Shopping2394</name>
      <uri>https://old.reddit.com/user/Any-Shopping2394</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il2tob/webui_alternative_for_vision_language_models_like/"&gt; &lt;img alt="Webui Alternative for Vision Language Models like LLaMA 3.2 11b" src="https://external-preview.redd.it/1_pBKncGwRNLzGo1mmmMXq2D7QqloSpEo2dTKKQOehg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76f7a48196167aa210e43098e30f88f28d494159" title="Webui Alternative for Vision Language Models like LLaMA 3.2 11b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made this because in the oobabooga text-generation-webui didn't have the capability to use the &amp;quot;multimodal&amp;quot; part of these kind of models (the image sending). It also has characters as you would have them in others webui. It's made using the transformers package.&lt;/p&gt; &lt;p&gt;This feature can be used for example send to the AI an image so it can recognize it, for example. If you want to send it a picture of an outfit and ask it how it looks. In syntesis, to talk about the content of an image. It depends a lot on the usage you give to your chats with AI.&lt;/p&gt; &lt;p&gt;Tell me what you think about &lt;a href="https://github.com/ricardo2001l/visual-text-generation-webui"&gt;this webui&lt;/a&gt;, also if you want to contribute by making a pull request, i'd be glad. So give it a try &lt;a href="https://github.com/ricardo2001l/visual-text-generation-webui"&gt;https://github.com/ricardo2001l/visual-text-generation-webui&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9yypk3s9j0ie1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1adda01b38917c7245f3f1f63210108ad5afa40c"&gt;The webui&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Shopping2394"&gt; /u/Any-Shopping2394 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il2tob/webui_alternative_for_vision_language_models_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il2tob/webui_alternative_for_vision_language_models_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1il2tob/webui_alternative_for_vision_language_models_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T01:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikguu9</id>
    <title>"Meta Torrented over 81 TB of Data Through Anna’s Archive, Despite Few Seeders"</title>
    <updated>2025-02-08T06:18:48+00:00</updated>
    <author>
      <name>/u/remixer_dec</name>
      <uri>https://old.reddit.com/user/remixer_dec</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remixer_dec"&gt; /u/remixer_dec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://torrentfreak.com/meta-torrented-over-81-tb-of-data-through-annas-archive-despite-few-seeders-250206/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikguu9/meta_torrented_over_81_tb_of_data_through_annas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikguu9/meta_torrented_over_81_tb_of_data_through_annas/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T06:18:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1iks9cl</id>
    <title>Notes on OpenAI o3-mini: How good is it compared to r1 and o1?</title>
    <updated>2025-02-08T17:16:46+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We finally have a reasonable reasoning model from OpenAI that has a reasonable cost; it must be Deepseek r1 impact. But anyway, we now have the first family of models from the o3 series. Also, It is the first reasoning model with official function-calling support.&lt;/p&gt; &lt;p&gt;Another interesting thing is that, unlike o1, we can now see the chain of thought (CoT). However, the CoT is not raw like Deepseek r1, but only a summarized version of it, and I am not sure why they are still keeping it under wraps.&lt;/p&gt; &lt;h1&gt;On pricing&lt;/h1&gt; &lt;p&gt;Perhaps the most highlighting aspect of the model is that it’s 15x cheaper than O1 with comparable performance and, in fact, better at times.&lt;/p&gt; &lt;p&gt;The fact that it is cheaper by 2x than even the GPT-4o is even more amusing. Then why do Chatgpt users have limited queries while GPT-4o has unlimited queries?&lt;/p&gt; &lt;p&gt;Did Deepseek force OpenAI to subsidize API costs?&lt;/p&gt; &lt;h1&gt;On performance&lt;/h1&gt; &lt;p&gt;To know if it actually is a better model than r1 and o1, I tested it on my benchmark questions for reasoning, Math, Coding, etc.&lt;/p&gt; &lt;p&gt;Here’s my observation:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;O3-mini-high is the best available model for reasoning tasks, apart from o1-pro.&lt;/li&gt; &lt;li&gt;For math, o1 and o3-mini-high are on par, a tad bit better than Deepseek r1.&lt;/li&gt; &lt;li&gt;Again, for coding, o3-mini-high felt better in my use cases but can vary from case to case. It is faster, so it is better to work with.&lt;/li&gt; &lt;li&gt;I can’t get over Deepseek r1 for creative writing, well, especially its CoT traces. I wish OpenAI would disclose the raw CoT in the coming models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model is actually good, and given the costs, it’s much better than o1. I would’ve loved if they showed us the actual CoT, and I think a lot of people are now more interested in thought patterns than actual responses.&lt;/p&gt; &lt;p&gt;For in-depth analysis, commentary, and remarks on the OpenAI o3-mini and comparison with Deepseek r1, check out this blog post: &lt;a href="https://composio.dev/blog/openai-o3-mini-vs-deepseek-r1/"&gt;On OpenAI o3-mini&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to know what have been your views and experiences with the o3-mini. How did you like it compared to Deepseek r1?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iks9cl/notes_on_openai_o3mini_how_good_is_it_compared_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iks9cl/notes_on_openai_o3mini_how_good_is_it_compared_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iks9cl/notes_on_openai_o3mini_how_good_is_it_compared_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T17:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikh3vz</id>
    <title>OpenAI is hiding the actual thinking tokens in o3-mini</title>
    <updated>2025-02-08T06:35:07+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikh3vz/openai_is_hiding_the_actual_thinking_tokens_in/"&gt; &lt;img alt="OpenAI is hiding the actual thinking tokens in o3-mini" src="https://external-preview.redd.it/7p74lVWOq7GrqAnrRqOdU-BQFqsEnbk8XozOhsD2YQc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65833b846107b44ca5029ac644441d85516e7126" title="OpenAI is hiding the actual thinking tokens in o3-mini" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Yesterday I discovered that in fact you can not even with the current &amp;quot;thinking&amp;quot; tag on ChatGPT UI have the actual thinking tokens. In a way, the thinking of the model.&lt;/p&gt; &lt;p&gt;How did i know it? By running one single simple prompt about &amp;quot;maze puzzle&amp;quot; over the two models. The sample can be found in this dataset we built at our company. &lt;a href="https://huggingface.co/datasets/jan-hq/Maze-Reasoning"&gt;https://huggingface.co/datasets/jan-hq/Maze-Reasoning&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;The prompt is a question about solving a maze: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Deepseek-R1 got it done in 483 seconds:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2vc5mhbqxuhe1.png?width=1690&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=611419e379856111070a3094ac3fbe7bac397381"&gt;Deepseek-R1 giving the correct result&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ChatGPT 53 seconds:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yoxgrs11yuhe1.png?width=1706&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f106a5f1553b8da728d9ca05dd32ce3702ecd5f"&gt;ChatGPT result also correct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Surprisingly both results are correct but the devil is in the detail. When I inspect Deepseek-R1 thought process, it's very long, with step by step imagination and testing how it would go through the maze.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Deepseek-R1 thinking tokens:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/flt2emzlyuhe1.png?width=1610&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a16c88501392216f8998f3d97277996fac64a18"&gt;Deepseek-R1 thought process is literal pages and pages of thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ChatGPT reason mode thinking tokens:&lt;/strong&gt;&lt;br /&gt; I will put the entire thinking tokens of ChatGPT here for everyone to inspect because it's much much shorter&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Let's break down the maze format. From what I'm seeing, the maze is made up of rows and columns, ranging from 0 to 4. Each cell is represented by its coordinates in the form &amp;lt;|row-col|&amp;gt;, followed by wall tokens like &amp;lt;|up_down_left_wall|&amp;gt; and &amp;lt;|blank|&amp;gt;. The walls specify which sides of each cell are blocked, and the origin and target tokens mark the start and end points. The way these tokens are structured seems important for determining the possible movements. Let’s work with this! Alright, I need to figure out where the origin and target are in the maze. I will look through the maze row by row to find them. Looking at row 1, it appears that the origin is at (1,3) and the target is at (1,2). We need to navigate from the origin (1,3) to the target (1,2). So now, it's just about predicting and outputting the individual moves one by one, separated by spaces. Let's figure out the path! I've figured out the maze's wall tokens, but before moving, I need to check if there's a direct path from the origin at (1,3) to the target at (1,2). The origin has walls on the up and left sides, according to the &amp;lt;|up_left_wall|&amp;gt; token. The target is to the left of the origin, and the cell next to the origin at (1,2) seems to have a wall on the right side, marked by &amp;lt;|down_right_wall|&amp;gt;. I’m thinking I’ll need to consider these walls carefully before predicting the next move. From what I’m seeing, the origin at (1,3) has walls on the left and up sides, so I can only move right or down. But moving left to reach the target is blocked because of the wall at (1,3). I checked the adjacency between (1,3) and (1,2), and it’s blocked by the walls in both cells. So, to get from (1,3) to (1,2), I'll need to find a detour. Let’s move down from (1,3) to (2,3) and explore. Okay, it looks like a direct route from (1,3) to (1,2) isn’t possible due to the walls blocking access. I checked various alternatives, including moving down and exploring different connections, but many of those led to dead ends. I've landed on a slightly longer route: get from (1,3) through (2,3) to (2,4), then to (3,4), and eventually backtracking westward while checking valid moves. After verifying the path through multiple neighbor checks, this looks like the best possible route. ANSWER: &amp;lt;|down|&amp;gt; &amp;lt;|right|&amp;gt; &amp;lt;|down|&amp;gt; &amp;lt;|down|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|up|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|down|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|up|&amp;gt; &amp;lt;|right|&amp;gt; &amp;lt;|up|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|up|&amp;gt; &amp;lt;|right|&amp;gt; &amp;lt;|right|&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Do you feel something is off about ChatGPT thought -&amp;gt; ANSWER?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It is very obvious that with the current thought process of ChatGPT reasoning model, &lt;strong&gt;it's glossing over so many details and somehow it can reach the result??&lt;/strong&gt; You basically cannot use the thinking tokens from ChatGPT to train a distilled thinking model because it is just not possible to reach the conclusion with those thinking, these are not actual thinking tokens of the models.&lt;/p&gt; &lt;p&gt;I have some hypothesis:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OpenAI is only providing &amp;quot;summarized&amp;quot; version of their thinking tokens.&lt;/li&gt; &lt;li&gt;OpenAI has a model to output bullshit thinking tokens to distract everyone from training a distill model, so they can say they provide the community with the actual data, but there is no data.&lt;/li&gt; &lt;li&gt;They don't have &amp;quot;readable&amp;quot; thinking model but it's just an approximation of the &amp;quot;latent&amp;quot; thinking tokens.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With the track record of OpenAI and ChatGPT, I am leaning towards &amp;quot;they are summarize or give bullshit thinking tokens&amp;quot; to the users more than they have more advanced model as option 3. Why? Because when I look at the UI it's obvious that the thought process is not outputting token per token but in chunk, which is either a summary, or a totally different model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What does this mean?&lt;/strong&gt;&lt;br /&gt; You can't just distill openAI model anymore, so don't assume everyone is distilling their model, &lt;strong&gt;THEY ARE CLOSED AI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The full logs of both answers from ChatGPT and Deepseek-R1 can be found here:&lt;/strong&gt; &lt;a href="https://gist.github.com/tikikun/cf037180f402c5183662768045b59eed"&gt;https://gist.github.com/tikikun/cf037180f402c5183662768045b59eed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The maze dataset we build can be found here:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/datasets/jan-hq/Maze-Reasoning"&gt;https://huggingface.co/datasets/jan-hq/Maze-Reasoning&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikh3vz/openai_is_hiding_the_actual_thinking_tokens_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikh3vz/openai_is_hiding_the_actual_thinking_tokens_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikh3vz/openai_is_hiding_the_actual_thinking_tokens_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T06:35:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikn5fg</id>
    <title>Glyphstral-24b: Symbolic Deductive Reasoning Model</title>
    <updated>2025-02-08T13:22:41+00:00</updated>
    <author>
      <name>/u/vesudeva</name>
      <uri>https://old.reddit.com/user/vesudeva</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn5fg/glyphstral24b_symbolic_deductive_reasoning_model/"&gt; &lt;img alt="Glyphstral-24b: Symbolic Deductive Reasoning Model" src="https://external-preview.redd.it/DghrhJAW-NKneHTJvXZ7IAcBmIpZ_fU36ahUXITL0bM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ad5bc295749b594e323b350912e29031d02d474" title="Glyphstral-24b: Symbolic Deductive Reasoning Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Everyone! &lt;/p&gt; &lt;p&gt;So I've been really obsessed lately with symbolic AI and the potential to improve reasoning and multi-dimensional thinking. I decided to go ahead and see if I could train a model to use a framework I am calling &amp;quot;Glyph Code Logic Flow&amp;quot;. &lt;/p&gt; &lt;p&gt;Essentially, it is a method of structured reasoning using deductive symbolic logic. You can learn more about it here &lt;a href="https://github.com/severian42/Computational-Model-for-Symbolic-Representations/tree/main"&gt;https://github.com/severian42/Computational-Model-for-Symbolic-Representations/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I first tried training Deepeek R1-Qwen-14 and QWQ-32 but their heavily pre-trained reasoning data seemed to conflict with my approach, which makes sense given the different concepts and ways of breaking down the problem.&lt;/p&gt; &lt;p&gt;I opted for Mistral-Small-24b to see the results, and after 7 days of pure training 24hrs a day (all locally using MLX-Dora at 4bit on my Mac M2 128GB). In all, the model trained on about 27mil tokens of my custom GCLF dataset (each example was around 30k tokens, with a total of 4500 examples)&lt;/p&gt; &lt;p&gt;I still need to get the docs and repo together, as I will be releasing it this weekend, but I felt like sharing a quick preview since this unexpectedly worked out awesomely.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ikn5fg/video/9h2mgdg02xhe1/player"&gt;https://reddit.com/link/1ikn5fg/video/9h2mgdg02xhe1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vesudeva"&gt; /u/vesudeva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn5fg/glyphstral24b_symbolic_deductive_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn5fg/glyphstral24b_symbolic_deductive_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn5fg/glyphstral24b_symbolic_deductive_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T13:22:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikt9an</id>
    <title>I Built lfind: A Natural Language File Finder Using LLMs</title>
    <updated>2025-02-08T17:58:53+00:00</updated>
    <author>
      <name>/u/Mahrkeenerh1</name>
      <uri>https://old.reddit.com/user/Mahrkeenerh1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikt9an/i_built_lfind_a_natural_language_file_finder/"&gt; &lt;img alt="I Built lfind: A Natural Language File Finder Using LLMs" src="https://preview.redd.it/rwb26a0yeyhe1.gif?width=640&amp;amp;crop=smart&amp;amp;s=653ea170e4f0049c8cfde86a3d70e9eb14484f48" title="I Built lfind: A Natural Language File Finder Using LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mahrkeenerh1"&gt; /u/Mahrkeenerh1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rwb26a0yeyhe1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikt9an/i_built_lfind_a_natural_language_file_finder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikt9an/i_built_lfind_a_natural_language_file_finder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T17:58:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1il2vwi</id>
    <title>AI.com Now Redirects to DeepSeek</title>
    <updated>2025-02-09T01:07:28+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It looks like AI.com is now redirecting to DeepSeek instead of ChatGPT. This is a surprising move, considering that AI.com had been pointing to OpenAI’s ChatGPT for quite some time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il2vwi/aicom_now_redirects_to_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il2vwi/aicom_now_redirects_to_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1il2vwi/aicom_now_redirects_to_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T01:07:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikp5ko</id>
    <title>GeForce RTX 5090 fails to topple RTX 4090 in GPU compute benchmark.</title>
    <updated>2025-02-08T15:02:14+00:00</updated>
    <author>
      <name>/u/el0_0le</name>
      <uri>https://old.reddit.com/user/el0_0le</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So uh. Anyone have a good reason to upgrade from 4090 to 5090? &lt;/p&gt; &lt;p&gt;VRAM? Power? Paper specs? Future updates? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/el0_0le"&gt; /u/el0_0le &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.notebookcheck.net/GeForce-RTX-5090-fails-to-topple-RTX-4090-in-GPU-compute-benchmark-while-RTX-5080-struggles-against-RTX-4070-Ti.958334.0.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikp5ko/geforce_rtx_5090_fails_to_topple_rtx_4090_in_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikp5ko/geforce_rtx_5090_fails_to_topple_rtx_4090_in_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T15:02:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1il188r</id>
    <title>How Mistral, ChatGPT and DeepSeek handle sensitive topics</title>
    <updated>2025-02-08T23:46:11+00:00</updated>
    <author>
      <name>/u/Touch105</name>
      <uri>https://old.reddit.com/user/Touch105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il188r/how_mistral_chatgpt_and_deepseek_handle_sensitive/"&gt; &lt;img alt="How Mistral, ChatGPT and DeepSeek handle sensitive topics" src="https://external-preview.redd.it/Mzl3Zm8xc2Q1MGllMa1zjpINaNlAwSPaIvApdB3LTGQP8LNtAbklarHk8uDT.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce0df316532ed266983f35b08f4ad35ba2291539" title="How Mistral, ChatGPT and DeepSeek handle sensitive topics" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Touch105"&gt; /u/Touch105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qkdra4wd50ie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il188r/how_mistral_chatgpt_and_deepseek_handle_sensitive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1il188r/how_mistral_chatgpt_and_deepseek_handle_sensitive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T23:46:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1il27em</id>
    <title>DeepSeek-R1 (official website) is busy 90% of the time. It's near unusable. Is there away to use it without worrying about that, even if paid?</title>
    <updated>2025-02-09T00:33:42+00:00</updated>
    <author>
      <name>/u/GrayPsyche</name>
      <uri>https://old.reddit.com/user/GrayPsyche</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I find DeepSeek-R1 (reasoning) to be the single best model I have ever used for coding. The problem, however, is that I can barely use it. Their website always tells me &amp;quot;The server is busy. Please try again later.&amp;quot;&lt;/p&gt; &lt;p&gt;I wonder why they don't offer paid tiers or servers to help with the traffic? I don't mind paying as long as it's reasonably priced. The free servers will always be there for those who can't or won't pay. And paid servers for those who are willing to pay will ensure stability and uptime.&lt;/p&gt; &lt;p&gt;In the meantime, are there other AI services/wesbites that host the DeepSeek-R1 model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GrayPsyche"&gt; /u/GrayPsyche &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il27em/deepseekr1_official_website_is_busy_90_of_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il27em/deepseekr1_official_website_is_busy_90_of_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1il27em/deepseekr1_official_website_is_busy_90_of_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T00:33:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikyq45</id>
    <title>DeepSeek Gained over 100+ Millions Users in 20 days.</title>
    <updated>2025-02-08T21:52:58+00:00</updated>
    <author>
      <name>/u/blacktiger3654</name>
      <uri>https://old.reddit.com/user/blacktiger3654</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since launching DeepSeek R1 on January 20, DeepSeek has gained over 100 million users, with $0 advertising or marketing cost. By February 1, its daily active users surpassed 30 million, making it the fastest application in history to reach this milestone. &lt;/p&gt; &lt;p&gt;Why? I also spend so much time chat with it, the profound answer, is the key reason for me. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blacktiger3654"&gt; /u/blacktiger3654 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikyq45/deepseek_gained_over_100_millions_users_in_20_days/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikyq45/deepseek_gained_over_100_millions_users_in_20_days/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikyq45/deepseek_gained_over_100_millions_users_in_20_days/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T21:52:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikyclh</id>
    <title>My little setup grows</title>
    <updated>2025-02-08T21:36:13+00:00</updated>
    <author>
      <name>/u/Flintbeker</name>
      <uri>https://old.reddit.com/user/Flintbeker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikyclh/my_little_setup_grows/"&gt; &lt;img alt="My little setup grows" src="https://preview.redd.it/ivuoew07izhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=805b6dbac5a45f4acb9b1e6962f74ce91e1b6aaa" title="My little setup grows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flintbeker"&gt; /u/Flintbeker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ivuoew07izhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikyclh/my_little_setup_grows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikyclh/my_little_setup_grows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T21:36:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikvnfx</id>
    <title>I really need to upgrade</title>
    <updated>2025-02-08T19:38:41+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikvnfx/i_really_need_to_upgrade/"&gt; &lt;img alt="I really need to upgrade" src="https://preview.redd.it/eto6oiq8xyhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9be19e4af4ac7edf93efb804fef99881270152ec" title="I really need to upgrade" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eto6oiq8xyhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikvnfx/i_really_need_to_upgrade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikvnfx/i_really_need_to_upgrade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T19:38:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikvo8a</id>
    <title>Your next home lab might have 48GB Chinese card😅</title>
    <updated>2025-02-08T19:39:39+00:00</updated>
    <author>
      <name>/u/Redinaj</name>
      <uri>https://old.reddit.com/user/Redinaj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://wccftech.com/chinese-gpu-manufacturers-push-out-support-for-running-deepseek-ai-models-on-local-systems/"&gt;https://wccftech.com/chinese-gpu-manufacturers-push-out-support-for-running-deepseek-ai-models-on-local-systems/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Things are accelerating. China might give us all the VRAM we want. 😅😅👍🏼 Hope they don't make it illegal to import. For security sake, of course &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Redinaj"&gt; /u/Redinaj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikvo8a/your_next_home_lab_might_have_48gb_chinese_card/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikvo8a/your_next_home_lab_might_have_48gb_chinese_card/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikvo8a/your_next_home_lab_might_have_48gb_chinese_card/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T19:39:39+00:00</published>
  </entry>
</feed>
