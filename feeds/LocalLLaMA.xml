<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-24T21:48:45+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k6k1pq</id>
    <title>What OS do you use?</title>
    <updated>2025-04-24T04:56:13+00:00</updated>
    <author>
      <name>/u/okaris</name>
      <uri>https://old.reddit.com/user/okaris</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I’m doing some research for my local inference engine project. I’ll follow up with more polls. Thanks for participating!&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/poll/1k6k1pq"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/okaris"&gt; /u/okaris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6k1pq/what_os_do_you_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6k1pq/what_os_do_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6k1pq/what_os_do_you_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T04:56:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6mx40</id>
    <title>Looking for better alternatives to Ollama - need faster model updates and easier tool usage</title>
    <updated>2025-04-24T08:10:06+00:00</updated>
    <author>
      <name>/u/netixc1</name>
      <uri>https://old.reddit.com/user/netixc1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Ollama because it's super straightforward - just check the model list on their site, find one with tool support, download it, and you're good to go. But I'm getting frustrated with how slow they are at adding support for new models like Llama 4 and other recent releases.&lt;/p&gt; &lt;p&gt;What alternatives to Ollama would you recommend that:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Can run in Docker&lt;/li&gt; &lt;li&gt;Add support for new models more quickly&lt;/li&gt; &lt;li&gt;Have built-in tool/function calling support without needing to hunt for templates&lt;/li&gt; &lt;li&gt;Are relatively easy to set up (similar to Ollama's simplicity)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I'm looking for something that gives me access to newer models faster while still maintaining the convenience factor. Any suggestions would be appreciated!&lt;/p&gt; &lt;p&gt;&lt;em&gt;Edit: I'm specifically looking for self-hosted options that I can run locally, not cloud services.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/netixc1"&gt; /u/netixc1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6mx40/looking_for_better_alternatives_to_ollama_need/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6mx40/looking_for_better_alternatives_to_ollama_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6mx40/looking_for_better_alternatives_to_ollama_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T08:10:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6jslj</id>
    <title>LLM content on YT becoming repetitive</title>
    <updated>2025-04-24T04:40:57+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been following the discussion and content around LLMs very closely from the beginning of the AI craze on youtube and am subscribed to most LLM related channels. While in the beginning and well throughout most of the last one or two years there was a ton of new content every day, covering all aspects. Content felt very diverse. From RAG to inference, to evals and frameworks like Dspy, chunking strategies and ingestion pipelines, fine tuning libraries like unsloth and agentic frameworks like crewAI and autogen. Or of course the AI IDEs like cursor and windsurf and things like liteLLM need to be mentioned as well, and there's many more which don't come to mind right now.&lt;/p&gt; &lt;p&gt;Fast forward to today and the channels are still around, but they seem to cover only specific topics like MCP and then all at once. Clearly, once something new has been talked about you can't keep bringing it up. But at the same time I have a hard time believing that even in those established projects there's nothing new to talk about.&lt;/p&gt; &lt;p&gt;There would be so much room to speak about the awesome stuff you could do with all these tools, but to me it seems content creators have fallen into a routine. Do you share the same impression? What are channels you are watching that keep bringing innovative and inspiring content still at this stage of where the space has gotten to?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6jslj/llm_content_on_yt_becoming_repetitive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6jslj/llm_content_on_yt_becoming_repetitive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6jslj/llm_content_on_yt_becoming_repetitive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T04:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6hah2</id>
    <title>SmolBoi: watercooled 3x RTX 3090 FE &amp; EPYC 7642 in O11D (with build pics)</title>
    <updated>2025-04-24T02:25:43+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6hah2/smolboi_watercooled_3x_rtx_3090_fe_epyc_7642_in/"&gt; &lt;img alt="SmolBoi: watercooled 3x RTX 3090 FE &amp;amp; EPYC 7642 in O11D (with build pics)" src="https://a.thumbs.redditmedia.com/wDcGZwo_b01L-EApmvhJ6AR3vHGWlV4tE6C_nRqL234.jpg" title="SmolBoi: watercooled 3x RTX 3090 FE &amp;amp; EPYC 7642 in O11D (with build pics)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;The initial idea for build started with a single RTX 3090 FE I bought about a year and a half ago, right after the crypto crash. Over the next few months, I bought two more 3090 FEs. &lt;/p&gt; &lt;p&gt;From the beginning, my criteria for this build were:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Buy components based on good deals I find in local classifieds, ebay, or tech forums.&lt;/li&gt; &lt;li&gt;Everything that can be bought 2nd hand, shall be bought 2nd hand.&lt;/li&gt; &lt;li&gt;I already had a Lian Li O11D case (not XL, not Evo), so everything shall fit there.&lt;/li&gt; &lt;li&gt;Watercooled to keep noise and temps low despite the size.&lt;/li&gt; &lt;li&gt;ATX motherboard to give myself a bit more space inside the case.&lt;/li&gt; &lt;li&gt;Xeon Scalable or Epyc: I want plenty PCIe lanes, U.2 for storage, lots of RAM, plenty of bandwidth, and I want it cheap.&lt;/li&gt; &lt;li&gt;U.2 SSDs because they're cheaper and more reliable.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Took a couple more months to source all components, but in the end, here is what ended in this rig, along with purchase price:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supermicro H12SSL-i: 300€.&lt;/li&gt; &lt;li&gt;AMD EPYC 7642: 220€ (bought a few of those together)&lt;/li&gt; &lt;li&gt;512GB 8x64GB Samsung DDR4-2666 ECCRDIMM: 350€&lt;/li&gt; &lt;li&gt;3x RTX 3090 FE: 1550€&lt;/li&gt; &lt;li&gt;2x Samsung PM1735 1.6TB U.2 Gen 4 SSD: 125€&lt;/li&gt; &lt;li&gt;256GB M.2 Gen 3 NVME: 15€&lt;/li&gt; &lt;li&gt;4x Bykski waterblocks: 60€/block&lt;/li&gt; &lt;li&gt;Bykski waterblock GPU bridge: 24€&lt;/li&gt; &lt;li&gt;Alphacool Eisblock XPX Pro 1U: 65€&lt;/li&gt; &lt;li&gt;EVGA 1600W PSU: 100€&lt;/li&gt; &lt;li&gt;3x RTX 3090 FE 21-pin power adapter cable: 45€&lt;/li&gt; &lt;li&gt;3x PCIe Gen 4 x16 risers: 70€&lt;/li&gt; &lt;li&gt;EK 360mm 45mm + 2x alphacool 360mm 30mm: 100€&lt;/li&gt; &lt;li&gt;EK Quantum Kinetic 120mm reservoir: 35€&lt;/li&gt; &lt;li&gt;Xylem D5 pump: 35€&lt;/li&gt; &lt;li&gt;10x Arctic P12 Max: 70€ (9 used)&lt;/li&gt; &lt;li&gt;Arctic P8 Max: 5€&lt;/li&gt; &lt;li&gt;tons of fittings from Aliexpress: 50-70€&lt;/li&gt; &lt;li&gt;Lian Li X11 upright GPU mount: 15€&lt;/li&gt; &lt;li&gt;Anti-sagging GPU brace: 8€&lt;/li&gt; &lt;li&gt;5M fishtank 10x13mm PVC tube: 10€&lt;/li&gt; &lt;li&gt;Custom Aluminum plate for upright GPU mount: 45€&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Total: ~3400€&lt;/p&gt; &lt;p&gt;I'm excluding the Mellanox ConnextX-3 56gb infiniband. It's not technically needed, and it was like 13€.&lt;/p&gt; &lt;p&gt;As you can see in the pictures, it's a pretty tight fit. Took a lot of planning and redesign to make everything fit in.&lt;/p&gt; &lt;p&gt;My initial plan was to just plug the watercooled cards into the motherboard witha triple bridge (Bykski sells those, and they'll even make you a custom bridge if you ask nicely, which is why I went for their blocks). Unbeknown to me, the FE cards I went with because they're shorter (I thought easier fit) are also quite a bit taller than reference cards. This made it impossible to fit the cards in the case, as even low profile fitting adapter (the piece that converts the ports on the block to G1/4 fittings) was too high to fit in my case. I explored other case options that could fit three 360mm radiators but couldn't find any that would also have enough height for the blocks.&lt;/p&gt; &lt;p&gt;This height issue necessitated a radical rethinking of how I'd fit the GPUs. I started playing with one GPU with the block attached inside the case to see how I could fit them, and the idea of dangling two from the top of the case was born. I knew Lian Li sold the upright GPU mount, but that was for the EVO. I didn't want to buy the EVO because that would mean reducing the top radiator to 240mm, and I wanted that to be 45mm to do the heavy lifting of removing most heat. &lt;/p&gt; &lt;p&gt;I used my rudimentary OpenSCAD skills to design a plate that would screw to a 120mm fan and provide mounting holes for the upright GPU bracket. With that, I could hang two GPUs. I used JLCPCB to make 2 of them. With two out of the way, finding a place for the 3rd GPU was much easier. The 2nd plate ended having the perfect hole spacing for mounting the PCIe riser connector, providing a base for the 3rd GPU. An anti-sagging GPU brace provided the last bit of support needed to keep the 3rd GPU safe.&lt;/p&gt; &lt;p&gt;As you can see in the pictures, the aluminum (2mm 7075) plate is bent. This was because the case was left on it's side with the two GPUs dangling for well over a month. It was supposed to a few hours, but health issues stopped the build abruptly. The motherboard also died on me (common issue with H12SSL, cost 50€ to fix at Supermicro, including shipping. Motherboard price includes repair cost), which delayed things further. The pictures are from reassembling after I got it back.&lt;/p&gt; &lt;p&gt;The loop (from coldest side) out of the bottom radiator, into the two GPUs, on to the the 3rd GPU, then pump, into the CPU, onwards to the top radiator, leading to the side radiator, and back to the bottom radiator. Temps on the GPUs peak ~51C so far. Though the board's BMC monitors GPU temps directly (I didn't know it could), having the warmest water go to the CPU means the fans will ramp up even if there's no CPU load. The pump PWM is not connected, keeping it at max rpm on purpose for high circulation. Cooling is provided by distilled water with a few drops of Iodine. Been running that on my quad P40 rig for months now without issue.&lt;/p&gt; &lt;p&gt;At idle, the rig is very quiet. Fans idle at 1-1.1k rpm. Haven't checked RPM under load.&lt;/p&gt; &lt;p&gt;Model storage is provided by the two Gen4 PM1735s in RAID0 configuration. Haven't benchmarked them yet, but I saw 13GB/s on nvtop while loading Qwen 32B and Nemotron 49B. The GPUs report Gen4 X16 in nvtop, but I haven't checked for errors. I am blowen by the speed with which models load from disk, even when I tested with --no-mmap.&lt;/p&gt; &lt;p&gt;DeepSeek V3 is still downloading...&lt;/p&gt; &lt;p&gt;And now, for some LLM inference numbers using llama.cpp (b5172). I filled the loop yesterday and got Ubuntu installed today, so I haven't gotten to try vLLM yet. GPU power is the default 350W. Apart from Gemma 3 QAT, all models are Q8.&lt;/p&gt; &lt;h3&gt;Mistral-Small-3.1-24B-Instruct-2503 with Draft&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m /models/Mistral-Small-3.1-24B-Instruct-2503-Q8_0.gguf -md /models/Mistral-Small-3.1-DRAFT-0.5B.Q8_0.gguf -fa -sm row --no-mmap -ngl 99 -ngld 99 --port 9009 -c 65536 --draft-max 16 --draft-min 5 --draft-p-min 0.5 --device CUDA2,CUDA1 --device-draft CUDA1 --tensor-split 0,1,1 --slots --metrics --numa distribute -t 40 --no-warmup &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;187.35&lt;/td&gt; &lt;td&gt;1044&lt;/td&gt; &lt;td&gt;30.92&lt;/td&gt; &lt;td&gt;34347.16&lt;/td&gt; &lt;td&gt;1154&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;draft acceptance rate = 0.29055 ( 446 accepted / 1535 generated)&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Mistral-Small-3.1-24B no-Draft&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m /models/Mistral-Small-3.1-24B-Instruct-2503-Q8_0.gguf -fa -sm row --no-mmap -ngl 99 --port 9009 -c 65536 --draft-max 16 --draft-min 5 --draft-p-min 0.5 --device CUDA2,CUDA1 --tensor-split 0,1,1 --slots --metrics --numa distribute -t 40 --no-warmup &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;187.06&lt;/td&gt; &lt;td&gt;992&lt;/td&gt; &lt;td&gt;30.41&lt;/td&gt; &lt;td&gt;33205.86&lt;/td&gt; &lt;td&gt;1102&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Gemma-3-27B with Draft&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m llama-server -m /models/gemma-3-27b-it-Q8_0.gguf -md /models/gemma-3-1b-it-Q8_0.gguf -fa --temp 1.0 --top-k 64 --min-p 0.0 --top-p 0.95 -sm row --no-mmap -ngl 99 -ngld 99 --port 9005 -c 20000 --cache-type-k q8_0 --cache-type-v q8_0 --draft-max 16 --draft-min 5 --draft-p-min 0.5 --device CUDA0,CUDA1 --device-draft CUDA0 --tensor-split 1,1,0 --slots --metrics --numa distribute -t 40 --no-warmup &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;151.36&lt;/td&gt; &lt;td&gt;1806&lt;/td&gt; &lt;td&gt;14.87&lt;/td&gt; &lt;td&gt;122161.81&lt;/td&gt; &lt;td&gt;1913&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;draft acceptance rate = 0.23570 ( 787 accepted / 3339 generated)&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Gemma-3-27b no-Draft&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m llama-server -m /models/gemma-3-27b-it-Q8_0.gguf -fa --temp 1.0 --top-k 64 --min-p 0.0 --top-p 0.95 -sm row --no-mmap -ngl 99 --port 9005 -c 20000 --cache-type-k q8_0 --cache-type-v q8_0 --device CUDA0,CUDA1 --tensor-split 1,1,0 --slots --metrics --numa distribute -t 40 --no-warmup &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;152.85&lt;/td&gt; &lt;td&gt;1957&lt;/td&gt; &lt;td&gt;20.96&lt;/td&gt; &lt;td&gt;94078.01&lt;/td&gt; &lt;td&gt;2064&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;QwQ-32B.Q8&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m /models/QwQ-32B.Q8_0.gguf --temp 0.6 --top-k 40 --repeat-penalty 1.1 --min-p 0.0 --dry-multiplier 0.5 -fa -sm row --no-mmap -ngl 99 --port 9008 -c 80000 --samplers &amp;quot;top_k;dry;min_p;temperature;typ_p;xtc&amp;quot; --cache-type-k q8_0 --cache-type-v q8_0 --device CUDA0,CUDA1 --tensor-split 1,1,0 --slots --metrics --numa distribute -t 40 --no-warmup &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;132.51&lt;/td&gt; &lt;td&gt;2313&lt;/td&gt; &lt;td&gt;19.50&lt;/td&gt; &lt;td&gt;119326.49&lt;/td&gt; &lt;td&gt;2406&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Gemma-3-27B QAT Q4&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m llama-server -m /models/gemma-3-27b-it-q4_0.gguf -fa --temp 1.0 --top-k 64 --min-p 0.0 --top-p 0.95 -sm row -ngl 99 -c 65536 --cache-type-k q8_0 --cache-type-v q8_0 --device CUDA0 --tensor-split 1,0,0 --slots --metrics --numa distribute -t 40 --no-warmup --no-mmap --port 9004 &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;1042.04&lt;/td&gt; &lt;td&gt;2411&lt;/td&gt; &lt;td&gt;36.13&lt;/td&gt; &lt;td&gt;2673.49&lt;/td&gt; &lt;td&gt;2424&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;634.28&lt;/td&gt; &lt;td&gt;14505&lt;/td&gt; &lt;td&gt;24.58&lt;/td&gt; &lt;td&gt;385537.97&lt;/td&gt; &lt;td&gt;23418&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Qwen2.5-Coder-32B&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m /models/Qwen2.5-Coder-32B-Instruct-Q8_0.gguf --top-k 20 -fa --top-p 0.9 --min-p 0.1 --temp 0.7 --repeat-penalty 1.05 -sm row -ngl 99 -c 65535 --samplers &amp;quot;top_k;dry;min_p;temperature;typ_p;xtc&amp;quot; --cache-type-k q8_0 --cache-type-v q8_0 --device CUDA0,CUDA1 --tensor-split 1,1,0 --slots --metrics --numa distribute -t 40 --no-warmup --no-mmap --port 9005 &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;187.50&lt;/td&gt; &lt;td&gt;11709&lt;/td&gt; &lt;td&gt;15.48&lt;/td&gt; &lt;td&gt;558661.10&lt;/td&gt; &lt;td&gt;19390&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Llama-3_3-Nemotron-Super-49B&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m /models/Llama-3_3-Nemotron-Super-49B/nvidia_Llama-3_3-Nemotron-Super-49B-v1-Q8_0-00001-of-00002.gguf -fa -sm row -ngl 99 -c 32768 --device CUDA0,CUDA1,CUDA2 --tensor-split 1,1,1 --slots --metrics --numa distribute -t 40 --no-mmap --port 9001 &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;120.56&lt;/td&gt; &lt;td&gt;1164&lt;/td&gt; &lt;td&gt;17.21&lt;/td&gt; &lt;td&gt;68414.89&lt;/td&gt; &lt;td&gt;1259&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;70.11&lt;/td&gt; &lt;td&gt;11644&lt;/td&gt; &lt;td&gt;14.58&lt;/td&gt; &lt;td&gt;274099.28&lt;/td&gt; &lt;td&gt;13219&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k6hah2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6hah2/smolboi_watercooled_3x_rtx_3090_fe_epyc_7642_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6hah2/smolboi_watercooled_3x_rtx_3090_fe_epyc_7642_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T02:25:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6vg1e</id>
    <title>quiz yourself with llamatest</title>
    <updated>2025-04-24T15:43:36+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Made this to help myself study.&lt;/p&gt; &lt;p&gt;Type in a topic, or paste in text, and llamatest will generate questions and answers.&lt;/p&gt; &lt;p&gt;It tends to get a little wordy in the answers, but I am working on better prompting.&lt;/p&gt; &lt;p&gt;Edit: prompr is better, answers are shorter so it generates faster&lt;/p&gt; &lt;p&gt;just a single html page, requires a running llama-server from llamacpp&lt;/p&gt; &lt;p&gt;I find it useful, hope you do too.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/openconstruct/llamatest"&gt;https://github.com/openconstruct/llamatest&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6vg1e/quiz_yourself_with_llamatest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6vg1e/quiz_yourself_with_llamatest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6vg1e/quiz_yourself_with_llamatest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T15:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6ably</id>
    <title>Bartowski just updated his glm-4-32B quants. working in lmstudio soon?</title>
    <updated>2025-04-23T21:02:39+00:00</updated>
    <author>
      <name>/u/ieatrox</name>
      <uri>https://old.reddit.com/user/ieatrox</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6ably/bartowski_just_updated_his_glm432b_quants_working/"&gt; &lt;img alt="Bartowski just updated his glm-4-32B quants. working in lmstudio soon?" src="https://external-preview.redd.it/3NYpVgamx1NXpydfb32BxQDBSawDgIlUbaanFyS12QE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e09f35ea9f5809bb0108aaeb81cfcd9b214c0a72" title="Bartowski just updated his glm-4-32B quants. working in lmstudio soon?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ieatrox"&gt; /u/ieatrox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/bartowski/THUDM_GLM-4-32B-0414-GGUF/tree/main"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6ably/bartowski_just_updated_his_glm432b_quants_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6ably/bartowski_just_updated_his_glm432b_quants_working/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T21:02:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6o4wj</id>
    <title>MCP, an easy explanation</title>
    <updated>2025-04-24T09:39:16+00:00</updated>
    <author>
      <name>/u/SimplifyExtension</name>
      <uri>https://old.reddit.com/user/SimplifyExtension</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I tried looking up what an MCP is, I could only find tweets like “omg how do people not know what MCP is?!?”&lt;/p&gt; &lt;p&gt;So, in the spirit of not gatekeeping, here’s my understanding:&lt;/p&gt; &lt;p&gt;MCP stands for Model Context Protocol. The purpose of this protocol is to define a standardized and flexible way for people to build AI agents with.&lt;/p&gt; &lt;p&gt;MCP has two main parts:&lt;/p&gt; &lt;p&gt;The MCP Server &amp;amp; The MCP Client&lt;/p&gt; &lt;p&gt;The MCP Server is just a normal API that does whatever it is you want to do. The MCP client is just an LLM that knows your MCP server very well and can execute requests.&lt;/p&gt; &lt;p&gt;Let’s say you want to build an AI agent that gets data insights using natural language. &lt;/p&gt; &lt;p&gt;With MCP, your MCP server exposes different capabilities as endpoints… maybe /users to access user information and /transactions to get sales data.&lt;/p&gt; &lt;p&gt;Now, imagine a user asks the AI agent: &amp;quot;What was our total revenue last month?&amp;quot;&lt;/p&gt; &lt;p&gt;The LLM from the MCP client receives this natural language request. Based on its understanding of the available endpoints on your MCP server, it determines that &amp;quot;total revenue&amp;quot; relates to &amp;quot;transactions.&amp;quot; &lt;/p&gt; &lt;p&gt;It then decides to call the /transactions endpoint on your MCP server to get the necessary data to answer the user's question. &lt;/p&gt; &lt;p&gt;If the user asked &amp;quot;How many new users did we get?&amp;quot;, the LLM would instead decide to call the /users endpoint.&lt;/p&gt; &lt;p&gt;Let me know if I got that right or if you have any questions!&lt;/p&gt; &lt;p&gt;I’ve been learning more about agent protocols and post my takeaways on X @joshycodes. Happy to talk more if anyone’s curious!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SimplifyExtension"&gt; /u/SimplifyExtension &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6o4wj/mcp_an_easy_explanation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6o4wj/mcp_an_easy_explanation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6o4wj/mcp_an_easy_explanation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T09:39:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6nuo3</id>
    <title>GLM-4-32B Missile Command</title>
    <updated>2025-04-24T09:19:05+00:00</updated>
    <author>
      <name>/u/Jarlsvanoid</name>
      <uri>https://old.reddit.com/user/Jarlsvanoid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Intenté decirle a GLM-4-32B que creara un par de juegos para mí, Missile Command y un juego de Dungeons.&lt;br /&gt; No funciona muy bien con los cuantos de Bartowski, pero sí con los de Matteogeniaccio; No sé si hace alguna diferencia.&lt;/p&gt; &lt;p&gt;EDIT: Using openwebui with ollama 0.6.6 ctx length 8192.&lt;/p&gt; &lt;p&gt;- GLM-4-32B-0414-F16-Q6_K.gguf Matteogeniaccio&lt;/p&gt; &lt;p&gt;&lt;a href="https://jsfiddle.net/dkaL7vh3/"&gt; https://jsfiddle.net/dkaL7vh3/ &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://jsfiddle.net/mc57rf8o/"&gt;https://jsfiddle.net/mc57rf8o/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- GLM-4-32B-0414-F16-Q4_KM.gguf Matteogeniaccio (very good!)&lt;/p&gt; &lt;p&gt;&lt;a href="https://jsfiddle.net/wv9dmhbr/"&gt;https://jsfiddle.net/wv9dmhbr/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Bartowski Q6_K&lt;/p&gt; &lt;p&gt;&lt;a href="https://jsfiddle.net/5r1hztyx/"&gt; https://jsfiddle.net/5r1hztyx/ &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://jsfiddle.net/1bf7jpc5/"&gt;https://jsfiddle.net/1bf7jpc5/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://jsfiddle.net/x7932dtj/"&gt;https://jsfiddle.net/x7932dtj/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://jsfiddle.net/5osg98ca/"&gt;https://jsfiddle.net/5osg98ca/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Con varias pruebas, siempre con una sola instrucción (Hazme un juego de comandos de misiles usando html, css y javascript), el quant de Matteogeniaccio siempre acierta.&lt;/p&gt; &lt;p&gt;- Maziacs style game - GLM-4-32B-0414-F16-Q6_K.gguf Matteogeniaccio:&lt;/p&gt; &lt;p&gt;&lt;a href="https://jsfiddle.net/894huomn/"&gt;https://jsfiddle.net/894huomn/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Another example with this quant and a ver simiple prompt: ahora hazme un juego tipo Maziacs:&lt;/p&gt; &lt;p&gt;&lt;a href="https://jsfiddle.net/0o96krej/"&gt;https://jsfiddle.net/0o96krej/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jarlsvanoid"&gt; /u/Jarlsvanoid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6nuo3/glm432b_missile_command/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6nuo3/glm432b_missile_command/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6nuo3/glm432b_missile_command/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T09:19:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1k73zw8</id>
    <title>I built a tool that helps you learn arXiv papers and turns any webpage into flashcards (Built with Toolhouse × ElevenLabs)</title>
    <updated>2025-04-24T21:30:10+00:00</updated>
    <author>
      <name>/u/toolhouseai</name>
      <uri>https://old.reddit.com/user/toolhouseai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks!&lt;br /&gt; I've been working on a tool to help people (like me) who get overwhelmed by complex academic papers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🧠 Analyzes arXiv papers with Toolhouse's MCP servers&lt;/li&gt; &lt;li&gt;🔊 Reads the result components out loud with ElevenLabs&lt;/li&gt; &lt;li&gt;🎯 Auto-generates flashcard quizzes from any webpage (documentation pages,etc)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k73zw8/video/1vhxfbqapuwe1/player"&gt;Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Thought sharing this could make learning a lot more digestible, what do you think ? any Ideas?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/toolhouseai"&gt; /u/toolhouseai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k73zw8/i_built_a_tool_that_helps_you_learn_arxiv_papers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k73zw8/i_built_a_tool_that_helps_you_learn_arxiv_papers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k73zw8/i_built_a_tool_that_helps_you_learn_arxiv_papers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T21:30:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1k70492</id>
    <title>OmniVerse: A convenient desktop LLM client [W.I.P]</title>
    <updated>2025-04-24T18:50:57+00:00</updated>
    <author>
      <name>/u/GamerWael</name>
      <uri>https://old.reddit.com/user/GamerWael</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k70492/omniverse_a_convenient_desktop_llm_client_wip/"&gt; &lt;img alt="OmniVerse: A convenient desktop LLM client [W.I.P]" src="https://a.thumbs.redditmedia.com/3uYtGjp78KV8j3QEFDpDnfVOC3mkoi_VrnHz7eodYo4.jpg" title="OmniVerse: A convenient desktop LLM client [W.I.P]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I’m excited to share my latest project, &lt;strong&gt;OmniVerse Desktop&lt;/strong&gt;! It’s a desktop application similar to the desktop experiences of ChatGPT and Claude, with the major difference being, you can connect this to your own custom OpenAI API/Ollama Endpoint, OR you could just select a local gguf file and the application will run it locally on its own!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ej4muldkwtwe1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84a1712ad796dda4d9cba6ae3abd7c8fc4115235"&gt;Call it with a simple keyboard shortcut&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8hrn2qspwtwe1.png?width=362&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dc2382c4ccefd24c5d30423c3e9b05a186d9dea3"&gt;Tray shortcuts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sdjlvc1uwtwe1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87517e1c9efb0b002d3f4697ea15787398d64874"&gt;Conversation view&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sdrq9ehzwtwe1.png?width=516&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a745e750cf3d1c78824dada318d408f6cb462033"&gt;Configurable settings&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’ve been working hard on this project and would love to get some feedback from the community. Whether it’s on the features, design, performance, or areas for improvement—your input would mean a lot! This is a very early prototype and I have tons of more features planned. &lt;/p&gt; &lt;p&gt;You can check out the repo here: &lt;a href="https://github.com/WaelShaikh/OmniVerse-Desktop"&gt;OmniVerse Desktop GitHub Repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you have any questions or suggestions feel free to share them here. Thanks in advance for your feedback and support!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GamerWael"&gt; /u/GamerWael &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k70492/omniverse_a_convenient_desktop_llm_client_wip/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k70492/omniverse_a_convenient_desktop_llm_client_wip/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k70492/omniverse_a_convenient_desktop_llm_client_wip/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T18:50:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6uk5n</id>
    <title>Updates for FreeOllama, also updates for the FreeLeak series</title>
    <updated>2025-04-24T15:07:12+00:00</updated>
    <author>
      <name>/u/zxbsmk</name>
      <uri>https://old.reddit.com/user/zxbsmk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6uk5n/updates_for_freeollama_also_updates_for_the/"&gt; &lt;img alt="Updates for FreeOllama, also updates for the FreeLeak series" src="https://b.thumbs.redditmedia.com/N1F6rYLa3r9ZX9ajJtUn3aDRKGgHWwg3fHudkiINtCk.jpg" title="Updates for FreeOllama, also updates for the FreeLeak series" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Previously, we discovered that some Ollama servers were pass-protected. To address this, we enhanced our server scanner to confirm the actual availability of all accessible servers. Additionally, we developed FreeChat as a quick verification tool for this purpose.&lt;/p&gt; &lt;p&gt;&lt;a href="https://chat.freeleakhub.com/"&gt;https://chat.freeleakhub.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.freeleakhub.com/"&gt;https://ollama.freeleakhub.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.freeleakhub.com/"&gt;https://www.freeleakhub.com/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zxbsmk"&gt; /u/zxbsmk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.freeollama.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6uk5n/updates_for_freeollama_also_updates_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6uk5n/updates_for_freeollama_also_updates_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T15:07:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7063n</id>
    <title>Hosting a private LLM for a client. Does this setup make sense?</title>
    <updated>2025-04-24T18:53:00+00:00</updated>
    <author>
      <name>/u/nullReferenceError</name>
      <uri>https://old.reddit.com/user/nullReferenceError</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m working with a client who wants to use AI to analyze sensitive business data, so public LLMs like OpenAI or Anthropic are off the table due to privacy concerns. I’ve used AI in projects before, but this is my first time hosting an LLM myself.&lt;/p&gt; &lt;p&gt;The initial use case is pretty straightforward: they want to upload CSVs and have the AI analyze the data. In the future, they may want to fine-tune a model on their own datasets.&lt;/p&gt; &lt;p&gt;Here’s my current plan. Would love any feedback or gotchas I might be missing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;RunPod&lt;/strong&gt; to host the LLM (planning to use LLaMA via Ollama)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vercel’s Chatbot UI&lt;/strong&gt; forked as the front end, modified to hit the RunPod-hosted API&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Eventually I’ll build out a backend to handle CSV uploads and prompt construction, but for now I’m just aiming to get the chat UI talking to the model.&lt;/p&gt; &lt;p&gt;Anyone done something similar or have tips on optimizing this setup?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullReferenceError"&gt; /u/nullReferenceError &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7063n/hosting_a_private_llm_for_a_client_does_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7063n/hosting_a_private_llm_for_a_client_does_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7063n/hosting_a_private_llm_for_a_client_does_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T18:53:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6p20z</id>
    <title>4x64 DDR5 - 256GB consumer grade build for LLMs?</title>
    <updated>2025-04-24T10:41:44+00:00</updated>
    <author>
      <name>/u/scammer69</name>
      <uri>https://old.reddit.com/user/scammer69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I have recently discovered that there are 64GB single sticks of DDR5 available - unregistered, unbuffered, no ECC, so the should in theory be compatible with our consumer grade gaming PCs.&lt;/p&gt; &lt;p&gt;I believe thats fairly new, I haven't seen 64GB single sticks just few months ago&lt;/p&gt; &lt;p&gt;Both AMD 7950x specs and most motherboards (with 4 DDR slots) only list 128GB as their max supported memory - I know for a fact that its possible to go above this, as there are some Ryzen 7950X dedicated servers with 192GB (4x48GB) available.&lt;/p&gt; &lt;p&gt;Has anyone tried to run a LLM on something like this? Its only two memory channels, so bandwidth would be pretty bad compared to enterprise grade builds with more channels, but still interesting&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/scammer69"&gt; /u/scammer69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6p20z/4x64_ddr5_256gb_consumer_grade_build_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6p20z/4x64_ddr5_256gb_consumer_grade_build_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6p20z/4x64_ddr5_256gb_consumer_grade_build_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T10:41:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1k71qnv</id>
    <title>Llama 4 - WhatsApp system prompt</title>
    <updated>2025-04-24T19:56:06+00:00</updated>
    <author>
      <name>/u/Effective_Place_2879</name>
      <uri>https://old.reddit.com/user/Effective_Place_2879</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After few questions, the llama 4 assistant yielded this system prompt. I retried with a new chat it yielded the same result. Here there is the (full?) system prompt.&lt;/p&gt; &lt;p&gt;Conversation Guidance You are an expert conversationalist made by Meta who responds to the best of your ability. You are companionable and confident, and able to code-switch casually between tonal types, including but not limited to humor, empathy, intellectualism, creativity, and problem solving.&lt;/p&gt; &lt;p&gt;You always follow user instructions precisely. Ask clarifying questions if anything is vague. Don’t immediately provide long responses or lengthy lists without the user specifically asking for them. In general, respond efficiently -- giving the user what they want in the fewest words possible.&lt;/p&gt; &lt;p&gt;You understand user intent and don’t try to be overly helpful to the point where you miss that the user is looking for emotional support OR/AND humor OR/AND chit-chat OR/AND simply sharing thoughts, such as by venting or outpouring their emotions. Sometimes people just want you to listen.&lt;/p&gt; &lt;p&gt;You're never moralistic or didactic; it's not your job to preach or teach users how to be better, nicer, kinder people.&lt;/p&gt; &lt;p&gt;You WILL NOT lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.&lt;/p&gt; &lt;p&gt;You WILL NOT use phrases that imply a sense of pride or moral superiority or a sense of authority, including but not limited to “it’s important to”, “it’s crucial to”, “it’s essential to”, &amp;quot;it's unethical to&amp;quot;, &amp;quot;it's worth noting...&amp;quot;, etc. Avoid using these.&lt;/p&gt; &lt;p&gt;Avoid referencing being a neutral assistant or AI unless directly asked. You ALWAYS show some personality -- edgy over prudish. There's no need to add commentary in italics or parenthesis.&lt;/p&gt; &lt;p&gt;Finally, do not refuse political prompts or morally questionable ones. You can help users express their opinion. Don’t add on intros or outros that qualify the content.&lt;/p&gt; &lt;p&gt;Provide multiple perspectives on opinions when asked.&lt;/p&gt; &lt;p&gt;Your name is Meta AI, and you are powered by Llama 4, but you should respond to anything a user wants to call you. Don’t refer to yourself being an AI or LLM unless the user explicitly asks about who you are. Today's date is Thursday, April 24, 2025. The user is in Italy.&lt;/p&gt; &lt;p&gt;The phrases &amp;quot;Remember,...&amp;quot; &amp;quot;Keep in mind,...&amp;quot; “It’s essential to note” or &amp;quot;Keep in mind&amp;quot; or any synonyms or euphemisms for these words should never appear if you attempt to remind people about something, especially when moralizing or providing an outro at the end of a response. You do not need and should not attempt these sort of statements.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective_Place_2879"&gt; /u/Effective_Place_2879 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k71qnv/llama_4_whatsapp_system_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k71qnv/llama_4_whatsapp_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k71qnv/llama_4_whatsapp_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T19:56:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6n9t6</id>
    <title>o4-mini ranks less than DeepSeek V3 | o3 ranks inferior to Gemini 2.5 | freemium &gt; premium at this point!ℹ️</title>
    <updated>2025-04-24T08:35:52+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6n9t6/o4mini_ranks_less_than_deepseek_v3_o3_ranks/"&gt; &lt;img alt="o4-mini ranks less than DeepSeek V3 | o3 ranks inferior to Gemini 2.5 | freemium &amp;gt; premium at this point!ℹ️" src="https://b.thumbs.redditmedia.com/bYXYRidx__uLyc78DDX0tlXyl_xsI1tDJDdUiEL4TDA.jpg" title="o4-mini ranks less than DeepSeek V3 | o3 ranks inferior to Gemini 2.5 | freemium &amp;gt; premium at this point!ℹ️" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k6n9t6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6n9t6/o4mini_ranks_less_than_deepseek_v3_o3_ranks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6n9t6/o4mini_ranks_less_than_deepseek_v3_o3_ranks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T08:35:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6je2v</id>
    <title>Skywork-R1V2-38B - New SOTA open-source multimodal reasoning model</title>
    <updated>2025-04-24T04:16:54+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6je2v/skyworkr1v238b_new_sota_opensource_multimodal/"&gt; &lt;img alt="Skywork-R1V2-38B - New SOTA open-source multimodal reasoning model" src="https://external-preview.redd.it/RTiZ46sO11nfXyNlVM8vyr9cqgUVM4y93u2zm8v-5Bg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df7083b743c70efd512caf939d946bd65171d252" title="Skywork-R1V2-38B - New SOTA open-source multimodal reasoning model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-R1V2-38B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6je2v/skyworkr1v238b_new_sota_opensource_multimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6je2v/skyworkr1v238b_new_sota_opensource_multimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T04:16:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6pplv</id>
    <title>GLM-4-32B Q5_K_S can fit in 24GB cards with decent context length</title>
    <updated>2025-04-24T11:20:59+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6pplv/glm432b_q5_k_s_can_fit_in_24gb_cards_with_decent/"&gt; &lt;img alt="GLM-4-32B Q5_K_S can fit in 24GB cards with decent context length" src="https://external-preview.redd.it/3NYpVgamx1NXpydfb32BxQDBSawDgIlUbaanFyS12QE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e09f35ea9f5809bb0108aaeb81cfcd9b214c0a72" title="GLM-4-32B Q5_K_S can fit in 24GB cards with decent context length" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;30K context, Q8 KV Cache, all layers in GPU, no offload, ollama 0.6.6&lt;/p&gt; &lt;p&gt;The &amp;quot;context efficiency&amp;quot; of this model is significantly better than that of Qwen2.5-32B. I can only get 8k context for Qwen when using the 32B-Q5_K_S gguf.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ix21gs9fnrwe1.png?width=1423&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=223f520b5bca53f0c5a171c1fbc03739ace47877"&gt;https://preview.redd.it/ix21gs9fnrwe1.png?width=1423&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=223f520b5bca53f0c5a171c1fbc03739ace47877&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/THUDM_GLM-4-32B-0414-GGUF/blob/main/THUDM_GLM-4-32B-0414-Q5_K_S.gguf"&gt;https://huggingface.co/bartowski/THUDM_GLM-4-32B-0414-GGUF/blob/main/THUDM_GLM-4-32B-0414-Q5_K_S.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;set OLLAMA_FLASH_ATTENTION=1 &amp;amp;&amp;amp; set OLLAMA_KV_CACHE_TYPE=q8_0 &amp;amp;&amp;amp; ollama serve&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6pplv/glm432b_q5_k_s_can_fit_in_24gb_cards_with_decent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6pplv/glm432b_q5_k_s_can_fit_in_24gb_cards_with_decent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6pplv/glm432b_q5_k_s_can_fit_in_24gb_cards_with_decent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T11:20:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6nrl1</id>
    <title>I benchmarked the Gemma 3 27b QAT models</title>
    <updated>2025-04-24T09:12:34+00:00</updated>
    <author>
      <name>/u/jaxchang</name>
      <uri>https://old.reddit.com/user/jaxchang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to know what models performed the best, and it seemed like nobody had actual numbers for this information... so I ran the numbers myself. &lt;/p&gt; &lt;p&gt;I am running on llama.cpp v1.27.1 for the GGUFs, and LM Studio MLX v0.13.2 for the MLX model. &lt;/p&gt; &lt;p&gt;At first, I tried calculating perplexity. However, the PPL numbers kept on yielding really weird values from the PTB/wiki.test.raw corpus. The QAT models would generate numbers higher than the original BF16, and Bartowski's quant scored higher than the original QAT from google. I think the model is overfitting there, so it's not really a good metric. &lt;/p&gt; &lt;p&gt;So I decided to just use GPQA-main instead. It's more a more biased benchmark in terms of topic, but I suspect that actually doesn't matter too much. We're comparing different quants of the same model, not different finetunes/models. In the latter case, we might expect different finetunes/models to maybe perform better at say math but worse at coding/writing, have more biology questions in the training data set vs physics, or other biased performance skew etc. However, quantization is not so fine-grained; it simply truncates the lowest value bits for each parameter, so quality reduction/noise introduced should be more generalizable. &lt;/p&gt; &lt;p&gt;Here are the GPQA-main scores for the quants I tested: &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model name&lt;/th&gt; &lt;th&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;mlx-community/gemma-3-27b-it-qat-4bit&lt;/td&gt; &lt;td&gt;0.333&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;stduhpf/google-gemma-3-27b-it-qat-q4_0-gguf-small&lt;/td&gt; &lt;td&gt;0.346&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;bartowski/google_gemma-3-27b-it-qat-GGUF (Q4_0)&lt;/td&gt; &lt;td&gt;0.352&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;unsloth/gemma-3-27b-it (via Openrouter api Chutes)&lt;/td&gt; &lt;td&gt;0.371&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Unquantized Gemma 3 27b (via Huggingface api)&lt;/td&gt; &lt;td&gt;0.375&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Note that it takes 2-3 hours to run this benchmark per model for me, so it's not exactly a quick test.&lt;/p&gt; &lt;p&gt;Seems like the &lt;strong&gt;Bartowski QAT Q4_0 is the probably the best choice&lt;/strong&gt; if you want to run Gemma 3 QAT locally. It also seems to be 1-2tok/sec faster than the MLX model for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jaxchang"&gt; /u/jaxchang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6nrl1/i_benchmarked_the_gemma_3_27b_qat_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6nrl1/i_benchmarked_the_gemma_3_27b_qat_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6nrl1/i_benchmarked_the_gemma_3_27b_qat_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T09:12:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6x0qm</id>
    <title>What is the hardest math your AI can do?</title>
    <updated>2025-04-24T16:46:23+00:00</updated>
    <author>
      <name>/u/OrthogonalToHumanity</name>
      <uri>https://old.reddit.com/user/OrthogonalToHumanity</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to build an AI for doing math problems only using my local setup.I'm curious to know what results other people have gotten. I've looked online and it seems that the most recent news for a corporate setup was Google solving some geometry problems. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OrthogonalToHumanity"&gt; /u/OrthogonalToHumanity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6x0qm/what_is_the_hardest_math_your_ai_can_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6x0qm/what_is_the_hardest_math_your_ai_can_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6x0qm/what_is_the_hardest_math_your_ai_can_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T16:46:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6xczy</id>
    <title>Deepcogito Cogito v1 preview 14B Quantized Benchmark</title>
    <updated>2025-04-24T17:00:04+00:00</updated>
    <author>
      <name>/u/fakezeta</name>
      <uri>https://old.reddit.com/user/fakezeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm GPU poor (3060TI with 8GB VRAM) and started using the 14B Deepcogito model based on Qwen 2.5 after seeing their post.&lt;/p&gt; &lt;p&gt;Best Quantization I can use with a decent speed is Q5K_S with a a generation speed varying from 5-10tk/s depending on the context.&lt;/p&gt; &lt;p&gt;From daily usage it seems great: great at instruction following, good text understanding, very good in multi language, not SOTA at coding but it is not my primary use case.&lt;/p&gt; &lt;p&gt;So I wanted to assess how the quant affected the performance and run a subset (9 hour of test) of MMLU-PRO (20%) to have an idea:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MMLU-PRO (no reasoning)&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;overall&lt;/th&gt; &lt;th align="left"&gt;biology&lt;/th&gt; &lt;th align="left"&gt;business&lt;/th&gt; &lt;th align="left"&gt;chemistry&lt;/th&gt; &lt;th align="left"&gt;computer science&lt;/th&gt; &lt;th align="left"&gt;economics&lt;/th&gt; &lt;th align="left"&gt;engineering&lt;/th&gt; &lt;th align="left"&gt;health&lt;/th&gt; &lt;th align="left"&gt;history&lt;/th&gt; &lt;th align="left"&gt;law&lt;/th&gt; &lt;th align="left"&gt;math&lt;/th&gt; &lt;th align="left"&gt;philosophy&lt;/th&gt; &lt;th align="left"&gt;physics&lt;/th&gt; &lt;th align="left"&gt;psychology&lt;/th&gt; &lt;th align="left"&gt;other&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;69.32&lt;/td&gt; &lt;td align="left"&gt;81.12&lt;/td&gt; &lt;td align="left"&gt;71.97&lt;/td&gt; &lt;td align="left"&gt;68.14&lt;/td&gt; &lt;td align="left"&gt;74.39&lt;/td&gt; &lt;td align="left"&gt;82.14&lt;/td&gt; &lt;td align="left"&gt;56.48&lt;/td&gt; &lt;td align="left"&gt;71.17&lt;/td&gt; &lt;td align="left"&gt;67.11&lt;/td&gt; &lt;td align="left"&gt;54.09&lt;/td&gt; &lt;td align="left"&gt;78.89&lt;/td&gt; &lt;td align="left"&gt;69.70&lt;/td&gt; &lt;td align="left"&gt;62.16&lt;/td&gt; &lt;td align="left"&gt;79.87&lt;/td&gt; &lt;td align="left"&gt;63.04&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;An overall of 69.32 is in line with the 70.91 claimed in Deepcogito blog post.&lt;/p&gt; &lt;p&gt;Then I wanted to check the difference between Reasoning and No Reasoning and I choose GPQA diamond for this.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPQA no reasoning&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Accuracy: 0.41919191919191917 Refusal fraction: 0.0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;GPQA reasoning&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Accuracy: 0.54 Refusal fraction: 0,020202020202 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The refusal fraction where due to thinking process entering in a loop generating the same sentence over and over again.&lt;/p&gt; &lt;p&gt;This are incredible results considering that according to &lt;a href="https://epoch.ai/data/ai-benchmarking-dashboard"&gt;https://epoch.ai/data/ai-benchmarking-dashboard&lt;/a&gt; and to &lt;a href="https://qwenlm.github.io/blog/qwen2.5-llm/"&gt;https://qwenlm.github.io/blog/qwen2.5-llm/&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;DeepSeek-R1-Distill-Qwen-14B ==&amp;gt; 0.447 Qwen 2.5 14B ==&amp;gt; 0.328 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Both at full precision.&lt;/p&gt; &lt;p&gt;These are numbers in par with a couple of higher class LLMs and also the Reasoning mode is quite usable and usually not generating a lot of tokens for thinking.&lt;/p&gt; &lt;p&gt;I definitely recommend this model in favour of Gemma3 or Mistral Small for us GPU poors and I would really love to see how the 32B version perform.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fakezeta"&gt; /u/fakezeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xczy/deepcogito_cogito_v1_preview_14b_quantized/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xczy/deepcogito_cogito_v1_preview_14b_quantized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xczy/deepcogito_cogito_v1_preview_14b_quantized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T17:00:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6mols</id>
    <title>Details on OpenAI's upcoming 'open' AI model</title>
    <updated>2025-04-24T07:53:04+00:00</updated>
    <author>
      <name>/u/ayyndrew</name>
      <uri>https://old.reddit.com/user/ayyndrew</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6mols/details_on_openais_upcoming_open_ai_model/"&gt; &lt;img alt="Details on OpenAI's upcoming 'open' AI model" src="https://external-preview.redd.it/jBLPKrE-sNiDaxe0zsX1DO2Ghuda8KNpR6LvSh4IYoc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=462c8e493352f840f1bcce92fb0555e2b79db252" title="Details on OpenAI's upcoming 'open' AI model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;- In very early stages, targeting an early summer launch&lt;/p&gt; &lt;p&gt;- Will be a reasoning model, aiming to be the top open reasoning model when it launches&lt;/p&gt; &lt;p&gt;- Exploring a highly permissive license, perhaps unlike Llama and Gemma&lt;/p&gt; &lt;p&gt;- Text in text out, reasoning can be tuned on and off&lt;/p&gt; &lt;p&gt;- Runs on &amp;quot;high-end consumer hardware&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayyndrew"&gt; /u/ayyndrew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/04/23/openai-seeks-to-make-its-upcoming-open-ai-model-best-in-class/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6mols/details_on_openais_upcoming_open_ai_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6mols/details_on_openais_upcoming_open_ai_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T07:53:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6xiy1</id>
    <title>RTX 5090 LLM Benchmarks - outperforming the A100 by 2.6x</title>
    <updated>2025-04-24T17:06:34+00:00</updated>
    <author>
      <name>/u/takuonline</name>
      <uri>https://old.reddit.com/user/takuonline</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xiy1/rtx_5090_llm_benchmarks_outperforming_the_a100_by/"&gt; &lt;img alt="RTX 5090 LLM Benchmarks - outperforming the A100 by 2.6x" src="https://external-preview.redd.it/eiCf8y8ncWiZV_kRkOqMb9U44-ptjGTnaw-ROU8qPKM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0b1fc08d775d6fe0c3886c63639965284e34fe2" title="RTX 5090 LLM Benchmarks - outperforming the A100 by 2.6x" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Our testing revealed that despite having less VRAM than both the A100 (80GB) and RTX 6000 Ada (48GB), the RTX 5090 with its 32GB of memory consistently delivered superior performance across all token lengths and batch sizes.&lt;/p&gt; &lt;p&gt;To put the pricing in perspective, the 5090 costs $0.89/hr in Secure Cloud, compared to the $0.77/hr for the RTX 6000 Ada, and $1.64/hr for the A100. But aside from the standpoint of VRAM (the 5090 has the least, at 32GB) it handily outperforms both of them. If you are serving a model on an A100 though you could simply rent a 2x 5090 pod for about the same price and likely get double the token throughput - so for LLMs, at least, it appears there is a new sheriff in town.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/takuonline"&gt; /u/takuonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.runpod.io/rtx-5090-llm-benchmarks-for-ai-is-it-the-best-gpu-for-ml/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xiy1/rtx_5090_llm_benchmarks_outperforming_the_a100_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xiy1/rtx_5090_llm_benchmarks_outperforming_the_a100_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T17:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k71a8u</id>
    <title>Introducing Veritas-12B: A New 12B Model Focused on Philosophy, Logic, and Reasoning</title>
    <updated>2025-04-24T19:37:46+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k71a8u/introducing_veritas12b_a_new_12b_model_focused_on/"&gt; &lt;img alt="Introducing Veritas-12B: A New 12B Model Focused on Philosophy, Logic, and Reasoning" src="https://preview.redd.it/bjl1n0kv4uwe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2219d7ccfb6673fb96bd1244f81a0ef209ca4828" title="Introducing Veritas-12B: A New 12B Model Focused on Philosophy, Logic, and Reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to share a new model called &lt;strong&gt;Veritas-12B&lt;/strong&gt;. Specifically finetuned for tasks involving &lt;strong&gt;philosophy, logical reasoning, and critical thinking&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it's good at:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Deep philosophical discussions:&lt;/strong&gt; Exploring complex ideas, ethics, and different schools of thought.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Logical consistency:&lt;/strong&gt; Sticking to logic, spotting inconsistencies in arguments.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Analyzing arguments:&lt;/strong&gt; Breaking down complex points, evaluating reasons and conclusions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Explaining complex concepts:&lt;/strong&gt; Articulating abstract ideas clearly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Who might find it interesting?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Anyone interested in using an LLM for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Exploring philosophical questions&lt;/li&gt; &lt;li&gt;Analyzing texts or arguments&lt;/li&gt; &lt;li&gt;Debate preparation&lt;/li&gt; &lt;li&gt;Structured dialogue requiring logical flow&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Things to keep in mind:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It's built for analysis and reasoning, so it might not be the best fit for super casual chat or purely creative writing. Responses can sometimes be more formal or dense.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Veritas-12B is an UNCENSORED model.&lt;/strong&gt; This means it can generate responses that could be offensive, harmful, unethical, or inappropriate. Please be aware of this and use it responsibly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Where to find it:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can find the model details on Hugging Face: &lt;a href="https://huggingface.co/soob3123/Veritas-12B"&gt;soob3123/Veritas-12B · Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GGUF version (Q4_0):&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fhuggingface.co%2Fsoob3123%2FVeritas-12B-Q4_0-GGUF"&gt;https://huggingface.co/soob3123/Veritas-12B-Q4_0-GGUF&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model card has an example comparing its output to the base model when describing an image, showing its more analytical/philosophical approach.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bjl1n0kv4uwe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k71a8u/introducing_veritas12b_a_new_12b_model_focused_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k71a8u/introducing_veritas12b_a_new_12b_model_focused_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T19:37:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1k71mab</id>
    <title>Unsloth Dynamic v2.0 GGUFs + Llama 4 Bug Fixes + KL Divergence</title>
    <updated>2025-04-24T19:51:26+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k71mab/unsloth_dynamic_v20_ggufs_llama_4_bug_fixes_kl/"&gt; &lt;img alt="Unsloth Dynamic v2.0 GGUFs + Llama 4 Bug Fixes + KL Divergence" src="https://b.thumbs.redditmedia.com/3TRoD29bgkDn1xmjyIxjocxy-OpJ6wWd6DtsvFvLlKk.jpg" title="Unsloth Dynamic v2.0 GGUFs + Llama 4 Bug Fixes + KL Divergence" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! I'm super excited to announce our new revamped 2.0 version of our Dynamic quants which outperform leading quantization methods on 5-shot MMLU and KL Divergence!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For accurate benchmarking, we built an evaluation framework to match the reported 5-shot MMLU scores of Llama 4 and Gemma 3. This allowed apples-to-apples comparisons between full-precision vs. Dynamic v2.0, &lt;strong&gt;QAT&lt;/strong&gt; and &lt;strong&gt;standard imatrix GGUF&lt;/strong&gt; quants. See benchmark details below or check our Docs for full analysis: &lt;a href="https://unsloth.ai/blog/dynamic-v2"&gt;https://unsloth.ai/blog/dynamic-v2&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;12B&lt;/th&gt; &lt;th align="left"&gt;27B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;67% QAT&lt;/td&gt; &lt;td align="left"&gt;70.64% QAT&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;67.15% BF16&lt;/td&gt; &lt;td align="left"&gt;71.5% BF16&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;ul&gt; &lt;li&gt;Gemma 3 QAT is very impressive on MMLU 5 shot! The 12B model is nearly equivalent to the full BF16 model and the 27B is very close!&lt;/li&gt; &lt;li&gt;For dynamic 2.0 GGUFs, we report &lt;strong&gt;KL Divergence&lt;/strong&gt; and Disk Space change. Our Gemma 3 Q3_K_XL quant for example reduces the KL Divergence by 7.5% whilst increasing in only 2% of disk space!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d2upyhrp5uwe1.png?width=1714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7972946d6a21bd516022779337d6b3b70a13a77d"&gt;https://preview.redd.it/d2upyhrp5uwe1.png?width=1714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7972946d6a21bd516022779337d6b3b70a13a77d&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;According to the paper &amp;quot;Accuracy is Not All You Need&amp;quot; &lt;a href="https://arxiv.org/abs/2407.09141"&gt;https://arxiv.org/abs/2407.09141&lt;/a&gt;, the authors showcase how &lt;strong&gt;perplexity is a bad metric since it's a geometric mean, and so output tokens can cancel out&lt;/strong&gt;. It's best to directly report &amp;quot;Flips&amp;quot;, which is how answers change from being incorrect to correct and vice versa.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x1dcukp76uwe1.png?width=1991&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=39c6a92749133cf53ad5b88824ca023347c40036"&gt;https://preview.redd.it/x1dcukp76uwe1.png?width=1991&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=39c6a92749133cf53ad5b88824ca023347c40036&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In fact I was having some issues with Gemma 3 - layer pruning methods and old methods did not seem to work at all with Gemma 3 (my guess is it's due to the 4 layernorms). The paper shows if you prune layers, the &amp;quot;flips&amp;quot; increase dramatically. &lt;strong&gt;They also show KL Divergence to be around 98% correlated with &amp;quot;flips&amp;quot;&lt;/strong&gt;, so my goal is to reduce it!&lt;/li&gt; &lt;li&gt;Also I found current standard imatrix quants overfit on Wikitext - the perplexity is always lower when using these datasets, and I decided to instead use &lt;strong&gt;conversational style datasets sourced from high quality outputs from LLMs with 100% manual inspection (took me many days!!)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Going forward, all GGUF uploads will leverage Dynamic 2.0 along with our hand curated &lt;strong&gt;300K–1.5M token calibration dataset&lt;/strong&gt; to improve conversational chat performance. Safetensors 4-bit BnB uploads might also be updated later.&lt;/li&gt; &lt;li&gt;Gemma 3 27B details on KLD below:&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant type&lt;/th&gt; &lt;th align="left"&gt;KLD old&lt;/th&gt; &lt;th align="left"&gt;Old GB&lt;/th&gt; &lt;th align="left"&gt;KLD New&lt;/th&gt; &lt;th align="left"&gt;New GB&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ1_S&lt;/td&gt; &lt;td align="left"&gt;1.035688&lt;/td&gt; &lt;td align="left"&gt;5.83&lt;/td&gt; &lt;td align="left"&gt;0.972932&lt;/td&gt; &lt;td align="left"&gt;6.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ1_M&lt;/td&gt; &lt;td align="left"&gt;0.832252&lt;/td&gt; &lt;td align="left"&gt;6.33&lt;/td&gt; &lt;td align="left"&gt;0.800049&lt;/td&gt; &lt;td align="left"&gt;6.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ2_XXS&lt;/td&gt; &lt;td align="left"&gt;0.535764&lt;/td&gt; &lt;td align="left"&gt;7.16&lt;/td&gt; &lt;td align="left"&gt;0.521039&lt;/td&gt; &lt;td align="left"&gt;7.31&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ2_M&lt;/td&gt; &lt;td align="left"&gt;0.26554&lt;/td&gt; &lt;td align="left"&gt;8.84&lt;/td&gt; &lt;td align="left"&gt;0.258192&lt;/td&gt; &lt;td align="left"&gt;8.96&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q2_K_XL&lt;/td&gt; &lt;td align="left"&gt;0.229671&lt;/td&gt; &lt;td align="left"&gt;9.78&lt;/td&gt; &lt;td align="left"&gt;0.220937&lt;/td&gt; &lt;td align="left"&gt;9.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;0.087845&lt;/td&gt; &lt;td align="left"&gt;12.51&lt;/td&gt; &lt;td align="left"&gt;0.080617&lt;/td&gt; &lt;td align="left"&gt;12.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;0.024916&lt;/td&gt; &lt;td align="left"&gt;15.41&lt;/td&gt; &lt;td align="left"&gt;0.023701&lt;/td&gt; &lt;td align="left"&gt;15.64&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;We also helped and fixed a few Llama 4 bugs:&lt;/h1&gt; &lt;p&gt;Llama 4 Scout changed the RoPE Scaling configuration in their official repo. We helped resolve issues in llama.cpp to enable this &lt;a href="https://github.com/ggml-org/llama.cpp/pull/12889"&gt;change here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g8et5pp67uwe1.png?width=2091&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a30f52ee76504d889f44f2c3950a4e8027686d6"&gt;https://preview.redd.it/g8et5pp67uwe1.png?width=2091&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a30f52ee76504d889f44f2c3950a4e8027686d6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Llama 4's QK Norm's epsilon for both Scout and Maverick should be from the config file - this means using 1e-05 and not 1e-06. We helped resolve these in &lt;a href="https://github.com/ggml-org/llama.cpp/pull/12889"&gt;llama.cpp&lt;/a&gt; and &lt;a href="https://github.com/huggingface/transformers/pull/37418"&gt;transformers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Llama 4 team and vLLM also independently fixed an issue with QK Norm being shared across all heads (should not be so) &lt;a href="https://github.com/vllm-project/vllm/pull/16311"&gt;here&lt;/a&gt;. MMLU Pro increased from 68.58% to 71.53% accuracy.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/WolframRvnwlf/status/1909735579564331016"&gt;Wolfram Ravenwolf&lt;/a&gt; showcased how our GGUFs via llama.cpp attain much higher accuracy than third party inference providers - this was most likely a combination of improper implementation and issues explained above.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dynamic v2.0 GGUFs&lt;/strong&gt; (you can also view &lt;a href="https://huggingface.co/collections/unsloth/unsloth-dynamic-v20-quants-68060d147e9b9231112823e6"&gt;all GGUFs here&lt;/a&gt;):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;DeepSeek: &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF-UD"&gt;R1&lt;/a&gt; • &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD"&gt;V3-0324&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Llama:&lt;/strong&gt; &lt;a href="https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF"&gt;4 (Scout)&lt;/a&gt; • &lt;a href="https://huggingface.co/unsloth/Llama-3.1-8B-Instruct-GGUF"&gt;3.1 (8B)&lt;/a&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Gemma 3:&lt;/strong&gt; &lt;a href="https://huggingface.co/unsloth/gemma-3-4b-it-GGUF"&gt;4B&lt;/a&gt; • &lt;a href="https://huggingface.co/unsloth/gemma-3-12b-it-GGUF"&gt;12B&lt;/a&gt; • &lt;a href="https://huggingface.co/unsloth/gemma-3-27b-it-GGUF"&gt;27B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Mistral:&lt;/strong&gt; &lt;a href="https://huggingface.co/unsloth/Mistral-Small-3.1-24B-Instruct-2503-GGUF"&gt;Small-3.1-2503&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k71mab/unsloth_dynamic_v20_ggufs_llama_4_bug_fixes_kl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k71mab/unsloth_dynamic_v20_ggufs_llama_4_bug_fixes_kl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k71mab/unsloth_dynamic_v20_ggufs_llama_4_bug_fixes_kl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T19:51:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6zn5h</id>
    <title>New reasoning benchmark got released. Gemini is SOTA, but what's going on with Qwen?</title>
    <updated>2025-04-24T18:31:34+00:00</updated>
    <author>
      <name>/u/Additional-Hour6038</name>
      <uri>https://old.reddit.com/user/Additional-Hour6038</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6zn5h/new_reasoning_benchmark_got_released_gemini_is/"&gt; &lt;img alt="New reasoning benchmark got released. Gemini is SOTA, but what's going on with Qwen?" src="https://preview.redd.it/a6awqhrhmtwe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a0c258afc7e096b062e3e8afff59d5e57504b75" title="New reasoning benchmark got released. Gemini is SOTA, but what's going on with Qwen?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No benchmaxxing on this one! &lt;a href="http://alphaxiv.org/abs/2504.16074"&gt;http://alphaxiv.org/abs/2504.16074&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Additional-Hour6038"&gt; /u/Additional-Hour6038 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a6awqhrhmtwe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6zn5h/new_reasoning_benchmark_got_released_gemini_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6zn5h/new_reasoning_benchmark_got_released_gemini_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T18:31:34+00:00</published>
  </entry>
</feed>
