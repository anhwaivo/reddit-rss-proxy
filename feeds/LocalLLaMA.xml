<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-27T15:06:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1izf5d9</id>
    <title>[QUESTION] LOCAL VIDEO ANALYSIS WITH MM-LLMs</title>
    <updated>2025-02-27T12:59:36+00:00</updated>
    <author>
      <name>/u/Anka098</name>
      <uri>https://old.reddit.com/user/Anka098</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi all, so I was looking for any tutorial on how to do video analysis with multimodal LLMs, but youtube and google results are no good (filled with low effort copy pasted tutorials on image models with clickbaity titels). &lt;/p&gt; &lt;p&gt;now the question is Do we just feed the video frames one by one to the model or is there a known way to do it? can you recommend good recources? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anka098"&gt; /u/Anka098 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izf5d9/question_local_video_analysis_with_mmllms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izf5d9/question_local_video_analysis_with_mmllms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izf5d9/question_local_video_analysis_with_mmllms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T12:59:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1iywa97</id>
    <title>Kokoro TTS app</title>
    <updated>2025-02-26T19:42:35+00:00</updated>
    <author>
      <name>/u/PureRely</name>
      <uri>https://old.reddit.com/user/PureRely</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iywa97/kokoro_tts_app/"&gt; &lt;img alt="Kokoro TTS app" src="https://b.thumbs.redditmedia.com/1xEM_gyLY9lewqwwB3yH-uXPaOlIDmI6kEqqs2i3snw.jpg" title="Kokoro TTS app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am building a &lt;strong&gt;Kokoro TTS&lt;/strong&gt; app for personal use. Is this something you think others would like?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/v4426f9svlle1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05a4e39b2a4d9cb675ca136dd22ca4d8d5d7abd0"&gt;https://preview.redd.it/v4426f9svlle1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05a4e39b2a4d9cb675ca136dd22ca4d8d5d7abd0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;update 02/26/25 11:04pm&lt;/em&gt;&lt;br /&gt; Okay, I do have the repo up but it is still private. I am still making sure that first public version is up to my standards.&lt;/p&gt; &lt;p&gt;Here is an idea of the codesize as of now:&lt;/p&gt; &lt;h1&gt;Code Statistics Summary&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Generated on 2025-02-26 23:00:58&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Ignored 7 files based on .gitignore patterns&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Files and Lines by Type&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Extension&lt;/th&gt; &lt;th align="left"&gt;Files&lt;/th&gt; &lt;th align="left"&gt;Lines&lt;/th&gt; &lt;th align="left"&gt;% of Codebase&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;.py&lt;/td&gt; &lt;td align="left"&gt;18&lt;/td&gt; &lt;td align="left"&gt;2,175&lt;/td&gt; &lt;td align="left"&gt;45.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;.md&lt;/td&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;1,358&lt;/td&gt; &lt;td align="left"&gt;28.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;.txt&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;1,081&lt;/td&gt; &lt;td align="left"&gt;22.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;.toml&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;68&lt;/td&gt; &lt;td align="left"&gt;1.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;.yaml&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;50&lt;/td&gt; &lt;td align="left"&gt;1.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;.json&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;0.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;.cfg&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;15&lt;/td&gt; &lt;td align="left"&gt;0.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;(no ext)&lt;/td&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;.lock&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;45&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;4,777&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;100.0%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Summary&lt;/h1&gt; &lt;p&gt;This project contains:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;45&lt;/strong&gt; files&lt;/li&gt; &lt;li&gt;&lt;strong&gt;4,777&lt;/strong&gt; lines of code&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Observations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;The primary language is &lt;strong&gt;.py&lt;/strong&gt; with 2,175 lines (45.5% of the codebase)&lt;/li&gt; &lt;li&gt;Strong documentation with 1,358 lines (28.4% of the codebase)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PureRely"&gt; /u/PureRely &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iywa97/kokoro_tts_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iywa97/kokoro_tts_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iywa97/kokoro_tts_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T19:42:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ize76t</id>
    <title>How does ollama pull always able to saturate my download bandwidth?</title>
    <updated>2025-02-27T12:05:12+00:00</updated>
    <author>
      <name>/u/Ok-Internal9317</name>
      <uri>https://old.reddit.com/user/Ok-Internal9317</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it just me or you guys are also having saturated download speed. For any other internet file download the server on the other end seemed to be the bottleneck, but for ollama pull my internet badnwidth is always saturated, how did ollama manage?&lt;/p&gt; &lt;p&gt;Sorry for people that don't have such speed lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Internal9317"&gt; /u/Ok-Internal9317 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ize76t/how_does_ollama_pull_always_able_to_saturate_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ize76t/how_does_ollama_pull_always_able_to_saturate_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ize76t/how_does_ollama_pull_always_able_to_saturate_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T12:05:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyuz01</id>
    <title>Tutorial: How to Train your own Reasoning model using Llama 3.1 (8B) + Unsloth + GRPO</title>
    <updated>2025-02-26T18:49:01+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyuz01/tutorial_how_to_train_your_own_reasoning_model/"&gt; &lt;img alt="Tutorial: How to Train your own Reasoning model using Llama 3.1 (8B) + Unsloth + GRPO" src="https://external-preview.redd.it/kwC-Tr7ndw31AC4xEyGoQV3L0FpYDQr7n0II8xbY0xU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d97f72696e1b21ced4b55a5892668a3fdd40d66a" title="Tutorial: How to Train your own Reasoning model using Llama 3.1 (8B) + Unsloth + GRPO" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! We created this mini quickstart tutorial so once completed, you'll be able to transform any open LLM like Llama to have chain-of-thought reasoning by using &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;You'll learn about Reward Functions, explanations behind GRPO, dataset prep, usecases and more! Hopefully it's helpful for you all! ðŸ˜ƒ&lt;/p&gt; &lt;p&gt;Full Guide (with pics): &lt;a href="https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/"&gt;https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;These instructions are for our Google Colab &lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks"&gt;notebooks&lt;/a&gt;. If you are installing Unsloth locally, you can also copy our notebooks inside your favorite code editor.&lt;/p&gt; &lt;p&gt;&lt;em&gt;The GRPO notebooks we are using:&lt;/em&gt; &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B"&gt;&lt;em&gt;Llama 3.1 (8B)&lt;/em&gt;&lt;/a&gt;-GRPO.ipynb)&lt;em&gt;,&lt;/em&gt; &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B"&gt;&lt;em&gt;Phi-4 (14B)&lt;/em&gt;&lt;/a&gt;-GRPO.ipynb) &lt;em&gt;and&lt;/em&gt; &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(3B"&gt;&lt;em&gt;Qwen2.5 (3B)&lt;/em&gt;&lt;/a&gt;-GRPO.ipynb)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;#1. Install Unsloth&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you're using our Colab notebook, click Runtime &amp;gt; Run all. We'd highly recommend you checking out our &lt;a href="https://docs.unsloth.ai/get-started/fine-tuning-guide"&gt;Fine-tuning Guide&lt;/a&gt; before getting started. If installing locally, ensure you have the correct &lt;a href="https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements"&gt;requirements&lt;/a&gt; and use pip install unsloth&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/26dgnth9tgle1.png?width=1618&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=61a748deef81e5a771bc2420947bfc67104d8956"&gt;https://preview.redd.it/26dgnth9tgle1.png?width=1618&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=61a748deef81e5a771bc2420947bfc67104d8956&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;#2. Learn about GRPO &amp;amp; Reward Functions&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Before we get started, it is recommended to learn more about GRPO, reward functions and how they work. Read more about them including &lt;a href="https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#basics-tips"&gt;tips &amp;amp; tricks&lt;/a&gt;&lt;a href="/o/HpyELzcNe0topgVLGCZY/s/xhOjnexMCB3dmuQFQ2Zq/%7E/changes/218/basics/reasoning-grpo-and-rl#basics-tips"&gt; here&lt;/a&gt;. You will also need enough VRAM. In general, model parameters = amount of VRAM you will need. In Colab, we are using their free 16GB VRAM GPUs which can train any model up to 16B in parameters.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;#3. Configure desired settings&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We have pre-selected optimal settings for the best results for you already and you can change the model to whichever you want listed in our &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;supported models&lt;/a&gt;. Would not recommend changing other settings if you're a beginner.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mh114uw0ugle1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c895c2b016a88d86d3a3c2138e2929ab3b927f53"&gt;https://preview.redd.it/mh114uw0ugle1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c895c2b016a88d86d3a3c2138e2929ab3b927f53&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;#4. Select your dataset&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We have pre-selected OpenAI's GSM8K dataset already but you could change it to your own or any public one on Hugging Face. You can read more about &lt;a href="/o/HpyELzcNe0topgVLGCZY/s/xhOjnexMCB3dmuQFQ2Zq/%7E/changes/218/basics/datasets-101"&gt;datasets here&lt;/a&gt;. Your dataset should still have at least 2 columns for question and answer pairs. However the answer must not reveal the reasoning behind how it derived the answer from the question. See below for an example:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pgrd3xamtgle1.png?width=2304&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4630f2d5aad304f8bebaec1d8e2acea877ac4c8f"&gt;https://preview.redd.it/pgrd3xamtgle1.png?width=2304&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4630f2d5aad304f8bebaec1d8e2acea877ac4c8f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;#5. Reward Functions/Verifier&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-functions-verifier"&gt;Reward Functions/Verifiers&lt;/a&gt; lets us know if the model is doing well or not according to the dataset you have provided. Each generation run will be assessed on how it performs to the score of the average of the rest of generations. You can create your own reward functions however we have already pre-selected them for you with &lt;a href="https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#gsm8k-reward-functions"&gt;Will's GSM8K&lt;/a&gt; reward functions.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2oadoawotgle1.png?width=2284&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ab31dedc2e4de01176b42606f06be9a0228c67e"&gt;https://preview.redd.it/2oadoawotgle1.png?width=2284&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ab31dedc2e4de01176b42606f06be9a0228c67e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With this, we have 5 different ways which we can reward each generation. You can also input your generations into an LLM like ChatGPT 4o or Llama 3.1 (8B) and design a reward function and verifier to evaluate it. For example, set a rule: &amp;quot;If the answer sounds too robotic, deduct 3 points.&amp;quot; This helps refine outputs based on quality criteria. See examples of what they can look like &lt;a href="https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-function-examples"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Example Reward Function for an Email Automation Task:&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Question: Inbound email&lt;/li&gt; &lt;li&gt;Answer: Outbound email&lt;/li&gt; &lt;li&gt;Reward Functions: &lt;ul&gt; &lt;li&gt;If the answer contains a required keyword â†’ +1&lt;/li&gt; &lt;li&gt;If the answer exactly matches the ideal response â†’ +1&lt;/li&gt; &lt;li&gt;If the response is too long â†’ -1&lt;/li&gt; &lt;li&gt;If the recipient's name is included â†’ +1&lt;/li&gt; &lt;li&gt;If a signature block (phone, email, address) is present â†’ +1&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;#6. Train your model&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We have pre-selected hyperparameters for the most optimal results however you could change them. Read all about &lt;a href="https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia"&gt;parameters here&lt;/a&gt;. You should see the reward increase overtime. We would recommend you train for at least 300 steps which may take 30 mins however, for optimal results, you should train for longer.&lt;/p&gt; &lt;p&gt;You will also see sample answers which allows you to see how the model is learning. Some may have steps, XML tags, attempts etc. and the idea is as trains it's going to get better and better because it's going to get scored higher and higher until we get the outputs we desire with long reasoning chains of answers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bckurqkutgle1.png?width=1487&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2bd09c09ee7c146b88502cbf627a9878e0c2c6ca"&gt;https://preview.redd.it/bckurqkutgle1.png?width=1487&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2bd09c09ee7c146b88502cbf627a9878e0c2c6ca&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;And that's it - really hope you guys enjoyed it and please leave us any feedback!! :)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyuz01/tutorial_how_to_train_your_own_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyuz01/tutorial_how_to_train_your_own_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyuz01/tutorial_how_to_train_your_own_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T18:49:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1izffwq</id>
    <title>made a real time voice agent with FastRTC, smolagents, and hugging face inference providers</title>
    <updated>2025-02-27T13:13:04+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izffwq/made_a_real_time_voice_agent_with_fastrtc/"&gt; &lt;img alt="made a real time voice agent with FastRTC, smolagents, and hugging face inference providers" src="https://external-preview.redd.it/bndlY2Uzdzlsb2xlMTJQN53bP0jkQd6Ha0hKebDSrLH7l6BLz9vlfya9ye5X.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7bdc41ab88b3ba079e37f395a3b00b045bc67e1" title="made a real time voice agent with FastRTC, smolagents, and hugging face inference providers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zx9o67w9lole1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izffwq/made_a_real_time_voice_agent_with_fastrtc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izffwq/made_a_real_time_voice_agent_with_fastrtc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T13:13:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz8brq</id>
    <title>Intel Xeon performance on R1 671B quants? Â· ggml-org llama.cpp Â· Discussion #12088</title>
    <updated>2025-02-27T05:14:32+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz8brq/intel_xeon_performance_on_r1_671b_quants_ggmlorg/"&gt; &lt;img alt="Intel Xeon performance on R1 671B quants? Â· ggml-org llama.cpp Â· Discussion #12088" src="https://external-preview.redd.it/Qrgms0D4yrUqXmbFMI_rh_jQxu588pHVj-fhR17cIOo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=378b5c7a59ce0aabc357e17094dc7ed391e2d57e" title="Intel Xeon performance on R1 671B quants? Â· ggml-org llama.cpp Â· Discussion #12088" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/12088"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz8brq/intel_xeon_performance_on_r1_671b_quants_ggmlorg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iz8brq/intel_xeon_performance_on_r1_671b_quants_ggmlorg/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T05:14:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyxdoj</id>
    <title>Wan2.1 Video Model Native Support in ComfyUI!</title>
    <updated>2025-02-26T20:28:27+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyxdoj/wan21_video_model_native_support_in_comfyui/"&gt; &lt;img alt="Wan2.1 Video Model Native Support in ComfyUI!" src="https://external-preview.redd.it/YzcwdWNidGltamxlMTtYNTK5bBV-Sy4X4Od7toYxpb_33iOS8jpvVSU_0FTg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=41079618205739c415b15bd7117833b093e236eb" title="Wan2.1 Video Model Native Support in ComfyUI!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ComfyUI announced native support for Wan 2.1. Blog post with workflow can be found here: &lt;a href="https://blog.comfy.org/p/wan21-video-model-native-support"&gt;https://blog.comfy.org/p/wan21-video-model-native-support&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ebt3wzuimjle1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyxdoj/wan21_video_model_native_support_in_comfyui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyxdoj/wan21_video_model_native_support_in_comfyui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T20:28:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz6avv</id>
    <title>DeepSeek OpenSourceWeek Day 4</title>
    <updated>2025-02-27T03:21:21+00:00</updated>
    <author>
      <name>/u/EssayHealthy5075</name>
      <uri>https://old.reddit.com/user/EssayHealthy5075</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Optimized Parallelism Strategies&lt;/p&gt; &lt;p&gt;âœ… DualPipe - a bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training. ðŸ”— &lt;a href="https://github.com/deepseek-ai/DualPipe"&gt;https://github.com/deepseek-ai/DualPipe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;âœ… EPLB - an expert-parallel load balancer for V3/R1. ðŸ”— &lt;a href="https://github.com/deepseek-ai/eplb"&gt;https://github.com/deepseek-ai/eplb&lt;/a&gt; &lt;/p&gt; &lt;p&gt;ðŸ“Š Analyze computation-communication overlap in V3/R1 (Profiling Data in DeepSeek Infra) ðŸ”— &lt;a href="https://github.com/deepseek-ai/profile-data"&gt;https://github.com/deepseek-ai/profile-data&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EssayHealthy5075"&gt; /u/EssayHealthy5075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz6avv/deepseek_opensourceweek_day_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz6avv/deepseek_opensourceweek_day_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iz6avv/deepseek_opensourceweek_day_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T03:21:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1izbfmg</id>
    <title>Deepseek release week, please explain us</title>
    <updated>2025-02-27T08:52:25+00:00</updated>
    <author>
      <name>/u/Leflakk</name>
      <uri>https://old.reddit.com/user/Leflakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, it may be too early, but are there some experts than can tell us why these release are so good? Any idea how they could affect randoms like me playing with local models? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leflakk"&gt; /u/Leflakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izbfmg/deepseek_release_week_please_explain_us/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izbfmg/deepseek_release_week_please_explain_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izbfmg/deepseek_release_week_please_explain_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T08:52:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyudod</id>
    <title>IBM launches Granite 3.2</title>
    <updated>2025-02-26T18:24:44+00:00</updated>
    <author>
      <name>/u/twavisdegwet</name>
      <uri>https://old.reddit.com/user/twavisdegwet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyudod/ibm_launches_granite_32/"&gt; &lt;img alt="IBM launches Granite 3.2" src="https://external-preview.redd.it/XzGfI5bXa9bZ2rE7qDhqEboSxpsg13nLvY3bLGQqSVc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a08f8956935d376bfc4a36bb05ac5bfa5fdb87d" title="IBM launches Granite 3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/twavisdegwet"&gt; /u/twavisdegwet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ibm.com/new/announcements/ibm-granite-3-2-open-source-reasoning-and-vision?lnk=hpls2us"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyudod/ibm_launches_granite_32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyudod/ibm_launches_granite_32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T18:24:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1izgxd6</id>
    <title>It ain't much but it's mine Xeon E5-2690 v4 2X P-104-100 8GB 1X GTX-1080 128GB DDR4 RAM</title>
    <updated>2025-02-27T14:24:09+00:00</updated>
    <author>
      <name>/u/Artur-Ochowiak</name>
      <uri>https://old.reddit.com/user/Artur-Ochowiak</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Artur-Ochowiak"&gt; /u/Artur-Ochowiak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c4798ziwxole1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izgxd6/it_aint_much_but_its_mine_xeon_e52690_v4_2x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izgxd6/it_aint_much_but_its_mine_xeon_e52690_v4_2x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T14:24:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz19bz</id>
    <title>I used llama to build an app that matches your resume to job postings</title>
    <updated>2025-02-26T23:13:58+00:00</updated>
    <author>
      <name>/u/_lambda1</name>
      <uri>https://old.reddit.com/user/_lambda1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz19bz/i_used_llama_to_build_an_app_that_matches_your/"&gt; &lt;img alt="I used llama to build an app that matches your resume to job postings" src="https://external-preview.redd.it/Yzdjb28wNXlma2xlMepyQmmEixHtst3ZdA-VnDEqb-GTZCTZWQwIb6p-d5co.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5dc72409ce7b3750e1809a2cd8ebe41613d109e7" title="I used llama to build an app that matches your resume to job postings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_lambda1"&gt; /u/_lambda1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/703zaz4yfkle1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz19bz/i_used_llama_to_build_an_app_that_matches_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iz19bz/i_used_llama_to_build_an_app_that_matches_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T23:13:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1izd62d</id>
    <title>Everyoneâ€™s saying AGI is just around the corner, but honestly, what even is AGI to you?</title>
    <updated>2025-02-27T10:58:54+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Feels like one of those things where the definition keeps shifting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izd62d/everyones_saying_agi_is_just_around_the_corner/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izd62d/everyones_saying_agi_is_just_around_the_corner/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izd62d/everyones_saying_agi_is_just_around_the_corner/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T10:58:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz6dik</id>
    <title>Phi-4 mini</title>
    <updated>2025-02-27T03:25:11+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz6dik/phi4_mini/"&gt; &lt;img alt="Phi-4 mini" src="https://external-preview.redd.it/HbA7FdS3-3IKs2fSROiouEFOClJB8eMuRXERbnX8pEE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=025db9aba033f1ba1b3e39d55a1e2c1c7e214393" title="Phi-4 mini" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz6dik/phi4_mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iz6dik/phi4_mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T03:25:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1izbw2g</id>
    <title>Are we becoming more or less dependent on CUDA as time goes on?</title>
    <updated>2025-02-27T09:26:44+00:00</updated>
    <author>
      <name>/u/Fringolicious</name>
      <uri>https://old.reddit.com/user/Fringolicious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking at my next GPU and seriously considering a 7900 XTX - 24GB VRAM, decent price, not catching on fire and readily available.&lt;/p&gt; &lt;p&gt;Question is, will this be a massive problem for running models etc locally? I know I've enabled CUDA support and used CUDA flags on a bunch of things recently for my 3070, so would it be a massive deal to not have CUDA? Are we moving in the direction of less reliance on CUDA over time or more?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fringolicious"&gt; /u/Fringolicious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izbw2g/are_we_becoming_more_or_less_dependent_on_cuda_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izbw2g/are_we_becoming_more_or_less_dependent_on_cuda_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izbw2g/are_we_becoming_more_or_less_dependent_on_cuda_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T09:26:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz2syr</id>
    <title>By the time Deepseek does make an actual R1 Mini, I won't even notice</title>
    <updated>2025-02-27T00:26:02+00:00</updated>
    <author>
      <name>/u/Cerebral_Zero</name>
      <uri>https://old.reddit.com/user/Cerebral_Zero</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Because everyone keeps referring to these distil models as R1 while ignoring the words distil or what foundation model it's finetuned on.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cerebral_Zero"&gt; /u/Cerebral_Zero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz2syr/by_the_time_deepseek_does_make_an_actual_r1_mini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz2syr/by_the_time_deepseek_does_make_an_actual_r1_mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iz2syr/by_the_time_deepseek_does_make_an_actual_r1_mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T00:26:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1izfy2d</id>
    <title>LLaDA - Large Language Diffusion Model (weights + demo)</title>
    <updated>2025-02-27T13:36:28+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;HF Demo:&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/spaces/multimodalart/LLaDA"&gt;https://huggingface.co/spaces/multimodalart/LLaDA&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Models:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct"&gt;https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/GSAI-ML/LLaDA-8B-Base"&gt;https://huggingface.co/GSAI-ML/LLaDA-8B-Base&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2502.09992"&gt;https://arxiv.org/abs/2502.09992&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Diffusion LLMs are looking promising for alternative architecture. Some lab also recently announced a proprietary one (inception) which you could test, it can generate code quite well. &lt;/p&gt; &lt;p&gt;This stuff comes with the promise of parallelized token generation.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;LLaDA predicts all masked tokens simultaneously during each step of the reverse process.&amp;quot; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So we wouldn't need super high bandwidth for fast t/s anymore. It's not memory bandwidth bottlenecked, it has a compute bottleneck. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izfy2d/llada_large_language_diffusion_model_weights_demo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izfy2d/llada_large_language_diffusion_model_weights_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izfy2d/llada_large_language_diffusion_model_weights_demo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T13:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1izazyk</id>
    <title>Kokoro TTS 1.1</title>
    <updated>2025-02-27T08:18:53+00:00</updated>
    <author>
      <name>/u/incognataa</name>
      <uri>https://old.reddit.com/user/incognataa</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/incognataa"&gt; /u/incognataa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/hexgrad/Kokoro-82M-v1.1-zh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izazyk/kokoro_tts_11/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izazyk/kokoro_tts_11/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T08:18:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz9fpc</id>
    <title>Phi Model Family: The rise of The Small Language Models (SLMs)!</title>
    <updated>2025-02-27T06:24:39+00:00</updated>
    <author>
      <name>/u/rbgo404</name>
      <uri>https://old.reddit.com/user/rbgo404</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz9fpc/phi_model_family_the_rise_of_the_small_language/"&gt; &lt;img alt="Phi Model Family: The rise of The Small Language Models (SLMs)!" src="https://preview.redd.it/1218qwefkmle1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0047de65b619a85493b7afdeb217512daf64f0b" title="Phi Model Family: The rise of The Small Language Models (SLMs)!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rbgo404"&gt; /u/rbgo404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1218qwefkmle1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz9fpc/phi_model_family_the_rise_of_the_small_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iz9fpc/phi_model_family_the_rise_of_the_small_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T06:24:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz54du</id>
    <title>DeepSeek Realse 4th Bomb! DualPipe an innovative bidirectional pipeline parallism algorithm</title>
    <updated>2025-02-27T02:20:47+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz54du/deepseek_realse_4th_bomb_dualpipe_an_innovative/"&gt; &lt;img alt="DeepSeek Realse 4th Bomb! DualPipe an innovative bidirectional pipeline parallism algorithm" src="https://external-preview.redd.it/8TUylBdHG6G-RlVpDiMU8uIUEktXyGwSKK-R9XNOIZE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1de464dc178abc09c8c46cb861ec265373d57c26" title="DeepSeek Realse 4th Bomb! DualPipe an innovative bidirectional pipeline parallism algorithm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DualPipe is an innovative bidirectional pipeline parallism algorithm introduced in the DeepSeek-V3 Technical Report. It achieves full overlap of forward and backward computation-communication phases, also reducing pipeline bubbles. For detailed information on computation-communication overlap, please refer to the profile data.&lt;/p&gt; &lt;p&gt;link: &lt;a href="https://github.com/deepseek-ai/DualPipe"&gt;https://github.com/deepseek-ai/DualPipe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qzu9ol3cdlle1.png?width=866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=69a9c17d6008c619f5b01ce6d145949f0ebe675b"&gt;https://preview.redd.it/qzu9ol3cdlle1.png?width=866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=69a9c17d6008c619f5b01ce6d145949f0ebe675b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz54du/deepseek_realse_4th_bomb_dualpipe_an_innovative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz54du/deepseek_realse_4th_bomb_dualpipe_an_innovative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iz54du/deepseek_realse_4th_bomb_dualpipe_an_innovative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T02:20:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz1fv4</id>
    <title>Microsoft announces Phi-4-multimodal and Phi-4-mini</title>
    <updated>2025-02-26T23:22:15+00:00</updated>
    <author>
      <name>/u/hedgehog0</name>
      <uri>https://old.reddit.com/user/hedgehog0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz1fv4/microsoft_announces_phi4multimodal_and_phi4mini/"&gt; &lt;img alt="Microsoft announces Phi-4-multimodal and Phi-4-mini" src="https://external-preview.redd.it/QxVX6RZwkbebYL7yNK-C4tfRXCDplq8w2ZjdBvIh-2c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4f4d685b401cb23df7ee1e63ea0579a77eea2bc" title="Microsoft announces Phi-4-multimodal and Phi-4-mini" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedgehog0"&gt; /u/hedgehog0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz1fv4/microsoft_announces_phi4multimodal_and_phi4mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iz1fv4/microsoft_announces_phi4multimodal_and_phi4mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T23:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1izbmbb</id>
    <title>Perplexity R1 1776 performs worse than DeepSeek R1 for complex problems.</title>
    <updated>2025-02-27T09:06:25+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perplexity claims the reasoning abilities of R1 1776 are not affected by the decensoring process, but after testing it in &lt;a href="https://github.com/fairydreaming/lineage-bench/"&gt;lineage-bench&lt;/a&gt; I found that for very complex problems there are significant differences in the model performance.&lt;/p&gt; &lt;p&gt;Below you can see benchmark results for different problem sizes:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;lineage-8&lt;/th&gt; &lt;th align="left"&gt;lineage-16&lt;/th&gt; &lt;th align="left"&gt;lineage-32&lt;/th&gt; &lt;th align="left"&gt;lineage-64&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek R1&lt;/td&gt; &lt;td align="left"&gt;0.965&lt;/td&gt; &lt;td align="left"&gt;0.980&lt;/td&gt; &lt;td align="left"&gt;0.945&lt;/td&gt; &lt;td align="left"&gt;0.780&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;R1 1776&lt;/td&gt; &lt;td align="left"&gt;0.980&lt;/td&gt; &lt;td align="left"&gt;0.975&lt;/td&gt; &lt;td align="left"&gt;0.675&lt;/td&gt; &lt;td align="left"&gt;0.205&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;While for lineage-8 and lineage-16 problem sizes the model performance matches or even exceeds the original DeepSeek R1, for lineage-32 we can already observe difference in scores, while for lineage-64 R1 1776 score reached random guessing level.&lt;/p&gt; &lt;p&gt;So it looks like Perplexity claims about reasoning abilities not being affected by the decensoring process are not true.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We also ensured that the modelâ€™s math and reasoning abilities remained intact after the decensoring process. Evaluations on multiple benchmarks showed that our post-trained model performed on par with the base R1 model, indicating that the decensoring had no impact on its core reasoning capabilities.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izbmbb/perplexity_r1_1776_performs_worse_than_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izbmbb/perplexity_r1_1776_performs_worse_than_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izbmbb/perplexity_r1_1776_performs_worse_than_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T09:06:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1izdrsd</id>
    <title>vLLM just landed FlashMLA (DeepSeek - day 1) in vLLM and it is already boosting output throughput 2-16% - expect more improvements in the coming days</title>
    <updated>2025-02-27T11:38:48+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izdrsd/vllm_just_landed_flashmla_deepseek_day_1_in_vllm/"&gt; &lt;img alt="vLLM just landed FlashMLA (DeepSeek - day 1) in vLLM and it is already boosting output throughput 2-16% - expect more improvements in the coming days" src="https://preview.redd.it/wnphfz5s4ole1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d23c35465c203ce3194ca69901bcbe56c0961102" title="vLLM just landed FlashMLA (DeepSeek - day 1) in vLLM and it is already boosting output throughput 2-16% - expect more improvements in the coming days" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wnphfz5s4ole1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izdrsd/vllm_just_landed_flashmla_deepseek_day_1_in_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izdrsd/vllm_just_landed_flashmla_deepseek_day_1_in_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T11:38:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ize4n0</id>
    <title>Dual 5090FE</title>
    <updated>2025-02-27T12:01:15+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ize4n0/dual_5090fe/"&gt; &lt;img alt="Dual 5090FE" src="https://preview.redd.it/defh49ux8ole1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=843767edca5506f54e0bdb3a8b57d7e022c97a89" title="Dual 5090FE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/defh49ux8ole1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ize4n0/dual_5090fe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ize4n0/dual_5090fe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T12:01:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1izf4zf</id>
    <title>Pythagoras : i should've guessed first hand ðŸ˜© !</title>
    <updated>2025-02-27T12:59:00+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izf4zf/pythagoras_i_shouldve_guessed_first_hand/"&gt; &lt;img alt="Pythagoras : i should've guessed first hand ðŸ˜© !" src="https://preview.redd.it/m3vrfaz8jole1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e5f34f13dda8627c1b31cbceebdf6cfb503c19e" title="Pythagoras : i should've guessed first hand ðŸ˜© !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m3vrfaz8jole1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izf4zf/pythagoras_i_shouldve_guessed_first_hand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izf4zf/pythagoras_i_shouldve_guessed_first_hand/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T12:59:00+00:00</published>
  </entry>
</feed>
