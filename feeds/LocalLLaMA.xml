<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-18T11:34:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jdrovh</id>
    <title>LLM Chess tournament - Single-elimination (includes DeepSeek &amp; Llama models)</title>
    <updated>2025-03-17T23:54:35+00:00</updated>
    <author>
      <name>/u/dubesor86</name>
      <uri>https://old.reddit.com/user/dubesor86</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dubesor86"&gt; /u/dubesor86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://dubesor.de/chess/tournament"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdrovh/llm_chess_tournament_singleelimination_includes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdrovh/llm_chess_tournament_singleelimination_includes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T23:54:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1je01rr</id>
    <title>On-premise structured extraction with LLM using Ollama</title>
    <updated>2025-03-18T08:13:46+00:00</updated>
    <author>
      <name>/u/Whole-Assignment6240</name>
      <uri>https://old.reddit.com/user/Whole-Assignment6240</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je01rr/onpremise_structured_extraction_with_llm_using/"&gt; &lt;img alt="On-premise structured extraction with LLM using Ollama" src="https://external-preview.redd.it/3Wa6kCQtkWB0JnOPC6HiCn4L82FAqJAVhi71NlE5vB8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57963e89b1a881c88b288511356bf78b75182d93" title="On-premise structured extraction with LLM using Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, would love to share my recent work on extracting structured data from PDF/Markdown with Ollama 's local LLM models. All running on premise without sending data to external APIs. You can pull any of your favorite LLM models by the &lt;code&gt;ollama pull&lt;/code&gt; command. Would love some feedbackðŸ¤—! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole-Assignment6240"&gt; /u/Whole-Assignment6240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/cocoindex-io/cocoindex/tree/main/examples/manuals_llm_extraction"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je01rr/onpremise_structured_extraction_with_llm_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je01rr/onpremise_structured_extraction_with_llm_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T08:13:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1je1skx</id>
    <title>PailGemma2 vs Gemma3 Image Capability</title>
    <updated>2025-03-18T10:27:28+00:00</updated>
    <author>
      <name>/u/Glittering-Bag-4662</name>
      <uri>https://old.reddit.com/user/Glittering-Bag-4662</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What have people found works best as a smaller but still powerful model that converts math equations / problems to texf?&lt;/p&gt; &lt;p&gt;Iâ€™m playing with qwen 2.5 VL, PailGemma2 and Gemma3 right now. Though I donâ€™t know if qwen2.5VL or PailGemma2 run on the ollama interface. &lt;/p&gt; &lt;p&gt;Lmk!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glittering-Bag-4662"&gt; /u/Glittering-Bag-4662 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je1skx/pailgemma2_vs_gemma3_image_capability/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je1skx/pailgemma2_vs_gemma3_image_capability/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je1skx/pailgemma2_vs_gemma3_image_capability/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T10:27:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdyf7c</id>
    <title>What is the best TTS model to generate conversations</title>
    <updated>2025-03-18T06:05:13+00:00</updated>
    <author>
      <name>/u/perbhatk</name>
      <uri>https://old.reddit.com/user/perbhatk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I want to build an app that ai-generates personalized daily-news podcasts for users. We are having trouble finding the right model to generate conversations. &lt;/p&gt; &lt;p&gt;What model should we use for TTS?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/perbhatk"&gt; /u/perbhatk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdyf7c/what_is_the_best_tts_model_to_generate/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdyf7c/what_is_the_best_tts_model_to_generate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdyf7c/what_is_the_best_tts_model_to_generate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T06:05:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdg29r</id>
    <title>AMD's Ryzen AI MAX+ 395 "Strix Halo" APU Is Over 3x Faster Than RTX 5080 In DeepSeek R1 AI Benchmarks</title>
    <updated>2025-03-17T15:56:46+00:00</updated>
    <author>
      <name>/u/cafedude</name>
      <uri>https://old.reddit.com/user/cafedude</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdg29r/amds_ryzen_ai_max_395_strix_halo_apu_is_over_3x/"&gt; &lt;img alt="AMD's Ryzen AI MAX+ 395 &amp;quot;Strix Halo&amp;quot; APU Is Over 3x Faster Than RTX 5080 In DeepSeek R1 AI Benchmarks" src="https://external-preview.redd.it/3IcJjSUCxi122hiuXmFpDMPiHArK-sIOKdws7ZIS6y4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=975ee513a9d290ecb7564adc784829a23a139908" title="AMD's Ryzen AI MAX+ 395 &amp;quot;Strix Halo&amp;quot; APU Is Over 3x Faster Than RTX 5080 In DeepSeek R1 AI Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cafedude"&gt; /u/cafedude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-ryzen-ai-max-395-strix-halo-apu-over-3x-faster-rtx-5080-in-deepseek-benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdg29r/amds_ryzen_ai_max_395_strix_halo_apu_is_over_3x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdg29r/amds_ryzen_ai_max_395_strix_halo_apu_is_over_3x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T15:56:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdvk7c</id>
    <title>Any m3 ultra test requests for MLX models in LM Studio?</title>
    <updated>2025-03-18T03:05:58+00:00</updated>
    <author>
      <name>/u/nomorebuttsplz</name>
      <uri>https://old.reddit.com/user/nomorebuttsplz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Got my 512 gb. Happy with it so far. Prompt processing is not too bad for 70b models -- with about 7800 tokens of context, 8 bit MLX Llama 3.3 70b processes at about 145 t/s per second - and then in LM studio does not need to process for additional prompts, as it caches the context, assuming you're not changing the previous context. It then generates at about 8.5 t/s. And Q4 70b models are about twice as fast for inference at these modest context sizes.&lt;/p&gt; &lt;p&gt;It's cool to be able to throw so much context into the model and still have it function pretty well. I just threw both the American and French Revolution Wikpedia articles into a L3.3 70b 8 bit fine tune, for a combined context of 39,686 tokens, which takes an additional roughly 30 gb of ram. I got eval at 101 t/s and inference at 6.53 t/s. With a 4 bit version, 9.57 t/s and similar prompt eval time of 103 t/s.&lt;/p&gt; &lt;p&gt;R1 is slower at prompt processing, but has faster inference -- getting the same 18 t/s reported elsewhere without much context. Prompt processing can be very slow though - like 30 t/s at large contexts. Not sure if this is some quirk of my settings as it's lower than I've seen elsewhere.&lt;/p&gt; &lt;p&gt;I should say I am measuring prompt eval by taking the &amp;quot;time to first prompt&amp;quot; and dividing the prompt tokens by that number of seconds. I don't know if there is a better way to find eval time on LM studio.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nomorebuttsplz"&gt; /u/nomorebuttsplz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdvk7c/any_m3_ultra_test_requests_for_mlx_models_in_lm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdvk7c/any_m3_ultra_test_requests_for_mlx_models_in_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdvk7c/any_m3_ultra_test_requests_for_mlx_models_in_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T03:05:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdqqq4</id>
    <title>Cohere Command-A on LMSYS -- 13th place</title>
    <updated>2025-03-17T23:11:08+00:00</updated>
    <author>
      <name>/u/Confident_Proof4707</name>
      <uri>https://old.reddit.com/user/Confident_Proof4707</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdqqq4/cohere_commanda_on_lmsys_13th_place/"&gt; &lt;img alt="Cohere Command-A on LMSYS -- 13th place" src="https://preview.redd.it/xf6tmhng0cpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c097a63b9849bbdd01e284112894b318f5ac165b" title="Cohere Command-A on LMSYS -- 13th place" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Confident_Proof4707"&gt; /u/Confident_Proof4707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xf6tmhng0cpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdqqq4/cohere_commanda_on_lmsys_13th_place/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdqqq4/cohere_commanda_on_lmsys_13th_place/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T23:11:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdytwm</id>
    <title>Thoughts on openai's new Responses API</title>
    <updated>2025-03-18T06:36:03+00:00</updated>
    <author>
      <name>/u/fripperML</name>
      <uri>https://old.reddit.com/user/fripperML</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been thinking about OpenAI's new Responses API, and I can't help but feel that it marks a significant shift in their approach, potentially moving toward a more closed, vendor-specific ecosystem.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://platform.openai.com/docs/api-reference/responses"&gt;https://platform.openai.com/docs/api-reference/responses&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://platform.openai.com/docs/guides/responses-vs-chat-completions"&gt;https://platform.openai.com/docs/guides/responses-vs-chat-completions&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Until now, the Completions API was essentially a standardâ€”stateless, straightforward, and easily replicated by local LLMs through inference engines like &lt;code&gt;llama.cpp&lt;/code&gt;, &lt;code&gt;ollama&lt;/code&gt;, or &lt;code&gt;vLLM&lt;/code&gt;. While OpenAI has gradually added features like structured outputs and tools, these were still possible to emulate without major friction.&lt;/p&gt; &lt;p&gt;The Responses API, however, feels different. It introduces statefulness and broader functionalities that include conversation management, vector store handling, file search, and even web search. In essence, it's not just an LLM endpoint anymoreâ€”it's an integrated, end-to-end solution for building AI-powered systems.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I find this concerning:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Statefulness and Lock-In:&lt;/strong&gt; Inference engines like &lt;code&gt;vLLM&lt;/code&gt; are optimized for stateless inference. They are not tied to databases or persistent storage, making it difficult to replicate a stateful approach like the Responses API.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Beyond Just Inference:&lt;/strong&gt; The integration of vector stores and external search capabilities means OpenAI's API is no longer a simple, isolated component. It becomes a broader AI platform, potentially discouraging open, interchangeable AI solutions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Breaking the &amp;quot;Standard&amp;quot;:&lt;/strong&gt; Many open-source tools and libraries have built around the OpenAI API as a standard. If OpenAI starts deprecating the Completions API or nudging developers toward Responses, it could disrupt a lot of the existing ecosystem.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I understand that from a developer's perspective, the new API might simplify certain use cases, especially for those already building around OpenAI's ecosystem. But I also fear it might create a kind of &amp;quot;walled garden&amp;quot; that other LLM providers and open-source projects struggle to compete with.&lt;/p&gt; &lt;p&gt;I'd love to hear your thoughts. Do you see this as a genuine risk to the open LLM ecosystem, or am I being too pessimistic?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fripperML"&gt; /u/fripperML &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdytwm/thoughts_on_openais_new_responses_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdytwm/thoughts_on_openais_new_responses_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdytwm/thoughts_on_openais_new_responses_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T06:36:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdw0bi</id>
    <title>Extended NYT Connections benchmark: Cohere Command A and Mistral Small 3.1 results</title>
    <updated>2025-03-18T03:30:18+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdw0bi/extended_nyt_connections_benchmark_cohere_command/"&gt; &lt;img alt="Extended NYT Connections benchmark: Cohere Command A and Mistral Small 3.1 results" src="https://preview.redd.it/kgqax8ncadpe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b13cf09bcbbe11a1626128149f421fbd46c7a654" title="Extended NYT Connections benchmark: Cohere Command A and Mistral Small 3.1 results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kgqax8ncadpe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdw0bi/extended_nyt_connections_benchmark_cohere_command/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdw0bi/extended_nyt_connections_benchmark_cohere_command/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T03:30:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1je2aup</id>
    <title>Kunlun Wanwei company released Skywork-R1V-38B (visual thinking chain reasoning model)</title>
    <updated>2025-03-18T11:00:49+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je2aup/kunlun_wanwei_company_released_skyworkr1v38b/"&gt; &lt;img alt="Kunlun Wanwei company released Skywork-R1V-38B (visual thinking chain reasoning model)" src="https://external-preview.redd.it/hY5-70ADAtxgy5HnAcnYxvRB7gjyBmbyBdd7tAqE0Ao.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c8f7ab157bb5241daabdeb91f42c439f60ebfc6" title="Kunlun Wanwei company released Skywork-R1V-38B (visual thinking chain reasoning model)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are thrilled to introduce Skywork R1V, the first industry open-sourced multimodal reasoning model with advanced visual chain-of-thought capabilities, pushing the boundaries of AI-driven vision and logical inference! ðŸš€&lt;/p&gt; &lt;p&gt;Feature Visual Chain-of-Thought: Enables multi-step logical reasoning on visual inputs, breaking down complex image-based problems into manageable steps. Mathematical &amp;amp; Scientific Analysis: Capable of solving visual math problems and interpreting scientific/medical imagery with high precision. Cross-Modal Understanding: Seamlessly integrates text and images for richer, context-aware comprehension.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/znbbim50jfpe1.gif"&gt;https://i.redd.it/znbbim50jfpe1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d3frvoh3jfpe1.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e219e876bdbe3028828509b808744bd1b076939c"&gt;https://preview.redd.it/d3frvoh3jfpe1.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e219e876bdbe3028828509b808744bd1b076939c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4420a206jfpe1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d54a248fdd3474acd452353a6513153029f4cc17"&gt;https://preview.redd.it/4420a206jfpe1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d54a248fdd3474acd452353a6513153029f4cc17&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7556xizrlfpe1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b1f2d3f616c8c9aebf8d512a570539410fdf78d"&gt;https://preview.redd.it/7556xizrlfpe1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b1f2d3f616c8c9aebf8d512a570539410fdf78d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-R1V-38B"&gt;HuggingFace &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SkyworkAI/Skywork-R1V/blob/main/Skywork_R1V.pdf"&gt;Paper &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SkyworkAI/Skywork-R1V"&gt;GitHub &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je2aup/kunlun_wanwei_company_released_skyworkr1v38b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je2aup/kunlun_wanwei_company_released_skyworkr1v38b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je2aup/kunlun_wanwei_company_released_skyworkr1v38b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T11:00:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdgnh4</id>
    <title>Mistral Small 3.1 (24B)</title>
    <updated>2025-03-17T16:20:23+00:00</updated>
    <author>
      <name>/u/xLionel775</name>
      <uri>https://old.reddit.com/user/xLionel775</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnh4/mistral_small_31_24b/"&gt; &lt;img alt="Mistral Small 3.1 (24B)" src="https://external-preview.redd.it/UDZBQmD4AJb2vSY-B7oM2DhQ3zjGzTcUOviRMRcUKkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=122efd46018c04117aca71d80db3640d390428bd" title="Mistral Small 3.1 (24B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xLionel775"&gt; /u/xLionel775 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/mistral-small-3-1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnh4/mistral_small_31_24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnh4/mistral_small_31_24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T16:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1je0ao2</id>
    <title>A new open-source reasoning model: Skywork-R1V (38B \ Multimodal \ Reasoning with CoT)</title>
    <updated>2025-03-18T08:33:58+00:00</updated>
    <author>
      <name>/u/Striking-Gene2724</name>
      <uri>https://old.reddit.com/user/Striking-Gene2724</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/SkyworkAI/Skywork-R1V"&gt;https://github.com/SkyworkAI/Skywork-R1V&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking-Gene2724"&gt; /u/Striking-Gene2724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je0ao2/a_new_opensource_reasoning_model_skyworkr1v_38b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je0ao2/a_new_opensource_reasoning_model_skyworkr1v_38b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je0ao2/a_new_opensource_reasoning_model_skyworkr1v_38b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T08:33:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1je1cus</id>
    <title>Gemma3 disappointment post</title>
    <updated>2025-03-18T09:56:52+00:00</updated>
    <author>
      <name>/u/EntertainmentBroad43</name>
      <uri>https://old.reddit.com/user/EntertainmentBroad43</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma2 was very good, but gemma3 27b just feels mediocre for STEM (finding inconsistent numbers in a medical paper). &lt;/p&gt; &lt;p&gt;I found Mistral small 3 and even phi-4 better than gemma3 27b. &lt;/p&gt; &lt;p&gt;Fwiw I tried up to q8 gguf and 8 bit mlx. &lt;/p&gt; &lt;p&gt;Is it just that gemma3 is tuned for general chat, or do you think future gguf and mlx fixes will improve it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EntertainmentBroad43"&gt; /u/EntertainmentBroad43 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je1cus/gemma3_disappointment_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je1cus/gemma3_disappointment_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je1cus/gemma3_disappointment_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T09:56:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdaq7x</id>
    <title>3x RTX 5090 watercooled in one desktop</title>
    <updated>2025-03-17T11:48:07+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdaq7x/3x_rtx_5090_watercooled_in_one_desktop/"&gt; &lt;img alt="3x RTX 5090 watercooled in one desktop" src="https://preview.redd.it/zsu6kw5pm8pe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=313ede5ac5797a563ccfc2f875620a34ab784cbe" title="3x RTX 5090 watercooled in one desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zsu6kw5pm8pe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdaq7x/3x_rtx_5090_watercooled_in_one_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdaq7x/3x_rtx_5090_watercooled_in_one_desktop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T11:48:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdu2kl</id>
    <title>LG releases Exaone Deep Thinking Model</title>
    <updated>2025-03-18T01:49:17+00:00</updated>
    <author>
      <name>/u/unemployed_capital</name>
      <uri>https://old.reddit.com/user/unemployed_capital</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdu2kl/lg_releases_exaone_deep_thinking_model/"&gt; &lt;img alt="LG releases Exaone Deep Thinking Model" src="https://external-preview.redd.it/Xzc7CaAUyXww2X_fqYFnasfbsy1Ttn8iznYPzjG7T8Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5dea72509723e72da35b7be603406680827841bb" title="LG releases Exaone Deep Thinking Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unemployed_capital"&gt; /u/unemployed_capital &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/LGAI-EXAONE/exaone-deep-67d119918816ec6efa79a4aa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdu2kl/lg_releases_exaone_deep_thinking_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdu2kl/lg_releases_exaone_deep_thinking_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T01:49:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jduff4</id>
    <title>Is it just me or is LG's EXAONE 2.4b crazy good?</title>
    <updated>2025-03-18T02:07:07+00:00</updated>
    <author>
      <name>/u/dp3471</name>
      <uri>https://old.reddit.com/user/dp3471</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Take a look at these benchmarks: &lt;a href="https://github.com/LG-AI-EXAONE/EXAONE-Deep"&gt;https://github.com/LG-AI-EXAONE/EXAONE-Deep&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I mean - you're telling me that a 2.4b model (46.6) outperforms gemma3 27b (29.7) on live code bench?&lt;/p&gt; &lt;p&gt;I understand that this is a reasoning model (and gemma3 was not technically trained for coding) - but how did they do such a good job condensing the size?&lt;/p&gt; &lt;p&gt;The 2.4b also outperforms gemma3 27b on GPQA diamond by 11.9 points&lt;/p&gt; &lt;p&gt;its 11.25x smaller.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dp3471"&gt; /u/dp3471 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jduff4/is_it_just_me_or_is_lgs_exaone_24b_crazy_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jduff4/is_it_just_me_or_is_lgs_exaone_24b_crazy_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jduff4/is_it_just_me_or_is_lgs_exaone_24b_crazy_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T02:07:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdpt2t</id>
    <title>When vibe coding no longer vibes back</title>
    <updated>2025-03-17T22:29:57+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdpt2t/when_vibe_coding_no_longer_vibes_back/"&gt; &lt;img alt="When vibe coding no longer vibes back" src="https://b.thumbs.redditmedia.com/BM8RPyZumaQmoKHcS0iEijgW19qWzdyFpuwPaLt9O_U.jpg" title="When vibe coding no longer vibes back" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/admw06obtbpe1.png?width=1072&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b103a08e631a5e1ac4705c4dd3ea0666e7ab6041"&gt;https://preview.redd.it/admw06obtbpe1.png?width=1072&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b103a08e631a5e1ac4705c4dd3ea0666e7ab6041&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdpt2t/when_vibe_coding_no_longer_vibes_back/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdpt2t/when_vibe_coding_no_longer_vibes_back/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdpt2t/when_vibe_coding_no_longer_vibes_back/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T22:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdwtz4</id>
    <title>Mistral Small 3.1 Tested</title>
    <updated>2025-03-18T04:18:03+00:00</updated>
    <author>
      <name>/u/Ok-Contribution9043</name>
      <uri>https://old.reddit.com/user/Ok-Contribution9043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Shaping up to be a busy week. I just posted the Gemma comparisons so here is Mistral against the same benchmarks.&lt;/p&gt; &lt;p&gt;Mistral has really surprised me here - Beating Gemma 3-27b on some tasks - which itself beat gpt-4-o mini. Most impressive was 0 hallucinations on our RAG test, which Gemma stumbled on... &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=pdwHxvJ80eM"&gt;https://www.youtube.com/watch?v=pdwHxvJ80eM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Contribution9043"&gt; /u/Ok-Contribution9043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdwtz4/mistral_small_31_tested/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdwtz4/mistral_small_31_tested/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdwtz4/mistral_small_31_tested/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T04:18:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdgqcj</id>
    <title>NEW MISTRAL JUST DROPPED</title>
    <updated>2025-03-17T16:23:29+00:00</updated>
    <author>
      <name>/u/Straight-Worker-4327</name>
      <uri>https://old.reddit.com/user/Straight-Worker-4327</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgqcj/new_mistral_just_dropped/"&gt; &lt;img alt="NEW MISTRAL JUST DROPPED" src="https://external-preview.redd.it/UDZBQmD4AJb2vSY-B7oM2DhQ3zjGzTcUOviRMRcUKkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=122efd46018c04117aca71d80db3640d390428bd" title="NEW MISTRAL JUST DROPPED" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Outperforms&lt;/strong&gt; GPT-4o Mini, Claude-3.5 Haiku, and others in text, vision, and multilingual tasks.&lt;br /&gt; &lt;strong&gt;128k context window&lt;/strong&gt;, blazing &lt;strong&gt;150 tokens/sec speed&lt;/strong&gt;, and runs on a &lt;strong&gt;single RTX 4090&lt;/strong&gt; or &lt;strong&gt;Mac (32GB RAM)&lt;/strong&gt;.&lt;br /&gt; &lt;strong&gt;Apache 2.0 license&lt;/strong&gt;â€”free to use, fine-tune, and deploy. Handles chatbots, docs, images, and coding.&lt;/p&gt; &lt;p&gt;&lt;a href="https://mistral.ai/fr/news/mistral-small-3-1"&gt;https://mistral.ai/fr/news/mistral-small-3-1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503"&gt;https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Straight-Worker-4327"&gt; /u/Straight-Worker-4327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgqcj/new_mistral_just_dropped/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgqcj/new_mistral_just_dropped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgqcj/new_mistral_just_dropped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T16:23:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdxzqt</id>
    <title>[codename] on lmarena is probably Llama4</title>
    <updated>2025-03-18T05:34:28+00:00</updated>
    <author>
      <name>/u/Most_Cap_1354</name>
      <uri>https://old.reddit.com/user/Most_Cap_1354</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i marked it as a tie, as it revealed its identity. but then i realised that it is an unreleased model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Most_Cap_1354"&gt; /u/Most_Cap_1354 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c8jfxwe9xdpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdxzqt/codename_on_lmarena_is_probably_llama4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdxzqt/codename_on_lmarena_is_probably_llama4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T05:34:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdgnw5</id>
    <title>Mistrall Small 3.1 released</title>
    <updated>2025-03-17T16:20:51+00:00</updated>
    <author>
      <name>/u/Dirky_</name>
      <uri>https://old.reddit.com/user/Dirky_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnw5/mistrall_small_31_released/"&gt; &lt;img alt="Mistrall Small 3.1 released" src="https://external-preview.redd.it/UDZBQmD4AJb2vSY-B7oM2DhQ3zjGzTcUOviRMRcUKkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=122efd46018c04117aca71d80db3640d390428bd" title="Mistrall Small 3.1 released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dirky_"&gt; /u/Dirky_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/fr/news/mistral-small-3-1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnw5/mistrall_small_31_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdgnw5/mistrall_small_31_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T16:20:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdt29q</id>
    <title>LG has released their new reasoning models EXAONE-Deep</title>
    <updated>2025-03-18T00:59:59+00:00</updated>
    <author>
      <name>/u/remixer_dec</name>
      <uri>https://old.reddit.com/user/remixer_dec</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdt29q/lg_has_released_their_new_reasoning_models/"&gt; &lt;img alt="LG has released their new reasoning models EXAONE-Deep" src="https://b.thumbs.redditmedia.com/lXcocC3WrsvsaYixAdXVzQqh6xX0Tav-ZRP20RDNdvw.jpg" title="LG has released their new reasoning models EXAONE-Deep" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EXAONE reasoning model series of 2.4B, 7.8B, and 32B, optimized for reasoning tasks including math and coding&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We introduce EXAONE Deep, which exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research. Evaluation results show that 1) EXAONE Deep 2.4B outperforms other models of comparable size, 2) EXAONE Deep 7.8B outperforms not only open-weight models of comparable scale but also a proprietary reasoning model OpenAI o1-mini, and 3) EXAONE Deep 32B demonstrates competitive performance against leading open-weight models.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://www.lgresearch.ai/news/view?seq=543"&gt;Blog post&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/LGAI-EXAONE/exaone-deep-67d119918816ec6efa79a4aa"&gt;HF collection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2503.12524"&gt;Arxiv paper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/LG-AI-EXAONE/EXAONE-Deep"&gt;Github repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The models are licensed under EXAONE AI Model License Agreement 1.1 - NC&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/64raac7jpcpe1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7c4f866c5cbdcace44f4c139efcb1c7366cd952"&gt;https://preview.redd.it/64raac7jpcpe1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7c4f866c5cbdcace44f4c139efcb1c7366cd952&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;sup&gt;P.S. I made a bot that monitors fresh public releases from large companies and research labs and posts them in a&lt;/sup&gt; &lt;a href="https://remixer-dec.com/genaimon"&gt;&lt;sup&gt;tg channel&lt;/sup&gt;&lt;/a&gt;&lt;sup&gt;, feel free to join.&lt;/sup&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remixer_dec"&gt; /u/remixer_dec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdt29q/lg_has_released_their_new_reasoning_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdt29q/lg_has_released_their_new_reasoning_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdt29q/lg_has_released_their_new_reasoning_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T00:59:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1je17el</id>
    <title>Open source 7.8B model beats o1 mini now on many benchmarks</title>
    <updated>2025-03-18T09:45:26+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je17el/open_source_78b_model_beats_o1_mini_now_on_many/"&gt; &lt;img alt="Open source 7.8B model beats o1 mini now on many benchmarks" src="https://preview.redd.it/211jtna16fpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e1d30576311226c74c3be9ef4b897ba63782ca9" title="Open source 7.8B model beats o1 mini now on many benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/211jtna16fpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je17el/open_source_78b_model_beats_o1_mini_now_on_many/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je17el/open_source_78b_model_beats_o1_mini_now_on_many/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T09:45:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdidoa</id>
    <title>Victory: My wife finally recognized my silly computer hobby as useful</title>
    <updated>2025-03-17T17:28:54+00:00</updated>
    <author>
      <name>/u/StandardLovers</name>
      <uri>https://old.reddit.com/user/StandardLovers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a local LLM, LAN-accessible, with a vector database covering all tax regulations, labor laws, and compliance data. Now she sees the value. A small step for AI, a giant leap for household credibility.&lt;/p&gt; &lt;p&gt;Edit: Insane response! To everyone askingâ€”yes, itâ€™s just web scraping with correct layers (APIs help), embedding, and RAG. Not that hard if you structure it right. I might put together a simple guide later when i actually use a more advanced method.&lt;/p&gt; &lt;p&gt;Edit 2: I see why this blew upâ€”the American tax system is insanely complex. Many tax pages require a login, making a full database a massive challenge. The scale of this project for the U.S. would be huge. For context, Iâ€™m not American.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StandardLovers"&gt; /u/StandardLovers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdidoa/victory_my_wife_finally_recognized_my_silly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdidoa/victory_my_wife_finally_recognized_my_silly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdidoa/victory_my_wife_finally_recognized_my_silly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T17:28:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdw7bg</id>
    <title>After these last 2 weeks of exciting releases, the only thing I know for certain is that benchmarks are largely BS</title>
    <updated>2025-03-18T03:41:11+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdw7bg/after_these_last_2_weeks_of_exciting_releases_the/"&gt; &lt;img alt="After these last 2 weeks of exciting releases, the only thing I know for certain is that benchmarks are largely BS" src="https://preview.redd.it/3lujka2ucdpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a9e581fb20610cdc9e8940f25cf7e4e9277ff42" title="After these last 2 weeks of exciting releases, the only thing I know for certain is that benchmarks are largely BS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3lujka2ucdpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdw7bg/after_these_last_2_weeks_of_exciting_releases_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdw7bg/after_these_last_2_weeks_of_exciting_releases_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T03:41:11+00:00</published>
  </entry>
</feed>
