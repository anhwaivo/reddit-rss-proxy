<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-26T10:48:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i9txf3</id>
    <title>Deepseek is way better in Python code generation than ChatGPT (talking about the "free" versions of both)</title>
    <updated>2025-01-25T18:49:57+00:00</updated>
    <author>
      <name>/u/ThiccStorms</name>
      <uri>https://old.reddit.com/user/ThiccStorms</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I haven't bought any subscriptions and im talking about the web based apps for both, and im just taking this opportunity to fanboy on deepseek because it produces super clean python code in one shot, whereas chat gpt generates a complex mess and i still had to specify some things again and again because it missed out on them in the initial prompt.&lt;br /&gt; I didn't generate a snippet out of scratch, i had an old function in python which i wanted to re-utilise for a similar use case, I wrote a detailed prompt to get what I need but ChatGPT still managed to screw up while deepseek nailed it in the first try. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThiccStorms"&gt; /u/ThiccStorms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9txf3/deepseek_is_way_better_in_python_code_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9txf3/deepseek_is_way_better_in_python_code_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9txf3/deepseek_is_way_better_in_python_code_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T18:49:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia6re7</id>
    <title>I made a Free &amp; Open-Source FastAPI Template to build online services that uses LLMs!</title>
    <updated>2025-01-26T05:22:35+00:00</updated>
    <author>
      <name>/u/AleksCube</name>
      <uri>https://old.reddit.com/user/AleksCube</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia6re7/i_made_a_free_opensource_fastapi_template_to/"&gt; &lt;img alt="I made a Free &amp;amp; Open-Source FastAPI Template to build online services that uses LLMs!" src="https://external-preview.redd.it/ZXozaWh2dGl3OWZlMY4brPbXXnlynLbYgxRYsYbRz1arEGB1SqG_c2u4ImT_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee0d09f41a1db91ed052ee91e35a9037651d7579" title="I made a Free &amp;amp; Open-Source FastAPI Template to build online services that uses LLMs!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AleksCube"&gt; /u/AleksCube &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/iesj4wtiw9fe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia6re7/i_made_a_free_opensource_fastapi_template_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia6re7/i_made_a_free_opensource_fastapi_template_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T05:22:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9x23l</id>
    <title>[Magnum/Rei] Mistral Nemo 12b</title>
    <updated>2025-01-25T21:06:33+00:00</updated>
    <author>
      <name>/u/lucyknada</name>
      <uri>https://old.reddit.com/user/lucyknada</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi again!&lt;/p&gt; &lt;p&gt;We've got something exciting for you all - a small preview of what might become the first (or second?) stepping stone for Magnum v5.&lt;/p&gt; &lt;p&gt;One of our members (DeltaVector) has too run some experiments - on a more attainable range of 12b, this time with the help of Gryphe, DoctorShotgun and PocketDoc.&lt;/p&gt; &lt;p&gt;Our internal testing shows this experiment already beats v4 in almost every metric just like DoctorShotguns experiment did on L3.3 70b - and it also follows opus-style prefills very well!&lt;/p&gt; &lt;p&gt;This should serve as an amazing taste of whats to come once we work through the rest of the datasets and pipelines to fully start v5.&lt;/p&gt; &lt;p&gt;Weights and quants are here: &lt;a href="https://huggingface.co/collections/Delta-Vector/rei-12b-6795505005c4a94ebdfdeb39"&gt;https://huggingface.co/collections/Delta-Vector/rei-12b-6795505005c4a94ebdfdeb39&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Have a great weekend! and thank you all for sticking with us for so long, we appreciate all of your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lucyknada"&gt; /u/lucyknada &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9x23l/magnumrei_mistral_nemo_12b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9x23l/magnumrei_mistral_nemo_12b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9x23l/magnumrei_mistral_nemo_12b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T21:06:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1iab0za</id>
    <title>Are local LLMs bad at using tools?</title>
    <updated>2025-01-26T10:19:18+00:00</updated>
    <author>
      <name>/u/Jakedismo</name>
      <uri>https://old.reddit.com/user/Jakedismo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iab0za/are_local_llms_bad_at_using_tools/"&gt; &lt;img alt="Are local LLMs bad at using tools?" src="https://b.thumbs.redditmedia.com/XmP4OmsUbbLKuV3lv7pWpR9Ho_gutAEiOafcYhcGEgA.jpg" title="Are local LLMs bad at using tools?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ex91jw2ocbfe1.png?width=2456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b8065346d27a37764a641452cfa85dbf45c38db"&gt;https://preview.redd.it/ex91jw2ocbfe1.png?width=2456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b8065346d27a37764a641452cfa85dbf45c38db&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I developed the following adaptive RAG workflow with the new llama-index AgentWorfklow functionality. I was spending most of the time debugging because I couldn't get the agents to work. The workflow always stopped at retrieval, luckily I tested running the flow with gpt-4-mini on surprise surprise it worked like a charm. I tested the following models with Ollama and they failed miserably at using tools only reaching the document grading step at best.&lt;br /&gt; Models tested:&lt;br /&gt; Qwen2.5:0.5b&lt;/p&gt; &lt;p&gt;Qwen2:7b&lt;/p&gt; &lt;p&gt;Llama3.2:latest&lt;/p&gt; &lt;p&gt;mistral-nemo&lt;/p&gt; &lt;p&gt;nemotron-mini&lt;/p&gt; &lt;p&gt;So my question is are local small models really bad at using tools in agentic workflows? Does anyone have similar experiences? I have to test running the workflow with 32b models to see if the hypothesis holds.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jakedismo"&gt; /u/Jakedismo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iab0za/are_local_llms_bad_at_using_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iab0za/are_local_llms_bad_at_using_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iab0za/are_local_llms_bad_at_using_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T10:19:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1iab1oe</id>
    <title>Best frameworks for fine-tuning models—what’s everyone using?</title>
    <updated>2025-01-26T10:20:25+00:00</updated>
    <author>
      <name>/u/Vivid-Entertainer752</name>
      <uri>https://old.reddit.com/user/Vivid-Entertainer752</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I’m new to fine-tuning LLMs/SLMs and trying to figure out what tools people usually use for this. From what I’ve seen so far, here are some options:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Hugging Face TRL&lt;/strong&gt; (e.g., SFT, PPO, etc.)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Unsloth AI&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;No-code tools&lt;/strong&gt; like Together AI, Predibase, FinetuneDB&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Any other thing?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vivid-Entertainer752"&gt; /u/Vivid-Entertainer752 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iab1oe/best_frameworks_for_finetuning_modelswhats/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iab1oe/best_frameworks_for_finetuning_modelswhats/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iab1oe/best_frameworks_for_finetuning_modelswhats/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T10:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia2vnu</id>
    <title>Aider polyglot benchmark w/ DeepSeek R1 + DeepSeek V3 near o1 performance</title>
    <updated>2025-01-26T01:43:29+00:00</updated>
    <author>
      <name>/u/serialx_net</name>
      <uri>https://old.reddit.com/user/serialx_net</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia2vnu/aider_polyglot_benchmark_w_deepseek_r1_deepseek/"&gt; &lt;img alt="Aider polyglot benchmark w/ DeepSeek R1 + DeepSeek V3 near o1 performance" src="https://external-preview.redd.it/1R6e2j35zcHWqfwZdGD3tdijh55XqSAoqHzUpCsZqFo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c0d3262bebfa9042aaf27888a03ccfe10eea737" title="Aider polyglot benchmark w/ DeepSeek R1 + DeepSeek V3 near o1 performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/serialx_net"&gt; /u/serialx_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Aider-AI/aider/pull/2998"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia2vnu/aider_polyglot_benchmark_w_deepseek_r1_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia2vnu/aider_polyglot_benchmark_w_deepseek_r1_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T01:43:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia3iwf</id>
    <title>Which one works better, llama 3.3 70b or deepseek r1 70b?</title>
    <updated>2025-01-26T02:17:57+00:00</updated>
    <author>
      <name>/u/SpecialistPear755</name>
      <uri>https://old.reddit.com/user/SpecialistPear755</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don’t see much comparison on this scale of parameters. Do you have any idea?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpecialistPear755"&gt; /u/SpecialistPear755 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia3iwf/which_one_works_better_llama_33_70b_or_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia3iwf/which_one_works_better_llama_33_70b_or_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia3iwf/which_one_works_better_llama_33_70b_or_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T02:17:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia0j9o</id>
    <title>So what is now the best local AI for coding?</title>
    <updated>2025-01-25T23:46:34+00:00</updated>
    <author>
      <name>/u/Tenkinn</name>
      <uri>https://old.reddit.com/user/Tenkinn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I heard that the distill versions of Deepseek r1 are not that good for coding compared to qwen 2.5 coder instruct and the full deepseek r1 version&lt;/p&gt; &lt;p&gt;also that the 32b qwen version is better than the 70b llama one&lt;/p&gt; &lt;p&gt;Is the full deepseek r1 the only model better than Claude sonnet 3.5 ? Is it worth it use use it through the api if we can run the 32b or 70b locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tenkinn"&gt; /u/Tenkinn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia0j9o/so_what_is_now_the_best_local_ai_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia0j9o/so_what_is_now_the_best_local_ai_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia0j9o/so_what_is_now_the_best_local_ai_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T23:46:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9t0x2</id>
    <title>How Chinese AI Startup DeepSeek Made a Model that Rivals OpenAI</title>
    <updated>2025-01-25T18:10:15+00:00</updated>
    <author>
      <name>/u/CarbonTail</name>
      <uri>https://old.reddit.com/user/CarbonTail</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9t0x2/how_chinese_ai_startup_deepseek_made_a_model_that/"&gt; &lt;img alt="How Chinese AI Startup DeepSeek Made a Model that Rivals OpenAI" src="https://external-preview.redd.it/GaYe6FpTRtNr23ADdM65dvNw3TVMjwFcEfKfrHC4ukE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa1a135c85bd082bf94671971fb8ea8e80f02eb2" title="How Chinese AI Startup DeepSeek Made a Model that Rivals OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CarbonTail"&gt; /u/CarbonTail &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wired.com/story/deepseek-china-model-ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9t0x2/how_chinese_ai_startup_deepseek_made_a_model_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9t0x2/how_chinese_ai_startup_deepseek_made_a_model_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T18:10:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia8h46</id>
    <title>How CPU inference speed scales with memory bandwidth</title>
    <updated>2025-01-26T07:18:25+00:00</updated>
    <author>
      <name>/u/TheSilverSmith47</name>
      <uri>https://old.reddit.com/user/TheSilverSmith47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia8h46/how_cpu_inference_speed_scales_with_memory/"&gt; &lt;img alt="How CPU inference speed scales with memory bandwidth" src="https://external-preview.redd.it/srgBSZYNdTn-urtCEL65uOO5QGOSSrTYFh6M4eazrmc.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8851aa0f5e9f9680cf0f2ab8bbc8d8819d519038" title="How CPU inference speed scales with memory bandwidth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's well known in the community by now that inference speed is currently memory bandwidth limited. I wanted to get hands-on experience with this bottleneck, so I set out to do test the CPU inference speed of my laptop at various memory bandwidths. Here are the results.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/57u2fk7idafe1.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=45b99c835893709e93209c8d38ebe1c306aa6fce"&gt;https://preview.redd.it/57u2fk7idafe1.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=45b99c835893709e93209c8d38ebe1c306aa6fce&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o8arwewxdafe1.png?width=1269&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43f0c153e8b87a82b8b11f927e358a9ba4ad29fa"&gt;https://preview.redd.it/o8arwewxdafe1.png?width=1269&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43f0c153e8b87a82b8b11f927e358a9ba4ad29fa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As you can see, inference speed scales pretty linearly with memory bandwidth, affirming what most of us probably already know.&lt;/p&gt; &lt;p&gt;My laptop is an MSI GP66 11UH-028. It has an Intel 11800H, 64GB of 3200 MHz DDR4 RAM, and an 8GB mobile 3080 (although the GPU is not important for this test). To control the memory bandwidth of my system, I set a memory frequency limit in my BIOS. Unfortunately, there is no way to set a custom memory frequency limit, so I had to use the frequency limit presets built into my BIOS. Thankfully, there were plenty of frequency limit presets to choose from.&lt;/p&gt; &lt;p&gt;To validate the frequency of my RAM, I used CPU-Z and multiplied the memory frequency by two.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wbhwk7b2fafe1.png?width=396&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b2f92d3408ec5c7cce23c016d345649f83bc929f"&gt;https://preview.redd.it/wbhwk7b2fafe1.png?width=396&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b2f92d3408ec5c7cce23c016d345649f83bc929f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm not sure why CPU-Z reads the frequency as half of what it actually is. When I set my frequency limit to 3200 MHz, the DRAM frequency read ~1600 MHz; when set to 2667 MHz, it read ~1333 MHz. I'm not sure why this is, but it did it consistently enough that I was comfortable using these values for my measured RAM frequency.&lt;/p&gt; &lt;p&gt;You can calculate the theoretical maximum memory bandwidth of your system using the formula found on &lt;a href="https://www.intel.com/content/www/us/en/support/articles/000056722/processors/intel-core-processors.html"&gt;this&lt;/a&gt; website. To validate the memory bandwidth of my system, I used &lt;a href="https://www.intel.com/content/www/us/en/download/736633/intel-memory-latency-checker-intel-mlc.html"&gt;Intel's Memory Latency Checker&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6tlc0nqufafe1.png?width=549&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6bb514350affe3e2a76a12653cc58e48f64a48c0"&gt;https://preview.redd.it/6tlc0nqufafe1.png?width=549&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6bb514350affe3e2a76a12653cc58e48f64a48c0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The test measured many different values, but the only value I was interested in was the peak injection memory bandwidth.&lt;/p&gt; &lt;p&gt;I then loaded Qwen2.5-0.5B-Q8 into KoboldCPP using my CPU, FlashAttention, and a context length of 4096. I ran an inference 10 times and recorded the total inference rate for each output. I then averaged the inference rate and repeated this test for the various RAM frequency configurations.&lt;/p&gt; &lt;p&gt;I'm pretty satisfied with these results because they show linear scaling of inference speed with memory frequency. Next I plan to do the same test with my &lt;a href="https://www.notebookcheck.net/Intel-UHD-Graphics-Xe-32EUs-GPU-Tiger-Lake-H-Benchmarks-and-Specs.527298.0.html"&gt;iGPU&lt;/a&gt; to see if it will also benefit from higher memory speeds. Then I'll do the same for my dGPU by underclocking and overclocking my VRAM in MSI Afterburner.&lt;/p&gt; &lt;p&gt;If anyone has a Ryzen AI HX 370 CPU, would you be willing to perform the same test that I did for CPU inference? I'm curious to know how that CPU is able to handle a larger LLM (&amp;gt;30b parameters) at high DDR5 frequencies.&lt;/p&gt; &lt;p&gt;I'm also pretty excited for the Ryzen AI Max+ 395, though, given how we are currently memory bandwidth limited, I'm not too sure how the extra compute would help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheSilverSmith47"&gt; /u/TheSilverSmith47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia8h46/how_cpu_inference_speed_scales_with_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia8h46/how_cpu_inference_speed_scales_with_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia8h46/how_cpu_inference_speed_scales_with_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T07:18:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9oqou</id>
    <title>Nvidia to wind down CUDA support for Maxwell and Pascal</title>
    <updated>2025-01-25T15:01:14+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Nvidia's release notes for CUDA 12.8 revealed that Maxwell, Pascal, and Volta GPUs will likely transition to the legacy driver branch. The document states that &amp;quot;architecture support for Maxwell, Pascal, and Volta is considered feature-complete and will be frozen in an upcoming release.&amp;quot; &lt;/p&gt; &lt;p&gt;I think most of us new this day was coming soon. I wouldn't fret too much about it though. This doesn't mean that the cards will stop working or any software built on CUDA will stop working anytime soon. Even if CUDA 12.8 is the last version to support Pascal, I think open source projects like Llama.cpp will continue supporting those cards for a few more years, given how widely used Pascal is in the community and the lack of any decently priced alternatives until now.&lt;/p&gt; &lt;p&gt;If anyone is considering buying a P40 for a new build, I don't think they should change their plans because of this announcement, especially if they find a good deal on the P40. &lt;/p&gt; &lt;p&gt;Personally, I have 10 P40s (just bought 5 last week at $180/card), 4 P100s, and 4 V100s and I'm not planning on retiring them anytime soon. They're great and work really well for my use cases. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpu-drivers/nvidia-starts-phasing-out-maxwell-pascal-and-volta-gpus-geforce-driver-support-status-unclear"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9oqou/nvidia_to_wind_down_cuda_support_for_maxwell_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9oqou/nvidia_to_wind_down_cuda_support_for_maxwell_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T15:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia95ap</id>
    <title>Reinforcement Learning Works! Reflecting on Chinese Models DeepSeek-R1 and Kimi k1.5</title>
    <updated>2025-01-26T08:07:40+00:00</updated>
    <author>
      <name>/u/phoneixAdi</name>
      <uri>https://old.reddit.com/user/phoneixAdi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia95ap/reinforcement_learning_works_reflecting_on/"&gt; &lt;img alt="Reinforcement Learning Works! Reflecting on Chinese Models DeepSeek-R1 and Kimi k1.5" src="https://external-preview.redd.it/rp7VJjsBbW35ue65hF_AusrsgbxRmLjaK2DB4dGUml4.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0047285645af07421832798a077a8010328c7dc" title="Reinforcement Learning Works! Reflecting on Chinese Models DeepSeek-R1 and Kimi k1.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phoneixAdi"&gt; /u/phoneixAdi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/MbX9J1Tt_I0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia95ap/reinforcement_learning_works_reflecting_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia95ap/reinforcement_learning_works_reflecting_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T08:07:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9zdh3</id>
    <title>Best NSFW model for story telling?</title>
    <updated>2025-01-25T22:51:40+00:00</updated>
    <author>
      <name>/u/Might-Be-A-Ninja</name>
      <uri>https://old.reddit.com/user/Might-Be-A-Ninja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't know if there are any models geared for it, but I want something that can write full stories, with me just giving it some direction&lt;/p&gt; &lt;p&gt;I am mainly into BDSM, if it matters&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Might-Be-A-Ninja"&gt; /u/Might-Be-A-Ninja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9zdh3/best_nsfw_model_for_story_telling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9zdh3/best_nsfw_model_for_story_telling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9zdh3/best_nsfw_model_for_story_telling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T22:51:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia2rzn</id>
    <title>Flash Attention T5</title>
    <updated>2025-01-26T01:38:07+00:00</updated>
    <author>
      <name>/u/bratao</name>
      <uri>https://old.reddit.com/user/bratao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia2rzn/flash_attention_t5/"&gt; &lt;img alt="Flash Attention T5" src="https://external-preview.redd.it/Ep1yoPi5mHpASN_9oXAcIW-Bnp0muHVqmp_U98PZOrY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9fd014f4d0958f4a616b8cac67feeaf17a0abf78" title="Flash Attention T5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bratao"&gt; /u/bratao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/CATIE-AQ/FAT5-report"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia2rzn/flash_attention_t5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia2rzn/flash_attention_t5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T01:38:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9wnfs</id>
    <title>Why do openai and meta etc plan to spend so much on data centers? how do they make the money back?</title>
    <updated>2025-01-25T20:48:52+00:00</updated>
    <author>
      <name>/u/lblblllb</name>
      <uri>https://old.reddit.com/user/lblblllb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chatgpt already has over 180mm users, which is over half of US population. With exception of limitation on o1, the service uptime seems mostly fine so far? why spend up to 500bln to build data centers for exclusive use of openai that will depreciate very quickly(due to GPU depreciation)? Same for meta spending 60bln on AI. how do they plan to make the money back? seems like they really have to be able to use AI to replace most of the knowledge workers in order to make a return.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reuters.com/business/media-telecom/stargate-artificial-intelligence-project-exclusively-serve-openai-ft-reports-2025-01-24/"&gt;https://www.reuters.com/business/media-telecom/stargate-artificial-intelligence-project-exclusively-serve-openai-ft-reports-2025-01-24/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lblblllb"&gt; /u/lblblllb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wnfs/why_do_openai_and_meta_etc_plan_to_spend_so_much/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wnfs/why_do_openai_and_meta_etc_plan_to_spend_so_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wnfs/why_do_openai_and_meta_etc_plan_to_spend_so_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T20:48:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia10ld</id>
    <title>Msty connecting to a Chinese server in Hong Kong</title>
    <updated>2025-01-26T00:09:44+00:00</updated>
    <author>
      <name>/u/urubuz</name>
      <uri>https://old.reddit.com/user/urubuz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia10ld/msty_connecting_to_a_chinese_server_in_hong_kong/"&gt; &lt;img alt="Msty connecting to a Chinese server in Hong Kong" src="https://b.thumbs.redditmedia.com/tlmfTviR5XharLxlY5KMT6R5OwwGM__phYc7Jk5IKhY.jpg" title="Msty connecting to a Chinese server in Hong Kong" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to &lt;a href="https://msty.app/privacy:"&gt;https://msty.app/privacy:&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;gt; We do not gather any telemetry data except for app open ping. All data is stored locally on your device and is NEVER transmitted to our servers.&lt;/p&gt; &lt;p&gt;Here's what Little Snitch Mini is reporting when the app booted up:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0twxvig8b8fe1.png?width=2064&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=788f2132c382b26e43f85871e216c1e03f833537"&gt;https://preview.redd.it/0twxvig8b8fe1.png?width=2064&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=788f2132c382b26e43f85871e216c1e03f833537&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/urubuz"&gt; /u/urubuz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia10ld/msty_connecting_to_a_chinese_server_in_hong_kong/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia10ld/msty_connecting_to_a_chinese_server_in_hong_kong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia10ld/msty_connecting_to_a_chinese_server_in_hong_kong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T00:09:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9wbya</id>
    <title>ByteDance announces Doubao-1.5-pro</title>
    <updated>2025-01-25T20:34:44+00:00</updated>
    <author>
      <name>/u/Outrageous-Win-3244</name>
      <uri>https://old.reddit.com/user/Outrageous-Win-3244</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wbya/bytedance_announces_doubao15pro/"&gt; &lt;img alt="ByteDance announces Doubao-1.5-pro" src="https://preview.redd.it/5pjykhaha7fe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0df07e6b549319488a93d42063d7e338ff3b8b7" title="ByteDance announces Doubao-1.5-pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ByteDance announces Doubao-1.5-pro&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Includes a &amp;quot;Deep Thinking&amp;quot; mode, surpassing O1-preview and O1 models on the AIME benchmark.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Outperforms deepseek-v3, gpt4o, and llama3.1-405B on popular benchmarks. &lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Built on a MoE architecture, with activated parameters far fewer than those in the above models. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Achieves a 7x MoE performance leverage—delivering dense model performance with just 1/7 of the activated parameters (e.g., 20B activated params = 140B dense performance). &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Engineering-wise, features heterogeneous system design for prefill-decode and attn-fffn, maximizing throughput under low-latency requirements.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous-Win-3244"&gt; /u/Outrageous-Win-3244 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5pjykhaha7fe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wbya/bytedance_announces_doubao15pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9wbya/bytedance_announces_doubao15pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T20:34:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia9nzm</id>
    <title>China Unicom announced Unichat-32B-c1 (Beat GPT-4 and Deepseek V3)</title>
    <updated>2025-01-26T08:47:19+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia9nzm/china_unicom_announced_unichat32bc1_beat_gpt4_and/"&gt; &lt;img alt="China Unicom announced Unichat-32B-c1 (Beat GPT-4 and Deepseek V3)" src="https://external-preview.redd.it/xPmyWRJan7ulb3PcWvyYjbtkiBhdqYVeIf1sY9iUMiM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=285acbaf1e44da73546abd95c35ff3a0369dd036" title="China Unicom announced Unichat-32B-c1 (Beat GPT-4 and Deepseek V3)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Yuansheng Thinking Chain Large Model achieves adaptive slow thinking through two strategies: task adaptation and difficulty adaptation. In the evaluation set of non-inference tasks, this model tends to generate shorter answers while ensuring accuracy, thus improving response efficiency. Additionally, when evaluating generated long thinking chain data, the model comprehensively considers the difficulty of the questions and the length of the generated answers, using reinforcement learning to match the answer length with the question difficulty, further enhancing the model's accuracy and practicality.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bd28qcf9xafe1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=214bdd271beeb39ee43991f43b78158e0077927e"&gt;https://preview.redd.it/bd28qcf9xafe1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=214bdd271beeb39ee43991f43b78158e0077927e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/UnicomAI/Unichat-32B-c1.git"&gt;Model Link (Chinese Only)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia9nzm/china_unicom_announced_unichat32bc1_beat_gpt4_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia9nzm/china_unicom_announced_unichat32bc1_beat_gpt4_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia9nzm/china_unicom_announced_unichat32bc1_beat_gpt4_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T08:47:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia1d4t</id>
    <title>7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Efficient</title>
    <updated>2025-01-26T00:26:33+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia1d4t/7b_model_and_8k_examples_emerging_reasoning_with/"&gt; &lt;img alt="7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Efficient" src="https://external-preview.redd.it/88h1mEnLx1t41jw5cSvvfzo0nRgYDTSe3ZMQUihAxm4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c3ddfdce52f96a0c0d08d64102e978a19be46554" title="7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Efficient" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hkust-nlp.notion.site/simplerl-reason"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia1d4t/7b_model_and_8k_examples_emerging_reasoning_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia1d4t/7b_model_and_8k_examples_emerging_reasoning_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T00:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia4mx6</id>
    <title>Project Digits Memory Speed</title>
    <updated>2025-01-26T03:17:58+00:00</updated>
    <author>
      <name>/u/LostMyOtherAcct69</name>
      <uri>https://old.reddit.com/user/LostMyOtherAcct69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I recently saw an accidentally leaked slide from Nvidia on Project Digits memory speed. It is 273 GB/s.&lt;/p&gt; &lt;p&gt;Also 128 GB is the base memory. Only storage will have “pay to upgrade” tiers.&lt;/p&gt; &lt;p&gt;Wanted to give credit to this user. Completely correct. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/s/tvWyPqdZuJ"&gt;https://www.reddit.com/r/LocalLLaMA/s/tvWyPqdZuJ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Hoping for a May launch I heard too.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostMyOtherAcct69"&gt; /u/LostMyOtherAcct69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia4mx6/project_digits_memory_speed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia4mx6/project_digits_memory_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia4mx6/project_digits_memory_speed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T03:17:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia40om</id>
    <title>Would give up a kidney for a local audio model that’s even half as good as Suno</title>
    <updated>2025-01-26T02:44:27+00:00</updated>
    <author>
      <name>/u/Effective_Garbage_34</name>
      <uri>https://old.reddit.com/user/Effective_Garbage_34</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alright, I’ve tried pretty much every local audio model out there—MusicGen, AudioCraft, Coqui TTS, NSynth—whatever. And they all sound… bad. Like, really bad. Meanwhile, Suno is out here sounding like magic, and I’m just sitting here wondering: what the hell are they doing differently?&lt;/p&gt; &lt;p&gt;Is it their training data? Some proprietary wizardry? Did they make a deal with the devil? Whatever it is, local models are so far behind it’s almost depressing.&lt;/p&gt; &lt;p&gt;I’d love to get even a fraction of Suno’s quality in something I can run locally. Has anyone figured out a way forward? Is there hope for local models, or are we stuck dreaming from a distance?&lt;/p&gt; &lt;p&gt;Seriously, what’s the secret sauce? If anyone has insight, please share—I’m desperate over here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective_Garbage_34"&gt; /u/Effective_Garbage_34 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia40om/would_give_up_a_kidney_for_a_local_audio_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia40om/would_give_up_a_kidney_for_a_local_audio_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia40om/would_give_up_a_kidney_for_a_local_audio_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T02:44:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia7v0x</id>
    <title>the MNN team at Alibaba has open-sourced multimodal Android app running without netowrk that supports: Audio , Image and Diffusion Models. with blazing-fast speeds on cpu with 2.3x faster decoding speeds compared to llama.cpp.</title>
    <updated>2025-01-26T06:34:30+00:00</updated>
    <author>
      <name>/u/Juude89</name>
      <uri>https://old.reddit.com/user/Juude89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia7v0x/the_mnn_team_at_alibaba_has_opensourced/"&gt; &lt;img alt="the MNN team at Alibaba has open-sourced multimodal Android app running without netowrk that supports: Audio , Image and Diffusion Models. with blazing-fast speeds on cpu with 2.3x faster decoding speeds compared to llama.cpp." src="https://external-preview.redd.it/GkVEj8SEXSUPkh7D3z-zTGfIOGq41yOTpDWE4Fj1WE4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01e2d3507d8b0dc551310e81cc376bab8fb4fc91" title="the MNN team at Alibaba has open-sourced multimodal Android app running without netowrk that supports: Audio , Image and Diffusion Models. with blazing-fast speeds on cpu with 2.3x faster decoding speeds compared to llama.cpp." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;app maim page: &lt;a href="https://github.com/alibaba/MNN"&gt;MNN-LLM-APP&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5xo6fjer8afe1.png?width=1780&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2da05212d5e1af8855cedc2a23a8166e4a5340dc"&gt;the mulitimodal app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;inference speed vs llama.cpp&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/elrqgjh59afe1.gif"&gt;https://i.redd.it/elrqgjh59afe1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juude89"&gt; /u/Juude89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia7v0x/the_mnn_team_at_alibaba_has_opensourced/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia7v0x/the_mnn_team_at_alibaba_has_opensourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia7v0x/the_mnn_team_at_alibaba_has_opensourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T06:34:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9nqj9</id>
    <title>Full open source reproduction of R1 in progress ⏳</title>
    <updated>2025-01-25T14:11:35+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9nqj9/full_open_source_reproduction_of_r1_in_progress/"&gt; &lt;img alt="Full open source reproduction of R1 in progress ⏳" src="https://preview.redd.it/s5rmvdhtd5fe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbf96bf7e9979be87994f66f0537b9e70492b54b" title="Full open source reproduction of R1 in progress ⏳" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s5rmvdhtd5fe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9nqj9/full_open_source_reproduction_of_r1_in_progress/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9nqj9/full_open_source_reproduction_of_r1_in_progress/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T14:11:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i9y42v</id>
    <title>New OpenAI</title>
    <updated>2025-01-25T21:54:09+00:00</updated>
    <author>
      <name>/u/notomarsol</name>
      <uri>https://old.reddit.com/user/notomarsol</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9y42v/new_openai/"&gt; &lt;img alt="New OpenAI" src="https://preview.redd.it/ppnejgtgo7fe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e4cae2970050d080916629397ce588f1598ea49" title="New OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notomarsol"&gt; /u/notomarsol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ppnejgtgo7fe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i9y42v/new_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i9y42v/new_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-25T21:54:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia9iy1</id>
    <title>DeepSeekR1 3D game 100% from scratch</title>
    <updated>2025-01-26T08:36:26+00:00</updated>
    <author>
      <name>/u/Trick-Independent469</name>
      <uri>https://old.reddit.com/user/Trick-Independent469</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia9iy1/deepseekr1_3d_game_100_from_scratch/"&gt; &lt;img alt="DeepSeekR1 3D game 100% from scratch" src="https://preview.redd.it/qrdlt6i8vafe1.gif?width=640&amp;amp;crop=smart&amp;amp;s=8d13a97797fa31e558155d2f6738fd891080c24b" title="DeepSeekR1 3D game 100% from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've asked DeepSeek R1 to make me a game like kkrieger ( where most of the things are generated on run ) and it made me this &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trick-Independent469"&gt; /u/Trick-Independent469 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qrdlt6i8vafe1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia9iy1/deepseekr1_3d_game_100_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia9iy1/deepseekr1_3d_game_100_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T08:36:26+00:00</published>
  </entry>
</feed>
