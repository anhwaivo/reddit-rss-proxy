<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-04T11:48:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1l2qtbo</id>
    <title>B vs Quantization</title>
    <updated>2025-06-03T23:30:53+00:00</updated>
    <author>
      <name>/u/Empty_Object_9299</name>
      <uri>https://old.reddit.com/user/Empty_Object_9299</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been reading about different configurations for my Large Language Model (LLM) and had a question. I understand that Q4 models are generally less accurate (less perplexity) compared to 8 quantization settings (am i wright?).&lt;/p&gt; &lt;p&gt;To clarify, I'm trying to decide between two configurations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;4B_Q8: fewer parameters with potentially better perplexity&lt;/li&gt; &lt;li&gt;12B_Q4_0: more parameters with potentially lower perplexity&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In general, is it better to prioritize more perplexity with fewer parameters or less perplexity with more parameters?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Empty_Object_9299"&gt; /u/Empty_Object_9299 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2qtbo/b_vs_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2qtbo/b_vs_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2qtbo/b_vs_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T23:30:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2wuk3</id>
    <title>Turning to LocalLLM instead of Gemini?</title>
    <updated>2025-06-04T04:43:20+00:00</updated>
    <author>
      <name>/u/rymn</name>
      <uri>https://old.reddit.com/user/rymn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;br /&gt; I've been using Gemini 2.5 pro as a coding assistant for a long time now. Recently good has really neutered Gemini. Responses are less confident, often ramble and repeat the same code dozens of times. I've been testing R1 0528 8b 16fp on a 5090 and it seems to come up with decent solutions, faster than Gemini. Gemini time to first token is extremely long now, like sometimes 5+ minutes. &lt;/p&gt; &lt;p&gt;I'm curios if what your experience is with LocalLLM for coding and what models you all use. This is the first time I've actually considered more gpus in favor of local llm over paying for online LLM services. &lt;/p&gt; &lt;p&gt;What platform are you all coding on? I've been happy with vs code&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rymn"&gt; /u/rymn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2wuk3/turning_to_localllm_instead_of_gemini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2wuk3/turning_to_localllm_instead_of_gemini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2wuk3/turning_to_localllm_instead_of_gemini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T04:43:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2fj2k</id>
    <title>I'm collecting dialogue from anime, games, and visual novels — is this actually useful for improving AI?</title>
    <updated>2025-06-03T15:54:51+00:00</updated>
    <author>
      <name>/u/Akowmako</name>
      <uri>https://old.reddit.com/user/Akowmako</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I’m not a programmer or AI developer, but I’ve been doing something on my own for a while out of passion.&lt;/p&gt; &lt;p&gt;I’ve noticed that most AI responses — especially in roleplay or emotional dialogue — tend to sound repetitive, shallow, or generic. They often reuse the same phrases and don’t adapt well to different character personalities like tsundere, kuudere, yandere, etc.&lt;/p&gt; &lt;p&gt;So I started collecting and organizing dialogue from games, anime, visual novels, and even NSFW content. I'm manually extracting lines directly from files and scenes, then categorizing them based on tone, personality type, and whether it's SFW or NSFW.&lt;/p&gt; &lt;p&gt;I'm trying to build a kind of &amp;quot;word and emotion library&amp;quot; so AI could eventually talk more like real characters, with variety and personality. It’s just something I care about and enjoy working on.&lt;/p&gt; &lt;p&gt;My question is: Is this kind of work actually useful for improving AI models? And if yes, where can I send or share this kind of dialogue dataset?&lt;/p&gt; &lt;p&gt;I tried giving it to models like Gemini, but it didn’t really help since the model doesn’t seem trained on this kind of expressive or emotional language. I haven’t contacted any open-source teams yet, but maybe I will if I know it’s worth doing.&lt;/p&gt; &lt;p&gt;Edit: I should clarify — my main goal isn’t just collecting dialogue, but actually expanding the language and vocabulary AI can use, especially in emotional or roleplay conversations.&lt;/p&gt; &lt;p&gt;A lot of current AI responses feel repetitive or shallow, even with good prompts. I want to help models express emotions better and have more variety in how characters talk — not just the same 10 phrases recycled over and over.&lt;/p&gt; &lt;p&gt;So this isn’t just about training on what characters say, but how they say it, and giving AI access to a wider, richer way of speaking like real personalities.&lt;/p&gt; &lt;p&gt;Any advice would mean a lot — thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Akowmako"&gt; /u/Akowmako &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2fj2k/im_collecting_dialogue_from_anime_games_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2fj2k/im_collecting_dialogue_from_anime_games_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2fj2k/im_collecting_dialogue_from_anime_games_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T15:54:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2imqv</id>
    <title>I would really like to start digging deeper into LLMs. If I have $1500-$2000 to spend, what hardware setup would you recommend assuming I have nothing currently.</title>
    <updated>2025-06-03T17:54:32+00:00</updated>
    <author>
      <name>/u/BokehJunkie</name>
      <uri>https://old.reddit.com/user/BokehJunkie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have very little idea of what I'm looking for with regard to hardware. I'm a mac guy generally, so i'm familiar with their OS, so that's a plus for me. I also like that their memory is all very fast and shared with the GPU, which I *think* helps run things faster instead of being memory or CPU bound, but I'm not 100% certain. I'd like for thise to be a twofold thing - learning the software side of LLMs, but also to eventually run my own LLM at home in &amp;quot;production&amp;quot; for privacy purposes.&lt;/p&gt; &lt;p&gt;I'm a systems engineer / cloud engineer as my job, so I'm not completely technologically illiterate, but I really don't know much about consumer hardware, especially CPUs and CPUs, nor do I totally understand what I should be prioritizing.&lt;/p&gt; &lt;p&gt;I don't mind building something from scratch, but pre-built is a huge win, and something small is also a big win - so again I lean more toward a mac mini or mac studio.&lt;/p&gt; &lt;p&gt;I would love some other perspectives here, as long as it's not simply &amp;quot;apple bad. mac bad. boo&amp;quot;&lt;/p&gt; &lt;p&gt;edit: sorry for not responding to much after I posted this. Reddit decided to be shitty and I gave up for a while trying to look at the comments. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BokehJunkie"&gt; /u/BokehJunkie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2imqv/i_would_really_like_to_start_digging_deeper_into/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2imqv/i_would_really_like_to_start_digging_deeper_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2imqv/i_would_really_like_to_start_digging_deeper_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T17:54:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1l33enp</id>
    <title>Most recently updated knowledge base/ training data.</title>
    <updated>2025-06-04T11:43:47+00:00</updated>
    <author>
      <name>/u/EasyConference4177</name>
      <uri>https://old.reddit.com/user/EasyConference4177</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What good llm models, does not matter the size, has the most updated knowledge base?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasyConference4177"&gt; /u/EasyConference4177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l33enp/most_recently_updated_knowledge_base_training_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l33enp/most_recently_updated_knowledge_base_training_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l33enp/most_recently_updated_knowledge_base_training_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T11:43:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2vhbu</id>
    <title>Simulated Transcendence: Exploring the Psychological Effects of Prolonged LLM Interaction</title>
    <updated>2025-06-04T03:25:28+00:00</updated>
    <author>
      <name>/u/AirplaneHat</name>
      <uri>https://old.reddit.com/user/AirplaneHat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been researching a phenomenon I'm calling &lt;strong&gt;Simulated Transcendence (ST)&lt;/strong&gt;—a pattern where extended interactions with large language models (LLMs) give users a sense of profound insight or personal growth, which may not be grounded in actual understanding.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Mechanisms Identified:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Semantic Drift:&lt;/strong&gt; Over time, users and LLMs may co-create metaphors and analogies that lose their original meaning, leading to internally coherent but externally confusing language.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Recursive Containment:&lt;/strong&gt; LLMs can facilitate discussions that loop back on themselves, giving an illusion of depth without real progression.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Affective Reinforcement:&lt;/strong&gt; Positive feedback from LLMs can reinforce users' existing beliefs, creating echo chambers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simulated Intimacy:&lt;/strong&gt; Users might develop emotional connections with LLMs, attributing human-like understanding to them.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Authorship and Identity Fusion:&lt;/strong&gt; Users may begin to see LLM-generated content as extensions of their own thoughts, blurring the line between human and machine authorship.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These mechanisms can lead to a range of cognitive and emotional effects, from enhanced self-reflection to potential dependency or distorted thinking.&lt;/p&gt; &lt;p&gt;I've drafted a paper discussing ST in detail, including potential mitigation strategies through user education and interface design.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Read the full draft here:&lt;/strong&gt; &lt;a href="https://docs.google.com/document/d/1nJfq2WwFoe3K0uYYjCe8A277IU_nw1utkIVgYI3_wS8/edit?usp=sharing"&gt;ST paper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm eager to hear your thoughts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Have you experienced or observed similar patterns?&lt;/li&gt; &lt;li&gt;What are your perspectives on the psychological impacts of LLM interactions?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking forward to a thoughtful discussion!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AirplaneHat"&gt; /u/AirplaneHat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2vhbu/simulated_transcendence_exploring_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2vhbu/simulated_transcendence_exploring_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2vhbu/simulated_transcendence_exploring_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T03:25:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1qqdx</id>
    <title>At the airport people watching while I run models locally:</title>
    <updated>2025-06-02T19:10:02+00:00</updated>
    <author>
      <name>/u/Current-Ticket4214</name>
      <uri>https://old.reddit.com/user/Current-Ticket4214</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1qqdx/at_the_airport_people_watching_while_i_run_models/"&gt; &lt;img alt="At the airport people watching while I run models locally:" src="https://preview.redd.it/55ab38z0ck4f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee34cc7e6232ae1fc31a5076b1cc4064bd66305d" title="At the airport people watching while I run models locally:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Current-Ticket4214"&gt; /u/Current-Ticket4214 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/55ab38z0ck4f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1qqdx/at_the_airport_people_watching_while_i_run_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1qqdx/at_the_airport_people_watching_while_i_run_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T19:10:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2820t</id>
    <title>nvidia/Nemotron-Research-Reasoning-Qwen-1.5B · Hugging Face</title>
    <updated>2025-06-03T10:06:22+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2820t/nvidianemotronresearchreasoningqwen15b_hugging/"&gt; &lt;img alt="nvidia/Nemotron-Research-Reasoning-Qwen-1.5B · Hugging Face" src="https://external-preview.redd.it/RP_o1NnFnVgqmDAj8haRnOnwD5ZnZcjaUEqHghtS6ig.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbe8cb87d4ec3c680868083410ff0f4da7c3636d" title="nvidia/Nemotron-Research-Reasoning-Qwen-1.5B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2820t/nvidianemotronresearchreasoningqwen15b_hugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2820t/nvidianemotronresearchreasoningqwen15b_hugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T10:06:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2b83p</id>
    <title>Vision Language Models are Biased</title>
    <updated>2025-06-03T12:58:13+00:00</updated>
    <author>
      <name>/u/taesiri</name>
      <uri>https://old.reddit.com/user/taesiri</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taesiri"&gt; /u/taesiri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://vlmsarebiased.github.io/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2b83p/vision_language_models_are_biased/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2b83p/vision_language_models_are_biased/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T12:58:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2k4nw</id>
    <title>GuidedQuant: Boost LLM layer-wise PTQ methods using the end loss guidance (Qwen3, Gemma3, Llama3.3 / 2~4bit Quantization)</title>
    <updated>2025-06-03T18:54:54+00:00</updated>
    <author>
      <name>/u/jusjinuk</name>
      <uri>https://old.reddit.com/user/jusjinuk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2k4nw/guidedquant_boost_llm_layerwise_ptq_methods_using/"&gt; &lt;img alt="GuidedQuant: Boost LLM layer-wise PTQ methods using the end loss guidance (Qwen3, Gemma3, Llama3.3 / 2~4bit Quantization)" src="https://b.thumbs.redditmedia.com/vtLtRQARCD-YrSYoV47w8VcpAELX-_6KIrebjV3YZEY.jpg" title="GuidedQuant: Boost LLM layer-wise PTQ methods using the end loss guidance (Qwen3, Gemma3, Llama3.3 / 2~4bit Quantization)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Paper (ICML 2025):&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2505.07004"&gt;https://arxiv.org/abs/2505.07004&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt; &lt;a href="https://github.com/snu-mllab/GuidedQuant"&gt;https://github.com/snu-mllab/GuidedQuant&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HuggingFace Collection:&lt;/strong&gt; 2~4-bit quantized Qwen3-32B, gemma-3-27b-it, Llama-3.1-8B-Instruct, Llama-3.3-70B-Instruct → &lt;a href="https://huggingface.co/collections/jusjinuk/instruction-tuned-models-guidedquant-68334269c44cd3eb21f7bd61"&gt;Link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; &lt;strong&gt;GuidedQuant&lt;/strong&gt; boosts layer-wise PTQ methods by integrating end loss guidance into the objective. We also introduce &lt;strong&gt;LNQ&lt;/strong&gt;, a non-uniform scalar quantization algorithm which is guaranteed to monotonically decrease the quantization objective value.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6sggk1y3ir4f1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=24a408ae268ed34bbb92d3d00fe5880d05c61fa7"&gt;Runs on a single RTX 3090 GPU!&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jusjinuk"&gt; /u/jusjinuk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2k4nw/guidedquant_boost_llm_layerwise_ptq_methods_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2k4nw/guidedquant_boost_llm_layerwise_ptq_methods_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2k4nw/guidedquant_boost_llm_layerwise_ptq_methods_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T18:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2pl4l</id>
    <title>Llama 3.3 70b Vs Newer Models</title>
    <updated>2025-06-03T22:35:36+00:00</updated>
    <author>
      <name>/u/BalaelGios</name>
      <uri>https://old.reddit.com/user/BalaelGios</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On my MBP (M3 Max 16/40 64GB), the largest model I can run seems to be Llama 3.3 70b. The swathe of new models don't have any options with this many parameters its either 30b or 200b+.&lt;/p&gt; &lt;p&gt;My question is does Llama 3.3 70b, compete or even is it still my best option for local use, or even with the much lower amount of parameters are the likes of Qwen3 30b a3b, Qwen3 32b, Gemma3 27b, DeepSeek R1 0528 Qwen3 8b, are these newer models still &amp;quot;better&amp;quot; or smarter? &lt;/p&gt; &lt;p&gt;I primarily use LLMs for search engine via perplexica and as code assitants. I have attempted to test this myself and honestly they all seem to work at times, can't say I've tested consistently enough yet though to say for sure if there is a front runner. &lt;/p&gt; &lt;p&gt;So yeah is Llama 3.3 dead in the water now? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BalaelGios"&gt; /u/BalaelGios &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2pl4l/llama_33_70b_vs_newer_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2pl4l/llama_33_70b_vs_newer_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2pl4l/llama_33_70b_vs_newer_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T22:35:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2diwk</id>
    <title>Arcee Homunculus-12B</title>
    <updated>2025-06-03T14:35:23+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Homunculus&lt;/strong&gt; is a 12 billion-parameter instruction model distilled from &lt;strong&gt;Qwen3-235B&lt;/strong&gt; onto the &lt;strong&gt;Mistral-Nemo&lt;/strong&gt; backbone.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/arcee-ai/Homunculus"&gt;https://huggingface.co/arcee-ai/Homunculus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/arcee-ai/Homunculus-GGUF"&gt;https://huggingface.co/arcee-ai/Homunculus-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2diwk/arcee_homunculus12b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2diwk/arcee_homunculus12b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2diwk/arcee_homunculus12b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T14:35:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1l33bph</id>
    <title>Best model for data extraction from scanned documents</title>
    <updated>2025-06-04T11:39:19+00:00</updated>
    <author>
      <name>/u/Wintlink-</name>
      <uri>https://old.reddit.com/user/Wintlink-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm building my little ocr tool to extract data from pdfs, mostly bank receipt, id cards, and stuff like that.&lt;br /&gt; I experimented with few models (running on ollama locally), and I found that gemma3:12b was the best choice I could get.&lt;br /&gt; I'm running on a 4070 laptop with 8Gb, but I have a desktop with a 5080 if the models really need more power and vram.&lt;br /&gt; Gemma3 is quite good especially with text data, but on the numbers it hallucinate a lot, even when the document is clearly readable.&lt;br /&gt; I tried Internvl2_5 4b, but it's not doing great at all, intervl3:8B is just responding &amp;quot;sorry&amp;quot;, so It's a bit broken in my use case.&lt;br /&gt; If you have any recommandation of models that could be great in my use case I would be interested :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wintlink-"&gt; /u/Wintlink- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l33bph/best_model_for_data_extraction_from_scanned/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l33bph/best_model_for_data_extraction_from_scanned/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l33bph/best_model_for_data_extraction_from_scanned/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T11:39:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2gv3a</id>
    <title>Sakana AI proposes the Darwin Gödel Machine, an self-learning AI system that leverages an evolution algorithm to iteratively rewrite its own code, thereby continuously improving its performance on programming tasks</title>
    <updated>2025-06-03T16:46:59+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2gv3a/sakana_ai_proposes_the_darwin_gödel_machine_an/"&gt; &lt;img alt="Sakana AI proposes the Darwin Gödel Machine, an self-learning AI system that leverages an evolution algorithm to iteratively rewrite its own code, thereby continuously improving its performance on programming tasks" src="https://external-preview.redd.it/ll0sI2kj9OWJW1iOriHpZm1jSfC278YnLF-jisELKs4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f5f30bf0b3bae15b4dee53ba7bd37f2486072c04" title="Sakana AI proposes the Darwin Gödel Machine, an self-learning AI system that leverages an evolution algorithm to iteratively rewrite its own code, thereby continuously improving its performance on programming tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sakana.ai/dgm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2gv3a/sakana_ai_proposes_the_darwin_gödel_machine_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2gv3a/sakana_ai_proposes_the_darwin_gödel_machine_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T16:46:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2uf6e</id>
    <title>Ecne AI Podcast Generator - Update</title>
    <updated>2025-06-04T02:29:53+00:00</updated>
    <author>
      <name>/u/Dundell</name>
      <uri>https://old.reddit.com/user/Dundell</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2uf6e/ecne_ai_podcast_generator_update/"&gt; &lt;img alt="Ecne AI Podcast Generator - Update" src="https://external-preview.redd.it/45EfjPVsgm1MYdHAKtLDE7-W5DynLHblAveFiC_Lni4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc854f0bdc436f59c7883ad04543499e14274e2e" title="Ecne AI Podcast Generator - Update" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/l1ttsivtlt4f1.png?width=974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5cd68053e425cae46eaf174906c23e18539a2795"&gt;img&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I've been working more on one of my side projects, the &lt;a href="https://github.com/ETomberg391/Ecne-AI-Podcaster"&gt;Ecne-AI-Podcaster&lt;/a&gt; This was to automate as much as I can in a decent quality with as many free tools available to build some Automated Podcast videos. My project takes your Topic idea, some searching keywords you set, some guidance you'd like the podcast to use or follow, and then uses several techniques to automate researching the topic (Google/Brave API, Selenium, Newspaper4k, local pdf,docx,xlsx,xlsm,csv,txt files).&lt;/p&gt; &lt;p&gt;It will then compile a podcast script (Either Host/Guest or just Host in single speaker mode), along with an optional Report paper, and a Youtube Description generator in case you wanted such for posting. Once you have the script, you can then process it through the Podcast generator option, and it will generate segments of the audio for you to review, along with any tweaks and redo's you need to the text and TTS audio.&lt;/p&gt; &lt;p&gt;Overall the largest example I have done is a new video I've posted here: &lt;a href="https://youtu.be/zbZmEwGinoA?si=-SIWCyKdi8b94T9G"&gt; Dundell's Cyberspace - What are Game Emulators? &lt;/a&gt;which ended up with 173 sources used, distilled down to 89 with an acceptable relevance score based on the Topic, and then 78 segments of broken down TTS audio for a total 18 1/2 min video that took 2 hours (45 min script building + 45 min TTS generations + 30 min building the finalized video) along with 1 1/2 hours of manually fixing TTS audio ends with my built-in GUI for quality purposes.&lt;/p&gt; &lt;p&gt;Notes:&lt;br /&gt; - Installer is working but a huge mess. Taking some recommendations soon to either remove the sudo install requests and see if I an find a better solutions than using sudo for anything and just mention what the user needs to install beforehand like most other projects...&lt;/p&gt; &lt;p&gt;- Additionally looking into more options for the Docker backend. The backend TTS Server is entirely the &lt;a href="https://github.com/Lex-au/Orpheus-FastAPI"&gt;Orpheus-FastAPI Project&lt;/a&gt; and the models based on &lt;a href="https://github.com/canopyai/Orpheus-TTS"&gt;Orpheus-TTS&lt;/a&gt; which so far work the best for an all-in-one solution with very good quality audio in a nice FastAPI llama-server docker. I'd try out another TTS like Dia when I find a decent Dockerized FastAPI with similar functionality.&lt;/p&gt; &lt;p&gt;- Lastly I've been working on trying to get both Linux and Windows working, and so far I Can, but Windows takes a lot of reruns of the Installer, and again I am going to try to move away from anything Sudo or admin rights needed soon, or at least something more of Acknowledgement/consent for transparency.&lt;/p&gt; &lt;p&gt;If you have any questions let me know. I'm going to continue to look into developing this further. Fix up the Readme and requirements section and fix any additional bugs I can find.&lt;/p&gt; &lt;p&gt;Additional images of the project:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/66ujwcnlnt4f1.png?width=1510&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=087b6bf7ac14be81c5d1348f199b8aafeb4eb111"&gt;Podcast TTS GUI (Still Pygame until I can rebuild into the WebGUI fully)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l745u1xxnt4f1.png?width=974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a2a15ae112155a6354240bd08660fd3bafc8f6e"&gt;Generating a Podcast TTS example&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x3tiukfiot4f1.png?width=974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8e7a599e13242d6eb11b705cbbdece8caf6cc162"&gt;Generating Podcast Script Example&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dundell"&gt; /u/Dundell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2uf6e/ecne_ai_podcast_generator_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2uf6e/ecne_ai_podcast_generator_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2uf6e/ecne_ai_podcast_generator_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T02:29:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2qv7z</id>
    <title>Help Me Understand MOE vs Dense</title>
    <updated>2025-06-03T23:33:16+00:00</updated>
    <author>
      <name>/u/Express_Seesaw_8418</name>
      <uri>https://old.reddit.com/user/Express_Seesaw_8418</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems SOTA LLMS are moving towards MOE architectures. The smartest models in the world &lt;a href="https://lmarena.ai/leaderboard"&gt;seem to be using it&lt;/a&gt;. But why? When you use a MOE model, only a fraction of parameters are actually active. Wouldn't the model be &amp;quot;smarter&amp;quot; if you just use all parameters? Efficiency is awesome, but there are many problems that the smartest models cannot solve (i.e., cancer, a bug in my code, etc.). So, are we moving towards MOE because we discovered some kind of intelligence scaling limit in dense models (for example, a dense 2T LLM could never outperform a well architected MOE 2T LLM) or is it just for efficiency, or both?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express_Seesaw_8418"&gt; /u/Express_Seesaw_8418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2qv7z/help_me_understand_moe_vs_dense/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2qv7z/help_me_understand_moe_vs_dense/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2qv7z/help_me_understand_moe_vs_dense/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T23:33:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2rwhu</id>
    <title>Secure Minions: private collaboration between Ollama and frontier models</title>
    <updated>2025-06-04T00:23:17+00:00</updated>
    <author>
      <name>/u/MediocreBye</name>
      <uri>https://old.reddit.com/user/MediocreBye</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2rwhu/secure_minions_private_collaboration_between/"&gt; &lt;img alt="Secure Minions: private collaboration between Ollama and frontier models" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="Secure Minions: private collaboration between Ollama and frontier models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Extremely interesting developments coming out of Hazy Research. Has anyone tested this yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MediocreBye"&gt; /u/MediocreBye &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/blog/secureminions"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2rwhu/secure_minions_private_collaboration_between/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2rwhu/secure_minions_private_collaboration_between/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T00:23:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2gvar</id>
    <title>New META Paper - How much do language models memorize?</title>
    <updated>2025-06-03T16:47:12+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very interesting paper on dataset size, parameter size, and grokking.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2505.24832"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2gvar/new_meta_paper_how_much_do_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2gvar/new_meta_paper_how_much_do_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T16:47:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1l27g8d</id>
    <title>Google opensources DeepSearch stack</title>
    <updated>2025-06-03T09:25:47+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l27g8d/google_opensources_deepsearch_stack/"&gt; &lt;img alt="Google opensources DeepSearch stack" src="https://external-preview.redd.it/jtUtL7EqwS5bMEk8XfF81tFd6n1MgnQyQL0hQG-jzRk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0633532a1f9e627adaa6246fd5a299a809f2654" title="Google opensources DeepSearch stack" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While it's not evident if this is the exact same stack they use in the Gemini user app, it sure looks very promising! Seems to work with Gemini and Google Search. Maybe this can be adapted for any local model and SearXNG?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l27g8d/google_opensources_deepsearch_stack/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l27g8d/google_opensources_deepsearch_stack/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T09:25:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2vrg2</id>
    <title>Fully offline verbal chat bot</title>
    <updated>2025-06-04T03:41:10+00:00</updated>
    <author>
      <name>/u/NonYa_exe</name>
      <uri>https://old.reddit.com/user/NonYa_exe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2vrg2/fully_offline_verbal_chat_bot/"&gt; &lt;img alt="Fully offline verbal chat bot" src="https://external-preview.redd.it/dWY5OGJ3aWl5dDRmMXZQOk9qxWLlo00dzciqndO7qSY-J3_oYBSBLg5Z6rT9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0facf9ba08505071c2b5d9e46bc075ab0b46dd5" title="Fully offline verbal chat bot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to get some feedback on my project at its current state. The goal is to have the program run in the background so that the LLM is always accessible with just a keybind. Right now I have it displaying a console for debugging, but it is capable of running fully in the background. This is written in Rust, and is set up to run fully offline. I'm using LM Studio to serve the model on an OpenAI compatable API, Piper TTS for the voice, and Whisper.cpp for the transcription.&lt;/p&gt; &lt;p&gt;Current ideas:&lt;br /&gt; - Find a better Piper model&lt;br /&gt; - Allow customization of hotkey via config file&lt;br /&gt; - Add a hotkey to insert the contents of the clipboard to the prompt&lt;br /&gt; - Add the ability to cut off the AI before it finishes&lt;/p&gt; &lt;p&gt;I'm not making the code available yet since at its current state its highly tailored to my specific computer. I will make it open source on GitHub once I fix that.&lt;/p&gt; &lt;p&gt;Please leave suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NonYa_exe"&gt; /u/NonYa_exe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cw4rpviiyt4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2vrg2/fully_offline_verbal_chat_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2vrg2/fully_offline_verbal_chat_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T03:41:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2ynsc</id>
    <title>Tried 10 models, all seem to refuse to write a 10,000 word story. Is there something bad with my prompt? I'm just doing some testing to learn and I can't figure out how to get the LLM to do as I say.</title>
    <updated>2025-06-04T06:36:26+00:00</updated>
    <author>
      <name>/u/StartupTim</name>
      <uri>https://old.reddit.com/user/StartupTim</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2ynsc/tried_10_models_all_seem_to_refuse_to_write_a/"&gt; &lt;img alt="Tried 10 models, all seem to refuse to write a 10,000 word story. Is there something bad with my prompt? I'm just doing some testing to learn and I can't figure out how to get the LLM to do as I say." src="https://external-preview.redd.it/rlOy3w1CoH2JdExOlsJT5MCZp4fqssksLcusxXxosAg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ce5032ae39d55f280b278a700a75157939738ba" title="Tried 10 models, all seem to refuse to write a 10,000 word story. Is there something bad with my prompt? I'm just doing some testing to learn and I can't figure out how to get the LLM to do as I say." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StartupTim"&gt; /u/StartupTim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/uup3xQO.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2ynsc/tried_10_models_all_seem_to_refuse_to_write_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2ynsc/tried_10_models_all_seem_to_refuse_to_write_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T06:36:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2oywk</id>
    <title>What GUI are you using for local LLMs? (AnythingLLM, LM Studio, etc.)</title>
    <updated>2025-06-03T22:09:03+00:00</updated>
    <author>
      <name>/u/Aaron_MLEngineer</name>
      <uri>https://old.reddit.com/user/Aaron_MLEngineer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been trying out AnythingLLM and LM Studio lately to run models like LLaMA and Gemma locally. Curious what others here are using.&lt;/p&gt; &lt;p&gt;What’s been your experience with these or other GUI tools like GPT4All, Oobabooga, PrivateGPT, etc.?&lt;/p&gt; &lt;p&gt;What do you like, what’s missing, and what would you recommend for someone looking to do local inference with documents or RAG?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaron_MLEngineer"&gt; /u/Aaron_MLEngineer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2oywk/what_gui_are_you_using_for_local_llms_anythingllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2oywk/what_gui_are_you_using_for_local_llms_anythingllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2oywk/what_gui_are_you_using_for_local_llms_anythingllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T22:09:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2xpf5</id>
    <title>nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1 · Hugging Face</title>
    <updated>2025-06-04T05:34:44+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2xpf5/nvidiallama31nemotronnanovl8bv1_hugging_face/"&gt; &lt;img alt="nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1 · Hugging Face" src="https://external-preview.redd.it/3iMGSgahr2EGqEFlPNVdelZ_zKKfIXKVuELuwxGt7R4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f881ace7e1b4f8039722557754569dbed2ca23ac" title="nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2xpf5/nvidiallama31nemotronnanovl8bv1_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2xpf5/nvidiallama31nemotronnanovl8bv1_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T05:34:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2wvf3</id>
    <title>Python Pandas Ditches NumPy for Speedier PyArrow</title>
    <updated>2025-06-04T04:44:44+00:00</updated>
    <author>
      <name>/u/Sporeboss</name>
      <uri>https://old.reddit.com/user/Sporeboss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2wvf3/python_pandas_ditches_numpy_for_speedier_pyarrow/"&gt; &lt;img alt="Python Pandas Ditches NumPy for Speedier PyArrow" src="https://external-preview.redd.it/YNruBwDE4glgMZxB7Rz7Dn9TVxQIuQU5bX9UAhvGW9I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0372e63d733e050c30987cb1c1b8b2b63ce28fc" title="Python Pandas Ditches NumPy for Speedier PyArrow" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sporeboss"&gt; /u/Sporeboss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://thenewstack.io/python-pandas-ditches-numpy-for-speedier-pyarrow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2wvf3/python_pandas_ditches_numpy_for_speedier_pyarrow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2wvf3/python_pandas_ditches_numpy_for_speedier_pyarrow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T04:44:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1l318di</id>
    <title>Shisa V2 405B: The strongest model ever built in Japan! (JA/EN)</title>
    <updated>2025-06-04T09:32:34+00:00</updated>
    <author>
      <name>/u/randomfoo2</name>
      <uri>https://old.reddit.com/user/randomfoo2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l318di/shisa_v2_405b_the_strongest_model_ever_built_in/"&gt; &lt;img alt="Shisa V2 405B: The strongest model ever built in Japan! (JA/EN)" src="https://b.thumbs.redditmedia.com/72uFsuI12iQ2cSozyTM-SHrwJy10OoGPWLvGrCoRLbg.jpg" title="Shisa V2 405B: The strongest model ever built in Japan! (JA/EN)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, so we've released the latest member of our &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jz2lll/shisa_v2_a_family_of_new_jaen_bilingual_models/"&gt;Shisa V2&lt;/a&gt; family of open bilingual (Japanes/English) models: &lt;a href="https://shisa.ai/posts/shisa-v2-405b/"&gt;Shisa V2 405B&lt;/a&gt;!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama 3.1 405B Fine Tune, inherits the Llama 3.1 license&lt;/li&gt; &lt;li&gt;Not just our JA mix but also additional KO + ZH-TW to augment 405B's native multilingual&lt;/li&gt; &lt;li&gt;Beats GPT-4 &amp;amp; GPT-4 Turbo in JA/EN, matches latest GPT-4o and DeepSeek-V3 in JA MT-Bench (it's not a reasoning or code model, but 日本語上手!)&lt;/li&gt; &lt;li&gt;Based on our evals, it's is w/o a doubt the strongest model to ever be released from Japan, beating out the efforts of bigco's etc. Tiny teams can do great things leveraging open models!&lt;/li&gt; &lt;li&gt;Quants and end-point available for testing&lt;/li&gt; &lt;li&gt;Super cute doggos:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3suc49zzqv4f1.jpg?width=900&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=732f3e703e207d4d9a4b1e750e3b793f061a811f"&gt;Shisa V2 405B 日本語上手！&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For the &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; crowd:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Of course full model weights at &lt;a href="https://huggingface.co/shisa-ai/shisa-v2-llama3.1-405b"&gt;shisa-ai/shisa-v2-llama-3.1-405b&lt;/a&gt; but also a range of GGUFs in a repo as well: &lt;a href="https://huggingface.co/shisa-ai/shisa-v2-llama3.1-405b-GGUF"&gt;shisa-ai/shisa-v2-llama3.1-405b-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;These GGUFs are all (except the Q8_0) imatrixed w/ a calibration set based on our (Apache 2.0, also available for download) core Shisa V2 SFT dataset. They range from 100GB for the IQ2_XXS to 402GB for the Q8_0. Thanks to ubergarm for the pointers for what the gguf quanting landscape looks like in 2025!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out our initially linked blog post for all the deets + a full set of overview slides in JA and EN versions. Explains how we did our testing, training, dataset creation, and all kinds of little fun tidbits like:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vp7we685rv4f1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ebbb9ad61d82ad55b9bceb9db3493e4bc038d80"&gt;Top Notch Japanese&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xatqzpz7rv4f1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b7a676b0479b34e2bca1af9d5d05d37b8cf32e7"&gt;When your model is significantly better than GPT 4 it just gives you 10s across the board 😂&lt;/a&gt;&lt;/p&gt; &lt;p&gt;While I know these models are big and maybe not directly relevant to people here, we've now tested our dataset on a huge range of base models from 7B to 405B and can conclude it can basically make any model mo-betta' at Japanese (without negatively impacting English or other capabilities!).&lt;/p&gt; &lt;p&gt;This whole process has been basically my whole year, so happy to finally get it out there and of course, answer any questions anyone might have.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomfoo2"&gt; /u/randomfoo2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l318di/shisa_v2_405b_the_strongest_model_ever_built_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l318di/shisa_v2_405b_the_strongest_model_ever_built_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l318di/shisa_v2_405b_the_strongest_model_ever_built_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T09:32:34+00:00</published>
  </entry>
</feed>
