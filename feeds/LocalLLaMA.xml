<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-16T14:22:32+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jbrwqf</id>
    <title>Deep Research Tools: Am I the only one feeling...underwhelmed? (OpenAI, Google, Open Source)</title>
    <updated>2025-03-15T10:07:54+00:00</updated>
    <author>
      <name>/u/mimirium_</name>
      <uri>https://old.reddit.com/user/mimirium_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been diving headfirst into these &amp;quot;Deep Research&amp;quot; AI tools lately - OpenAI's thing, Google's Gemini version, Perplexity, even some of the open-source ones on GitHub. You know, the ones that promise to do all the heavy lifting of in-depth research for you. I was so hyped!&lt;/p&gt; &lt;p&gt;I mean, the idea is amazing, right? Finally having an AI assistant that can handle literature reviews, synthesize data, and write full reports? Sign me up! But after using them for a while, I keep feeling like something's missing.&lt;/p&gt; &lt;p&gt;Like, the biggest issue for me is accuracy. I’ve had to fact-check so many things, and way too often it's just plain wrong. Or even worse, it makes up sources that don't exist! It's also pretty surface-level. It can pull information, sure, but it often misses the whole context. It's rare I find truly new insights from it. Also, it just grabs stuff from the web without checking if a source is a blog or a peer reviewed journal. And once it starts down a wrong path, its so hard to correct the tool.&lt;/p&gt; &lt;p&gt;And don’t even get me started on the limitations with data access - I get it, it's early days. But being able to pull private information would be so useful!&lt;/p&gt; &lt;p&gt;I can see the potential here, I really do. Uploading files, asking tough questions, getting a structured report… It’s a big step, but I was kinda hoping for a breakthrough in saving time. I am just left slightly unsatisfied and wishing for something a little bit better.&lt;/p&gt; &lt;p&gt;So, am I alone here? What have your experiences been like? Has anyone actually found one of these tools that nails it, or are we all just beta-testing expensive (and sometimes inaccurate) search engines?&lt;/p&gt; &lt;p&gt;TL;DR: These &amp;quot;Deep Research&amp;quot; AI tools are cool, but they still have accuracy issues, lack context, and need more data access. Feeling a bit underwhelmed tbh.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mimirium_"&gt; /u/mimirium_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbrwqf/deep_research_tools_am_i_the_only_one/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T10:07:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbyolh</id>
    <title>Diffusion Language Models in 2 minutes</title>
    <updated>2025-03-15T16:17:22+00:00</updated>
    <author>
      <name>/u/CasulaScience</name>
      <uri>https://old.reddit.com/user/CasulaScience</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbyolh/diffusion_language_models_in_2_minutes/"&gt; &lt;img alt="Diffusion Language Models in 2 minutes" src="https://external-preview.redd.it/l0zCB3Hsl6Zyg_xqk4dL1W5McbBOHzb917AfOF8NKWc.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8423b4d3358e31cc2c420bbdb7ab3ad555fb7a3" title="Diffusion Language Models in 2 minutes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CasulaScience"&gt; /u/CasulaScience &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/_6jekTwBxow?si=yvEamKu9ommm8v7T"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbyolh/diffusion_language_models_in_2_minutes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbyolh/diffusion_language_models_in_2_minutes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T16:17:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcgqyd</id>
    <title>I want to run machine translation on my laptop. What should I do?</title>
    <updated>2025-03-16T08:10:38+00:00</updated>
    <author>
      <name>/u/salic428</name>
      <uri>https://old.reddit.com/user/salic428</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello community! Recently I need to translate some Chinese and Japanese news articles to share with my firends.&lt;/p&gt; &lt;p&gt;I have downloaded ollama and tried some models. My laptop has 16GB RAM and 8GM VRAM, it can run qwen-2.5 7b smoothly, and DeepSeek-R1-Distill-Qwen-14B with a proper quant.&lt;/p&gt; &lt;p&gt;But I feel that using a chat interface is kinda inefficient. I want to feed a txt or docx file to it and expect it to output to a file–is it possible?&lt;/p&gt; &lt;p&gt;Also, will it be better if I use an &amp;quot;instruct&amp;quot; model? I heard they are better at giving a structured output or something.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/salic428"&gt; /u/salic428 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcgqyd/i_want_to_run_machine_translation_on_my_laptop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcgqyd/i_want_to_run_machine_translation_on_my_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcgqyd/i_want_to_run_machine_translation_on_my_laptop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T08:10:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbufek</id>
    <title>Local LLM on cheap machine, a one page summary</title>
    <updated>2025-03-15T12:51:41+00:00</updated>
    <author>
      <name>/u/gitcommitshow</name>
      <uri>https://old.reddit.com/user/gitcommitshow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbufek/local_llm_on_cheap_machine_a_one_page_summary/"&gt; &lt;img alt="Local LLM on cheap machine, a one page summary" src="https://preview.redd.it/ft673vxiouoe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc8946332f8bbd79475fa1a4c79d32c90025ac4f" title="Local LLM on cheap machine, a one page summary" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gitcommitshow"&gt; /u/gitcommitshow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ft673vxiouoe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbufek/local_llm_on_cheap_machine_a_one_page_summary/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbufek/local_llm_on_cheap_machine_a_one_page_summary/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T12:51:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbn4a4</id>
    <title>DeepSeek's owner asked R&amp;D staff to hand in passports so they can't travel abroad. How does this make any sense considering Deepseek open sources everything?</title>
    <updated>2025-03-15T04:24:47+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbn4a4/deepseeks_owner_asked_rd_staff_to_hand_in/"&gt; &lt;img alt="DeepSeek's owner asked R&amp;amp;D staff to hand in passports so they can't travel abroad. How does this make any sense considering Deepseek open sources everything?" src="https://external-preview.redd.it/4hsJXx_AcJxGPq4lJQrABcvHj2M_s3N2kv5MhyZHOZM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=337e519de49d47a13546c9c6c2c323282203aee5" title="DeepSeek's owner asked R&amp;amp;D staff to hand in passports so they can't travel abroad. How does this make any sense considering Deepseek open sources everything?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/amir/status/1900583042659541477"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbn4a4/deepseeks_owner_asked_rd_staff_to_hand_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbn4a4/deepseeks_owner_asked_rd_staff_to_hand_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T04:24:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jc1nw1</id>
    <title>Actual Electricity Consumption and Cost to Run Local LLMs. From Gemma3 to QwQ.</title>
    <updated>2025-03-15T18:28:09+00:00</updated>
    <author>
      <name>/u/QuantuisBenignus</name>
      <uri>https://old.reddit.com/user/QuantuisBenignus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tokens/WattHour and Tokens/US cent calculated for 17 local LLMs, including the new Gemma3 models. Wall plug power measured for each run under similar conditions and prompt.&lt;/p&gt; &lt;p&gt;Table, graph and formulas for estimate here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/QuantiusBenignus/Zshelf/discussions/2"&gt;https://github.com/QuantiusBenignus/Zshelf/discussions/2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Average, consumer-grade hardware and local LLMs quantized to Q5 on average.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuantuisBenignus"&gt; /u/QuantuisBenignus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc1nw1/actual_electricity_consumption_and_cost_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc1nw1/actual_electricity_consumption_and_cost_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jc1nw1/actual_electricity_consumption_and_cost_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T18:28:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jc5iqi</id>
    <title>AI Scientists By Sakana AI passed ICLR review bar!!!</title>
    <updated>2025-03-15T21:21:33+00:00</updated>
    <author>
      <name>/u/mansurul11</name>
      <uri>https://old.reddit.com/user/mansurul11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An amazing experiment was conducted by Sakana.ai. They collaborated with ICLR workshop organizers to submit three original research papers, all originated and written entirely by this AI scientist. The review process was double-blind, but reviewers were informed that three out of the 43 submitted papers were original research from an AI scientist. 🤯&lt;/p&gt; &lt;p&gt;TLDR from the blog post: The AI Scientist-v2, after being given a broad topic to conduct research on, generated a paper titled “Compositional Regularization: Unexpected Obstacles in Enhancing Neural Network Generalization”. This paper reported a negative result that The AI Scientist encountered while trying to innovate on novel regularization methods for training neural networks that can improve their compositional generalization. This manuscript received an average reviewer score of 6.33 at the ICLR workshop, placing it above the average acceptance threshold.&lt;/p&gt; &lt;p&gt;&lt;a href="https://sakana.ai/ai-scientist-first-publication/"&gt;https://sakana.ai/ai-scientist-first-publication/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mansurul11"&gt; /u/mansurul11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc5iqi/ai_scientists_by_sakana_ai_passed_iclr_review_bar/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc5iqi/ai_scientists_by_sakana_ai_passed_iclr_review_bar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jc5iqi/ai_scientists_by_sakana_ai_passed_iclr_review_bar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T21:21:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbpesk</id>
    <title>Block Diffusion</title>
    <updated>2025-03-15T06:58:36+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbpesk/block_diffusion/"&gt; &lt;img alt="Block Diffusion" src="https://external-preview.redd.it/ajRoczdkcGd4c29lMVSExINF4ZLZPdurGgKB_nt4a17rDQ79hHF6tCkUtZhB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0a9c1a1e32d732770b3bfd59c242574fc96a2e3" title="Block Diffusion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ze05rmrgxsoe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbpesk/block_diffusion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbpesk/block_diffusion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T06:58:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcgonz</id>
    <title>Has anyone tried &gt;70B LLMs on M3 Ultra?</title>
    <updated>2025-03-16T08:05:44+00:00</updated>
    <author>
      <name>/u/TechNerd10191</name>
      <uri>https://old.reddit.com/user/TechNerd10191</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since the Mac Studio is the only machine with 0.5TB of memory at decent memory bandwidth under $15k, I'd like to know what's the PP and token generation speeds for &lt;strong&gt;dense&lt;/strong&gt; LLMs, such Llama 3.1 70B and 3.1 405B.&lt;/p&gt; &lt;p&gt;Has anyone acquired the new Macs and tried them? Or, what speculations you have if you used M2 Ultra/M3 Max/M4 Max?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechNerd10191"&gt; /u/TechNerd10191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcgonz/has_anyone_tried_70b_llms_on_m3_ultra/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcgonz/has_anyone_tried_70b_llms_on_m3_ultra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcgonz/has_anyone_tried_70b_llms_on_m3_ultra/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T08:05:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbyg29</id>
    <title>GPT-Sovits V3 TTS (407M) Release - 0-Shot Voice Cloning , Multi Language</title>
    <updated>2025-03-15T16:06:43+00:00</updated>
    <author>
      <name>/u/MaruluVR</name>
      <uri>https://old.reddit.com/user/MaruluVR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/RVC-Boss/GPT-SoVITS/releases/tag/20250228v3"&gt;https://github.com/RVC-Boss/GPT-SoVITS/releases/tag/20250228v3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Version 3 of GPT Sovits released two weeks ago and I havent really seen any discussion about it outside of China. &lt;/p&gt; &lt;p&gt;The new version increased the parameter count from 167m to 407m, also the voice cloning capability has improved a lot over the previous versions. Both 0 shot (uses a single audio sample shorter then 10 seconds) and trained voices are now a lot closer to the original and it is capable of staying in the emotion of the sample more consistently. &lt;/p&gt; &lt;p&gt;GPT Sovits supports English, Chinese, Japanese, Korean and Cantonese. From my personal testing it currently is the best option for 0 shot voice cloning in Japanese.&lt;/p&gt; &lt;p&gt;Here is a link to the machine translated changelog: &lt;a href="https://github-com.translate.goog/RVC-Boss/GPT-SoVITS/wiki/GPT%E2%80%90SoVITS%E2%80%90v3%E2%80%90features-(%E6%96%B0%E7%89%B9%E6%80%A7"&gt;https://github-com.translate.goog/RVC-Boss/GPT-SoVITS/wiki/GPT‐SoVITS‐v3‐features-(新特性)?_x_tr_sl=auto&amp;amp;_x_tr_tl=en&amp;amp;_x_tr_hl=ja&amp;amp;_x_tr_pto=wapp&lt;/a&gt;?_x_tr_sl=auto&amp;amp;_x_tr_tl=en&amp;amp;_x_tr_hl=ja&amp;amp;_x_tr_pto=wapp)&lt;/p&gt; &lt;p&gt;Note: the audio examples on their Github page are still from V2 not V3. Also once you start the Gradio interface you need to select v3 from the dropdown menu as it defaults to v2 still.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaruluVR"&gt; /u/MaruluVR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbyg29/gptsovits_v3_tts_407m_release_0shot_voice_cloning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbyg29/gptsovits_v3_tts_407m_release_0shot_voice_cloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbyg29/gptsovits_v3_tts_407m_release_0shot_voice_cloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T16:06:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcef1d</id>
    <title>How I used entropy and varentropy to detect and remediate hallucinations in LLMs</title>
    <updated>2025-03-16T05:18:48+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The following blog is a high-level introduction to a series of research work we are doing with fast and efficient language models for routing and function calling scenarios. For experts this might be too high-level, but for people learning more about LLMs this might be a decent introduction to some machine learning concepts. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.archgw.com/blogs/detecting-hallucinations-in-llm-function-calling-with-entropy-and-varentropy"&gt;https://www.archgw.com/blogs/detecting-hallucinations-in-llm-function-calling-with-entropy-and-varentropy&lt;/a&gt; (part 1). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcef1d/how_i_used_entropy_and_varentropy_to_detect_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcef1d/how_i_used_entropy_and_varentropy_to_detect_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcef1d/how_i_used_entropy_and_varentropy_to_detect_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T05:18:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jc3fkd</id>
    <title>I hope uncensored gemma3b come soon enough... the model is unbearable boring as it is know.</title>
    <updated>2025-03-15T19:47:15+00:00</updated>
    <author>
      <name>/u/pumukidelfuturo</name>
      <uri>https://old.reddit.com/user/pumukidelfuturo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I honestly had more fun with Darkest Muse or even the Gemma2 9b simpo version (which is my fav model).&lt;/p&gt; &lt;p&gt;I'm not even talking about NSFW stuff, i'm just chatting with it and its visions about everything are just lame, safe, boring, patronising, preachy, holier-than-you attitude and such... the lack of personality it just bores me too much. It's lame vanilla corpo mumbo jumbo style all over the place. If i wanted that i'd use Llama 3 instead.&lt;/p&gt; &lt;p&gt;I hope trainers can fix this and make this fun somewhat. It's gonna be a hard job. I'm just experiencing brainrot of how dull it is. It's dumb as a rock.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pumukidelfuturo"&gt; /u/pumukidelfuturo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc3fkd/i_hope_uncensored_gemma3b_come_soon_enough_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc3fkd/i_hope_uncensored_gemma3b_come_soon_enough_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jc3fkd/i_hope_uncensored_gemma3b_come_soon_enough_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T19:47:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcievp</id>
    <title>Estimates of next gen releases</title>
    <updated>2025-03-16T10:16:52+00:00</updated>
    <author>
      <name>/u/TechnicalGeologist99</name>
      <uri>https://old.reddit.com/user/TechnicalGeologist99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We had Gemma3 which didn't really blow my socks off...&lt;/p&gt; &lt;p&gt;Wondering what other next gen open models are up and coming? What are you hoping they will feature? When do you think we will see them? &lt;/p&gt; &lt;p&gt;Personally im hoping for llama4-8B (and maybe a ~14B version) by the end of this quarter. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechnicalGeologist99"&gt; /u/TechnicalGeologist99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcievp/estimates_of_next_gen_releases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcievp/estimates_of_next_gen_releases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcievp/estimates_of_next_gen_releases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T10:16:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbwk65</id>
    <title>Made a ManusAI alternative that run locally</title>
    <updated>2025-03-15T14:40:35+00:00</updated>
    <author>
      <name>/u/fawendeshuo</name>
      <uri>https://old.reddit.com/user/fawendeshuo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I have been working with a friend on a fully local Manus that can run on your computer, it started as a fun side project but it's slowly turning into something useful.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/Fosowl/agenticSeek"&gt;https://github.com/Fosowl/agenticSeek&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We already have a lot of features :: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Web agent:&lt;/strong&gt; Autonomous web search and web browsing with selenium&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code agent:&lt;/strong&gt; Semi-autonomous coding ability, automatic trial and retry&lt;/li&gt; &lt;li&gt;&lt;strong&gt;File agent:&lt;/strong&gt; Bash execution and file system interaction&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Routing system:&lt;/strong&gt; The best agent is selected given the user prompt&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Session management&lt;/strong&gt; : save and load previous conversation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;API tool:&lt;/strong&gt; We will integrate many API tool, for now we only have webi and flight search.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory system&lt;/strong&gt; : Individual agent memory and compression. Quite experimental but we use a summarization model to compress the memory over time. it is disabled by default for now.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text to speech &amp;amp; Speech to text&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Coming features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tasks planning&lt;/strong&gt; (development started) : Breaks down tasks and spins up the right agents&lt;/li&gt; &lt;li&gt;&lt;strong&gt;User Preferences Memory&lt;/strong&gt; (in development)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OCR System&lt;/strong&gt; – Enables the agent to see what you are seing&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAG Agent&lt;/strong&gt; – Chat with personal documents&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How does it differ from openManus ?&lt;/p&gt; &lt;p&gt;We want to run everything locally and avoid the use of fancy frameworks, build as much from scratch as possible.&lt;/p&gt; &lt;p&gt;We still have a long way to go and probably will never match openManus in term of capabilities but it is more accessible, it show how easy it is to created a hyped product like ManusAI.&lt;/p&gt; &lt;p&gt;We are a very small team of 2 from France and Taiwan. We are seeking feedback, love and and contributors!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fawendeshuo"&gt; /u/fawendeshuo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbwk65/made_a_manusai_alternative_that_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbwk65/made_a_manusai_alternative_that_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbwk65/made_a_manusai_alternative_that_run_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-15T14:40:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcm5p2</id>
    <title>OCR + LLM for Invoice Extraction</title>
    <updated>2025-03-16T14:04:40+00:00</updated>
    <author>
      <name>/u/JumpyHouse</name>
      <uri>https://old.reddit.com/user/JumpyHouse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m starting to get a bit frustrated. I’m trying to develop a mobile application for an academic project involving invoice information extraction. Since this is a non-commercial project, I’m not allowed to use paid solutions like Google Vision or Azure AI Vision. So far, I’ve studied several possibilities, with the best being SuryaOCR/Marker for data extraction and Qwen 2.5 14B for data interpretation, along with some minor validation through RegEx.&lt;/p&gt; &lt;p&gt;I’m also limited in terms of options because I have an RX 6700 XT with 12GB of VRAM and can’t run Hugging Face models due to the lack of support for my GPU. I’ve also tried a few Vision models like Llama 3.2 Vision and various OCR solutions like PaddleOCR , PyTesseract and EasyOCR and they all came short due to the lack of layout detection.&lt;/p&gt; &lt;p&gt;I wanted to ask if any of you have faced a similar situation and if you have any ideas or tips because I’m running out of options for data extraction. The invoices are predominantly Portuguese, so many OCR models end up lacking support for the layout detection.&lt;/p&gt; &lt;p&gt;Thank you in advance.🫡&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JumpyHouse"&gt; /u/JumpyHouse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcm5p2/ocr_llm_for_invoice_extraction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcm5p2/ocr_llm_for_invoice_extraction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcm5p2/ocr_llm_for_invoice_extraction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T14:04:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jciyso</id>
    <title>What’s your secret sauce in creating high quality Q&amp;A datasets?</title>
    <updated>2025-03-16T10:57:20+00:00</updated>
    <author>
      <name>/u/Secure_Archer_1529</name>
      <uri>https://old.reddit.com/user/Secure_Archer_1529</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can you fine tune a local model (13b and up) on domain specific knowledge and processes to perform on pair with the richness and depth of gpt 4o/4.5?&lt;/p&gt; &lt;p&gt;Do you use SOTA paid models to create your Q&amp;amp;A datasets for fine tuning models?&lt;/p&gt; &lt;p&gt;Maybe use cloud gpus for bigger models to generate Q&amp;amp;A dataset?&lt;/p&gt; &lt;p&gt;Any specific secret sauce you use in getting that depth and richness you get from a SOTA paid model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Archer_1529"&gt; /u/Secure_Archer_1529 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jciyso/whats_your_secret_sauce_in_creating_high_quality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jciyso/whats_your_secret_sauce_in_creating_high_quality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jciyso/whats_your_secret_sauce_in_creating_high_quality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T10:57:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jch5go</id>
    <title>Unvibe: Generate code that pass Unit-Tests with Qwen-coder 7B</title>
    <updated>2025-03-16T08:41:53+00:00</updated>
    <author>
      <name>/u/inkompatible</name>
      <uri>https://old.reddit.com/user/inkompatible</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jch5go/unvibe_generate_code_that_pass_unittests_with/"&gt; &lt;img alt="Unvibe: Generate code that pass Unit-Tests with Qwen-coder 7B" src="https://external-preview.redd.it/AoihjkMZZrKFb6oZh0_uoP159L4b86Wv3R2wyjBYumc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a409d905bf15acbd562decd0487e21014e17c40f" title="Unvibe: Generate code that pass Unit-Tests with Qwen-coder 7B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inkompatible"&gt; /u/inkompatible &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://claudio.uk/posts/unvibe.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jch5go/unvibe_generate_code_that_pass_unittests_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jch5go/unvibe_generate_code_that_pass_unittests_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T08:41:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcdsat</id>
    <title>A tip to make QwQ less verbose</title>
    <updated>2025-03-16T04:37:40+00:00</updated>
    <author>
      <name>/u/Aggressive-Stop-9091</name>
      <uri>https://old.reddit.com/user/Aggressive-Stop-9091</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my experience, QwQ tends to overthink because it's fine-tuned to interpret the writer's intentions. One effective way to minimize this is by providing examples. QwQ is an excellent few-shot learner that doesnt merely copy the examples, but also and when given a few well-crafted examples, it can generate a more articulate prompt than I initially wrote (which I then included in subsequent generations). Yes, I know this is prompt engineering 101, but what I find interesting about QwQ is that, unlike most local models I've tried, it doesn't get fixated on wording or style. Instead, it focuses on understanding the 'bigger picture' in the examples, like it had some sort 'meta learning'. For instance, I was working on condensing a research paper into a highly engaging and conversational format. The model when provided examples was able to outline what I wanted on its own, based on my instruction and the examples:&lt;/p&gt; &lt;p&gt;Hook: Why can't you stop scrolling TikTok?&lt;/p&gt; &lt;p&gt;Problem: Personalized content triggers brain regions linked to attention and reward.&lt;/p&gt; &lt;p&gt;Mechanism: DMN activation, VTA activity, reduced self-control regions coupling.&lt;/p&gt; &lt;p&gt;Outcome: Compulsive use, especially in those with low self-control.&lt;/p&gt; &lt;p&gt;Significance: Algorithm exploits neural pathways, need for understanding tech addiction.&lt;/p&gt; &lt;p&gt;Needless to say, it doesn't always work perfectly, but in my experience, it significantly improves the output. (The engine I use is ExLlama, and I follow the recommended settings for the model.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aggressive-Stop-9091"&gt; /u/Aggressive-Stop-9091 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcdsat/a_tip_to_make_qwq_less_verbose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcdsat/a_tip_to_make_qwq_less_verbose/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcdsat/a_tip_to_make_qwq_less_verbose/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T04:37:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jc9meu</id>
    <title>Who's still running ancient models?</title>
    <updated>2025-03-16T00:44:12+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had to take a pause from my experiments today, gemma3, mistralsmall, phi4, qwq, qwen, etc and marvel at how good they are for their size. A year ago most of us thought that we needed 70B to kick ass. 14-32B is punching super hard. I'm deleting my Q2/Q3 llama405B, and deepseek dyanmic quants.&lt;/p&gt; &lt;p&gt;I'm going to re-download guanaco, dolphin-llama2, vicuna, wizardLM, nous-hermes-llama2, etc&lt;br /&gt; For old times sake. It's amazing how far we have come and how fast. Some of these are not even 2 years old! Just a year plus! I'm going to keep some ancient model and run them so I can remember and don't forget and to also have more appreciation for what we have.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc9meu/whos_still_running_ancient_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc9meu/whos_still_running_ancient_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jc9meu/whos_still_running_ancient_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T00:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcchtq</id>
    <title>Baidu releases X1, a (closed?) model that matches R1 and ERNIE 4.5, that matches GPT 4.5</title>
    <updated>2025-03-16T03:20:25+00:00</updated>
    <author>
      <name>/u/ortegaalfredo</name>
      <uri>https://old.reddit.com/user/ortegaalfredo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Baidu_Inc/status/1901094083508220035"&gt;https://x.com/Baidu_Inc/status/1901094083508220035&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ortegaalfredo"&gt; /u/ortegaalfredo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcchtq/baidu_releases_x1_a_closed_model_that_matches_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcchtq/baidu_releases_x1_a_closed_model_that_matches_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcchtq/baidu_releases_x1_a_closed_model_that_matches_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T03:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jchrro</id>
    <title>Top 5 Model Recommendations for Newbie with 24GB</title>
    <updated>2025-03-16T09:29:41+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It’s only March, but there’s already been incredible progress in open-weight LLMs this year.&lt;/p&gt; &lt;p&gt;Here’s my top 5 recommendation for a beginner with 24GB to try out. The list is from smallest to biggest.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Phi-4 14B for speed&lt;/li&gt; &lt;li&gt;Mistral Small 24B for RAG (only 32k context but best compromise length/quality IMHO)&lt;/li&gt; &lt;li&gt;Gemma 3 27B for general use&lt;/li&gt; &lt;li&gt;Qwen2.5 Coder 32B for coding (older than rest but still best)&lt;/li&gt; &lt;li&gt;QWQ 32B for reasoning (better than distilled deepseek-r1-qwen-32b)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hoping Llama 4 will earn a spot soon!&lt;/p&gt; &lt;p&gt;What's your recommendation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jchrro/top_5_model_recommendations_for_newbie_with_24gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jchrro/top_5_model_recommendations_for_newbie_with_24gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jchrro/top_5_model_recommendations_for_newbie_with_24gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T09:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcggwb</id>
    <title>Qwen2 72b VL is actually really impressive. It's not perfect, but for a local model I'm certainly impressed (more info in comments)</title>
    <updated>2025-03-16T07:48:53+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcggwb/qwen2_72b_vl_is_actually_really_impressive_its/"&gt; &lt;img alt="Qwen2 72b VL is actually really impressive. It's not perfect, but for a local model I'm certainly impressed (more info in comments)" src="https://preview.redd.it/1t90zqok80pe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e53739cdf1ff5da3fd4597e535c09945ad40c21" title="Qwen2 72b VL is actually really impressive. It's not perfect, but for a local model I'm certainly impressed (more info in comments)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1t90zqok80pe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcggwb/qwen2_72b_vl_is_actually_really_impressive_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcggwb/qwen2_72b_vl_is_actually_really_impressive_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T07:48:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcjrp2</id>
    <title>MetaStone-L1 ---The lightweight reasoning model launched by Yuanshi Zhisuan</title>
    <updated>2025-03-16T11:51:44+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcjrp2/metastonel1_the_lightweight_reasoning_model/"&gt; &lt;img alt="MetaStone-L1 ---The lightweight reasoning model launched by Yuanshi Zhisuan" src="https://external-preview.redd.it/9esdwOtFZ9-xPU4Z6uQ-hTei0HrHXFK8YzAB8QTzGNo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b862f612fd291a426e5b068d932bb8c71a97d4c2" title="MetaStone-L1 ---The lightweight reasoning model launched by Yuanshi Zhisuan" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MetaStone-L1 is the lite reasoning model of the MetaStone series, which aims to enhance the performance in hard downstream tasks.&lt;/p&gt; &lt;p&gt;On core reasoning benchmarks including mathematics and code, MetaStone-L1-7B achieved SOTA results in the parallel-level models, and it also achieved the comparable results as the API models such as Claude-3.5-Sonnet-1022 and GPT4o-0513.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/21s0h0i8i1pe1.png?width=2626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=83367c2fdfa32018cc402076222f7d2c2060c41d"&gt;https://preview.redd.it/21s0h0i8i1pe1.png?width=2626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=83367c2fdfa32018cc402076222f7d2c2060c41d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This repo contains the MetaStone-L1-7B model, which is trained based on DeepSeek-R1-Distill-Qwen-7B by GRPO&lt;/p&gt; &lt;p&gt;Optimization tips for specific tasks: For math problems, you can add a hint like &amp;quot;Please reason step by step and put your final answer in \\boxed{}.&amp;quot; For programming problems, add specific formatting requirements to further improve the reasoning effect of the model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/MetaStoneTec/MetaStone-L1-7B"&gt;https://huggingface.co/MetaStoneTec/MetaStone-L1-7B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcjrp2/metastonel1_the_lightweight_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcjrp2/metastonel1_the_lightweight_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcjrp2/metastonel1_the_lightweight_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T11:51:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcbt5l</id>
    <title>These guys never rest!</title>
    <updated>2025-03-16T02:41:35+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcbt5l/these_guys_never_rest/"&gt; &lt;img alt="These guys never rest!" src="https://preview.redd.it/4hmgoyhlsyoe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=455f74ad35ad822af5cb2fe29f909a6835248ce7" title="These guys never rest!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4hmgoyhlsyoe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcbt5l/these_guys_never_rest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcbt5l/these_guys_never_rest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T02:41:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jch36h</id>
    <title>Got new mac studio. What model can I run?</title>
    <updated>2025-03-16T08:36:53+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jch36h/got_new_mac_studio_what_model_can_i_run/"&gt; &lt;img alt="Got new mac studio. What model can I run?" src="https://preview.redd.it/l2st9obzj0pe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2fc3caa2f1e7db99de52ec789092d5d26966c369" title="Got new mac studio. What model can I run?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l2st9obzj0pe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jch36h/got_new_mac_studio_what_model_can_i_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jch36h/got_new_mac_studio_what_model_can_i_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T08:36:53+00:00</published>
  </entry>
</feed>
