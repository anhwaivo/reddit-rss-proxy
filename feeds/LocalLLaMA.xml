<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-19T12:28:02+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mtct4y</id>
    <title>Elon didn't deliver on this announcement. It's already Monday.</title>
    <updated>2025-08-18T04:59:00+00:00</updated>
    <author>
      <name>/u/Outside-Iron-8242</name>
      <uri>https://old.reddit.com/user/Outside-Iron-8242</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtct4y/elon_didnt_deliver_on_this_announcement_its/"&gt; &lt;img alt="Elon didn't deliver on this announcement. It's already Monday." src="https://preview.redd.it/rt8xgjaampjf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2b6da79d84d1e52439441cafb251d7e1dc508f7" title="Elon didn't deliver on this announcement. It's already Monday." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outside-Iron-8242"&gt; /u/Outside-Iron-8242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rt8xgjaampjf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtct4y/elon_didnt_deliver_on_this_announcement_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtct4y/elon_didnt_deliver_on_this_announcement_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T04:59:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mucj1p</id>
    <title>Which models are suitable for websearch?</title>
    <updated>2025-08-19T08:18:57+00:00</updated>
    <author>
      <name>/u/runsleeprepeat</name>
      <uri>https://old.reddit.com/user/runsleeprepeat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using LibreChat, together with a local only Seaxng (search), firecrawl (scrape) and jina (reranker). &lt;/p&gt; &lt;p&gt;I see that search, scraping, and reranking is active, but my current model ( qwen3:30b with 16k context window ) gets the data, but the results are missing my initial questions. &lt;/p&gt; &lt;p&gt;To ensure that the model is not the issue in this loop, which models and context windows are you successfully using together Librechat ( Websearch, Scraper, Reranker) Concept? &lt;/p&gt; &lt;p&gt;(On a side note: OpenWebUI somehow makes a better result. Also, Perplexica gets it done, but the upper combination should work as well, and I am wondering where it got derailed. )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/runsleeprepeat"&gt; /u/runsleeprepeat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mucj1p/which_models_are_suitable_for_websearch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mucj1p/which_models_are_suitable_for_websearch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mucj1p/which_models_are_suitable_for_websearch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T08:18:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtqz3u</id>
    <title>Test: can Qwen 2.5 Omni actually hear guitar chords?</title>
    <updated>2025-08-18T16:30:44+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtqz3u/test_can_qwen_25_omni_actually_hear_guitar_chords/"&gt; &lt;img alt="Test: can Qwen 2.5 Omni actually hear guitar chords?" src="https://external-preview.redd.it/ZHl5dWUwbXl6c2pmMYMQwk04cVObnLQ8K0JceAGBOHlOc1BqIEw787XqJHGO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f00d3d8d78e6d2d5c31925df7d0e8a596f74c0a" title="Test: can Qwen 2.5 Omni actually hear guitar chords?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tried Qwen 2.5 Omni locally with vision + speech to see if it could hear music instead of just speech.&lt;/p&gt; &lt;p&gt;I played guitar, and it did a surprisingly solid job telling me which chords I was playing in real-time. At the end, I debugged what the LLM was “hearing,” and input quality likely explained some of the misses it did have.&lt;/p&gt; &lt;p&gt;Next test: run it with the guitar out of sight, so we can confirm it’s not cheating by “seeing” the guitar instead of listening. Will share results when I try that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/848676nyzsjf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtqz3u/test_can_qwen_25_omni_actually_hear_guitar_chords/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtqz3u/test_can_qwen_25_omni_actually_hear_guitar_chords/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T16:30:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mugrii</id>
    <title>llama.cpp + ngrok</title>
    <updated>2025-08-19T12:17:17+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mugrii/llamacpp_ngrok/"&gt; &lt;img alt="llama.cpp + ngrok" src="https://external-preview.redd.it/gPMH5N9P7LrlLK9XBWYWsOwYsEUDqDGG215n2qqPFuo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=133ff0779923c95030c73be653e21ece82c6156f" title="llama.cpp + ngrok" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For llama.cpp to work on any device with ngrok or programs, it must have the http configuration.&lt;/p&gt; &lt;p&gt;1 - run llama.cpp&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w9ue9yzotyjf1.png?width=1320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=543fb957b35de8cbdaa11f2ec6a922b16ba26bd2"&gt;https://preview.redd.it/w9ue9yzotyjf1.png?width=1320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=543fb957b35de8cbdaa11f2ec6a922b16ba26bd2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;2 - Follow Ngrok's instructions on their website.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/010alpy8wyjf1.png?width=1364&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0265bcde4dbad303b658774066d9bd2bf06be975"&gt;https://preview.redd.it/010alpy8wyjf1.png?width=1364&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0265bcde4dbad303b658774066d9bd2bf06be975&lt;/a&gt;&lt;/p&gt; &lt;p&gt;3 - Once downloaded, all that remains is to execute our llama.cpp which is on port 8080.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kxshce1owyjf1.png?width=380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7c056919cb70b3299dd44c373df3648e7f6d4e9"&gt;https://preview.redd.it/kxshce1owyjf1.png?width=380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7c056919cb70b3299dd44c373df3648e7f6d4e9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4 - Once the command is executed, this should appear. The one that says Forwarding is our temporary website&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cou40litwyjf1.png?width=868&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=25ecd19d0df12d5fdbf2fd8db5a5a6f8edd8d6c3"&gt;https://preview.redd.it/cou40litwyjf1.png?width=868&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=25ecd19d0df12d5fdbf2fd8db5a5a6f8edd8d6c3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;----&lt;/p&gt; &lt;p&gt;more sites like these:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/anderspitman/awesome-tunneling"&gt;https://github.com/anderspitman/awesome-tunneling&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mugrii/llamacpp_ngrok/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mugrii/llamacpp_ngrok/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mugrii/llamacpp_ngrok/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T12:17:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mttad3</id>
    <title>Qwen-Image-Edit</title>
    <updated>2025-08-18T17:54:01+00:00</updated>
    <author>
      <name>/u/lomero</name>
      <uri>https://old.reddit.com/user/lomero</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttad3/qwenimageedit/"&gt; &lt;img alt="Qwen-Image-Edit" src="https://external-preview.redd.it/_phhztHOXP1EaSigwjpmdclnYlgiIY_nMfXGtHApDV0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b9e4ac0aed522efe5c46173ca8aa2cff20f9af4" title="Qwen-Image-Edit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lomero"&gt; /u/lomero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttad3/qwenimageedit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mttad3/qwenimageedit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T17:54:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtzn4b</id>
    <title>Tiny finance “thinking” model (Gemma-3 270M) with verifiable rewards (SFT → GRPO) — structured outputs + auto-eval (with code)</title>
    <updated>2025-08-18T21:47:13+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtzn4b/tiny_finance_thinking_model_gemma3_270m_with/"&gt; &lt;img alt="Tiny finance “thinking” model (Gemma-3 270M) with verifiable rewards (SFT → GRPO) — structured outputs + auto-eval (with code)" src="https://preview.redd.it/db62qfi7mujf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cacfd1e6d3138f9479b2df53130a1d980a490810" title="Tiny finance “thinking” model (Gemma-3 270M) with verifiable rewards (SFT → GRPO) — structured outputs + auto-eval (with code)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I taught a tiny model to &lt;em&gt;think like a finance analyst&lt;/em&gt; by enforcing a strict output contract and only rewarding it when the output is &lt;strong&gt;verifiably&lt;/strong&gt; correct.&lt;/p&gt; &lt;h1&gt;What I built&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Task &amp;amp; contract&lt;/strong&gt; (always returns): &lt;ul&gt; &lt;li&gt;&lt;code&gt;&amp;lt;REASONING&amp;gt;&lt;/code&gt; concise, balanced rationale&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;lt;SENTIMENT&amp;gt;&lt;/code&gt; positive | negative | neutral&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;lt;CONFIDENCE&amp;gt;&lt;/code&gt; 0.1–1.0 (calibrated)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training:&lt;/strong&gt; SFT → GRPO (Group Relative Policy Optimization)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Rewards (RLVR):&lt;/strong&gt; format gate, reasoning heuristics, FinBERT alignment, confidence calibration (Brier-style), directional consistency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stack:&lt;/strong&gt; Gemma-3 270M (IT), Unsloth 4-bit, TRL, HF Transformers (Windows-friendly)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Quick peek&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;REASONING&amp;gt; Revenue and EPS beat; raised FY guide on AI demand. However, near-term spend may compress margins. Net effect: constructive. &amp;lt;/REASONING&amp;gt; &amp;lt;SENTIMENT&amp;gt; positive &amp;lt;/SENTIMENT&amp;gt; &amp;lt;CONFIDENCE&amp;gt; 0.78 &amp;lt;/CONFIDENCE&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Why it matters&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Small + fast:&lt;/strong&gt; runs on modest hardware with low latency/cost&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Auditable:&lt;/strong&gt; structured outputs are easy to log, QA, and govern&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Early results vs base:&lt;/strong&gt; cleaner structure, better agreement on mixed headlines, steadier confidence&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Code: &lt;a href="https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/financial-reasoning-enhanced"&gt;Reinforcement-learning-with-verifable-rewards-Learnings/projects/financial-reasoning-enhanced at main · Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am planning to make more improvements essentially trying to add a more robust reward eval and also better synthetic data , I am exploring ideas on how i can make small models really intelligent in some domains ,&lt;/p&gt; &lt;p&gt;It is still rough around the edges will be actively improving it&lt;/p&gt; &lt;p&gt;&lt;em&gt;P.S. I'm currently looking for my next role in the LLM / Computer Vision space and would love to connect about any opportunities&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Portfolio:&lt;/em&gt; &lt;a href="https://pavan-portfolio-tawny.vercel.app/"&gt;Pavan Kunchala - AI Engineer &amp;amp; Full-Stack Developer&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/db62qfi7mujf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtzn4b/tiny_finance_thinking_model_gemma3_270m_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtzn4b/tiny_finance_thinking_model_gemma3_270m_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T21:47:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1muabfm</id>
    <title>My PR that adds Mikupad (with extra features) as an alternative webUI for ik_llama.cpp</title>
    <updated>2025-08-19T06:03:36+00:00</updated>
    <author>
      <name>/u/AdventLogin2021</name>
      <uri>https://old.reddit.com/user/AdventLogin2021</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdventLogin2021"&gt; /u/AdventLogin2021 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ikawrakow/ik_llama.cpp/pull/558"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muabfm/my_pr_that_adds_mikupad_with_extra_features_as_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muabfm/my_pr_that_adds_mikupad_with_extra_features_as_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T06:03:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtrn1y</id>
    <title>Drummer's Cydonia 24B v4.1 - Nothing like its predecessors. A stronger, less positive, less Mistral, performant tune!</title>
    <updated>2025-08-18T16:55:07+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtrn1y/drummers_cydonia_24b_v41_nothing_like_its/"&gt; &lt;img alt="Drummer's Cydonia 24B v4.1 - Nothing like its predecessors. A stronger, less positive, less Mistral, performant tune!" src="https://external-preview.redd.it/S-o7drNLzmVPRPUM6Ap2mbj65SEf6wzf8867vZcT5JE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9303bece3c938ba362682f24d93acffc4066003c" title="Drummer's Cydonia 24B v4.1 - Nothing like its predecessors. A stronger, less positive, less Mistral, performant tune!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v4.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtrn1y/drummers_cydonia_24b_v41_nothing_like_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtrn1y/drummers_cydonia_24b_v41_nothing_like_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T16:55:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mucw6t</id>
    <title>GLM 4.5 Air Suddenly running 5-6x Slower on Hybrid CPU/RoCM inference.</title>
    <updated>2025-08-19T08:42:41+00:00</updated>
    <author>
      <name>/u/ROS_SDN</name>
      <uri>https://old.reddit.com/user/ROS_SDN</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a pc of the specs...&lt;/p&gt; &lt;p&gt;CPU: 7900x&lt;/p&gt; &lt;p&gt;RAM: 2x32gb 6000 mhz cl 30&lt;/p&gt; &lt;p&gt;GPU: 7900XTX&lt;/p&gt; &lt;p&gt;I'm loading up a quant of GLM 4.5 air in llama cpp with..&lt;/p&gt; &lt;p&gt;&lt;code&gt;./build/bin/llama-cli -ngl 99 -sm none -m ~/models/unsloth/GLM-4.5-Air-GGUF/GLM-4.5-Air-IQ4_XS-00001-of-00002.gguf --flash-attn --n-cpu-moe 34 -c 32000 -p &amp;quot; Hello&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This is taking up roughly 23.5gbs of my gpus space, but the weird thing is just a few days ago when I ran this I was getting a very workable 10-12 t/s and now I'm near ~2 t/s.&lt;/p&gt; &lt;p&gt;I did just delete and have to re-download the model today, but it's in the same directory I had it in before, but I'm severely confused what I could have possibly changed outside that to completely destroy performance.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Never mind... I just reset my computer and now I'm back at 11 t/s... I'd love an explanation for that because I was not eating up 20gb of RAM running electron apps (as much as they may try) and web browsers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ROS_SDN"&gt; /u/ROS_SDN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mucw6t/glm_45_air_suddenly_running_56x_slower_on_hybrid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mucw6t/glm_45_air_suddenly_running_56x_slower_on_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mucw6t/glm_45_air_suddenly_running_56x_slower_on_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T08:42:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mttchz</id>
    <title>Deepseek R2 coming out ... when it gets more cowbell</title>
    <updated>2025-08-18T17:56:08+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttchz/deepseek_r2_coming_out_when_it_gets_more_cowbell/"&gt; &lt;img alt="Deepseek R2 coming out ... when it gets more cowbell" src="https://external-preview.redd.it/JNAOp8mejhheiBappEWRGE1kVdbTWLMVP5NLPsOJx9c.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f39a5e1f22d317e74a5e301e71bbdc92c571323" title="Deepseek R2 coming out ... when it gets more cowbell" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From what’s floating around it seems like we'll have to keep waiting a bit longer for Deepseek R2 to be released.&lt;/p&gt; &lt;p&gt;Apparently&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Liang Wenfeng has been sitting on R2's release because it still needs more cowbell&lt;/li&gt; &lt;li&gt;Training DeepSeek R2 on Huawei Ascend chips ran into persistent stability and software problems and no full training run ever succeeded. So Deepseek went back to Nvidia GPUs for training and is using Ascend chips for inference only&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here is the same story .. but with more cowbell &lt;a href="https://youtu.be/PzlqRsuIo1w"&gt;https://youtu.be/PzlqRsuIo1w&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/psltmf3youjf1.gif"&gt;https://i.redd.it/psltmf3youjf1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttchz/deepseek_r2_coming_out_when_it_gets_more_cowbell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttchz/deepseek_r2_coming_out_when_it_gets_more_cowbell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mttchz/deepseek_r2_coming_out_when_it_gets_more_cowbell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T17:56:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtk03a</id>
    <title>Kimi K2 is really, really good.</title>
    <updated>2025-08-18T12:00:19+00:00</updated>
    <author>
      <name>/u/ThomasAger</name>
      <uri>https://old.reddit.com/user/ThomasAger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve spent a long time waiting for an open source model I can use in production for both multi-agent multi-turn workflows, as well as a capable instruction following chat model. &lt;/p&gt; &lt;p&gt;This was the first model that has ever delivered. &lt;/p&gt; &lt;p&gt;For a long time I was stuck using foundation models, writing prompts that did the job I knew fine-tuning an open source model could do so much more effectively.&lt;/p&gt; &lt;p&gt;This isn’t paid or sponsored. It’s available to talk to for free and on the LM arena leaderboard (a month or so ago it was #8 there). I know many of ya’ll are already aware of this but I strongly recommend looking into integrating them into your pipeline.&lt;/p&gt; &lt;p&gt;They are already effective at long term agent workflows like building research reports with citations or websites. You can even try it for free. Has anyone else tried Kimi out? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasAger"&gt; /u/ThomasAger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtk03a/kimi_k2_is_really_really_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtk03a/kimi_k2_is_really_really_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtk03a/kimi_k2_is_really_really_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T12:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1muf2zr</id>
    <title>Backend for GLM 4.5 Air and the 96Gb Blackwell</title>
    <updated>2025-08-19T10:54:35+00:00</updated>
    <author>
      <name>/u/UltrMgns</name>
      <uri>https://old.reddit.com/user/UltrMgns</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;br /&gt; I've been struggling with this for quite a bit of time now. Last week I got my RTX Pro 6000 after ~ 7 months of saving cash. All I managed to compile as a backend is llama cpp, but I really want to get a proper backend working on it. Llama cpp struggles heavily with parallel requests, it accepts ANY api key as valid for authentication and those for me are true showstoppers because I can't expose an API endpoint and work on it remotely or share it with a couple of friends.&lt;br /&gt; Has anyone managed to get vLLM or TGI to work with GLM4.5 Air AWQ? I want to use this model specifically because it really seems to perform amazing as a local model (running the Q4_XL gguf currently).&lt;br /&gt; It's been an endless trail of errors, even though I compiled vLLM with compute 86+120 (because I have an 3090 in my server) it just wouldn't start serving, the sheer amount of different errors are just extremely discouraging, hence why I'm writing this plea for help. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UltrMgns"&gt; /u/UltrMgns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muf2zr/backend_for_glm_45_air_and_the_96gb_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muf2zr/backend_for_glm_45_air_and_the_96gb_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muf2zr/backend_for_glm_45_air_and_the_96gb_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T10:54:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mto8fa</id>
    <title>New code benchmark puts Qwen 3 Coder at the top of the open models</title>
    <updated>2025-08-18T14:51:49+00:00</updated>
    <author>
      <name>/u/mr_riptano</name>
      <uri>https://old.reddit.com/user/mr_riptano</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mto8fa/new_code_benchmark_puts_qwen_3_coder_at_the_top/"&gt; &lt;img alt="New code benchmark puts Qwen 3 Coder at the top of the open models" src="https://external-preview.redd.it/-FhGljRcqsXlvJ4R58hFA0RMnpSa9fBguxJ8Dc9Mg-4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15659a7787e6ab90c61f6ea82b0300809513a758" title="New code benchmark puts Qwen 3 Coder at the top of the open models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR of the open models results:&lt;/p&gt; &lt;p&gt;Q3C fp16 &amp;gt; Q3C fp8 &amp;gt; GPT-OSS-120b &amp;gt; V3 &amp;gt; K2&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_riptano"&gt; /u/mr_riptano &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://brokk.ai/power-ranking?round=open&amp;amp;models=flash-2.5%2Cgpt-oss-120b%2Cgpt5-mini%2Ck2%2Cq3c%2Cq3c-fp8%2Cv3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mto8fa/new_code_benchmark_puts_qwen_3_coder_at_the_top/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mto8fa/new_code_benchmark_puts_qwen_3_coder_at_the_top/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T14:51:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mu9hpi</id>
    <title>Seems like many open source models struggle with this.</title>
    <updated>2025-08-19T05:15:55+00:00</updated>
    <author>
      <name>/u/Neither_Egg_4773</name>
      <uri>https://old.reddit.com/user/Neither_Egg_4773</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many open source models, except for the GPT-OSS model, fail to genuinely grasp the details of &lt;strong&gt;recently published articles&lt;/strong&gt;. For example, if an article states, &amp;quot;last week this event happened...&amp;quot;. When asked about the date of the event, it becomes lost, even though the news articles on websites show the dates when they were published. Somehow, the GPT-OSS model understands the article and identifies the dates. Not only that, open-source models like Qwen have trouble quoting information from articles found during searches; however, it can definitely quote material from it's training data (&lt;em&gt;downside: it will hallucinate&lt;/em&gt;). But again, GPT-OSS can quote from recently published articles during searches. Hopefully, there will be a change soon in many of these open-source LLMs, as I use these open-source LLMs for source findings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neither_Egg_4773"&gt; /u/Neither_Egg_4773 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu9hpi/seems_like_many_open_source_models_struggle_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu9hpi/seems_like_many_open_source_models_struggle_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mu9hpi/seems_like_many_open_source_models_struggle_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T05:15:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mu49q3</id>
    <title>We're Updating the Wiki To Be More Current, And We Want Your Feedback</title>
    <updated>2025-08-19T01:01:14+00:00</updated>
    <author>
      <name>/u/N8Karma</name>
      <uri>https://old.reddit.com/user/N8Karma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; subreddit has long had a wiki: &lt;a href="https://www.reddit.com/r/LocalLLaMA/wiki/wiki/"&gt;https://www.reddit.com/r/LocalLLaMA/wiki/wiki/&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;However, the wiki hadn't been updated in a year or two (it still was mainly focused on LLaMA 2)! So we renovated the FAQ, Resources, and Models sections to reflect the present ecosystem. You can see a direct comparison &lt;a href="https://github.com/N8python/local-llama-wiki-revamp"&gt;here&lt;/a&gt; - with llama-old.md being the prior wiki and llama.md being the new one. Everything has been brought up to date.&lt;/p&gt; &lt;p&gt;But this is just the beginning - we want the wiki to reflect the desires of the community, and we want your feedback on what should be added/removed/altered. Models that are missing? Good resources that need a spotlight? We want to hear it! There will be a comment under this post to place your suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/N8Karma"&gt; /u/N8Karma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu49q3/were_updating_the_wiki_to_be_more_current_and_we/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu49q3/were_updating_the_wiki_to_be_more_current_and_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mu49q3/were_updating_the_wiki_to_be_more_current_and_we/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T01:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1muf6ry</id>
    <title>NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale</title>
    <updated>2025-08-19T11:00:19+00:00</updated>
    <author>
      <name>/u/lomero</name>
      <uri>https://old.reddit.com/user/lomero</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muf6ry/nextstep1_toward_autoregressive_image_generation/"&gt; &lt;img alt="NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale" src="https://external-preview.redd.it/ojIYaD1O8xYRW9Q-A7BHJQx5N3b1m-3M6OVRuLj2lzI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bfc0130326f6be10fdb5d6657d21b292210a99b2" title="NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lomero"&gt; /u/lomero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/stepfun-ai/NextStep-1-Large"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muf6ry/nextstep1_toward_autoregressive_image_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muf6ry/nextstep1_toward_autoregressive_image_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T11:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mueqhs</id>
    <title>When will low-cost Chinese GPUs hit the market?</title>
    <updated>2025-08-19T10:35:22+00:00</updated>
    <author>
      <name>/u/noellarkin</name>
      <uri>https://old.reddit.com/user/noellarkin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've heard of some Chinese GPUs, but I'm curious when they'll release low-cost alternatives that can seriously challenge NVIDIA 50xx dominance. Are there any indications that this will happen anytime soon? I'd love the hardware equivalent of a &amp;quot;deepseek moment&amp;quot; for OpenAI earlier this year.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noellarkin"&gt; /u/noellarkin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mueqhs/when_will_lowcost_chinese_gpus_hit_the_market/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mueqhs/when_will_lowcost_chinese_gpus_hit_the_market/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mueqhs/when_will_lowcost_chinese_gpus_hit_the_market/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T10:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mu0djr</id>
    <title>Qwen Code CLI has generous FREE Usage option</title>
    <updated>2025-08-18T22:15:33+00:00</updated>
    <author>
      <name>/u/NoobMLDude</name>
      <uri>https://old.reddit.com/user/NoobMLDude</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who didnt know, Qwen-Code which is a clone of Gemini CLI has a good &lt;a href="https://github.com/QwenLM/qwen-code?tab=readme-ov-file#-free-options-available"&gt;Free usage plan&lt;/a&gt;: - 2,000 requests per day with no token limits - 60 requests per minute rate limit It allows us to use Qwen3Coder for FREE.&lt;/p&gt; &lt;p&gt;Made a small video to showcase how to setup and use here: &lt;a href="https://youtu.be/M6ubLFqL-OA"&gt;https://youtu.be/M6ubLFqL-OA&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoobMLDude"&gt; /u/NoobMLDude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu0djr/qwen_code_cli_has_generous_free_usage_option/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu0djr/qwen_code_cli_has_generous_free_usage_option/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mu0djr/qwen_code_cli_has_generous_free_usage_option/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T22:15:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mttgrf</id>
    <title>Qwen-Image-Edit Released!</title>
    <updated>2025-08-18T18:00:24+00:00</updated>
    <author>
      <name>/u/MohamedTrfhgx</name>
      <uri>https://old.reddit.com/user/MohamedTrfhgx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alibaba’s Qwen team just released &lt;strong&gt;Qwen-Image-Edit&lt;/strong&gt;, an image editing model built on the &lt;strong&gt;20B Qwen-Image&lt;/strong&gt; backbone. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit"&gt;https://huggingface.co/Qwen/Qwen-Image-Edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It supports &lt;strong&gt;precise bilingual (Chinese &amp;amp; English) text editing&lt;/strong&gt; while preserving style, plus both &lt;strong&gt;semantic&lt;/strong&gt; and &lt;strong&gt;appearance-level&lt;/strong&gt; edits.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text editing with bilingual support&lt;/li&gt; &lt;li&gt;High-level semantic editing (object rotation, IP creation, concept edits)&lt;/li&gt; &lt;li&gt; Low-level appearance editing (add / delete / insert objects)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1957500569029079083"&gt;https://x.com/Alibaba_Qwen/status/1957500569029079083&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen has been really prolific lately what do you think of the new model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MohamedTrfhgx"&gt; /u/MohamedTrfhgx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttgrf/qwenimageedit_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttgrf/qwenimageedit_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mttgrf/qwenimageedit_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T18:00:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mu15vr</id>
    <title>bilbo.high.reasoning.medium.mini.3lightbulbs.ultra</title>
    <updated>2025-08-18T22:47:26+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu15vr/bilbohighreasoningmediummini3lightbulbsultra/"&gt; &lt;img alt="bilbo.high.reasoning.medium.mini.3lightbulbs.ultra" src="https://preview.redd.it/bfdlovjpvujf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b521d9b5416a36368655d8645cd92560559169e" title="bilbo.high.reasoning.medium.mini.3lightbulbs.ultra" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bfdlovjpvujf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu15vr/bilbohighreasoningmediummini3lightbulbsultra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mu15vr/bilbohighreasoningmediummini3lightbulbsultra/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T22:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mua1k4</id>
    <title>GPT OSS quality on Nebius - fixed (update)</title>
    <updated>2025-08-19T05:47:34+00:00</updated>
    <author>
      <name>/u/ai_devrel_eng</name>
      <uri>https://old.reddit.com/user/ai_devrel_eng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mua1k4/gpt_oss_quality_on_nebius_fixed_update/"&gt; &lt;img alt="GPT OSS quality on Nebius - fixed (update)" src="https://b.thumbs.redditmedia.com/sK7Bm1tG5gwQQ5s7xz2h10LEJrqGwQrDc2Udh4wNqoE.jpg" title="GPT OSS quality on Nebius - fixed (update)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai_devrel_eng"&gt; /u/ai_devrel_eng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mua1k4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mua1k4/gpt_oss_quality_on_nebius_fixed_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mua1k4/gpt_oss_quality_on_nebius_fixed_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T05:47:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mufslp</id>
    <title>5 Lessons from Evaluating AI Voice Agents</title>
    <updated>2025-08-19T11:30:46+00:00</updated>
    <author>
      <name>/u/Otherwise_Flan7339</name>
      <uri>https://old.reddit.com/user/Otherwise_Flan7339</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;strong&gt;Latency matters more than anything&lt;/strong&gt; - A 500ms delay feels tolerable in text. In voice, it feels broken. Testing latency across providers is a must.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Edge cases are the real test&lt;/strong&gt; - Scripted happy-path calls make every agent look good. The moment you throw in “interruptions” or background noise, big gaps appear.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evaluation can’t just be about transcripts&lt;/strong&gt; - Most tools stop at text accuracy. For voice, you need to factor in timing, interruptions, tone, and handling of silences.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Human-in-the-loop is still needed&lt;/strong&gt; - Automated scoring is great for scale, but subjective call quality still benefits from quick human checks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Specialized eval tooling helps a ton&lt;/strong&gt; - Most generic prompt testing setups don’t cover voice. We had better results when using platforms that support voice evals natively (e.g. Maxim AI, along with standard eval frameworks).&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Otherwise_Flan7339"&gt; /u/Otherwise_Flan7339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mufslp/5_lessons_from_evaluating_ai_voice_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mufslp/5_lessons_from_evaluating_ai_voice_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mufslp/5_lessons_from_evaluating_ai_voice_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T11:30:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtvgjx</id>
    <title>NVIDIA Releases Nemotron Nano 2 AI Models</title>
    <updated>2025-08-18T19:12:01+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtvgjx/nvidia_releases_nemotron_nano_2_ai_models/"&gt; &lt;img alt="NVIDIA Releases Nemotron Nano 2 AI Models" src="https://preview.redd.it/pzrpnuykutjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d61af91b9dcdda4649c24e581ac3941490ab82c0" title="NVIDIA Releases Nemotron Nano 2 AI Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;• 6X faster than similarly sized models, while also being more accurate&lt;/p&gt; &lt;p&gt;• NVIDIA is also releasing most of the data they used to create it, including the pretraining corpus&lt;/p&gt; &lt;p&gt;• The hybrid Mamba-Transformer architecture supports 128K context length on single GPU.&lt;/p&gt; &lt;p&gt;Full research paper here: &lt;a href="https://research.nvidia.com/labs/adlr/NVIDIA-Nemotron-Nano-2/"&gt;https://research.nvidia.com/labs/adlr/NVIDIA-Nemotron-Nano-2/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pzrpnuykutjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtvgjx/nvidia_releases_nemotron_nano_2_ai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtvgjx/nvidia_releases_nemotron_nano_2_ai_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T19:12:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mttcr9</id>
    <title>🚀 Qwen released Qwen-Image-Edit!</title>
    <updated>2025-08-18T17:56:23+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttcr9/qwen_released_qwenimageedit/"&gt; &lt;img alt="🚀 Qwen released Qwen-Image-Edit!" src="https://b.thumbs.redditmedia.com/oRveemue3RG8vuBdHGpOCwiYY2B-M7S5WjEjTkW73hM.jpg" title="🚀 Qwen released Qwen-Image-Edit!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 Excited to introduce Qwen-Image-Edit! Built on 20B Qwen-Image, it brings precise bilingual text editing (Chinese &amp;amp; English) while preserving style, and supports both semantic and appearance-level editing.&lt;/p&gt; &lt;p&gt;✨ Key Features&lt;/p&gt; &lt;p&gt;✅ Accurate text editing with bilingual support&lt;/p&gt; &lt;p&gt;✅ High-level semantic editing (e.g. object rotation, IP creation)&lt;/p&gt; &lt;p&gt;✅ Low-level appearance editing (e.g. addition/delete/insert)&lt;/p&gt; &lt;p&gt;Try it now: &lt;a href="https://chat.qwen.ai/?inputFeature=image_edit"&gt;https://chat.qwen.ai/?inputFeature=image_edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit"&gt;https://huggingface.co/Qwen/Qwen-Image-Edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ModelScope: &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit"&gt;https://modelscope.cn/models/Qwen/Qwen-Image-Edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://qwenlm.github.io/blog/qwen-image-edit/"&gt;https://qwenlm.github.io/blog/qwen-image-edit/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/QwenLM/Qwen-Image"&gt;https://github.com/QwenLM/Qwen-Image&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mttcr9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttcr9/qwen_released_qwenimageedit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mttcr9/qwen_released_qwenimageedit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T17:56:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1muft1w</id>
    <title>DeepSeek v3.1</title>
    <updated>2025-08-19T11:31:24+00:00</updated>
    <author>
      <name>/u/Just_Lifeguard_5033</name>
      <uri>https://old.reddit.com/user/Just_Lifeguard_5033</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muft1w/deepseek_v31/"&gt; &lt;img alt="DeepSeek v3.1" src="https://preview.redd.it/143veukbpyjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d9ae73ae246ccabb3b567735711ae0639d2819f2" title="DeepSeek v3.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It’s happening!&lt;/p&gt; &lt;p&gt;DeepSeek online model version has been updated to V3.1, context length extended to 128k, welcome to test on the official site and app. API calling remains the same.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just_Lifeguard_5033"&gt; /u/Just_Lifeguard_5033 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/143veukbpyjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muft1w/deepseek_v31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muft1w/deepseek_v31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T11:31:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
