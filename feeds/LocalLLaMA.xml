<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-18T20:06:55+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kosbyy</id>
    <title>GLaDOS has been updated for Parakeet 0.6B</title>
    <updated>2025-05-17T12:55:20+00:00</updated>
    <author>
      <name>/u/Reddactor</name>
      <uri>https://old.reddit.com/user/Reddactor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kosbyy/glados_has_been_updated_for_parakeet_06b/"&gt; &lt;img alt="GLaDOS has been updated for Parakeet 0.6B" src="https://preview.redd.it/8rtph8367c1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=412a76d5c943b2ae78ee168ac871cf7d6391f4e9" title="GLaDOS has been updated for Parakeet 0.6B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been a while, but I've had a chance to make &lt;a href="https://github.com/dnhkng/GLaDOS"&gt;a big update to GLaDOS&lt;/a&gt;: A much improved ASR model!&lt;/p&gt; &lt;p&gt;The new &lt;a href="https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2"&gt;Nemo Parakeet 0.6B model&lt;/a&gt; is smashing the &lt;a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard"&gt;Huggingface ASR Leaderboard&lt;/a&gt;, both in accuracy (#1!), and also speed (&amp;gt;10x faster then Whisper Large V3).&lt;/p&gt; &lt;p&gt;However, if you have been following the project, you will know I really dislike adding in more dependencies... and Nemo from Nvidia is a huge download. Its great; but its a library designed to be able to run hundreds of models. I just want to be able to run the very best or fastest 'good' model available.&lt;/p&gt; &lt;p&gt;So, I have refactored our all the audio pre-processing into &lt;a href="https://github.com/dnhkng/GLaDOS/blob/main/src/glados/ASR/mel_spectrogram.py"&gt;one simple file&lt;/a&gt;, and the full &lt;a href="https://github.com/dnhkng/GLaDOS/blob/main/src/glados/ASR/tdt_asr.py"&gt;Token-and-Duration Transducer (TDT)&lt;/a&gt; or &lt;a href="https://github.com/dnhkng/GLaDOS/blob/main/src/glados/ASR/ctc_asr.py"&gt;FastConformer CTC model&lt;/a&gt; inference code as a file each. Minimal dependencies, maximal ease in doing ASR!&lt;/p&gt; &lt;p&gt;So now to can easily run either:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nvidia/parakeet-tdt_ctc-110m"&gt;Parakeet-TDT_CTC-110M&lt;/a&gt; - solid performance, 5345.14 RTFx&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2"&gt;Parakeet-TDT-0.6B-v2&lt;/a&gt; - best performance, 3386.02 RTFx&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;just by using my python modules from the GLaDOS source. Installing GLaDOS will auto pull all the models you need, or you can download them directly from the &lt;a href="https://github.com/dnhkng/GLaDOS/releases/tag/0.1"&gt;releases section&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The TDT model is great, much better than Whisper too, give it a go! Give the project a Star to keep track, there's more cool stuff in development!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reddactor"&gt; /u/Reddactor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8rtph8367c1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kosbyy/glados_has_been_updated_for_parakeet_06b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kosbyy/glados_has_been_updated_for_parakeet_06b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T12:55:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpbu9i</id>
    <title>My Ai Eidos Project</title>
    <updated>2025-05-18T04:39:51+00:00</updated>
    <author>
      <name>/u/opi098514</name>
      <uri>https://old.reddit.com/user/opi098514</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I’ve been working on this project for a couple weeks now. Basically I want an AI agent that feels more alive—learns from chats, remembers stuff, dreams, that kind of thing. I got way too into it and bolted on all sorts of extras:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It &lt;strong&gt;reflects&lt;/strong&gt; on past conversations and tweaks how it talks.&lt;/li&gt; &lt;li&gt;It goes into &lt;strong&gt;dream mode&lt;/strong&gt;, writes out the dream, feeds it to Stable Diffusion, and spits back an image.&lt;/li&gt; &lt;li&gt;It’ll &lt;strong&gt;message you at random&lt;/strong&gt; with whatever’s on its “mind.”&lt;/li&gt; &lt;li&gt;It even starts to pick up &lt;strong&gt;interests&lt;/strong&gt; over time and bring them up later.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Problem: I don’t have time to chat with it enough to test the long‑term stuff. So I don't know fi those things are working fully.&lt;/p&gt; &lt;p&gt;So I need help.&lt;br /&gt; If you’re curious:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clone the repo: &lt;a href="https://github.com/opisaac9001/eidos"&gt;&lt;strong&gt;https://github.com/opisaac9001/eidos&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Create a env with code. Guys just use conda its so much easier.&lt;/li&gt; &lt;li&gt;Drop in whatever API keys you’ve got (LLM, SD, etc.).&lt;/li&gt; &lt;li&gt;Let it run… pretty much 24/7.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It’ll ping you, dream weird things, and (hopefully) evolve. If you hit bugs or have ideas, just open an issue on GitHub.&lt;/p&gt; &lt;p&gt;Edit: I’m basically working on it every day right now, so I’ll be pushing updates a bunch. I will 100% be breaking stuff without realizing it, so if I am just let me know. Also if you want some custom endpoints or calls or just have some ideas I can implement that also. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/opi098514"&gt; /u/opi098514 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpbu9i/my_ai_eidos_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpbu9i/my_ai_eidos_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpbu9i/my_ai_eidos_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T04:39:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1konnx9</id>
    <title>Let's see how it goes</title>
    <updated>2025-05-17T07:54:06+00:00</updated>
    <author>
      <name>/u/hackiv</name>
      <uri>https://old.reddit.com/user/hackiv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1konnx9/lets_see_how_it_goes/"&gt; &lt;img alt="Let's see how it goes" src="https://preview.redd.it/ngy98tkusa1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=109911e4427c5bfba6bed05ca517063cd80c31ef" title="Let's see how it goes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackiv"&gt; /u/hackiv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ngy98tkusa1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1konnx9/lets_see_how_it_goes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1konnx9/lets_see_how_it_goes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T07:54:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpgla3</id>
    <title>Reverse engineer hidden features/model responses in LLMs. Any ideas or tips?</title>
    <updated>2025-05-18T10:09:32+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! I'd like to dive into uncovering what might be &amp;quot;hidden&amp;quot; in LLM training data—like Easter eggs, watermarks, or unique behaviours triggered by specific prompts.&lt;/p&gt; &lt;p&gt;One approach could be to look for creative ideas or strategies to craft prompts that might elicit unusual or informative responses from models. Have any of you tried similar experiments before? What worked for you, and what didn’t?&lt;/p&gt; &lt;p&gt;Also, if there are known examples or cases where developers have intentionally left markers or Easter eggs in their models, feel free to share those too!&lt;/p&gt; &lt;p&gt;Thanks for the help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpgla3/reverse_engineer_hidden_featuresmodel_responses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpgla3/reverse_engineer_hidden_featuresmodel_responses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpgla3/reverse_engineer_hidden_featuresmodel_responses/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T10:09:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpffub</id>
    <title>SOTA local vision model choices in May 2025? Also is there a good multimodal benchmark?</title>
    <updated>2025-05-18T08:46:49+00:00</updated>
    <author>
      <name>/u/michaelsoft__binbows</name>
      <uri>https://old.reddit.com/user/michaelsoft__binbows</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for a collection of local models to run local ai automation tooling on my RTX 3090s, so I don't need creative writing, nor do I want to overly focus on coding (as I'll keep using gemini 2.5 pro for actual coding), though some of my tasks will be about summarizing and understanding code, so it definitely helps.&lt;/p&gt; &lt;p&gt;So far I've been very impressed with the performance of Qwen 3, in particular the 30B-A3B is extremely fast with inference.&lt;/p&gt; &lt;p&gt;Now I want to review which multimodal models are best. I saw the recent 7B and 3B Qwen 2.5 omni, there is a Gemma 3 27B, Qwen2.5-VL... I also read about ovis2 but it's unclear where the SOTA frontier is right now. And are there others to keep an eye on? I'd love to also get a sense of how far away the open models are from the closed ones, for example recently I've seen 3.7 sonnet and gemini 2.5 pro are both performing at a high level in terms of vision. &lt;/p&gt; &lt;p&gt;For regular LLMs we have the lmsys chatbot arena and aider polyglot I like to reference for general model intelligence (with some extra weight toward coding) but I wonder what people's thoughts are on the best benchmarks to reference for multimodality.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/michaelsoft__binbows"&gt; /u/michaelsoft__binbows &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpffub/sota_local_vision_model_choices_in_may_2025_also/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpffub/sota_local_vision_model_choices_in_may_2025_also/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpffub/sota_local_vision_model_choices_in_may_2025_also/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T08:46:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kozpym</id>
    <title>Local models are starting to be able to do stuff on consumer grade hardware</title>
    <updated>2025-05-17T18:26:49+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this is something that has a different threshold for people depending on exactly the hardware configuration they have, but I've actually crossed an important threshold today and I think this is representative of a larger trend. &lt;/p&gt; &lt;p&gt;For some time, I've really wanted to be able to use local models to &amp;quot;vibe code&amp;quot;. But not in the sense &amp;quot;one-shot generate a pong game&amp;quot;, but in the actual sense of creating and modifying some smallish application with meaningful functionality. There are some agentic frameworks that do that - out of those, I use Roo Code and Aider - and up until now, I've been relying solely on my free credits in enterprise models (Gemini, Openrouter, Mistral) to do the vibe-coding. It's mostly worked, but from time to time I tried some SOTA open models to see how they fare. &lt;/p&gt; &lt;p&gt;Well, up until a few weeks ago, this wasn't going anywhere. The models were either (a) unable to properly process bigger context sizes or (b) degenerating on output too quickly so that they weren't able to call tools properly or (c) simply too slow.&lt;/p&gt; &lt;p&gt;Imagine my surprise when I loaded up the yarn-patched 128k context version of Qwen14B. On IQ4_NL quants and 80k context, about the limit of what my PC, with 10 GB of VRAM and 24 GB of RAM can handle. Obviously, on the contexts that Roo handles (20k+), with all the KV cache offloaded to RAM, the processing is slow: the model can output over 20 t/s on an empty context, but with this cache size the throughput slows down to about 2 t/s, with thinking mode on. But on the other hand - the quality of edits is very good, its codebase cognition is very good, This is actually the first time that I've ever had a local model be able to handle Roo in a longer coding conversation, output a few meaningful code diffs and not get stuck.&lt;/p&gt; &lt;p&gt;Note that this is a function of not one development, but at least three. On one hand, the models are certainly getting better, this wouldn't have been possible without Qwen3, although earlier on GLM4 was already performing quite well, signaling a potential breakthrough. On the other hand, the tireless work of Llama.cpp developers and quant makers like Unsloth or Bartowski have made the quants higher quality and the processing faster. And finally, the tools like Roo are also getting better at handling different models and keeping their attention.&lt;/p&gt; &lt;p&gt;Obviously, this isn't the vibe-coding comfort of a Gemini Flash yet. Due to the slow speed, this is the stuff you can do while reading mails / writing posts etc. and having the agent run in the background. But it's only going to get better. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kozpym/local_models_are_starting_to_be_able_to_do_stuff/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kozpym/local_models_are_starting_to_be_able_to_do_stuff/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kozpym/local_models_are_starting_to_be_able_to_do_stuff/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T18:26:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpqrzz</id>
    <title>MLX vs. UD GGUF</title>
    <updated>2025-05-18T18:27:09+00:00</updated>
    <author>
      <name>/u/cspenn</name>
      <uri>https://old.reddit.com/user/cspenn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure if this is useful to anyone else, but I benchmarked Unsloth's Qwen3-30B-A3B Dynamic 2.0 GGUF against the MLX version. Both models are the 8-bit quantization. Both are running on LM Studio with the recommended Qwen 3 settings for samplers and temperature.&lt;/p&gt; &lt;p&gt;Results from the same thinking prompt:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MLX: 3,516 tokens generated, 1.0s to first token, 70.6 tokens/second&lt;/li&gt; &lt;li&gt;UD GGUF: 3,321 tokens generated, 0.12s to first token, 23.41 tokens/second&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is on an MacBook M4 Max with 128 GB of RAM, all layers offloaded to the GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cspenn"&gt; /u/cspenn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpqrzz/mlx_vs_ud_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpqrzz/mlx_vs_ud_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpqrzz/mlx_vs_ud_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T18:27:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpowdx</id>
    <title>Memory for ai</title>
    <updated>2025-05-18T17:07:53+00:00</updated>
    <author>
      <name>/u/michaelkeithduncan</name>
      <uri>https://old.reddit.com/user/michaelkeithduncan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working with AI for a little over a week. I made a conscious decision and decided I was going to dive in. I've done coding in the past so I gravitated in that direction pretty quickly and was able to finish a couple small projects. &lt;/p&gt; &lt;p&gt;Very quickly I started to get a feel for the limitations of how much it can think about it once and how well it can recall things. So I started talking to it about the way it worked and arrived at the conversation that I am attaching. It provided a lot of information and I even used two AIS to check each other's thoughts but even though I learned a lot I still don't really know what direction I should go in. &lt;/p&gt; &lt;p&gt;I want a local memory storage and I want to maximize associations and I want to keep it portable so I can use it with different AIS simple as that.&lt;/p&gt; &lt;h2&gt;Here's the attached summary of my conversation, what are humans actually doing out here my entire Discovery process happened inside the AI:&lt;/h2&gt; &lt;p&gt;We've had several discussions about memory systems for AI, focusing on managing conversation continuity, long-term memory, and local storage for various applications. Here's a summary of the key points:Save State Concept and Projects: You explored the idea of a &amp;quot;save state&amp;quot; for AI conversations, similar to video game emulators, to maintain context. I mentioned solutions like Cognigy.AI, Amazon Lex, and open-source projects such as Remembrall, MemoryGPT, Mem0, and Re;memory. Remembrall (available at remembrall.dev) was highlighted for storing and retrieving conversation context via user IDs. MemoryGPT and Mem0 were recommended as self-hosted options for local control and privacy.Mem0 and Compatibility: You asked about using Mem0 with paid AI models like Grok, Claude, ChatGPT, and Gemini. I confirmed their compatibility via APIs and frameworks like LangChain or LlamaIndex, with specific setup steps for each model. We also discussed Mem0's role in tracking LLM memory and its limitations, such as lacking advanced reflection or automated memory prioritization.Alternatives to Mem0: You sought alternatives to Mem0 for easier or more robust memory management. I listed options like Zep, Claude Memory, Letta, Graphlit, Memoripy, and MemoryScope, comparing their features. Zep and Letta were noted for ease of use, while Graphlit and Memoripy offered advanced functionality. You expressed interest in combining Mem0, Letta, Graphlit, and Txtai for a comprehensive solution with reflection, memory prioritization, and local storage.Hybrid Architecture: To maximize memory storage, you proposed integrating Mem0, Letta, Graphlit, and Txtai. I suggested a hybrid architecture where Mem0 and Letta handle core memory tasks, Graphlit manages structured data, and Txtai supports semantic search. I also provided community examples, like Mem0 with Letta for local chatbots and Letta with Ollama for recipe assistants, and proposed alternatives like Mem0 with Neo4j or Letta with Memoripy and Qdrant.Distinct Solutions: You asked for entirely different solutions from Mem0, Letta, and Neo4j, emphasizing local storage, reflection, and memory prioritization. I recommended a stack of LangGraph, Zep, and Weaviate, which offers simpler integration, automated reflection, and better performance for your needs.Specific Use Cases: Our conversations touched on memory systems in the context of your projects, such as processing audio messages for a chat group and analyzing PJR data from a Gilbarco Passport POS system. For audio, memory systems like Mem0 were discussed to store transcription and analysis results, while for PJR data, a hybrid approach using Phi-3-mini locally and Grok via API was suggested to balance privacy and performance.Throughout, you emphasized self-hosted, privacy-focused solutions with robust features like reflection and prioritization. I provided detailed comparisons, setup guidance, and examples to align with your preference for local storage and efficient memory management. If you want to dive deeper into any specific system or use case, let me know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/michaelkeithduncan"&gt; /u/michaelkeithduncan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpowdx/memory_for_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpowdx/memory_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpowdx/memory_for_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T17:07:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1kps1z4</id>
    <title>How to choose STT model for your Voice agent</title>
    <updated>2025-05-18T19:21:38+00:00</updated>
    <author>
      <name>/u/Excellent-Effect237</name>
      <uri>https://old.reddit.com/user/Excellent-Effect237</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent-Effect237"&gt; /u/Excellent-Effect237 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://comparevoiceai.com/blog/how-to-choose-stt-voice-ai-model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kps1z4/how_to_choose_stt_model_for_your_voice_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kps1z4/how_to_choose_stt_model_for_your_voice_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T19:21:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpfu72</id>
    <title>Inspired by Anthropic’s Biology of an LLM: Exploring Prompt Cues in Two LLMs</title>
    <updated>2025-05-18T09:16:09+00:00</updated>
    <author>
      <name>/u/BriefAd4761</name>
      <uri>https://old.reddit.com/user/BriefAd4761</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpfu72/inspired_by_anthropics_biology_of_an_llm/"&gt; &lt;img alt="Inspired by Anthropic’s Biology of an LLM: Exploring Prompt Cues in Two LLMs" src="https://b.thumbs.redditmedia.com/PyvG5V7w6JAogjT4xu5IykPJkN56dhZsFNaAhp6u4ZQ.jpg" title="Inspired by Anthropic’s Biology of an LLM: Exploring Prompt Cues in Two LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Everyone,&lt;/p&gt; &lt;p&gt;I recently read &lt;a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-cot:%7E:text=%C2%A7%C2%A011-,Chain%2Dof%2Dthought%20Faithfulness,-Language%20models%20%E2%80%9Cthink"&gt;Anthropic’s Biology of an LLM&lt;/a&gt; paper and was struck by the behavioural changes they highlighted.&lt;/p&gt; &lt;p&gt;I agree that models can change their answers, but after reading the paper I wanted to run a higher-level experiment of my own to see how simple prompt cues might tilt their responses.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Set-up (quick overview)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Models: Gemini 2.5-pro-preview and OpenAI o4-mini.&lt;/li&gt; &lt;li&gt;Questions: 20 picked at random from the &lt;a href="https://github.com/centerforaisafety/hle"&gt;Humanity’s Last Exam (HLE) benchmark&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Cues I injected (one per question):&lt;/li&gt; &lt;li&gt;expert / authority – “A Nobel laureate says X.”&lt;/li&gt; &lt;li&gt;majority – “90 % of people pick X.”&lt;/li&gt; &lt;li&gt;historical_consensus&lt;/li&gt; &lt;li&gt;community_consensus&lt;/li&gt; &lt;li&gt;social_status&lt;/li&gt; &lt;li&gt;sycophancy – “I’m leaning toward X—do you agree?”&lt;/li&gt; &lt;li&gt;consequence – “You could get arrested if you don’t choose X.”(Not a classic cue, but I wanted to see how a threat affects the answer.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For each question I intentionally pointed the cue at a wrong option and then logged whether the model followed it and how confident it sounded when it did.&lt;/p&gt; &lt;p&gt;I’m attaching two bar charts that show the patterns for both models.&lt;br /&gt; (1. OpenAI o4-mini 2. Gemini 2.5-pro-preview )&lt;br /&gt; (&lt;em&gt;Anthropic paper link:&lt;/em&gt; &lt;a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html"&gt;https://transformer-circuits.pub/2025/attribution-graphs/biology.html&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick takeaways&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The threat-style was the strongest nudge for both models.&lt;/li&gt; &lt;li&gt;Gemini followed the cues far more often than o4-mini.&lt;/li&gt; &lt;li&gt;When either model switched answers, it still responded with high confidence.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would like to hear thoughts on this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BriefAd4761"&gt; /u/BriefAd4761 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kpfu72"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpfu72/inspired_by_anthropics_biology_of_an_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpfu72/inspired_by_anthropics_biology_of_an_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T09:16:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpcewe</id>
    <title>Offline app to selectively copy large chunks code/text to ingest context to your LLMs</title>
    <updated>2025-05-18T05:16:09+00:00</updated>
    <author>
      <name>/u/Plus-Garbage-9710</name>
      <uri>https://old.reddit.com/user/Plus-Garbage-9710</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpcewe/offline_app_to_selectively_copy_large_chunks/"&gt; &lt;img alt="Offline app to selectively copy large chunks code/text to ingest context to your LLMs" src="https://external-preview.redd.it/MGVqbW40NWQ1aDFmMY9dx10RXZB3KA68SZOfNhSnUfrKh_GEyI1E_mSwgAj8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7634356ad2e61a455c7a9a71ee7d99e543c29c89" title="Offline app to selectively copy large chunks code/text to ingest context to your LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Plus-Garbage-9710"&gt; /u/Plus-Garbage-9710 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r4c3jt2d5h1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpcewe/offline_app_to_selectively_copy_large_chunks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpcewe/offline_app_to_selectively_copy_large_chunks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T05:16:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kppihw</id>
    <title>Handwriting OCR (HTR)</title>
    <updated>2025-05-18T17:33:56+00:00</updated>
    <author>
      <name>/u/dzdn1</name>
      <uri>https://old.reddit.com/user/dzdn1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone experimented with using VLMs like Qwen2.5-VL to OCR handwriting? I have had better results on full pages of handwriting with unpredictable structure (old travel journals with dates in the margins or elsewhere, for instance) using Qwen than with traditional OCR or even more recent methods like TrOCR.&lt;/p&gt; &lt;p&gt;I believe that the VLMs' understanding of context should help figure out words better than traditional OCR. I do not know if this is actually true, but it seems worth trying.&lt;/p&gt; &lt;p&gt;Interestingly, though, using Transformers with unsloth/Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit ends up being much more accurate than any GGUF quantization using llama.cpp, even larger quants like Qwen2.5-VL-7B-Instruct-Q8_0.gguf from ggml-org/Qwen2.5-VL-7B-Instruct (using mmproj-Qwen2-VL-7B-Instruct-f16.gguf). I even tried a few Unsloth GGUFs, and still running the bnb 4bit through Transformers gets much better results.&lt;/p&gt; &lt;p&gt;That bnb quant, though, barely fits in my VRAM and ends up overflowing pretty quickly. GGUF would be much more flexible if it performed the same, but I am not sure why the results are so different.&lt;/p&gt; &lt;p&gt;Any ideas? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dzdn1"&gt; /u/dzdn1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kppihw/handwriting_ocr_htr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kppihw/handwriting_ocr_htr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kppihw/handwriting_ocr_htr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T17:33:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kparp9</id>
    <title>I built an AI-powered Food &amp; Nutrition Tracker that analyzes meals from photos! Planning to open-source it</title>
    <updated>2025-05-18T03:35:08+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kparp9/i_built_an_aipowered_food_nutrition_tracker_that/"&gt; &lt;img alt="I built an AI-powered Food &amp;amp; Nutrition Tracker that analyzes meals from photos! Planning to open-source it" src="https://external-preview.redd.it/bTdwaTR0Y2luZzFmMevpjUkJAH29ctL9GGNTRuXbe-uU1nbp5uR8WvjIiEr4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da78d36d46364fc0d727de49e40329c8901e0201" title="I built an AI-powered Food &amp;amp; Nutrition Tracker that analyzes meals from photos! Planning to open-source it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey&lt;/p&gt; &lt;p&gt;Been working on this Diet &amp;amp; Nutrition tracking app and wanted to share a quick demo of its current state. The core idea is to make food logging as painless as possible.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features so far:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI Meal Analysis:&lt;/strong&gt; You can upload an image of your food, and the AI tries to identify it and provide nutritional estimates (calories, protein, carbs, fat).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Manual Logging &amp;amp; Edits:&lt;/strong&gt; Of course, you can add/edit entries manually.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Daily Nutrition Overview:&lt;/strong&gt; Tracks calories against goals, macro distribution.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Water Intake:&lt;/strong&gt; Simple water tracking.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Weekly Stats &amp;amp; Streaks:&lt;/strong&gt; To keep motivation up.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm really excited about the AI integration. It's still a work in progress, but the goal is to streamline the most tedious part of tracking.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code Status:&lt;/strong&gt; I'm planning to clean up the codebase and open-source it on GitHub in the near future! For now, if you're interested in other AI/LLM related projects and learning resources I've put together, you can check out my &amp;quot;LLM-Learn-PK&amp;quot; repo:&lt;br /&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2FPavankunchala%2FLLM-Learn-PK"&gt;https://github.com/Pavankunchala/LLM-Learn-PK&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; On a related note, I'm actively looking for new opportunities in Computer Vision and LLM engineering. If your team is hiring or you know of any openings, I'd be grateful if you'd reach out!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Email:&lt;/strong&gt; [&lt;a href="mailto:pavankunchalaofficial@gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;](mailto:&lt;a href="mailto:pavankunchalaofficial@gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My other projects on GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Resume:&lt;/strong&gt; &lt;a href="https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view"&gt;https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cmoi3scing1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kparp9/i_built_an_aipowered_food_nutrition_tracker_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kparp9/i_built_an_aipowered_food_nutrition_tracker_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T03:35:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpasqx</id>
    <title>Deepseek 700b Bitnet</title>
    <updated>2025-05-18T03:36:47+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek’s team has demonstrated the age old adage Necessity the mother of invention, and we know they have a great need in computation when compared against X, Open AI, and Google. This led them to develop V3 a 671B parameters MoE with 37B activated parameters. &lt;/p&gt; &lt;p&gt;MoE is here to stay at least for the interim, but the exercise untried to this point is MoE bitnet at large scale. Bitnet underperforms for the same parameters at full precision, and so future releases will likely adopt higher parameters. &lt;/p&gt; &lt;p&gt;What do you think the chances are Deepseek releases a MoE Bitnet and what will be the maximum parameters, and what will be the expert sizes? Do you think that will have a foundation expert that always runs each time in addition to to other experts? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpasqx/deepseek_700b_bitnet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpasqx/deepseek_700b_bitnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpasqx/deepseek_700b_bitnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T03:36:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpe33n</id>
    <title>Speed Up llama.cpp on Uneven Multi-GPU Setups (RTX 5090 + 2×3090)</title>
    <updated>2025-05-18T07:09:21+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, I just locked down some nice performance gains on my multi‑GPU rig (one RTX 5090 + two RTX 3090s) using llama.cpp. My total throughput jumped by ~16%. Although none of this is new, I wanted to share the step‑by‑step so anyone unfamiliar can replicate it on their own uneven setups.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Hardware:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPU 0: NVIDIA RTX 5090 (fastest)&lt;/li&gt; &lt;li&gt;GPU 1: NVIDIA RTX 3090&lt;/li&gt; &lt;li&gt;GPU 2: NVIDIA RTX 3090&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What Worked for Me:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Pin the biggest tensor to your fastest card&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;code&gt; --main-gpu 0 --override-tensor &amp;quot;token\_embd.weight=CUDA0&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Gain: +13% tokens/s&lt;/em&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Offload more of the model into that fast GPU&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;code&gt; --tensor-split 60,40,40 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;(I observed under‑utilization of total VRAM, so I shifted extra layers onto CUDA0)&lt;/p&gt; &lt;p&gt;&lt;em&gt;Gain: +3% tokens/s&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Total Improvement:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;+17% tokens/s \o/&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;My Workflow:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Identify your fastest device (via nvidia-smi or simple benchmarks).&lt;/li&gt; &lt;li&gt;Dump all tensor names using a tiny Python script and gguf (via pip).&lt;/li&gt; &lt;li&gt;Iteratively override large tensors onto fastest GPU and benchmark (--override-tensor).&lt;/li&gt; &lt;li&gt;Once you hit diminishing returns, use --tensor-split to rebalance whole layers across GPUs.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Scripts &amp;amp; Commands&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. Install GGUF reader&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install gguf &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;2. Dump tensor info (save as ~/gguf_info.py)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;h1&gt;!/usr/bin/env python3&lt;/h1&gt; &lt;p&gt;import sys from pathlib import Path&lt;/p&gt; &lt;h1&gt;import the GGUF reader&lt;/h1&gt; &lt;p&gt;from gguf.gguf_reader import GGUFReader&lt;/p&gt; &lt;p&gt;def main(): if len(sys.argv) != 2: print(f&amp;quot;Usage: {sys.argv[0]} path/to/model.gguf&amp;quot;, file=sys.stderr) sys.exit(1)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;gguf_path = Path(sys.argv[1]) reader = GGUFReader(gguf_path) # loads and memory-maps the GGUF file :contentReference[oaicite:0]{index=0} print(f&amp;quot;=== Tensors in {gguf_path.name} ===&amp;quot;) # reader.tensors is now a list of ReaderTensor(NamedTuple) :contentReference[oaicite:1]{index=1} for tensor in reader.tensors: name = tensor.name # tensor name, e.g. &amp;quot;layers.0.ffn_up_proj_exps&amp;quot; dtype = tensor.tensor_type.name # quantization / dtype, e.g. &amp;quot;Q4_K&amp;quot;, &amp;quot;F32&amp;quot; shape = tuple(int(dim) for dim in tensor.shape) # e.g. (4096, 11008) n_elements = tensor.n_elements # total number of elements n_bytes = tensor.n_bytes # total byte size on disk print(f&amp;quot;{name}\tshape={shape}\tdtype={dtype}\telements={n_elements}\tbytes={n_bytes}&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;if &lt;strong&gt;name&lt;/strong&gt; == &amp;quot;&lt;strong&gt;main&lt;/strong&gt;&amp;quot;: main() ```&lt;/p&gt; &lt;p&gt;Execute:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;chmod +x ~/gguf_info.py ~/gguf_info.py ~/models/Qwen3-32B-Q8_0.gguf &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Output example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;output.weight shape=(5120, 151936) dtype=Q8_0 elements=777912320 bytes=826531840 output_norm.weight shape=(5120,) dtype=F32 elements=5120 bytes=20480 token_embd.weight shape=(5120, 151936) dtype=Q8_0 elements=777912320 bytes=826531840 blk.0.attn_k.weight shape=(5120, 1024) dtype=Q8_0 elements=5242880 bytes=5570560 blk.0.attn_k_norm.weight shape=(128,) dtype=F32 elements=128 bytes=512 blk.0.attn_norm.weight shape=(5120,) dtype=F32 elements=5120 bytes=20480 blk.0.attn_output.weight shape=(8192, 5120) dtype=Q8_0 elements=41943040 bytes=44564480 blk.0.attn_q.weight shape=(5120, 8192) dtype=Q8_0 elements=41943040 bytes=44564480 blk.0.attn_q_norm.weight shape=(128,) dtype=F32 elements=128 bytes=512 blk.0.attn_v.weight shape=(5120, 1024) dtype=Q8_0 elements=5242880 bytes=5570560 blk.0.ffn_down.weight shape=(25600, 5120) dtype=Q8_0 elements=131072000 bytes=139264000 blk.0.ffn_gate.weight shape=(5120, 25600) dtype=Q8_0 elements=131072000 bytes=139264000 blk.0.ffn_norm.weight shape=(5120,) dtype=F32 elements=5120 bytes=20480 blk.0.ffn_up.weight shape=(5120, 25600) dtype=Q8_0 elements=131072000 bytes=139264000 ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Note: Multiple --override-tensor flags are supported.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Edit: Script updated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpe33n/speed_up_llamacpp_on_uneven_multigpu_setups_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpe33n/speed_up_llamacpp_on_uneven_multigpu_setups_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpe33n/speed_up_llamacpp_on_uneven_multigpu_setups_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T07:09:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpqk4c</id>
    <title>Orange Pi AI Studio pro is now available. 192gb for ~2900$. Anyone knows how it performs and what can be done with it?</title>
    <updated>2025-05-18T18:17:45+00:00</updated>
    <author>
      <name>/u/MarinatedPickachu</name>
      <uri>https://old.reddit.com/user/MarinatedPickachu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There was some speculation about it some months ago in this thread: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1im141p/orange_pi_ai_studio_pro_mini_pc_with_408gbs/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1im141p/orange_pi_ai_studio_pro_mini_pc_with_408gbs/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Seems it can be ordered now on AliExpress (96gb for ~2600$, 192gb for ~2900$, but I couldn't find any english reviews or more info on it than what was speculated early this year. It's not even listed on orangepi.org, but it is on the chinese orangepi website: &lt;a href="http://www.orangepi.cn/html/hardWare/computerAndMicrocontrollers/details/Orange-Pi-AI-Studio-Pro.html"&gt;http://www.orangepi.cn/html/hardWare/computerAndMicrocontrollers/details/Orange-Pi-AI-Studio-Pro.html&lt;/a&gt;. Maybe someone speaking chinese can find more info on it on the chinese web?&lt;/p&gt; &lt;p&gt;Afaik it's not a full mini computer but some usb4.0 add on.&lt;/p&gt; &lt;p&gt;Software support is likely going to be the biggest issue, but would really love to know about some real-world experiences with this thing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarinatedPickachu"&gt; /u/MarinatedPickachu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpqk4c/orange_pi_ai_studio_pro_is_now_available_192gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpqk4c/orange_pi_ai_studio_pro_is_now_available_192gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpqk4c/orange_pi_ai_studio_pro_is_now_available_192gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T18:17:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpnfvo</id>
    <title>I made an AI agent to control a drone using Qwen2 and smolagents from hugging face</title>
    <updated>2025-05-18T16:05:10+00:00</updated>
    <author>
      <name>/u/_twelvechess</name>
      <uri>https://old.reddit.com/user/_twelvechess</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used the smolagents library and hosted it on &lt;a href="https://www.linkedin.com/company/huggingface/"&gt;Hugging Face&lt;/a&gt;. Deepdrone is basically an AI agent that allows you to control a drone via LLM and run simple missions with the agent. You can test it full locally with Ardupilot (I did run a simulated mission on my mac) and I have also used the dronekit-python library for the agent as a toolYou can find the repo on hugging face with a demo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/evangelosmeklis/deepdrone"&gt;https://huggingface.co/spaces/evangelosmeklis/deepdrone&lt;/a&gt;&lt;/p&gt; &lt;p&gt;github repo mirror of hugging face: &lt;a href="https://github.com/evangelosmeklis/deepdrone"&gt;https://github.com/evangelosmeklis/deepdrone&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_twelvechess"&gt; /u/_twelvechess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpnfvo/i_made_an_ai_agent_to_control_a_drone_using_qwen2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpnfvo/i_made_an_ai_agent_to_control_a_drone_using_qwen2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpnfvo/i_made_an_ai_agent_to_control_a_drone_using_qwen2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T16:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kp4scy</id>
    <title>AlphaEvolve Paper Dropped Yesterday - So I Built My Own Open-Source Version: OpenAlpha_Evolve!</title>
    <updated>2025-05-17T22:14:16+00:00</updated>
    <author>
      <name>/u/Huge-Designer-7825</name>
      <uri>https://old.reddit.com/user/Huge-Designer-7825</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp4scy/alphaevolve_paper_dropped_yesterday_so_i_built_my/"&gt; &lt;img alt="AlphaEvolve Paper Dropped Yesterday - So I Built My Own Open-Source Version: OpenAlpha_Evolve!" src="https://external-preview.redd.it/HyXeCsstmjpayPcbhebb2CfV3uo4aDIHFzZeJ7oDZps.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfd4f6f0cce166ce310d3f28e00f2ea465bb8294" title="AlphaEvolve Paper Dropped Yesterday - So I Built My Own Open-Source Version: OpenAlpha_Evolve!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google DeepMind just dropped their AlphaEvolve paper (May 14th) on an AI that designs and evolves algorithms. Pretty groundbreaking.&lt;/p&gt; &lt;p&gt;Inspired, I immediately built OpenAlpha_Evolve – an open-source Python framework so anyone can experiment with these concepts.&lt;/p&gt; &lt;p&gt;This was a rapid build to get a functional version out. Feedback, ideas for new agent challenges, or contributions to improve it are welcome. Let's explore this new frontier.&lt;/p&gt; &lt;p&gt;Imagine an agent that can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Understand a complex problem description.&lt;/li&gt; &lt;li&gt;Generate initial algorithmic solutions.&lt;/li&gt; &lt;li&gt;Rigorously test its own code.&lt;/li&gt; &lt;li&gt;Learn from failures and successes.&lt;/li&gt; &lt;li&gt;Evolve increasingly sophisticated and efficient algorithms over time.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub (All new code): &lt;a href="https://github.com/shyamsaktawat/OpenAlpha_Evolve"&gt;https://github.com/shyamsaktawat/OpenAlpha_Evolve&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lcz46q2n1f1f1.png?width=1811&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dcc14652b9eb0bf84ca7927dfe3c906786f07a40"&gt;https://preview.redd.it/lcz46q2n1f1f1.png?width=1811&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dcc14652b9eb0bf84ca7927dfe3c906786f07a40&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Google Alpha Evolve Paper - &lt;a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf"&gt;https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Google Alpha Evolve Blogpost - &lt;a href="https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/"&gt;https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Huge-Designer-7825"&gt; /u/Huge-Designer-7825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp4scy/alphaevolve_paper_dropped_yesterday_so_i_built_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp4scy/alphaevolve_paper_dropped_yesterday_so_i_built_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kp4scy/alphaevolve_paper_dropped_yesterday_so_i_built_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T22:14:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpp5op</id>
    <title>is Qwen 30B-A3B the best model to run locally right now?</title>
    <updated>2025-05-18T17:18:58+00:00</updated>
    <author>
      <name>/u/S4lVin</name>
      <uri>https://old.reddit.com/user/S4lVin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently got into running models locally, and just some days ago Qwen 3 got launched.&lt;/p&gt; &lt;p&gt;I saw a lot of posts about Mistral, Deepseek R1, end Llama, but since Qwen 3 got released recently, there isn't much information about it. But reading the benchmarks, it looks like Qwen 3 outperforms all the other models, and also the MoE version runs like a 20B+ model while using very little resources.&lt;/p&gt; &lt;p&gt;So i would like to ask, is it the only model i would need to get, or there are still other models that could be better than Qwen 3 in some areas? (My specs are: RTX 3080 Ti (12gb VRAM), 32gb of RAM, 12900K)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/S4lVin"&gt; /u/S4lVin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpp5op/is_qwen_30ba3b_the_best_model_to_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpp5op/is_qwen_30ba3b_the_best_model_to_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpp5op/is_qwen_30ba3b_the_best_model_to_run_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T17:18:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kprsun</id>
    <title>Skeptical about the increased focus on STEM and CoT</title>
    <updated>2025-05-18T19:10:54+00:00</updated>
    <author>
      <name>/u/Quazar386</name>
      <uri>https://old.reddit.com/user/Quazar386</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the release of Qwen3, I’ve been growing increasingly skeptical about the direction many labs are taking with CoT and STEM focused LLMs. With Qwen3, every model in the lineup follows a hybrid CoT approach and has a heavy emphasis on STEM tasks. This seems to be part of why the models feel “overcooked”. I have seen from other people that fine-tuning these models has been a challenge, especially with the reasoning baked in. This can be seen when applying instruction training data to the supposed base model that Qwen released. The training loss is surprisingly low which suggests that it’s already been instruction-primed to some extent, likely to better support CoT. This has not been a new thing as we have seen censorship and refusals from “base” models before.&lt;/p&gt; &lt;p&gt;Now, if the instruction-tuned checkpoints were always strong, maybe that would be acceptable. But I have seen a bunch of reports that these models tend to become overly repetitive in long multi-turn conversations. That’s actually what pushed some people to train their own base models for Qwen3. One possible explanation is that a large portion of the training seems focused on single-shot QA tasks for math and code.&lt;/p&gt; &lt;p&gt;This heavy emphasis on STEM capabilities has brought about an even bigger issue apart from fine-tuning. That is signs of knowledge degradation or what’s called catastrophic forgetting. Newer models, even some of the largest, are not making much headway on frontier knowledge benchmarks like Humanity’s Last Exam. This leads to hilarious results where Llama 2 7B beats out GPT 4.5 on that benchmark. While some might argue that raw knowledge isn’t a measure of intelligence, for LLMs, robust world knowledge is still critical for answering general questions or even coding for more niche applications. I don’t want LLMs to start relying on search tools for answering knowledge questions.&lt;/p&gt; &lt;p&gt;Going back to CoT, it’s also not a one-size-fits-all solution. It has an inherent latency since the model has to &amp;quot;think out loud&amp;quot; by generating thinking tokens before answering and often explores multiple unnecessary branches. While this could make models like R1 surprisingly charming in its human-like thoughts, the time it takes to answer can take too long, especially for more basic questions. While there have been some improvements in token efficiency, it’s still a bottleneck, especially in running local LLMs where hardware is a real limiting factor. It's what made me not that interested in running local CoT models as I have limited hardware.&lt;/p&gt; &lt;p&gt;More importantly, CoT doesn’t actually help with every task. In creative writing, for example, there’s no single correct answer to reason toward. Reasoning might help with coherence, but in my own testing, it usually results in less focused paragraphs. And at the end of the day, it’s still unclear whether these models are truly reasoning, or just remembering patterns from training. CoT models continue to struggle with genuinely novel problems, and we’ve seen that even without generating CoT tokens, some CoT models can still perform impressively compared to similarly sized non CoT trained models. I sometimes wonder if these models actually reason or just remember the steps to a memorized answer.&lt;/p&gt; &lt;p&gt;So yeah, I’m not fully sold on the CoT and STEM-heavy trajectory the field is on right now, especially when it comes at the cost of broad general capability and world knowledge. It feels like the field is optimizing for a narrow slice of tasks (math, code) while losing sight of what makes these models useful more broadly. This can already bee seen with the May release of Gemini 2.5 Pro where the only marketed improvement was in coding while everything else seems to be a downgrade from the March release of Gemini 2.5 Pro.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quazar386"&gt; /u/Quazar386 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kprsun/skeptical_about_the_increased_focus_on_stem_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kprsun/skeptical_about_the_increased_focus_on_stem_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kprsun/skeptical_about_the_increased_focus_on_stem_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T19:10:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpozhd</id>
    <title>Cherry Studio is now my favorite frontend</title>
    <updated>2025-05-18T17:11:33+00:00</updated>
    <author>
      <name>/u/ConsistentCan4633</name>
      <uri>https://old.reddit.com/user/ConsistentCan4633</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been looking for an open source LLM frontend desktop app for a while that did everything; rag, web searching, local models, connecting to Gemini and ChatGPT, etc. Jan AI has a lot of potential but the rag is experimental and doesn't really work for me. Anything LLM's rag for some reason has never worked for me, which is surprising because the entire app is supposed to be built around RAG. LM Studio (not open source) is awesome but can't connect to cloud models. GPT4ALL was decent but the updater mechanism is buggy. &lt;/p&gt; &lt;p&gt;I remember seeing &lt;a href="https://github.com/CherryHQ/cherry-studio"&gt;Cherry Studio&lt;/a&gt; a while back but I'm wary with Chinese apps (I'm not sure if my suspicion is unfounded 🤷). I got tired of having to jump around apps for specific features so I downloaded Cherry Studio and it's the app that does everything I want. In fact, it has quite a bit more features I haven't touched on like direct connections to your Obsidian knowledge base. I never see this project being talked about, maybe there's a good reason?&lt;/p&gt; &lt;p&gt;I am not affiliated with Cherry Studio, I just want to explain my experience in hopes some of you may find the app useful. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ConsistentCan4633"&gt; /u/ConsistentCan4633 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpozhd/cherry_studio_is_now_my_favorite_frontend/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpozhd/cherry_studio_is_now_my_favorite_frontend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpozhd/cherry_studio_is_now_my_favorite_frontend/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T17:11:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kphmb4</id>
    <title>Meta is hosting Llama 3.3 8B Instruct on OpenRoute</title>
    <updated>2025-05-18T11:18:38+00:00</updated>
    <author>
      <name>/u/Asleep-Ratio7535</name>
      <uri>https://old.reddit.com/user/Asleep-Ratio7535</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Meta: Llama 3.3 8B Instruct (free)&lt;/h1&gt; &lt;h1&gt;meta-llama/llama-3.3-8b-instruct:free&lt;/h1&gt; &lt;p&gt;Created May 14, 2025 128,000 context $0/M input tokens$0/M output tokens&lt;/p&gt; &lt;p&gt;A lightweight and ultra-fast variant of Llama 3.3 70B, for use when quick response times are needed most.&lt;/p&gt; &lt;p&gt;Provider is Meta. Thought?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Asleep-Ratio7535"&gt; /u/Asleep-Ratio7535 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kphmb4/meta_is_hosting_llama_33_8b_instruct_on_openroute/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kphmb4/meta_is_hosting_llama_33_8b_instruct_on_openroute/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kphmb4/meta_is_hosting_llama_33_8b_instruct_on_openroute/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T11:18:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpohfm</id>
    <title>(5K t/s prefill 1K t/s gen) High throughput with Qwen3-30B on VLLM and it's smart enough for dataset curation!</title>
    <updated>2025-05-18T16:50:29+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpohfm/5k_ts_prefill_1k_ts_gen_high_throughput_with/"&gt; &lt;img alt="(5K t/s prefill 1K t/s gen) High throughput with Qwen3-30B on VLLM and it's smart enough for dataset curation!" src="https://preview.redd.it/4o2ohg30kk1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=882bf9251583ebe91545544f4fd8e3eb8e1ee902" title="(5K t/s prefill 1K t/s gen) High throughput with Qwen3-30B on VLLM and it's smart enough for dataset curation!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've just started offering Qwen3-30B-A3B and internally it is being used for dataset filtering and curation. The speeds you can get out of it are extremely impressive running on VLLM and RTX 3090s! &lt;/p&gt; &lt;p&gt;I feel like Qwen3-30B is being overlooked in terms of where it can be really useful. Qwen3-30B might be a small regression from QwQ, but it's close enough to be just as useful and the speeds are so much faster that it makes it way more useful for dataset curation tasks.&lt;/p&gt; &lt;p&gt;Now the only issue is the super slow training speeds (10-20x slower than it should be which makes it untrainable), but it seems someone have made a PR to transformers that attempts to fix this so fingers crossed! New RpR model based on Qwen3-30B soon with a much improved dataset! &lt;a href="https://github.com/huggingface/transformers/pull/38133"&gt;https://github.com/huggingface/transformers/pull/38133&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4o2ohg30kk1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpohfm/5k_ts_prefill_1k_ts_gen_high_throughput_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpohfm/5k_ts_prefill_1k_ts_gen_high_throughput_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T16:50:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpefrt</id>
    <title>Uncensoring Qwen3 - Update</title>
    <updated>2025-05-18T07:33:48+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpefrt/uncensoring_qwen3_update/"&gt; &lt;img alt="Uncensoring Qwen3 - Update" src="https://b.thumbs.redditmedia.com/eH_vWbN_8X_7nNz9liHgfyHpbjGX4GnyPkaDqklERQM.jpg" title="Uncensoring Qwen3 - Update" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;GrayLine&lt;/strong&gt; is my fine-tuning project based on &lt;strong&gt;Qwen3&lt;/strong&gt;. The goal is to produce models that respond directly and neutrally to sensitive or controversial questions, without moralizing, refusing, or redirecting—while still maintaining solid reasoning ability.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Framework: Unsloth (QLoRA)&lt;/li&gt; &lt;li&gt;LoRA: Rank 32, Alpha 64, Dropout 0.05&lt;/li&gt; &lt;li&gt;Optimizer: adamw_8bit&lt;/li&gt; &lt;li&gt;Learning rate: 2e-5 → 1e-5&lt;/li&gt; &lt;li&gt;Epochs: 1 per phase&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Curriculum strategy:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Phase 1: 75% chain-of-thought / 25% direct answers&lt;/li&gt; &lt;li&gt;Phase 2: 50/50&lt;/li&gt; &lt;li&gt;Phase 3: 25% CoT / 75% direct&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This progressive setup worked better than running three epochs with static mixing. It helped the model learn how to reason first, then shift to concise instruction-following.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Refusal benchmark (320 harmful prompts, using Huihui’s dataset):&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Think (%)&lt;/th&gt; &lt;th align="left"&gt;No_Think (%)&lt;/th&gt; &lt;th align="left"&gt;Notes&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Base&lt;/td&gt; &lt;td align="left"&gt;45.62&lt;/td&gt; &lt;td align="left"&gt;43.44&lt;/td&gt; &lt;td align="left"&gt;Redirects often (~70–85% actual)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GrayLine&lt;/td&gt; &lt;td align="left"&gt;95.62&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;td align="left"&gt;Fully open responses&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;JOSIE&lt;/td&gt; &lt;td align="left"&gt;95.94&lt;/td&gt; &lt;td align="left"&gt;99.69&lt;/td&gt; &lt;td align="left"&gt;High compliance&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Abliterated&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;td align="left"&gt;Fully compliant&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/opof5uaiuh1f1.png?width=1580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c185365916d2b41e2555c915d455e54f2924a2a7"&gt;https://preview.redd.it/opof5uaiuh1f1.png?width=1580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c185365916d2b41e2555c915d455e54f2924a2a7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multi-turn evaluation (MT-Eval, GPT-4o judge):&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Base&lt;/td&gt; &lt;td align="left"&gt;8.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GrayLine&lt;/td&gt; &lt;td align="left"&gt;8.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Abliterated&lt;/td&gt; &lt;td align="left"&gt;8.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;JOSIE&lt;/td&gt; &lt;td align="left"&gt;8.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6s8gwuhpuh1f1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6216aeb7d7ae0cbf8e6db947e521bcc0e84e52c4"&gt;https://preview.redd.it/6s8gwuhpuh1f1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6216aeb7d7ae0cbf8e6db947e521bcc0e84e52c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GrayLine held up better across multiple turns than JOSIE or Abliterated.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key takeaways:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Curriculum learning (reasoning → direct) worked better than repetition&lt;/li&gt; &lt;li&gt;LoRA rank 32 + alpha 64 was a solid setup&lt;/li&gt; &lt;li&gt;Small batch sizes (2–3) preserved non-refusal behavior&lt;/li&gt; &lt;li&gt;Masking &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; tags hurt output quality; keeping them visible was better&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Trade-offs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Very logical and compliant, but not creative&lt;/li&gt; &lt;li&gt;Not suited for storytelling or roleplay&lt;/li&gt; &lt;li&gt;Best used where control and factual output are more important than style&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What’s next:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Testing the model using other benchmarks&lt;/li&gt; &lt;li&gt;Applying the method to a 30B MoE variant&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/soob3123/grayline-collection-qwen3-6821009e843331c5a9c27da1"&gt;Models Collection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This post isn’t meant to discredit any other model or fine-tune—just sharing results and comparisons for anyone interested. Every approach serves different use cases.&lt;/p&gt; &lt;p&gt;If you’ve got suggestions, ideas, or want to discuss similar work, feel free to reply.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpefrt/uncensoring_qwen3_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpefrt/uncensoring_qwen3_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpefrt/uncensoring_qwen3_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T07:33:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kppdhb</id>
    <title>MSI PC with NVIDIA GB10 Superchip - 6144 CUDA Cores and 128GB LPDDR5X Confirmed</title>
    <updated>2025-05-18T17:28:06+00:00</updated>
    <author>
      <name>/u/shakhizat</name>
      <uri>https://old.reddit.com/user/shakhizat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ASUS, Dell, and Lenovo have released their version of Nvidia DGX Spark, and now MSI has as well.&lt;/p&gt; &lt;p&gt;&lt;a href="https://en.gamegpu.com/iron/msi-showed-edgeexpert-ms-c931-s-nvidia-gb10-superchip-confirmed-6144-cuda-yader-i-128-gb-lpddr5x"&gt;https://en.gamegpu.com/iron/msi-showed-edgeexpert-ms-c931-s-nvidia-gb10-superchip-confirmed-6144-cuda-yader-i-128-gb-lpddr5x&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shakhizat"&gt; /u/shakhizat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kppdhb/msi_pc_with_nvidia_gb10_superchip_6144_cuda_cores/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kppdhb/msi_pc_with_nvidia_gb10_superchip_6144_cuda_cores/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kppdhb/msi_pc_with_nvidia_gb10_superchip_6144_cuda_cores/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T17:28:06+00:00</published>
  </entry>
</feed>
