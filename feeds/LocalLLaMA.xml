<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-17T10:36:28+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i340rd</id>
    <title>GPU Enclosure Experiences?</title>
    <updated>2025-01-17T00:39:15+00:00</updated>
    <author>
      <name>/u/ilovepolthavemybabie</name>
      <uri>https://old.reddit.com/user/ilovepolthavemybabie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry for the noob question, but will an eGPU enclosure work as well for LLM loading as it would for gaming?&lt;/p&gt; &lt;p&gt;I have a 4070ti incompatible with my PC (OEM XPS PSU can’t handle it). The card I have now is a 3060Ti. I got the 4070 so cheap that even with an enclosure it’d be less than avg used price. &lt;/p&gt; &lt;p&gt;If anyone has good/bad eGPU experience, that might sway me on keeping vs selling. It’s just been sitting in the box for awhile. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilovepolthavemybabie"&gt; /u/ilovepolthavemybabie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i340rd/gpu_enclosure_experiences/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i340rd/gpu_enclosure_experiences/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i340rd/gpu_enclosure_experiences/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T00:39:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1i31sxj</id>
    <title>How to use chat templates for multicharacter roleplays?</title>
    <updated>2025-01-16T22:54:17+00:00</updated>
    <author>
      <name>/u/martinerous</name>
      <uri>https://old.reddit.com/user/martinerous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have implemented my own roleplay front-end for KoboldCpp. In contrast to SillyTavern and BackyardAI, my approach is not character-centric but rather scenario-centric. Both AI and the user can control multiple characters, and AI makes its own choice of who should speak next.&lt;/p&gt; &lt;p&gt;At first, I did not even bother to figure out how to use chat templates. I just send a simple example dialogue to the LLM together with my scenario:&lt;/p&gt; &lt;p&gt;Bob: Hi!&lt;/p&gt; &lt;p&gt;Anna: Hello!&lt;/p&gt; &lt;p&gt;Then I launch the generation and poll the API to check for the result. I look for a valid `Character Name:` marker in the response and allow only the characters that are setup for AI control. If I receive one more character marker, I stop the generation to avoid the infamous &amp;quot;speaking for others&amp;quot; issue, and clean up the response to remove the unnecessary text.&lt;/p&gt; &lt;p&gt;I'm testing it now and even Llama 3.2 3B seems to work quite OK with this setup.&lt;/p&gt; &lt;p&gt;However, I've heard that some models benefit from system prompts, and, as I understand, to pass the system prompt to the model, I need to use a proper chat template for the specific model. &lt;/p&gt; &lt;p&gt;And now we come to the root of the problem. &lt;strong&gt;Chat templates seem to be centered on the idea of only two parties - the user and the assistant. I have more parties. How would I encode their messages in a chat template?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A naive approach would be to send the system prompt with the proper formatting for the template, and then just dump the entire accumulated context with the scenario, character descriptions and all the chat messages into a single &amp;quot;assistant&amp;quot; message and ignore the user part of the template completely. &lt;/p&gt; &lt;p&gt;But wouldn't this somehow make the model less smart and not obey the scenario as well as it would if I separate the chat messages and create a single assistant (or user) message for every character's reply? &lt;/p&gt; &lt;p&gt;What are the practical effects of the chat template on the inference quality? Is the chat template just a convenient wrapper to properly separate messages in more complex situations or does it actually improve the model's behavior?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martinerous"&gt; /u/martinerous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i31sxj/how_to_use_chat_templates_for_multicharacter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i31sxj/how_to_use_chat_templates_for_multicharacter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i31sxj/how_to_use_chat_templates_for_multicharacter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T22:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2nkui</id>
    <title>Do you think that LLMs can do better natural language translation than services like DeepL, GoogleTranslate, Microsoft Translate etc.?</title>
    <updated>2025-01-16T12:14:11+00:00</updated>
    <author>
      <name>/u/sassyhusky</name>
      <uri>https://old.reddit.com/user/sassyhusky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My personal experience (which could be very subjective) with these translators is that even regular old chat bots with not much prompt engineering already produce better results with translations. Is this really just an unpopular opinion?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sassyhusky"&gt; /u/sassyhusky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2nkui/do_you_think_that_llms_can_do_better_natural/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2nkui/do_you_think_that_llms_can_do_better_natural/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2nkui/do_you_think_that_llms_can_do_better_natural/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T12:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3d6t0</id>
    <title>Hugging Face Spaces make the perfect agent tools!</title>
    <updated>2025-01-17T10:09:40+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3d6t0/hugging_face_spaces_make_the_perfect_agent_tools/"&gt; &lt;img alt="Hugging Face Spaces make the perfect agent tools!" src="https://a.thumbs.redditmedia.com/2x8xoBKRakcFYgt6BTinie0kUaqgzqu8c3brufszWM0.jpg" title="Hugging Face Spaces make the perfect agent tools!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Figured out that you could use Gradio based spaces on the hub as tools for agents. I don't get why everyone isn't doing this.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/etq92tij3jde1.png?width=1092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=baf38c94e6240885d8d4d02953e16f9414a12a02"&gt;https://preview.redd.it/etq92tij3jde1.png?width=1092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=baf38c94e6240885d8d4d02953e16f9414a12a02&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Made a guide here: &lt;a href="https://huggingface.co/blog/burtenshaw/gradio-spaces-agent-tools"&gt;https://huggingface.co/blog/burtenshaw/gradio-spaces-agent-tools&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3d6t0/hugging_face_spaces_make_the_perfect_agent_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3d6t0/hugging_face_spaces_make_the_perfect_agent_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3d6t0/hugging_face_spaces_make_the_perfect_agent_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T10:09:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2e23v</id>
    <title>I used Kokoro-82M, Llama 3.2, and Whisper Small to build a real-time speech-to-speech chatbot that runs locally on my MacBook!</title>
    <updated>2025-01-16T01:57:31+00:00</updated>
    <author>
      <name>/u/tycho_brahes_nose_</name>
      <uri>https://old.reddit.com/user/tycho_brahes_nose_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2e23v/i_used_kokoro82m_llama_32_and_whisper_small_to/"&gt; &lt;img alt="I used Kokoro-82M, Llama 3.2, and Whisper Small to build a real-time speech-to-speech chatbot that runs locally on my MacBook!" src="https://external-preview.redd.it/ajBjajZ2YTFpOWRlMdVERFdEQKrY8cptLv00gyZBVqtju60x3iy8w-FpWSZ2.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7aa615b1ccbb81cee65b5735b41605e27fcb9ed" title="I used Kokoro-82M, Llama 3.2, and Whisper Small to build a real-time speech-to-speech chatbot that runs locally on my MacBook!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tycho_brahes_nose_"&gt; /u/tycho_brahes_nose_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yw01bva1i9de1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2e23v/i_used_kokoro82m_llama_32_and_whisper_small_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2e23v/i_used_kokoro82m_llama_32_and_whisper_small_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T01:57:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3bieb</id>
    <title>Models for shorter context</title>
    <updated>2025-01-17T07:58:56+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recent trends have been to push for larger context windows and to compensate for the ballooning VRAM and compute costs of longer contexts by using techniques such as GQA etc.&lt;/p&gt; &lt;p&gt;But let's say you have a task that requires only 4k or 8k of context. And you want to have the best performance possible for this context size.&lt;/p&gt; &lt;p&gt;Are there models that perform better within this limited context or a way of tuning existing models to perform better with a 4k or 8k context window?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3bieb/models_for_shorter_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3bieb/models_for_shorter_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3bieb/models_for_shorter_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T07:58:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i29wz5</id>
    <title>Google just released a new architecture</title>
    <updated>2025-01-15T22:38:26+00:00</updated>
    <author>
      <name>/u/FeathersOfTheArrow</name>
      <uri>https://old.reddit.com/user/FeathersOfTheArrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like a big deal? &lt;a href="https://x.com/behrouz_ali/status/1878859086227255347"&gt;Thread by lead author&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeathersOfTheArrow"&gt; /u/FeathersOfTheArrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2501.00663"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i29wz5/google_just_released_a_new_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i29wz5/google_just_released_a_new_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T22:38:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2ucxu</id>
    <title>I created a vscode extension that does inline edits using deepseek</title>
    <updated>2025-01-16T17:34:09+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ucxu/i_created_a_vscode_extension_that_does_inline/"&gt; &lt;img alt="I created a vscode extension that does inline edits using deepseek" src="https://external-preview.redd.it/c2Y1NHdiamk1ZWRlMSXLRLoBTWH7BkELeo8cMATHejXfU-O8HPWWGk2XwKZI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=822e126be45f676ab516da632460316140e9e985" title="I created a vscode extension that does inline edits using deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wo2fucji5ede1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ucxu/i_created_a_vscode_extension_that_does_inline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ucxu/i_created_a_vscode_extension_that_does_inline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T17:34:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2pbyp</id>
    <title>Seems like used 3090 price is up near $850/$900?</title>
    <updated>2025-01-16T13:50:36+00:00</updated>
    <author>
      <name>/u/Synaps3</name>
      <uri>https://old.reddit.com/user/Synaps3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for a bit of a sanity check here; it seems like used 3090's on eBay are up from around $650-$700 two weeks ago to $850-$1000 depending on the model after the disappointing 5090 announcement. Is this still a decent value proposition for an inference box? I'm about to pull the trigger on an H12SSL-i, but am on the fence about whether to wait for a potentially non-existent price drop on 3090 after 5090's are actually available and people try to flip their current cards. Short term goal is 70b Q4 inference server and NVLink for training non-language models. Any thoughts from secondhand GPU purchasing veterans?&lt;/p&gt; &lt;p&gt;Edit: also, does anyone know how long NVIDIA tends to provide driver support for their cards? I read somehow that 3090s inherit A100 driver support but I haven't been able to find any verification of this. It'd be a shame to buy two and have them be end-of-life in a year or two.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Synaps3"&gt; /u/Synaps3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2pbyp/seems_like_used_3090_price_is_up_near_850900/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2pbyp/seems_like_used_3090_price_is_up_near_850900/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2pbyp/seems_like_used_3090_price_is_up_near_850900/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T13:50:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3csqz</id>
    <title>Thinking about finetuning an SLM (i.e 0.5B, 2B) for PII a as a way to learn. Worth the shot?</title>
    <updated>2025-01-17T09:39:23+00:00</updated>
    <author>
      <name>/u/GeorgiaWitness1</name>
      <uri>https://old.reddit.com/user/GeorgiaWitness1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3csqz/thinking_about_finetuning_an_slm_ie_05b_2b_for/"&gt; &lt;img alt="Thinking about finetuning an SLM (i.e 0.5B, 2B) for PII a as a way to learn. Worth the shot?" src="https://b.thumbs.redditmedia.com/PH94xm8eiN7x1NwNJ3IFuRCx7gAYOvWPtZT0_42KVOM.jpg" title="Thinking about finetuning an SLM (i.e 0.5B, 2B) for PII a as a way to learn. Worth the shot?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! &lt;/p&gt; &lt;p&gt;I posted something similar a few months ago, but after evaluating the quality of the new SLM models, I think it would make sense to undertake a project to finetune a model specifically for PII. Additionally, perhaps developing a Docker container with a complete solution incorporating the model, agentic behavior, and possibly &lt;a href="https://microsoft.github.io/presidio/"&gt;Presidio&lt;/a&gt; could be beneficial.&lt;/p&gt; &lt;p&gt;Could be a good way to learn all the finetuning pipeline with &lt;a href="https://unsloth.ai/"&gt;unsloth&lt;/a&gt;?&lt;/p&gt; &lt;p&gt;Tell me what you think. Thank you!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wrdfedt0yide1.png?width=3190&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98842a4696a8cf4ac8780dc0749565e32856bfb1"&gt;https://preview.redd.it/wrdfedt0yide1.png?width=3190&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98842a4696a8cf4ac8780dc0749565e32856bfb1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GeorgiaWitness1"&gt; /u/GeorgiaWitness1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3csqz/thinking_about_finetuning_an_slm_ie_05b_2b_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3csqz/thinking_about_finetuning_an_slm_ie_05b_2b_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3csqz/thinking_about_finetuning_an_slm_ie_05b_2b_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T09:39:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i30yy4</id>
    <title>Where do people get news about upcoming LLM releases?</title>
    <updated>2025-01-16T22:17:01+00:00</updated>
    <author>
      <name>/u/gamblingapocalypse</name>
      <uri>https://old.reddit.com/user/gamblingapocalypse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m curious about how people stay up-to-date on news about upcoming LLM releases, especially ones that haven’t been released yet. Are there specific websites, forums, newsletters, or communities you follow to learn about this kind of stuff?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gamblingapocalypse"&gt; /u/gamblingapocalypse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i30yy4/where_do_people_get_news_about_upcoming_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i30yy4/where_do_people_get_news_about_upcoming_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i30yy4/where_do_people_get_news_about_upcoming_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T22:17:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i37x87</id>
    <title>Whats the current State-of-The-Art for voice cloning?</title>
    <updated>2025-01-17T04:02:10+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;last time i checked which was quite a while voice cloning and like making AI song covers and etc used RVC v2 but im sure a LOT has changed since then Ive heard a lot of stuff about tts models like the new 82M model but i dont think ive heard specifically about voice cloning and cover tools&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i37x87/whats_the_current_stateoftheart_for_voice_cloning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i37x87/whats_the_current_stateoftheart_for_voice_cloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i37x87/whats_the_current_stateoftheart_for_voice_cloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T04:02:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3dcxz</id>
    <title>Table extraction from Finance PDF's</title>
    <updated>2025-01-17T10:22:21+00:00</updated>
    <author>
      <name>/u/Maleficent_Repair359</name>
      <uri>https://old.reddit.com/user/Maleficent_Repair359</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;By any chance is there any way which is 100% accurate to extract tabular data from finance pdfs which basically contains balance sheets and tables.&lt;/p&gt; &lt;p&gt;I tried everything pytesseract , camelot , tabula , Microsoft table transformer , but there isnt any accuracy with proper headers , empty columns.&lt;/p&gt; &lt;p&gt;I even tried openai's assistant api with code_interpreter as tool but that also lacks with the accuracy.&lt;/p&gt; &lt;p&gt;Anyone has ever tried to work on this solution ??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent_Repair359"&gt; /u/Maleficent_Repair359 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3dcxz/table_extraction_from_finance_pdfs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3dcxz/table_extraction_from_finance_pdfs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3dcxz/table_extraction_from_finance_pdfs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T10:22:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1i357ov</id>
    <title>4x AMD Instinct AI Server + Mistral 7B + vLLM</title>
    <updated>2025-01-17T01:38:35+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i357ov/4x_amd_instinct_ai_server_mistral_7b_vllm/"&gt; &lt;img alt="4x AMD Instinct AI Server + Mistral 7B + vLLM" src="https://external-preview.redd.it/OXZzbzY0dmNrZ2RlMYrnczNrVsQkdH3BrjnNDBSvBen7AmAirsnxCxjuWUYQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1928bed39a3edac8783d4face60cb49bffa9a3a0" title="4x AMD Instinct AI Server + Mistral 7B + vLLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1sni53vckgde1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i357ov/4x_amd_instinct_ai_server_mistral_7b_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i357ov/4x_amd_instinct_ai_server_mistral_7b_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T01:38:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2p6n3</id>
    <title>Why can't GPUs have removable memory like PC ram?</title>
    <updated>2025-01-16T13:43:00+00:00</updated>
    <author>
      <name>/u/Delicious-Farmer-234</name>
      <uri>https://old.reddit.com/user/Delicious-Farmer-234</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was thinking, why doesn't Intel, Nvidia, or AMD come up with the idea of being able to expand the memory? I get it that DDR6 is pricey but if one of them were to create modules and sell them wouldn't they be able to profit? Image if Intel came out with this first, I bet most of us will max out the vram and the whole community will push away from Nvidia and create better or comparable frameworks other cuda. Thoughts ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious-Farmer-234"&gt; /u/Delicious-Farmer-234 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2p6n3/why_cant_gpus_have_removable_memory_like_pc_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2p6n3/why_cant_gpus_have_removable_memory_like_pc_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2p6n3/why_cant_gpus_have_removable_memory_like_pc_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T13:43:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3cdws</id>
    <title>"Can't live without tool" for LLM datasets?</title>
    <updated>2025-01-17T09:07:39+00:00</updated>
    <author>
      <name>/u/Secure_Archer_1529</name>
      <uri>https://old.reddit.com/user/Secure_Archer_1529</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I thought it would be interesting to know what tool people absolutely love using when it comes to LLM training - more specifically creating and preparing datasets?&lt;/p&gt; &lt;p&gt;Also, feel free to just share any knowledge you feel is a &amp;quot;cheatsheet&amp;quot; or too good to be true?&lt;/p&gt; &lt;p&gt;Have a great weekend!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Archer_1529"&gt; /u/Secure_Archer_1529 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3cdws/cant_live_without_tool_for_llm_datasets/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3cdws/cant_live_without_tool_for_llm_datasets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3cdws/cant_live_without_tool_for_llm_datasets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T09:07:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i35hs3</id>
    <title>My Tesla P40 just caught on fire and exploded… help?</title>
    <updated>2025-01-17T01:52:52+00:00</updated>
    <author>
      <name>/u/Cressio</name>
      <uri>https://old.reddit.com/user/Cressio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://imgur.com/a/1ViaFVL"&gt;https://imgur.com/a/1ViaFVL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Um… so, this GPU has an insanely long lore. To summarize, I ended up trying to sell it, UPS ravaged the box and the buyer claimed the GPU didn’t work anymore (wouldn’t power on), I received it back, tried to power it up, and it immediately caught on fire in catastrophic fashion and shot flames into my motherboard. &lt;/p&gt; &lt;p&gt;I’m powering them with a good quality PCIe to EPS adapter, which I just used again to try and check if it was indeed dead. Well, it sure as hell is now.&lt;/p&gt; &lt;p&gt;Uh, what the hell happened? What is the component that exploded? It looks to be power related and it had a thermal pad on the backplate that is now scorched. &lt;/p&gt; &lt;p&gt;I actually have ANOTHER P40 from this shipment that I’m wanting to test and I’m absolutely mortified to plug it in now. I don’t think I’ll ever trust a PC build again.&lt;/p&gt; &lt;p&gt;Edit: just to super clarify, these P40s worked before with this exact same setup and adapters. It just… happened to explode this time for some reason. System still powers on just fine without the GPU, thank god.&lt;/p&gt; &lt;p&gt;Edit 2: I was right, the other one works and he claimed both didn’t. I bet the one that just exploded worked too. Fuck. My life. The one that’s powered on right now is using the other (I had 2) adapter and a different PCIe cable. I am so absolutely terrified to try the other cables. This is almost a worst outcome than them both being DOA. Now I don’t even know what to do or trust lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cressio"&gt; /u/Cressio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i35hs3/my_tesla_p40_just_caught_on_fire_and_exploded_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i35hs3/my_tesla_p40_just_caught_on_fire_and_exploded_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i35hs3/my_tesla_p40_just_caught_on_fire_and_exploded_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T01:52:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2y810</id>
    <title>Is DeepSeek V3 overhyped?</title>
    <updated>2025-01-16T20:17:06+00:00</updated>
    <author>
      <name>/u/YourAverageDev0</name>
      <uri>https://old.reddit.com/user/YourAverageDev0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have been using DeepSeek V3 for some time after all the time it came out. Coding wise (I work on web frontend, mostly react/svelte), I do not find it nearly as impressive as 3.5 Sonnet. The benchmarks seems to be matching, but the feel is just different, sometimes DeepSeek does give interesting stuff when asked. For me personally, it feels like a base 405B that has even been further scaled, it has little scars of brutal human RLHF (unlike OAI, LLaMa and etc Models). It just doesn't have that taste of Claude 3.5 Sonnet.&lt;/p&gt; &lt;p&gt;Curious what you guys think&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YourAverageDev0"&gt; /u/YourAverageDev0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2y810/is_deepseek_v3_overhyped/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2y810/is_deepseek_v3_overhyped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2y810/is_deepseek_v3_overhyped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T20:17:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2qokt</id>
    <title>Introducing Kokoro.js: a new JavaScript library for running Kokoro TTS (82M) locally in the browser w/ WASM.</title>
    <updated>2025-01-16T14:55:36+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2qokt/introducing_kokorojs_a_new_javascript_library_for/"&gt; &lt;img alt="Introducing Kokoro.js: a new JavaScript library for running Kokoro TTS (82M) locally in the browser w/ WASM." src="https://external-preview.redd.it/c2Y2dHB4cGdkZGRlMblxftDnj1ubBLQxBS031TPNonm7GOuytqVIBIDUD3XU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b475390d835ebe3032e27539598bb0f968273c4" title="Introducing Kokoro.js: a new JavaScript library for running Kokoro TTS (82M) locally in the browser w/ WASM." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uv6trvpgddde1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2qokt/introducing_kokorojs_a_new_javascript_library_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2qokt/introducing_kokorojs_a_new_javascript_library_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T14:55:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3b1jb</id>
    <title>New framework aims to mimic human thinking for writing long-form content (OmniThink)</title>
    <updated>2025-01-17T07:22:39+00:00</updated>
    <author>
      <name>/u/emanuilov</name>
      <uri>https://old.reddit.com/user/emanuilov</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3b1jb/new_framework_aims_to_mimic_human_thinking_for/"&gt; &lt;img alt="New framework aims to mimic human thinking for writing long-form content (OmniThink)" src="https://external-preview.redd.it/P3wPulsj-vbHIfL8pdoJemWboTREaTu--SoaotPYjzU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=098b6934967a73dbc796419d5bd3b3397ed04814" title="New framework aims to mimic human thinking for writing long-form content (OmniThink)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing a paper about OmniThink - an approach that tries to replicate how humans write long-form content. The framework focuses on continuous reflection and exploration, similar to how we gather information and refine our understanding when writing detailed articles.&lt;/p&gt; &lt;p&gt;(Not affiliated with the authors)&lt;/p&gt; &lt;p&gt;The paper's style reminds me of Google Deep Research's functionality. I couldn't get their online demo to work, but the ideas in the paper are worth checking out, IMO. I will spend some time on their repo to see if that will work out of the box.&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://huggingface.co/papers/2501.09751"&gt;https://huggingface.co/papers/2501.09751&lt;/a&gt;&lt;br /&gt; Project page: &lt;a href="https://zjunlp.github.io/project/OmniThink/"&gt;https://zjunlp.github.io/project/OmniThink/&lt;/a&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/zjunlp/OmniThink"&gt;https://github.com/zjunlp/OmniThink&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/alrt6fyh9ide1.png?width=3875&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a41e77eac565e5bf61deeaae9c0de535fb45feb"&gt;https://preview.redd.it/alrt6fyh9ide1.png?width=3875&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a41e77eac565e5bf61deeaae9c0de535fb45feb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emanuilov"&gt; /u/emanuilov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3b1jb/new_framework_aims_to_mimic_human_thinking_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3b1jb/new_framework_aims_to_mimic_human_thinking_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3b1jb/new_framework_aims_to_mimic_human_thinking_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T07:22:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2n0il</id>
    <title>How would you build an LLM agent application without using LangChain?</title>
    <updated>2025-01-16T11:37:48+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2n0il/how_would_you_build_an_llm_agent_application/"&gt; &lt;img alt="How would you build an LLM agent application without using LangChain?" src="https://preview.redd.it/q1d445cdecde1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d53957b0b5cd83b245d383aec699f6fb075f1d50" title="How would you build an LLM agent application without using LangChain?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q1d445cdecde1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2n0il/how_would_you_build_an_llm_agent_application/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2n0il/how_would_you_build_an_llm_agent_application/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T11:37:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3cxm0</id>
    <title>Korea AI Chip - DEEPX NPU . Price? Under 50$ . Better that GPU?</title>
    <updated>2025-01-17T09:50:04+00:00</updated>
    <author>
      <name>/u/bi4key</name>
      <uri>https://old.reddit.com/user/bi4key</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3cxm0/korea_ai_chip_deepx_npu_price_under_50_better/"&gt; &lt;img alt="Korea AI Chip - DEEPX NPU . Price? Under 50$ . Better that GPU?" src="https://external-preview.redd.it/P-NYeqvu1eSDu3ZqaeWja5plMoJW5E-Wg-nWjjs5CuU.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80557a815c31a1454adfc473c446fa4d55b69fdb" title="Korea AI Chip - DEEPX NPU . Price? Under 50$ . Better that GPU?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. &lt;/p&gt; &lt;p&gt;This will be game changer? Better that GPU?&lt;/p&gt; &lt;p&gt;DEEPX NPU. Edge Computing&lt;/p&gt; &lt;p&gt;Website: &lt;a href="https://deepx.ai/"&gt;https://deepx.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bi4key"&gt; /u/bi4key &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/5aJNJLRsVlk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3cxm0/korea_ai_chip_deepx_npu_price_under_50_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3cxm0/korea_ai_chip_deepx_npu_price_under_50_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T09:50:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2t82i</id>
    <title>Introducing Wayfarer: a brutally challenging roleplay model trained to let you fail and die.</title>
    <updated>2025-01-16T16:46:20+00:00</updated>
    <author>
      <name>/u/Nick_AIDungeon</name>
      <uri>https://old.reddit.com/user/Nick_AIDungeon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One frustration we’ve heard from many AI Dungeon players is that AI models are too nice, never letting them fail or die. So we decided to fix that. We trained a model we call Wayfarer where adventures are much more challenging with failure and death happening frequently.&lt;/p&gt; &lt;p&gt;We released it on AI Dungeon several weeks ago and players loved it, so we’ve decided to open source the model for anyone to experience unforgivingly brutal AI adventures!&lt;/p&gt; &lt;p&gt;Would love to hear your feedback as we plan to continue to improve and open source similar models.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LatitudeGames/Wayfarer-12B"&gt;https://huggingface.co/LatitudeGames/Wayfarer-12B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nick_AIDungeon"&gt; /u/Nick_AIDungeon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2t82i/introducing_wayfarer_a_brutally_challenging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2t82i/introducing_wayfarer_a_brutally_challenging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2t82i/introducing_wayfarer_a_brutally_challenging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T16:46:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i31ji5</id>
    <title>What is ElevenLabs doing? How is it so good?</title>
    <updated>2025-01-16T22:42:26+00:00</updated>
    <author>
      <name>/u/Independent_Aside225</name>
      <uri>https://old.reddit.com/user/Independent_Aside225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically the title. What's their trick? On everything but voice, local models are pretty good for what they are, but ElevenLabs just blows everyone out of the water. &lt;/p&gt; &lt;p&gt;Is it full Transformer? Some sort of Diffuser? Do they model the human anatomy to add accuracy to the model? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent_Aside225"&gt; /u/Independent_Aside225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i31ji5/what_is_elevenlabs_doing_how_is_it_so_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i31ji5/what_is_elevenlabs_doing_how_is_it_so_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i31ji5/what_is_elevenlabs_doing_how_is_it_so_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T22:42:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3as1m</id>
    <title>OpenWebUI Canvas Implementation -- Coming Soon! (Better Artifacts)</title>
    <updated>2025-01-17T07:02:43+00:00</updated>
    <author>
      <name>/u/maxwell321</name>
      <uri>https://old.reddit.com/user/maxwell321</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3as1m/openwebui_canvas_implementation_coming_soon/"&gt; &lt;img alt="OpenWebUI Canvas Implementation -- Coming Soon! (Better Artifacts)" src="https://a.thumbs.redditmedia.com/11a6AQbm8PHTqUzymosrsOz6WrQ1h5fnohaqQF7icF0.jpg" title="OpenWebUI Canvas Implementation -- Coming Soon! (Better Artifacts)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ytezb1q05ide1.png?width=1862&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=93364222443da5f695a745265842c91ee604d9e5"&gt;C# and XML View&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1ttzjm4s5ide1.png?width=1862&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd00eb16ef20e090d9f5ebee0d69f48c4f3b8bf0"&gt;Design View&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7tj92xav5ide1.png?width=1749&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=81d8f9dec9bd3575fb4fc4ea8d399627b2eacd4a"&gt;Code View&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi all! I'm implementing Canvas (beefing up Artifacts) on OpenWebUI.&lt;/p&gt; &lt;p&gt;This was my only issue ever with OpenWebUI, just the very limited canvas feature (only restricted to HTML, CSS, JavaScript and SVG).&lt;/p&gt; &lt;p&gt;I've expanded support for the following languages:&lt;/p&gt; &lt;p&gt;C#, Python, Java, PHP, Ruby, Bash, Shell, AppleScript, SQL, JSON, XML, YAML, Markdown, HTML&lt;/p&gt; &lt;p&gt;If I'm missing one feel free to comment it! It's super easy to add at this point.&lt;/p&gt; &lt;p&gt;Another notable feature I'm adding is to switch between Design view and Code view for web design.&lt;/p&gt; &lt;p&gt;I'm super close to finishing! I just need to clean it up and visualize/track changes between revisions. Expect my pull request it in the next couple of weeks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxwell321"&gt; /u/maxwell321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3as1m/openwebui_canvas_implementation_coming_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3as1m/openwebui_canvas_implementation_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3as1m/openwebui_canvas_implementation_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T07:02:43+00:00</published>
  </entry>
</feed>
