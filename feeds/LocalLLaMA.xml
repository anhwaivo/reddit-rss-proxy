<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-24T15:24:01+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1my39ja</id>
    <title>It's Mamba time: Comparing Nemotron Nano v2 vs Falcon-H1 vs Qwen (og) vs Qwen (2507)</title>
    <updated>2025-08-23T14:42:37+00:00</updated>
    <author>
      <name>/u/kryptkpr</name>
      <uri>https://old.reddit.com/user/kryptkpr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my39ja/its_mamba_time_comparing_nemotron_nano_v2_vs/"&gt; &lt;img alt="It's Mamba time: Comparing Nemotron Nano v2 vs Falcon-H1 vs Qwen (og) vs Qwen (2507)" src="https://external-preview.redd.it/-5dEDvOvZEMy2pHuEuazSJpFNmFIHXICPgHs74NtU5U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47bcb8449e292f736972fc476ddb801eee0e77e6" title="It's Mamba time: Comparing Nemotron Nano v2 vs Falcon-H1 vs Qwen (og) vs Qwen (2507)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the recent release of not one but two transformers-mamba hybrids both claiming to outperform baseline transformers, I thought this would be a fun application of ReasonScape to see what's going on under the hood.&lt;/p&gt; &lt;h1&gt;Test Model 1: Falcon-H1 7B&lt;/h1&gt; &lt;p&gt;Blog: &lt;a href="https://falcon-lm.github.io/blog/falcon-h1/"&gt;https://falcon-lm.github.io/blog/falcon-h1/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct"&gt;https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7i2z9yciyrkf1.png?width=683&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1d03fc28117947e2313a514e051fabba3e01682"&gt;Claim: Falcon-7B (61.8) outperforms Qwen3-8B (58.5)&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Test Model 2: NVidia Nemotron Nano v2&lt;/h1&gt; &lt;p&gt;Blog: &lt;a href="https://research.nvidia.com/labs/adlr/NVIDIA-Nemotron-Nano-2/"&gt;https://research.nvidia.com/labs/adlr/NVIDIA-Nemotron-Nano-2/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2"&gt;https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ao6fzh5tyrkf1.png?width=2304&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fb457ae99043c267682b39ce4c29581daa1f7e64"&gt;Claim: Nemotron-Nano-9B outperforms Qwen3-8B across the board&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Reference Model 1: Qwen3-8B OG&lt;/h1&gt; &lt;p&gt;Blog: &lt;a href="https://qwenlm.github.io/blog/qwen3/"&gt;https://qwenlm.github.io/blog/qwen3/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Qwen/Qwen3-8B"&gt;https://huggingface.co/Qwen/Qwen3-8B&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Reference Model 2: Qwen3-4B-2507-Instruct&lt;/h1&gt; &lt;p&gt;Blog: &lt;a href="https://qwen3lm.com/qwen3-4b-instruct-2507/"&gt;https://qwen3lm.com/qwen3-4b-instruct-2507/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Test Setup&lt;/h1&gt; &lt;p&gt;All models were evaluated with 2x RTX3090 using vLLM 0.10.1&lt;/p&gt; &lt;p&gt;Nemotron Nano v2 was launched with the recommended &lt;code&gt;--mamba_ssm_cache_dtype float32&lt;/code&gt; flag.&lt;/p&gt; &lt;p&gt;The evaluation being performed here is one of my design: ReasonScape M6. See &lt;a href="https://reasonscape.com/"&gt;https://reasonscape.com/&lt;/a&gt; for details and documentation.&lt;/p&gt; &lt;h1&gt;Results: Difficulty Tiered Leaderboards&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cfscchg50skf1.png?width=1137&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d81f8f61ee585eca5e9dd8eb9283e3382f3fce9"&gt;Hybrid-SSM Results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Nemotron Nano v2 demonstrates &lt;strong&gt;significantly improved all-around complexity robustness&lt;/strong&gt; over Falcon-H1, but it does as the expense of &lt;strong&gt;3x thinking tokens.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1x226ztf0skf1.png?width=1136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3126d6e6fdd0133a5ba248d069748c2df46aa1ef"&gt;Qwen3 Results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Performance on the &lt;strong&gt;Boolean, Dates&lt;/strong&gt; and &lt;strong&gt;Movies&lt;/strong&gt; tasks (see &lt;a href="https://reasonscape.com/docs/tasks/"&gt;https://reasonscape.com/docs/tasks/&lt;/a&gt; for more info on the tasks!) is indeed comparable but the &lt;strong&gt;Objects&lt;/strong&gt;, &lt;strong&gt;Arithmetic&lt;/strong&gt; and &lt;strong&gt;Shuffle&lt;/strong&gt; tasks present significant challenges for the hybrids. &lt;/p&gt; &lt;p&gt;The old Qwen3 models &lt;strong&gt;think way too much&lt;/strong&gt; but the new 2507-Instruct do really well when simply asked to &lt;em&gt;&amp;quot;think-step-by-step&amp;quot;.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Results: Performance Surfaces&lt;/h1&gt; &lt;p&gt;I will merge the Test and Reference sets together for the remainder of plots to make comparisons easier:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o264zvgb1skf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63420e7384da7c0f4dd3a3387a2023cf1e67f804"&gt;ReasonScape M6 Difficulty Manifolds for the 4 models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Nemotron &lt;strong&gt;Dates&lt;/strong&gt; processing is robust but &lt;strong&gt;Objects&lt;/strong&gt; (a selective attention task) collapses in both difficulty dimensions very quickly compared to pure transformers. &lt;strong&gt;Arithmetic&lt;/strong&gt; (under randomized whitespace conditions) holds up ok with depth, but collapses under length. &lt;strong&gt;Shuffle&lt;/strong&gt; (a working memory churn task) shows a similar pattern: depth is ok, but total collapse under length leading to a smaller island of competency.&lt;/p&gt; &lt;p&gt;All models struggled with truncation on the &lt;strong&gt;Boolean&lt;/strong&gt; task, but Falcon least so.&lt;/p&gt; &lt;h1&gt;Results: Token-FFT Analysis&lt;/h1&gt; &lt;p&gt;ReasonScape offers a unique kind of plot, showing exactly how chat template and tokenization affect the frequency-domain representation of what the LLM actually sees.&lt;/p&gt; &lt;p&gt;These allow to peek even below the surfaces and understand WHY some things are tougher for certain models and split training problems from architectural problems.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4nqoy43d2skf1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=acd11dcdc896c0392529a2f172bcdaeb7334f04a"&gt;Token-FFT: Arithmetic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here we see exactly why Nemotron isn't very good at arithmetic:&lt;/p&gt; &lt;p&gt;- The whitespace/no-whitespace representations of math problems look VERY different to this tokenizer and it has had trouble generalizing as a result&lt;/p&gt; &lt;p&gt;- As length increases, the information content .. disappears! No change at DC, but the middle and high-band information is lost. Performance predictably collapses as a result.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8c0zoiv73skf1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9374f97bf696d29d40084700b219e41e7a7ed8a1"&gt;Token-FFT: Boolean&lt;/a&gt;&lt;/p&gt; &lt;p&gt;An interesting comparison here is the Boolean task which demonstrates similar information-compression along with the ON/OFF and YES/NO formats. These formats have the weakest results on the surfaces compared to the others (because at the end of the day, compressing your signal is bad) but they manage to eek out &amp;quot;satisfactory&amp;quot; scores because the DC had a corresponding upward shift. This is a 'lower-tier of information loss' vs when the DC stays the same and we just lose signal.&lt;/p&gt; &lt;h1&gt;Conclusions&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Nemotron Nano is the most powerful hybrid I've evaluated so far.&lt;/strong&gt; It's major weakness is that it seems to have failed to generalize Arithmetic and it's selective attention (information-filtering ability) is noticeably weaker then SOTA transformers. Mid-tier for reasoning length.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;While Hybrids are getting better, they don't yet beat pure Transformers&lt;/strong&gt; when I evaluated Falcon-Mamba it got a big fat 0 - these new hybrid guys actually do work and are getting better with each iteration. I hope to see this conclusion flip in the future!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-4B-Instruct-2507 is a little beast&lt;/strong&gt; and can replace older 8B with similar if not better performance and lower token usage.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I need more RTX3090&lt;/strong&gt; as these evaluations require up to 100M tokens when the average responses get up to 3-4k.&lt;/p&gt; &lt;h1&gt;Resources&lt;/h1&gt; &lt;p&gt;To learn more about ReasonScape evaluations check out the Documentation at &lt;a href="https://reasonscape.com/docs/"&gt;https://reasonscape.com/docs/&lt;/a&gt; or grab the latest code from GitHub at &lt;a href="https://github.com/the-crypt-keeper/reasonscape"&gt;https://github.com/the-crypt-keeper/reasonscape&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you enjoyed the plots, check out the M6 explorer &lt;a href="https://reasonscape.com/m6/explorer/"&gt;https://reasonscape.com/m6/explorer/&lt;/a&gt; and it's documentation &lt;a href="https://reasonscape.com/docs/tools/explorer/"&gt;https://reasonscape.com/docs/tools/explorer/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2hwrdrug6skf1.png?width=1848&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a5d69ab1018467ca9ef8445d022dd76df0c73544"&gt;M6 explorer showing detailed result projections along the Arithmetic surface&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To see how these models compare to the rest of the flocks, the full M6 Leaderboard is available at &lt;a href="https://reasonscape.com/m6/leaderboard/"&gt;https://reasonscape.com/m6/leaderboard/&lt;/a&gt; (spoiler: &lt;strong&gt;GPT-OSS-20b is a broken mess&lt;/strong&gt;) with documentation at &lt;a href="https://reasonscape.com/docs/tools/leaderboard/"&gt;https://reasonscape.com/docs/tools/leaderboard/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for reading! &amp;lt;3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kryptkpr"&gt; /u/kryptkpr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my39ja/its_mamba_time_comparing_nemotron_nano_v2_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my39ja/its_mamba_time_comparing_nemotron_nano_v2_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1my39ja/its_mamba_time_comparing_nemotron_nano_v2_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T14:42:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1myvpqv</id>
    <title>Open Source Tool for Manga translation</title>
    <updated>2025-08-24T13:33:30+00:00</updated>
    <author>
      <name>/u/New_Blueberry9858</name>
      <uri>https://old.reddit.com/user/New_Blueberry9858</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are some paid tools for manga translation, like INKR studio, but turns out to be pretty expensive. Thus our team at curify-ai worked on our custom manga translation tool and decided to open source the prototype at : &lt;a href="https://huggingface.co/spaces/Curify/manga_translation"&gt;https://huggingface.co/spaces/Curify/manga_translation&lt;/a&gt; &lt;/p&gt; &lt;p&gt;The prototype features the following:&lt;br /&gt; a. Horizontally cropping skinny manga images to improve its visibility.&lt;/p&gt; &lt;p&gt;b. Using PaddleOCR to detect text and use a polygon based approach for inpaint. Still need to improve OCR and inpainting method, Qwen might be a good candidate.&lt;/p&gt; &lt;p&gt;c. Translate with Microsoft translator and allow customization of translated text.&lt;/p&gt; &lt;p&gt;d. Render the translated image.&lt;/p&gt; &lt;p&gt;It's still work in progress, welcome to use and suggest improvements. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Blueberry9858"&gt; /u/New_Blueberry9858 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myvpqv/open_source_tool_for_manga_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myvpqv/open_source_tool_for_manga_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myvpqv/open_source_tool_for_manga_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T13:33:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mywo8s</id>
    <title>Recommendations for using a Ryzen 5 PRO 4650U for Code Assistant</title>
    <updated>2025-08-24T14:13:43+00:00</updated>
    <author>
      <name>/u/pablo2m</name>
      <uri>https://old.reddit.com/user/pablo2m</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I have some experience using Ollam and running models locally. Lately, I've been experimenting with code agents and would like to run one on a secondary machine. I know it won't be fast, but I really wouldn't mind keeping it busy and checking what it did when it's finished. I have a Ryzen 5 PRO 4650U notebook with 32GB of RAM, and I have experience using Linux if necessary. What software do you recommend to try to take advantage of the processor's capabilities?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pablo2m"&gt; /u/pablo2m &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mywo8s/recommendations_for_using_a_ryzen_5_pro_4650u_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mywo8s/recommendations_for_using_a_ryzen_5_pro_4650u_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mywo8s/recommendations_for_using_a_ryzen_5_pro_4650u_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T14:13:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1myxkd9</id>
    <title>Free Preview of Qoder from Alibaba: The Future of Agentic Coding?</title>
    <updated>2025-08-24T14:49:32+00:00</updated>
    <author>
      <name>/u/NoobMLDude</name>
      <uri>https://old.reddit.com/user/NoobMLDude</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did a deeper look into Qoder - the new Agentic Coding Platform from Alibaba.&lt;br /&gt; Check it out if you like: &lt;a href="https://youtu.be/4Zipfp4qdV4"&gt;https://youtu.be/4Zipfp4qdV4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What I liked:&lt;br /&gt; - It does what developer don't like to do like writing detailed wiki and docs (Repo Wiki feature).&lt;br /&gt; - Before implementing any feature it writes a detailed spec about the feature, takes feedback from developer, updates the spec. (just like Devs use RFC before implementing a feature)&lt;br /&gt; - It creates a semantic representation of the code, to find the appropriate context to be used for context engineering.&lt;br /&gt; - Long-term memory that evolves based on developer preferences, coding styles, past choices.&lt;/p&gt; &lt;p&gt;What I didn't like:&lt;br /&gt; - It's only Free during preview. Wish it was Free forever (Can't be greedy :-D )&lt;/p&gt; &lt;p&gt;- Couldn't get Quest mode to work.&lt;/p&gt; &lt;p&gt;- Couldn't get the Free Web Search to work.&lt;/p&gt; &lt;p&gt;I really liked the Repo Wiki and Spec feature in Quest Mode and I'll try to generate a wiki for all my projects during the free preview ;-)&lt;/p&gt; &lt;p&gt;Did you try it? What are your impressions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoobMLDude"&gt; /u/NoobMLDude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myxkd9/free_preview_of_qoder_from_alibaba_the_future_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myxkd9/free_preview_of_qoder_from_alibaba_the_future_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myxkd9/free_preview_of_qoder_from_alibaba_the_future_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T14:49:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1my3why</id>
    <title>RTX PRO 6000 MAX-Q Blackwell for LLM</title>
    <updated>2025-08-23T15:08:27+00:00</updated>
    <author>
      <name>/u/AdventurousSwim1312</name>
      <uri>https://old.reddit.com/user/AdventurousSwim1312</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just received my brand new Blackwell card, so did a quick bench to let the community grasp the pros and cons&lt;/p&gt; &lt;h1&gt;Setup Details:&lt;/h1&gt; &lt;p&gt;GPU : Rtx pro 6000 max-q workstation edition, 12% less TFLOPs than the complete, but with half the power draw, on 2 slots and with same memory bandwidth.&lt;/p&gt; &lt;p&gt;CPU : Ryzen 9 3950X, 24 channels, 16 cores / 32 threads&lt;/p&gt; &lt;p&gt;RAM : 128go DDR4 3600Ghz&lt;/p&gt; &lt;p&gt;GPU1 : RTX 3090 24gb blower edition. 2 slots, unused here&lt;/p&gt; &lt;p&gt;GPU2 : RTX 3090 24gb founder edition. 3 slots, unused here&lt;/p&gt; &lt;h1&gt;Software details&lt;/h1&gt; &lt;h1&gt;OS&lt;/h1&gt; &lt;p&gt;- Ubuntu 22.04&lt;/p&gt; &lt;p&gt;- Nvidia Drivers : 770 open &lt;/p&gt; &lt;p&gt;- Cuda toolkit 13&lt;/p&gt; &lt;p&gt;- Cudnn 9&lt;/p&gt; &lt;p&gt;(ask if you want a quick install tutorial in comments)&lt;/p&gt; &lt;h1&gt;Env&lt;/h1&gt; &lt;p&gt;conda create --name vllm python=3.12&lt;/p&gt; &lt;p&gt;conda activate vllm&lt;/p&gt; &lt;p&gt;uv pip install flashinfer-python --prerelease=allow --upgrade --extra-index-url &lt;a href="https://download.pytorch.org/whl/nightly/cu128"&gt;https://download.pytorch.org/whl/nightly/cu128&lt;/a&gt;&lt;/p&gt; &lt;p&gt;uv pip install vllm --torch-backend=cu128&lt;/p&gt; &lt;h1&gt;Training Benchmark&lt;/h1&gt; &lt;p&gt;Two stuff are diferenciating for training on that card:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;the number of tensor core is outstanding, about 60% more than a single B100 gpu&lt;/li&gt; &lt;li&gt;the 96GB vram is a game changer for training, enabling very large batch, so faster and smoother training&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Experiment:&lt;/h1&gt; &lt;p&gt;Pretraining of a SLM with 35M parameters, based on GQA architecture with 8 layers, trained with pytorch lightning. Training dataset is TinyStories, with a budget of 1B tokens (2 epochs), a sequence length of 256 tokens, and a virtual batch size of 100k tokens. Models are trained in mixed bf16 precision (additionnal improvement could be expected from using black well fp8 training)&lt;/p&gt; &lt;h1&gt;Results:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;1 x 4090 Laptop (similar perf as a 3090 Desktop) : ~2.5 hours to complete the training run&lt;/li&gt; &lt;li&gt;1 x RTX 6000 pro maxq workstation : ~20 min to complete the training run&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;With proper optim, the card can single handedly deliver the training compute of 7.5 rtx 3090 card, while pulling only 300W of electricity (and being very quiet).&lt;/p&gt; &lt;h1&gt;Inference Benchmark&lt;/h1&gt; &lt;p&gt;In inference, bandwith can be the bottleneck factor, especially in batch 1 inference.&lt;/p&gt; &lt;p&gt;Let's assess the results in batch 1, 4, 8, 16 and 32 to see how much token we can squeeze out of the card.&lt;/p&gt; &lt;h1&gt;Launch&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;export NVCC_THREADS=16 export MAX_JOBS=16 export OMP_NUM_THREADS=16 export VLLM_ATTENTION_BACKEND=FLASHINFER export ENABLE_NVFP4_SM120=1 export VLLM_USE_FLASHINFER_MOE_FP4=1 export MODEL_NAME=&amp;quot;DeepSeek-R1-0528-Qwen3-8B-FP4&amp;quot; vllm serve &amp;quot;$MODEL_NAME&amp;quot; \ --served-model-name gpt-4 \ --port 5000 \ --max-model-len 16000 \ --gpu-memory-utilization 0.9 \ --trust_remote_code \ --max-seq-len-to-capture 8196 \ --enable-chunked-prefill \ --kv-cache-dtype fp8 \ --compilation-config '{&amp;quot;pass_config&amp;quot;:{&amp;quot;enable_fusion&amp;quot;:true,&amp;quot;enable_noop&amp;quot;:true},&amp;quot;cudagraph_mode&amp;quot;:1,&amp;quot;max_capture_size&amp;quot;:2048}' &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Launch &amp;gt;20B Active&lt;/h1&gt; &lt;p&gt;On larger models, tensor cores can do wonders, so above 20B active parameters, the following additionnal env variables can provide a small speed increase, especially for batching.&lt;/p&gt; &lt;p&gt;export VLLM_USE_TRTLLM_ATTENTION=1&lt;/p&gt; &lt;p&gt;export VLLM_USE_TRTLLM_FP4_GEMM=1&lt;/p&gt; &lt;p&gt;export VLLM_FLASHINFER_FORCE_TENSOR_CORES=1&lt;/p&gt; &lt;p&gt;Note: i ran every speed test without these flags, but for example Mistral Small would give around 95 t/s on batch 1, and 1950 t/s on batch 32&lt;/p&gt; &lt;h1&gt;Launch QWEN Moe&lt;/h1&gt; &lt;p&gt;Add flag --enable-expert-parallel&lt;/p&gt; &lt;h1&gt;Launch GPT-OSS&lt;/h1&gt; &lt;p&gt;GPT OSS relies on MXFP4 quant (cause why would they do like everyone else uh?), an hybrid format that will most likely disapear once NVFP4 is fully supported. They also are leveraging their own library for prompt formatting, that is not really compatible with vllm as of now, so don't expect to get anything good from these, i am just testing the speed, but most of the time they only send you blank tokens, which is not really usefull.&lt;/p&gt; &lt;h1&gt;DOWNLOADS&lt;/h1&gt; &lt;p&gt;You'll need to download the following to make vllm work with special snowflake tokenizer, and not break on start:&lt;/p&gt; &lt;p&gt;sudo wget -O /etc/encodings/o200k_base.tiktoken &lt;a href="https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken"&gt;https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken&lt;/a&gt;&lt;/p&gt; &lt;p&gt;sudo wget -O /etc/encodings/cl100k_base.tiktoken &lt;a href="https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken"&gt;https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Launch Command&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;export ENABLE_NVFP4_SM120=1 export VLLM_USE_TRTLLM_ATTENTION=1 export OMP_NUM_THREADS=16 export TIKTOKEN_ENCODINGS_BASE=/etc/encodings export VLLM_USE_FLASHINFER_MXFP4_BF16_MOE=1 export VLLM_USE_FLASHINFER_MXFP4_MOE=1 export VLLM_ATTENTION_BACKEND=FLASHINFER export MODEL_NAME=&amp;quot;gpt-oss-120b&amp;quot; vllm serve &amp;quot;$MODEL_NAME&amp;quot; \ --async-scheduling \ --served-model-name gpt-4 \ --port 5000 \ --max-model-len 16000 \ --gpu-memory-utilization 0.9 \ --trust_remote_code \ --max-seq-len-to-capture 8196 \ --compilation-config '{&amp;quot;pass_config&amp;quot;:{&amp;quot;enable_fusion&amp;quot;:true,&amp;quot;enable_noop&amp;quot;:true},&amp;quot;cudagraph_mode&amp;quot;:1,&amp;quot;max_capture_size&amp;quot;:2048}' \ &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Model Tested:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Qwen3-Coder-30B-A3B-Instruct-GPTQ-4bit&lt;/li&gt; &lt;li&gt;Qwen3-4B-Instruct-2507-GPTQ&lt;/li&gt; &lt;li&gt;Qwen3-32B-AWQ&lt;/li&gt; &lt;li&gt;Mistral-Small-3.2-24B-Instruct-hf-AWQ&lt;/li&gt; &lt;li&gt;gpt-oss-20b&lt;/li&gt; &lt;li&gt;gpt-oss-120b&lt;/li&gt; &lt;li&gt;Hunyuan-A13B-Instruct-GPTQ-Int4&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Failed Test&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;DeepSeek-R1-0528-Qwen3-8B-FP4 : could not start GEMM FP4 kernels, i'll investigate&lt;/li&gt; &lt;li&gt;Qwen3-32B-FP4 : could not start GEMM FP4 kernels, i'll investigate&lt;/li&gt; &lt;li&gt;Llama-4-Scout-17B-16E-Instruct-AWQ : KeyError: 'layers.17.feed_forward.shared_expert.activation_fn.scales', the quant wasn't done properly and i couldn't find an other version in 4bit except bnb that would be much slower :/&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;p&gt;Read :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;0-64 : batch 1 token generation speed between first token and 64th (token / second)&lt;/li&gt; &lt;li&gt;64-128 : batch 1 token generation speed between 64th and 128th (token / second)&lt;/li&gt; &lt;li&gt;...&lt;/li&gt; &lt;li&gt;batch_4 : total throughtput token per second while running 4 concurrent request&lt;/li&gt; &lt;li&gt;batch_8 : total throughtput token per second while running 8 concurrent request&lt;/li&gt; &lt;li&gt;...&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Name&lt;/th&gt; &lt;th align="left"&gt;0-64&lt;/th&gt; &lt;th align="left"&gt;64-128&lt;/th&gt; &lt;th align="left"&gt;128-256&lt;/th&gt; &lt;th align="left"&gt;256-512&lt;/th&gt; &lt;th align="left"&gt;512-1024&lt;/th&gt; &lt;th align="left"&gt;1024-2048&lt;/th&gt; &lt;th align="left"&gt;batch_4&lt;/th&gt; &lt;th align="left"&gt;batch_8&lt;/th&gt; &lt;th align="left"&gt;batch_16&lt;/th&gt; &lt;th align="left"&gt;batch_32&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss-120b&lt;/td&gt; &lt;td align="left"&gt;182.14&lt;/td&gt; &lt;td align="left"&gt;147.11&lt;/td&gt; &lt;td align="left"&gt;158.66&lt;/td&gt; &lt;td align="left"&gt;143.20&lt;/td&gt; &lt;td align="left"&gt;154.57&lt;/td&gt; &lt;td align="left"&gt;148.10&lt;/td&gt; &lt;td align="left"&gt;~403-409&lt;/td&gt; &lt;td align="left"&gt;~770-776&lt;/td&gt; &lt;td align="left"&gt;~1294-1302&lt;/td&gt; &lt;td align="left"&gt;~1986-2146&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss-20b&lt;/td&gt; &lt;td align="left"&gt;196.09&lt;/td&gt; &lt;td align="left"&gt;199.98&lt;/td&gt; &lt;td align="left"&gt;214.26&lt;/td&gt; &lt;td align="left"&gt;198.01&lt;/td&gt; &lt;td align="left"&gt;196.56&lt;/td&gt; &lt;td align="left"&gt;194.38&lt;/td&gt; &lt;td align="left"&gt;~564-624&lt;/td&gt; &lt;td align="left"&gt;~1054-1117&lt;/td&gt; &lt;td align="left"&gt;~1887-1912&lt;/td&gt; &lt;td align="left"&gt;~2904-2911&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-32B-AWQ&lt;/td&gt; &lt;td align="left"&gt;60.47&lt;/td&gt; &lt;td align="left"&gt;68.94&lt;/td&gt; &lt;td align="left"&gt;62.53&lt;/td&gt; &lt;td align="left"&gt;62.36&lt;/td&gt; &lt;td align="left"&gt;61.99&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;~227-233&lt;/td&gt; &lt;td align="left"&gt;~447-452&lt;/td&gt; &lt;td align="left"&gt;~920-936&lt;/td&gt; &lt;td align="left"&gt;~1448-1482&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral-Small-3.2-24B-Instruct-hf-AWQ&lt;/td&gt; &lt;td align="left"&gt;89.39&lt;/td&gt; &lt;td align="left"&gt;95.77&lt;/td&gt; &lt;td align="left"&gt;89.29&lt;/td&gt; &lt;td align="left"&gt;87.29&lt;/td&gt; &lt;td align="left"&gt;86.95&lt;/td&gt; &lt;td align="left"&gt;86.59&lt;/td&gt; &lt;td align="left"&gt;~288-336&lt;/td&gt; &lt;td align="left"&gt;~631-646&lt;/td&gt; &lt;td align="left"&gt;~1109-1153&lt;/td&gt; &lt;td align="left"&gt;~1714-1790&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-4B-Instruct-2507-GPTQ&lt;/td&gt; &lt;td align="left"&gt;208.21&lt;/td&gt; &lt;td align="left"&gt;205.15&lt;/td&gt; &lt;td align="left"&gt;223.60&lt;/td&gt; &lt;td align="left"&gt;210.72&lt;/td&gt; &lt;td align="left"&gt;211.67&lt;/td&gt; &lt;td align="left"&gt;207.49&lt;/td&gt; &lt;td align="left"&gt;~721-743&lt;/td&gt; &lt;td align="left"&gt;~1158-1377&lt;/td&gt; &lt;td align="left"&gt;~2044-2236&lt;/td&gt; &lt;td align="left"&gt;~2400-2666&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Coder-30B-A3B-Instruct-GPTQ-4bit&lt;/td&gt; &lt;td align="left"&gt;179.42&lt;/td&gt; &lt;td align="left"&gt;176.71&lt;/td&gt; &lt;td align="left"&gt;176.01&lt;/td&gt; &lt;td align="left"&gt;175.81&lt;/td&gt; &lt;td align="left"&gt;175.44&lt;/td&gt; &lt;td align="left"&gt;172.64&lt;/td&gt; &lt;td align="left"&gt;~490-510&lt;/td&gt; &lt;td align="left"&gt;~950-1000&lt;/td&gt; &lt;td align="left"&gt;~1520-1602&lt;/td&gt; &lt;td align="left"&gt;~2200-2400&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hunyuan-A13B-Instruct-GPTQ-Int4&lt;/td&gt; &lt;td align="left"&gt;94.91&lt;/td&gt; &lt;td align="left"&gt;89.74&lt;/td&gt; &lt;td align="left"&gt;64.91&lt;/td&gt; &lt;td align="left"&gt;87.40&lt;/td&gt; &lt;td align="left"&gt;89.71&lt;/td&gt; &lt;td align="left"&gt;88.03&lt;/td&gt; &lt;td align="left"&gt;~200-202&lt;/td&gt; &lt;td align="left"&gt;~300-307&lt;/td&gt; &lt;td align="left"&gt;~477-485&lt;/td&gt; &lt;td align="left"&gt;~755-777&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;No surprise, in batch 1, the performance is good but not outstanding, limited by the 1.7 TB/s of GDDR7 memory. The blackwell optimizations allow to squeeze a bit more performance though (that might explode when flash attention 4 will be released) and just slightly beats the speed of 2 x 3090 with tensor parallelism.&lt;/p&gt; &lt;p&gt;The game changer is on batch 32, with an almost linear scaling of number of tokens delivered with batch size, so might be really usefull for small scale serving and multi agent deployment purpose.&lt;/p&gt; &lt;p&gt;So far, support is still not completely ready, but sufficient to play with some models.&lt;/p&gt; &lt;h1&gt;Code to reproduce the results&lt;/h1&gt; &lt;p&gt;Training scripts can be found on this repo for pretraining:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/gabrielolympie/ArchiFactory"&gt;https://github.com/gabrielolympie/ArchiFactory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Speed Benchmark for inference + used prompts can be found in :&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/gabrielolympie/PromptServer"&gt;https://github.com/gabrielolympie/PromptServer&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Next steps&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I might update this post when NVFP4 support is stable enough to give a glimpse of it potential&lt;/li&gt; &lt;li&gt;If you want me to test a specific model, propose in the comments, i'll add those who are either in a different weight category, or different architecture&lt;/li&gt; &lt;li&gt;If i can find the time, i will make a similar post with diffusion models (image + video) where the archi might deliver even more impressive results&lt;/li&gt; &lt;li&gt;If you want me to test additionnal vllm tuning parameters, let me know in the comments (i might give a try to sglang and exllama v3 as well when their own support will be more mature)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Global conclusion&lt;/h1&gt; &lt;p&gt;Pros:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;large vram&lt;/li&gt; &lt;li&gt;impressive raw compute&lt;/li&gt; &lt;li&gt;impressive scaling with batch size&lt;/li&gt; &lt;li&gt;very quiet, i could sleep during a training run with computer in the same room&lt;/li&gt; &lt;li&gt;very low power consumption, stable 300W at full power and most likely room for overclocking&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Cons:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;still limited bandwith compared to latest HBM memory&lt;/li&gt; &lt;li&gt;software support still a bit messy but quickly improving&lt;/li&gt; &lt;li&gt;cannot be used with tensor paralellism with Ampere (i tried doing tensor parallelism with a 3090 and it did not go well)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Sweet spots / for what need?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Any model with 10-20B active parameters and up to 160B total parameters will be incredible on it&lt;/li&gt; &lt;li&gt;Processing large amount of texts (classification / labeling / synthetic data generation )&lt;/li&gt; &lt;li&gt;Small serving for up to 30 - 60 concurrent users&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;When not to use?&lt;/p&gt; &lt;p&gt;If your use case involve getting max tokens / seconds in batch 1 and you don't care for power draw, building a battlestation with 4*4090 will provide much better speed at the same price.&lt;/p&gt; &lt;p&gt;Edit / Addtions:&lt;br /&gt; Added Hunyuan A13B : for some reason the FP8 kv cache must be removed. And the model is far slower than it should be for large batches for its size (might be due to the gptq format though).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdventurousSwim1312"&gt; /u/AdventurousSwim1312 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my3why/rtx_pro_6000_maxq_blackwell_for_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my3why/rtx_pro_6000_maxq_blackwell_for_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1my3why/rtx_pro_6000_maxq_blackwell_for_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T15:08:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mymak3</id>
    <title>Google new Research Paper : Measuring the environmental impact of delivering AI</title>
    <updated>2025-08-24T04:32:07+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google has dropped in a very important research paper measuring the impact of AI on the environment, suggesting how much carbon emission, water, and energy consumption is done for running a prompt on Gemini. Surprisingly, the numbers have been quite low compared to the previously reported numbers by other studies, suggesting that the evaluation framework is flawed. &lt;/p&gt; &lt;p&gt;Google measured the environmental impact of &lt;strong&gt;a single Gemini prompt&lt;/strong&gt; and here’s what they found:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;0.24 Wh of energy&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;0.03 grams of CO₂&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;0.26 mL of water&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Paper : &lt;a href="https://services.google.com/fh/files/misc/measuring_the_environmental_impact_of_delivering_ai_at_google_scale.pdf"&gt;https://services.google.com/fh/files/misc/measuring_the_environmental_impact_of_delivering_ai_at_google_scale.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video : &lt;a href="https://www.youtube.com/watch?v=q07kf-UmjQo"&gt;https://www.youtube.com/watch?v=q07kf-UmjQo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mymak3/google_new_research_paper_measuring_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mymak3/google_new_research_paper_measuring_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mymak3/google_new_research_paper_measuring_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T04:32:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1my7j1x</id>
    <title>support for ByteDance Seed-OSS model has been merged into llama.cpp</title>
    <updated>2025-08-23T17:29:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my7j1x/support_for_bytedance_seedoss_model_has_been/"&gt; &lt;img alt="support for ByteDance Seed-OSS model has been merged into llama.cpp" src="https://external-preview.redd.it/WFGEPRY69pmnCNsVihL350z048IpLks_fdEjrmNlkmg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=447948ecb5a77a2d635a9ce18c86729398f84896" title="support for ByteDance Seed-OSS model has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;model: &lt;a href="https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct"&gt;https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15490"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my7j1x/support_for_bytedance_seedoss_model_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1my7j1x/support_for_bytedance_seedoss_model_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T17:29:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1myr49h</id>
    <title>Trying to get llama.cpp to run Qwen3 model and use its server for Qwen Code</title>
    <updated>2025-08-24T09:28:51+00:00</updated>
    <author>
      <name>/u/eur0child</name>
      <uri>https://old.reddit.com/user/eur0child</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the life of me, I cannot get a Qwen3 model to work properly with Qwen Code CLI.&lt;/p&gt; &lt;p&gt;First, I have naively tried to run it through ollama, but there is a known discrepancy for the tool usage with ollama. So I have tried to use an unsloth model as described &lt;a href="https://docs.unsloth.ai/basics/qwen3-coder-how-to-run-locally#llama.cpp-run-qwen3-tutorial"&gt;here&lt;/a&gt; supposedly fixing the issues with the Qwen3 models. Still didn't work with tooling, Qwen Code just outputs informations about using a tool without actually using it.&lt;/p&gt; &lt;p&gt;So I turned to using llama.cpp instead of ollama. Because I am lazy, I use a pre-compiled release and try running a server out of it since I don't want to use it directly, but use it with Qwen Code.&lt;/p&gt; &lt;p&gt;Hence, I try to adapt the configuration for Qwen Code accordingly with the following : &lt;/p&gt; &lt;p&gt;&lt;code&gt;OPENAI_API_KEY=my_api_key&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;OPENAI_BASE_URL=http://localhost:8080(/v1) (instead of&lt;/code&gt; &lt;a href="http://localhost:11434/v1"&gt;&lt;code&gt;http://localhost:11434/v1&lt;/code&gt;&lt;/a&gt; &lt;code&gt;for ollama)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;OPENAI_MODEL=hf.co/unsloth/[...]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I then run Qwen Code and all I get is an error with : &lt;/p&gt; &lt;p&gt;&lt;code&gt;code: null,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;param: null,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;type: 'api_error'&lt;/code&gt; &lt;/p&gt; &lt;p&gt;Obviously it looks like the server url is incorrect or something.&lt;/p&gt; &lt;p&gt;What am I doing wrong ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eur0child"&gt; /u/eur0child &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myr49h/trying_to_get_llamacpp_to_run_qwen3_model_and_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myr49h/trying_to_get_llamacpp_to_run_qwen3_model_and_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myr49h/trying_to_get_llamacpp_to_run_qwen3_model_and_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T09:28:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mydvzs</id>
    <title>DeepSeek-V3.1: Much More Powerful With Thinking!</title>
    <updated>2025-08-23T21:41:04+00:00</updated>
    <author>
      <name>/u/JeepyTea</name>
      <uri>https://old.reddit.com/user/JeepyTea</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mydvzs/deepseekv31_much_more_powerful_with_thinking/"&gt; &lt;img alt="DeepSeek-V3.1: Much More Powerful With Thinking!" src="https://preview.redd.it/8mqccjjdhtkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5820adfda045edf06fcecc9d2c49c79a6c447037" title="DeepSeek-V3.1: Much More Powerful With Thinking!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yesterday, I posted the results for TiānshūBench (天书Bench) 0.0.1-mini for DeepSeek-V3.1. I noted at the time that it seemed rather weak compared to similar models. That test was conducted without thinking enabled for the model. It turns out that DeepSeek-V3.1 has a particular &amp;quot;in-band&amp;quot; method of enabling thinking as part of the model, by setting the prompt format. &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1"&gt;HuggingFace has more details&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It turns out that enabling thinking in this way gives a huge boost to V3.1's performance, as you can see above, putting it above DeepSeek R1-0528 and on par with GPT-oss.&lt;/p&gt; &lt;p&gt;TiānshūBench tests fluid intelligence and coding ability by forcing the models to solve problems in a programming language that they've never seen before. The benchmark tests provide the language's definition, then let the models write code.&lt;/p&gt; &lt;p&gt;More info:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Introduction to &lt;a href="https://jeepytea.github.io/general/introduction/2025/05/29/tianshubenchintro.html"&gt;TiānshūBench&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/JeepyTea/TianShu"&gt;TiānshūBench on Github&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JeepyTea"&gt; /u/JeepyTea &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8mqccjjdhtkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mydvzs/deepseekv31_much_more_powerful_with_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mydvzs/deepseekv31_much_more_powerful_with_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T21:41:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mytilm</id>
    <title>What are my best options for using Video Understanding Vision Language Models?</title>
    <updated>2025-08-24T11:49:47+00:00</updated>
    <author>
      <name>/u/LivingMNML</name>
      <uri>https://old.reddit.com/user/LivingMNML</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Reddit,&lt;/p&gt; &lt;p&gt;I am working on a project that uses VLM models to analyse high fps tennis matches.&lt;/p&gt; &lt;p&gt;I am currently using Google Gemini 2.5 Pro, however they are limited to 1fps above 20mb and also I am not able to finetune it, I have been looking at benchmarks and have seen Salmonn 7b+ PEFT (on top of Qwen2.5), and now there is VLM 4.5, which I tried to use via the online demo but it didn't get good results, maybe it was confused with FPS etc.&lt;/p&gt; &lt;p&gt;What is the current best strategy for using a VLM to understand video at high FPS (5-10fps).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LivingMNML"&gt; /u/LivingMNML &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytilm/what_are_my_best_options_for_using_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytilm/what_are_my_best_options_for_using_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mytilm/what_are_my_best_options_for_using_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T11:49:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mycmn2</id>
    <title>How long do you think it will take Chinese AI labs to respond to NanoBanana?</title>
    <updated>2025-08-23T20:48:55+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mycmn2/how_long_do_you_think_it_will_take_chinese_ai/"&gt; &lt;img alt="How long do you think it will take Chinese AI labs to respond to NanoBanana?" src="https://preview.redd.it/gn3t9xnyztkf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1bf5c98d7901fa481076fe8c443580997af8c8d6" title="How long do you think it will take Chinese AI labs to respond to NanoBanana?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gn3t9xnyztkf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mycmn2/how_long_do_you_think_it_will_take_chinese_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mycmn2/how_long_do_you_think_it_will_take_chinese_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T20:48:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1myhawv</id>
    <title>Ever Wondered What’s Hiding in the “System Prompt” of Your Favorite AI Tool? I Scraped 10k+ Lines of Them</title>
    <updated>2025-08-24T00:11:14+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So… turns out a lot of the magic in today’s “smart” AI tools isn’t just the model, it’s the system prompt quietly steering it behind the scenes. I’ve been extracting these for months, and I published everything I found into a repo:&lt;/p&gt; &lt;p&gt;👉 &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Inside you’ll find: - The hidden prompts from V0, Cursor, Manus, Lovable, Devin, Replit Agent, VSCode Agent, Windsor, Warp.dev, etc. - Over 10,000+ lines of text, showing how different companies structure reasoning, enforce rules, and sometimes… straight-up contradict themselves.&lt;/p&gt; &lt;p&gt;It’s weirdly fascinating to see how varied these scaffolds are: some are verbose manifestos, others are brittle one-liners, some try to sound “human,” and some read like legal contracts.&lt;/p&gt; &lt;p&gt;If you’re into red-teaming, agent design, prompt engineering, or just model anthropology, this repo is a candy store.&lt;/p&gt; &lt;p&gt;Curious which ones you find the most unhinged or overengineered, drop your favorite discoveries if you dig through.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myhawv/ever_wondered_whats_hiding_in_the_system_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myhawv/ever_wondered_whats_hiding_in_the_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myhawv/ever_wondered_whats_hiding_in_the_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T00:11:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mysjww</id>
    <title>What is the Claude equivalent of DeepSeek v3.1 in coding ability?</title>
    <updated>2025-08-24T10:55:39+00:00</updated>
    <author>
      <name>/u/Livid-Self-5770</name>
      <uri>https://old.reddit.com/user/Livid-Self-5770</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been testing &lt;strong&gt;DeepSeek v3.1&lt;/strong&gt; for coding tasks and found it to be pretty solid so far. Out of curiosity, for those who have tried both, what would be the &lt;strong&gt;Claude model that’s roughly equivalent to DeepSeek v3.1&lt;/strong&gt; in terms of coding ability?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Livid-Self-5770"&gt; /u/Livid-Self-5770 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mysjww/what_is_the_claude_equivalent_of_deepseek_v31_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mysjww/what_is_the_claude_equivalent_of_deepseek_v31_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mysjww/what_is_the_claude_equivalent_of_deepseek_v31_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T10:55:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1myigna</id>
    <title>"Why are you all so worried whenever the big companies talk about LLM safety? What's the worst that could happen?"</title>
    <updated>2025-08-24T01:08:43+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myigna/why_are_you_all_so_worried_whenever_the_big/"&gt; &lt;img alt="&amp;quot;Why are you all so worried whenever the big companies talk about LLM safety? What's the worst that could happen?&amp;quot;" src="https://external-preview.redd.it/amthMTBncThhdmtmMeYkHvQl6ANcbp9DAX5oa2nUyz5pQDo1cq9KjrP_m95D.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=23c6d3e5b22e0c35924b85610790740771d73e6b" title="&amp;quot;Why are you all so worried whenever the big companies talk about LLM safety? What's the worst that could happen?&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r0ym4gq8avkf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myigna/why_are_you_all_so_worried_whenever_the_big/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myigna/why_are_you_all_so_worried_whenever_the_big/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T01:08:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mytbfz</id>
    <title>Accuracy recovery adapter with self-generated data (magpie-style)</title>
    <updated>2025-08-24T11:39:03+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLama"&gt;r/LocalLLama&lt;/a&gt;! Wanted to share a technique that's been working really well for recovering performance after INT4 quantization. &lt;/p&gt; &lt;p&gt;Typically, quantizing the LLM to INT4 (unlike say INT8) for inference can incur some accuracy loss. Instead of accepting the quality loss, we used the FP16 model as a teacher to train a tiny LoRA adapter (rank=16) for the quantized model. The cool part: the model generates its own training data using the Magpie technique so no external datasets needed. This is critical because we want to remain as much as possible in the distribution of the model's natural responses. &lt;/p&gt; &lt;p&gt;Last year Apple's foundational models paper (&lt;a href="https://arxiv.org/pdf/2407.21075"&gt;https://arxiv.org/pdf/2407.21075&lt;/a&gt;) had proposed a similar technique and found &amp;quot;By using accuracy-recovery LoRA adapters with only rank 16, Alpaca win rate can be improved by 7-18%, GMS8K accuracy is boosted by 5-10%.&amp;quot; (page 47). &lt;/p&gt; &lt;p&gt;We saw similar results on Qwen3-0.6B:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Perplexity: 2.40 → 2.09 (only 5.7% degradation from FP16 baseline)&lt;/li&gt; &lt;li&gt;Memory: Only 0.28GB vs 1.0GB for FP16 (75% reduction)&lt;/li&gt; &lt;li&gt;Speed: 3.0x faster inference than FP16&lt;/li&gt; &lt;li&gt;Quality: Generates correct, optimized code solutions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Resources&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://colab.research.google.com/github/codelion/ellora/blob/main/Ellora_Recipe_1_Self_Distillation_For_Quantization_Recovery.ipynb"&gt;Colab notebook with full implementation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/codelion/Qwen3-0.6B-accuracy-recovery-lora"&gt;Pre-trained adapter on HuggingFace&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/codelion/ellora"&gt;GitHub repo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions about the implementation or help anyone trying to replicate this. The key insight is that quantization errors are systematic and learnable - a small adapter can bridge the gap without negating the benefits of quantization.&lt;/p&gt; &lt;p&gt;Has anyone else experimented with self-distillation for quantization recovery? Would love to hear about different approaches!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytbfz/accuracy_recovery_adapter_with_selfgenerated_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytbfz/accuracy_recovery_adapter_with_selfgenerated_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mytbfz/accuracy_recovery_adapter_with_selfgenerated_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T11:39:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mytkpp</id>
    <title>Do you still use mikupad or is there a replacement?</title>
    <updated>2025-08-24T11:52:58+00:00</updated>
    <author>
      <name>/u/aeroumbria</name>
      <uri>https://old.reddit.com/user/aeroumbria</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mikupad was my go-to tool for generating text with the option to show alternative tokens. This is especially useful for getting a feel of a model's preferences, writing stories, hacking context, or just working with non-conversational tasks in general. However, it has not been updated for a while, and although still fully functional, I actually had to revert to an earlier commit to make alternative tokens work, as the last commit broke the function, and the prospect of this function breaking again with no fix is not reassuring. Has anyone found a good alternative for mikupad, or is it still the best tool we have for now? &lt;/p&gt; &lt;p&gt;In case this is not clear enough, by &amp;quot;alternative tokens&amp;quot; I mean the ability to see the top K options at each step of the generation, and in mikupad you can even click any of them and restart generation using the selected choice as the last input.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aeroumbria"&gt; /u/aeroumbria &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytkpp/do_you_still_use_mikupad_or_is_there_a_replacement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytkpp/do_you_still_use_mikupad_or_is_there_a_replacement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mytkpp/do_you_still_use_mikupad_or_is_there_a_replacement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T11:52:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mymyfu</id>
    <title>A timeline of LLM Context Windows, Over the past 5 years. (done right this time)</title>
    <updated>2025-08-24T05:09:43+00:00</updated>
    <author>
      <name>/u/jack-ster</name>
      <uri>https://old.reddit.com/user/jack-ster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1mymyfu/video/hi8umq5ehwkf1/player"&gt;https://reddit.com/link/1mymyfu/video/hi8umq5ehwkf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sources:&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/CD9QEbCZ"&gt;https://pastebin.com/CD9QEbCZ&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jack-ster"&gt; /u/jack-ster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mymyfu/a_timeline_of_llm_context_windows_over_the_past_5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mymyfu/a_timeline_of_llm_context_windows_over_the_past_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mymyfu/a_timeline_of_llm_context_windows_over_the_past_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T05:09:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1myb09v</id>
    <title>Google and Anthropic struggle to keep marketshare as everyone else catches up</title>
    <updated>2025-08-23T19:44:10+00:00</updated>
    <author>
      <name>/u/ObnoxiouslyVivid</name>
      <uri>https://old.reddit.com/user/ObnoxiouslyVivid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myb09v/google_and_anthropic_struggle_to_keep_marketshare/"&gt; &lt;img alt="Google and Anthropic struggle to keep marketshare as everyone else catches up" src="https://preview.redd.it/35p1pim9ntkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e48d4b2543aa0cd859924de94edd03937a9fc35a" title="Google and Anthropic struggle to keep marketshare as everyone else catches up" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Data from last 6 months on OpenRouter compared to now&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ObnoxiouslyVivid"&gt; /u/ObnoxiouslyVivid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/35p1pim9ntkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myb09v/google_and_anthropic_struggle_to_keep_marketshare/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myb09v/google_and_anthropic_struggle_to_keep_marketshare/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T19:44:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1myx4l5</id>
    <title>Which local model are you currently using the most? What’s your main use case, and why do you find it good?</title>
    <updated>2025-08-24T14:32:16+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myx4l5/which_local_model_are_you_currently_using_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myx4l5/which_local_model_are_you_currently_using_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myx4l5/which_local_model_are_you_currently_using_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T14:32:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mypokb</id>
    <title>GPT OSS 20b is Impressive at Instruction Following</title>
    <updated>2025-08-24T07:56:56+00:00</updated>
    <author>
      <name>/u/crodjer</name>
      <uri>https://old.reddit.com/user/crodjer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have found GPT OSS 20b to be consistently great at following complex instructions. For instance, it did performed perfectly with a test prompt I used: &lt;a href="https://github.com/crodjer/glaince/tree/main/cipher#results"&gt;https://github.com/crodjer/glaince/tree/main/cipher#results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All other models in the same size (Gemma 3, Qwen 3, Mistral Small) make the same mistake, resulting them to deviate from expectation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crodjer"&gt; /u/crodjer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mypokb/gpt_oss_20b_is_impressive_at_instruction_following/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mypokb/gpt_oss_20b_is_impressive_at_instruction_following/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mypokb/gpt_oss_20b_is_impressive_at_instruction_following/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T07:56:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mytpf1</id>
    <title>Apple M3 Ultra w/28-Core CPU, 60-Core GPU (256GB RAM) Running Deepseek-R1-UD-IQ1_S (140.23GB)</title>
    <updated>2025-08-24T11:59:59+00:00</updated>
    <author>
      <name>/u/Mass2018</name>
      <uri>https://old.reddit.com/user/Mass2018</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytpf1/apple_m3_ultra_w28core_cpu_60core_gpu_256gb_ram/"&gt; &lt;img alt="Apple M3 Ultra w/28-Core CPU, 60-Core GPU (256GB RAM) Running Deepseek-R1-UD-IQ1_S (140.23GB)" src="https://b.thumbs.redditmedia.com/x4bULEyzYY3EEBanwjUi7dZiywWjI4EP8X8WNQAbYyk.jpg" title="Apple M3 Ultra w/28-Core CPU, 60-Core GPU (256GB RAM) Running Deepseek-R1-UD-IQ1_S (140.23GB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen a lot of discussion recently about the performance of the Apple studios with large models, so I thought I'd share actual data from about a month of usage in our household.&lt;/p&gt; &lt;p&gt;This is mainly used by the non-me part of our household, so it sits nice and stable and just runs Deepseek 24/7, where my personal rig is constantly being swapped between different things that I'm working on.&lt;/p&gt; &lt;p&gt;The Apple Studio replaced the 10xP100 rig I had previously built for this purpose, and I have to say for what we're using it for it's been a godsend. It's much, much faster, can load larger models, has a much lower power footprint, and it was just... so easy to get it up and running. Honestly, it felt a bit like cheating after the hell that the P100 rig put me through.&lt;/p&gt; &lt;p&gt;Anyway, actual numbers:&lt;/p&gt; &lt;p&gt;|| || |Total logged requests:|161| |Context Average:|643.72| |Average Prompt Eval Tokens/Second:|64.73 tokens/second| |Average Tokens Generated:|343.16| |Average Tokens Generated/Second:|13.97 tokens/second|&lt;/p&gt; &lt;p&gt;My personal opinion is if all you're going to do is inferencing, it's a great option. I absolutely loathe the Mac GUI, and my constant attempt to control-c/control-v is infuriating, but other than that... NO RAGRETS.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mass2018"&gt; /u/Mass2018 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mytpf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytpf1/apple_m3_ultra_w28core_cpu_60core_gpu_256gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mytpf1/apple_m3_ultra_w28core_cpu_60core_gpu_256gb_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T11:59:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mybft5</id>
    <title>grok 2 weights</title>
    <updated>2025-08-23T20:00:52+00:00</updated>
    <author>
      <name>/u/HatEducational9965</name>
      <uri>https://old.reddit.com/user/HatEducational9965</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mybft5/grok_2_weights/"&gt; &lt;img alt="grok 2 weights" src="https://external-preview.redd.it/4tfHT9vpFrwHCpX5cn0_tHyoUS8M6oeQ7jwWbePCicw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9576154cc1820a09f2c9b345d4d88427c3729b9a" title="grok 2 weights" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HatEducational9965"&gt; /u/HatEducational9965 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/xai-org/grok-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mybft5/grok_2_weights/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mybft5/grok_2_weights/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T20:00:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1myjzmn</id>
    <title>There are at least 15 open source models I could find that can be run on a consumer GPU and which are better than Grok 2 (according to Artificial Analysis)</title>
    <updated>2025-08-24T02:26:33+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myjzmn/there_are_at_least_15_open_source_models_i_could/"&gt; &lt;img alt="There are at least 15 open source models I could find that can be run on a consumer GPU and which are better than Grok 2 (according to Artificial Analysis)" src="https://preview.redd.it/2t25pwj6ovkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8c8abd5ee1bf8381408ed5b298fc42879b01bd1" title="There are at least 15 open source models I could find that can be run on a consumer GPU and which are better than Grok 2 (according to Artificial Analysis)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And they have better licenses, less restrictions. What exactly is the point of Grok 2 then? I appreciate open source effort, but wouldn't it make more sense to open source a competitive model that can at least be run locally by most people?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2t25pwj6ovkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myjzmn/there_are_at_least_15_open_source_models_i_could/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myjzmn/there_are_at_least_15_open_source_models_i_could/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T02:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1myrdtb</id>
    <title>Mistral Large soon?</title>
    <updated>2025-08-24T09:45:24+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myrdtb/mistral_large_soon/"&gt; &lt;img alt="Mistral Large soon?" src="https://preview.redd.it/m9zk5bipuxkf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52ff9d33632d0268f989230460a6dbd3328b7244" title="Mistral Large soon?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;source &lt;a href="https://mistral.ai/news/mistral-medium-3"&gt;https://mistral.ai/news/mistral-medium-3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m9zk5bipuxkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myrdtb/mistral_large_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myrdtb/mistral_large_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T09:45:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1myqkqh</id>
    <title>Elmo is providing</title>
    <updated>2025-08-24T08:54:37+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"&gt; &lt;img alt="Elmo is providing" src="https://preview.redd.it/n6p9jpdvlxkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e03cd4c5782959f5dca22ea135d42d7032a20b59" title="Elmo is providing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n6p9jpdvlxkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T08:54:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
