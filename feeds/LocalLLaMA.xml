<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-25T17:23:29+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mzl9tp</id>
    <title>I built Husk, a native, private, and open-source iOS client for your local models</title>
    <updated>2025-08-25T08:53:17+00:00</updated>
    <author>
      <name>/u/nathan12581</name>
      <uri>https://old.reddit.com/user/nathan12581</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Ollama a lot and wanted a really clean, polished, and native way to interact with my privately hosted models on my iPhone. While there are some great options out there, I wanted something that felt like a first-party Apple app—fast, private, and simple.&lt;/p&gt; &lt;p&gt;Husk is an open-source, Ollama-compatible app for iOS. The whole idea is to provide a beautiful and seamless experience for chatting with your models without your data ever leaving your control.&lt;/p&gt; &lt;h1&gt;Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fully Offline &amp;amp; Private:&lt;/strong&gt; It's a native Ollama client. Your conversations stay on your devices.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optional iCloud Sync:&lt;/strong&gt; If you want, you can sync your chat history across your devices using Apple's end-to-end encryption (macOS support coming soon!).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Attachments:&lt;/strong&gt; You can attach text-based files to your chats (image support for multimodal models is on the roadmap!).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Highly Customisable:&lt;/strong&gt; You can set custom names, system prompts, and other parameters for your models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open Source:&lt;/strong&gt; The entire project is open-source under the MIT license.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To help support me, I've put Husk on the App Store with a small fee. If you buy it, thank you so much! It directly funds continued development.&lt;/p&gt; &lt;p&gt;However, since it's fully open-source, you are more than welcome to build and install yourself from the GitHub repo. The instructions are all in the README.&lt;/p&gt; &lt;p&gt;I'm also planning to add macOS support and integrations for other model providers soon.&lt;/p&gt; &lt;p&gt;I'd love to hear what you all think! Any feedback, feature requests, or bug reports are super welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; I made a native, private, open-source iOS app for Ollama. It's a paid app on the App Store to support development, but you can also build it yourself for free from the Github Repo&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nathan12581"&gt; /u/nathan12581 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzl9tp/i_built_husk_a_native_private_and_opensource_ios/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzl9tp/i_built_husk_a_native_private_and_opensource_ios/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzl9tp/i_built_husk_a_native_private_and_opensource_ios/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T08:53:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzd0ik</id>
    <title>PSA: Filling those empty DIMM slots will slow down inference if you don’t have enough memory channels</title>
    <updated>2025-08-25T01:04:16+00:00</updated>
    <author>
      <name>/u/DealingWithIt202s</name>
      <uri>https://old.reddit.com/user/DealingWithIt202s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 7900x on a x670e Pro RS mobo with 2x32GB &lt;a href="mailto:DDR5@5200"&gt;DDR5@5200&lt;/a&gt;. I really wanted to run GPT-OSS 120B with CPU moe but it wasn’t fully able to load. I obtained another pair of the same RAM (different batch, but same model/specs) and was able to run 120B, but only at 15 tk/s. I noticed that other models were slower as well. Then I realized that my RAM was running at 3600MTS as opposed to the 4800 it was at before. After digging into this issue it appears to be the grim reality with AMD AM5 boards that there isn’t much support for full throttle with DDR5 at 4 DIMMs. One would need an Intel build to get there apparently. In my case I think I’ll try to exchange for 2x48GB and sell my old RAM. &lt;/p&gt; &lt;p&gt;Does anyone know any way to use 4 slots at decent speeds and stability without buying a TR/EPYC?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DealingWithIt202s"&gt; /u/DealingWithIt202s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzd0ik/psa_filling_those_empty_dimm_slots_will_slow_down/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzd0ik/psa_filling_those_empty_dimm_slots_will_slow_down/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzd0ik/psa_filling_those_empty_dimm_slots_will_slow_down/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T01:04:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzw6oi</id>
    <title>Tiny LLM that can run on legacy hardware?</title>
    <updated>2025-08-25T17:02:42+00:00</updated>
    <author>
      <name>/u/Skibidirot</name>
      <uri>https://old.reddit.com/user/Skibidirot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to run a small speech to text model like whisper on my old PC that doesn't support AVX2 instruction which is needed to run llama.cpp or any other AI backends. is there a practical way to achieve this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Skibidirot"&gt; /u/Skibidirot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzw6oi/tiny_llm_that_can_run_on_legacy_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzw6oi/tiny_llm_that_can_run_on_legacy_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzw6oi/tiny_llm_that_can_run_on_legacy_hardware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:02:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzsqse</id>
    <title>AWS Noob here: EC2 vs SageMaker vs Bedrock for fine-tuning &amp; serving a custom LLM?</title>
    <updated>2025-08-25T14:57:05+00:00</updated>
    <author>
      <name>/u/JustPa55ion</name>
      <uri>https://old.reddit.com/user/JustPa55ion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! I am a Computer Vision Engineer, previously I have used the HPC center (basically lots of nodes with fancy GPUs) that we had partnership with to train / inference DL models and build pipelines.&lt;/p&gt; &lt;p&gt;Recently, started a new project, tho slightly different domain to what I used to work in - the task is to build a yet another &amp;quot;fancy and unique&amp;quot; chatbot.&lt;br /&gt; Generally speaking, we want 1) fine-tune open-source LLM (llama in our case) for our specific narrow domain (yes, we do want to do it), 2) design an app that will allow users to communicate with an LLM through Telegram, 3) be able to offload the weights of the trained model to our local machines.&lt;/p&gt; &lt;p&gt;I have never ever worked with AWS services before that, I have spent a couple of days going through the docs and some forums. Still have some questions left to answer :(&lt;/p&gt; &lt;p&gt;So my questions are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;For the fine-tuning purpose should I use EC2 with GPU nodes / Sagemaker / Bedrock? The EC2+GPU looks like what I am most familiar with. However, there is also an opportunity to fine-tune on Bedrock as well as Sagemaker. Why should I choose one over another? Will I be able to easily offload weights after tuning the model? Generally speaking, I am trying to wrap my mind around what are the unique features of each of these services?&lt;/li&gt; &lt;li&gt;What is the best practice / common strat for deploying and serving custom models? E.g. using ollama / vllm in EC2+GPU vs Creating an Sagemaker endpoint?&lt;/li&gt; &lt;li&gt;Any potential &amp;quot;beginner traps&amp;quot; that I should be aware of during doing things with AWS?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Would like to hear about your experience. Will appreciate any advice!&lt;br /&gt; Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustPa55ion"&gt; /u/JustPa55ion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzsqse/aws_noob_here_ec2_vs_sagemaker_vs_bedrock_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzsqse/aws_noob_here_ec2_vs_sagemaker_vs_bedrock_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzsqse/aws_noob_here_ec2_vs_sagemaker_vs_bedrock_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T14:57:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mza0wy</id>
    <title>Made Chatterbox TTS a bit faster again on CUDA (155it/s on 3090)</title>
    <updated>2025-08-24T22:49:20+00:00</updated>
    <author>
      <name>/u/RSXLV</name>
      <uri>https://old.reddit.com/user/RSXLV</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Code: &lt;a href="https://github.com/rsxdalv/chatterbox/tree/faster"&gt;https://github.com/rsxdalv/chatterbox/tree/faster&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Previous version discussion: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lfnn7b/optimized_chatterbox_tts_up_to_24x_nonbatched/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lfnn7b/optimized_chatterbox_tts_up_to_24x_nonbatched/&lt;/a&gt; (hopefully most of the old questions will become obsolete)&lt;/p&gt; &lt;p&gt;Disclaimer - for batched generation in dedicated deployments Chatterbox-VLLM should be the better choice.&lt;/p&gt; &lt;p&gt;I have mostly exhausted the options for speeding up almost vanilla HF Transformers' Llama with torch. Inductor, Triton, Max Autotune, different cache sizes etc, and they are available in the codebase. In the end, manually capturing cuda-graphs was the fastest. The model should be able to run around 230 it/s with fused kernels and better code. (I was unable to remedy the kv_cache code to enable cuda graph capture with torch.compile's max autotune.) Besides the speed, the main benefit is that setting a small cache size is no longer necessary, neither are max_new_tokens important. I plan to make it compile by default to facilitate drop-in use in other projects. Since the main effort is exhausted, I will keep on updating incrementally - for example, speeding up the s3gen (which is now a bottleneck).&lt;/p&gt; &lt;h1&gt;Results for 1500 cache size with BFloat16&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 304 Input embeds shape before padding: torch.Size([2, 188, 1024]) Sampling: 32%|███▏ | 320/1000 [00:02&amp;lt;00:04, 159.15it/s] Stopping at 321 because EOS token was generated Generated 321 tokens in 2.05 seconds 156.29 it/s Estimated token count: 304 Input embeds shape before padding: torch.Size([2, 188, 1024]) Sampling: 32%|███▏ | 320/1000 [00:01&amp;lt;00:03, 170.52it/s] Stopping at 321 because EOS token was generated Generated 321 tokens in 1.88 seconds 170.87 it/s Estimated token count: 606 Input embeds shape before padding: torch.Size([2, 339, 1024]) Sampling: 62%|██████▏ | 620/1000 [00:04&amp;lt;00:02, 154.58it/s] Stopping at 621 because EOS token was generated Generated 621 tokens in 4.01 seconds 154.69 it/s Estimated token count: 20 Input embeds shape before padding: torch.Size([2, 46, 1024]) Sampling: 4%|▍ | 40/1000 [00:00&amp;lt;00:05, 182.08it/s] Stopping at 41 because EOS token was generated Generated 41 tokens in 0.22 seconds 184.94 it/s &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Disabling classifier free guidance (cfg_weight=0)&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 304 Input embeds shape before padding: torch.Size([1, 187, 1024]) Sampling: 100%|██████████| 300/300 [00:01&amp;lt;00:00, 169.38it/s] Stopping at 300 because max_new_tokens reached Generated 300 tokens in 1.89 seconds 158.95 it/s Estimated token count: 304 Input embeds shape before padding: torch.Size([1, 187, 1024]) Sampling: 100%|██████████| 300/300 [00:01&amp;lt;00:00, 194.04it/s] Stopping at 300 because max_new_tokens reached Generated 300 tokens in 1.55 seconds 193.66 it/s Estimated token count: 606 Input embeds shape before padding: torch.Size([1, 338, 1024]) Sampling: 100%|██████████| 300/300 [00:01&amp;lt;00:00, 182.28it/s] Stopping at 300 because max_new_tokens reached Generated 300 tokens in 1.65 seconds 182.22 it/s Estimated token count: 20 Input embeds shape before padding: torch.Size([1, 45, 1024]) Sampling: 20%|██ | 60/300 [00:00&amp;lt;00:01, 208.54it/s] Stopping at 61 because EOS token was generated Generated 61 tokens in 0.29 seconds 210.54 it/s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Current code example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def t3_to(model: ChatterboxTTS, dtype): model.t3.to(dtype=dtype) model.conds.t3.to(dtype=dtype) torch.cuda.empty_cache() return model # Most new GPUs would work the fastest with this, but not all. t3_to(model, torch.bfloat16) audio = model.generate(&amp;quot;fast generation using cudagraphs-manual, warmup&amp;quot;) audio = model.generate(&amp;quot;fast generation using cudagraphs-manual, full speed&amp;quot;) # Extra options: audio = model.generate( text, t3_params={ # &amp;quot;initial_forward_pass_backend&amp;quot;: &amp;quot;eager&amp;quot;, # slower - default # &amp;quot;initial_forward_pass_backend&amp;quot;: &amp;quot;cudagraphs&amp;quot;, # speeds up set up # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;cudagraphs-manual&amp;quot;, # fastest - default # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;cudagraphs&amp;quot;, # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;eager&amp;quot;, # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;inductor&amp;quot;, # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;inductor-strided&amp;quot;, # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;cudagraphs-strided&amp;quot;, # &amp;quot;stride_length&amp;quot;: 4, # &amp;quot;strided&amp;quot; options compile &amp;lt;1-2-3-4&amp;gt; iteration steps together, which improves performance by reducing memory copying issues in torch.compile # &amp;quot;skip_when_1&amp;quot;: True, # skips Top P when it's set to 1.0 # &amp;quot;benchmark_t3&amp;quot;: True, # Synchronizes CUDA to get the real it/s } ) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RSXLV"&gt; /u/RSXLV &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mza0wy/made_chatterbox_tts_a_bit_faster_again_on_cuda/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mza0wy/made_chatterbox_tts_a_bit_faster_again_on_cuda/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mza0wy/made_chatterbox_tts_a_bit_faster_again_on_cuda/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T22:49:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1myqkqh</id>
    <title>Elmo is providing</title>
    <updated>2025-08-24T08:54:37+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"&gt; &lt;img alt="Elmo is providing" src="https://preview.redd.it/n6p9jpdvlxkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e03cd4c5782959f5dca22ea135d42d7032a20b59" title="Elmo is providing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n6p9jpdvlxkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T08:54:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz6som</id>
    <title>Almost done with the dashboard for local llama.cpp agents</title>
    <updated>2025-08-24T20:38:43+00:00</updated>
    <author>
      <name>/u/PayBetter</name>
      <uri>https://old.reddit.com/user/PayBetter</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz6som/almost_done_with_the_dashboard_for_local_llamacpp/"&gt; &lt;img alt="Almost done with the dashboard for local llama.cpp agents" src="https://b.thumbs.redditmedia.com/7LaV7Jli4Sm51VyrQaQKuYWfN3-w_vEMntHaCP24k1w.jpg" title="Almost done with the dashboard for local llama.cpp agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This won't be for sale and will be released as open source with a non commercial license. No code will be released until after the hackathon I've entered is over next month.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PayBetter"&gt; /u/PayBetter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mz6som"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz6som/almost_done_with_the_dashboard_for_local_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz6som/almost_done_with_the_dashboard_for_local_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T20:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mznzt6</id>
    <title>Testers for Seed-OSS tool calling wanted!</title>
    <updated>2025-08-25T11:33:19+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following the adoption of the model architecture itself, I've added a pull request to llama.cpp to support Seed-OSS native toolcalls and reasoning:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15552"&gt;https://github.com/ggml-org/llama.cpp/pull/15552&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This one has been somewhat annoying because Seed has its own toolcalling format, very similar to the infamous Qwen-Coder, so I would be grateful if someone being able to run the model at a higher quant than Q2_K_S could test it send report on any potential problems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mznzt6/testers_for_seedoss_tool_calling_wanted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mznzt6/testers_for_seedoss_tool_calling_wanted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mznzt6/testers_for_seedoss_tool_calling_wanted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T11:33:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzk0jr</id>
    <title>Efficiently detecting spam e-mails: can super small LLMs like Gemma 3 270M do it?</title>
    <updated>2025-08-25T07:29:32+00:00</updated>
    <author>
      <name>/u/s101c</name>
      <uri>https://old.reddit.com/user/s101c</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been reiterated many times that the 270M Gemma has been created to be finetuned for specific narrow tasks and that it works wells as a classifier.&lt;/p&gt; &lt;p&gt;So here's a use-case: a website with a contact form receives human-written messages, all the conventional spam filters work, but plenty of the irrelevant messages still get through because they are copy-pasted and written by actual people.&lt;/p&gt; &lt;p&gt;Does Gemma 270M and other similar sized models effectively classify those messages as spam? Is there a reason to use bigger models for this kind of tasks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/s101c"&gt; /u/s101c &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk0jr/efficiently_detecting_spam_emails_can_super_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk0jr/efficiently_detecting_spam_emails_can_super_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk0jr/efficiently_detecting_spam_emails_can_super_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T07:29:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzfh73</id>
    <title>Intel Granite Rapids CPU on sale at Newegg up to 65% off MSRP</title>
    <updated>2025-08-25T03:04:12+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very good news for people who want to run the huge MoE models nowadays.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;CPU&lt;/th&gt; &lt;th align="left"&gt;MSRP&lt;/th&gt; &lt;th align="left"&gt;newegg&lt;/th&gt; &lt;th align="left"&gt;% off&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;6980P&lt;/td&gt; &lt;td align="left"&gt;$17800&lt;/td&gt; &lt;td align="left"&gt;$6179&lt;/td&gt; &lt;td align="left"&gt;65.29%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6972P&lt;/td&gt; &lt;td align="left"&gt;$14600&lt;/td&gt; &lt;td align="left"&gt;$5433.2&lt;/td&gt; &lt;td align="left"&gt;62.79%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6944P&lt;/td&gt; &lt;td align="left"&gt;$6850&lt;/td&gt; &lt;td align="left"&gt;$4208&lt;/td&gt; &lt;td align="left"&gt;38.57%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6781P&lt;/td&gt; &lt;td align="left"&gt;$8960&lt;/td&gt; &lt;td align="left"&gt;$7590&lt;/td&gt; &lt;td align="left"&gt;15.29%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6761P&lt;/td&gt; &lt;td align="left"&gt;$6570&lt;/td&gt; &lt;td align="left"&gt;$6001&lt;/td&gt; &lt;td align="left"&gt;8.66%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6741P&lt;/td&gt; &lt;td align="left"&gt;$4421&lt;/td&gt; &lt;td align="left"&gt;$3900&lt;/td&gt; &lt;td align="left"&gt;11.78%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6731P&lt;/td&gt; &lt;td align="left"&gt;$2700&lt;/td&gt; &lt;td align="left"&gt;$2260.1&lt;/td&gt; &lt;td align="left"&gt;16,29%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6521P&lt;/td&gt; &lt;td align="left"&gt;$1250&lt;/td&gt; &lt;td align="left"&gt;$1208.2&lt;/td&gt; &lt;td align="left"&gt;3.34%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzfh73/intel_granite_rapids_cpu_on_sale_at_newegg_up_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzfh73/intel_granite_rapids_cpu_on_sale_at_newegg_up_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzfh73/intel_granite_rapids_cpu_on_sale_at_newegg_up_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T03:04:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzpi3o</id>
    <title>Biased comparison of frontends</title>
    <updated>2025-08-25T12:46:08+00:00</updated>
    <author>
      <name>/u/moritzchow</name>
      <uri>https://old.reddit.com/user/moritzchow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since day 1 of my journey on using local LLMs (I jumped right in without actually trying the ChatGPT that kind of providers) I’ve been using Open-WebUI that is kind of vanilla when it comes to an Unraid server setup (Ollama + Open WebUI).&lt;/p&gt; &lt;p&gt;After going deeper into this I switched hardwares, backends, frontends, and become a little bit frustrated in the recent development of OWUI.&lt;/p&gt; &lt;p&gt;Let’s cut short (not short tbh):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open WebUI: Pros: &lt;/li&gt; &lt;li&gt;easy to use and setup on docker&lt;/li&gt; &lt;li&gt;integrated web search&lt;/li&gt; &lt;li&gt;customisation including parameters, TTS&lt;/li&gt; &lt;li&gt;WebUI to serve LLM across devices&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cons: - No native support on MCP servers (a dealbreaker for me since recent MCP development) - separate backend is required&lt;/p&gt; &lt;ol&gt; &lt;li&gt;LM Studio: Pros:&lt;/li&gt; &lt;li&gt;one-stop solution for downloading and running local LLM on different hardwares including Apple Silicon&lt;/li&gt; &lt;li&gt;native MCP server support&lt;/li&gt; &lt;li&gt;easy to setup and run (can’t be easier tbh)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cons: - no web search (it can be done via MCP tool tho) - no WebUI for serving LLM across devices (sad it’s almost perfect) - no plug-ins (the registration on beta channel did not work for me)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;AnythingLLM: Pros:&lt;/li&gt; &lt;li&gt;Support Serving LLM on docker&lt;/li&gt; &lt;li&gt;Support different backends&lt;/li&gt; &lt;li&gt;AI Agent setup made easy&lt;/li&gt; &lt;li&gt;Sophisticated RAG setup&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cons: - No Serving LLM across devices if running desktop version - No customisation on using different external TTS endpoints - Agent has to be called out in each chat&lt;/p&gt; &lt;ol&gt; &lt;li&gt;LibreChat: Pros:&lt;/li&gt; &lt;li&gt;Native support on MCP servers&lt;/li&gt; &lt;li&gt;Support different backends&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cons: - Pain in the bud in setting up&lt;/p&gt; &lt;ol&gt; &lt;li&gt;SillyTavern Pros:&lt;/li&gt; &lt;li&gt;Support different backends&lt;/li&gt; &lt;li&gt;Sophisticated RP setting (some find it useful)&lt;/li&gt; &lt;li&gt;Extension available at ease on supporting MCP servers&lt;/li&gt; &lt;li&gt;customisable TTS setup&lt;/li&gt; &lt;li&gt;once it’s up and running you can get things out of it that no other frontends can give you&lt;/li&gt; &lt;li&gt;WebUI serving across devices is available&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Cons: - Setting up docker is not the most easiest thing - setting up the rest through UI is a daunting task before things can be up and running - Seriously SillyTavern? How can it be named like that while having such full features available? I can’t even tell people I learn things through it&lt;/p&gt; &lt;p&gt;Verdict: I’m using ST now while it’s not the perfect solution and the damn silly name.&lt;/p&gt; &lt;p&gt;All the frontends tested here are quite good actually, it’s just that ST seems to offer more while meaning it’s another rabbit hole.&lt;/p&gt; &lt;p&gt;LM Studio is my go to backend + frontend for its support on different architectures including Apple Silicon (I switched to Apple from ROCm). If ever they can offer same interfaces via webUI it will be a killer.&lt;/p&gt; &lt;p&gt;Not tested much on LibreChat cuz it’s a painful setup and maintenance&lt;/p&gt; &lt;p&gt;Open WebUI started to becoming a No No for me since it’s MCPO model of supporting MCP servers&lt;/p&gt; &lt;p&gt;AnythingLLM - I’m not a big RAG user but it’s quite nice on that plus the nice interface. I just hated that I need to call the agent every new chat.&lt;/p&gt; &lt;p&gt;So to wrap up - give them a try yourself if you’re looking for different frontends. Plz let me know if you have some UI recommendations as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moritzchow"&gt; /u/moritzchow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzpi3o/biased_comparison_of_frontends/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzpi3o/biased_comparison_of_frontends/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzpi3o/biased_comparison_of_frontends/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T12:46:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzvk44</id>
    <title>Codebase to Knowledge Graph generator</title>
    <updated>2025-08-25T16:39:59+00:00</updated>
    <author>
      <name>/u/DeathShot7777</name>
      <uri>https://old.reddit.com/user/DeathShot7777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvk44/codebase_to_knowledge_graph_generator/"&gt; &lt;img alt="Codebase to Knowledge Graph generator" src="https://external-preview.redd.it/aXlnMWRvdXExN2xmMW6IHesd2IpIEgbCcYmw7k3fEr5nk2vPdZm2_jU5G_lC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0cf22eb683182f2e28b2652a8c7fac245c1add93" title="Codebase to Knowledge Graph generator" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m working on a side project that generates a Knowledge Graph from codebases and provides a Graph-RAG-based chatbot. It runs entirely client-side in the browser, making it privacy-focused. I’m using &lt;strong&gt;tree-sitter.wasm&lt;/strong&gt; to parse code inside the browser and logic to use the generated AST to map out all relations. Now trying to optimize it through parallel processing with Web Workers, worker pool. For the in-memory graph database, I’m using &lt;strong&gt;KuzuDB&lt;/strong&gt;, which also runs through WebAssembly (&lt;strong&gt;kuzu.wasm&lt;/strong&gt;). Graph RAG chatbot uses langchains ReAct agent, generating cypher queries to get information.&lt;/p&gt; &lt;p&gt;In theory since its graph based, it should be much more accurate than traditional RAG, hoping to make it as useful and easy to use as gitingest / gitdiagram, and be helpful in understanding big repositories. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Need advice from anyone who has experience in graph rag agents, will this be better than rag based grep features which is popular in all AI IDEs.&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathShot7777"&gt; /u/DeathShot7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gix425uq17lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvk44/codebase_to_knowledge_graph_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvk44/codebase_to_knowledge_graph_generator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T16:39:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mztnjn</id>
    <title>Explaining the Real Reason I Started My AI Chatbot Project</title>
    <updated>2025-08-25T15:30:14+00:00</updated>
    <author>
      <name>/u/RIPT1D3_Z</name>
      <uri>https://old.reddit.com/user/RIPT1D3_Z</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, &lt;/p&gt; &lt;p&gt;Since I’ve been sharing my progress here for a while, I realized I never actually explained why I decided to build my own chatbot platform in the first place. So I wanted to share the story behind it — and hear your thoughts. &lt;/p&gt; &lt;p&gt;I’ve been a SillyTavern user for over a year. It’s an amazing project — powerful, flexible, and full of features. But when I tried to get some of my friends (non-devs) into it… it was a disaster. And that experience is what pushed me to start building something new. &lt;/p&gt; &lt;p&gt;Here’s what happened: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Installation&lt;br /&gt; For people without a tech background, even the first step was too much.&lt;br /&gt; “Why do I need Node.js?” “Why isn’t this working?”&lt;br /&gt; Most didn’t even make it past setup. I had to handhold every step, including setting up a local LLM. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Interface&lt;br /&gt; Once they finally got it running, they were overwhelmed. The UI is super dense, menus and sliders everywhere, with no clear explanations. Questions I got: &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;“What does this slider even do?” &lt;/p&gt; &lt;p&gt;“How do I actually start chatting with a character?” &lt;/p&gt; &lt;p&gt;“Why does the chat keep resetting?” &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Characters, models, prompts&lt;br /&gt; Total confusion. Where to find characters? How to write prompts? Which models to pick, how to run them, whether their hardware could handle it?&lt;br /&gt; One of my friends literally asked if they needed to learn Python just to talk to a chatbot. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Extensions and advanced features&lt;br /&gt; Most didn’t even know extensions or agents existed. And even if they did, all the info is scattered across Discord threads. Documentation is spotty at best, and half the knowledge is just “tribal.” &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So here’s where my project comes in&lt;br /&gt; That frustration gave me an idea: what if there was a dead-simple LLM chatbot platform? Something that just runs in the browser — no GitHub setup, no config hell, no Discord archaeology. &lt;/p&gt; &lt;p&gt;You’d just: &lt;/p&gt; &lt;p&gt;Pick a model &lt;/p&gt; &lt;p&gt;Load a character &lt;/p&gt; &lt;p&gt;Maybe tweak some behavior &lt;/p&gt; &lt;p&gt;And it just works. &lt;/p&gt; &lt;p&gt;Right now, it’s just me building this solo. I’ve been sharing my development journey here in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, and I’ll keep posting progress updates, demos, and breakdowns as I go. &lt;/p&gt; &lt;p&gt;I’d love to hear your thoughts on this problem - do you see the same barriers for newcomers?&lt;br /&gt; And if anyone here wants to help test my platform (currently with unlimited tokens), just DM me and I’ll send you an invite.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RIPT1D3_Z"&gt; /u/RIPT1D3_Z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mztnjn/explaining_the_real_reason_i_started_my_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mztnjn/explaining_the_real_reason_i_started_my_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mztnjn/explaining_the_real_reason_i_started_my_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T15:30:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzsg6v</id>
    <title>DeepSeek V3.1 - Getting token " extreme" / "极" / "極" out of nowhere</title>
    <updated>2025-08-25T14:46:02+00:00</updated>
    <author>
      <name>/u/notdba</name>
      <uri>https://old.reddit.com/user/notdba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did some testing with DeepSeek V3.1, and found that somehow the model likes to generate the token:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot; extreme&amp;quot; (id:15075)&lt;/li&gt; &lt;li&gt;&amp;quot;极&amp;quot; (id:2577, extreme in Simplified Chinese)&lt;/li&gt; &lt;li&gt;&amp;quot;極&amp;quot; (id:16411, extreme in Traditional Chinese)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;in totally unexpected places.&lt;/p&gt; &lt;p&gt;At first I thought it was due to the extreme IQ1_S quantization that I did or some edge case with imatrix calibration dataset, but then the same issue also happened with the FP8 full precision model from Fireworks.&lt;/p&gt; &lt;p&gt;Case 1 (local ik_llama.cpp, top_k=1, temperature=1):&lt;br /&gt; Expected: time.Second&lt;br /&gt; Generated: time.Se极&lt;br /&gt; Logprobs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;top_logprobs&amp;quot;: [ { &amp;quot;id&amp;quot;: 2577, &amp;quot;token&amp;quot;: &amp;quot;极&amp;quot;, &amp;quot;bytes&amp;quot;: [230,158,129], &amp;quot;logprob&amp;quot;: -1.3718461990356445 }, { &amp;quot;id&amp;quot;: 1511, &amp;quot;token&amp;quot;: &amp;quot;cond&amp;quot;, &amp;quot;bytes&amp;quot;: [99,111,110,100], &amp;quot;logprob&amp;quot;: -1.5412302017211914 }, { &amp;quot;id&amp;quot;: 1957, &amp;quot;token&amp;quot;: &amp;quot; second&amp;quot;, &amp;quot;bytes&amp;quot;: [32,115,101,99,111,110,100], &amp;quot;logprob&amp;quot;: -1.9008493423461914 } ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Case 2 (local ik_llama.cpp, top_k=1, temperature=1):&lt;br /&gt; Expected: time.Second&lt;br /&gt; Generated: &lt;a href="http://time.Se"&gt;time.Se&lt;/a&gt; extreme&lt;br /&gt; Logprobs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;top_logprobs&amp;quot;: [ { &amp;quot;id&amp;quot;: 15075, &amp;quot;token&amp;quot;: &amp;quot; extreme&amp;quot;, &amp;quot;bytes&amp;quot;: [32,101,120,116,114,101,109,101], &amp;quot;logprob&amp;quot;: -1.0279325246810913 }, { &amp;quot;id&amp;quot;: 2577, &amp;quot;token&amp;quot;: &amp;quot;极&amp;quot;, &amp;quot;bytes&amp;quot;: [230,158,129], &amp;quot;logprob&amp;quot;: -1.077283263206482 }, { &amp;quot;id&amp;quot;: 9189, &amp;quot;token&amp;quot;: &amp;quot; extrem&amp;quot;, &amp;quot;bytes&amp;quot;: [32,101,120,116,114,101,109], &amp;quot;logprob&amp;quot;: -1.8691496849060059 } ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Case 3 (fireworks, top_k=1, temperature=1):&lt;br /&gt; Expected: V1&lt;br /&gt; Generated: V极&lt;br /&gt; Logprobs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;top_logprobs&amp;quot;: [ { &amp;quot;token&amp;quot;: &amp;quot;极&amp;quot;, &amp;quot;logprob&amp;quot;: -0.27936283, &amp;quot;token_id&amp;quot;: 2577, &amp;quot;bytes&amp;quot;: [230,158,129] }, { &amp;quot;token&amp;quot;: &amp;quot;1&amp;quot;, &amp;quot;logprob&amp;quot;: -1.90436232, &amp;quot;token_id&amp;quot;: 19, &amp;quot;bytes&amp;quot;: [49] }, { &amp;quot;token&amp;quot;: &amp;quot;極&amp;quot;, &amp;quot;logprob&amp;quot;: -2.40436196, &amp;quot;token_id&amp;quot;: 16411, &amp;quot;bytes&amp;quot;: [230,165,181] } ], &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Worse still, other than these 3 cases where an extreme token was the top choice in greedy decoding, these extreme tokens are also constantly lurking as the 2nd or 3rd choice in other unexpected places as well.&lt;/p&gt; &lt;p&gt;I have done this exact eval for all the popular coding models, and this is the first time I am seeing this kind of issue. Has anyone experienced this?&lt;/p&gt; &lt;p&gt;EDIT: Seeing the same issue with Novita as well, so it is quite unlikely to be an issue with the inference stack.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notdba"&gt; /u/notdba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzsg6v/deepseek_v31_getting_token_extreme_极_極_out_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzsg6v/deepseek_v31_getting_token_extreme_极_極_out_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzsg6v/deepseek_v31_getting_token_extreme_极_極_out_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T14:46:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz4hrg</id>
    <title>All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th</title>
    <updated>2025-08-24T19:10:09+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"&gt; &lt;img alt="All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th" src="https://b.thumbs.redditmedia.com/fUU-BLlYX-WkpMfx3LdfGqjKydfcxu7DsHg7PwU2cQk.jpg" title="All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;China is not only the main competitor to the US in the overall AI race, but dominating the open-source landscape. Out of the open source models listed on &lt;a href="https://www.designarena.ai/"&gt;Design Arena&lt;/a&gt; (a UI/UX and frontend benchmark for LLMs), Chinese models take up all of the top 15 spots with the first non-Chinese model making its appearing at #16 as GPT OSS 120B, developed by Open AI. &lt;/p&gt; &lt;p&gt;It's really remarkable what DeepSeek, Zhipu, Kimi, and Qwen have been able to do while staying OS. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mz4hrg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T19:10:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzm5dk</id>
    <title>support interns1-mini has been merged into llama.cpp</title>
    <updated>2025-08-25T09:49:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm5dk/support_interns1mini_has_been_merged_into_llamacpp/"&gt; &lt;img alt="support interns1-mini has been merged into llama.cpp" src="https://external-preview.redd.it/C4PZMcjKvXogRwaLothTEm2AuNm9c8ehdTTP3nuiquQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=136a713391edcd4645ecfc6fd874eb5f837f3b30" title="support interns1-mini has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-mini"&gt;https://huggingface.co/internlm/Intern-S1-mini&lt;/a&gt;&lt;/p&gt; &lt;p&gt;model description:&lt;/p&gt; &lt;p&gt;We introduce &lt;strong&gt;Intern-S1-mini&lt;/strong&gt;, a lightweight open-source multimodal reasoning model based on the same techniques as &lt;a href="https://huggingface.co/internlm/Intern-S1"&gt;&lt;strong&gt;Intern-S1&lt;/strong&gt;&lt;/a&gt;. Built upon an 8B dense language model (Qwen3) and a 0.3B Vision encoder (InternViT), Intern-S1-mini has been further pretrained on &lt;strong&gt;5 trillion tokens&lt;/strong&gt; of multimodal data, including over &lt;strong&gt;2.5 trillion scientific-domain tokens&lt;/strong&gt;. This enables the model to retain strong general capabilities while excelling in specialized scientific domains such as &lt;strong&gt;interpreting chemical structures, understanding protein sequences, and planning compound synthesis routes&lt;/strong&gt;, making Intern-S1-mini to be a capable research assistant for real-world scientific applications.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-mini#features"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Strong performance across language and vision reasoning benchmarks, especially scientific tasks.&lt;/li&gt; &lt;li&gt;Continuously pretrained on a massive 5T token dataset, with over 50% specialized scientific data, embedding deep domain expertise.&lt;/li&gt; &lt;li&gt;Dynamic tokenizer enables native understanding of molecular formulas and protein sequences.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15412"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm5dk/support_interns1mini_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm5dk/support_interns1mini_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T09:49:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzvns5</id>
    <title>You can run GGUFs with Lemonade straight from Hugging Face now</title>
    <updated>2025-08-25T16:43:50+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvns5/you_can_run_ggufs_with_lemonade_straight_from/"&gt; &lt;img alt="You can run GGUFs with Lemonade straight from Hugging Face now" src="https://b.thumbs.redditmedia.com/dwJPSl-GCLGC8P_zJkDjJc59pTe5_mdagvacnnAFmhc.jpg" title="You can run GGUFs with Lemonade straight from Hugging Face now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Huge shoutout to the Hugging Face team for this, along with all the other amazing libraries and services they provide for free to the community.&lt;/p&gt; &lt;p&gt;Quick way to run any GGUF model on your PC with Lemonade:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Go to any model page, like &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"&gt;Unsloth's Qwen3-Coder-30B-A3B&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Click &amp;quot;Use this model&amp;quot; in the top-right.&lt;/li&gt; &lt;li&gt;Clicking Lemonade will give you instructions like &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF?local-app=lemonade"&gt;this&lt;/a&gt; (second picture in the post).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Links in comments if anyone wants to tinker with us.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mzvns5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvns5/you_can_run_ggufs_with_lemonade_straight_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvns5/you_can_run_ggufs_with_lemonade_straight_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T16:43:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzm677</id>
    <title>u/RSXLV appreciation post for releasing his updated faster Chatterbox-TTS fork yesterday. Major speed increase indeed, response is near real-time now. Let's all give him a big ol' thank you! Fork in the comments.</title>
    <updated>2025-08-25T09:51:02+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm677/ursxlv_appreciation_post_for_releasing_his/"&gt; &lt;img alt="u/RSXLV appreciation post for releasing his updated faster Chatterbox-TTS fork yesterday. Major speed increase indeed, response is near real-time now. Let's all give him a big ol' thank you! Fork in the comments." src="https://external-preview.redd.it/Nm9qN2ppZGIwNWxmMYB8gfxVUG7ntLAy6UFGKU3bfv7xh4HVFM-UizvnZAOP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e82b54df6d326f2b562855d3069b5bdeddfccffd" title="u/RSXLV appreciation post for releasing his updated faster Chatterbox-TTS fork yesterday. Major speed increase indeed, response is near real-time now. Let's all give him a big ol' thank you! Fork in the comments." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fork: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mza0wy/comment/nak1lea/?context=3"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mza0wy/comment/nak1lea/?context=3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="/u/RSXLV"&gt;u/RSXLV&lt;/a&gt; again, huge shoutout to you, my guy. This fork is so fast now &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9txv4idb05lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm677/ursxlv_appreciation_post_for_releasing_his/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzm677/ursxlv_appreciation_post_for_releasing_his/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T09:51:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzk3ft</id>
    <title>So, even the Sheikh of Dubai is waiting for the DGX SPARK</title>
    <updated>2025-08-25T07:34:50+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk3ft/so_even_the_sheikh_of_dubai_is_waiting_for_the/"&gt; &lt;img alt="So, even the Sheikh of Dubai is waiting for the DGX SPARK" src="https://preview.redd.it/ouehxl1lc4lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f15a36d80110b140f159feccb9e39f5909232e6" title="So, even the Sheikh of Dubai is waiting for the DGX SPARK" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone will get one for Christmas, Jensen said.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ouehxl1lc4lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk3ft/so_even_the_sheikh_of_dubai_is_waiting_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzk3ft/so_even_the_sheikh_of_dubai_is_waiting_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T07:34:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzwcs8</id>
    <title>Qwen Wan2.2-S2V is coming soon</title>
    <updated>2025-08-25T17:08:46+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"&gt; &lt;img alt="Qwen Wan2.2-S2V is coming soon" src="https://preview.redd.it/9xwkq1az67lf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d418420a969fcd5b88779cc4eb2389257267480c" title="Qwen Wan2.2-S2V is coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Wan/status/1959963989703880866?t=F29sqn8rWrVM-ia3qz4cvQ&amp;amp;s=19"&gt;https://x.com/Alibaba_Wan/status/1959963989703880866?t=F29sqn8rWrVM-ia3qz4cvQ&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9xwkq1az67lf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:08:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzquqi</id>
    <title>GRPO please stop punishing your correct token</title>
    <updated>2025-08-25T13:42:56+00:00</updated>
    <author>
      <name>/u/Gildarts777</name>
      <uri>https://old.reddit.com/user/Gildarts777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzquqi/grpo_please_stop_punishing_your_correct_token/"&gt; &lt;img alt="GRPO please stop punishing your correct token" src="https://preview.redd.it/mdaobm9t56lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9172793aa7a56b0f2e4540faa0f91d3bddb43291" title="GRPO please stop punishing your correct token" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been experimenting with a training approach I’m calling &lt;strong&gt;GTPO (Group-relative Trajectory-based Policy Optimization)&lt;/strong&gt;.&lt;br /&gt; It started as a way to fix some quirks I ran into with GRPO, like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Conflicting gradients&lt;/strong&gt;: tokens showing up in both “good” and “bad” completions getting pulled in opposite directions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Policy collapse&lt;/strong&gt;: models flattening out when some completions had strong negative updates.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I tried&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I added a small mechanism to &lt;em&gt;skip negative updates&lt;/em&gt; on “conflict tokens.”&lt;/li&gt; &lt;li&gt;Instead of using KL with a reference model, I tried filtering out high-entropy completions (trajectories that are basically too noisy).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I noticed&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Training was more stable and didn’t wreck formatting.&lt;/li&gt; &lt;li&gt;I didn’t need a reference model, which made runs lighter.&lt;/li&gt; &lt;li&gt;Even on Colab (using Unsloth) I could fine-tune without things blowing up.&lt;/li&gt; &lt;li&gt;On reasoning datasets like &lt;strong&gt;GSM8K, MATH, AIME 2024 (see Figure)&lt;/strong&gt; with LLaMA 8B and Qwen 3B, results were consistently better than my GRPO baselines.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Links if you want to poke around&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2508.03772"&gt;arXiv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/winstonsmith1897/GTPO"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Colab example: &lt;a href="https://colab.research.google.com/github/winstonsmith1897/GTPO/blob/main/colab/GTPO_training_example.ipynb"&gt;Notebook&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m curious what others think, especially folks who’ve been fine-tuning with GRPO or similar. Do you have any benchmarks or setups you’d like me to test it on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gildarts777"&gt; /u/Gildarts777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mdaobm9t56lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzquqi/grpo_please_stop_punishing_your_correct_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzquqi/grpo_please_stop_punishing_your_correct_token/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T13:42:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzu2e6</id>
    <title>GLM-4.5 appreciation post</title>
    <updated>2025-08-25T15:45:21+00:00</updated>
    <author>
      <name>/u/wolttam</name>
      <uri>https://old.reddit.com/user/wolttam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM-4.5 is my favorite model at the moment, full stop.&lt;/p&gt; &lt;p&gt;I don't work on insanely complex problems; I develop pretty basic web applications and back-end services. I don't vibe code. LLMs come in when I have a well-defined task, and I have generally always been able to get frontier models to one or two-shot the code I'm looking for with the context I manually craft for it.&lt;/p&gt; &lt;p&gt;I've kept (near religious) watch on open models, and it's only been since the recent Qwen updates, Kimi, and GLM-4.5 that I've really started to take them seriously. All of these models are fantastic, but GLM-4.5 especially has completely removed any desire I've had to reach for a proprietary frontier model for the tasks I work on.&lt;/p&gt; &lt;p&gt;Chinese models have effectively captured me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wolttam"&gt; /u/wolttam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T15:45:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzn0zm</id>
    <title>InternVL3_5 series is out!!</title>
    <updated>2025-08-25T10:40:58+00:00</updated>
    <author>
      <name>/u/kironlau</name>
      <uri>https://old.reddit.com/user/kironlau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt; &lt;img alt="InternVL3_5 series is out!!" src="https://external-preview.redd.it/oVE1-EnaLKFKvov2KcAAd41NTqlkCry1b2bYAP90Upw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e47ab110109abf15025f25857e6f9890fe89966c" title="InternVL3_5 series is out!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/organizations/internlm/activity/all"&gt;internlm (InternLM)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/resy0a6n95lf1.png?width=1294&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9db47fa8a145b99f6bd74750e0d3a0791d85137f"&gt;https://preview.redd.it/resy0a6n95lf1.png?width=1294&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9db47fa8a145b99f6bd74750e0d3a0791d85137f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kironlau"&gt; /u/kironlau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T10:40:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzrb4l</id>
    <title>llama.ui - minimal privacy focused chat interface</title>
    <updated>2025-08-25T14:01:17+00:00</updated>
    <author>
      <name>/u/COBECT</name>
      <uri>https://old.reddit.com/user/COBECT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"&gt; &lt;img alt="llama.ui - minimal privacy focused chat interface" src="https://preview.redd.it/6g2icqwi96lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93145b5e6ac2c5f127d14e540cb4261819454a6b" title="llama.ui - minimal privacy focused chat interface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/COBECT"&gt; /u/COBECT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6g2icqwi96lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T14:01:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzqy3z</id>
    <title>InternVL3.5 - Best OpenSource VLM</title>
    <updated>2025-08-25T13:46:45+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"&gt; &lt;img alt="InternVL3.5 - Best OpenSource VLM" src="https://b.thumbs.redditmedia.com/nVzY4GlZP996KhrAM5_W8vRFK-rnOrWqnRnOhiYSBYI.jpg" title="InternVL3.5 - Best OpenSource VLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/internlm/InternVL3_5-241B-A28B"&gt;https://huggingface.co/internlm/InternVL3_5-241B-A28B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;InternVL3.5 with a variety of new capabilities including GUI agent, embodied agent, etc. Specifically, InternVL3.5-241B-A28B achieves the highest overall score on multimodal general, reasoning, text, and agency tasks among leading open source MLLMs, and narrows the gap with top commercial models such as GPT-5.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mzqy3z"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T13:46:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
