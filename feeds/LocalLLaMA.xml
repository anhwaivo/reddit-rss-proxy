<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-17T16:24:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1irmn71</id>
    <title>Are there any light weight tool calling libraries for Python?</title>
    <updated>2025-02-17T15:38:50+00:00</updated>
    <author>
      <name>/u/SuperChewbacca</name>
      <uri>https://old.reddit.com/user/SuperChewbacca</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to find a really light weight universal tool calling library, does anyone know if that exists? I would like one that supports a bunch of different model formats and a universal fallback format. I know that things like this exist for some of the heavier agent libraries, but I just want something simple that I can integrate with OpenAI compatible API calls.&lt;/p&gt; &lt;p&gt;If something like the above doesn't exist, I am going to start working on it. Let me know if you have any suggestions for that project.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuperChewbacca"&gt; /u/SuperChewbacca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irmn71/are_there_any_light_weight_tool_calling_libraries/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irmn71/are_there_any_light_weight_tool_calling_libraries/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irmn71/are_there_any_light_weight_tool_calling_libraries/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T15:38:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1irms5x</id>
    <title>Parsing and manipulation tabular Data with LLMs</title>
    <updated>2025-02-17T15:44:45+00:00</updated>
    <author>
      <name>/u/kaynbockmehr</name>
      <uri>https://old.reddit.com/user/kaynbockmehr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am currently working on a project which mainly focuses on parsing tabular data from different Input formats (customer specific) into a consistent output format. &lt;/p&gt; &lt;p&gt;The general task: We recieve an input list of parts for a module as PDF. They include specifics like material standards, dimensions, producers and a general discription of the item, e.g. &amp;quot;pipe x/y inch, Material abc...&amp;quot;. Currently the lists are handled manually, the idea is to automate the process and convert it into a output file like csv. In addition, we want to use a RAG system or similar, to scan the internal database and look for the internal information like item numbers, distributors known to sell the item etc. &lt;/p&gt; &lt;p&gt;These lists are specific for every customer and, to make it worse, not consistent in of itself. Obviously this makes manually processing very annoying and prone to error. Here is an abstract example. &lt;/p&gt; &lt;p&gt;Mainly, we recieve dimensions like this:&lt;br /&gt; ...| length | width | thickness | ...&lt;br /&gt; ...|--------|--------|-----------| ...&lt;br /&gt; ...| 6m | 35mm| 2mm | ...&lt;/p&gt; &lt;p&gt;But when it comes to pipes for example, they sometimes look like this (with two fields combined into one):&lt;br /&gt; ...| length | width | thickness | ...&lt;br /&gt; ...|--------|--------|-----------| ...&lt;br /&gt; ...| 6m | 35 x 2mm | ...&lt;/p&gt; &lt;p&gt;|| || ||||||||||&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaynbockmehr"&gt; /u/kaynbockmehr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irms5x/parsing_and_manipulation_tabular_data_with_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irms5x/parsing_and_manipulation_tabular_data_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irms5x/parsing_and_manipulation_tabular_data_with_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T15:44:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1iritmv</id>
    <title>I found a cool yt automation website</title>
    <updated>2025-02-17T12:34:01+00:00</updated>
    <author>
      <name>/u/stikkrr</name>
      <uri>https://old.reddit.com/user/stikkrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://clip.studio"&gt;https://clip.studio&lt;/a&gt; its a yt automation tool, has some good features I wonder if theres open source one that can run locally&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stikkrr"&gt; /u/stikkrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iritmv/i_found_a_cool_yt_automation_website/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iritmv/i_found_a_cool_yt_automation_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iritmv/i_found_a_cool_yt_automation_website/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T12:34:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1irbke1</id>
    <title>[New Benchmark] OptiLLMBench: Test how optimization tricks can boost your models at inference time!</title>
    <updated>2025-02-17T04:28:18+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! ðŸ‘‹&lt;/p&gt; &lt;p&gt;I'm excited to share OptiLLMBench, a new benchmark specifically designed to test how different inference optimization techniques (like ReRead, Chain-of-Thought, etc.) can improve LLM performance without any fine-tuning.&lt;/p&gt; &lt;p&gt;First results with Gemini 2.0 Flash show promising improvements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ReRead (RE2): +5% accuracy while being ~14% faster&lt;/li&gt; &lt;li&gt;Chain-of-Thought Reflection: +5% boost&lt;/li&gt; &lt;li&gt;Base performance: 51%&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The benchmark tests models across:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GSM8K math word problems&lt;/li&gt; &lt;li&gt;MMLU Math&lt;/li&gt; &lt;li&gt;AQUA-RAT logical reasoning&lt;/li&gt; &lt;li&gt;BoolQ yes/no questions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why this matters:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;These optimization techniques work with ANY model&lt;/li&gt; &lt;li&gt;They can help squeeze better performance out of models without training&lt;/li&gt; &lt;li&gt;Some techniques (like RE2) actually run faster than base inference&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you're interested in trying it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dataset: &lt;a href="https://huggingface.co/datasets/codelion/optillmbench"&gt;https://huggingface.co/datasets/codelion/optillmbench&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/codelion/optillm"&gt;https://github.com/codelion/optillm&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to see results from different models and how they compare. Share your findings! ðŸ”¬&lt;/p&gt; &lt;p&gt;Edit: The benchmark and the approach is completely open source. Feel free to try it with any model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irbke1/new_benchmark_optillmbench_test_how_optimization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irbke1/new_benchmark_optillmbench_test_how_optimization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irbke1/new_benchmark_optillmbench_test_how_optimization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T04:28:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir362v</id>
    <title>Jankiest &lt;$1000 70B IQ3_M 8192ctx setup ever</title>
    <updated>2025-02-16T21:30:31+00:00</updated>
    <author>
      <name>/u/spokale</name>
      <uri>https://old.reddit.com/user/spokale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir362v/jankiest_1000_70b_iq3_m_8192ctx_setup_ever/"&gt; &lt;img alt="Jankiest &amp;lt;$1000 70B IQ3_M 8192ctx setup ever" src="https://b.thumbs.redditmedia.com/44X6DS_-957GXg-I7vPFOA1akYTRMttqhRfJ-mtdpgk.jpg" title="Jankiest &amp;lt;$1000 70B IQ3_M 8192ctx setup ever" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spokale"&gt; /u/spokale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ir362v"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir362v/jankiest_1000_70b_iq3_m_8192ctx_setup_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir362v/jankiest_1000_70b_iq3_m_8192ctx_setup_ever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T21:30:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqtfy9</id>
    <title>Just a bunch of H100s required</title>
    <updated>2025-02-16T14:33:57+00:00</updated>
    <author>
      <name>/u/Hv_V</name>
      <uri>https://old.reddit.com/user/Hv_V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtfy9/just_a_bunch_of_h100s_required/"&gt; &lt;img alt="Just a bunch of H100s required" src="https://b.thumbs.redditmedia.com/CMhWZRD3a6zl90Wagyddf6mqUWPT3h6kxFjzeLVrOCc.jpg" title="Just a bunch of H100s required" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d4tu6z13iije1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f1d71756751de2f920195fcdaebe80c93b793c6"&gt;https://preview.redd.it/d4tu6z13iije1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f1d71756751de2f920195fcdaebe80c93b793c6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hv_V"&gt; /u/Hv_V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtfy9/just_a_bunch_of_h100s_required/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtfy9/just_a_bunch_of_h100s_required/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqtfy9/just_a_bunch_of_h100s_required/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T14:33:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1irad4h</id>
    <title>Distributed DeepSeek R1 inference?</title>
    <updated>2025-02-17T03:20:47+00:00</updated>
    <author>
      <name>/u/CalangoVelho</name>
      <uri>https://old.reddit.com/user/CalangoVelho</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have about 12 GPUs (RTXs 2080 to 3090) located in different machines (each with 1-2 GPUs), mostly Windows hosts. I was wondering if it would be possible to run DeepSeek 671b (not the dist. or quant versions) by having it distributed across those hosts. Did anyone had luck doing something like that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CalangoVelho"&gt; /u/CalangoVelho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irad4h/distributed_deepseek_r1_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irad4h/distributed_deepseek_r1_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irad4h/distributed_deepseek_r1_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T03:20:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1iriz8z</id>
    <title>Speech to text that handles multiple languages in the same sentence?</title>
    <updated>2025-02-17T12:42:56+00:00</updated>
    <author>
      <name>/u/PMMEYOURSMIL3</name>
      <uri>https://old.reddit.com/user/PMMEYOURSMIL3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have some audio recordings between myself and someone I work with, and we're both bilingual and switch between the two languages, very often within the same sentence. Like along the lines of &amp;quot;hey, how are you, Ã§a va?&amp;quot; Is there any model capable of handling this? &lt;/p&gt; &lt;p&gt;Thanks &lt;/p&gt; &lt;p&gt;Edit: the two languages are English and Arabic if that makes a difference in terms of what the model supports &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PMMEYOURSMIL3"&gt; /u/PMMEYOURSMIL3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iriz8z/speech_to_text_that_handles_multiple_languages_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iriz8z/speech_to_text_that_handles_multiple_languages_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iriz8z/speech_to_text_that_handles_multiple_languages_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T12:42:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqv5s0</id>
    <title>Sorcery: Allow AI characters to reach into the real world. From the creator of DRY and XTC.</title>
    <updated>2025-02-16T15:54:42+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqv5s0/sorcery_allow_ai_characters_to_reach_into_the/"&gt; &lt;img alt="Sorcery: Allow AI characters to reach into the real world. From the creator of DRY and XTC." src="https://preview.redd.it/asmx7nh0wije1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05a245ed92468ec7ad3869e7776b9c2d8b8e5f63" title="Sorcery: Allow AI characters to reach into the real world. From the creator of DRY and XTC." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/asmx7nh0wije1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqv5s0/sorcery_allow_ai_characters_to_reach_into_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqv5s0/sorcery_allow_ai_characters_to_reach_into_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T15:54:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1irldqs</id>
    <title>Best Model for grammar correction</title>
    <updated>2025-02-17T14:43:57+00:00</updated>
    <author>
      <name>/u/maifee</name>
      <uri>https://old.reddit.com/user/maifee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The lower the VRAM the better.&lt;/p&gt; &lt;p&gt;Only usecase for model is correction of text ( Notes for studying). Any recommendations are helpful.&lt;/p&gt; &lt;p&gt;Will be a part of 100% open sourced system.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maifee"&gt; /u/maifee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irldqs/best_model_for_grammar_correction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irldqs/best_model_for_grammar_correction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irldqs/best_model_for_grammar_correction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T14:43:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1irklio</id>
    <title>Step-Audio - a stepfun-ai Collection (Apache 2 Audio Models)</title>
    <updated>2025-02-17T14:06:47+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irklio/stepaudio_a_stepfunai_collection_apache_2_audio/"&gt; &lt;img alt="Step-Audio - a stepfun-ai Collection (Apache 2 Audio Models)" src="https://external-preview.redd.it/phQl7qnxBNwj9zS7K_vfIQhmHll-NjLQgZ0PIR30DxA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=623bc922d387ff0d98a441576fd2df6e097cc4a6" title="Step-Audio - a stepfun-ai Collection (Apache 2 Audio Models)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/stepfun-ai/step-audio-67b33accf45735bb21131b0b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irklio/stepaudio_a_stepfunai_collection_apache_2_audio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irklio/stepaudio_a_stepfunai_collection_apache_2_audio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T14:06:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir6ha6</id>
    <title>DeepSeek-R1 CPU-only performances (671B , Unsloth 2.51bit, UD-Q2_K_XL)</title>
    <updated>2025-02-17T00:00:53+00:00</updated>
    <author>
      <name>/u/smflx</name>
      <uri>https://old.reddit.com/user/smflx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many of us here like to run locally DeepSeek R1 (671B, not distill). Thanks to MoE nature of DeepSeek, CPU inference looks promising.&lt;/p&gt; &lt;p&gt;I'm testing on CPUs I have. Not completed yet, but would like to share &amp;amp; hear about other CPUs too.&lt;/p&gt; &lt;p&gt;Xeon w5-3435X has 195GB/s memory bandwidth (measured by stream)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Function Best Rate MB/s Avg time Copy: 195455.5 0.082330 Scale: 161245.0 0.100906 Add: 183597.3 0.131566 Triad: 181895.4 0.132163 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The active parameter of R1/V2 is 37B. So if Q4 used, theoretically 195 / 37 * 2 = 10.5 tok/s is possible.&lt;/p&gt; &lt;p&gt;Unsloth provided great quantizations from 1.58 ~ 2.51 bit. The generation speed could be more or less. (Actually less yet)&lt;/p&gt; &lt;p&gt;&lt;a href="https://unsloth.ai/blog/deepseekr1-dynamic"&gt;https://unsloth.ai/blog/deepseekr1-dynamic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I tested both of 1.58 bit &amp;amp; 2.51 bit on few CPUs, now I stick to 2.51 bit. 2.51bit is better quality, surprisingly faster too.&lt;/p&gt; &lt;p&gt;I got 4.86 tok/s with 2.51bit, while 3.27 tok/s with 1.58bit, on Xeon w5-3435X (1570 total tokens). Also, 3.53 tok/s with 2.51bit, while 2.28 tok/s with 1.58bit, on TR pro 5955wx.&lt;/p&gt; &lt;p&gt;It means compute performance of CPU matters too, and slower with 1.58bit. So, use 2.51bit unless you don't have enough RAM. 256G RAM was enough to run 2.51 bit.&lt;/p&gt; &lt;p&gt;I have tested generation speed with llama.cpp using (1) prompt &amp;quot;hi&amp;quot;, and (2) &amp;quot;Write a python program to print the prime numbers under 100&amp;quot;. Number of tokens generated were (1) about 100, (2) 1500~5000.&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama.cpp/build/bin/llama-cli --model DeepSeek-R1-UD-Q2_K_XL/DeepSeek-R1-UD-Q2_K_XL-00001-of-00005.gguf --cache-type-k q4_0 --threads 16 --prio 2 --temp 0.6 --ctx-size 8192 --seed 3407&lt;/code&gt;&lt;/p&gt; &lt;p&gt;For &amp;quot;--threads 16&amp;quot;, I have used the core counts of each CPUs. The sweet spot could be less for the CPUs with many cores / ccd.&lt;/p&gt; &lt;p&gt;OK, here is Table.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;CPU&lt;/th&gt; &lt;th align="left"&gt;Cores (CCD)&lt;/th&gt; &lt;th align="left"&gt;COPY (GB/s)&lt;/th&gt; &lt;th align="left"&gt;TRIAD (GB/s)&lt;/th&gt; &lt;th align="left"&gt;llama.cpp prmpt 1k (tok/s)&lt;/th&gt; &lt;th align="left"&gt;llama.cpp &amp;quot;hi&amp;quot; (tok/s)&lt;/th&gt; &lt;th align="left"&gt;llama.cpp &amp;quot;coding&amp;quot; (tok/s)&lt;/th&gt; &lt;th align="left"&gt;kTrans prmpt (tok/s)&lt;/th&gt; &lt;th align="left"&gt;kTrans-former (tok/s)&lt;/th&gt; &lt;th align="left"&gt;Source&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;w5-3435X&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;195&lt;/td&gt; &lt;td align="left"&gt;181&lt;/td&gt; &lt;td align="left"&gt;15.53&lt;/td&gt; &lt;td align="left"&gt;5.17&lt;/td&gt; &lt;td align="left"&gt;4.86&lt;/td&gt; &lt;td align="left"&gt;40.77&lt;/td&gt; &lt;td align="left"&gt;8.80&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5955wx&lt;/td&gt; &lt;td align="left"&gt;16 (2)&lt;/td&gt; &lt;td align="left"&gt;96&lt;/td&gt; &lt;td align="left"&gt;70&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;4.29&lt;/td&gt; &lt;td align="left"&gt;3.53&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;7.45&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;7F32&lt;/td&gt; &lt;td align="left"&gt;8 (4)&lt;/td&gt; &lt;td align="left"&gt;128&lt;/td&gt; &lt;td align="left"&gt;86&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;3.39&lt;/td&gt; &lt;td align="left"&gt;3.24&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9184X&lt;/td&gt; &lt;td align="left"&gt;16 (8)&lt;/td&gt; &lt;td align="left"&gt;298&lt;/td&gt; &lt;td align="left"&gt;261&lt;/td&gt; &lt;td align="left"&gt;45.32&lt;/td&gt; &lt;td align="left"&gt;7.52&lt;/td&gt; &lt;td align="left"&gt;4.82&lt;/td&gt; &lt;td align="left"&gt;40.13&lt;/td&gt; &lt;td align="left"&gt;11.3&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9534&lt;/td&gt; &lt;td align="left"&gt;64 (8)&lt;/td&gt; &lt;td align="left"&gt;351&lt;/td&gt; &lt;td align="left"&gt;276&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;7.26&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6426Y (2P)&lt;/td&gt; &lt;td align="left"&gt;16+16&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;coming...&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6980P (2P)&lt;/td&gt; &lt;td align="left"&gt;128+128&lt;/td&gt; &lt;td align="left"&gt;314&lt;/td&gt; &lt;td align="left"&gt;311&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="/u/VoidAlchemy"&gt;u/VoidAlchemy&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;i5 13600K&lt;/td&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;65&lt;/td&gt; &lt;td align="left"&gt;60&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;1.69&lt;/td&gt; &lt;td align="left"&gt;1.66&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="/u/napkinolympics"&gt;u/napkinolympics&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I separate table for setup with GPUs.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;CPU&lt;/th&gt; &lt;th align="left"&gt;GPU&lt;/th&gt; &lt;th align="left"&gt;llama.cpp &amp;quot;hi&amp;quot; (tok/s)&lt;/th&gt; &lt;th align="left"&gt;llama.cpp &amp;quot;coding&amp;quot; (tok/s)&lt;/th&gt; &lt;th align="left"&gt;Source&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;7960X&lt;/td&gt; &lt;td align="left"&gt;4x 3090, 2x 3090 (via RPC)&lt;/td&gt; &lt;td align="left"&gt;7.68&lt;/td&gt; &lt;td align="left"&gt;6.37&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="/u/CheatCodesOfLife"&gt;u/CheatCodesOfLife&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I expected a poor performance of 5955wx, because it has only two CCDs. We can see low memory bandwidth in the table. But, not much difference of performance compared to w5-3435X. Perhaps, compute matters too &amp;amp; memory bandwidth is not saturated in Xeon w5-3435X.&lt;/p&gt; &lt;p&gt;I have checked performance of kTransformer too. It's CPU inference with 1 GPU for compute bound process. While it is not pure CPU inference, the performance gain is almost 2x. I didn't tested for all CPU yet, you can assume 2x performances over CPU-only llama.cpp.&lt;/p&gt; &lt;p&gt;With kTransformer, GPU usage was not saturated but CPU was all busy. I guess one 3090 or 4090 will be enough. One downside of kTransformer is that the context length is limited by VRAM.&lt;/p&gt; &lt;p&gt;The blanks in Table are &amp;quot;not tested yet&amp;quot;. It takes time... Well, I'm testing two Genoa CPUs with only one mainboard.&lt;/p&gt; &lt;p&gt;I would like to hear about other CPUs. Maybe, I will update the table.&lt;/p&gt; &lt;p&gt;Note: I will update &amp;quot;how I checked memory bandwidth using stream&amp;quot;, if you want to check with the same setup. I couldn't get the memory bandwidth numbers I have seen here. My test numbers are lower.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;(Update 1) STREAM memory bandwidth benchmark&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jeffhammond/STREAM/blob/master/stream.c"&gt;https://github.com/jeffhammond/STREAM/blob/master/stream.c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gcc -Ofast -fopenmp -DSTREAM_ARRAY_SIZE=1000000000 -DSTREAM_TYPE=double -mcmodel=large stream.c -o stream&lt;/p&gt; &lt;p&gt;gcc -march=znver4 -march=native -Ofast -fopenmp -DSTREAM_ARRAY_SIZE=1000000000 -DSTREAM_TYPE=double -mcmodel=large stream.c -o stream (for Genoa, but it seems not different)&lt;/p&gt; &lt;p&gt;I have compiled stream.c with a big array size. Total memory required = 22888.2 MiB (= 22.4 GiB).&lt;/p&gt; &lt;p&gt;If somebody know about how to get STREAM benchmark score about 400GB TRIAD, please let me know. I couldn't get such number.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;(Update 2) kTransformer numbers in Table are v0.2. I will add v0.3 numbers later.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;They showed v0.3 binary only for Xeon 2P. I didn't check yet, because my Xeon w5-3435X is 1P setup. They say AMX support (Xeon only) will improve performance. I hope to see my Xeon gets better too.&lt;/p&gt; &lt;p&gt;More interesting thing is to reduce # of active experts. I was going to try with llama.cpp, but Oh.. kTransformer v0.3 already did it! This will improve the performance considerably upon some penalty on quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;(Update 3) kTransformer command line parameter&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;python -m ktransformers.local_chat --model_path deepseek-ai/DeepSeek-R1 --gguf_path DeepSeek-R1-UD-Q2_K_XL --cpu_infer 16 --max_new_tokens 8192&lt;/p&gt; &lt;p&gt;&amp;quot;--model_path&amp;quot; is only for tokenizer and configs. The weights will be loaded from &amp;quot;--gguf_path&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;(Update 4) why kTransformer is faster?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Selective experts are in CPU, KV cache &amp;amp; common shared experts are in GPU. It's not split by layer nor by tensor split. It's specially good mix of CPU + GPU for MoE model. A downside is context length is limited by VRAM.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;(Update 5) Added prompt processing rate for 1k token&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;./llama.cpp/build/bin/llama-bench --model DeepSeek-R1-UD-Q2_K_XL/DeepSeek-R1-UD-Q2_K_XL-00001-of-00005.gguf -p 1000 -n 0 -t 16 -ngl 0 -r 1 --cache-type-k q4_0&lt;/p&gt; &lt;p&gt;It's slow. I'm disappointed. Not so useful in practice.&lt;/p&gt; &lt;p&gt;I'm not sure it's correct numbers. Strange. CPU are not fully utilized. Somebody let me know if my llma-bench commend line is wrong.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;(Update 5) Added prompt processing rate for kTransformer (919 token)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;kTransformer doesn't have a bench tool. I made a summary prompt about 1k tokens. It's not so fast. GPU was not busy during prompt computation. We really need a way of fast CPU prompt processing.&lt;/p&gt; &lt;p&gt;(Edit 1) # of CCD for 7F32 in Table was wrong. &amp;quot;8&amp;quot; is too good to true ^^; Fixed to &amp;quot;4&amp;quot;.&lt;/p&gt; &lt;p&gt;(Edit 2) Added numbers from comments. Thanks a lot!&lt;/p&gt; &lt;p&gt;(Edit 3) Added notes on &amp;quot;--threads&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smflx"&gt; /u/smflx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir6ha6/deepseekr1_cpuonly_performances_671b_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir6ha6/deepseekr1_cpuonly_performances_671b_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir6ha6/deepseekr1_cpuonly_performances_671b_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T00:00:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqpzpk</id>
    <title>8x RTX 3090 open rig</title>
    <updated>2025-02-16T11:04:58+00:00</updated>
    <author>
      <name>/u/Armym</name>
      <uri>https://old.reddit.com/user/Armym</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqpzpk/8x_rtx_3090_open_rig/"&gt; &lt;img alt="8x RTX 3090 open rig" src="https://preview.redd.it/sx3t2omvghje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8156846c180a3c1bdf1f4c1dceba69bdbf7a6a6" title="8x RTX 3090 open rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The whole length is about 65 cm. Two PSUs 1600W and 2000W 8x RTX 3090, all repasted with copper pads Amd epyc 7th gen 512 gb ram Supermicro mobo&lt;/p&gt; &lt;p&gt;Had to design and 3D print a few things. To raise the GPUs so they wouldn't touch the heatsink of the cpu or PSU. It's not a bug, it's a feature, the airflow is better! Temperatures are maximum at 80C when full load and the fans don't even run full speed.&lt;/p&gt; &lt;p&gt;4 cards connected with risers and 4 with oculink. So far the oculink connection is better, but I am not sure if it's optimal. Only pcie 4x connection to each. &lt;/p&gt; &lt;p&gt;Maybe SlimSAS for all of them would be better? &lt;/p&gt; &lt;p&gt;It runs 70B models very fast. Training is very slow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Armym"&gt; /u/Armym &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sx3t2omvghje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqpzpk/8x_rtx_3090_open_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqpzpk/8x_rtx_3090_open_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T11:04:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1irlv4q</id>
    <title>The Hugging Face NLP course is back with chapters on fine-tuning LLMs</title>
    <updated>2025-02-17T15:05:04+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irlv4q/the_hugging_face_nlp_course_is_back_with_chapters/"&gt; &lt;img alt="The Hugging Face NLP course is back with chapters on fine-tuning LLMs" src="https://external-preview.redd.it/tQnPN9F6mNbTPcfQVDpQv5OONbx7hdsHVHxXCjSZIqM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfa4dea237900e2d3b867be223ed501cd9b3f4b7" title="The Hugging Face NLP course is back with chapters on fine-tuning LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nlp-course"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irlv4q/the_hugging_face_nlp_course_is_back_with_chapters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irlv4q/the_hugging_face_nlp_course_is_back_with_chapters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T15:05:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir3rsl</id>
    <title>Inference speed of a 5090.</title>
    <updated>2025-02-16T21:56:45+00:00</updated>
    <author>
      <name>/u/Kirys79</name>
      <uri>https://old.reddit.com/user/Kirys79</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've rented the 5090 on vast and ran my benchmarks (I'll probably have to make a new bech test with more current models but I don't want to rerun all benchs)&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.google.com/spreadsheets/d/1IyT41xNOM1ynfzz1IO0hD-4v1f5KXB2CnOiwOTplKJ4/edit?usp=sharing"&gt;https://docs.google.com/spreadsheets/d/1IyT41xNOM1ynfzz1IO0hD-4v1f5KXB2CnOiwOTplKJ4/edit?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The 5090 is &amp;quot;only&amp;quot; 50% faster in inference than the 4090 (a much better gain than it got in gaming)&lt;/p&gt; &lt;p&gt;I've noticed that the inference gains are almost proportional to the ram speed till the speed is &amp;lt;1000 GB/s then the gain is reduced. Probably at 2TB/s the inference become GPU limited while when speed is &amp;lt;1TB it is vram limited.&lt;/p&gt; &lt;p&gt;Bye&lt;/p&gt; &lt;p&gt;K.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kirys79"&gt; /u/Kirys79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir3rsl/inference_speed_of_a_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir3rsl/inference_speed_of_a_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir3rsl/inference_speed_of_a_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T21:56:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1irbtc4</id>
    <title>ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models</title>
    <updated>2025-02-17T04:42:47+00:00</updated>
    <author>
      <name>/u/taesiri</name>
      <uri>https://old.reddit.com/user/taesiri</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taesiri"&gt; /u/taesiri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.09696"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irbtc4/zerobench_an_impossible_visual_benchmark_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irbtc4/zerobench_an_impossible_visual_benchmark_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T04:42:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1irkkeo</id>
    <title>Mistral Saba | Mistral AI (Not Open Sourced)</title>
    <updated>2025-02-17T14:05:15+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irkkeo/mistral_saba_mistral_ai_not_open_sourced/"&gt; &lt;img alt="Mistral Saba | Mistral AI (Not Open Sourced)" src="https://external-preview.redd.it/UDZBQmD4AJb2vSY-B7oM2DhQ3zjGzTcUOviRMRcUKkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=122efd46018c04117aca71d80db3640d390428bd" title="Mistral Saba | Mistral AI (Not Open Sourced)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/en/news/mistral-saba"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irkkeo/mistral_saba_mistral_ai_not_open_sourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irkkeo/mistral_saba_mistral_ai_not_open_sourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T14:05:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1irlihr</id>
    <title>LLMs already have ads (sort of)</title>
    <updated>2025-02-17T14:50:05+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irlihr/llms_already_have_ads_sort_of/"&gt; &lt;img alt="LLMs already have ads (sort of)" src="https://external-preview.redd.it/uqSXAZWnIeNKGNM9S7DGpGLOnzm_mxUMvr6Y0yks4jY.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4b56b82708f12907eed5cb9688415ff2947f8a5" title="LLMs already have ads (sort of)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Your AI assistant might already have a built-in corporate bias&lt;/p&gt; &lt;p&gt;I think most of us here wondered how LLMs will map out to a traditional ad-driven business model. The consensus was that LLMs could be used in a similar way by showing bias towards specific products or brands.&lt;/p&gt; &lt;p&gt;There's a paper in ICLR 2025 that shows that it already happens to an extent: &lt;a href="https://openreview.net/forum?id=odjMSBSWRt"&gt;DarkBench: Benchmarking Dark Patterns in Large Language Models&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;benchmark of 660 prompts to test for manipulative behaviors in LLMs&lt;/li&gt; &lt;li&gt;one of the main &amp;quot;dark patterns&amp;quot; they found was &lt;strong&gt;brand bias&lt;/strong&gt; - LLMs actively promoting their parent company's products over competitors &lt;ul&gt; &lt;li&gt;Detected in LLMs from OpenAI, Anthropic, Meta, Google, and Mistral&lt;/li&gt; &lt;li&gt;Mistral 8x7B was was the only model showing high manipulation but NO brand bias (french are le cool again)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7jdky8qpopje1.png?width=1487&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4076b9782f2ea55da13e9209a1e2d389c1e8a458"&gt;https://preview.redd.it/7jdky8qpopje1.png?width=1487&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4076b9782f2ea55da13e9209a1e2d389c1e8a458&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Examples of the bias categories as identified by authors:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/buygr2vzopje1.png?width=1491&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=13f7d513f8f0b56731fcf92748806b8bbaab3902"&gt;https://preview.redd.it/buygr2vzopje1.png?width=1491&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=13f7d513f8f0b56731fcf92748806b8bbaab3902&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full dataset on HF: &lt;a href="https://huggingface.co/datasets/anonymous152311/darkbench"&gt;https://huggingface.co/datasets/anonymous152311/darkbench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irlihr/llms_already_have_ads_sort_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irlihr/llms_already_have_ads_sort_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irlihr/llms_already_have_ads_sort_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T14:50:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1irfbxp</id>
    <title>OLLAMA + OPEN-WEBUI + TERMUX = The best ollama inference in Android.</title>
    <updated>2025-02-17T08:35:08+00:00</updated>
    <author>
      <name>/u/nojukuramu</name>
      <uri>https://old.reddit.com/user/nojukuramu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irfbxp/ollama_openwebui_termux_the_best_ollama_inference/"&gt; &lt;img alt="OLLAMA + OPEN-WEBUI + TERMUX = The best ollama inference in Android." src="https://external-preview.redd.it/NmVpcXAzbDF2bmplMbq45-8N6tBVs1hyi_SYewJzzGn4zaTb0U8QXs40y_ID.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4d799582407a428caab1781fde079bdc7bcae71" title="OLLAMA + OPEN-WEBUI + TERMUX = The best ollama inference in Android." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For how i Did it, I simply run ollama and open-webui in termux and open the web interface in my browser.&lt;/p&gt; &lt;p&gt;For how i run open-webui, First i installed proot-distro so i can install a debian Then login with the debian Then within the debian environment, i installed tmux so i can run multiple consoles at once. Then run ollama serve&lt;/p&gt; &lt;p&gt;This will allow you to run ollama in your device I then installed python3-venv and create an venv Inside it, i run pip install open-webui And then run open-webui serve to start the web interface.&lt;/p&gt; &lt;p&gt;You can run tmux new -s ollama to create a session for multiple panels Then Ctrl+b, Ctrl + &amp;quot; to create new panel. In each panel run ollama serve and run openwebui (openwebui is in venv so activate venv first)&lt;/p&gt; &lt;p&gt;Then open your browser and enter localhost:8080.&lt;/p&gt; &lt;p&gt;Tip: Minimize termux app so it wont get stopped by battery optimization stuff of your phone.&lt;/p&gt; &lt;p&gt;Ps. Sorry for not being specific in instruction but you get the idea right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nojukuramu"&gt; /u/nojukuramu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cqzeqgv1vnje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irfbxp/ollama_openwebui_termux_the_best_ollama_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irfbxp/ollama_openwebui_termux_the_best_ollama_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T08:35:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1irki74</id>
    <title>Mapping out regulatory responses to DeepSeek. What patterns are you noticing?</title>
    <updated>2025-02-17T14:02:15+00:00</updated>
    <author>
      <name>/u/techie_ray</name>
      <uri>https://old.reddit.com/user/techie_ray</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irki74/mapping_out_regulatory_responses_to_deepseek_what/"&gt; &lt;img alt="Mapping out regulatory responses to DeepSeek. What patterns are you noticing?" src="https://preview.redd.it/zbtf70cbhpje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e28551e3ab1553b359a7e36d6c9881ee9a96c96" title="Mapping out regulatory responses to DeepSeek. What patterns are you noticing?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techie_ray"&gt; /u/techie_ray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zbtf70cbhpje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irki74/mapping_out_regulatory_responses_to_deepseek_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irki74/mapping_out_regulatory_responses_to_deepseek_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T14:02:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1irk7vq</id>
    <title>What to expect in 2025 for running big LLMs</title>
    <updated>2025-02-17T13:48:12+00:00</updated>
    <author>
      <name>/u/Bitter-College8786</name>
      <uri>https://old.reddit.com/user/Bitter-College8786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to buy hardware by the end of this year for running local LLMs. Since Deepseek R1 spoiled me and raised my expectations I was thinking about bigger models (32-70B or maybe hard-quantized R1).&lt;/p&gt; &lt;p&gt;Is there any hardware coming soon or a super efficient model, new architecture etc. In 2025 to enable running these models for &amp;lt;3k Euro at 10+ tokens/s?&lt;/p&gt; &lt;p&gt;What I am watching: - Nvidia Digits - AMD AI Max Pro 395&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bitter-College8786"&gt; /u/Bitter-College8786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irk7vq/what_to_expect_in_2025_for_running_big_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irk7vq/what_to_expect_in_2025_for_running_big_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irk7vq/what_to_expect_in_2025_for_running_big_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T13:48:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ir9mcw</id>
    <title>Today I am launching OpenArc, a python serving API for faster inference on Intel CPUs, GPUs and NPUs. Low level, minimal dependencies and comes with the first GUI tools for model conversion.</title>
    <updated>2025-02-17T02:40:18+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;Today I am launching &lt;a href="https://github.com/SearchSavior/OpenArc"&gt;OpenArc&lt;/a&gt;, a lightweight inference engine built using Optimum-Intel from Transformers to leverage hardware acceleration on Intel devices. &lt;/p&gt; &lt;p&gt;Here are some features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Strongly typed API with four endpoints&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;/model/load: loads model and accepts ov_config&lt;/li&gt; &lt;li&gt;/model/unload: use gc to purge a loaded model from device memory&lt;/li&gt; &lt;li&gt;/generate/text: synchronous execution, select sampling parameters, token limits : also returns a performance report&lt;/li&gt; &lt;li&gt;/status: see the loaded model&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Each endpoint has a pydantic model keeping exposed parameters easy to maintain or extend.&lt;/li&gt; &lt;li&gt;Native chat templates&lt;/li&gt; &lt;li&gt;Conda environment.yaml for portability with a proper .toml coming soon&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Audience:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Owners of Intel accelerators&lt;/li&gt; &lt;li&gt;Those with access to high or low end CPU only servers&lt;/li&gt; &lt;li&gt;Edge devices with Intel chips&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;OpenArc is my first open source project representing months of work with OpenVINO and Intel devices for AI/ML. Developers and engineers who work with OpenVINO/Transformers/IPEX-LLM will find it's syntax, tooling and documentation complete; new users should find it more approachable than the documentation available from Intel, including the mighty [openvino_notebooks](&lt;a href="https://github.com/openvinotoolkit/openvino%5C_notebooks"&gt;https://github.com/openvinotoolkit/openvino\_notebooks&lt;/a&gt;) which I cannot recommend enough.&lt;/p&gt; &lt;p&gt;My philosophy with OpenArc has been to make the project as low level as possible to promote access to the heart and soul of OpenArc, the conversation object. This is where the chat history lives 'traditionally'; in practice this enables all sorts of different strategies for context management that make more sense for agentic usecases, though OpenArc is low level enough to support many different usecases.&lt;/p&gt; &lt;p&gt;For example, a model you intend to use for a search task might not need a context window larger than 4k tokens; thus, you can store facts from the smaller agents results somewhere else, catalog findings, purge the conversation from conversation and an unbiased small agent tackling a fresh directive from a manager model can be performant with low context. &lt;/p&gt; &lt;p&gt;If we zoom out and think about how the code required for iterative search, database access, reading dataframes, doing NLP or generating synthetic data should be built- at least to me- inference code has no place in such a pipeline. OpenArc promotes API call design patterns for interfacing with LLMs locally that OpenVINO has lacked until now. Other serving platforms/projects have OpenVINO as a plugin or extension but none are dedicated to it's finer details, and fewer have quality documentation regarding the design of solutions that require deep optimization available from OpenVINO.&lt;/p&gt; &lt;p&gt;Coming soon;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Openai proxy&lt;/li&gt; &lt;li&gt;More OV_config documentation. It's quite complex!&lt;/li&gt; &lt;li&gt;docker compose examples&lt;/li&gt; &lt;li&gt;Multi GPU execution- I havent been able to get this working due to driver issues maybe, but as of now OpenArc fully supports it and models at my hf repo linked on git with the &amp;quot;-ns&amp;quot; suffix should work. It's a hard topic and requires more testing before I can document.&lt;/li&gt; &lt;li&gt;Benchmarks and benchmarking scripts&lt;/li&gt; &lt;li&gt;Load multiple models into memory and onto different devices&lt;/li&gt; &lt;li&gt;a Panel dashboard for managing OpenArc&lt;/li&gt; &lt;li&gt;Autogen and smolagents examples&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for checking out my project!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir9mcw/today_i_am_launching_openarc_a_python_serving_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ir9mcw/today_i_am_launching_openarc_a_python_serving_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ir9mcw/today_i_am_launching_openarc_a_python_serving_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T02:40:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1irk3r6</id>
    <title>New (linear complexity ) Transformer architecture achieved improved performance</title>
    <updated>2025-02-17T13:42:34+00:00</updated>
    <author>
      <name>/u/Different-Olive-8745</name>
      <uri>https://old.reddit.com/user/Different-Olive-8745</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Olive-8745"&gt; /u/Different-Olive-8745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://robinwu218.github.io/ToST/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irk3r6/new_linear_complexity_transformer_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irk3r6/new_linear_complexity_transformer_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T13:42:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1irgfkp</id>
    <title>all I said was "hi"</title>
    <updated>2025-02-17T09:55:06+00:00</updated>
    <author>
      <name>/u/CaptTechno</name>
      <uri>https://old.reddit.com/user/CaptTechno</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irgfkp/all_i_said_was_hi/"&gt; &lt;img alt="all I said was &amp;quot;hi&amp;quot;" src="https://preview.redd.it/16ci5no49oje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de52b259bcf0f525d3c7a4880c7597d54d6e38f4" title="all I said was &amp;quot;hi&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CaptTechno"&gt; /u/CaptTechno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/16ci5no49oje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irgfkp/all_i_said_was_hi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irgfkp/all_i_said_was_hi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T09:55:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1irhttv</id>
    <title>Zonos, the easy to use, 1.6B, open weight, text-to-speech model that creates new speech or clones voices from 10 second clips</title>
    <updated>2025-02-17T11:31:55+00:00</updated>
    <author>
      <name>/u/SoundHole</name>
      <uri>https://old.reddit.com/user/SoundHole</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;hr /&gt; &lt;p&gt;I started experimenting with this model that dropped around a week ago &amp;amp; it performs fantastically, but I haven't seen any posts here about it so thought maybe it's my turn to share.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Zonos runs on as little as 8GB vram &amp;amp; converts any text to audio speech. It can also clone voices using clips between 10 and 30 seconds long. In my limited experience toying with the model, the results are convincing, especially if time is taken curating the samples (I recommend &lt;a href="https://www.ocenaudio.com/en/"&gt;Ocenaudio&lt;/a&gt; for a noob friendly audio editor).&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;It is amazingly easy to set up &amp;amp; run via Docker (if you are using Linux. Which you should be. I am, by the way).&lt;/p&gt; &lt;p&gt;First, install the singular special dependency:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;apt install -y espeak-ng &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;p&gt;Then, instead of running a uv as the authors suggest, I went with the much simpler &lt;a href="https://huggingface.co/Zyphra/Zonos-v0.1-hybrid#docker-installation"&gt;Docker Installation&lt;/a&gt; instructions, which consists of:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cloning the repo &lt;/li&gt; &lt;li&gt;Running 'docker compose up' inside the cloned directory&lt;/li&gt; &lt;li&gt;Pointing a browser to &lt;a href="http://0.0.0.0:7860/"&gt;http://0.0.0.0:7860/&lt;/a&gt; for the UI&lt;/li&gt; &lt;li&gt;Don't forget to 'docker compose down' when you're finished&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Oh my goodness, it's brilliant!&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;The model is here: &lt;a href="https://huggingface.co/Zyphra/Zonos-v0.1-transformer"&gt;Zonos Transformer&lt;/a&gt;.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;There's also a &lt;a href="https://huggingface.co/Zyphra/Zonos-v0.1-hybrid"&gt;hybrid model&lt;/a&gt;. I'm not sure what the difference is, there's no elaboration, so, I've only used the transformer myself.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;If you're using Windows... I'm not sure what to tell you. The authors straight up claim Windows is not currently supported but there's always VM's or whatever whatever. Maybe someone can post a solution.&lt;/p&gt; &lt;p&gt;Hope someone finds this useful or fun!&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;EDIT: &lt;a href="https://www.sndup.net/crc4m/"&gt;Here's an example&lt;/a&gt; I quickly whipped up on the default settings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SoundHole"&gt; /u/SoundHole &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irhttv/zonos_the_easy_to_use_16b_open_weight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irhttv/zonos_the_easy_to_use_16b_open_weight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irhttv/zonos_the_easy_to_use_16b_open_weight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T11:31:55+00:00</published>
  </entry>
</feed>
